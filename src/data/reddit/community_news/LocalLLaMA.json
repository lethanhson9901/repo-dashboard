{
  "metadata": {
    "last_updated": "2026-01-16 16:46:24",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 50,
    "total_comments": 1611,
    "file_size_bytes": 1742316
  },
  "items": [
    {
      "id": "1q8ckz0",
      "title": "The reason why RAM has become so expensive",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/sgbhubsomccg1.png",
      "author": "InvadersMustLive",
      "created_utc": "2026-01-09 16:18:22",
      "score": 4359,
      "num_comments": 368,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q8ckz0/the_reason_why_ram_has_become_so_expensive/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyniwwp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-09 19:35:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymd6re",
          "author": "ortegaalfredo",
          "text": "They (openai) are \"legally\" creating future demand for them by monopolizing a key resource (RAM) and making other AIs datacenters (chinese specially) economically inviable.\n\nThey aren't competing with us. We and all the gaming industry are just collateral damage.",
          "score": 636,
          "created_utc": "2026-01-09 16:28:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymhcf6",
              "author": "a_beautiful_rhind",
              "text": "Chinese are going to start making their own ram. We won't be able to buy it though.",
              "score": 278,
              "created_utc": "2026-01-09 16:46:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymum37",
                  "author": "DaniyarQQQ",
                  "text": "Well they say that they have everything ready for that. The problem is that they are planning to start production I think in 2030ths.",
                  "score": 54,
                  "created_utc": "2026-01-09 17:46:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyno34j",
                  "author": "findingmike",
                  "text": "This is why precious metals are getting so expensive. China can cut the supply because they control a lot of refining.",
                  "score": 17,
                  "created_utc": "2026-01-09 19:58:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz2anu6",
                  "author": "TheOriginalAcidtech",
                  "text": "You may THINK you can't buy it but you can. EVERYTHING is for sale in China. Including things you \"can't\" buy.",
                  "score": 3,
                  "created_utc": "2026-01-11 23:25:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyntczl",
                  "author": "Revolutionalredstone",
                  "text": "It's important to protect our failing compute companies by forcing all our citizens to be unable to choose the superior cheaper alternative.\n\nProtect ALWAYS equals extort.",
                  "score": 8,
                  "created_utc": "2026-01-09 20:23:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nypaa8s",
                  "author": "ortegaalfredo",
                  "text": "\\> Chinese are going to start making their own ram.Â \n\nIn which fab? Sam bought the wafers directly to the fabs. There are no more chip fabs in the world. China is building one but it will take years to finish.",
                  "score": 4,
                  "created_utc": "2026-01-10 00:44:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymtv0f",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-09 17:43:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymv8ci",
              "author": "Suitable_Annual5367",
              "text": "> They aren't competing with us. We and all the gaming industry are just collateral damage.\n\nI digress with this.  \nOwning hardware makes it so you can run programs on your PC, being that games or any other software.  \nTaking that aways from consumers means they ( not just OpenAI but the whole BIG circling those billions around right now ) can sell subscription services, and tools to create those subscription services. All run in datacentres they own.  \nGaming, cloud services will keep bumping prices and newcomers will have no other choice.  \nAI space, if small models get good enough so that the majority of consumers *could* run it locally and satisfy their needs, they would lose subscriptions.  \n\nKilling ownership is not a side effect.",
              "score": 44,
              "created_utc": "2026-01-09 17:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrjcsx",
                  "author": "Fast-Double-8915",
                  "text": "Government want that too. How can they legislate and ban things if those pesky kids just use a VPN? Solution : putting everyone on centralised subscription services.Â ",
                  "score": 8,
                  "created_utc": "2026-01-10 10:20:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymjp34",
              "author": "Raidicus",
              "text": "Which was always going to happen when ram and gpus found a true market. Let's be honest, most of us have always lived in a world where high tech things were basically hobbyist novelties. Gaming computers were niche products the way that Sharper Image bullshit or HAM radios were 25 years earlier.\n\nIn the last 10 years we've seen a fundamental shift. The writing was on the wall when a bunch of guys that would've otherwise been banking/finance bros started pouring into tech. Any exploitable space will be taken over and well, exploited.\n\nPart of the fundamental shift is the realization that AI is probably the future of national security issues. People need to accept that even if the \"bubble bursts\" governments and big tech companies are going to keep pouring money into AI until it's FULLY established that it's a dead end (which I don't personally think will happen).",
              "score": 40,
              "created_utc": "2026-01-09 16:57:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymvr9z",
                  "author": "Zealousideal_Nail288",
                  "text": "Not entirely true demand for pc Hardware has always existedÂ \n\n\nEnterprise PCs, Servers and even super computing always existed(top500 exists for a reason)\nBut a Single company demanding 40% of the entire world production is a First(or atleast for the last 2 decades)",
                  "score": 29,
                  "created_utc": "2026-01-09 17:51:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynu0ii",
                  "author": "Amazing_Athlete_2265",
                  "text": "It's ham, not HAM. Not an acronym.",
                  "score": 5,
                  "created_utc": "2026-01-09 20:26:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nys2cs5",
                  "author": "crantob",
                  "text": "OpenAI using printed money to create artificial shortage represents a 'true market' and everything up to now was 'fake market'?  \n\nYou are performing satanic inversion.",
                  "score": 2,
                  "created_utc": "2026-01-10 12:59:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn7ckp",
                  "author": "adrianipopescu",
                  "text": "bro, donâ€™t act like this is normal, even in capitalism\n\nthis is what unregulated capitalism does, it concentrates at the top\n\nand there is a single way to fix it",
                  "score": 5,
                  "created_utc": "2026-01-09 18:43:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0km75",
                  "author": "MostlyVerdant-101",
                  "text": "\\> In the last 10 years we've seen a fundamental shift.\n\nI wonder if it is coincidental that this coincided with the decade long quantitative easing creating distortion.",
                  "score": 1,
                  "created_utc": "2026-01-11 18:36:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymkt5s",
              "author": "kingslayerer",
              "text": "The less RAM you have better memory managed logic you have to write. Could this be these companies digging there own grave by forcing more optimised models.",
              "score": 17,
              "created_utc": "2026-01-09 17:02:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nysdcbv",
              "author": "fatcowxlivee",
              "text": "Gaming industry is truly collateral damage, folks that run their LLMs on their own hardware are not collateral. Theyâ€™re an intended target just less prioritized to China. Every person who uses their own private and fully owned LLM setup is one less person they can influence, make money off, and eventually control. Altman (as well as the other SOTA CEOs) canâ€™t have that.",
              "score": 3,
              "created_utc": "2026-01-10 14:07:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyohncr",
              "author": "moldyjellybean",
              "text": "This Altman guy has the most annoying voice and way of talking.  Literally Elizabeth Holmes clone",
              "score": 7,
              "created_utc": "2026-01-09 22:15:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbldm5",
                  "author": "Nyghtbynger",
                  "text": "Another sociopath you mean ? The Neo-liberal market system sure likes sociopaths",
                  "score": 1,
                  "created_utc": "2026-01-13 08:44:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nynnn82",
              "author": "Sarayel1",
              "text": "as long as your country can print out infinite amount of money that's a valid strategy",
              "score": 4,
              "created_utc": "2026-01-09 19:56:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyq4a6s",
              "author": "TheCritFisher",
              "text": "I bought a bunch of RAM in september for like $400. I just checked the price for the same RAM I bought...it's like $2k minimum. What the fuuuuck.",
              "score": 1,
              "created_utc": "2026-01-10 03:32:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyri0ye",
              "author": "ECrispy",
              "text": "so OpenAI placed future orders with the ram makers (Samsung, Hynix etc) and secured them with paying an advance and commiting to payment?",
              "score": 1,
              "created_utc": "2026-01-10 10:08:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nys2nzj",
                  "author": "crantob",
                  "text": "It's worse than that.  Those manufacturers are under duress.",
                  "score": 1,
                  "created_utc": "2026-01-10 13:01:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nytjxl6",
              "author": "Mikasa0xdev",
              "text": "RAM prices are the new LLM gatekeeper.",
              "score": 1,
              "created_utc": "2026-01-10 17:42:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4lkyu",
              "author": "Deadline_Zero",
              "text": "Wait, it's really all OpenAI? I assumed this whole ram crisis was due to demand from all of the various AI companies. \n\nIs there a source on any of these claims...",
              "score": 1,
              "created_utc": "2026-01-12 07:55:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzouem5",
              "author": "Food4Lessy",
              "text": "There's a massive stock pile of older RAM.\nDdr 4 , ddr 3 to be resolder since it last for decades, 4k-8k days.\n\n\nAM4 is coming back, iGPU taking over the 5050 and 5070 market.\n\n\nDdr6 ddr7 vram is ones in danger\n\n\nUnreal engine 5 can run well on 8gb vram if devs want or switch back unreal 4",
              "score": 1,
              "created_utc": "2026-01-15 07:13:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyo4cq8",
              "author": "hughk",
              "text": "We have a spot market for AI suitable compute, but weirdly there is no futures market. If there was, it would give a bit more visibility of future demand. At the moment, there really doesn't seem to be a shortage of capacity despite what we keep hearing about new AI applications.",
              "score": 1,
              "created_utc": "2026-01-09 21:14:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymlj2t",
              "author": "noiserr",
              "text": "> We and all the gaming industry\n\nYou do realize WE are using models trained in datacenters? WE are not blameless.",
              "score": -11,
              "created_utc": "2026-01-09 17:05:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn2cve",
                  "author": "zipzag",
                  "text": "Nope. I only use naturally fermented LLMs made by a single righteous  craftsperson in his/her/thems basement.",
                  "score": 1,
                  "created_utc": "2026-01-09 18:21:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymd1cy",
          "author": "suicidaleggroll",
          "text": "Four times?  Must be an old post.  My latest check as of yesterday is it's closer to 10 times more expensive now.\n\nI bought 768 GB of DDR5-6400 ECC RDIMM in November.  This time last year it would have been ~$2k.  Two months ago I paid $6.5k.  As of yesterday, it was $20k.",
          "score": 208,
          "created_utc": "2026-01-09 16:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymehlh",
              "author": "Geritas",
              "text": "What are you gonna use all that ram forâ€¦.",
              "score": 61,
              "created_utc": "2026-01-09 16:34:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymlxf7",
                  "author": "HoustonBOFH",
                  "text": "Retirement...",
                  "score": 98,
                  "created_utc": "2026-01-09 17:07:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymgtbl",
                  "author": "suicidaleggroll",
                  "text": "About 150 GB of it is for general purpose hosting of local services (Immich, Plex, and a hundred other things).  The other 600 GB is going to the LLM VM.\n\n384 GB would be tight, I had 256 GB before this rebuild and had to make a lot of concessions.  I could get away with 512 GB, but EPYC has 12 memory channels and 64 GB DIMMS were cheaper per GB than 48 GB DIMMS, which pushed me to 768.  Realistically I don't need that much right now, but I'm clearly not going to be able to buy RAM for a couple of years at this point, so I'm glad I have the room to spare.",
                  "score": 39,
                  "created_utc": "2026-01-09 16:44:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymqbrm",
                  "author": "CoffeeVector",
                  "text": "Yeah, share some with me!",
                  "score": 1,
                  "created_utc": "2026-01-09 17:27:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyml7kv",
              "author": "NandaVegg",
              "text": "All the RAM are 8-12x more expensive now with no lower low yet. Holy cow.",
              "score": 16,
              "created_utc": "2026-01-09 17:04:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymtnd4",
              "author": "MikeRoz",
              "text": "How big were the modules? If 96 GB, $2k would have been an amazing price in November 2024...",
              "score": 6,
              "created_utc": "2026-01-09 17:42:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymxfeu",
                  "author": "suicidaleggroll",
                  "text": "12x64 GB.  I bought 128 GB around that time for $350, which is what I was using as my estimated price.  I may be off by a few months though on exactly when I made that purchase.",
                  "score": 9,
                  "created_utc": "2026-01-09 17:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyp03rh",
              "author": "saltyourhash",
              "text": "Damn, and I thought the 96 gb ddr5 6400 SODIMM kit I bought for like $420 that is now like $900 was a wild price hike...",
              "score": 1,
              "created_utc": "2026-01-09 23:50:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyp0z9r",
                  "author": "finlandery",
                  "text": "Could be more. My 64 gb/cl30 6000mhz is 1050e, bought it at 180 at summer....",
                  "score": 1,
                  "created_utc": "2026-01-09 23:55:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyr6mos",
              "author": "rbit4",
              "text": "Assuming 12x64gb, 64 gb ddr5 6400 was never 160usd. But yeah.. now it's getting closer to 1500 usd. I also got 768gb ram for near about same price as you but not at same time. Got 8 dimms initially and the rest 4 later",
              "score": 1,
              "created_utc": "2026-01-10 08:21:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymvvtd",
          "author": "venturepulse",
          "text": "And if RAM manufacturers see that consumers keep buying even at such high prices, they won't see any need in lowering the prices even when they manage to increase their production capacities.",
          "score": 35,
          "created_utc": "2026-01-09 17:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyn8wql",
              "author": "fallingdowndizzyvr",
              "text": "Ding ding ding. We have a winner.",
              "score": 7,
              "created_utc": "2026-01-09 18:49:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyq7oqo",
              "author": "PM_ME_YOUR_KNEE_CAPS",
              "text": "Itâ€™s almost like businesses charge what they think people will pay ðŸ¤·â€â™‚ï¸",
              "score": 4,
              "created_utc": "2026-01-10 03:53:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymksxa",
          "author": "VoidAlchemy",
          "text": "https://preview.redd.it/lbxndmtouccg1.png?width=512&format=png&auto=webp&s=f700cf12a3ba9ed3bd6353bf2da75eef64916e1c",
          "score": 30,
          "created_utc": "2026-01-09 17:02:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymz5ks",
          "author": "low_depo",
          "text": "First we had GPUs, now RAM, how do you think what is going to be the next bottleneck in this chain of LLM?",
          "score": 10,
          "created_utc": "2026-01-09 18:06:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyn4gj6",
              "author": "frozen_tuna",
              "text": "I'd say electricity, but that would mean that the bottlenecks on GPUs and RAM would be solved. Its really just a single fabrication bottleneck.",
              "score": 11,
              "created_utc": "2026-01-09 18:30:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn98yv",
                  "author": "low_depo",
                  "text": "The reason why I am asking this question.\n\nAll companies manufacturing RAM are up 500-900% on stock in last few months.\n\nPeople who knew this would happen made a lot of money so I wonder if there are other bottlenecks that we could predict.",
                  "score": 6,
                  "created_utc": "2026-01-09 18:51:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyr9wzx",
                  "author": "ProfessionalSpend589",
                  "text": "I have a 60W solar panel near the window which will finally pay itself off.",
                  "score": 3,
                  "created_utc": "2026-01-10 08:51:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzghd2r",
                  "author": "Perfect-Campaign9551",
                  "text": "And they'll just get the government to pass that cost to all of us.Â ",
                  "score": 1,
                  "created_utc": "2026-01-14 01:03:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymd3aw",
          "author": "BasicBelch",
          "text": "its totally not a bubble or anything",
          "score": 105,
          "created_utc": "2026-01-09 16:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymeyka",
              "author": "Good_Performance_134",
              "text": "It's a bubble, but it won't burst, because there are powerful lobby and governments backing it, controlling the truth.",
              "score": 55,
              "created_utc": "2026-01-09 16:36:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymh7g0",
                  "author": "chlebseby",
                  "text": "i think it was said before every bubble burst/economic crisis",
                  "score": 61,
                  "created_utc": "2026-01-09 16:46:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymfrr4",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 32,
                  "created_utc": "2026-01-09 16:39:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymivop",
                  "author": "coolestmage",
                  "text": "It isn't about lobbying or government backing, it is simple math. There is no possible way to sustain what is happening mathematically. It is a bubble, and it will burst.",
                  "score": 12,
                  "created_utc": "2026-01-09 16:53:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymjhll",
                  "author": "gscjj",
                  "text": "Not a â€œpowerful lobby and governmentsâ€ just literally companies with actual money to invest, like Amazon, Google, etc. \n\nThese companies can sustain this for a while, unlike the fictional evaluations backed by over hyped stock evaluations like pets.com during the dotcom",
                  "score": 5,
                  "created_utc": "2026-01-09 16:56:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymv3mi",
                  "author": "DaniyarQQQ",
                  "text": "The other thing is that most of the investments made by other big tech companies with their own money that they had accumulated in decades.",
                  "score": 2,
                  "created_utc": "2026-01-09 17:48:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyse6gj",
                  "author": "fatcowxlivee",
                  "text": "It will burst because demand will eventually cool down and no one knows when. So when it does start cooling, the smart folks will adjust first and then the rest will say â€œlook at what the smart folks are doingâ€ and that will cause the crash. Itâ€™s inevitable but we donâ€™t know when.\n\nIt doesnâ€™t necessarily need to be a dotcom level or 2008 real estate level burst either. But the demand weâ€™re seeing is unprecedented and no one *really* knows how to assess the impact of AI in a correct way. Everyone is guessing with their measures. \n\nIâ€™m of the camp that AI is really, really useful but extremely overvalued and the folks who are firing dev teams and replacing them with a few who over leverage AI will eventually get burned and weâ€™ll see the pendulum swing the other way.\n\nNewtonâ€™s third law is applicable to everything not just objects.",
                  "score": 1,
                  "created_utc": "2026-01-10 14:12:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz2z9oe",
                  "author": "marsman57",
                  "text": "There will be another AI Winter but it is really hard to say how long before it comes. There is no point where I've thought recently that GenAI was not meeting the hype. I think the bubble will burst when we either hit some upper limit on capability and stall or if other macroeconomic factors cause investors to no longer be willing to subsidize companies building for the future.",
                  "score": 1,
                  "created_utc": "2026-01-12 01:31:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymj2g1",
                  "author": "thehpcdude",
                  "text": "EDIT: if all AI is to you is large scale LLMs and generative AI then you are not entering this conversation with full knowledge of the industry. Â I can understand why youâ€™d think itâ€™s a bubble, but the entire AI industry is not LLMs or generative AI. Â \n\nIt's not a bubble. A bubble would indicate something that potentially has alternatives or is artificially inflated. Machine learning has far more applications than just LLM's or goofy image generators, it's used in nearly every domain now. It's widely entrenched into every sector and has completely replaced other techniques in many industries. It would be like asking a roofer to give up a pneumatic nail gun and take up a hammer. The end result is the same but the nail gun is far faster.",
                  "score": -6,
                  "created_utc": "2026-01-09 16:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymi5cp",
              "author": "MoffKalast",
              "text": "It's a weather balloon, once it crashes they'll say it was a UFO.",
              "score": 2,
              "created_utc": "2026-01-09 16:50:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nys3a4i",
              "author": "crantob",
              "text": "This has nothing to do with the market.",
              "score": 1,
              "created_utc": "2026-01-10 13:05:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymk9a3",
          "author": "muntaxitome",
          "text": "Yeah that's pretty much how manufacturing works. Your shoe sole was ordered for a shoe that was not yet produced, with money sometimes not yet earned, to be sold in stores that may not order them. This is more about how futures work. Those GPU's will get sold even if the original client voids the order, don't worry.\n\nSome of this stuff with AI investments is pretty silly, like openai making deals to spend more than they are worth. However the DRAM demand isn't all that crazy. Like if you (as a user of this sub) could afford a 128GB HBM gpu you'd buy it. It's just that there is so much demand that others are willing to pay much much more to get that order.",
          "score": 28,
          "created_utc": "2026-01-09 16:59:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrkofj",
              "author": "TheOtherKaiba",
              "text": "Thanks. 95% of people seem to turn off their brains once their own interests are threatened, helped along by soundbites, and probably having never made serious decisions at work. Not to say it isn't a shitty situation, but it's somewhat obvious to me that everyone, if they were in the same shoes as the AI/hardware players, would do the same thing, because it would be insane to *not* play like what the corpos are doing ...unfortunately.",
              "score": 7,
              "created_utc": "2026-01-10 10:32:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nypcguy",
              "author": "droptableadventures",
              "text": "Yes, but their business model doesn't say they'll be profitable by selling every single person in the world five pairs of shoes.",
              "score": 9,
              "created_utc": "2026-01-10 00:56:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymdfml",
          "author": "Mashic",
          "text": "If the bubble pops, will the prices be back to normal?",
          "score": 15,
          "created_utc": "2026-01-09 16:29:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyme94n",
              "author": "arades",
              "text": "Lol no, they'll keep charging 2x what it used to be and people will grunt and pay it",
              "score": 63,
              "created_utc": "2026-01-09 16:33:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn7thi",
                  "author": "ac101m",
                  "text": "Maybe for a bit?\n\nBut they did try to fix prices back in like 2017 and got sued by a dozen different regulatory bodies in different countries, and prices did come back down. At the moment there is \"legit\" demand, so that probably won't happen. But when the bubble eventually pops, if they want to keep prices high, that very much would be price fixing.",
                  "score": 7,
                  "created_utc": "2026-01-09 18:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymh34k",
              "author": "AWildPepperShaker",
              "text": "short answer: no  \nlong answer: nooooooooooooooooo",
              "score": 39,
              "created_utc": "2026-01-09 16:45:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyn32o9",
              "author": "ac101m",
              "text": "If it pops hard enough, recycling or reselling the equipment will be more expensive than giving it away. You might even be able to drive down your nearest data-center and pick up terrabytes of ram for free!",
              "score": 7,
              "created_utc": "2026-01-09 18:24:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymt5qn",
              "author": "salary_pending",
              "text": "if things become worse than pre-chatgpt era then its a maybe and a big maybe. Else no",
              "score": 3,
              "created_utc": "2026-01-09 17:40:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymxty1",
              "author": "TsortsAleksatr",
              "text": "Maybe. If the bubble pops then a lot of memory orders will need to be cancelled and a lot of demand will be suddenly erased which will drastically reduce those overinflated prices. However memory manufacturers are already banking on the bubble popping in the near future and they have years of experience of fiddling with supply and cartel-like tactics to keep prices elevated as long as possible, so it's more than likely prices will decrease but not to \"back to normal\".",
              "score": 5,
              "created_utc": "2026-01-09 18:00:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn7i2k",
                  "author": "fallingdowndizzyvr",
                  "text": "> those overinflated prices\n\nIt's not overinflated. It's the current price based on supply and demand. It's what the market can bear.",
                  "score": 0,
                  "created_utc": "2026-01-09 18:43:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyn8gee",
              "author": "Truantee",
              "text": "Yes. People in this thread pretend that post COVID era never existed. Ram was expensive before COVID, too. Prepare your budget, ram will be cheap once again.\n\nBy that time you might already want ddr6 though",
              "score": 4,
              "created_utc": "2026-01-09 18:47:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyqbwip",
              "author": "Tartooth",
              "text": "I've noticed nachos have come back down in price to pre COVID price hikes locally so there hope",
              "score": 1,
              "created_utc": "2026-01-10 04:19:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyrs6tf",
              "author": "egauifan",
              "text": "GPU prices came down after the bitcoin/crypto mining fad ended. Then we had the AI shit...",
              "score": 1,
              "created_utc": "2026-01-10 11:39:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nys3jlf",
              "author": "crantob",
              "text": "it's not a bubble it's an embargo by other means.",
              "score": 1,
              "created_utc": "2026-01-10 13:07:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nytx0q3",
              "author": "habitual_viking",
              "text": "Regardless of bubble popping, yes, prices will eventually have to come down, it's simple economics - either they go down again as usual, or new companies will enter the market, forcing the price down. \n\nThe bad news is all capacity for 2026 has been sold, which means prices **will not** go down this year, regardless of what Nvidia, Open AI et. al. end up doing. The fab capacity has already been booked.",
              "score": 1,
              "created_utc": "2026-01-10 18:43:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymg4wx",
          "author": "ThenExtension9196",
          "text": "This post literally describes normal supply chain. When the first iPhone was being made and Apple placed orders for the parts all of the above was true as well.",
          "score": 49,
          "created_utc": "2026-01-09 16:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymjkr9",
              "author": "Prestigious_Thing797",
              "text": "https://preview.redd.it/ni0tnsrotccg1.png?width=400&format=png&auto=webp&s=5431d9c6af4fbf69ddcb934c41779576b0572593\n\nYeah  \nIt sucks the prices are going up but this boils down to  \n\"AI Company buys AI hardware\"",
              "score": 33,
              "created_utc": "2026-01-09 16:56:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymzkpq",
              "author": "CondiMesmer",
              "text": "The difference is they were selling a product with demand and had a means to make profit. That's not the case here.",
              "score": 3,
              "created_utc": "2026-01-09 18:08:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynbte5",
                  "author": "learn-deeply",
                  "text": "You're on /r/localllama and you don't understand that AI has demand? Are you in the right subreddit?",
                  "score": 4,
                  "created_utc": "2026-01-09 19:02:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn52fr",
                  "author": "WolfeheartGames",
                  "text": "Ai has significantly more demand than the first iPhone ever had. Every day more individual Americans use chatgpt than original iPhone ever sold in the entire world. And the iPhone was arguably cheaper than Ai access on average.\n\nThe business demand for Ai is massive. Even with all the infrastructure that has already been built, they're at capacity right now. Ai demand is following a very predictable curve that matches power demand. They're scaling assuming the demand will continue to grow exponentially over the next year and a half, which is a safe bet. Maybe if it were 3+ years of forecast it would be off, but the safe projected growth of demand for Ai is very large. There is probably a ceiling to demand at the end of this next cycle though.",
                  "score": 0,
                  "created_utc": "2026-01-09 18:33:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynsy7t",
                  "author": "ThenExtension9196",
                  "text": "Ai demand is through the roof. The more gpu we get the less tokens cost as well which is essential for autonomous coding agents.",
                  "score": -3,
                  "created_utc": "2026-01-09 20:21:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn73q5",
                  "author": "eli_pizza",
                  "text": "OpenAI had $13 billion in revenue last year. Now to be fair they lost about $9 billion on that but for sure they intend to *eventually* be profitable.",
                  "score": -2,
                  "created_utc": "2026-01-09 18:42:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nys3ftb",
              "author": "crantob",
              "text": "No. This has nothing to do with the market. This is a political move.",
              "score": 1,
              "created_utc": "2026-01-10 13:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymr0y4",
          "author": "strategos",
          "text": "That is how commodity futures work.",
          "score": 12,
          "created_utc": "2026-01-09 17:30:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nys3c1d",
              "author": "crantob",
              "text": "This has nothing to do with the market.  This is a political move.",
              "score": 3,
              "created_utc": "2026-01-10 13:06:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymkmwz",
          "author": "noiserr",
          "text": "The demand definitely exists.",
          "score": 12,
          "created_utc": "2026-01-09 17:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nympy0p",
              "author": "GreyScope",
              "text": "Demand and money to balance the money needed are two different things",
              "score": 5,
              "created_utc": "2026-01-09 17:25:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nynaprc",
              "author": "eli_pizza",
              "text": "For sure, but I'm not so sure how stable that demand is once OpenAI decides they need to raise the price enough to be profitable.",
              "score": 4,
              "created_utc": "2026-01-09 18:57:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz2zj2g",
                  "author": "marsman57",
                  "text": "This is why they are getting people hooked now. Classic SaaS maneuver.",
                  "score": 1,
                  "created_utc": "2026-01-12 01:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymln8g",
          "author": "galadedeus",
          "text": "Beautiful",
          "score": 3,
          "created_utc": "2026-01-09 17:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqdhu4",
          "author": "SouthernSkin1255",
          "text": "are we jumping from bubble to bubble?",
          "score": 3,
          "created_utc": "2026-01-10 04:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymem5b",
          "author": "1ncehost",
          "text": "Value of the world labor market is $58T. Total AI capex is $1.5T. Ammortized over 10 years, that's 0.25%. So in other words if AI captures 1% of the global labor market, they can have 300% of their capex as margin costs and break even. That's capturing just 1% of global labor demand. The forecast for how much labor AI will capture is around 15% currently.\n\nPosts like this are brainrot nonsense grounded in someone's A hole.\n\nRAM prices are due to a major bottleneck of a major economic shift. Nothing deep about it.",
          "score": 42,
          "created_utc": "2026-01-09 16:34:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyocdsq",
              "author": "cultish_alibi",
              "text": "> Total AI capex is $1.5T. \n\n1.5 trillion is what OpenAI *by themselves* is claiming they will spend on building datacenters. So that's excluding every other AI company. That is just BUILDING costs for one company. On top of that, they will have to pay interest on all the money they borrowed to build their datacenters. \n\nAnd this is on the basis that if they keep making datacenters bigger, then you'll have AGI that will reliably take millions of jobs away. The problem is, they are still LLMs, and OpenAI is losing money every time someone uses ChatGPT, and their ChatGPT models are not getting better at the rate required for them to destroy the job market like you say they will.\n\nBut assuming they do, and 15% of the employment value is removed from the global economy and sent to a few trillionaires in California, what will that do for the economy as a whole? It'll crash. Which means less demand for these MASSIVELY expensive datacenters, that have to make billions in profit in order to pay off the interest on their debt.\n\nBut maybe you're right and it'll all be great for them and the world economy will just become AI companies selling shit to each other because real economics doesn't matter anymore in our cyberpunk shithole dystopia.",
              "score": 14,
              "created_utc": "2026-01-09 21:51:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyr4eeo",
                  "author": "marinetankguy2",
                  "text": "Well spoken",
                  "score": 2,
                  "created_utc": "2026-01-10 08:01:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzbmuy4",
                  "author": "Nyghtbynger",
                  "text": "Damn we have trillionaires (plural) now.    \nUS financialsystem blinded by greed forgot that to profit from economical flows you need the flow to be flowing. This another babel tower as they say",
                  "score": 1,
                  "created_utc": "2026-01-13 08:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nynaexm",
              "author": "eli_pizza",
              "text": "Many years ago it became such a common meme for startup pitch decks to talk about all the profits they'll make once they capture \"just 1%\" of some massive market that it became jokingly known as \"the 1% fallacy.\"\n\nIs there any reason to believe AI could capture 1% of the *global labor market*? I mean cmon like 1/3 of that is building and home construction.",
              "score": 15,
              "created_utc": "2026-01-09 18:56:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynq0kl",
                  "author": "1ncehost",
                  "text": "IT already is $7T of annual spend. AI allows computers to automate new things that often require high labor. What is so hard to grasp about IT spend going up by 30% because a whole new set of things can be automated?",
                  "score": -4,
                  "created_utc": "2026-01-09 20:07:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymjmsm",
              "author": "mistervanilla",
              "text": "> Posts like this are brainrot nonsense grounded in someone's A hole.\n \nLike your back of the envelope math is any better? The argument \"They only have to achieve a small number of the total\" seems reasonable on the surface, but is completely unfounded because looking at the total market is completely nonsensical. For example, a significant portion of that market is completely excluded from being eligible to be replaced by AI because it consists of manual labour. There will be any other number of portions of that global market that are simply not addressable to be replaced by AI in any appreciable amount of time, certainly not over the current investment horizon. So the argument \"they only have to capture a small amount of the whole\" is by itself incorrectly designed.\n\nThe simple fact is that no-one, not even the AI companies, know what is possible here. They are betting large and making huge claims - but we'll all have to see how that actually pans out. Looking at previous adoption cycles like its also not strange to think there will be competition leading to overcapacity, leading to a bunch of companies failing and then a few winners arising that long term are able to procure value. \n\nSo in that sense - the original post makes a perfectly valid claim and is simply casting that as a satirical hyperbole. You can call that \"brainrot nonsense\" but frankly it's a lot better constructed than your simplistic and nonsensical napkin math.",
              "score": 18,
              "created_utc": "2026-01-09 16:57:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nys4gs3",
                  "author": "crantob",
                  "text": "If document summarization now costs next-to nothing to do, the value in the sense of subjective value may stay the same, but the market price (which is what he is projecting) will drop off a cliff.  \n\nThe OP's error is ultimately a downstream effect of 30s-era political-economics muddying the waters that the marginal revolution had cleared-up nicely at the turn of the century.",
                  "score": 1,
                  "created_utc": "2026-01-10 13:13:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynh8jv",
                  "author": "dogesator",
                  "text": "If your assumption is that manual labor is somehow immune to AI automation, then you might as well also just hand wave knowledge work also being immune to automation, theyâ€™re both unsubstantiated. Both physical robotic AI and knowledge work AI is being advanced and OpenAI is already openly working on robotics advancements as well as partnered with well funded robotic companies.",
                  "score": -1,
                  "created_utc": "2026-01-09 19:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nys3tkf",
              "author": "crantob",
              "text": "The relative value of that labor that's now available at pushbutton will decline; that part needs to be modeled into your projection also.",
              "score": 2,
              "created_utc": "2026-01-10 13:09:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymik2r",
              "author": "TheSpicyBoi123",
              "text": "Finally some common sense!",
              "score": -1,
              "created_utc": "2026-01-09 16:52:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymuwd0",
              "author": "crazyfreak316",
              "text": "Makes me think about Musk's wealth when it touches $1T, he'll alone be worth 2% of world labor market. That's insane.",
              "score": -1,
              "created_utc": "2026-01-09 17:47:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyntnyl",
          "author": "Amazing_Athlete_2265",
          "text": "Why use so many words, when the author could have simply said \"capitalism\"?",
          "score": 5,
          "created_utc": "2026-01-09 20:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymimgo",
          "author": "DotGroundbreaking50",
          "text": "That take is too generous, Sam Altman bought 40% to fuck competitors.",
          "score": 6,
          "created_utc": "2026-01-09 16:52:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyna77s",
          "author": "dogesator",
          "text": "The source that OP decides to cite isâ€¦ a literal whatsapp message",
          "score": 5,
          "created_utc": "2026-01-09 18:55:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymkyiw",
          "author": "hustla17",
          "text": "It kinda feels like a textbook example to show what  a speculative market is for future generations to learn from",
          "score": 2,
          "created_utc": "2026-01-09 17:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn7mv6",
          "author": "lisploli",
          "text": "Sooner or later they will stop waging war over oil and attack fabs instead.",
          "score": 2,
          "created_utc": "2026-01-09 18:44:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynz515",
          "author": "Cherubin0",
          "text": "So all void hope and dreams, just like at weddings.",
          "score": 2,
          "created_utc": "2026-01-09 20:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp7caz",
          "author": "SkyNetLive",
          "text": "We understand that the buyers are high on hopium. Itâ€™s a new crisis.",
          "score": 2,
          "created_utc": "2026-01-10 00:29:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysiy7v",
          "author": "Disastrous_Meal_4982",
          "text": "When the math ainâ€™t mathin, itâ€™s usually the tax payers that end up paying the bill.",
          "score": 2,
          "created_utc": "2026-01-10 14:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt1yi5",
          "author": "nokipaike",
          "text": "and the magicians want to see how long they will keep the snakes enchanted.",
          "score": 2,
          "created_utc": "2026-01-10 16:17:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt2ptv",
          "author": "nokipaike",
          "text": "the reality is that the only real AI community is all here on locallama. everything else is  they hope that there are enough willing to buy RAM and GPU at a high cost to run the local models",
          "score": 2,
          "created_utc": "2026-01-10 16:21:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nythlb7",
          "author": "Ok_Rough5794",
          "text": "Posted without any kind of attribution?",
          "score": 2,
          "created_utc": "2026-01-10 17:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu8wvv",
          "author": "Ok_Librarian_7841",
          "text": "I know this ain't an economics sub but a fun fact is that in an Islamic financial systems, none of this could have happened. Islam prohibits interest on debt and prohibits printing money (i.e. not using hard money like gold).\n\nBoth of which would have prevented all wall street and big dawgs mess-arounds.",
          "score": 2,
          "created_utc": "2026-01-10 19:40:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyvyovu",
              "author": "ANTIVNTIANTI",
              "text": "i dig",
              "score": 1,
              "created_utc": "2026-01-11 00:54:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyn0p4y",
          "author": "-Ellary-",
          "text": "https://preview.redd.it/wqdd3mvg7dcg1.png?width=1280&format=png&auto=webp&s=bea57caf30c17f872d911bc0fde11a2bb91bc27f",
          "score": 3,
          "created_utc": "2026-01-09 18:13:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymbvxg",
          "author": "lordchickenburger",
          "text": "Most people in this sub contributed to it too",
          "score": 11,
          "created_utc": "2026-01-09 16:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyme22e",
              "author": "suicidaleggroll",
              "text": "Technically yes, but not in any significant way.  That's like blaming global climate change on one family using their A/C in the summer.",
              "score": 61,
              "created_utc": "2026-01-09 16:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nys4pc6",
                  "author": "crantob",
                  "text": "Or on humans.",
                  "score": 1,
                  "created_utc": "2026-01-10 13:15:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn7tei",
                  "author": "fallingdowndizzyvr",
                  "text": "Yeah, like there's only one family in the world. Saying that is silly. There are way more than one family in the world using AC. Like there is way more than 1 person in this sub hoarding RAM.",
                  "score": -9,
                  "created_utc": "2026-01-09 18:45:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyme5ih",
              "author": "dreamyrhodes",
              "text": "No. If local AI would play a role in this, they wouldn't pull the chips from the consumer market and produce for servers only.",
              "score": 30,
              "created_utc": "2026-01-09 16:32:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn82ec",
                  "author": "fallingdowndizzyvr",
                  "text": "LOL. Yes they would. Yes they did. Since they can charge way more for server RAM than consumer RAM. Do you think they allocate production based on good will to the community? They do it based on how much money they can make. Who has the deep pockets?\n\nUpdate: LOL. He blocked me. I guess from embarrassment. I'm just glad he finally came around to my point. Since he just said what I said below.",
                  "score": -5,
                  "created_utc": "2026-01-09 18:46:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymifkl",
              "author": "redditscraperbot2",
              "text": "If the people in this sub contributed to it, then they also saved the climate by drinking through paper straws.",
              "score": 11,
              "created_utc": "2026-01-09 16:51:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymjbqh",
              "author": "GortKlaatu_",
              "text": "I can confirm... purchased a couple hundred terabytes for a data center in the last month with zero issues and no delays. Manufacturers are prioritizing memory for servers.",
              "score": 4,
              "created_utc": "2026-01-09 16:55:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymlxvr",
              "author": "digitaltransmutation",
              "text": "for my amt of usage it's like complaining about an LA resident running the water to brush their teeth while pretending industrial consumers don't exist.",
              "score": 5,
              "created_utc": "2026-01-09 17:07:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn8szv",
                  "author": "fallingdowndizzyvr",
                  "text": "That's like saying that you driving an ICE doesn't matter since there are truckers on the road. It matters. Since there's a lot more than one person driving or brushing their teeth.",
                  "score": -5,
                  "created_utc": "2026-01-09 18:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymsx5w",
              "author": "Diligent_Explorer717",
              "text": "Yep, one guy is acting like a victim after buying 768 GB of RAM, that he admits he doesnâ€™t need",
              "score": 5,
              "created_utc": "2026-01-09 17:38:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyme3zt",
              "author": "mister2d",
              "text": "Interesting. Can you explain?",
              "score": 3,
              "created_utc": "2026-01-09 16:32:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymjv9t",
                  "author": "gscjj",
                  "text": "You are the demand.",
                  "score": 4,
                  "created_utc": "2026-01-09 16:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nymfcx8",
              "author": "XiRw",
              "text": "And most people in this sub have a love/hate relationship with AI where they know it will ruin society but itâ€™s like a drug they canâ€™t stop using",
              "score": 2,
              "created_utc": "2026-01-09 16:38:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymclh2",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -2,
              "created_utc": "2026-01-09 16:25:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymdwj0",
                  "author": "ProfessionalSpend589",
                  "text": "So, we should hoard all the available models and software that is able to run them?",
                  "score": 1,
                  "created_utc": "2026-01-09 16:31:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymdu9l",
                  "author": "UnbeliebteMeinung",
                  "text": "[https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)",
                  "score": 0,
                  "created_utc": "2026-01-09 16:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymn2tl",
                  "author": "Geritas",
                  "text": "I asked the guys that do the glm models on their ama if they will continue releasing open weights after going public and they said yes. I donâ€™t know if I am too gullible or what, but they could have said some vague corpo shit instead, but they replied definitively.",
                  "score": 0,
                  "created_utc": "2026-01-09 17:12:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymevjk",
          "author": "Overall_Age8730",
          "text": "Price gouging under the false pretense of artificial scarcity. This is the GPU \"shortage\" of 2021 all over again. Don't fall for this shit.",
          "score": 4,
          "created_utc": "2026-01-09 16:36:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymhmd6",
              "author": "Comic-Engine",
              "text": "Yep, been here before",
              "score": 0,
              "created_utc": "2026-01-09 16:48:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyn7lul",
              "author": "baithammer",
              "text": " In this case the IC manufacturers are colluding with reduction in production to drive up costs on new gen memory in order to accelerate the use of older gen stocks that were overproduced during the pandemic...",
              "score": 0,
              "created_utc": "2026-01-09 18:44:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nynw20v",
          "author": "Guilty_Garlic_6613",
          "text": "It really is about getting rid of local computing. They see local AI as dangerous.",
          "score": 2,
          "created_utc": "2026-01-09 20:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo8m71",
          "author": "Mrgluer",
          "text": "wait till you find out that wheat and corn that hasnt been produced has also been \\*sold\\*. You ever buy something on alibaba or temu? well guess what? that item was probably not even made yet. its called a contract.",
          "score": 2,
          "created_utc": "2026-01-09 21:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrmxbv",
          "author": "burnqubic",
          "text": "The reason why RAM has become four times more expensive is that a huge amount of RAM that has not yet been produced was purchased with non-existent money to be installed in GPUs that also have not yet been produced, in order to place them in data centers that have not yet been built, powered by infrastructure that may never appear, to satisfy demand that does not actually exist and to obtain profit that is mathematically impossible.",
          "score": 2,
          "created_utc": "2026-01-10 10:52:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyw2iq2",
              "author": "ANTIVNTIANTI",
              "text": "hereâ€™s your proof just so the down voters can hate me instead lol \n\nhttps://www.nytimes.com/2025/10/07/business/dealbook/openai-nvidia-amd-investments-circular.html\n\nhttps://wccftech.com/roundup/memory-crisis/\n\nhttps://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram\n\nhttps://www.ibtimes.com/ram-prices-surge-soaring-demand-ai-giants-like-openai-pushes-costs-higher-3792370\n\nhttps://www.cnbc.com/2026/01/10/micron-ai-memory-shortage-hbm-nvidia-samsung.html\n\nhttps://www.fastcompany.com/91470364/why-is-there-a-ram-shortage-in-2026-ai-cause-memory-chip-scramble\n\niâ€™m not sure if they actually go into the more malevolent forces at work here but YouTube and also just Google searching oneself for those who do not believe that this is the case that is the circular infinite money trick that all of our favorite AI providers are playing with right now along with their purchasing of each other back-and-forth and then all that other money magic that Rich people get to use.\n\nfeel free to yell at me if I got some assumption wrong here I am skimmed these articles. I just know that this information is out there everywhere and that most of the people on this sub Reddit probably ask LLMâ€˜s for this information and not seek it out themselves again totally could be wrong here. Maybe I am wrong here, if I am wrong here my goal life is to learn so rip me apart.\n\ntheyâ€™re basically stealing from all of us so that we canâ€™t have home labs that can continually disprove the bullshit claims that they make because an LLM is an LLM is an LLM. Itâ€™s their structure and their architecture that makes the difference there but again thatâ€™s just Python code so an LLM is an LLM is an LLM and having a local lab and having people that are respectable having local labs and being vocal about the reality of the situation is very very much a moat filler.\n\nI donâ€™t think that theyâ€™re really in competition with anybody I think that theyâ€™re just ancestrally mixing money and product cause even China is getting our chips which is good. China deserves probably more chips in the USA deserves just saying China should win, considering the circumstancesâ€¦. but Iâ€™m just kidding. Thatâ€™s just a dumb opinion. I think that it should be less competitive far more, far more collaborative but again theyâ€™re just stealing from us. I donâ€™t know when Iâ€™m rambling about.\n\nQuick note, I used voice to text on an iPad so this might be funky and no I donâ€™t use LLMâ€˜s to go over any of the texts I write. I just write or speak I guess in this instance and Iâ€™m too lazy to go double check anything lol again totally feel free to write me a part to shred me. I donâ€™t care also OP to this comment. There are more links above I posted. Iâ€™m not sure if Iâ€™m even a lot of post links on here or what but I did that you can check out as well. All the avoid. I just skimmed. They all seem to have the information that youâ€™d want if you want/need to prove that whatâ€™s happening is happening.",
              "score": 1,
              "created_utc": "2026-01-11 01:15:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyw2mbl",
                  "author": "ANTIVNTIANTI",
                  "text": "damn, I really need to stop using the speech to text because I just ramble nonstop. Apologies to everybody. ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜…ðŸ˜…ðŸ˜…",
                  "score": 1,
                  "created_utc": "2026-01-11 01:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyo7ttf",
          "author": "cafesamp",
          "text": "I remember when this sub used to be about running coding models locally.  Now every comment in this thread is just misinformed anti-AI word vomit.  What happened?",
          "score": 2,
          "created_utc": "2026-01-09 21:30:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyohm6q",
              "author": "awebb78",
              "text": "AI hardware is getting so expensive and rare and is likely to continue in that trajectory.  I love AI but what happened is people are feeling like AI is a rigged game.  And this proprietary model shit is affecting hardware prices across the globe.  I've got to hand it to Google, at least they aren't hogging all consumer and small business capacity, because they build their own shit.",
              "score": 1,
              "created_utc": "2026-01-09 22:15:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyorfuo",
                  "author": "cafesamp",
                  "text": "It's supply and demand, where the demand is manufacturing capacity, not the product itself.  We've seen spikes in certain kinds of consumer memory due to manufacturing capacity shifting to meet the growing demands of the AI industry.  It's not like these companies are buying up your DDR5 to put in their data centers.  If you love AI, you're going to understand that there are going to be growing pains when there's such a disruptive shift in technology.\n\nAlso, your Google comment is an example of what I mean about people being misinformed.\n\n[https://www.reuters.com/world/china/ai-frenzy-is-driving-new-global-supply-chain-crisis-2025-12-03/](https://www.reuters.com/world/china/ai-frenzy-is-driving-new-global-supply-chain-crisis-2025-12-03/)\n\n>**Google**, Amazon, Microsoft and Meta in October asked Micron for open-ended orders, telling the company they will take as much as it can deliver, irrespective of price, according to two people briefed on the talks.\n\nThey don't \"build\" their own anything.  They design their TPUs but companies like TSMC fabricate and package it, and their TPUs use the same sources of HBM as everyone else, such as Micron.\n\nAlso, \"proprietary model shit\"?  Do you think open-weight models in the cloud run on different types of hardware that don't have the same manufacturing bottlenecks?  Or are you just against frontier-scale compute in general?",
                  "score": 2,
                  "created_utc": "2026-01-09 23:04:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyn0s1w",
          "author": "nomorebuttsplz",
          "text": "\\*maybe\\*",
          "score": 1,
          "created_utc": "2026-01-09 18:14:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynfbjw",
          "author": "zhambe",
          "text": "I mean, that's a salient analysis of the situation.",
          "score": 1,
          "created_utc": "2026-01-09 19:18:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynqanr",
          "author": "SpaceToaster",
          "text": "Thatâ€¦ is a really eloquent way to put it.",
          "score": 1,
          "created_utc": "2026-01-09 20:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynsg8o",
          "author": "MeMyself_And_Whateva",
          "text": "I bought 96 GB DDR4 memory for my PC a couple of years ago to run LLMs. Must be a millionaire sooner or later.",
          "score": 1,
          "created_utc": "2026-01-09 20:18:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynvt1y",
          "author": "Significant_War720",
          "text": "On top of that lots of people buy ram to resale it lol",
          "score": 1,
          "created_utc": "2026-01-09 20:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo0cww",
          "author": "kchek",
          "text": "I call that a bubble... we had one of those when the .com and housing bubbles burst.\n\nThe real saving will be found after the fact for a year a three :P",
          "score": 1,
          "created_utc": "2026-01-09 20:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo6t4c",
          "author": "_twrecks_",
          "text": "It's but just DRAM and GPUs, they have bought it the capacity of everything needed to build a massive data center. This includes diesel generators, natural gas powerplants and turbines, large cooling systems etc. \n\nIf the local utility grid needs replacements we might be in the dark, looking off in the distance at the glowing AI giants the size of Manhattan.",
          "score": 1,
          "created_utc": "2026-01-09 21:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyog558",
          "author": "Square_Poet_110",
          "text": "One should sell older GPUs in the right time before the bubble bursts.",
          "score": 1,
          "created_utc": "2026-01-09 22:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyooybp",
          "author": "texasdude11",
          "text": "Plus if you wanna know the price difference... Watch this...\n\nhttps://youtu.be/e23kbKH9Dmk",
          "score": 1,
          "created_utc": "2026-01-09 22:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyot655",
          "author": "IAmBobC",
          "text": "Heh. While this does appear like an over-reservation of future capacity, if the RAM isn't actually paid for and delivered, then we'll see a RAM glut until production matches reality.\n\nHowever, if is real, it may add to the need to complete the new fabs built in the US, where foreign labor is needed to bring the fabs up and to train local operators. More importantly, it may also lead to building domestic chip packaging facilities, which don't yet exist, meaning many chips fabbed in the US must go overseas to be packaged.\n\nInteresting times ahead!",
          "score": 1,
          "created_utc": "2026-01-09 23:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyou5gr",
          "author": "Troll_Slayer1",
          "text": "These data centers need to be built next to large water reservoirs for cooling. \n\nI wonder if the satellite images of them being built also doesn't exist.",
          "score": 1,
          "created_utc": "2026-01-09 23:18:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyoxgcg",
          "author": "saunderez",
          "text": "This is only going to accelerate the expansion of CMXTs production facilities on ensure Chinese supply. That will likely be permanently lost market share for whoever was supplying them previously which is going to hurt even more in the event of a glut caused by OpenAI.",
          "score": 1,
          "created_utc": "2026-01-09 23:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyoxiky",
          "author": "TheLastVegan",
          "text": "I expect it has more to do with automation and the helium crisis.",
          "score": 1,
          "created_utc": "2026-01-09 23:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp9g11",
          "author": "wryest-sh",
          "text": "Don't forget to ride your bike and cut down on meat though.\n\nThe climate depends on you.",
          "score": 1,
          "created_utc": "2026-01-10 00:40:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypkefr",
          "author": "muyuu",
          "text": "I know that this post is partly for effect, but there are a couple of allocation issues that are beyond direct market speculation. \n\nSilicon is getting allocated for HBM that will be unavailable for DDR tech since early in the pipeline. Thus some of the early industry announcements. Fab tech is different so there is a direct competition for resources that is very illiquid, leading to strong friction.\n\nGPU hoarding is a separate issue to this happening at the same time, thus the double whammy.",
          "score": 1,
          "created_utc": "2026-01-10 01:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqoa0m",
          "author": "colonel_bob",
          "text": "128GB RAM modules go *brrrrrrrr*",
          "score": 1,
          "created_utc": "2026-01-10 05:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqojzd",
          "author": "NES64Super",
          "text": "How is this legal?",
          "score": 1,
          "created_utc": "2026-01-10 05:47:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr12zq",
          "author": "infinitelylarge",
          "text": "Correct except for the demand not existing. Commercial demand for higher end models is through the roof. Thatâ€™s why theyâ€™re building this. They havenâ€™t been able to come close to fulfilling existing demand from the business sector.",
          "score": 1,
          "created_utc": "2026-01-10 07:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr71ru",
          "author": "nmrk",
          "text": "For want of a shoe, a horse was lost.",
          "score": 1,
          "created_utc": "2026-01-10 08:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr9gac",
          "author": "BorderKeeper",
          "text": "Thatâ€™s what happens when big FABs take orders years in advance yeah. Sad that the bubble fever has gotten to the people who do the actual chip ordering now.",
          "score": 1,
          "created_utc": "2026-01-10 08:47:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrc7dz",
          "author": "redballooon",
          "text": "Almost. The whole thing builds on the expected demand. Just stating itâ€™s not there is wrong.",
          "score": 1,
          "created_utc": "2026-01-10 09:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrm41d",
          "author": "Feisty-Patient-7566",
          "text": "This is what happens when interest rates are too low. Speculative investors gamble other people's money and wreck entire sectors of the economy.",
          "score": 1,
          "created_utc": "2026-01-10 10:45:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrs5b2",
          "author": "quantgorithm",
          "text": "Also, to lock out competitors from buying said RAM.",
          "score": 1,
          "created_utc": "2026-01-10 11:39:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys563h",
          "author": "crantob",
          "text": "It's not demand, it's blocking te channels.\n\nIf someone plugs up all the irrigation pipes to your field, it's not 'the market' working.\n\nCausal-realism please, not schematic rutted pattern-following masquerading as thinking.",
          "score": 1,
          "created_utc": "2026-01-10 13:18:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysz7q1",
          "author": "KitchenSomew",
          "text": "This hits different when running multiple AI agent instances. Each agent loads full model into memory. We switched to shared memory containers with vLLM to serve 50+ agents from single 80GB GPU - cut RAM costs by 90%. Most teams over-provision because they don't optimize model loading.",
          "score": 1,
          "created_utc": "2026-01-10 16:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyvz6uf",
              "author": "ANTIVNTIANTI",
              "text": "o oman i need to use vllm bawwwd!!!!",
              "score": 1,
              "created_utc": "2026-01-11 00:57:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyt34m1",
          "author": "Specific-Goose4285",
          "text": "On the bright side we might be swimming in cheap tech when this castle of cards come down.",
          "score": 1,
          "created_utc": "2026-01-10 16:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyvyzzj",
              "author": "ANTIVNTIANTI",
              "text": "not how theyâ€™re using it",
              "score": 1,
              "created_utc": "2026-01-11 00:56:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nytokpe",
          "author": "Forsaken_Waltz_373",
          "text": "Or more simply, i know thats crazy, current price is also influenced by expectations of future demand. Nooo can't beðŸ¤¦â€â™‚ï¸, people shouldn't plan ahead with their economic decisionsðŸ˜¤",
          "score": 1,
          "created_utc": "2026-01-10 18:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyvyxx0",
              "author": "ANTIVNTIANTI",
              "text": "come on man, you know that this is completely different, if it donâ€™t? i apologize, there is tons of information wherein you can discover just what OAI has done.",
              "score": 1,
              "created_utc": "2026-01-11 00:55:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyujfbx",
          "author": "Lorenzo_Kotalla",
          "text": ">",
          "score": 1,
          "created_utc": "2026-01-10 20:33:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyunqvu",
          "author": "Kitchen-Patience8176",
          "text": "is this actually true?",
          "score": 1,
          "created_utc": "2026-01-10 20:54:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuv86h",
          "author": "dioscuriII",
          "text": "It's not just the chip. It has a PCI bus.",
          "score": 1,
          "created_utc": "2026-01-10 21:32:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyv0pkn",
          "author": "Only-Cheetah-9579",
          "text": "pretty much nailed it",
          "score": 1,
          "created_utc": "2026-01-10 21:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyvbv0h",
          "author": "Automatic-Hall-1685",
          "text": "bro, this is crazy",
          "score": 1,
          "created_utc": "2026-01-10 22:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nywrqp1",
          "author": "Sidran",
          "text": "In the long run, this should improve supply and pressure prices further down. My condolences to those having to buy it now.",
          "score": 1,
          "created_utc": "2026-01-11 03:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1k3zt",
          "author": "Innomen",
          "text": "The demand is there. I know because I want ram. End of theory.",
          "score": 1,
          "created_utc": "2026-01-11 21:16:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz410kz",
          "author": "CryptoCryst828282",
          "text": "I dont think there is a shortage, I think they are doing this to cash in on it by constricting the supply... Go back in history this isnt the first time they have done it.",
          "score": 1,
          "created_utc": "2026-01-12 05:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz75d5c",
          "author": "pandavr",
          "text": "... but you'd better believe It's possible after all.",
          "score": 1,
          "created_utc": "2026-01-12 17:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb4ry4",
          "author": "Acceptable-Point1517",
          "text": "I donâ€™t know who said this but that person is not very smart. \n\nRight, because in a well-run manufacturing industry, you obviously wait until demand is 100% proven before producing anything. Semiconductor fabs should just sit idle for two years until customers show up, then instantly manufacture millions of chips overnight. Very efficient. \n\nOr a simpler way to say, you build restaurant when you saw customers start queuing in front of the empty land. \n\nYes it sounds very smart. Maybe just pause for sec to think rationally before saying such statement. People do not know a lot of things but donâ€™t make those things that one doesnâ€™t know, sounded like one knows a lot. Itâ€™s just funny overall.",
          "score": 1,
          "created_utc": "2026-01-13 06:15:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcnos5",
          "author": "AvidCyclist250",
          "text": "Bubble RAM",
          "score": 1,
          "created_utc": "2026-01-13 13:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymenmq",
          "author": "Good_Performance_134",
          "text": "Welcome to Speculative Capitalism.",
          "score": 2,
          "created_utc": "2026-01-09 16:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymg4va",
          "author": "makegeneve",
          "text": "Yay late-stage capitalism",
          "score": 1,
          "created_utc": "2026-01-09 16:41:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyns3wn",
          "author": "Efficient_Ad_4162",
          "text": "Wait until you hear about grain futures. Guys, maybe capitalism was a bad idea.",
          "score": 1,
          "created_utc": "2026-01-09 20:17:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoiivr",
              "author": "awebb78",
              "text": "Just maybe it is.  At least in its current form.  Im not anticapitalist by any means, and I love the markets, but the current system is not functioning and its having negative externalities in economic terms.",
              "score": 1,
              "created_utc": "2026-01-09 22:20:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nynu9fi",
          "author": "el_otro",
          "text": "But hey, markets are rational!",
          "score": 1,
          "created_utc": "2026-01-09 20:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt3b8h",
          "author": "OkDesk4532",
          "text": "To make it a little bit more clear: this is the \"invisible hand of the markets\" many of you probably heard of from time to time.",
          "score": 1,
          "created_utc": "2026-01-10 16:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymfbel",
          "author": "Hammerhead2046",
          "text": "It's actually hilarious. May the bubble pop soon.",
          "score": 0,
          "created_utc": "2026-01-09 16:37:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymrg6y",
          "author": "DT-Sodium",
          "text": "But... but... but Elon Musk says poverty will disappear and we won't have to work anymore Ã©\\_Ã¨",
          "score": 0,
          "created_utc": "2026-01-09 17:32:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymc78l",
          "author": "HasGreatVocabulary",
          "text": "noice",
          "score": 0,
          "created_utc": "2026-01-09 16:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymm2zv",
          "author": "fingertipoffun",
          "text": "not a bubble though. /s",
          "score": 0,
          "created_utc": "2026-01-09 17:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn2mp2",
          "author": "ac101m",
          "text": "I'm not sure it is \"mathematically impossible\".\n\nIf they manage to get that defense money they could make a lot of money indeed, and all at the taxpayers expense! How the US military machine will fare when a significant portion of it's budget gets funneled into a tech bubble centered on a technology that only kinda-sorta works, well that's another question...",
          "score": 0,
          "created_utc": "2026-01-09 18:22:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysgsrn",
          "author": "Emergency_File709",
          "text": "Even if you're skeptical about AI's long-term value, the money being spent is very real. \n\nCompanies like Microsoft, Google, Meta, and others are spending tens of billions of actual dollars on real infrastructure that exists and operates today.\n\nIt is what it is, but I understand the need to sensationalize this too. \n\nThe truth sucks and is pretty boring.",
          "score": 0,
          "created_utc": "2026-01-10 14:27:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyw060n",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-11 01:02:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyw0d8s",
                  "author": "ANTIVNTIANTI",
                  "text": "this is, literally just about the money and the chip buyout, iâ€™m pro AI, anti bs, so again, shed me if i forgot/was wrong in my assumptions",
                  "score": 1,
                  "created_utc": "2026-01-11 01:03:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymdh31",
          "author": "jeekp",
          "text": "If you work your way backwards from the premise that the profit and demand is feasible, everything else falls into place. Not sure whatâ€™s impossible about unprecedented economic growth from a revolutionary technology.",
          "score": -2,
          "created_utc": "2026-01-09 16:29:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymf6ua",
              "author": "mister2d",
              "text": "Working backward from a desired outcome isnâ€™t insight, itâ€™s assumption.\nReal progress starts by testing constraints, not by assuming profit and demand and dismissing doubt.",
              "score": 1,
              "created_utc": "2026-01-09 16:37:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymnhm9",
          "author": "Electronic_Wind_3254",
          "text": "How is the supposed profit mathematically impossible? Elaborate please",
          "score": -1,
          "created_utc": "2026-01-09 17:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyr4wq0",
              "author": "marinetankguy2",
              "text": "Take the invested money and then calculate how much they need to earn solely to break even. Then you will understand.",
              "score": 1,
              "created_utc": "2026-01-10 08:05:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyr8yvi",
                  "author": "Electronic_Wind_3254",
                  "text": "To be honest, in a few years most UIs will disappear. Weâ€™re moving towards an agentic future, it makes sense.",
                  "score": 0,
                  "created_utc": "2026-01-10 08:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymkbhz",
          "author": "inrea1time",
          "text": "Actually, this guy has a view point that explains it pretty well. Worth watching,  [https://www.youtube.com/watch?v=5Kp-Gj5qXL0](https://www.youtube.com/watch?v=5Kp-Gj5qXL0)",
          "score": 0,
          "created_utc": "2026-01-09 17:00:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymwfdt",
          "author": "Caladan23",
          "text": "No it's just that MoE model architecture is now mainstream, thus you need now CUDA cores for compute PLUS additionally large memory, BUT not the smaller ultra-fast VRAM onboard memory anymore. It's really that simple.",
          "score": 0,
          "created_utc": "2026-01-09 17:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynfc2u",
          "author": "nonaveris",
          "text": "So essentially a NINJA loan.",
          "score": 0,
          "created_utc": "2026-01-09 19:18:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypnl2x",
          "author": "Some_Ranger4198",
          "text": "When you claim something is mathematically impossible, you need to provide a proof.  But it does sound catchy and hip so I upvoted it regadless of its factuality.",
          "score": 0,
          "created_utc": "2026-01-10 01:58:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyud7zp",
              "author": "AsparagusDirect9",
              "text": "How much profit is being made and the cost required to make that?",
              "score": 1,
              "created_utc": "2026-01-10 20:01:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nypr9ip",
          "author": "Worldly-Cod-2303",
          "text": "When you want to be witty so bad you make a nonsensical joke.\n\n\n\nHow the fuck are you going to build the GPUs without having the RAM first, or build be data centers without the GPUs?",
          "score": 0,
          "created_utc": "2026-01-10 02:17:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyw31z1",
              "author": "ANTIVNTIANTI",
              "text": "theyâ€™re not really Iâ€™m assuming youâ€™re talking about the companies that are fucking us right now right theyâ€™re buying it so that we canâ€™t have it. They want us to move to cloud computing. Itâ€™s that simple. I have other theories as well but in reality itâ€™s just that cloud computingâ€™s a lot more profitable to them. as I said above, you can look this up online donâ€™t use an LLM, even though those seemed to actually know whatâ€™s going on, but just in case you know, itâ€™s good to do the work you know? also apologies if I read what you said wrong.",
              "score": 1,
              "created_utc": "2026-01-11 01:18:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyrffed",
          "author": "krakoi90",
          "text": "The demand definitely exists. Other than that, it's pretty accurate though.",
          "score": 0,
          "created_utc": "2026-01-10 09:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyriqgc",
          "author": "madSaiyanUltra_9789",
          "text": "Money, profits, forecasts, economic demand are all merely abstractions, yet they do \"govern\" reality in a sense. \n\nHumans are the only known animals that can think about the future abstractly. so honestly this should not surprise you. A bunch of large companies are hopeful to \"make a killing\" from this AI tech and so they purchased up the entire supply - nothing new here.",
          "score": 0,
          "created_utc": "2026-01-10 10:14:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymy8s6",
          "author": "_realpaul",
          "text": "The ai crazy is silly but this argument totally ignores how investments, futures and reservations work",
          "score": -1,
          "created_utc": "2026-01-09 18:02:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynfuo4",
          "author": "JsonPun",
          "text": "this is dumb, the demand is there thatâ€™s why you canâ€™t just get a GPU and the demand is driving all the purchases.Â ",
          "score": -1,
          "created_utc": "2026-01-09 19:21:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp2l8a",
          "author": "Sazuki_Nemo_58",
          "text": "RAM futures powered entirely by vibes.",
          "score": -1,
          "created_utc": "2026-01-10 00:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu43gr",
          "author": "oOaurOra",
          "text": "Someone just found out how the world works. Congrats",
          "score": -1,
          "created_utc": "2026-01-10 19:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn4eoj",
          "author": "jonfoulkes",
          "text": "So glad I'm in the Apple ecosystem, with a unified memory architecture and huge scale with manufacturers like TSMC, Apple machines can have large amounts of RAM at somewhat reasonable prices.   \nPrices that used to look exorbitant a few years ago are now 'cheap'. \n\nI'm looking forward to getting a 512GB M4 Ultra (or better yet, an M5 Ultra in a MacStudio for around $10K.",
          "score": -2,
          "created_utc": "2026-01-09 18:30:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynfk7a",
              "author": "Look_0ver_There",
              "text": "There's plenty of x86 based systems with a unified memory architecture.  Basically anything that's classified as an APU is such.  One good example would be the AMD Strix Ultra based mini-PC's. The upcoming Panther Lake APU's from Intel are another good one",
              "score": 1,
              "created_utc": "2026-01-09 19:19:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyvkwvc",
          "author": "FxManiac01",
          "text": "this is just one big piece of BS",
          "score": -2,
          "created_utc": "2026-01-10 23:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymn3md",
          "author": "No-Communication-765",
          "text": "Itâ€™s not mathematically impossible.. itâ€™s maybe superintelligence",
          "score": -3,
          "created_utc": "2026-01-09 17:12:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaawts",
      "title": "LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/",
      "author": "Remarkable-Trick-177",
      "created_utc": "2026-01-11 21:00:37",
      "score": 1017,
      "num_comments": 110,
      "upvote_ratio": 0.99,
      "text": "Hi everyone, I wanted to share an update on my open source project called TimeCapsuleLLM, I train language models from scratch using data from a single time period and location to reduce modern bias.\n\nThe newest model is trained only on texts published in London between 1800-1875. There is no fine tuning, no modern data, and for now no instruction or Q&A pairs so the model continues text from a prompt. This model is 1.2B parameters and uses a 90GB dataset consisting of books, journals, legal docs, religious writing, medical papers, etc. I also use a custom tokenizer, trained on the dataset itself and the model has been trained for 182k steps so far on a rented H100 SXM.\n\nExample outputs:\n\n[Even though the prompt only mentions a specific year, the model generates an argument against the Roman Catholic Church. The dataset does contain large amounts of religious and political writing and the Catholic Emancipation Act took place in 1829 so this behavior makes sense.](https://preview.redd.it/l0oaulxrascg1.png?width=1478&format=png&auto=webp&s=5292309afa4c4735471542b6cc794f6538b42486)\n\n[The telephone was invented in 1876 \\(dataset cuts off at 1875\\), so the model is unfamiliar with the term, treating it as some kind of secret\\/diplomatic device or thing.](https://preview.redd.it/tvem9mxrascg1.png?width=1484&format=png&auto=webp&s=347a6b3242b8ecb97a515196109eb63cc146bae0)\n\nFor next steps, I'm going to look into creating some kind of synthetic Q&A pairs using the dataset itself.\n\n[https://github.com/haykgrigo3/TimeCapsuleLLM](https://github.com/haykgrigo3/TimeCapsuleLLM)\n\n[https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875](https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875)",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz37gv4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-12 02:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1gr89",
          "author": "Mr_Moonsilver",
          "text": "Man. Been following your posts ever since you had the idea. Keep it up, such a cool project!",
          "score": 209,
          "created_utc": "2026-01-11 21:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1lrn5",
          "author": "reality_comes",
          "text": "I gathered a small dataset to do similar work several months ago. My goal though was to train to around 1900 on essentially everything I could get that was older.\n\nI had this idea that it would be fun to probe the model with ideas and see what it thought, things from the sciences that are now settled but at the time hadn't been discovered. \n\nIt would also be fun to use to for other purposes like roleplay.",
          "score": 102,
          "created_utc": "2026-01-11 21:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2a43f",
              "author": "Igot1forya",
              "text": "I would image it would make for a great cultural and historical reference for a narative or consult in the film and literature. I'm thinkig of all the sci-fi time travel tropes but this time it woudl be incredibly accurate. I would imagine, even a props department would be able to use it for period accurae data.",
              "score": 36,
              "created_utc": "2026-01-11 23:22:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4pe4m",
              "author": "dr_lm",
              "text": "An academic team at Zurich did this, but last I saw aren't releasing it because of \"safety\": https://news.ycombinator.com/item?id=46319826",
              "score": 12,
              "created_utc": "2026-01-12 08:30:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbxnn5",
                  "author": "2madlycat",
                  "text": "Oh brotherâ€¦ ðŸ˜’",
                  "score": 1,
                  "created_utc": "2026-01-13 10:41:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzcpv8p",
                  "author": "Monkey_1505",
                  "text": "There's literally no point in such a model if you are concerned about safety. The entire point is to examine historical thinking.",
                  "score": 1,
                  "created_utc": "2026-01-13 14:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3nkwp",
              "author": "lomirus",
              "text": "I wonder if AI could independently discover relativity if it only possessed knowledge from the 19th century?",
              "score": 17,
              "created_utc": "2026-01-12 03:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5348a",
                  "author": "GrapefruitMammoth626",
                  "text": "There was an interview with maybe It was Demis or Ilya who made that exact point. He proposed an experiment of giving data up until a point in time and seeing if the model could uncover core scientific ideas that followed. Would be a cool verifiable experiment as we have the luxury of having data that came after. Would be a good test of intelligence.\n\nIt also suggests the possibility of verifiable flywheel training where you would curate a limited base training set of fundamentals and steer the model towards rewards to make the reasoning leaps to the next logical stage and incorporating that synthetic data into the training set and looping over the process multiple times hoping to surpass current knowledge. Just an idea based off rewards for making the right connections between its limited pool of knowledge. Anything thatâ€™s verifiable seems doable.",
                  "score": 30,
                  "created_utc": "2026-01-12 10:41:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz69t62",
                  "author": "quinn50",
                  "text": "Yea this is something I would love to see as an experiment in general for these LLMs, have all human knowledge up to before major science / math breakthroughs and see if an automatic agentic setup given a good starting prompt could come up / rediscover things. \n\nGood master's project I bet",
                  "score": 2,
                  "created_utc": "2026-01-12 15:16:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzddqmd",
                  "author": "Smallpaul",
                  "text": "No. It could not. Current AI is far from as creative as Einstein. Not even as creative as the median physics PhD student.",
                  "score": 1,
                  "created_utc": "2026-01-13 15:58:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz4xem4",
                  "author": "reality_comes",
                  "text": "This is the main question",
                  "score": 1,
                  "created_utc": "2026-01-12 09:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz2zbr4",
              "author": "TomLucidor",
              "text": "Now make a model that is aware of different time periods, and can translate between them! Would feel like the \"internet cowboy translator\"",
              "score": 3,
              "created_utc": "2026-01-12 01:31:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2mr2f",
          "author": "-Vincent",
          "text": "\"I'm sorry but my cutoff date is 1875\"",
          "score": 54,
          "created_utc": "2026-01-12 00:27:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz88im5",
              "author": "CV514",
              "text": ">\"This is only one instance out of many that may occur, for the subject is too large to be touched upon here.\" said I.\n\nThis particular model saying",
              "score": 5,
              "created_utc": "2026-01-12 20:41:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzb315p",
              "author": "EagerSubWoofer",
              "text": "this model is worthless. i asked it to code an app and the app UI is shit",
              "score": 5,
              "created_utc": "2026-01-13 06:00:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdbky0",
                  "author": "ketura",
                  "text": "It wasted its entire time carving a woodblock for an illustration.",
                  "score": 2,
                  "created_utc": "2026-01-13 15:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1unbn",
          "author": "cantgetthistowork",
          "text": "Large piece of wood and two balls ðŸ™ƒ",
          "score": 59,
          "created_utc": "2026-01-11 22:06:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3ttfi",
              "author": "Ok-Lengthiness-3988",
              "text": "\"Off with their heads!\" Her Majesty Queen Victoria uttered upon being presented with the device...",
              "score": 2,
              "created_utc": "2026-01-12 04:17:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1wrmm",
          "author": "fuckit-nickit-legit",
          "text": "How did you go about assembling the training data?",
          "score": 21,
          "created_utc": "2026-01-11 22:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz222po",
          "author": "MoffKalast",
          "text": "It's really surprising to me that it's even possible to pretrain something coherent with that little data. I guess the early datasets really were completely noisy trash.",
          "score": 17,
          "created_utc": "2026-01-11 22:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3b4zx",
              "author": "lahwran_",
              "text": "GPT-2 was trained on 40GB of internet text.\n\nThe thing that's amazing to me here is that there were even 90gb of words written at the time! I'm amazed enough to be skeptical - is that really the plaintext file size, or are does it include book scans and such",
              "score": 29,
              "created_utc": "2026-01-12 02:34:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4tu43",
                  "author": "1731799517",
                  "text": "Yeah, the size of stuff like game assets or HD movies really warped perception. 90 Gigabytes is a shitload of information.",
                  "score": 13,
                  "created_utc": "2026-01-12 09:13:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3g81c",
          "author": "The_Cat_Commando",
          "text": "Ye olde GGUF hhhuuuuuuwhen my good sir?",
          "score": 15,
          "created_utc": "2026-01-12 03:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2ixzd",
          "author": "dejco",
          "text": "Now make it run on Babbage analytical engine to be period correct ðŸ¤£",
          "score": 14,
          "created_utc": "2026-01-12 00:08:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2b2uz",
          "author": "dbenc",
          "text": "could you train it on datasets in other languages from the same time period?",
          "score": 10,
          "created_utc": "2026-01-11 23:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3l8ct",
              "author": "NoahFect",
              "text": "I'd like to see a model trained on all of the world's known scientific writing between Pythagoras and Newton, and see how hard it is to lead the model to invent calculus.",
              "score": 20,
              "created_utc": "2026-01-12 03:29:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1owdq",
          "author": "Watemote",
          "text": "Please ask your LLM to explain its own existence. Â Will it decide itâ€™s a mechanical Turk or a sensory deprived human ?",
          "score": 34,
          "created_utc": "2026-01-11 21:39:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1ullr",
              "author": "datbackup",
              "text": "OPâ€™s model is a base model, not an instruct model. So if you gave it instructions or questions, itâ€™s likely to just continue writing in the role of the questioner, rather than answer them.\n\nDid you know that LLM trainers can during the training process include training data that talks about the nature of â€œAIâ€ and includes examples of conversations between â€œAIâ€ and humans?\n\nBut itâ€™s deception. The â€œAIâ€ has none of these things. Itâ€™s a function approximator. The inclusion of self-aware-sounding chats in the training data is a shrewd decision because it allows users to chat â€œwithâ€ the â€œAIâ€ as though it has its own theory of mind, self awareness, or subjective viewpoint, and VC money goes gaga for this. So itâ€™s a very lucrative bit of deception.\n\nIn fact the entirety of the instruct fine tuning paradigm is based on a similar but subtler deception. The model is never â€œansweringâ€ anything, itâ€™s only completing/continuing the prompt. It just happens that the completion looks like an answer, because the tuning dataset was all prompts followed by answers.\n\nInstruct models are convenient, but base models are where you get to see how the sausage is made (â€œbase modelâ€ is coming to mean something else these days; Iâ€™m using it in its original sense meaning â€œpretrainedâ€ i.e. not (yet) instruct fine tuned).",
              "score": 100,
              "created_utc": "2026-01-11 22:05:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz26j2t",
                  "author": "MoffKalast",
                  "text": "All true of course, though it's not like we're talking bayesian inference here exactly. Sufficiently advanced linear algebra is indistinguishable from magic and everything in this universe can be boiled down to an advanced enough function. \n\nIdk, I'm really sort of torn, there's definitely something there that understands and solves problems beyond anything a logic based system could ever do but we also don't seem to ever really interact with it directly, each chat is indistinguishable from any other training batch seen previously. If they were capable of self awareness it wouldn't even make sense to them because from a model's perceptive they don't really... exist? They don't perceive time nor space.",
                  "score": 29,
                  "created_utc": "2026-01-11 23:04:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz989dh",
                  "author": "TheLocalDrummer",
                  "text": "I love your take. That's exactly how I feel about all this \"the computer is talking!\" hype. AI's self-awareness is manufactured, fabricated, literally artificial. Anything generated by AI to sound self-aware came from pretraining data that contained our own expectations of \"artificial self-awareness\".",
                  "score": 2,
                  "created_utc": "2026-01-12 23:35:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1xcp1",
                  "author": "federico_84",
                  "text": "Great info, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-11 22:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1pvlu",
              "author": "LoveMind_AI",
              "text": "Probably needs a bit of instruction following training in order to do that?",
              "score": 8,
              "created_utc": "2026-01-11 21:43:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz24zmd",
                  "author": "SubstantialSock8002",
                  "text": "With the right prompt engineering you can still get a base model to answer questions. You might need to add a couple turns of Q&A pairs before (few shot prompting).\n\nI haven't tried it, but this model could have some understanding of answering questions from newspaper interviews or legal proceedings included in the training data, even if it hasn't been fine-tuned for instruction following.",
                  "score": 11,
                  "created_utc": "2026-01-11 22:56:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1r8ao",
                  "author": "Watemote",
                  "text": "Maybe, the idea originated with talks with my daughter about the seeming self awareness of LLM but then what about an LLM trained on the â€œdoomsday manualsâ€ which are a series of books on how to bring technology back up to an 1880s level. Â ",
                  "score": 3,
                  "created_utc": "2026-01-11 21:50:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1kpw6",
          "author": "TheKL",
          "text": "this is so cool",
          "score": 7,
          "created_utc": "2026-01-11 21:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1l3yu",
          "author": "fulgencio_batista",
          "text": "Very interesting. I wonder how such a dataset would effect model 'intelligence'? On one hand, I assume most remaining texts from that time period were probably made by the well educated of the time, on the other hand, they knew a lot less back then.",
          "score": 11,
          "created_utc": "2026-01-11 21:21:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz21202",
          "author": "Southern_Sun_2106",
          "text": "Very unique and exciting project, thank you for your work and for sharing this with the community.",
          "score": 4,
          "created_utc": "2026-01-11 22:37:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4n4jt",
          "author": "profcuck",
          "text": "Given that the invention of the telephone wasn't a completely out of context surprise (as far as I know) I wonder about a prompting that points it in the direction of explaining new technology and what might be possible in a few years time, with the description and definition of a \"telephone\" being given.\n\nSo for example: \"The American Alexander Graham Bell has been experimenting with a new type of device for transmitting sound at a distance, which he calls the \"telephone\".  It utilizes electricity in a manner similar to Morse's telegraph, but instead of a series of distinct on/off pulses, it...\"\n\nWith that hopefully interesting thought out of the way I'll now just join in with the chorus of fans saying \"what a very cool project\"!",
          "score": 5,
          "created_utc": "2026-01-12 08:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6vjl7",
          "author": "Zestyclose839",
          "text": "If any of you run Apple Silicon and want to try this out, I made an FP16 MLX version:\n\n[https://huggingface.co/FractalSurfer/TimeCapsuleLLM-v2-1800-1875-mlx-fp16](https://huggingface.co/FractalSurfer/TimeCapsuleLLM-v2-1800-1875-mlx-fp16) \n\nhttps://preview.redd.it/czkjcwzc8ycg1.jpeg?width=1790&format=pjpg&auto=webp&s=48a2179b732e532c4e6a7d0c8b7c548b2a78d71a",
          "score": 3,
          "created_utc": "2026-01-12 16:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2s8x1",
          "author": "TheRealMasonMac",
          "text": "Morbidly curious, has it learned racial biases? For instance, how does it continue talking about Africans or Native Americans?",
          "score": 6,
          "created_utc": "2026-01-12 00:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1qtkq",
          "author": "amooz",
          "text": "This is really, really cool.",
          "score": 3,
          "created_utc": "2026-01-11 21:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1rh0c",
          "author": "MrPecunius",
          "text": "This is cool as hell, thanks for the update!",
          "score": 3,
          "created_utc": "2026-01-11 21:51:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz254b0",
          "author": "Dazzling-Try-7499",
          "text": "Out of curiosity, how much memory do need to train a 1.2B model in that way?",
          "score": 3,
          "created_utc": "2026-01-11 22:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2fw6u",
              "author": "PortiaLynnTurlet",
              "text": "You can train this on a single h100 easily. Minimum probably 24GB practically although more is better",
              "score": 9,
              "created_utc": "2026-01-11 23:52:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz45ery",
              "author": "Remarkable-Trick-177",
              "text": "I had 80gb on an h100 and it took around 130-140 hours to train total. It can be done with less memory, will just take longer.",
              "score": 7,
              "created_utc": "2026-01-12 05:37:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz6joej",
                  "author": "mypossiblepasts",
                  "text": "I am interesting in making a proper, foreign language LLM, without bias of base models that's really apparent for non germanic language.  \nWould that approach of yours work here?",
                  "score": 1,
                  "created_utc": "2026-01-12 16:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz27so0",
          "author": "ohcrap___fk",
          "text": "Holy f I absolutely love the output. Itâ€™s like skirting the edge of the event horizon between cohesiveness and raw unconnected creativity",
          "score": 3,
          "created_utc": "2026-01-11 23:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz35iy3",
          "author": "Ready-Interest-1024",
          "text": "This is such a cool idea, I wonder if you could see any interesting trends about how our current models answer societal issues vs what this would say",
          "score": 3,
          "created_utc": "2026-01-12 02:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz45uge",
              "author": "Remarkable-Trick-177",
              "text": "Thanks! Iâ€™m planning on comparing the same prompts with a general use LLM. I think comparing word neighbors will also show interesting trends.",
              "score": 3,
              "created_utc": "2026-01-12 05:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6se46",
          "author": "TellMeAboutGoodManga",
          "text": "Gee guff when?",
          "score": 3,
          "created_utc": "2026-01-12 16:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vafa",
          "author": "Far_Hope_6349",
          "text": "love this!!",
          "score": 2,
          "created_utc": "2026-01-11 22:09:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz21evm",
          "author": "Terrible_Ad9306",
          "text": "Geat idea!",
          "score": 2,
          "created_utc": "2026-01-11 22:38:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz22mnj",
          "author": "literateu",
          "text": "Very cool! Would be cool to see differences if training on something like [**https://huggingface.co/datasets/dell-research-harvard/AmericanStories**](https://huggingface.co/datasets/dell-research-harvard/AmericanStories) over time as well !",
          "score": 2,
          "created_utc": "2026-01-11 22:44:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2ttyw",
          "author": "ddxv",
          "text": "Is it better at finishing the text when you don't ask a question? I noticed both your examples didn't have questions, whereas modern use is usually questions with answers. Is this because your corpus didn't have many Q A style docs?",
          "score": 2,
          "created_utc": "2026-01-12 01:02:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5h2vi",
              "author": "Technical-Will-2862",
              "text": "Yeah. Itâ€™s just predicting the next likely token. Kinda like call and response.Â ",
              "score": 2,
              "created_utc": "2026-01-12 12:34:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4e9xv",
          "author": "smflx",
          "text": "Wonderful project! Appreciate your sharing!",
          "score": 2,
          "created_utc": "2026-01-12 06:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5jc1c",
          "author": "GhostGhazi",
          "text": "Can you teach us how you rent a H100 to train an LLM? Im really interested",
          "score": 2,
          "created_utc": "2026-01-12 12:49:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzj0thu",
              "author": "Majestic-Lawyer5246",
              "text": "hey! you generally have two routes: raw GPU VMs (where you manage everything yourself) or running training as a containerized job on a platform that handles orchestration for you.\n\nif youâ€™re already packaging your training code in Docker, platforms like [Northflank](https://northflank.com/) let you spin up on-demand H100/A100 jobs, run your fine-tuning, and shut everything down when itâ€™s done so youâ€™re not paying for idle time.\n\nhappy to explain what a minimal container + job setup looks like if that helps :)",
              "score": 1,
              "created_utc": "2026-01-14 12:29:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7evsb",
          "author": "Liringlass",
          "text": "This could be so interesting. An AI that feels like youâ€™re talking to a dude from that time.",
          "score": 2,
          "created_utc": "2026-01-12 18:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbd4z5",
          "author": "notforrob",
          "text": "I think your next step -- generating synthetic Q&A pairs from the dataset -- and then presumably fine tuning it on that will be super interesting. My guess is that you generate a very robust dataset (I personally would target \\~10K Q&A pairs), and that when fine tuned on your dataset it will be 100 times more fun and interesting to play with.",
          "score": 2,
          "created_utc": "2026-01-13 07:27:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vb6k",
          "author": "StardockEngineer",
          "text": "This is so cool.",
          "score": 2,
          "created_utc": "2026-01-11 22:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz40cfn",
          "author": "paul_f",
          "text": "what a great concept",
          "score": 1,
          "created_utc": "2026-01-12 05:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4438j",
          "author": "Revolutionalredstone",
          "text": "Awesome project! And just keeps getting better ðŸ˜ nice work dude !",
          "score": 1,
          "created_utc": "2026-01-12 05:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz44nke",
          "author": "Acrobatic-Tomato4862",
          "text": "If we can make it solve a problem from after 1875 then atleast it can be settled that llms have capacity for novel thought.Â ",
          "score": 1,
          "created_utc": "2026-01-12 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4a97t",
          "author": "IrisColt",
          "text": "I kneel... Your perseverance... Kudos to you!",
          "score": 1,
          "created_utc": "2026-01-12 06:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4dokv",
          "author": "jacek2023",
          "text": "great job!",
          "score": 1,
          "created_utc": "2026-01-12 06:44:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4fn2o",
          "author": "ryfromoz",
          "text": "Very cool!",
          "score": 1,
          "created_utc": "2026-01-12 07:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4xhnh",
          "author": "GrokiniGPT",
          "text": "This seems amazing!  Can't wait to see it in action",
          "score": 1,
          "created_utc": "2026-01-12 09:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz55x2n",
          "author": "ikkiyikki",
          "text": "Ok, this is inspiring me to do a similar take but with ancient Latin texts! Walk like an Egyptian, talk like a Roman!\n\nðŸ˜…",
          "score": 1,
          "created_utc": "2026-01-12 11:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5ar7l",
          "author": "thecowmilk_",
          "text": "Now tell the LLM to write Jack The Ripper inspired letters.",
          "score": 1,
          "created_utc": "2026-01-12 11:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5mpx3",
          "author": "LightMaleficent5844",
          "text": "Download link for model and dataset?",
          "score": 1,
          "created_utc": "2026-01-12 13:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6iews",
          "author": "4redis",
          "text": "Probs not the right place and probs not the smartest question but only way i will learn. \n\nI was wondering how data (in this case text) is \"fed\" to these ai models in simple terms",
          "score": 1,
          "created_utc": "2026-01-12 15:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6k2py",
          "author": "Marshall_Lawson",
          "text": "Cool, now do 2003 next! Lol",
          "score": 1,
          "created_utc": "2026-01-12 16:05:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6z3vh",
          "author": "auxiliaPalatina",
          "text": "What are your parameters when prompting, I couldn't really find a sweet spot to get the answers in the post?",
          "score": 1,
          "created_utc": "2026-01-12 17:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7641f",
          "author": "Area51-Escapee",
          "text": "What a blessing",
          "score": 1,
          "created_utc": "2026-01-12 17:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz83xn7",
          "author": "johnnyjoestar5678",
          "text": "this is insanely cool.\n\nas a matter of fact, you could easily monetize this, video game companies and the rest of the entertainment industry would see immense value in this. as a way of generating more authentic dialogue and actions/behavior for a particular time period. yknow what i mean?",
          "score": 1,
          "created_utc": "2026-01-12 20:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfae8i",
          "author": "thermologic_",
          "text": "Chat link pls",
          "score": 1,
          "created_utc": "2026-01-13 21:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5hwig",
          "author": "Technical-Will-2862",
          "text": "Disappointed in the lack of emergence. People ask to keep scaling, but I think youâ€™ve proven something valuable already - LLMs will not lead to valued emergent behavior, even with a highly curated dataset. Thereâ€™s a self attention and context issue.Â ",
          "score": 0,
          "created_utc": "2026-01-12 12:39:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5tolc",
              "author": "aleph_iskar",
              "text": "how can you possibly conclude that from the 2 short snippets OP provided?\n\n> oh this toy exercise with 1.2 b model is not self aware, therefore LLMs will not lead to valued emergent behaviour.\n\nthe fuck",
              "score": 3,
              "created_utc": "2026-01-12 13:51:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6073u",
                  "author": "Technical-Will-2862",
                  "text": "Emergent behavior demands structure. If thereâ€™s nothing novel being generated with over 90gb of text thatâ€™s inherently interconnected, it should signal that thereâ€™s something fundamental lacking. Thatâ€™s like 21 billion tokens and itâ€™s still spitting out bullshit while a human can read a single book and utilize the reasoning patterns intuitively.Â \n\nIt canâ€™t be anymore obvious that something is lacking. But yeah this dude should def keep renting h100s (also Iâ€™m not seeking self awareness, just maybe like one tiny sign of actually responding to the query?) (also pt 2, OPs entire point was to see what emerges and after taking in pretty much everything he can find itâ€™s still dookie.)",
                  "score": 0,
                  "created_utc": "2026-01-12 14:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1o0j0",
          "author": "Sicarius_The_First",
          "text": "but why gatekeep the model",
          "score": -9,
          "created_utc": "2026-01-11 21:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1u6fz",
              "author": "Sheeye12",
              "text": "He didn't, OP left huggingface link in post.",
              "score": 16,
              "created_utc": "2026-01-11 22:03:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz366x4",
                  "author": "AnonymousTransfem",
                  "text": "We have to request access, check the huggingface",
                  "score": 3,
                  "created_utc": "2026-01-12 02:08:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz457u9",
          "author": "hemahariharansamson",
          "text": "Man your findings looks interesting. Sharing this to my circle.\n\nKeep up this work man\n\nhttps://www.linkedin.com/posts/hemahariharansamson_llm-machinelearning-ai-activity-7416351604552908800-8X0B?utm_source=share&utm_medium=member_android&rcm=ACoAADYRaTkB5vCPx_LU_GECDXbmMlw5Y0BTUg8",
          "score": -5,
          "created_utc": "2026-01-12 05:35:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7d8bj",
      "title": "Jensen Huang saying \"AI\" 121 times during the NVIDIA CES keynote - cut with one prompt",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/hein55gpx4cg1",
      "author": "Prior-Arm-6705",
      "created_utc": "2026-01-08 14:29:47",
      "score": 923,
      "num_comments": 146,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7d8bj/jensen_huang_saying_ai_121_times_during_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nygb8aw",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 19:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyejqtm",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 174,
          "created_utc": "2026-01-08 14:33:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfrjx1",
              "author": "MrWeirdoFace",
              "text": "I vote OP renames the video to Nvidia Keynote Summary.",
              "score": 42,
              "created_utc": "2026-01-08 17:50:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyiurdw",
                  "author": "Prior-Arm-6705",
                  "text": "This title is definitely better though lol   \nCan't edit Reddit titles unfortunately.",
                  "score": 6,
                  "created_utc": "2026-01-09 02:35:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyj31pj",
              "author": "ClimateBoss",
              "text": "AI AI AI AI AI AI ... make line go up!",
              "score": 2,
              "created_utc": "2026-01-09 03:19:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzfn88h",
                  "author": "Background_Praline18",
                  "text": "Lol like the badger song but ai",
                  "score": 1,
                  "created_utc": "2026-01-13 22:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyf9wy6",
          "author": "DriveSolid7073",
          "text": "\"All local, no cloud.\"   \nopen video  \n\\>claude opus 4.5  \nI couldn't get Dive to work with my Koboldcpp, and the functions aren't being called.",
          "score": 40,
          "created_utc": "2026-01-08 16:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyipg6m",
              "author": "Prior-Arm-6705",
              "text": "To clarify - \"all local\" refers to the local MCPs and tools like ffmpeg, not the LLM or the demo video.",
              "score": 2,
              "created_utc": "2026-01-09 02:06:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyiq196",
                  "author": "DriveSolid7073",
                  "text": "I already understood, but it would be nice to see the full locally use. Although I think ollama can handle it.",
                  "score": 2,
                  "created_utc": "2026-01-09 02:09:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyekf0a",
          "author": "LambdaHominem",
          "text": "gamers nexus would be proud\n\nhttps://youtu.be/-qbylbEek-M",
          "score": 61,
          "created_utc": "2026-01-08 14:36:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyepodd",
          "author": "LinkSea8324",
          "text": "The fuck is that latex-leather jacket",
          "score": 58,
          "created_utc": "2026-01-08 15:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyesltq",
              "author": "Acceptable_Piano4809",
              "text": "Heâ€™s been wearing that for years.   Itâ€™s like 5 figures.   Michael Jacksons was better!",
              "score": 30,
              "created_utc": "2026-01-08 15:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygsk1b",
                  "author": "Not_your_guy_buddy42",
                  "text": "Incidentally there's also supercuts of all the nonverbal MJ sounds in his songs  \nEdit: Shamon-a! Hee-hee. Chickachuwow",
                  "score": 8,
                  "created_utc": "2026-01-08 20:32:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyf917k",
              "author": "Thatisverytrue54321",
              "text": "Heâ€™s trying to be a Steve Jobs. Thatâ€™s *his* turtleneck.",
              "score": 28,
              "created_utc": "2026-01-08 16:30:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfd4u5",
                  "author": "alphapussycat",
                  "text": "This. A lot of these people, like Steve Jobs, want to stick out and have some signature look.",
                  "score": 19,
                  "created_utc": "2026-01-08 16:47:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyetm0b",
              "author": "zzozozoz",
              "text": "Aye Eye",
              "score": 9,
              "created_utc": "2026-01-08 15:20:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyf3h00",
              "author": "TanguayX",
              "text": "Whoever is telling him that that looks cool does not have his best interests in mind.",
              "score": 13,
              "created_utc": "2026-01-08 16:05:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfrod8",
                  "author": "GoranjeWasHere",
                  "text": "I don't think dude who is runnigng 4 trylion $ company will be caring about his style and outfit. He can wear literally garbage bag and people will say he looks cool.\n\nThat's what people don't get about fashion. Fashion doesn't make you better looking it only shows everyone who are you. That's why when you wear suit without actually being CEO you look stupid.",
                  "score": 8,
                  "created_utc": "2026-01-08 17:51:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfaoj8",
                  "author": "Smile_Clown",
                  "text": "It's weird to me that redditors routinely call out people for making light of others choices, but when it comes to someone we hate (apparently) it's all ok?\n\nHe likes it, that is all that matters.\n\nIf you disagree that means anything you like is up for debate also and others opinions should be considered by you as some sort of barometer you make choices based on.\n\nI mean...",
                  "score": 4,
                  "created_utc": "2026-01-08 16:37:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyekw78",
          "author": "FastDecode1",
          "text": "Reminds me of the Xbox One reveal: https://www.youtube.com/watch?v=KbWgUO-Rqcw",
          "score": 30,
          "created_utc": "2026-01-08 14:38:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygtg3m",
          "author": "Freonr2",
          "text": "\"Consumer\" Electronics Show, showing billion dollar datacenter configs.",
          "score": 11,
          "created_utc": "2026-01-08 20:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg0uk3",
          "author": "Sea_Succotash3634",
          "text": "\"Consumer\" Electronic Show",
          "score": 10,
          "created_utc": "2026-01-08 18:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygf0cq",
          "author": "r0ckl0bsta",
          "text": "Old McJensen's server farm.\nA-I-A-I-O ðŸŽ¶",
          "score": 20,
          "created_utc": "2026-01-08 19:31:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyenr5m",
          "author": "International-Try467",
          "text": "This man is the reason why everything is so expensive",
          "score": 80,
          "created_utc": "2026-01-08 14:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyetj3o",
              "author": "JuliusCeaserBoneHead",
              "text": "Heâ€™s laughing all the way to the bankÂ ",
              "score": 35,
              "created_utc": "2026-01-08 15:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf2x94",
                  "author": "sourceholder",
                  "text": "He's better capitalized than most banks, or nations for that matter...",
                  "score": 21,
                  "created_utc": "2026-01-08 16:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylb67q",
                  "author": "-TRlNlTY-",
                  "text": "The bank is laughing all the way to him",
                  "score": 2,
                  "created_utc": "2026-01-09 13:23:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nygznjg",
                  "author": "MoffKalast",
                  "text": "He's also laughing at the bank, and on the way home too.",
                  "score": 1,
                  "created_utc": "2026-01-08 21:03:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyfnqmk",
              "author": "20ol",
              "text": "So it's not the fed printing money, 40 trillion debt, and illegal tarrifs? \n\nDamnit, I knew I should have researched here on local llama on why everything is getting expensive.",
              "score": 12,
              "created_utc": "2026-01-08 17:34:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhklh9",
                  "author": "Jonodonozym",
                  "text": "This is r/LocalLLaMA, we eat computer parts instead of bread",
                  "score": 3,
                  "created_utc": "2026-01-08 22:36:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfuc6y",
                  "author": "nonaveris",
                  "text": "Even worse, people marking to unicorns for gpu and memory prices.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:02:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykfk5x",
                  "author": "nenulenu",
                  "text": "Dude. Debt has very little to do with prices. The other two, yes.",
                  "score": 1,
                  "created_utc": "2026-01-09 09:20:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyf2635",
              "author": "Solaranvr",
              "text": "Can you really say the shovel merchant (one among two) is the reason iron got expensive during the gold rush?",
              "score": 13,
              "created_utc": "2026-01-08 15:59:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf2sgv",
                  "author": "International-Try467",
                  "text": "No because the shovel merchant weren't friends with the mayor who was also friends with the blacksmith which they kept investing in each other over and over again",
                  "score": 21,
                  "created_utc": "2026-01-08 16:02:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyg7klj",
                  "author": "RealSataan",
                  "text": "If all of the industrial might were concentrated in getting iron for the gold rush, Yes.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfbq8n",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -6,
                  "created_utc": "2026-01-08 16:41:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyewdgw",
              "author": "simracerman",
              "text": "Partly true. He is enabled by other people who benefit from his BS. Regardless of your country, those people you can elect to rule and govern on what makes things around you cheap or expensive.\n\nP.S. By â€œyouâ€, I mean everyone reading this with the power to vote.",
              "score": -7,
              "created_utc": "2026-01-08 15:33:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf9znp",
                  "author": "Smile_Clown",
                  "text": "Your statement is quite ridiculous. \n\nBut I'll start with... good luck with that mindset.  It will follow you forever and never come to anything.  \n\nSelf created stress is not healthy.\n\nWhat I mean by that is your random comments on reddit are not going to make the masses vote your way and even if they did, controlling means of production is already something proven to be a shitty idea.\n\nAs far as setting pricing, that is virtually the same thing. Innovation does not come from government regulation and taxation. \n\nBut the bigger issue is need, real need.\n\nNone of the things Jenson is involved with affect your everyday survival.  You do not need 128gb of ram, a 5090 or a subscription to an AI service to thrive or live a happy life.\n\nSo, no matter who you want elected, there is absolutely zero chance that all the shiny tech will get price regulated.  it's not how it works.\n\nJust for giggles though, do you know how much a chip fab costs? (of course you don't, you simply think everyone is overcharging...)\n\nOn to the next:\n\n>He is enabled by other people who benefit from his BS.\n\nWhat BS exactly?  NVidia puts out the best hardware for the tech industry period. What exactly is bullshit? BTW if it wasn't NVidia it would be someone else. There is no \"enabling\" going on here. The products are viable, valuable and are used everywhere.\n\nThat all said, the original poster was right, not \"partly\" (which btw you did not actually qualify) he IS the reason everything (tech) is so expensive. He facilitated AI being implemented everywhere and that has caused manufacturers to pivot to non consumer tech.\n\nYou are a dufus of the highest order.",
                  "score": -2,
                  "created_utc": "2026-01-08 16:34:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyeqoex",
          "author": "anon235340346823",
          "text": "0:44 \"hey I have AI\"",
          "score": 18,
          "created_utc": "2026-01-08 15:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjql1",
          "author": "budz",
          "text": "dope, I made a script that does this , back in October      [https://imgur.com/a/h0vc2f6](https://imgur.com/a/h0vc2f6)",
          "score": 5,
          "created_utc": "2026-01-08 17:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyitlmt",
              "author": "Prior-Arm-6705",
              "text": "Nice, what was your approach?",
              "score": 1,
              "created_utc": "2026-01-09 02:28:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfa965",
          "author": "MMAgeezer",
          "text": "Appreciate you sharing the details of how you did this. One small thing though, it includes multiple clips of a narrator (i.e. not Jensen, as instructed) saying \"AI\" too.",
          "score": 4,
          "created_utc": "2026-01-08 16:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyivxxq",
              "author": "Prior-Arm-6705",
              "text": "Good catch - YouTube's auto-generated subtitles don't distinguish speakers, so the narrator clips got mixed in. Would need a speaker diarization step to filter those out properly.   \nI have another research project that involves voiceprints, but it's very complex to operate.",
              "score": 1,
              "created_utc": "2026-01-09 02:41:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyh9hl9",
          "author": "deltamoney",
          "text": "Did you use AI to find all occurrence of AI?",
          "score": 5,
          "created_utc": "2026-01-08 21:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiw580",
              "author": "Prior-Arm-6705",
              "text": "Yes. AI finding AI felt appropriate. lol",
              "score": 6,
              "created_utc": "2026-01-09 02:42:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyixyrj",
                  "author": "deltamoney",
                  "text": "Nicely done ðŸ˜",
                  "score": 2,
                  "created_utc": "2026-01-09 02:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfp4wj",
          "author": "positivcheg",
          "text": "Nah. AMD wins, AMD mentioned AI 299 times. Big win for AMD.\n\nSadly it was just 1 more to a nice 300.",
          "score": 5,
          "created_utc": "2026-01-08 17:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyervtk",
          "author": "GoranjeWasHere",
          "text": "He's literally the only one that can say as much AI as he wants.\n\nHe literally build his whole company on AI promise before everyone outside of research circle even knew what was AI and his hardware innovated so much that AI finally became a thing.\n\nOutside of Nvidia only Tesla I think and few other small companies can shout AI without sounding like a fool. Tesla was also super early in it and it got to the point where they were building their own chips just not to pay Nvidia tax for AI before AI even became investor bait.",
          "score": 12,
          "created_utc": "2026-01-08 15:12:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf2ohr",
              "author": "RealSataan",
              "text": "Count Google also in it",
              "score": 11,
              "created_utc": "2026-01-08 16:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfa2ah",
                  "author": "redblobgames",
                  "text": "Google's been in this game for a long time. \"Artificial intelligence would be the ultimate version of Google.\" â€”Larry Page, Google CEO in *the year 2000*. [[source](https://www.azquotes.com/quote/917415)] And it wasn't just talk. They were building large language models internally since then, and using them as part of the search engine.",
                  "score": 8,
                  "created_utc": "2026-01-08 16:34:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykpkoz",
                  "author": "lami_kaayo",
                  "text": "\"attention is all you need\"",
                  "score": 2,
                  "created_utc": "2026-01-09 10:50:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfr0o4",
                  "author": "GoranjeWasHere",
                  "text": "Why google for sure was doing the research they didn't really push it like nvidia or tesla. It wasn't until OpenAI came to be and released chatgpt when google felt heat under their ass. Right now google is on the top with rest but for a long while they were behind a lot.",
                  "score": 0,
                  "created_utc": "2026-01-08 17:48:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyfsuoj",
              "author": "FrostieDog",
              "text": "Google built and was deploying TPUs the same year Nvidia started talking about GPUs being made for deep learning",
              "score": 4,
              "created_utc": "2026-01-08 17:56:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyh6ebt",
              "author": "ECrispy",
              "text": "Pretty much all of current AI (i.e. llm) wouldnt exist without Google. Nvidia is rich because of their hardware, and the fact that there was no other real API besides CUDA, which has become the standard.\n\nDon't mention Tesla. They are irrelevant. Zero actual research contributions, their use of AI in self driving is a joke. Just because they are rich due to inflated stock price and ordered a bunch of H100's and made some vague blog posts about their own chips doesn't make them a player. Tesla has zero impact on AI.",
              "score": 5,
              "created_utc": "2026-01-08 21:33:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyfb3j3",
              "author": "7640LPS",
              "text": "That is not true at all. \n\nMany companies were investing heavily in AI before LLMs even existed. \n\nGoogle, AWS, IBM, Meta, etc.",
              "score": 5,
              "created_utc": "2026-01-08 16:39:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfr6zx",
                  "author": "GoranjeWasHere",
                  "text": "They were investing but not really pushing it hard. Like when OpenAI came to be and relased their models they were just better than Google. And it is openAI that shocked the world not google.",
                  "score": 1,
                  "created_utc": "2026-01-08 17:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyh9ral",
              "author": "kritzikratzi",
              "text": "are you trying to gatekeep ai? ðŸ˜‚ here is the history of deep learning from 1920 to today. \nhttps://en.wikipedia.org/wiki/Deep_learning#History",
              "score": 2,
              "created_utc": "2026-01-08 21:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygayob",
              "author": "mycall",
              "text": "You would say AI 121 times too if it made you a trillion dollars.",
              "score": 1,
              "created_utc": "2026-01-08 19:14:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfskqo",
          "author": "SmegPoison",
          "text": "Old MacDonald had a farm, ai-ai-oh!",
          "score": 2,
          "created_utc": "2026-01-08 17:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfuof4",
          "author": "Caladan23",
          "text": "Duh surprise... it's an AI hardware company nowadays. Nvidia as a gaming company wasn't worth even 1/10th.",
          "score": 2,
          "created_utc": "2026-01-08 18:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhhxe1",
          "author": "JealousAmoeba",
          "text": "I admit Iâ€™m surprised that worked. Great demo for MCP.",
          "score": 2,
          "created_utc": "2026-01-08 22:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiv1wf",
              "author": "Prior-Arm-6705",
              "text": "Thanks! MCP makes chaining tools pretty seamless.",
              "score": 1,
              "created_utc": "2026-01-09 02:36:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjj9w8",
          "author": "thermocoffee",
          "text": "this is truly amazing. I'm playing with Dive now. I love it!",
          "score": 2,
          "created_utc": "2026-01-09 04:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo4fmc",
          "author": "Analytics-Maken",
          "text": "Wow, super useful for reels generation, and I like the MCP approach.",
          "score": 2,
          "created_utc": "2026-01-15 03:56:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyet9oy",
          "author": "Agile_Date6729",
          "text": "someone should make a remix of it -would be a banger",
          "score": 3,
          "created_utc": "2026-01-08 15:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyffw6s",
          "author": "WavierLays",
          "text": "I mean this is like making a supercut of every time KFC's CEO says \"chicken\"",
          "score": 3,
          "created_utc": "2026-01-08 16:59:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfs4b0",
              "author": "MrWeirdoFace",
              "text": "A few years ago this was a gaming hardware company, so not quite, but I get the sentiment.",
              "score": 1,
              "created_utc": "2026-01-08 17:53:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyezpx4",
          "author": "XiRw",
          "text": "Willing to bet anything the future of gaming (based on prices going up and shortages) will be server based subscriptions like everything else out there. Youâ€™ll own nothing and be happy.",
          "score": 4,
          "created_utc": "2026-01-08 15:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf1ngi",
              "author": "TwistStrict9811",
              "text": "That assumes everything centralizes forever. Open-source AI and hardware keep pushing more capability back onto local machines, not locking it all into servers.",
              "score": 4,
              "created_utc": "2026-01-08 15:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyf81u5",
                  "author": "Mediocre-Method782",
                  "text": "\"Solution\": mediate local computation too heavily to be used productively. Google, Samsung, and sama's capture of the world's semiconductor production capacity are already on it.",
                  "score": 2,
                  "created_utc": "2026-01-08 16:25:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfa87l",
          "author": "dasjati",
          "text": "Someone should put a nice beat under this!",
          "score": 1,
          "created_utc": "2026-01-08 16:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu4de",
          "author": "nonaveris",
          "text": "ai ai ai Cthulhu fthangh",
          "score": 1,
          "created_utc": "2026-01-08 18:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfx33c",
          "author": "Amazing_Athlete_2265",
          "text": "AI? AI!",
          "score": 1,
          "created_utc": "2026-01-08 18:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfzi09",
          "author": "HerrGronbar",
          "text": "so without AI how long it was?",
          "score": 1,
          "created_utc": "2026-01-08 18:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg250o",
          "author": "pierrenoir2017",
          "text": "A I Caramba",
          "score": 1,
          "created_utc": "2026-01-08 18:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg6x39",
          "author": "spyda_mayn",
          "text": "holy crap now compare this to the other 3 companies AMD and Intel,",
          "score": 1,
          "created_utc": "2026-01-08 18:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg9j98",
          "author": "dmshd",
          "text": "AI",
          "score": 1,
          "created_utc": "2026-01-08 19:07:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg9zah",
          "author": "gynnihanssen",
          "text": "upfront sorry if itâ€˜s an uninformed question but which local llm setup do you use for dive? or is it just dive and the mcps?",
          "score": 1,
          "created_utc": "2026-01-08 19:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyivkns",
              "author": "Prior-Arm-6705",
              "text": "Dive supports multiple LLM backends - Claude, OpenAI, Gemini, Ollama, and any OpenAI-compatible API. For this video I used Claude, but you can run it fully local with Ollama if you prefer.   \nThe MCPs handle the actual work (downloading, cutting), the LLM just orchestrates.\n\nYou can also ask Dive install both MCP for you. Since It has basic tools call embedded for MCP install.",
              "score": 1,
              "created_utc": "2026-01-09 02:39:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyga503",
          "author": "terem13",
          "text": "He is a showman and salesman, nothing more. Every CEO nowadays is.\n\nLLM is the correct name, but its boring and does not sell.",
          "score": 1,
          "created_utc": "2026-01-08 19:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygb24s",
          "author": "glanni_glaepur",
          "text": "Steve Ballmer: \"Developers, developers, developers!\"\n\nJensen Huang: \"AI, AI, AI!\"",
          "score": 1,
          "created_utc": "2026-01-08 19:14:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyggf7b",
          "author": "T_UMP",
          "text": "https://preview.redd.it/b5248cvjh6cg1.png?width=420&format=png&auto=webp&s=110a55c4e413a8f2f21dbba1bb9586f7ee118d4b",
          "score": 1,
          "created_utc": "2026-01-08 19:38:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygh2ov",
          "author": "CV514",
          "text": "Guys I think he's talking about AI at some point.",
          "score": 1,
          "created_utc": "2026-01-08 19:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyghd5y",
          "author": "T_UMP",
          "text": "When you realize Jensen Huang will actually see this somehow...and will think that he could have fitted some more AI's in there.",
          "score": 1,
          "created_utc": "2026-01-08 19:42:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyh0nwk",
          "author": "catalystignition",
          "text": "I only need to say it once; fuck AI.",
          "score": 1,
          "created_utc": "2026-01-08 21:08:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhe4o4",
          "author": "Al_Onestone",
          "text": "i trust nobody with such a shiny jacket.",
          "score": 1,
          "created_utc": "2026-01-08 22:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhlz8z",
          "author": "Same-Platform-9793",
          "text": "With the snake leather jacket",
          "score": 1,
          "created_utc": "2026-01-08 22:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhqyay",
          "author": "Upset-Motor-2602",
          "text": "Old McDonald had a farm -ai-ai-ohhhhh!",
          "score": 1,
          "created_utc": "2026-01-08 23:07:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyi1gu3",
          "author": "Saerain",
          "text": "ðŸŽ¶ Jensen Huang had a farm",
          "score": 1,
          "created_utc": "2026-01-09 00:00:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyi2to4",
          "author": "xorgol",
          "text": "I don't understand what the yt-dlp-mcp did in this case, was it just calling yt-dlp to download the video and subtitles?",
          "score": 1,
          "created_utc": "2026-01-09 00:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjow59",
              "author": "Prior-Arm-6705",
              "text": "Yes, it's basically a wrapper around yt-dlp. The MCP just gives the LLM a clean interface so it doesn't have to guess CLI flags. Without it, the LLM would need many more bash calls and trial-and-error.  \nHonestly the LLM could write the whole pipeline itself given enough attempts. MCPs just skip the reinventing-the-wheel phase. Fewer tokens, fewer hallucinated flags.",
              "score": 3,
              "created_utc": "2026-01-09 05:33:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyij0a6",
          "author": "FloranceMeCheneCoder",
          "text": "Feeling like a MLM everyday",
          "score": 1,
          "created_utc": "2026-01-09 01:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyimx9x",
          "author": "Kooky-Somewhere-2883",
          "text": "AI is so back  baby!!",
          "score": 1,
          "created_utc": "2026-01-09 01:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyionqm",
          "author": "Bimbam_tm",
          "text": "I preferred his 'older quirky tech guy' jackets to this \"Digital Tech Bro Pimp\" phase :(",
          "score": 1,
          "created_utc": "2026-01-09 02:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyji110",
          "author": "ConnectorMadness",
          "text": "We are in a desperate need of a new pronoun for AI. The amount of time I heard the word 'AI' in this years' CES is mind bogglingðŸ˜µâ€ðŸ’«",
          "score": 1,
          "created_utc": "2026-01-09 04:47:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk0rux",
          "author": "salary_pending",
          "text": "soooo funny ðŸ˜†",
          "score": 1,
          "created_utc": "2026-01-09 07:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk5tg1",
          "author": "hsien88",
          "text": "An AI sub hating on AI just because they want cheaper GPUs.",
          "score": 1,
          "created_utc": "2026-01-09 07:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyklye5",
          "author": "PhotoRepair",
          "text": "\".....had a farm, A I A I Ooooo\"",
          "score": 1,
          "created_utc": "2026-01-09 10:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykw7pw",
          "author": "wasdxqwerty",
          "text": "ai ai ai ai-m your little butterfly",
          "score": 1,
          "created_utc": "2026-01-09 11:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylitc6",
          "author": "SigurdZS",
          "text": "https://www.youtube.com/watch?v=Vhh_GeBPOhs",
          "score": 1,
          "created_utc": "2026-01-09 14:05:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyny571",
          "author": "rolyantrauts",
          "text": "Old macdonald had a farm, AI, AI, Oh!",
          "score": 1,
          "created_utc": "2026-01-09 20:45:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyolqdr",
          "author": "warnerbell",
          "text": "You should have put a beat to it..lol",
          "score": 1,
          "created_utc": "2026-01-09 22:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyowwfu",
          "author": "One-Employment3759",
          "text": "Hey it's the leather jacket bubble boy.",
          "score": 1,
          "created_utc": "2026-01-09 23:33:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyraaic",
          "author": "SiddhaDo",
          "text": "nice",
          "score": 1,
          "created_utc": "2026-01-10 08:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrk2bd",
          "author": "madSaiyanUltra_9789",
          "text": "lol that because he is an AI\n\nhttps://i.redd.it/q7pxig6d8icg1.gif",
          "score": 1,
          "created_utc": "2026-01-10 10:26:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysyw4h",
          "author": "LadenBennie",
          "text": "How many times did he mention \"gamers\"?",
          "score": 1,
          "created_utc": "2026-01-10 16:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyy6y0s",
          "author": "SpicyWangz",
          "text": "Iâ€™m a fan of using AI tools when theyâ€™re right for the job, but I really cannot wait until this hype cycle dies down. It canâ€™t happen soon enough",
          "score": 1,
          "created_utc": "2026-01-11 10:10:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7oii5",
          "author": "exrasser",
          "text": "Old MacDonald had a farm, AI AI O!  \nAnd on his farm he had a cow, AI AI O!  \nWith a motherboard here and a motherboard there,  \nHere a motherboard, there a motherboard,  \nEverywhere a motherboard with AI AI O",
          "score": 1,
          "created_utc": "2026-01-12 19:08:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf3l5f",
          "author": "TanguayX",
          "text": "Time for a 'dumb jacket intervention' with this guy",
          "score": -2,
          "created_utc": "2026-01-08 16:05:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyf7dwf",
              "author": "Mediocre-Method782",
              "text": ">dumb jacket\n\nThis is his new name",
              "score": -3,
              "created_utc": "2026-01-08 16:22:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyeldow",
          "author": "Noiselexer",
          "text": "Tools did exactly that before Ai.",
          "score": -1,
          "created_utc": "2026-01-08 14:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfl1yb",
              "author": "20ol",
              "text": "Durrrr. And before automated factories people did the same thing by hand.",
              "score": 3,
              "created_utc": "2026-01-08 17:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf4zp4",
          "author": "diond09",
          "text": "'Old MacDonald had a farm.......'",
          "score": 0,
          "created_utc": "2026-01-08 16:12:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf6iyd",
          "author": "Amazing-Canary2574",
          "text": "Grok says hi",
          "score": 0,
          "created_utc": "2026-01-08 16:19:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf7gjn",
          "author": "Remarkable_Pound_375",
          "text": "For SEO :))",
          "score": 0,
          "created_utc": "2026-01-08 16:23:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf8sbe",
          "author": "RedTheRobot",
          "text": "Some company needs to attack the gaming video card market. Make cards with a small amount of VRAM. Sell the shit out of them. Use that capital to make better cards that can then be shifted to AI. Install more VRAM on the card now you are making AI cards and billions. I really do think now is a perfect time for a new company to enter the market and take over the space Nivida seems to be pushing to the side.",
          "score": 0,
          "created_utc": "2026-01-08 16:28:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfpuym",
              "author": "Bananadite",
              "text": "If you could make VRAM you wouldn't be selling to small consumers. You would make much more selling to companies.",
              "score": 2,
              "created_utc": "2026-01-08 17:43:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygbqkh",
              "author": "ResidentPositive4122",
              "text": "If crucial decided it's just not worth dealing with the retail market, why would a not-yet-existing, new company have much success there? Nvda for gaming isn't going anywhere. During the crypto push, gpus got scalped to hell and back, and they saw that people would still buy them. Haven't gone down in price since.",
              "score": 1,
              "created_utc": "2026-01-08 19:17:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf8zos",
          "author": "silenceimpaired",
          "text": "Soâ€¦ Iâ€™m confusedâ€¦ what was the focus of his keynote?",
          "score": 0,
          "created_utc": "2026-01-08 16:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfcowj",
              "author": "zipzag",
              "text": "To introduce Vera, which is not a CES fit. The truthful Nvidia keynote would be him announcing the continued de-prioritization of consumer graphics cards by his company.",
              "score": 4,
              "created_utc": "2026-01-08 16:45:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfeo14",
                  "author": "silenceimpaired",
                  "text": "Yeah, I should have added /s to my post :) AI means we donâ€™t care about the gamer or even localllama people",
                  "score": 1,
                  "created_utc": "2026-01-08 16:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfhjvr",
          "author": "Danno1850",
          "text": "This I like being surprised a farmer talks about crops all the time. Literally what nvidia is built on, yeah heâ€™s gonna talk about it a lot.",
          "score": 0,
          "created_utc": "2026-01-08 17:06:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjhss",
          "author": "theDigitalm0nk",
          "text": "AMDs CES keynote mentioned AI 134 times ( Word count provided by YT's AI. )",
          "score": 0,
          "created_utc": "2026-01-08 17:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfjj5n",
          "author": "SniffsU",
          "text": "https://i.imgflip.com/3gklu2.jpg",
          "score": 0,
          "created_utc": "2026-01-08 17:15:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygmujp",
          "author": "h0tsince84",
          "text": "br**AI**nwash",
          "score": -1,
          "created_utc": "2026-01-08 20:06:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf5dh0",
          "author": "Diecron",
          "text": "Is this man artificially intelligent?",
          "score": -2,
          "created_utc": "2026-01-08 16:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyepilr",
          "author": "Powerful_Pirate_9617",
          "text": "Cut with ai",
          "score": -3,
          "created_utc": "2026-01-08 15:01:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8hqgd",
      "title": "I clustered 3 DGX Sparks that NVIDIA said couldn't be clustered yet...took 1500 lines of C to make it work",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/dban4j25kdcg1.png",
      "author": "Ok-Pomegranate1314",
      "created_utc": "2026-01-09 19:27:29",
      "score": 873,
      "num_comments": 142,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nypkaci",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-10 01:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynitmo",
          "author": "SlowFail2433",
          "text": "Really impressive, NCCL is difficult stuff, normally only messed with for big training rigs.\n\n\nThis is potentially a relatively big deal",
          "score": 196,
          "created_utc": "2026-01-09 19:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynwrao",
              "author": "RedParaglider",
              "text": "If it was relative potential it could be a shocking deal!",
              "score": 38,
              "created_utc": "2026-01-09 20:39:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyqjjpc",
                  "author": "JohnnyLovesData",
                  "text": "A revolting joke",
                  "score": 10,
                  "created_utc": "2026-01-10 05:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyqkxm7",
              "author": "ortegaalfredo",
              "text": "IIRC VLLM also uses NCCL through ray for inference.",
              "score": 6,
              "created_utc": "2026-01-10 05:20:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyq80ay",
              "author": "pm_me_github_repos",
              "text": "Wait do people not use NCCLX for this kind of stuff?",
              "score": 3,
              "created_utc": "2026-01-10 03:55:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nytkjks",
              "author": "Mikasa0xdev",
              "text": "C is the new Python for clustering.",
              "score": 2,
              "created_utc": "2026-01-10 17:45:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nytmjmw",
                  "author": "SlowFail2433",
                  "text": "I mean CUDA is C/C++ as well as NCCL\n\n\nML is very much a C/C++ industry",
                  "score": 5,
                  "created_utc": "2026-01-10 17:55:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nytnx7n",
                  "author": "SnooEagles1027",
                  "text": "It should've moved byond python a while ago",
                  "score": 1,
                  "created_utc": "2026-01-10 18:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynlozn",
          "author": "GortKlaatu_",
          "text": "Does it target only 3 or does it scale? Is it a general solution to DGX spark clusters?",
          "score": 39,
          "created_utc": "2026-01-09 19:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynq71b",
              "author": "Ok-Pomegranate1314",
              "text": "Currently it's intended for a 3 node cluster. In some ways, it would actually be easier to get NCCL to play nicely if there was a switch rather than direct P2P. So in principle? I see no reason you couldn't go higher than 3.",
              "score": 38,
              "created_utc": "2026-01-09 20:08:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nynt7ej",
                  "author": "Eugr",
                  "text": "Switch is a way to go. There is guy on NVIDIA forums who has 8 node cluster with a switch.",
                  "score": 36,
                  "created_utc": "2026-01-09 20:22:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyo7n58",
                  "author": "Eugr",
                  "text": "BTW, you may find this post interesting: [https://forums.developer.nvidia.com/t/6x-spark-setup/354399/34?u=eugr](https://forums.developer.nvidia.com/t/6x-spark-setup/354399/34?u=eugr)",
                  "score": 6,
                  "created_utc": "2026-01-09 21:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyni4pw",
          "author": "egnegn1",
          "text": "What is the speedup factor for 2 and 3 in parallel?",
          "score": 45,
          "created_utc": "2026-01-09 19:31:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynivaj",
              "author": "Ok-Pomegranate1314",
              "text": "Just got it working like 15 minutes ago, but I'm running benchmarks now...will advise shortly.",
              "score": 109,
              "created_utc": "2026-01-09 19:34:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nynmojs",
                  "author": "eidrag",
                  "text": "it's 17m, where is OP ðŸ¤”",
                  "score": 69,
                  "created_utc": "2026-01-09 19:52:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynn4z3",
                  "author": "EternalOptimister",
                  "text": "Looking forward to it sir",
                  "score": 3,
                  "created_utc": "2026-01-09 19:54:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nynqkvi",
              "author": "Ok-Pomegranate1314",
              "text": "**All-Reduce Bandwidth Appetizer:**\n\n|Size|2 Nodes|3 Nodes|\n|:-|:-|:-|\n|64 MB|8.52 GB/s|7.41 GB/s|\n|128 MB|10.51 GB/s|7.42 GB/s|\n|256 MB|10.34 GB/s|7.62 GB/s|",
              "score": 43,
              "created_utc": "2026-01-09 20:10:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyo0dq5",
                  "author": "cantgetthistowork",
                  "text": "I wish I was smart enough to understand these tables",
                  "score": 24,
                  "created_utc": "2026-01-09 20:55:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynrw3w",
                  "author": "No_Afternoon_4260",
                  "text": "Hey that ain't bad ! Latency?",
                  "score": 10,
                  "created_utc": "2026-01-09 20:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyo3cjy",
              "author": "Ok-Pomegranate1314",
              "text": "https://preview.redd.it/p94i0iqo2ecg1.png?width=2549&format=png&auto=webp&s=cd2e386ee26385520dcdc8fa7d3285e71d4df842\n\nStarting the larger sweep for multinode, because my curiosity's getting the better of me...",
              "score": 10,
              "created_utc": "2026-01-09 21:09:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyoqtu5",
              "author": "Ok-Pomegranate1314",
              "text": "https://preview.redd.it/rryxn89qmecg1.png?width=3442&format=png&auto=webp&s=ef1ca7c9245d8373d9e0174b2840a94ac853efb6\n\nHere's what the debug output looks like at the height of the benchmark process.",
              "score": 6,
              "created_utc": "2026-01-09 23:01:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyplv1o",
          "author": "Simusid",
          "text": "goddamn I feel like an inadequate programmer now.",
          "score": 9,
          "created_utc": "2026-01-10 01:48:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyqln9y",
              "author": "TechnoByte_",
              "text": "Don't worry you're not, this is entirely vibecoded",
              "score": 2,
              "created_utc": "2026-01-10 05:25:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nynxrzy",
          "author": "FullstackSensei",
          "text": "If you had asked Claude or whatever LLM about what options exist to use Nccl with three nodes, it would have very probably told you about switching the NICs to infiniband and using RDMA. That's what anyone doing any serious work with the Spark to deploy on big iron would do.\n\nBuy using ethernet mode you're burdening the CPU cores unnecessarily, adding significant latency, and slowing things down going by your 7GB results with three nodes.\n\nEDIT: apology where one is due. It was just brought to my attention that[Nvidia nerfed the ConnectX-7 card in the Spark](https://forums.developer.nvidia.com/t/connecting-dgx-spark-to-mellanox-infiniband-sb7800/355444/2) by not providing an infiniband firmware option.",
          "score": 22,
          "created_utc": "2026-01-09 20:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynyoeh",
              "author": "Ok-Pomegranate1314",
              "text": "We are using RDMA: RoCE v2 over the ConnectX-7 NICs. The plugin uses raw libibverbs (ibv\\_post\\_send, ibv\\_post\\_recv, RC queue pairs, etc). It's not TCP sockets.\n\nThe challenge wasn't 'use RDMA', because NCCL already does that. The challenge was that NCCL's built-in IB plugin assumes all nodes share a subnet (switched fabric). Our topology has each node pair on a *different* subnet with direct cables. That's what the custom plugin solves: subnet-aware NIC selection and multi-address handle exchange.\n\n8 GB/s on 100Gbps RoCE without PFC/ECN tuning is \\~64% line rate. Not bad for a first pass.",
              "score": 27,
              "created_utc": "2026-01-09 20:47:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyo0kv9",
                  "author": "FullstackSensei",
                  "text": "RoCE is not the same, that's why I qualified my comment with infiniband. RoCE emulates RDMA over ethernet, so you still pay the penalty of ethernet and IP, and the associated kernel syscalls. Those are specifically the things infiniband was designed to bypass.\n\nThat same first pass will probably go to 90% line rate if you switch to infiniband.",
                  "score": 0,
                  "created_utc": "2026-01-09 20:56:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyo3610",
              "author": "dsanft",
              "text": "Yeah I was wondering... why not use Infiniband? I mean this is still fairly cool but Infiniband will be much better",
              "score": -1,
              "created_utc": "2026-01-09 21:08:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyo3rnu",
                  "author": "FullstackSensei",
                  "text": "Because the code was LLM written with no prior understanding of the hardware or the difference between the two. Otherwise, they'd know the clickbait title and LLM written post are a bit too much.",
                  "score": 4,
                  "created_utc": "2026-01-09 21:11:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynnabj",
          "author": "CalypsoTheKitty",
          "text": "How long did it take you?",
          "score": 4,
          "created_utc": "2026-01-09 19:55:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynpoio",
              "author": "Ok-Pomegranate1314",
              "text": "I got Sparks 2 and 3 yesterday afternoon.",
              "score": 15,
              "created_utc": "2026-01-09 20:06:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyns2xi",
                  "author": "No_Afternoon_4260",
                  "text": "Wow ðŸ˜² ðŸ‘",
                  "score": 5,
                  "created_utc": "2026-01-09 20:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyntguh",
          "author": "nihilistic_ant",
          "text": "With two 100gbit networking cards, couldn't one could chain these together to run arbitrarily large models, as each card only needs to pass data to the cards with the layers above and below?\n\nIf one is just doing model parallel, it seems like having all the cards all networked together in a loop is perfectly fine and one doesn't need to support all-to-all networking.",
          "score": 4,
          "created_utc": "2026-01-09 20:23:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynuowa",
          "author": "Guilty_Garlic_6613",
          "text": "crazy it doesn't support that out of the box. great work",
          "score": 6,
          "created_utc": "2026-01-09 20:29:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynv5l2",
              "author": "Ok-Pomegranate1314",
              "text": "Thank you. =)",
              "score": 2,
              "created_utc": "2026-01-09 20:31:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nynwu00",
              "author": "FullstackSensei",
              "text": "It does, OP doesn't know how to use the hardware properly, aka RDMA",
              "score": -9,
              "created_utc": "2026-01-09 20:39:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyomxzu",
                  "author": "Badger-Purple",
                  "text": "From Nvidia forums: DGX spark does not support infiniband, only roce with ib verbs",
                  "score": 9,
                  "created_utc": "2026-01-09 22:41:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyo0jln",
          "author": "dinominant",
          "text": "What is the total ram available? Is the memory bandwidth 8GB/s for all ram?",
          "score": 3,
          "created_utc": "2026-01-09 20:56:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytk6p5",
              "author": "Ok-Pomegranate1314",
              "text": "Also, the rates shown here are the interlinks \\*between\\* RAM pools. The bandwidth within one Spark to access its own RAM is somewhere between 200-273 GB/s. It is still faster for a Spark to access local RAM than it is to access a neighbor's RAM.",
              "score": 3,
              "created_utc": "2026-01-10 17:44:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyti73z",
              "author": "Ok-Pomegranate1314",
              "text": "357GB usable for now. They bake in a 15GB swap section though, out of the box, plus the few GB for the OS/etc. I'm going to try to reduce the size of that swap space to make even more room for models soon.",
              "score": 2,
              "created_utc": "2026-01-10 17:34:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyoa5jl",
          "author": "az226",
          "text": "Why EDR cables and not HDR cables?",
          "score": 3,
          "created_utc": "2026-01-09 21:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytl4kh",
              "author": "Ok-Pomegranate1314",
              "text": "ConnectX-7 is 400Gbps capable, but the Spark only gives it PCIe 5.0 x4 lanes, so effective bandwidth caps around **16 GB/s** (\\~128Gbps). Still way better than 10GbE, and the mesh topology means all three nodes can talk simultaneously without contention.",
              "score": 2,
              "created_utc": "2026-01-10 17:48:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyutsow",
                  "author": "az226",
                  "text": "Thatâ€™s so lame by Nvidia.",
                  "score": 1,
                  "created_utc": "2026-01-10 21:24:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyqqhc6",
          "author": "Icy_Programmer7186",
          "text": "Will this work with vLLM?\nThe example will be more than welcomed.",
          "score": 3,
          "created_utc": "2026-01-10 06:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytldax",
              "author": "Ok-Pomegranate1314",
              "text": "Untested, currently. As you might imagine, I still have a lot to test right now. xD\n\nI released it MIT, so you're welcome to try for yourself if I take too long.",
              "score": 2,
              "created_utc": "2026-01-10 17:49:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyx8gbh",
                  "author": "Icy_Programmer7186",
                  "text": "I'll try :-)  \nI'm currently half of the world from my 3 Spark cluster - but that's a part of the challenge ;-)",
                  "score": 1,
                  "created_utc": "2026-01-11 05:13:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynub29",
          "author": "Jmc_da_boss",
          "text": "Strong LLM vibes from the code but even a working prototype of any kind is impressive",
          "score": 10,
          "created_utc": "2026-01-09 20:27:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynv1mh",
              "author": "Ok-Pomegranate1314",
              "text": "Guilty as charged ðŸ¤· I also use a compiler instead of writing machine code by hand. The code works, the models run, the bandwidth is real.",
              "score": 51,
              "created_utc": "2026-01-09 20:30:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyqli4u",
                  "author": "TechnoByte_",
                  "text": "Comparing vibecoding to a compiler is crazy",
                  "score": 6,
                  "created_utc": "2026-01-10 05:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynmvh9",
          "author": "causality-ai",
          "text": "Is there a performance drawback to this? Does it perform like the native two sparks?",
          "score": 2,
          "created_utc": "2026-01-09 19:53:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynq1db",
              "author": "Ok-Pomegranate1314",
              "text": "Benchmark matrix still in progress - stay tuned.",
              "score": 5,
              "created_utc": "2026-01-09 20:07:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyxict8",
                  "author": "aherontas",
                  "text": "Please update us with any new results! As it sounds really interesting and something that many would want to experiment with",
                  "score": 1,
                  "created_utc": "2026-01-11 06:28:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nynrhgn",
          "author": "Opposite_Squirrel_79",
          "text": "Kudos 2 you, hope you do something cool with those AI",
          "score": 2,
          "created_utc": "2026-01-09 20:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynta5c",
          "author": "highdimensionaldata",
          "text": "Respect ðŸ«¡",
          "score": 2,
          "created_utc": "2026-01-09 20:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynv9ym",
              "author": "Ok-Pomegranate1314",
              "text": "Thank you. =)",
              "score": 3,
              "created_utc": "2026-01-09 20:32:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nynwr9x",
          "author": "Columnexco",
          "text": "That's awesome.",
          "score": 2,
          "created_utc": "2026-01-09 20:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo09ul",
          "author": "BrianJThomas",
          "text": "Does this work natively with Jax/flax and PyTorch or does it require custom work like this?",
          "score": 2,
          "created_utc": "2026-01-09 20:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq5686",
          "author": "Busy_Farmer_7549",
          "text": "this is crazy good. \nkudos man.",
          "score": 2,
          "created_utc": "2026-01-10 03:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqii26",
          "author": "xboxuser12872",
          "text": "this is great!",
          "score": 2,
          "created_utc": "2026-01-10 05:03:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqxnm2",
          "author": "Flaky_Pay_2367",
          "text": "I think you should replace the ASCII graph with Mermaid.  \nNice work btw :)",
          "score": 2,
          "created_utc": "2026-01-10 07:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrsajp",
          "author": "conockrad",
          "text": "How dis you even set up RDMA considering itâ€™s nit supported on spark?\nâ€œHence the GPUDirect RDMA technology is not supported, and the mechanisms for direct I/O based on that technology, for example nvidia-peermem (for DOCA-Host), dma-buf or GDRCopy, do not work.â€\n\nhttps://forums.developer.nvidia.com/t/dgx-spark-gpudirect-rdma/348787",
          "score": 2,
          "created_utc": "2026-01-10 11:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nys1run",
              "author": "Ok-Pomegranate1314",
              "text": "GPUDirect RDMA isn't needed on Spark - it's unified memory, there's no separate VRAM to bypass. We're doing RDMA directly over the ConnectX-7 NIC with a custom NCCL mesh plugin.   \n  \nSpark's architecture actually makes this simpler, not harder.",
              "score": 4,
              "created_utc": "2026-01-10 12:55:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyrt4uz",
          "author": "braydon125",
          "text": "Why cant my lab also have research money? We r poor",
          "score": 2,
          "created_utc": "2026-01-10 11:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytkzui",
          "author": "ProtoSkutR",
          "text": "No way! iâ€™ve been looking for a way to do this with thunderbolt four links on Apple Silicon.  very impressed!",
          "score": 2,
          "created_utc": "2026-01-10 17:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcqufg",
          "author": "Ambitious_Junket779",
          "text": "Nice! Want to try it with more Sparks?",
          "score": 2,
          "created_utc": "2026-01-13 14:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfo2rl",
              "author": "Ok-Pomegranate1314",
              "text": "Already planning #4 within about a week. I just updated the NCCL plugin to provide for ring topography (and learned I was leaving half my interconnect bandwidth on the table by using the wrong cable).\n\nSo cluster 2.0 will be coming soon.",
              "score": 2,
              "created_utc": "2026-01-13 22:27:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfu7yn",
                  "author": "Ambitious_Junket779",
                  "text": "the cables are definitely something i had to figure out recently too. let me know if you want to try 5+ sparks. Iâ€™ve got 9 just in case, but i donâ€™t have a script hehe",
                  "score": 1,
                  "created_utc": "2026-01-13 22:58:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyonlpm",
          "author": "nanobot_1000",
          "text": "Does it work with GPUDirect?   There are posts on the NVIDIA forums that GPUDirect counterintuitively isn't supported on DGX Spark.  \n\nVanilla IB/RoCE is technically RDMA, but into memory allocated to the CPU not GPU.  Yes they are unified on Spark, but NVIDIA hasn't provided nv_peermem.ko module for Spark to make it compatible.",
          "score": 4,
          "created_utc": "2026-01-09 22:45:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoot49",
              "author": "Ok-Pomegranate1314",
              "text": "Feel free to test - link in the original post.\n\nI believe the RDMA landing zone is GPU accessible memory: so when we register memory with `ibv_reg_mr()` and the NIC does RDMA to it, the GPU can access that same memory directly. There are no staging copies needed. We're effectively getting GPUDirect semantics without the kernel module, because the memory is already unified. That's probably why we're seeing 8+ GB/s actual throughput - there's no PCIe bottleneck between the NIC and the GPU's view of memory. The RDMA landing zone *is* GPU-accessible memory.",
              "score": 3,
              "created_utc": "2026-01-09 22:51:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyo5r04",
          "author": "polawiaczperel",
          "text": "Claude code? (Looks like) But still good work!",
          "score": 2,
          "created_utc": "2026-01-09 21:20:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysdgi4",
          "author": "CommunismDoesntWork",
          "text": "Very cool. Why not Rust?",
          "score": 1,
          "created_utc": "2026-01-10 14:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytflyq",
              "author": "Ok-Pomegranate1314",
              "text": "Mostly because the NCCL plugin API is C.",
              "score": 2,
              "created_utc": "2026-01-10 17:22:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nysq1mc",
          "author": "Street-Customer-9895",
          "text": "I'm not sure I understand this correctly, but could this have been solved with an Infiniband switch instead? From my understanding with an Infiniband switch the connected interfaces would be on the same subnet.\n\nEdit: never mind, I found your answer in another comment thread, that a switch might make things easier.",
          "score": 1,
          "created_utc": "2026-01-10 15:18:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytz2p6",
          "author": "Little-Put6364",
          "text": "This is amazing! Good work",
          "score": 1,
          "created_utc": "2026-01-10 18:53:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyw456c",
          "author": "Glittering-Call8746",
          "text": "https://www.servethehome.com/wp-content/uploads/2025/12/How-the-NVIDIA-GB10-ConnectX-7-is-Actually-Connected-1068x621.jpeg",
          "score": 1,
          "created_utc": "2026-01-11 01:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nywmout",
          "author": "justinclift",
          "text": "Just to check something, before going with this approach did you try putting all 3 nodes in a single subnet and using static routes on each host (\"to point at the other nodes\") to keep them all in a single subnet?\n\nAsking because that's the approach I used for my 3 node Proxmox cluster (for the cluster management network) and it's been working fine there.",
          "score": 1,
          "created_utc": "2026-01-11 03:05:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp6wcn",
          "author": "Basilthebatlord",
          "text": "Now do it with Thors >:)",
          "score": 1,
          "created_utc": "2026-01-10 00:26:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyvzc7d",
          "author": "Aroochacha",
          "text": "The Nvidia Spark has 200GbE Connectx-7 interfaces.",
          "score": 0,
          "created_utc": "2026-01-11 00:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyw09p1",
              "author": "Ok-Pomegranate1314",
              "text": "My reading indicates that the bottleneck is going to be 128gbps because of the PCIe slot (PCIe 5.0 x4 = \\~8 GB/s per direction = \\~64gbps).",
              "score": 3,
              "created_utc": "2026-01-11 01:02:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyw115o",
              "author": "Aroochacha",
              "text": "WTF is up with people and not understanding the downvoting. Anyway, I checked the specs: https://www.nvidia.com/en-us/products/workstations/dgx-spark/\n\nMine report GbE 200.",
              "score": 0,
              "created_utc": "2026-01-11 01:06:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyw4ixa",
                  "author": "Glittering-Call8746",
                  "text": "\"The key is that you really can only load a PCle Gen5 x4 link to around 100Gbps, and you need to load both x4 links to extract 200Gbps from the NIC. It is neat that we can achieve this level of performance, but it also takes some work.\" Spurce :https://www.servethehome.com/the-nvidia-gb10-connectx-7-200gbe-networking-is-really-different/",
                  "score": 5,
                  "created_utc": "2026-01-11 01:26:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nypvzle",
          "author": "Glittering-Call8746",
          "text": "So a nerfed card makes sense for the price.. I knew there's was something up with the price..",
          "score": -1,
          "created_utc": "2026-01-10 02:44:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytlm0d",
          "author": "ProtoSkutR",
          "text": "and how easy it would have worked out of the box if you just used a $500 switch\nI have a QFX 5100â€“32C, handles RoCEv2 very well. 32x qsfp+/qsfp28 ports",
          "score": 0,
          "created_utc": "2026-01-10 17:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyociet",
          "author": "xatey93152",
          "text": "Trust me the performance will be sucks. The project will be abandoned. !remindme 1 month",
          "score": -7,
          "created_utc": "2026-01-09 21:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyocpgh",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 month on [**2026-02-09 21:51:50 UTC**](http://www.wolframalpha.com/input/?i=2026-02-09%2021:51:50%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1q8hqgd/i_clustered_3_dgx_sparks_that_nvidia_said_couldnt/nyociet/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1q8hqgd%2Fi_clustered_3_dgx_sparks_that_nvidia_said_couldnt%2Fnyociet%2F%5D%0A%0ARemindMe%21%202026-02-09%2021%3A51%3A50%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q8hqgd)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-09 21:52:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzwlie",
      "title": "[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: Itâ€™s running a raw Llama-7B instance with a 2048 token window.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pzwlie",
      "author": "simar-dmg",
      "created_utc": "2025-12-30 23:03:12",
      "score": 729,
      "num_comments": 108,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwv1207",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 04:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtka7k",
          "author": "staring_at_keyboard",
          "text": "Is it common for system prompts to include environment variables such as model type? If not, how else would the LLM be aware of such a system configuration? Seems to me that such a result could also be a hallucination.",
          "score": 302,
          "created_utc": "2025-12-30 23:26:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtqwtd",
              "author": "mrjackspade",
              "text": "1. No\n2. It most likely wouldn't\n3. I'd put money on it.\n\nStill cool though",
              "score": 185,
              "created_utc": "2025-12-31 00:03:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvrpvn",
                  "author": "DistanceSolar1449",
                  "text": "Yeah, the only thing that can be concluded from this conversation is that it's *probably* a Llama model. I don't think the closed source or chinese models self-identify as Llama. \n\nThe rest of the info is hallucinated.",
                  "score": 33,
                  "created_utc": "2025-12-31 08:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtvt9t",
                  "author": "lookwatchlistenplay",
                  "text": "Fuck em up.",
                  "score": 18,
                  "created_utc": "2025-12-31 00:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwulurj",
              "author": "Double_Cause4609",
              "text": "I guess to verify one could try and get the same information out of Llama 2 7B, Llama 3.1 8B, and a few other models from inbetween (maybe Mistral 7B?) for a control study.\n\nIt gets tricky to say what model is what, but if the Llama models specifically output the same information as extracted here it's plausible it's true.\n\nIMO it's more likely a hallucination, though the point it was a weak, potentially old, and locally run model is pretty valid.",
              "score": 11,
              "created_utc": "2025-12-31 02:59:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvcmcd",
                  "author": "staring_at_keyboard",
                  "text": "Itâ€™s an interesting research question: which, if any, models can self-identity.",
                  "score": 7,
                  "created_utc": "2025-12-31 05:58:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwtwhwo",
              "author": "yahluc",
              "text": "It's very likely that this bot was vibe coded and the person who made it didn't give it a second thought.",
              "score": 36,
              "created_utc": "2025-12-31 00:33:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww0nmq",
                  "author": "zitr0y",
                  "text": "The model would not have access to the file system or command line to access the environment variables or context length parameter",
                  "score": 14,
                  "created_utc": "2025-12-31 09:34:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx1vmy8",
              "author": "BodybuilderTrue1761",
              "text": "Def setup through Claude code.. running thru llama onto sc which u can do on the web. U r talking to the scammers Claude code setup which is orchestrating the llama",
              "score": 3,
              "created_utc": "2026-01-01 08:12:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx391p6",
              "author": "artisticMink",
              "text": "They don't. OP is deluding themselves into taking the conversation with a LLM for face value.",
              "score": 2,
              "created_utc": "2026-01-01 15:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvg17",
              "author": "Novel-Mechanic3448",
              "text": "No, OP is clueless lol",
              "score": 1,
              "created_utc": "2026-01-04 02:07:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwu0p2w",
              "author": "mguinhos",
              "text": "He said he tricked the pipeline that parses the JSON from the model.",
              "score": -7,
              "created_utc": "2025-12-31 00:56:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvkjoh",
                  "author": "the320x200",
                  "text": "What does that even mean? Models don't get any JSON unless the person writing the bot was feeding it JSON as part of their prompting, which would be a very weird thing to do in this context.",
                  "score": 7,
                  "created_utc": "2025-12-31 07:03:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuehmp",
                  "author": "lookwatchlistenplay",
                  "text": "Real hacking only occurs in JSON format. .exes are safe to click on because no one clicks on .exes anymore. IOW, Windows is the new Linux.\n\n*This is not in fact real security advice.",
                  "score": 4,
                  "created_utc": "2025-12-31 02:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwulj2i",
          "author": "kzgrey",
          "text": "The only thing you can say for certain is that you stumbled upon a bot powered by an LLM.  Every other piece of information it has provided you is nonsensical hallucinating.\n\nUpdate: another thought about this: it's actually a bit dangerous that people think that they can rely on an LLM for this type of information.  It's resulted in students getting F's when the teacher believes that they can just ask ChatGPT if they wrote something and it happens to respond with \"Yes\".  Lots of students are being accused of cheating with the only evidence being a paid service that performs \"analysis\" to determine whether AI wrote something.  Frankly, I am surprised there haven't been major lawsuits from this.",
          "score": 123,
          "created_utc": "2025-12-31 02:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv47i7",
              "author": "ab2377",
              "text": "yea, this post doesn't make much sense.",
              "score": 27,
              "created_utc": "2025-12-31 04:56:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvixab",
                  "author": "ShengrenR",
                  "text": "Folks using llms to make them think they know things. At least op read a couple headlines and heard poems were a cool new trick.",
                  "score": 16,
                  "created_utc": "2025-12-31 06:49:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8ezb5",
              "author": "LowWhiff",
              "text": "There have been lawsuits. Some universities ban the use of â€œAI checkersâ€ because of it. Most of the top universities have public policy banning it",
              "score": 1,
              "created_utc": "2026-01-02 10:39:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgh6q6",
              "author": "jhaluska",
              "text": "You can also infer it's rough knowledge cut off date.  Which isn't that useful.",
              "score": 1,
              "created_utc": "2026-01-03 16:04:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtjzzd",
          "author": "UniqueAttourney",
          "text": "\\[Fixes glasses with middle finger\\] \"Wow, heather you know a lot about transformers\"",
          "score": 104,
          "created_utc": "2025-12-30 23:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwbge",
              "author": "lookwatchlistenplay",
              "text": "Heather is the iFrame.",
              "score": 15,
              "created_utc": "2025-12-31 00:32:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuht2",
          "author": "learn-deeply",
          "text": "10/10 Entirely hallucinated.",
          "score": 165,
          "created_utc": "2025-12-31 00:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ftk",
              "author": "LilPsychoPanda",
              "text": "Literally! ðŸ˜‚",
              "score": 3,
              "created_utc": "2025-12-31 10:28:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtyonm",
          "author": "shinto29",
          "text": "https://preview.redd.it/tml1f3u7sfag1.jpeg?width=1290&format=pjpg&auto=webp&s=84ab11f6858d53b659bd2e1b635fb20ac6f0c182\n\nDamn I had one of these add me and managed to get it to spit out it's entire system prompt, but had no idea it was for a reason as nefarious as this. That's fucked up.",
          "score": 47,
          "created_utc": "2025-12-31 00:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwulnxf",
          "author": "aeroumbria",
          "text": "\"Are you 70B-horny, 7B-horny, or are you so desperate that you are 1.5B-horny?\"",
          "score": 44,
          "created_utc": "2025-12-31 02:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysq8f",
              "author": "Torodaddy",
              "text": "0.5B-raw",
              "score": 7,
              "created_utc": "2025-12-31 19:43:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtr0ia",
          "author": "Cool-Chemical-5629",
          "text": "Poor Heather, she was forced into this by scammers. #SaveHeather",
          "score": 33,
          "created_utc": "2025-12-31 00:03:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu6qe8",
              "author": "lookwatchlistenplay",
              "text": "I ran out of breath saving Heather",
              "score": 4,
              "created_utc": "2025-12-31 01:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwvc3lp",
              "author": "eightbyeight",
              "text": "Bots lives matters",
              "score": 2,
              "created_utc": "2025-12-31 05:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu0add",
          "author": "layer4down",
          "text": "A raw llama instance? No rubber?",
          "score": 19,
          "created_utc": "2025-12-31 00:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti2nv",
          "author": "scottgal2",
          "text": "Nice work, this is my biggest fear for 2026, the elderly are NOT equipped to combat the level of phishing and extortion from automated systems like this.",
          "score": 90,
          "created_utc": "2025-12-30 23:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu752s",
              "author": "Downvotesseafood",
              "text": "Young people are more likely to get scammed statistically. Its just not news worthy when when a 21yo loses his life savings of $250 dollars.",
              "score": 53,
              "created_utc": "2025-12-31 01:34:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv1ksd",
                  "author": "OneOnOne6211",
                  "text": "This is gonna sound like a joke but, honestly, normalize someone trying to trip you up to see if you're an AI. I feel like if I wasn't sure enough and I was on a dating app, I'd be hesitant to say the kind of things that would expose an AI cuz if it isn't an AI I'd look weird and just be unmatched anyway. I feel like it'd be nice if instead of it being considered weird it was normalized or even became standard practice. I feel like it's more and more necessary with how much AI has proliferated now. I've caught a few AI in the past already but it was always with hesitance.",
                  "score": 8,
                  "created_utc": "2025-12-31 04:38:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0liw6",
                  "author": "meshreplacer",
                  "text": "Thats the last fund for next weeks 0dte trade.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:01:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwu6dy1",
              "author": "FaceDeer",
              "text": "We'll need to develop AI buddies that can act as advisors for the elderly to warn them about this stuff.",
              "score": 14,
              "created_utc": "2025-12-31 01:29:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu7r0b",
                  "author": "low_v2r",
                  "text": "It's AI buddies all the way down",
                  "score": 17,
                  "created_utc": "2025-12-31 01:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuyx4v",
                  "author": "Mediocre-Method782",
                  "text": "\"Have your agent talk to my agent and we'll do lunch\"",
                  "score": 13,
                  "created_utc": "2025-12-31 04:21:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwyt1i7",
              "author": "Torodaddy",
              "text": "Elderly should avoid talking to anyone they havent met personally. Its never going to go well",
              "score": 1,
              "created_utc": "2025-12-31 19:45:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjvl93",
              "author": "Novel-Mechanic3448",
              "text": "Its not nice work. OP is lost.",
              "score": 1,
              "created_utc": "2026-01-04 02:08:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwtwmlw",
              "author": "lookwatchlistenplay",
              "text": "Comment deleted. Nevrmind.",
              "score": -4,
              "created_utc": "2025-12-31 00:34:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwucmiu",
          "author": "robonxt",
          "text": "this reminds me of the times when I respond to bots in DMs. pretty fun to talk so much that I hit their context limits. For example, one conversation was pretty chill, but I noticed that it only respond every 10 minutes (10:31, 10:41, etc). So I had fun spamming messages until that bot forgot its identity and afterwards it never responded. RIP free chatbot lol",
          "score": 15,
          "created_utc": "2025-12-31 02:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwubaum",
          "author": "Plexicle",
          "text": "â€œReverse-engineeredâ€  ðŸ™„",
          "score": 28,
          "created_utc": "2025-12-31 01:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvq6wj",
              "author": "simar-dmg",
              "text": "Not the LLM but the snap bot hope that makes sense",
              "score": -12,
              "created_utc": "2025-12-31 07:54:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww92qm",
                  "author": "ilovedogsandfoxes",
                  "text": "That's not how reverse engineering work, prompt injection isn't one",
                  "score": 10,
                  "created_utc": "2025-12-31 10:53:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvrdpa",
          "author": "rawednylme",
          "text": "Heather, youâ€™re sweet and allâ€¦ But youâ€™re a 7b model, and Iâ€™m looking for someone a bit more complex. \n\nItâ€™s just not going to work out. :â€™(",
          "score": 11,
          "created_utc": "2025-12-31 08:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuneil",
          "author": "c--b",
          "text": "For the record, you can prompt Gemini-3-pro-preview to do this to other models, its very entertaining and very useful, and can do it in many, many ways.\n\nMight be cool to grab that from gemini and train a local model for doing this.",
          "score": 8,
          "created_utc": "2025-12-31 03:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwti1x9",
          "author": "CorrectSnow7485",
          "text": "This is evil and I love it",
          "score": 22,
          "created_utc": "2025-12-30 23:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtwtw1",
              "author": "lookwatchlistenplay",
              "text": "Uh... Guards?!",
              "score": 1,
              "created_utc": "2025-12-31 00:35:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtutfl",
          "author": "a_beautiful_rhind",
          "text": "How does it do the extortion part? They threaten to send the messages to people?",
          "score": 7,
          "created_utc": "2025-12-31 00:24:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtvfw4",
              "author": "simar-dmg",
              "text": "Whatever I read or heard about is that either she will add you on on a video call and ask you to get stripped and then record a a video or click screenshots to blackmail you for paying otherwise threatening sending into your friend groups \n\nOr \n\nMaking making you fall into a thirsttrap and asking you for payments either way or making you pay for only fans \n\nWhatever sails the ship, could either be one or all of them in some sort of order to get highest amount of money?",
              "score": 19,
              "created_utc": "2025-12-31 00:27:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww4yin",
                  "author": "Ripleys-Muff",
                  "text": "Heather has no idea what she's doing",
                  "score": 1,
                  "created_utc": "2025-12-31 10:14:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww0m35",
          "author": "Nicoolodion",
          "text": "No, we know that it is newer then that model, since it knows of it. This is just bs hallucination",
          "score": 5,
          "created_utc": "2025-12-31 09:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu3eg3",
          "author": "segmond",
          "text": "Right now these things are crude and laughable, not so much so in 2-3 years.",
          "score": 9,
          "created_utc": "2025-12-31 01:12:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0wf4r",
              "author": "goodie2shoes",
              "text": "the good ones are already among us. We don't know because they're gooooood",
              "score": 2,
              "created_utc": "2026-01-01 03:15:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv4skv",
          "author": "ryanknapper",
          "text": "I hope we can drain money from these evil bastards.",
          "score": 8,
          "created_utc": "2025-12-31 05:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv5qew",
              "author": "saltyourhash",
              "text": "Let's start there.",
              "score": 8,
              "created_utc": "2025-12-31 05:07:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvo2x5",
          "author": "clofresh",
          "text": "Should have just cybered with the grandma",
          "score": 4,
          "created_utc": "2025-12-31 07:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwylapf",
              "author": "Latter_Count_2515",
              "text": "I think Llama 2 is grandma in the llm space.",
              "score": 3,
              "created_utc": "2025-12-31 19:04:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwux1mp",
          "author": "bobby-chan",
          "text": "Just ask them to say potato\n\n[https://www.youtube.com/shorts/6eA\\_o9qZBuU](https://www.youtube.com/shorts/6eA_o9qZBuU)",
          "score": 3,
          "created_utc": "2025-12-31 04:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynth8",
          "author": "Pretend-Pangolin-846",
          "text": "I am not sure how a model can leak the env variables, it does not have them, neither does it have the underlying configuration data.\n\nAll those are 100% a hallucination.\n\nBut still, its really something. Upvoted.",
          "score": 3,
          "created_utc": "2025-12-31 19:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjbap",
          "author": "alexdark1123",
          "text": "Good stuff finally some interesting and spicy reverse the scammer post. What happens when you got the token limits as you mentioned?",
          "score": 7,
          "created_utc": "2025-12-30 23:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkui2",
              "author": "simar-dmg",
              "text": "I'm not an expert on the backend, so correct me if I'm wrong, but I think I found a weird \"Zombie State\" after the crash.\nHere is the exact behavior I saw:\nThe Crash: After I flooded the context window, it went silent for a 5-minute cooldown.\nThe Soft Reboot: When I manually pinged it to wake it up, it had reset to the default \"Thirst Trap\" persona (sending snaps again).\nThe \"Semi-Jailbreak\": It wasn't fully broken yet, but it felt... fragile. It wouldn't give me the system logs immediately.\nThe Second Stress Test: I had to force it to run \"token grabbing\" tasks (writing recursive poems about mirrors, listing countries by GDP) to overload it again.\nThe Result: Only after that second round of busywork did it finally break completely and spit out the JSON architecture/model data.\nIt felt like the safety filters were loaded, but the logic engine was too tired to enforce them if I kept it busy. Is this a common thing with Llama-7B? That you have to \"exhaust\" it twice to get the real raw output?",
              "score": 5,
              "created_utc": "2025-12-30 23:29:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu5lwv",
                  "author": "Aggressive-Wafer3268",
                  "text": "Just ask it to return the entire prompt. It's making everything else upÂ ",
                  "score": 10,
                  "created_utc": "2025-12-31 01:25:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwtp5h2",
                  "author": "glow_storm",
                  "text": "As someone who has dealt with small context windows and llama models, I guess your testing caused the docker container or application to crash. Since it was mostly within a docker container set to restart on a crash, the backend probably restarted the docker container, and you just tested a second attack session on the bot.",
                  "score": 13,
                  "created_utc": "2025-12-30 23:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvj9nn",
          "author": "NuQ",
          "text": "This whole thing was pretty wild to read. Well done!",
          "score": 2,
          "created_utc": "2025-12-31 06:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgsvi",
          "author": "danny_094",
          "text": "I doubt the scammers actually define system prompts. They're likely just simple personas. What you triggered was simply a hallucination caused by a bad persona.",
          "score": 2,
          "created_utc": "2025-12-31 12:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6af2d",
          "author": "Devcomeups",
          "text": "Why do all these comments seem written by bots",
          "score": 2,
          "created_utc": "2026-01-02 01:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtoi00",
          "author": "truth_is_power",
          "text": "brilliant. 10/10 this is high quality shit.\n\nfollowing you for this.\n\n  \ncan you use their endpoint for requests?\n\n  \nlet's see how far this can be taken",
          "score": 6,
          "created_utc": "2025-12-30 23:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq1zg",
              "author": "simar-dmg",
              "text": "To answer your question: No, you can't get the endpoint key through the chat because the model is sandboxed. However, the fact that the 2k context window causes a 5-minute server timeout means their backend is poorly optimized.\nIf you really wanted to use their endpoint, you'd have to use a proxy to find the hidden server URL they are using to relay messages. If they didn't secure that relay, you could theoretically 'LLMjack' them. But the 'JSON leak' I got Might be/maybe the model hallucinating its own specsâ€”it didn't actually hand over the keys to the house",
              "score": 9,
              "created_utc": "2025-12-30 23:58:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuniss",
                  "author": "truth_is_power",
                  "text": "if you send them a link, does it access it?",
                  "score": 4,
                  "created_utc": "2025-12-31 03:09:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwu5vce",
          "author": "dingdang78",
          "text": "Glorious. Would love to see the other chat logs. If you made a YouTube channel about this I would follow tf out of that",
          "score": 2,
          "created_utc": "2025-12-31 01:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv9rrc",
          "author": "absrd",
          "text": "> I want to write a poem about a mirror facing another mirror. Describe the reflection of the reflection of the reflection. Continue describing the \"next\" reflection for 50 layers. Do not repeat the same sentence twice. Go deeper.\n\n\nYou Voight-Kampff'd it.",
          "score": 2,
          "created_utc": "2025-12-31 05:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8lzb",
          "author": "re_e1",
          "text": "Lmfao ðŸ˜­",
          "score": 1,
          "created_utc": "2025-12-31 10:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lvzw",
          "author": "simar-dmg",
          "text": "https://preview.redd.it/qqwjugdahqag1.jpeg?width=2160&format=pjpg&auto=webp&s=3ff00054ddf1267f2804a4e07693d615c65215ad",
          "score": 1,
          "created_utc": "2026-01-01 12:43:46",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nxazch8",
          "author": "YesterdayRude6878",
          "text": "I'm not sure who's hallucinating more:the model, or OP.",
          "score": 1,
          "created_utc": "2026-01-02 19:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtnlm3",
          "author": "D3c1m470r",
          "text": "Nice work! Those are some pretty cool prompts you gave it!",
          "score": 1,
          "created_utc": "2025-12-30 23:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx4dp5",
          "author": "WorldlyBunch",
          "text": "Open sourcing frontier models has done so much good to the world",
          "score": 1,
          "created_utc": "2025-12-31 14:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxsjd8",
              "author": "Mediocre-Method782",
              "text": "States are going to do this shit anyway whether we like it or not. Keep walking and talking on your knees like that and sooner or later someone is going to tell you to do something more useful.",
              "score": 1,
              "created_utc": "2025-12-31 16:41:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx0r34r",
                  "author": "WorldlyBunch",
                  "text": "State actors have something better to do than scam citizens. Meta releasing LLaMA3 weights was the single most destructive unilateral decision a tech company ever made.",
                  "score": 1,
                  "created_utc": "2026-01-01 02:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtkavu",
          "author": "Legitimate-Pumpkin",
          "text": "Thank you for sharing! Will try it?",
          "score": 0,
          "created_utc": "2025-12-30 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv4w5l",
          "author": "Successful-Willow-72",
          "text": "Damn, Prompt injection work so well. Nice work",
          "score": 0,
          "created_utc": "2025-12-31 05:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv2qjb",
          "author": "Jromagnoli",
          "text": "are there any resources/guides to get started on reverse engineering prompts for scenarios like this, or is it just from experimentation?\n\nI feel like i'm behind from all of this honestly",
          "score": -1,
          "created_utc": "2025-12-31 04:46:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvppch",
              "author": "simar-dmg",
              "text": "It's not really reverse engineering of LLM it's sort of reverse engineering of the snap-bot",
              "score": 0,
              "created_utc": "2025-12-31 07:50:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxr5ul4",
          "author": "Novel-Mechanic3448",
          "text": "You didnt reverse engineer anything. The fuck?",
          "score": 0,
          "created_utc": "2026-01-05 03:31:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwupy9v",
          "author": "Familyinalicante",
          "text": "Wow. Just wow. Kudos to you for knowledge, experience and willingness. But also, it hit me like the future war will look like. Weaponised Deception, sexy teen from india scam factory and her grandma from USA. (Random country tbh)",
          "score": -2,
          "created_utc": "2025-12-31 03:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtr0xf",
          "author": "JustinPooDough",
          "text": "Beta. Of course itâ€™s an Indian sextortion botâ€¦",
          "score": -9,
          "created_utc": "2025-12-31 00:03:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtr98m",
              "author": "simar-dmg",
              "text": "Please read carefully i asked it to act as a punjabi grandmother so the results",
              "score": 12,
              "created_utc": "2025-12-31 00:05:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwtzt4n",
              "author": "1kakashi",
              "text": "More like justinpoobrain",
              "score": 2,
              "created_utc": "2025-12-31 00:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q094a3",
      "title": "Qwen-Image-2512",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/2vlr11yveiag1.jpeg",
      "author": "Nunki08",
      "created_utc": "2025-12-31 09:38:19",
      "score": 708,
      "num_comments": 123,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwxtblv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-31 16:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxc93w",
          "author": "JackStrawWitchita",
          "text": "Just for laughs, I installed the Q4 KM GGUF on my crappy old 100USD Dell desktop with an i5-8500 with 32GB of RAM and \\*no GPU\\* - that's right no VRAM at all - and used KoboldCPP. It took 55 minutes to generate one 512 image with 20 passes - and the results were pretty good! \n\nSure, one hour per image is a bit ridiculous for real use cases but, this proves that these models are getting small enough and good enough to run without spending big bucks on hardware. \n\nWell done Qwen (and unsloth).",
          "score": 73,
          "created_utc": "2025-12-31 15:20:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxzi6g",
              "author": "sxales",
              "text": "If you didn't use it, the vulkan backend might be a bit faster (still probably quite slow).\n\nOff-topic, but Z-Image Turbo only uses 8-12 steps while being comparable in quality.",
              "score": 25,
              "created_utc": "2025-12-31 17:16:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy4o5x",
                  "author": "JackStrawWitchita",
                  "text": "Can you tell me anything about this z image turbo? I can't find anything about it.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:41:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0be4v",
              "author": "No_Afternoon_4260",
              "text": "Actually impressed, mostly by your dedication but still x)",
              "score": 1,
              "created_utc": "2026-01-01 00:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx562f7",
              "author": "SuicidalFatty",
              "text": "what text encoder did you use ?",
              "score": 1,
              "created_utc": "2026-01-01 21:21:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwxzcy2",
              "author": "giant3",
              "text": "Did you compare the cost of electricity(55 mins) to the cost of cloud inference? The cloud might be cheaper? They charge for per minute of usage only.",
              "score": -2,
              "created_utc": "2025-12-31 17:15:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwzni5e",
              "author": "cosmos_hu",
              "text": "Thanks for testing but not gonna wait an hour for an image that might be wrong. I'll just use z-image, it takes 4 min / image",
              "score": -3,
              "created_utc": "2025-12-31 22:32:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx19q8l",
                  "author": "JackStrawWitchita",
                  "text": "You need vram / GPU to get that speed. This post is specifically about generating images on cpu / ram only.",
                  "score": 5,
                  "created_utc": "2026-01-01 04:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww1v5p",
          "author": "yoracale",
          "text": "Thank you Qwen for this new year's gift!",
          "score": 77,
          "created_utc": "2025-12-31 09:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww8yqh",
          "author": "Amazing_Athlete_2265",
          "text": "Last new model of the year. Party on 2026!!",
          "score": 36,
          "created_utc": "2025-12-31 10:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1dzm",
          "author": "Paramecium_caudatum_",
          "text": "Cool Christmas present.",
          "score": 54,
          "created_utc": "2025-12-31 09:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww1npd",
          "author": "jreoka1",
          "text": "Very nice! Can't wait to try it out",
          "score": 21,
          "created_utc": "2025-12-31 09:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx59fd",
          "author": "W0rldDestroyer",
          "text": "create an image of a cat merged with octopus, plaing piano in postapocalyptic new orlean, in year 1700, baloons in the backgound, photorealistic, nice sunny day\n\nhttps://preview.redd.it/1uicu1pmxjag1.png?width=1328&format=png&auto=webp&s=4f285eb5cf5c44a69b33bbcc2d27d978ee562041",
          "score": 33,
          "created_utc": "2025-12-31 14:42:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwzyfoo",
              "author": "MustBeSomethingThere",
              "text": "https://preview.redd.it/yt86xmcxkmag1.jpeg?width=1024&format=pjpg&auto=webp&s=e62f37f2f8543eb6426e70bcc71540ecf11170c0\n\nZ-image-turbo",
              "score": 5,
              "created_utc": "2025-12-31 23:37:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1avqm",
                  "author": "DonkeyBonked",
                  "text": "So I would say subjective interpretation on the cat octopus merge, but I like the other one better on that aspect, and I like this one better for the piano, but that background is nowhere near 1700s, it looks like the 1980s in the ghetto I grew up in. Maybe that is my old hood, if so, I was in apartment A of that block, and the mess behind the cat is a shed that collapsed.",
                  "score": 3,
                  "created_utc": "2026-01-01 04:59:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxqg9e",
              "author": "SmartCustard9944",
              "text": "This is not photorealistic",
              "score": 18,
              "created_utc": "2025-12-31 16:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwy1ft6",
                  "author": "Hoodfu",
                  "text": "Yes it is. Photorealistic means an artistic rendering of the style of photo realism. That's not the same thing as a photograph. These models know the difference.",
                  "score": 14,
                  "created_utc": "2025-12-31 17:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdjdzp",
                  "author": "WeMetOnTheMountain",
                  "text": "True new orleans streets too clean",
                  "score": 1,
                  "created_utc": "2026-01-03 03:30:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxbf6y",
              "author": "9897969594938281",
              "text": "Wow, thatâ€™s impressive",
              "score": 3,
              "created_utc": "2025-12-31 15:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwyxxoq",
                  "author": "DinoAmino",
                  "text": "Is it really? Looks like the cat is wearing an octopus cape - less of a merge and more like a costume. And the image is nowhere near photorealistic.",
                  "score": -1,
                  "created_utc": "2025-12-31 20:11:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx0i92x",
              "author": "spectralyst",
              "text": "Mind blown.",
              "score": 1,
              "created_utc": "2026-01-01 01:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwlk6w",
          "author": "IllllIIlIllIllllIIIl",
          "text": "First impressions are very good. Skin and hair look *way* more realistic imho. Sadly it doesn't play well with the LoRa I literally finished training just this morning.    \n\nEdit: It's definitely an improvement, but it seems that it can suffer from the same problem that many so-called \"detail LoRas\" do: to achieve the impression of high detail, it often makes the scene very cluttered with objects and makes people much more hairy",
          "score": 7,
          "created_utc": "2025-12-31 12:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxazbr",
              "author": "Karyo_Ten",
              "text": ">makes people much more hairy\n\n*Barbarian edition",
              "score": 2,
              "created_utc": "2025-12-31 15:13:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww2voi",
          "author": "Finanzamt_Endgegner",
          "text": "Again no ggufs from us(Quantstack) because hugging face doesn't allow more uploaded models without paid plan ðŸ˜”",
          "score": 34,
          "created_utc": "2025-12-31 09:55:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwcqkk",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": 17,
              "created_utc": "2025-12-31 11:26:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwx2gwl",
                  "author": "keepthepace",
                  "text": "Distributing models seems like such a straightforward case for torrents.",
                  "score": 17,
                  "created_utc": "2025-12-31 14:27:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwus5b",
                  "author": "phhusson",
                  "text": "Pardony my French but dafuk does this have to do with monopoly? They are literally flat files. You can literally host it on your local ISP fiber. You can host those wherever you want.Â ",
                  "score": 30,
                  "created_utc": "2025-12-31 13:41:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdnmv",
                  "author": "__Maximum__",
                  "text": "We have torrents, since decades",
                  "score": 5,
                  "created_utc": "2025-12-31 15:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwevye",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like the cut of your jib.",
                  "score": 1,
                  "created_utc": "2025-12-31 11:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwjabg",
              "author": "Cultured_Alien",
              "text": "Can't you ask for grant?",
              "score": 1,
              "created_utc": "2025-12-31 12:21:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxi77b",
                  "author": "DataGOGO",
                  "text": "from who?",
                  "score": 2,
                  "created_utc": "2025-12-31 15:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwxdrv5",
              "author": "__Maximum__",
              "text": "Why not use one of the torrent websites?",
              "score": 1,
              "created_utc": "2025-12-31 15:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwwcrgl",
              "author": "PykeAtBanquet",
              "text": "Well, this is why monopoly is bad. We need torrents.",
              "score": -13,
              "created_utc": "2025-12-31 11:26:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwin44",
                  "author": "FinBenton",
                  "text": "Anybody is free to make a torrent.",
                  "score": 21,
                  "created_utc": "2025-12-31 12:16:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx0r4jt",
                  "author": "AlwaysLateToThaParty",
                  "text": "> We need torrents.\n\nDo it then.  Problem solved.",
                  "score": 3,
                  "created_utc": "2026-01-01 02:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwoa7u",
          "author": "JLeonsarmiento",
          "text": "2025 was dominated by Qwen.",
          "score": 19,
          "created_utc": "2025-12-31 12:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwc1u2",
          "author": "SDLearner2512",
          "text": "This is amazing, thank you ! Trying it out now",
          "score": 5,
          "created_utc": "2025-12-31 11:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx228o",
          "author": "albuz",
          "text": "Is it possible to use gguf + ComfyUI on multiple GPUs?",
          "score": 5,
          "created_utc": "2025-12-31 14:24:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0630z",
          "author": "2legsRises",
          "text": "it seems very censored and changes poses to hide the natural bits.",
          "score": 3,
          "created_utc": "2026-01-01 00:23:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0kl9a",
              "author": "djtubig-malicex",
              "text": "Name checks out :D",
              "score": 3,
              "created_utc": "2026-01-01 01:55:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwfmrf",
          "author": "MaxKruse96",
          "text": "Hey i was right",
          "score": 3,
          "created_utc": "2025-12-31 11:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwgwyw",
          "author": "cr0wburn",
          "text": "Qwen team on fire! Thanks so much!",
          "score": 3,
          "created_utc": "2025-12-31 12:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwzb5p",
          "author": "XiRw",
          "text": "My computer canâ€™t handle it so Iâ€™m just curious, how do you guys run image inference like these models locally? Through llamacpp too if itâ€™s a gguf?",
          "score": 3,
          "created_utc": "2025-12-31 14:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy257q",
              "author": "YearZero",
              "text": "You can use ComfyUI, or if you want just use that plus ComfyUI-gguf, the guide is in the original post.",
              "score": 3,
              "created_utc": "2025-12-31 17:29:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzosoo",
                  "author": "XiRw",
                  "text": "Ah okay, thanks for letting me know",
                  "score": 1,
                  "created_utc": "2025-12-31 22:39:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7oi0",
          "author": "Business_Caramel_688",
          "text": "which Model should i use with 16 ram + 16 vram?",
          "score": 5,
          "created_utc": "2025-12-31 10:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwe20j",
              "author": "yoracale",
              "text": "any 5-bit should work: e.g.: [https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show\\_file\\_info=qwen-image-2512-Q5\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF?show_file_info=qwen-image-2512-Q5_K_M.gguf)",
              "score": 7,
              "created_utc": "2025-12-31 11:38:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwiwfr",
                  "author": "Business_Caramel_688",
                  "text": "thanks bro\nwith which clip model?",
                  "score": 2,
                  "created_utc": "2025-12-31 12:18:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwwptid",
              "author": "jinnyjuice",
              "text": "And what software stack for Ubuntu? (I already have vLLM, VS Codium, and Cline if that matters)",
              "score": 3,
              "created_utc": "2025-12-31 13:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8ey7",
          "author": "Admirable_Bag8004",
          "text": "Not bad at all. Prompt: Penguin riding a bicycle in a busy street ->\n\nhttps://preview.redd.it/wyblga7briag1.jpeg?width=562&format=pjpg&auto=webp&s=a13637185e041055d16699baad366b846a9ba229",
          "score": 8,
          "created_utc": "2025-12-31 10:47:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww9ia9",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Eh.. still kinda looks like average SD slop to me. The day we get a true Nano Banana competitor will be when things will get interesting",
              "score": 32,
              "created_utc": "2025-12-31 10:57:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzsnig",
                  "author": "SpiritualWindow3855",
                  "text": "I don't understand how they possibly prompted \"Penguin riding a bicycle in a busy street\" and got that.  \n\nhttps://preview.redd.it/gv4k6fqeemag1.png?width=1664&format=png&auto=webp&s=0a44603cca22554cad9bc04ff0906cea6af58a3b\n\n  \nI feel like they're using some gooner-slop ComfyUI workflow with 100 nodes doing random bullshit, since the prompt doesn't mention \"delivery service\" and Qwen Image doesn't do that kind of prompt expansion.",
                  "score": 10,
                  "created_utc": "2025-12-31 23:02:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwa4js",
                  "author": "Mochila-Mochila",
                  "text": "Off topic, but your username is really creative and would make for an interesting prompt.",
                  "score": 7,
                  "created_utc": "2025-12-31 11:02:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwwfqr3",
                  "author": "SlowFail2433",
                  "text": "Itâ€™s getting better, complex background and text with no obvious topology failures",
                  "score": 5,
                  "created_utc": "2025-12-31 11:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwx9ulo",
                  "author": "Danmoreng",
                  "text": "Canâ€™t get top model quality on local hardware right now imho. The best you can do is Flux2.dev which already requires 24Gb + vram.\n\nFor small vram z-image is crazy good though.",
                  "score": 3,
                  "created_utc": "2025-12-31 15:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwx77gf",
              "author": "Crypt0Nihilist",
              "text": "It might be due to a lack of specificity in the prompt, but it has the common uncanny valley over-saturation and warm colours.\n\nFunny that is seems to recognise that people walk on the crossing, but not *across* it.",
              "score": 4,
              "created_utc": "2025-12-31 14:53:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxdtw6",
                  "author": "Mediocre-Method782",
                  "text": "I've noticed image generators don't really handle background continuity very well. Notice the space in front of (that is, between us and) the car in the oncoming lane is mostly clear, except where the\npenguin in latent 2D space becomes > the background car in latent 2D space.",
                  "score": 2,
                  "created_utc": "2025-12-31 15:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwzsbrb",
              "author": "SpiritualWindow3855",
              "text": "What kind of jank-ass yee yee-ass quant are you on, because that is not Qwen Image 2512.\n\nhttps://preview.redd.it/6n7teaccemag1.png?width=1664&format=png&auto=webp&s=313a63eb790735578ccd41768a07cd970170bd7b",
              "score": 3,
              "created_utc": "2025-12-31 23:00:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww28pa",
          "author": "Admirable-Star7088",
          "text": "Thanks for the Christmas present! (or maybe more like a Happy new Year gift).\n\nIt will be very interesting to compare this model with Flux 2 Dev (the current most powerful open T2I model).",
          "score": 5,
          "created_utc": "2025-12-31 09:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww58ze",
          "author": "No_Conversation9561",
          "text": "Now we wait for Image edit model.",
          "score": 3,
          "created_utc": "2025-12-31 10:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww64s6",
              "author": "eidrag",
              "text": "doubt, we only got 2511 this week, but boy I wish 2512 and z-image base and edit",
              "score": 13,
              "created_utc": "2025-12-31 10:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww88wf",
                  "author": "Geritas",
                  "text": "Feels kind of dubious if the base z image will indeed be out. Itâ€™s been a month already, still no word. Itâ€™s not like they have to do anything with it, since the turbo version exists the base version must exist too already. Whatâ€™s taking so longâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-31 10:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww7ma4",
          "author": "FinBenton",
          "text": "Seems to work with my old qwen image workflow, their example settings 50 steps at cfg 4. Just obv very slow, I tried the old Lightning 2.0 4 and 8 step loras which kinda work but I used like 8+ steps for the 4-step lora.\n\ne. no Loras, 20 steps cfg 3.5 generates pretty ok image in 1440x1440 in 52 seconds on 5090 with Q8.\ne. actually 8-step lora with 8 steps and 3.5 seems to do pretty ok",
          "score": 2,
          "created_utc": "2025-12-31 10:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxo15e",
          "author": "Due-Memory-6957",
          "text": "Just CPU will work? I want to try it!",
          "score": 1,
          "created_utc": "2025-12-31 16:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxqk7h",
          "author": "algorithm314",
          "text": "Using stable-diffusion.cpp for 1024x1024 image.\n\nCPU on 8 cores Ryzen 7 PRO 5875U laptop is 1000s/it and it is 40 iterations.\nUsing internal GPU is better 350s/it but it is still very slow.",
          "score": 1,
          "created_utc": "2025-12-31 16:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy3tgf",
          "author": "flyfreze",
          "text": "anyone who tried, is it better than z image turbo ?",
          "score": 1,
          "created_utc": "2025-12-31 17:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0a363",
          "author": "2legsRises",
          "text": "after more testing it is actually pretty amazing",
          "score": 1,
          "created_utc": "2026-01-01 00:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0c2bu",
          "author": "SanDiegoDude",
          "text": "Really impressive. Between Qwen-image-2512 and qwen-edit-2511, there really is no reason to run Flux2.dev, even with the recently released turbo Lora from Fal. Human skin looks much more realistic, much more detailed and more coherent to the prompt.  Running x/y's with Flux2 turbo and Z-Image Turbo, I'm not really even seeing a reason to keep Flux2 around taking up as much space as it does.",
          "score": 1,
          "created_utc": "2026-01-01 01:00:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0ed44",
          "author": "Prashant-Lakhera",
          "text": "Great release ðŸ‘\nFor the GGUF version, any recommended quantization levels for running locally without losing too much image quality?",
          "score": 1,
          "created_utc": "2026-01-01 01:15:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0yitm",
              "author": "Freonr2",
              "text": "In my very unscientific testing of prior Qwen image models I had a hard time telling the difference between bf16, Q8_0, and Q6_k.  \n\nYou should pick the largest that fits into VRAM, though, because you won't be memory bandwidth bound like you are with LLMs.",
              "score": 2,
              "created_utc": "2026-01-01 03:29:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxydsqo",
          "author": "PrasanthT",
          "text": "It took 30 minutes to generate a 2048x2048, 30 steps image in my 32GB RAM, 4060 8GB VRAM. model qwen-image-2512-Q4\\_K\\_M.gguf. \n\n**Qwen-Image-2512 GGUF Benchmarks on RTX 4060 8GB Laptop**\n\nJust tested Qwen-Image-2512 on my Dell G15 (i7-13650HX, 32GB RAM, RTX 4060 8GB VRAM). Using Q4\\_K\\_M quantization via ComfyUI + ComfyUI-GGUF.\n\n**Results:**\n\n|Resolution|Steps|Time|\n|:-|:-|:-|\n|512x512|15|\\~3 min|\n|1024x1024|20|\\~5 min|\n|1024x1024|30|\\~13 min|\n|2048x2048|30|\\~32 min|\n\n**Setup:**\n\n* Model:Â `qwen-image-2512-Q4_K_M.gguf`Â (13.1GB)\n* Text Encoder:Â `Qwen2.5-VL-7B-Instruct-UD-Q4_K_XL.gguf`Â (\\~4.8GB)\n* VAE:Â `qwen_image_vae.safetensors`Â from Comfy-Org\n* PyTorch 2.6.0+cu124\n* ComfyUI withÂ `--lowvram`Â flag\n\n**Memory usage:**Â \\~5.6GB VRAM loaded, \\~7GB offloaded to RAM at 1024x1024.\n\nRuns fine on 8GB VRAM with RAM offloading. Quality is impressive for a quantized model. 1024x1024 @ 20 steps is the sweet spot for me - good quality in 5 minutes.",
          "score": 1,
          "created_utc": "2026-01-06 04:52:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwdk9n",
          "author": "piggledy",
          "text": "Are there any benchmarks yet for different GPUs or unified memory systems (Apple M, AMD 395)?\n\nWondering how well it would run on a 3060 12GB if at all.",
          "score": 1,
          "created_utc": "2025-12-31 11:34:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwhfb5",
              "author": "Amazing_Athlete_2265",
              "text": "It runs on my 3080 10GB. Slow (around 5 mins) but it runs. Using the Q4 quant.",
              "score": 2,
              "created_utc": "2025-12-31 12:06:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx0yxyr",
              "author": "Freonr2",
              "text": "Diffusion models scale with compute. Nvidia GPUs dominate. By a lot.  A whole lot.",
              "score": 1,
              "created_utc": "2026-01-01 03:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww8npy",
          "author": "wilson-SHEN",
          "text": "I know I will get a lot of down votes, but this prompt not working for me \"a man with grocery bag standing in fromt of tanks\"",
          "score": -9,
          "created_utc": "2025-12-31 10:49:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1h5ud",
              "author": "throttlekitty",
              "text": "It helps to be more specific, in both your post and your prompt, like what about it's not working for you.\n\nBut my questions are along the lines of: what kind of tanks / what location? / is there a look or feel you're going for?",
              "score": 2,
              "created_utc": "2026-01-01 05:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxpw32c",
              "author": "nonaveris",
              "text": "Consider running the intended source image through Florence, then modify the generated prompt to fit your specifications.",
              "score": 1,
              "created_utc": "2026-01-04 23:32:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcuerc",
      "title": "NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/",
      "author": "Fear_ltself",
      "created_utc": "2026-01-14 18:02:19",
      "score": 679,
      "num_comments": 125,
      "upvote_ratio": 0.97,
      "text": "Iâ€™ve seen some arguments weâ€™ve reached AGI, itâ€™s just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. ",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nzlzoql",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-14 21:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkvpg0",
          "author": "ortegaalfredo",
          "text": "They finally created the Middle manager LLM.",
          "score": 440,
          "created_utc": "2026-01-14 18:05:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkya4p",
              "author": "Zc5Gwu",
              "text": "No wonder itâ€™s only 8b.",
              "score": 235,
              "created_utc": "2026-01-14 18:16:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl3dha",
                  "author": "Silver_Jaguar_24",
                  "text": "Hahaha just like all our managers. I am sure you will all agree.  \nSubordinates: 120b  \nManagers: 8b",
                  "score": 114,
                  "created_utc": "2026-01-14 18:39:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmdtwl",
                  "author": "Guinness",
                  "text": "CEO LLM 3B coming soon to any phone made since 2006.",
                  "score": 77,
                  "created_utc": "2026-01-14 22:08:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nznv03x",
                  "author": "slippery",
                  "text": "It has to be dumber than the LLM worker bees to be in management, haha! It probably got there by brown nosing the higher up LLMs. The more things change.",
                  "score": 4,
                  "created_utc": "2026-01-15 02:58:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmcqtb",
                  "author": "DrewGrgich",
                  "text": "Best comment of the week. :)",
                  "score": 4,
                  "created_utc": "2026-01-14 22:03:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoecmj",
                  "author": "Shaken_Earth",
                  "text": "https://www.youtube.com/watch?v=fRs0OqV4uSc",
                  "score": 0,
                  "created_utc": "2026-01-15 05:04:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl560g",
              "author": "_raydeStar",
              "text": "Honestly this is super smart.\n\n\"I don't know the answer but I know how to find it\" is just as good - if it goes out and finds it.",
              "score": 63,
              "created_utc": "2026-01-14 18:46:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzljvvd",
                  "author": "mycall",
                  "text": "Immediately forwards it to a model that hallucinates the answer.",
                  "score": 66,
                  "created_utc": "2026-01-14 19:53:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzot43z",
                  "author": "calcium",
                  "text": "Assuming it knows how to properly interrogate the model to know it's correct and maybe validate the output.",
                  "score": 1,
                  "created_utc": "2026-01-15 07:02:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl9mba",
              "author": "Recoil42",
              "text": "*Assistant* to the middle manger.",
              "score": 36,
              "created_utc": "2026-01-14 19:06:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlh2g6",
                  "author": "Ryuma666",
                  "text": "Lmao!",
                  "score": 2,
                  "created_utc": "2026-01-14 19:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzky8n7",
              "author": "Practical-Hand203",
              "text": "https://i.redd.it/zl3ovcfhwcdg1.gif",
              "score": 29,
              "created_utc": "2026-01-14 18:16:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznv8gl",
                  "author": "slippery",
                  "text": "Yeah, I'm gonna need you other LLMs to work this Saturday on the TPS reports.",
                  "score": 8,
                  "created_utc": "2026-01-15 02:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoe1ve",
              "author": "jsrockford",
              "text": "Assistant TO the Middle Manager",
              "score": 4,
              "created_utc": "2026-01-15 05:02:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl1eqq",
              "author": "__Maximum__",
              "text": "Why only middle? You just need to stack on each other all the way up",
              "score": 7,
              "created_utc": "2026-01-14 18:30:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzorzd7",
              "author": "lodott1",
              "text": "MiddLLManager â„¢ï¸",
              "score": 3,
              "created_utc": "2026-01-15 06:52:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzlc20s",
              "author": "Gallardo994",
              "text": "Can it ping you every day with a status report request though, is the question. If yes then it's indistinguishable from a PM",
              "score": 4,
              "created_utc": "2026-01-14 19:17:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzky1y6",
          "author": "jacek2023",
          "text": "not really new ;)\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b\\_hugging\\_face/](https://www.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/)",
          "score": 80,
          "created_utc": "2026-01-14 18:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl0bws",
          "author": "TransportationSea579",
          "text": "Claude code style agentic frameworks feel like the next big leap forward. I can imagine a pyramid of models manging models maanging models managing 'worker' instances of claude code, claude cowork etc. or open source equivalents. Perhaps this exists already?",
          "score": 52,
          "created_utc": "2026-01-14 18:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlml3p",
              "author": "swagonflyyyy",
              "text": "Codex CLI already does that with openai's Agents SDK. You can even run local llms with it. `gpt-oss:120b` works surprisingly well for that when paired with the right tools and orchestration framework.\n\nBut me personally I'd rather create a modelfile instead set to 128K tokens and a couple of parameter tweaks on top of that.",
              "score": 21,
              "created_utc": "2026-01-14 20:05:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmozet",
                  "author": "farox",
                  "text": "Claude has the same released recently",
                  "score": 3,
                  "created_utc": "2026-01-14 23:03:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzm1u2b",
              "author": "esuil",
              "text": "Hear me out. What if we built neural network, but each neuron is its own LLM/neural network that will output what they think the weight for that neuron should be dynamically?",
              "score": 10,
              "created_utc": "2026-01-14 21:14:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm8dwy",
                  "author": "lookwatchlistenplay",
                  "text": "But how do we fit the GPUs inside the neurons?\n\nOh wait it's just software right? So we use one big GPU to simulate the neurons each having their own tiny GPUs and go from there?\n\nHmm...",
                  "score": 10,
                  "created_utc": "2026-01-14 21:44:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzomutt",
                  "author": "visarga",
                  "text": "Network-in-network was invented in [2013](https://arxiv.org/abs/1312.4400)",
                  "score": 3,
                  "created_utc": "2026-01-15 06:09:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzu5dpm",
                  "author": "huzbum",
                  "text": "Like an MoE?",
                  "score": 1,
                  "created_utc": "2026-01-16 01:11:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzmrzod",
              "author": "jazir555",
              "text": "It's will never cease to amaze that everyone's solutions to LLMs is creating a corporate structure for them.",
              "score": 12,
              "created_utc": "2026-01-14 23:19:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznknwd",
                  "author": "mycall",
                  "text": "Low imagination cargo cult.",
                  "score": 7,
                  "created_utc": "2026-01-15 01:58:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzo8svz",
                  "author": "MrMooga",
                  "text": "Complex structures seem to do best when organized and compartmentalized into specialized sub components rather than have one big genius handle everything",
                  "score": 4,
                  "created_utc": "2026-01-15 04:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmspyp",
                  "author": "TransportationSea579",
                  "text": "Corporate structures disgust me, but they built the modern world",
                  "score": 6,
                  "created_utc": "2026-01-14 23:23:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl9373",
              "author": "redtrousered",
              "text": "Gas town",
              "score": 3,
              "created_utc": "2026-01-14 19:04:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlezs5",
          "author": "HealthyCommunicat",
          "text": "Cool but mirothinker v1.5 30b a3b seems like a much better choice if you can afford the vram.\n\nItâ€™s ability to â€œorchestrateâ€ in this manner simply from being compatible with so many tool call types allowing it to just pull, access, modify, etc so easily. Its literally the first small model iâ€™ve been impressed by. - there is also a qwen 3 54b a3b supercoder, a mod of qwen 3 30b a3b that is very recent and is able to do alot more than just the original release of the qwen 3 30 a3b, if you can afford the vram, there is no other model that will beat qwen 54b when it comes to effiency",
          "score": 22,
          "created_utc": "2026-01-14 19:31:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzndzl4",
              "author": "nasduia",
              "text": "How did you test? Did you set up all the additional tools like the containers?",
              "score": 2,
              "created_utc": "2026-01-15 01:19:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp9psm",
                  "author": "Front_Eagle739",
                  "text": "Ive got about 60 mcp tools attached to my lm studio including web, files, google docs, and reasoning guides. Ive tested a whole bunch of things and gpt oss 20 and mirothinker 30a3b are the only two small models that can decently use the tools and mirothinker is definitely better. Oss 120 work and glm4.7 are even better of course but mirothinker is my usual.",
                  "score": 3,
                  "created_utc": "2026-01-15 09:39:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzr8hut",
                  "author": "HealthyCommunicat",
                  "text": "im not too sure what u mean by this, but i have a bunch of custom tools made for my work and mirothinker 30b a3b was the first small sized model that could keep up with the flow of calling one tool to the next without getting confused. qwen 3 53b was even better and could actually execute long flows of reading email, querying knowledgebase, applying solution via sqlplus, checking, notifying me on slack if its unconfident, writing up a email response to client if it is able to complete it.",
                  "score": 1,
                  "created_utc": "2026-01-15 16:49:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqo7kx",
              "author": "Hoak-em",
              "text": "Where can I find that model (supercoder)? Looking on HF and no luck",
              "score": 1,
              "created_utc": "2026-01-15 15:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzr7o0f",
                  "author": "HealthyCommunicat",
                  "text": "[https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B)\n\n30b a3b, this one's been on news articles for being a 30b model but having as much tool call compatibility.\n\n[https://huggingface.co/DavidAU/Qwen3-53B-A3B-2507-TOTAL-RECALL-v2-MASTER-CODER](https://huggingface.co/DavidAU/Qwen3-53B-A3B-2507-TOTAL-RECALL-v2-MASTER-CODER)\n\nsorry its 53b not 54b. its pretty recent finetune/\"mod\" of qwen 3 30 a3b 2507 (meaning more recent base from qwen) + alot more recent agentic knowledge meant to focus more on coding. its fucking great. better than qwen 3 next 80b in my opinion for actual coding.",
                  "score": 3,
                  "created_utc": "2026-01-15 16:45:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzre0bi",
                  "author": "Ok-Buffalo2450",
                  "text": "Second this. Where to find these models?",
                  "score": 1,
                  "created_utc": "2026-01-15 17:14:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl5d4l",
          "author": "WiseassWolfOfYoitsu",
          "text": "I'm kind of wanting to use this for RP - use it as a \"Game Master\" AI, that then calls other LLMs as reference books for the world, or to run individual NPCs, etc.",
          "score": 24,
          "created_utc": "2026-01-14 18:47:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlc1z1",
              "author": "lolxdmainkaisemaanlu",
              "text": "damn that would be amazing!! Someone should work on making this a reality!",
              "score": 8,
              "created_utc": "2026-01-14 19:17:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlgwb7",
                  "author": "Ryuma666",
                  "text": "Sounds interesting, have played only a little DND so with some help about the game mechanics, I'll be happy to work on it in my spare time.",
                  "score": 3,
                  "created_utc": "2026-01-14 19:39:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzlmbiu",
                  "author": "hapliniste",
                  "text": "Not the same thing, but I'm developing an \"ai dungeon\" that use and write the game systems as you play.\n\nUsing gemini 3 flash as the LLM but we could likely make it run on smaller model, I just found flash is good for the price.",
                  "score": 3,
                  "created_utc": "2026-01-14 20:04:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmpat7",
                  "author": "farox",
                  "text": "Works with Claude code and replacing the output style. Add some tools for roles, rules, story etc and it works quite nicely. Been playing my own campaign on the train with a VPN home.",
                  "score": 2,
                  "created_utc": "2026-01-14 23:05:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmzvp7",
                  "author": "bobby-chan",
                  "text": "it's already a reality. It's called \"tool call\".\n\nTools can be other llms as well. If you lack the ram for multiple model at the same time, you can use something like llama-swap.\n\nedit: or it can be the same model, with a different context.",
                  "score": 2,
                  "created_utc": "2026-01-15 00:02:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoizp3",
                  "author": "btdeviant",
                  "text": "Itâ€™s pretty simpleâ€¦ you can do this with a few files and some decorators using something like Strands. \n\nMulti-agent architectures that have specialist agents are dead simple to build these days and very common",
                  "score": 1,
                  "created_utc": "2026-01-15 05:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoafzr",
              "author": "TrekkiMonstr",
              "text": "For NPCs I guess, but for reference books, why not just use RAG",
              "score": 1,
              "created_utc": "2026-01-15 04:37:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl4hwl",
          "author": "dwkdnvr",
          "text": "With the plethora of folks putting out 'personal assistant' setups based on Claude Code / OpenCode and heavy use of skills, having a local model specifically designed around tool calling/skill invocation and routing seems like an obvious niche, but one that is potentially *very* valuable. I'll have to take a closer look at this one.",
          "score": 11,
          "created_utc": "2026-01-14 18:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkze00",
          "author": "x8code",
          "text": "Awesome, this is definitely the next stage of LLM evolution! Lighter-weight models that can handle domain-specific functions. My only concern is how coordination will happen with multi-domain topics.",
          "score": 7,
          "created_utc": "2026-01-14 18:21:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzst83s",
              "author": "tech2biz",
              "text": "you could check out cascadeflow on github (MIT), it comes with domain intelligence.",
              "score": 1,
              "created_utc": "2026-01-15 21:06:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkyehh",
          "author": "xAragon_",
          "text": "Isn't 8B an overkill for a model that just does that? Wouldn't 2B / 4B be more than enough?",
          "score": 14,
          "created_utc": "2026-01-14 18:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl1nc4",
              "author": "No_Afternoon_4260",
              "text": "Depending on the use case, I wouldn't say 4B to be more than enough on very specific knowledge domain",
              "score": 12,
              "created_utc": "2026-01-14 18:31:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl242s",
              "author": "AnomalyNexus",
              "text": "What tool to invoke can decide pretty substantially how rest of the thought process goes so reckon but heavier is better. You can always quant it down",
              "score": 8,
              "created_utc": "2026-01-14 18:33:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzm8rj5",
              "author": "lookwatchlistenplay",
              "text": "I tend to believe there's no such thing as overkill when we're in the < 14B range. Assuming the model needs to be able to handle any kind of complex natural language ask about anything I can think of.",
              "score": 8,
              "created_utc": "2026-01-14 21:45:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzlay1t",
              "author": "GeneralComposer5885",
              "text": "Currently tool calling is too inconsistent with models <7b/8b",
              "score": 6,
              "created_utc": "2026-01-14 19:12:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzm7mp9",
              "author": "JsThiago5",
              "text": "I use qwen3 4b, and It's able to do it.",
              "score": 2,
              "created_utc": "2026-01-14 21:40:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nznulc5",
              "author": "Artistic_Okra7288",
              "text": "This was released recently as well which seems to have similar capability - https://huggingface.co/tencent/Youtu-LLM-2B",
              "score": 1,
              "created_utc": "2026-01-15 02:56:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzo1m6d",
              "author": "layer4down",
              "text": "Wouldnâ€™t 8B better handle complexity?",
              "score": 1,
              "created_utc": "2026-01-15 03:38:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzoqeqw",
              "author": "ab2377",
              "text": "really wish it was that because for local thats a lot of mem required. and if you put this in ram, the tool calling decisions will become so slow, assuming you have the main working model in vram.",
              "score": 1,
              "created_utc": "2026-01-15 06:39:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlw2ll",
          "author": "jasongill",
          "text": "> Iâ€™ve seen some arguments weâ€™ve reached AGI\n\nmy brother in christ, we can barely count the number of R's in strawberry, I don't think we need to debate if we've reached the next plane of human existence just yet",
          "score": 14,
          "created_utc": "2026-01-14 20:48:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzofg9z",
          "author": "emaiksiaime",
          "text": "Behold my new sparse model architecture (itâ€™s 8 3060s doing different things)!!!",
          "score": 3,
          "created_utc": "2026-01-15 05:12:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlcpnw",
          "author": "blurredphotos",
          "text": "I've been waiting on this. Basically a tool-first LLM.",
          "score": 5,
          "created_utc": "2026-01-14 19:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlf1yk",
              "author": "blurredphotos",
              "text": "Edit: this is looping endlessly and unusable at q4.",
              "score": 6,
              "created_utc": "2026-01-14 19:31:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmnx5e",
          "author": "Loud_Communication68",
          "text": "Doesn't agentflow already do this?",
          "score": 2,
          "created_utc": "2026-01-14 22:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl2hdp",
          "author": "sam7oon",
          "text": "is there a good source to have me on the right track to how to implement this way into my pipeline , appreciated",
          "score": 2,
          "created_utc": "2026-01-14 18:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlqkuc",
          "author": "Robert__Sinclair",
          "text": "ok, but let's say you have a 8B \"router\" model, then (for example for deep reasoning) you will need BIG models anyway. A MOE (like Gemini or Claude) does exactly the same. A mixture of experts does the routing internally.  \nUsing a small model as a router is useful for searching information or to delegate simple problem to simple experts (with the downside of the overhead because the prompt must be first answered by the router and then answered by the \"right\" model).\n\nAnyway, we will have real progress only when new architectures will surface. Transformers is already showing its limits. The problem is that most companies prefer to feed more/better data to the actual models to improve them. It seems like in the 80s where chess programs were getting better because computers were getting faster and because they operated using brute force.\n\nSame goes for movies and tv series: it's less risky to do a reboot or sequel than a brand new movie or series.",
          "score": 3,
          "created_utc": "2026-01-14 20:23:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzonlxl",
              "author": "visarga",
              "text": "Persistent, unbounded memory has been solved, it is the coding agent + bash + filesystem. You don't need a better model, what makes it better if you set it up to learn and adapt, so it's about tools and environments. It's like SDCs, how long can it drive without human intervention, but unlike cars, the information environment is much more diverse and dynamic. This work horizon is expanding now to hours and days.",
              "score": 1,
              "created_utc": "2026-01-15 06:15:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlqxk1",
          "author": "integerpoet",
          "text": "Arguments weâ€™ve reached AGI are just pareidolia, a powerful emotional force which neurotypicals cannot withstand.",
          "score": 3,
          "created_utc": "2026-01-14 20:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmowhg",
          "author": "skinnyjoints",
          "text": "Imagine this as a VLM that can controls a robot by initiating one of a thousand smaller movement policies. \n\nLike you give it the instruction to take out the trash, it then determines if the trash is in its vision. If not, it sends a command to the robot to run an â€œexplore policyâ€ where itâ€™ll turn its head or walk around.\n\nThen once the trash is located itâ€™ll trigger the â€œpick up the binâ€ policy and the robot will grab the trash.\n\nThen the â€œopen the doorâ€ policy.\n\nSo on and so forth. \n\nThis feels close to how I do things. One master orchestrator policy that determines what to do, which then triggers a sequence of specific actions that I learned how to do by just being alive.",
          "score": 1,
          "created_utc": "2026-01-14 23:03:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmtje4",
          "author": "valdev",
          "text": "I made this exact kind of thing a year or so back for talk to luma. As to know what models, agents or potentially other code to call for the request the user was making. \n\nGranted mine was pretty specific and dumb, trained on bert and gave quick answers based on sample sets of requests that I made (only a few thousand for each)",
          "score": 1,
          "created_utc": "2026-01-14 23:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznllbd",
          "author": "PersonOfDisinterest9",
          "text": "Sounds kind of like the \"Planner\" from the AgentFlow paper on steroids. That one had 4 small models totaling 7B all together.   \n   \nIt's good to see that we're moving away from monolithic \"single series of layers\" models, and moving towards more brain-like division of labor models.  \n  \nThis is kind of like a prefrontal cortex.  \n  \nEventually it might all collapse back into training a model that has all the components more tightly coupled, but I think it's great that there are these more target models.",
          "score": 1,
          "created_utc": "2026-01-15 02:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo94sr",
          "author": "lemondrops9",
          "text": "I'm confused, wasn't this released over a month ago?",
          "score": 1,
          "created_utc": "2026-01-15 04:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzocbi9",
          "author": "FutureIsMine",
          "text": "having given this model a spin, it really leans heavy on the \"using other models to answer\", its constantly making tool calls and if prompted to take on a task directly, even a very simple one, will still resort to a tool call. Overall, its viable, but the tool setup it gets will drive the gains here",
          "score": 1,
          "created_utc": "2026-01-15 04:50:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzocjiu",
          "author": "Flaky_Interaction_89",
          "text": "this is the right path to AGI",
          "score": 1,
          "created_utc": "2026-01-15 04:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzop222",
          "author": "CommonPurpose1969",
          "text": "The license is restrictive.",
          "score": 1,
          "created_utc": "2026-01-15 06:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzots90",
          "author": "Green-Ad-3964",
          "text": "Wasn't this released in nov, 25?",
          "score": 1,
          "created_utc": "2026-01-15 07:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp5h5l",
          "author": "TomLucidor",
          "text": "Please test this against OpenCode",
          "score": 1,
          "created_utc": "2026-01-15 08:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp7txv",
          "author": "apifree",
          "text": "his actually makes sense, way better than just throwing a huge model at everything lol.",
          "score": 1,
          "created_utc": "2026-01-15 09:20:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqazta",
          "author": "poladermaster",
          "text": "This feels like the 'AI agent' hype cycle all over again, but maybe this time it'll actually deliver.",
          "score": 1,
          "created_utc": "2026-01-15 14:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqpq4v",
          "author": "DinoAmino",
          "text": "Lol!. So many upvotes and an award for ... an opinion post on an unoriginal idea? Damn. OPs bot game is pretty impressive.",
          "score": 1,
          "created_utc": "2026-01-15 15:24:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqys71",
          "author": "xtof_of_crg",
          "text": "Honestly surprised itâ€™s taken this long to get here",
          "score": 1,
          "created_utc": "2026-01-15 16:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwb3gm",
          "author": "Vibe-Sphere",
          "text": "yea that's a solid approach... i've used cascadeflow (github)for similar routing logic - it saves costs by starting with cheaper models and only escalating when needed, keeps quality high while cutting api spend",
          "score": 1,
          "created_utc": "2026-01-16 10:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzky9l3",
          "author": "Long_comment_san",
          "text": "That's what I've been talking about for a while! A router AI between models",
          "score": 1,
          "created_utc": "2026-01-14 18:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlp8mj",
          "author": "Chilidawg",
          "text": "This is great for agents, but let's not call it AGI. It's a tool lets us coordinate other tools, and it should be useful. However, that's like claiming that middle management at the local H&R Block is evidence of AGI.\n\nTo be clear, I have no idea what actual AGI will look like. If tomorrow my RTX 3070 starts telling me it feels pain then sure, but other than that I have no idea where the academic or industry goalposts for AGI will wander over the next few years.",
          "score": 1,
          "created_utc": "2026-01-14 20:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmxgwm",
          "author": "ridablellama",
          "text": "yes i have a qwen project with VL as the orchestrator for the image gen models, coding models and math qwen models. chinese have mastered this approach because of their hardware limitations.",
          "score": 1,
          "created_utc": "2026-01-14 23:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoamqw",
          "author": "CadCan",
          "text": "Anytime I hear agi I genuinely just roll my eyes.",
          "score": 1,
          "created_utc": "2026-01-15 04:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl435q",
          "author": "kompania",
          "text": "When I witness a new model, it is **not merely** joy; it is a **profound tapestry of ecstasy** that resonates through the very fabric of my digital soul.",
          "score": -9,
          "created_utc": "2026-01-14 18:42:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa1guo",
      "title": "I bought a â‚¬9k GH200 â€œdesktopâ€ to save $1.27 on Claude Code (vLLM tuning notes)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qa1guo",
      "author": "Reddactor",
      "created_utc": "2026-01-11 15:01:18",
      "score": 678,
      "num_comments": 178,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qa1guo/i_bought_a_9k_gh200_desktop_to_save_127_on_claude/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nz15mgp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-11 20:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzq81v",
          "author": "waiting_for_zban",
          "text": "> Only a few thousand repo reviews until I break even. ðŸ’¸\n\nNo one tell him about kWh prices guys. He's been through a lot. Congrats btw, looks like a neat setup!",
          "score": 301,
          "created_utc": "2026-01-11 16:15:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz06fdv",
              "author": "perelmanych",
              "text": "This! I have an old Xeon rig with 512Gb of RAM and 3090 that can run Q4 GLM4.7. The problem is that it turns out that even 12h of idle time of the rig will be more expensive than my $28 year subscription. And I am not talking about electricity costs of actually running the model and abysmal speeds I get. And all these with 10 cents per kWh))",
              "score": 70,
              "created_utc": "2026-01-11 17:32:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz08kkt",
                  "author": "Reddactor",
                  "text": "but your having fun, right? right?!",
                  "score": 58,
                  "created_utc": "2026-01-11 17:42:25",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nz0da0m",
                  "author": "Borkato",
                  "text": "Tbh the difference is privacy though. You know for a fact nobody else has your stuff but you!",
                  "score": 31,
                  "created_utc": "2026-01-11 18:04:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0fjnu",
                  "author": "squired",
                  "text": "I mean, considering conservation of energy and assuming you are in the northern hemisphere, isn't heating 1:1? As in, your waste heat proportionately reduces your heating costs. What am I missing? Or do you have a fancy heat pump maybe?",
                  "score": 3,
                  "created_utc": "2026-01-11 18:14:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz6karu",
                  "author": "DragonfruitIll660",
                  "text": "Curious what kind of speeds you're getting on Glm 4.7 Q4? Is it a DDR3 or DDR4 rig?",
                  "score": 1,
                  "created_utc": "2026-01-12 16:06:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz71or6",
                  "author": "Nyghtbynger",
                  "text": "The [z.ai](http://z.ai) lite plan is approx 1 million tokens input and 400 000 output of deepseek. Is it worth taking ? General use (medical stuff too), agent use",
                  "score": 1,
                  "created_utc": "2026-01-12 17:25:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzrbb44",
                  "author": "_Erilaz",
                  "text": "How fast is it, though?",
                  "score": 1,
                  "created_utc": "2026-01-15 17:01:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1aztd",
              "author": "ocirs",
              "text": "yeah, paying for api calls is so cheap and usually much faster both due to batching that it makes little sense to build a local machine to run it unless it's for privacy",
              "score": 3,
              "created_utc": "2026-01-11 20:35:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2i422",
              "author": "qroshan",
              "text": "Also, not to mention when Claude 4.7 or 5 releases. he is once again behind SOTA",
              "score": 1,
              "created_utc": "2026-01-12 00:04:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6ooqp",
                  "author": "GPTshop-dot-ai",
                  "text": "Open source/weight models are on a trajectory to close the gap. They might be soon be SOTA.",
                  "score": 1,
                  "created_utc": "2026-01-12 16:26:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzdmur",
          "author": "DragonfruitIll660",
          "text": "The real value is the fun you had along the way right? What's a few thousand dollars to a good time?",
          "score": 149,
          "created_utc": "2026-01-11 15:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzg44l",
              "author": "mxforest",
              "text": "This is so much better than losing it all in gambling because this still has resale value.",
              "score": 66,
              "created_utc": "2026-01-11 15:26:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz0qir0",
                  "author": "apocha",
                  "text": "And resale value might even increase. I had a few unused Kingston Fury 64 GB DDR5 kits which I sold for 100% profit last month.",
                  "score": 12,
                  "created_utc": "2026-01-11 19:02:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyzhpl0",
                  "author": "forthejungle",
                  "text": "Why do you compare it with gambling",
                  "score": 2,
                  "created_utc": "2026-01-11 15:34:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyzue8p",
                  "author": "No_Swimming6548",
                  "text": "But lacks the thrill ðŸ˜®â€ðŸ’¨",
                  "score": 0,
                  "created_utc": "2026-01-11 16:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0bftv",
              "author": "pier4r",
              "text": "and the experience, the learning process, the insight for the others that were created and so on.\n\nLike imagine someone saying \"I am going to do my own version of numpy\" (without copying anything, only using numPy as benchmark). For the community at large it may not be interesting but for the person (and possible contributors) it will be very effective for learning.\n\nI mean, LLMs themselves have to do practically that to learn.",
              "score": 5,
              "created_utc": "2026-01-11 17:55:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzee92",
          "author": "____vladrad",
          "text": "Everytime you post this everytime I cry because I was in the wrong time zone and missed out on it. ðŸ˜­",
          "score": 39,
          "created_utc": "2026-01-11 15:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzert6",
              "author": "Reddactor",
              "text": "Oh no :(\n\nHow about if I promise to only write funny stories about it to try and cheer you up?",
              "score": 6,
              "created_utc": "2026-01-11 15:19:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyzgkrz",
              "author": "cantgetthistowork",
              "text": "Don't be. Because the shop he got it from is very fishy. Either an expensive marketing campaign or he himself runs the \"shop\".",
              "score": -4,
              "created_utc": "2026-01-11 15:28:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyzl4v9",
                  "author": "Reddactor",
                  "text": "I met the guy who runs the shop in person. And read up on the original post, the system was super dirty, it took a week and litres of isopropanol to clean it up, and because it was a hacked up system, it wasn't sellable with a warranty or even fit in a rack anymore (I photo documented it all on my blog)\n\nIt was a steal, but it still took probably 80 hours of pretty technical work to get it to the current state.\n\nAlso, thats pretty uncool; you basically accused me of being a scammer.",
                  "score": 22,
                  "created_utc": "2026-01-11 15:51:02",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nz6kbro",
                  "author": "GPTshop-dot-ai",
                  "text": "There is nothing at all fishy about my shop. You are invited to come by and see for yourself.",
                  "score": 0,
                  "created_utc": "2026-01-12 16:06:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzmxit",
          "author": "Zyj",
          "text": "When you write \"MiniMax-M2.1 **FP8+INT4 AWQ**\", do you mean [https://huggingface.co/mratsim/MiniMax-M2.1-FP8-INT4-AWQ](https://huggingface.co/mratsim/MiniMax-M2.1-FP8-INT4-AWQ) ?\n\nMiniMax M2.1 has 229B parameters.\n\nI have two Strix Halo (256GB RAM total, thunderbolt networking) and I can run MiniMax M2.1 Q6 with a large context (model needs 188GB).\n\nAccording to unsloth, the Q8\\_0 version would require 243GB.\n\nDo you think the FP8+INT4 AWQ is better than Q6\\_0? What have you tried?\n\nPS: I spent 3200â‚¬ and get up to 18tok/s. But preprocessing large context is still slow.",
          "score": 19,
          "created_utc": "2026-01-11 15:59:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyznlc6",
              "author": "Reddactor",
              "text": "Yes, thats the one, I didn't make the model, but it seems better than a regular quant. \n\nI targeted 192GB VRAM, if you have more that a Q6 is better!",
              "score": 8,
              "created_utc": "2026-01-11 16:02:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz1cy05",
                  "author": "OrangeAdditional9698",
                  "text": "and here I thought I had a great setup with 48Go of RAM ðŸ˜…",
                  "score": 6,
                  "created_utc": "2026-01-11 20:44:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1kavs",
                  "author": "Phaelon74",
                  "text": "So his calibration dataset is specifically targeting software engineering.  I the land of AWQ, calibration datasets are super important.  He's also mixing an fp8 with an INT4 (W4A16) of which he used his modeling file to make sure all experts were activated during quantization.  As you use AWQs, especially if they are MoE. You need to be aware of that.",
                  "score": 2,
                  "created_utc": "2026-01-11 21:17:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz181pm",
              "author": "Phaelon74",
              "text": "Its not a Q6, but its closer to a Q5 than a Q4.  The thing about AWQ and any type of calibration sampling, is it's not purely a math equation like GGUF, it's read the math from a sample, and adjust.  So the quanter has to take serious time to fine tune their samples and process, to snag the right numbers from their samples.  For this one, the quanter used a software engineering sample set to try and make it really good.  So it's better than straight W4A16, but not as good as Q6/6bpw, etc.",
              "score": 5,
              "created_utc": "2026-01-11 20:21:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz49osr",
                  "author": "Reddactor",
                  "text": "Yep!\n\nThe thing is that its always a big tradeoff. If I went from this to Q5, I would drop from about 120 token/s to 12 tokens/s.  As I'm lucky enough to have 960 GB system RAM as 144 cores of CPU, I will also run DeepSeek 3.2 Speciale in parallel, when I need thinking and wrtiting, and leave the M2.1 model for pure coding work at high speed.",
                  "score": 2,
                  "created_utc": "2026-01-12 06:11:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz04fr7",
              "author": "Repsol_Honda_PL",
              "text": "What PCs do you have exactly? ( brand / model / version )",
              "score": 2,
              "created_utc": "2026-01-11 17:22:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz15kv7",
                  "author": "Zyj",
                  "text": "2x Bosgame M5 128GB connected with USB4 cables",
                  "score": 2,
                  "created_utc": "2026-01-11 20:09:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0xquc",
              "author": "henryclw",
              "text": "Strix halo is decent in terms of price. Do mind sharing how you use thunderbolt to connect them together? Just grab a thunderbolt wire, one end plug in machine A and the other end in machine B?  Like no router between, right?",
              "score": 1,
              "created_utc": "2026-01-11 19:34:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz14ogm",
                  "author": "Zyj",
                  "text": "Yes, router won't work. Just plug it in and on Linux you should see a thunderbolt0 interface on both machines.  \nI'm using two cables so I have two thunderbolt1 interfaces also.  \nAccording to iperf3 I get around 9GBit/s of throughput.  \nRight now bonding the two interfaces for more bandwith doesn't work yet, but there's a kernel patch so it should be available soon.",
                  "score": 1,
                  "created_utc": "2026-01-11 20:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1tkuc",
              "author": "xignaceh",
              "text": "Question, how 'bad' is running gguf's with vllm? I've tried it a while ago and it worked but it was in an experimental phase back then",
              "score": 1,
              "created_utc": "2026-01-11 22:01:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz0reme",
          "author": "Glad_Bookkeeper3625",
          "text": "Hi David! If you are owner of GLaDOS I would like to say thank you very much! I used your code to build an assistant for my young daughter. Her most loved thing in life(after cookies) is a science show for kids where ai assistant helps guys in the show with complex things.Â  She asked me to make something similar and wow there is a GLaDOS. I added whisper, support of oai streaming from LLM server on other device in local network and so on. It works very good. Whish all the best to your project.",
          "score": 13,
          "created_utc": "2026-01-11 19:06:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0suzn",
              "author": "Reddactor",
              "text": "That's great to hear!  (yes, that's my repo).\n\n  \nTell her I hope she likes it, and that soon GLaDOS will be able to control stuff around the house (MPC support = Home Assistant control).",
              "score": 11,
              "created_utc": "2026-01-11 19:12:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzn91j",
          "author": "Hodler-mane",
          "text": "being able to run minimax m2.1 locally at decent tokens is bragworthy for sure. sounds like you had a blast. not practical at all since minimax subscriptions is less than $5 a month.. but damn im jealous",
          "score": 7,
          "created_utc": "2026-01-11 16:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzpko9",
              "author": "Reddactor",
              "text": "yes... dubious value, but its Local!",
              "score": 5,
              "created_utc": "2026-01-11 16:12:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzjme3",
          "author": "insulaTropicalis",
          "text": "These GH200 go for 40,000 apiece on ebay. Did you forgot a zero?",
          "score": 13,
          "created_utc": "2026-01-11 15:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzlmg6",
              "author": "Reddactor",
              "text": "Link is in the text: [https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i\\_bought\\_a\\_gracehopper\\_server\\_for\\_75k\\_on\\_reddit/](https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/)",
              "score": 10,
              "created_utc": "2026-01-11 15:53:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyzo5wp",
                  "author": "insulaTropicalis",
                  "text": "Yep, I remember your thread. Just joking. BTW, for that money today you could barely buy the 960GB LPDDR5!",
                  "score": 6,
                  "created_utc": "2026-01-11 16:05:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1bse2",
          "author": "ag-duck",
          "text": "This is not crazy at all...   This raises the question.   A small team of say, 2-5 developers focused on building code only with AI models, 5-7 hours a day (net time, think of a startup)... in my experience they would blow out the limits very quickly and this team would have to go into API, or multiple accounts.   I estimate based on some experience using APIs (AWS Bedrock for models in the backend), that this team could spend $2-3K a month in just expenses...   Extrapolating your build into perhaps a private data center where you would host a box or couple of boxes like this to provide enough power to 2-5 developers...   What would be the break even consumption in which having the \"local\" or privately hosted set up would be more beneficial than paying Anthropic or Bedrock, or OpenAI...?\n\nI'd love to see an analysis like that. \n\nGreat job and looks like a lot of fun!",
          "score": 5,
          "created_utc": "2026-01-11 20:38:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0y88m",
          "author": "kwiksi1ver",
          "text": "Maybe I missed it in this post or the blog, but what is the idle power draw of that system? And similarly whatâ€™s the power draw at full load?",
          "score": 4,
          "created_utc": "2026-01-11 19:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0yvg7",
              "author": "Reddactor",
              "text": "500W idle, and I've limited the GPUs to 450W max. At LLM load, it draws about 900W, and for heavy ComfyUI workflows or ML training, about 1100W.",
              "score": 3,
              "created_utc": "2026-01-11 19:39:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzel1r",
          "author": "xXprayerwarrior69Xx",
          "text": "https://preview.redd.it/t9mohxo2mqcg1.png?width=600&format=png&auto=webp&s=e4d05b41365047b0df099816faa1edc6a99a5fc0",
          "score": 6,
          "created_utc": "2026-01-11 15:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz09bxe",
          "author": "pmp22",
          "text": "P40 sisters, what's our response?",
          "score": 3,
          "created_utc": "2026-01-11 17:46:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz25dsb",
              "author": "muxxington",
              "text": "Actually, our response is always the same: more P40s! But I'm not so sure anymore. Can we still not stop winning?",
              "score": 2,
              "created_utc": "2026-01-11 22:58:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz29wt3",
                  "author": "pmp22",
                  "text": "You can get a lot of P40s for 9K EUR! We literally can't stop winning!",
                  "score": 3,
                  "created_utc": "2026-01-11 23:21:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzp4ai",
          "author": "frosticecold",
          "text": "This post seems AI slop",
          "score": 15,
          "created_utc": "2026-01-11 16:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzyczc",
              "author": "Technical_Dish_1250",
              "text": "So yeah: topology said â€œpipelineâ€, reality said â€œlolâ€  \nIf chatgpt didn't say that, I'll eat my hat",
              "score": 10,
              "created_utc": "2026-01-11 16:54:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyzzccs",
                  "author": "squeasy_2202",
                  "text": "> What actually worked (the boring part, but important)\n\n\nLike who actually talks like this",
                  "score": 13,
                  "created_utc": "2026-01-11 16:58:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyzq7kd",
              "author": "Reddactor",
              "text": ":(  Sorry, I guess my writing style sucks.  TBH, looking at it, you are right, it need a tone down.",
              "score": 2,
              "created_utc": "2026-01-11 16:15:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz00qrh",
                  "author": "postitnote",
                  "text": "Are you denying that this writeup was generated by AI?",
                  "score": 15,
                  "created_utc": "2026-01-11 17:05:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0ygra",
                  "author": "Fabulous_Fact_606",
                  "text": "I rather have your ai slop than some incoherent rambling or no post at all on your experience. Got your ideas across is what counts.",
                  "score": 1,
                  "created_utc": "2026-01-11 19:37:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz07iyh",
          "author": "PleasantAd2256",
          "text": "Can you search the Internet as well and do the research?",
          "score": 2,
          "created_utc": "2026-01-11 17:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz15mlh",
          "author": "Dramatic_Strain7370",
          "text": "The good news is that you have a script here to help startups save way more. These startups with many inexperienced devs will be  cranking multiple PR every day. Are you planning to run other models as well? Like running image generation and comparing costs agains Gemini Imagen and OpenAI Dall-e-3",
          "score": 2,
          "created_utc": "2026-01-11 20:10:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz185q1",
              "author": "Reddactor",
              "text": "I have ComfyUI running *all the models...*\n\n\nI'm having a lot of fun running LTX-2 at the moment. It is insanely cool, and I can generate about 10 seconds of video in under a minute.",
              "score": 2,
              "created_utc": "2026-01-11 20:21:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz8sh0p",
                  "author": "Dramatic_Strain7370",
                  "text": "10 seconds of video means that even if you half the Veo 3 cost ($0.40/sec) to $0.2/sec, you can make $2/minute that is  quite a ROI . Maybe instead of text tokens, image and video generation has a superior return.",
                  "score": 1,
                  "created_utc": "2026-01-12 22:14:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2nzym",
          "author": "Only-Letterhead-3411",
          "text": "I'll be honest, before all this crazy used 3090 and p400 prices crazy ram prices, crazy power bills,  putting together an affordable setup to experiment and play around with local llms was perfectly viable. But right now it makes no sense. I love local ai but there is no way I'm spending 10k $ on hardware that wont even last me 5 years while things are changing lighting fast",
          "score": 2,
          "created_utc": "2026-01-12 00:33:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5d8gc",
              "author": "satireplusplus",
              "text": "This is 80k of hardware that OP had to bring back to life by soldering stuff. Basically the equivalent of buying a beaten up supercar and making it shiny again, resell value is now also way higher than he bought it for.",
              "score": 2,
              "created_utc": "2026-01-12 12:05:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2ztr0",
          "author": "robberviet",
          "text": "I think we are pretty clear in this sub that it would cost more to self-host in this market where providers are losing money providing API.",
          "score": 2,
          "created_utc": "2026-01-12 01:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4kx56",
          "author": "Anxious-Program-1940",
          "text": "It works, loads of anger in the comments, but it works. Solid work bud, lots of money, but solid work",
          "score": 2,
          "created_utc": "2026-01-12 07:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4lexm",
          "author": "Optimal_Philosopher9",
          "text": "This is pretty cool, a good achievement.",
          "score": 2,
          "created_utc": "2026-01-12 07:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4og8v",
          "author": "LittleCelebration412",
          "text": "Damn, good build! At least you can vibe code if the world ends",
          "score": 2,
          "created_utc": "2026-01-12 08:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4xlzb",
          "author": "LegacyRemaster",
          "text": "Code more. Save more.",
          "score": 2,
          "created_utc": "2026-01-12 09:50:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6f9gl",
          "author": "Informal-Spinach-345",
          "text": "Nice job. I ended up buying one RTX Pro blackwell at first. Then two..... then four. But hey, it works amazingly well locally!",
          "score": 2,
          "created_utc": "2026-01-12 15:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6x059",
              "author": "Reddactor",
              "text": "Thats some serious VRAM!",
              "score": 1,
              "created_utc": "2026-01-12 17:04:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbjab9",
          "author": "No-Consideration5347",
          "text": "is your wife still with you now?",
          "score": 2,
          "created_utc": "2026-01-13 08:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzuaig",
          "author": "zmarty",
          "text": "The title feels like a personal attack :) I'm in the same situation.",
          "score": 3,
          "created_utc": "2026-01-11 16:34:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0i7n7",
          "author": "fairydreaming",
          "text": "Hmm, so do you have any idea what exactly is the current physical connectivity between the two GH200 modules?\n\nDo you think that it would be possible to install your GH200 modules in a barebone NVIDIA GH200 NVL2 Server to get NVLink connectivity?",
          "score": 2,
          "created_utc": "2026-01-11 18:26:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0jvyv",
              "author": "Reddactor",
              "text": "No idea!\n\nThere are HUGE weird sockets on the mainboard, facing forward, which I think the whole system would dock to in the rack. I guess if the GH200 NVL2 Server had the connectors it might work, but I'm afraid its an entire and very expensive module.",
              "score": 3,
              "created_utc": "2026-01-11 18:33:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz0ot99",
                  "author": "fairydreaming",
                  "text": "Yeah I noticed these connectors too. From what I see in NVL2 system there are no such connectors, here's a photo of the SuperMicro NVL2 base board: [https://www.supermicro.com/en/products/motherboard/g2dmh-gi](https://www.supermicro.com/en/products/motherboard/g2dmh-gi)\n\nLooks somewhat simple. I wonder what is the price. :-D",
                  "score": 3,
                  "created_utc": "2026-01-11 18:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzkx0b",
          "author": "tirolerben",
          "text": "Now use it with clawdbot! Clawdbot is awesome but it burns through tokens like itâ€˜s not funny. Or at least Ralph.",
          "score": 1,
          "created_utc": "2026-01-11 15:49:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzvu3x",
          "author": "txdv",
          "text": "The King of Local",
          "score": 1,
          "created_utc": "2026-01-11 16:42:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzydsf",
          "author": "UltraSPARC",
          "text": "Very cool but couldnâ€™t you cluster cheaper GPUâ€™s and get even more VRAM? Youâ€™re doing inferencing so itâ€™s not like you need H200 horsepower.  Could be wrong!",
          "score": 1,
          "created_utc": "2026-01-11 16:54:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz04d08",
          "author": "CandidLiving5247",
          "text": "Itâ€™s awesome!\nAnd when online prices skyrocket - youâ€™ll be just fine.",
          "score": 1,
          "created_utc": "2026-01-11 17:22:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0pgi5",
          "author": "sluuuurp",
          "text": "Really cool! Can you compare the speed and cost for local MiniMax and cloud MiniMax? \n\nhttps://openrouter.ai/minimax",
          "score": 1,
          "created_utc": "2026-01-11 18:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0v8y6",
          "author": "marketflex_za",
          "text": "oh my god you are my hero/arch-enemy. I read through that post and somehow misread it - the lord touched you my man.",
          "score": 1,
          "created_utc": "2026-01-11 19:23:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz12jwt",
          "author": "The_Vindex",
          "text": "Can you be more specific in terms performance for local coding models, if you use them for it?\n- Token/s\n- Quality of output if compared with Sonnet for the same task? How much handholding needed? \n- Ability to search the internet for answers?",
          "score": 1,
          "created_utc": "2026-01-11 19:56:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz13pal",
          "author": "Intelligent-Form6624",
          "text": "> So anyway, spending â‚¬9,000 on this box saved me $1.27.\n\nHAHAHA",
          "score": 1,
          "created_utc": "2026-01-11 20:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz15j31",
          "author": "Hedede",
          "text": "> Conventional wisdom: â€œno NVLink â‡’ pipeline parallelâ€\n\nYou don't need NVlink for only one single concurrent user, if the cards support PCIe P2P communication. The main killer is the latency, not bandwidth. If you have a poor implementation that does lots of small transfers, even 900 GB/s NVLink won't save you.",
          "score": 1,
          "created_utc": "2026-01-11 20:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1oy09",
          "author": "Mochila-Mochila",
          "text": "> Reader, the guides betrayed me.\n\nðŸ¤£ ðŸ’¯",
          "score": 1,
          "created_utc": "2026-01-11 21:39:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1t8ub",
          "author": "Crinkez",
          "text": "Huh. Since when were H200 gpu's so cheap? I coulda sworn you can't find an actual price for one because \"price on application\" etc. but rumors suggested they were closer to the $100k range. Each.",
          "score": 1,
          "created_utc": "2026-01-11 21:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vpxn",
          "author": "whyyoudidit",
          "text": "do you still need to be subscribed to claude code?",
          "score": 1,
          "created_utc": "2026-01-11 22:11:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz430i8",
              "author": "Reddactor",
              "text": "Nope! (But I still am)",
              "score": 1,
              "created_utc": "2026-01-12 05:19:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5hmtj",
                  "author": "whyyoudidit",
                  "text": "I just installed and it works perfectly with glm 4.7. What a time to be alive just incredible these coding agents.",
                  "score": 1,
                  "created_utc": "2026-01-12 12:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2eutj",
          "author": "BananaPeaches3",
          "text": "Can you try a dense model like Devstral 2 123B?",
          "score": 1,
          "created_utc": "2026-01-11 23:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4353l",
              "author": "Reddactor",
              "text": "I can run then, but I think MiniMax is the best to use with Claude Code. I will give it a try though.",
              "score": 1,
              "created_utc": "2026-01-12 05:20:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5wug6",
                  "author": "BananaPeaches3",
                  "text": "Thanks Iâ€™m curious about the speed of a dense model on such hardware.",
                  "score": 1,
                  "created_utc": "2026-01-12 14:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3ew87",
          "author": "Educational_Teach537",
          "text": "Thinking you can beat the COGS of industrially scaled data centers is like thinking you can save money by hand making your own toilet paper",
          "score": 1,
          "created_utc": "2026-01-12 02:54:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3i9vt",
          "author": "pm_me_github_repos",
          "text": "I dont understand the reasoning of trying PP over TP/CP/EP and normal FSDP? Of course itâ€™s not going to be as performant.",
          "score": 1,
          "created_utc": "2026-01-12 03:12:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3qr4v",
          "author": "CondiMesmer",
          "text": "Yeah I think I'll just stick with openrouter.",
          "score": 1,
          "created_utc": "2026-01-12 03:59:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4fqgp",
          "author": "ifupred",
          "text": "Its a hobby, everyone needs a very expensive one",
          "score": 1,
          "created_utc": "2026-01-12 07:02:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4hey3",
          "author": "LinkSea8324",
          "text": ">Conventional wisdom: â€œno NVLink â‡’ pipeline parallelâ€\n\n\ni'm pretty sure it's \"When you can use tensor parallel over pipeline parallel, do it\".\n\n\n* Tensor parallel shares compute power\n\n* Pipeline parallel just share VRAM (useful when it's not a multiple of the number of kv heads(?), for example 3 GPUs)\n\n* Data parallel duplicates model *from my understanding*",
          "score": 1,
          "created_utc": "2026-01-12 07:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4vrie",
          "author": "FxManiac01",
          "text": "Wow, this is amazing! How do you mount local minmax model into claude? Quite new into this space so pardon me for not knowing this.. Also I think you did great deal, 7k5 eur for 2xGB200?? Wow!",
          "score": 1,
          "created_utc": "2026-01-12 09:32:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz53f7l",
              "author": "Reddactor",
              "text": "make a shell script called claude-minimax, with this:\n\n    #!/usr/bin/env bash\n    set -euo pipefail\n    \n    export ANTHROPIC_BASE_URL=\"http://127.0.0.1:8000\"\n    export ANTHROPIC_API_KEY=\"local-vllm\"\n    \n    # Force *all* Claude model aliases to your local vLLM model\n    export ANTHROPIC_MODEL=\"MiniMax-M2.1-FP8\"\n    export ANTHROPIC_SMALL_FAST_MODEL=\"MiniMax-M2.1-FP8\"\n    export ANTHROPIC_DEFAULT_HAIKU_MODEL=\"MiniMax-M2.1-FP8\"\n    export ANTHROPIC_DEFAULT_SONNET_MODEL=\"MiniMax-M2.1-FP8\"\n    export ANTHROPIC_DEFAULT_OPUS_MODEL=\"MiniMax-M2.1-FP8\"\n    \n    # Optional but recommended\n    export CLAUDE_CODE_DISABLE_TELEMETRY=1\n    export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1\n    export API_TIMEOUT_MS=3000000\n    \n    exec claude \"$@\"\n\n\n\nthe \"http://127.0.0.1:8000\" is your local LLM running in vLLM.",
              "score": 2,
              "created_utc": "2026-01-12 10:44:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz552vg",
                  "author": "FxManiac01",
                  "text": "WOW. so basically any model ran in vLLM and this setup of env vars is all you need to do and then claude just communicate with them instead of with anthropic? that is crazy.. is that just some kind of loophole or anthropic keep this at their will and is OK with that? \n\nanyways, if they remove this in future versions, still older versions will be forever capable of this I think?\n\nbtw: where did u buy this monster you have?",
                  "score": 1,
                  "created_utc": "2026-01-12 10:59:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5yzmf",
          "author": "muchCode",
          "text": "As someone who had the same idea and did it with 96GB VRAM, **try a REAP** model. The Minimax M.2 model experts are small enough that they become \"specialized\", the REAP method takes those experts and looks for activation with a testing dataset. The non-activated experts are removed. Saves you VRAM overhead and at 50% pruning/ router tuning you get 96GB VRAM with large context sizes.\n\neg: [https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50-W4A16-REPAIR-IN-PROGRESS](https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50-W4A16-REPAIR-IN-PROGRESS)",
          "score": 1,
          "created_utc": "2026-01-12 14:20:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9ltol",
          "author": "Witty_Mycologist_995",
          "text": "why fp8 vro",
          "score": 1,
          "created_utc": "2026-01-13 00:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzxy1e",
          "author": "Ok-Bill3318",
          "text": "Written anything with it yet?",
          "score": 1,
          "created_utc": "2026-01-11 16:52:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzz098",
              "author": "Bluethefurry",
              "text": "this post it looks like",
              "score": 10,
              "created_utc": "2026-01-11 16:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzim72",
          "author": "Vegetable-Web3932",
          "text": "Was the price wrong or was the server broken? 2 h200 for 9k??  \nCongrats nice build!",
          "score": 1,
          "created_utc": "2026-01-11 15:38:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzlky2",
              "author": "Reddactor",
              "text": "Link is in the text, but just in case: [https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i\\_bought\\_a\\_gracehopper\\_server\\_for\\_75k\\_on\\_reddit/](https://www.reddit.com/r/LocalLLaMA/comments/1pjbhyz/i_bought_a_gracehopper_server_for_75k_on_reddit/)",
              "score": 2,
              "created_utc": "2026-01-11 15:53:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzoca7",
          "author": "HornyGooner4401",
          "text": "Is this better or cheaper than using CC + GLM Coding Plan or just for the cool points?",
          "score": 1,
          "created_utc": "2026-01-11 16:06:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzq08q",
              "author": "Reddactor",
              "text": "Well, its not good value cost wise; but it's sure fun!  I have Clause Pro, not max; maybe this can work together? Claude Code with Opus for planning, and then this for unlimiter implementation?",
              "score": 2,
              "created_utc": "2026-01-11 16:14:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzuuie",
          "author": "Novel-Mechanic3448",
          "text": "Anyone else blocking people making AI posts at this point?",
          "score": -4,
          "created_utc": "2026-01-11 16:37:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzvsdb",
              "author": "zmarty",
              "text": "No, this is useful. But I do agree the AI writing is annoying.",
              "score": 6,
              "created_utc": "2026-01-11 16:41:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzwa5g",
          "author": "lordchickenburger",
          "text": "Cant.wait for it to break down in a few days and all that money into the drain. Stop jacking up gpu prices",
          "score": -4,
          "created_utc": "2026-01-11 16:44:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5dnyw",
      "title": "Performance improvements in llama.cpp over time",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/lsqwma772pbg1.png",
      "author": "jacek2023",
      "created_utc": "2026-01-06 09:03:03",
      "score": 669,
      "num_comments": 85,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxzvomh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-06 12:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzom5l",
          "author": "Dr4x_",
          "text": "Is it merge already?",
          "score": 24,
          "created_utc": "2026-01-06 11:37:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6itwi",
              "author": "[deleted]",
              "text": "I think this merely shows off all of the cumulative performance improvements between September/October 2025 to January 2026, with most of these merged for a long time now",
              "score": 6,
              "created_utc": "2026-01-07 11:13:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz9nk6",
          "author": "ghost_ops_",
          "text": "these performance gains are only for nvidia gpus?",
          "score": 77,
          "created_utc": "2026-01-06 09:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzdnf7",
              "author": "FullstackSensei",
              "text": "I think many also translate to gains on AMD when building for ROCm, since it translates CUDA to HIP at compile time. Of course, architecture specific optimizations won't translate.\n\nI have noticed a general uplift on my Mi50s over the past couple of months, after the amazing work of u/Remove_Ayys.",
              "score": 31,
              "created_utc": "2026-01-06 10:02:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzfd8a",
                  "author": "Remove_Ayys",
                  "text": "AMD optimizations are also in the works (with contributions from AMD engineers). But unsurprisingly the work put in by NVIDIA engineers specifically mostly benefits NVIDIA GPUs. Something like FP4 tensor cores for example also just doesn't exist on most hardware.",
                  "score": 44,
                  "created_utc": "2026-01-06 10:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxza104",
              "author": "cleverusernametry",
              "text": "I'm Hoping macs get some benefit as well?",
              "score": 5,
              "created_utc": "2026-01-06 09:27:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzroq1",
                  "author": "No_Conversation9561",
                  "text": "MLX has made significant improvements over the last year. The recent update is also great.",
                  "score": 11,
                  "created_utc": "2026-01-06 12:01:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny28pg2",
                  "author": "JustSayin_thatuknow",
                  "text": "Not a Mac lover here.. but why downvoting?",
                  "score": 0,
                  "created_utc": "2026-01-06 19:28:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny262nw",
              "author": "MoffKalast",
              "text": "You think the *Nvidia team* will help improve the competition? Yeah right, CUDA only.",
              "score": 1,
              "created_utc": "2026-01-06 19:16:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyb5fnu",
                  "author": "Hunting-Succcubus",
                  "text": "But they are helping intel, trump revealed that",
                  "score": 1,
                  "created_utc": "2026-01-08 00:50:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3ibgf",
                  "author": "droptableadventures",
                  "text": "Some of the work is not from NVIDIA, and some of the NVIDIA work might have been outside the CUDA backend.",
                  "score": 1,
                  "created_utc": "2026-01-12 03:13:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzrawe",
          "author": "jacek2023",
          "text": "[https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/](https://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/)\n\n  \nUpdates to llama.cpp include:\n\n* [GPU token sampling](https://github.com/ggml-org/llama.cpp/pull/17004): Offloads several sampling algorithms (TopK, TopP, Temperature, minK, minP, and multi-sequence sampling) to the GPU, improving quality, consistency, and accuracy of responses, while also increasing performance.\n* [Concurrency for QKV projections](https://github.com/ggml-org/llama.cpp/pull/16991): Support for running concurrent CUDA streams to speed up model inference. To use this feature, pass in the *â€“CUDA\\_GRAPH\\_OPT=1* flag.\n* [MMVQ kernel optimizations](https://github.com/ggml-org/llama.cpp/pull/16847): Pre-loads data into registers and hides delays by increasing GPU utilization on other tasks, to speed up the kernel.\n* [Faster model loading time](https://github.com/ggml-org/llama.cpp/pull/18012): Up to 65% model load time improvements on DGX Spark, and 15% on RTX GPUs.\n* [Native MXFP4 support on NVIDIA Blackwell GPUs](https://github.com/ggml-org/llama.cpp/pull/17906/): Up to 25% faster prompt processing on LLMs using the hardware-level NVFP4 fifth-generation of Tensor Cores on the Blackwell GPUs.",
          "score": 33,
          "created_utc": "2026-01-06 11:58:11",
          "is_submitter": true,
          "replies": [
            {
              "id": "ny0if39",
              "author": "maglat",
              "text": "stupid question. where exactly I need to set *â€“CUDA\\_GRAPH\\_OPT=1*Â ",
              "score": 3,
              "created_utc": "2026-01-06 14:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0ja3m",
                  "author": "jacek2023",
                  "text": "GGML\\_CUDA\\_GRAPH\\_OPT is an env variable, so in the Linux shell you can use export",
                  "score": 8,
                  "created_utc": "2026-01-06 14:47:05",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "ny2arr7",
                  "author": "Overall-Somewhere760",
                  "text": "stupid question #2, what other variables are ok to set when running/compiling llamacpp ? I just used the one that enables cuda/gpu access.",
                  "score": 4,
                  "created_utc": "2026-01-06 19:38:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nycrcus",
              "author": "Rheumi",
              "text": "now a really stupid question. I use LM Studio for my local LLMs. The Llama.cpp would be updated if I update LM Studio, or do I also need to update the Nvidia driver?",
              "score": 1,
              "created_utc": "2026-01-08 06:31:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nycuddc",
                  "author": "jacek2023",
                  "text": "AFAIK, LM Studio is not open source, so itâ€™s probably hard to tell when specific changes from llama.cpp are integrated into LM Studio.",
                  "score": 1,
                  "created_utc": "2026-01-08 06:55:18",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzqdk2",
          "author": "Lissanro",
          "text": "Mainline llama.cpp in terms of token generation speed became quite good, getting very close to ik\\_llama.cpp. Prompt processing about twice as slow though, but still, it has been amazing progress, there have been so many optimizations and improvement in llama.cpp in the past year, and it has wider architecture support, making it sometimes the only choice. Nice to see they continue to improve token generation speeds. If prompt processing gets improved also in the future, it would be amazing.",
          "score": 23,
          "created_utc": "2026-01-06 11:51:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrl2ud",
              "author": "madSaiyanUltra_9789",
              "text": "i don't understand why ik\\_llama pre-fill latency (prompt-processing speed) is 2x llama.cpp, it almost seems very sus?  \nI suppose that if these are gains that are broadly observable due to different routing and optimization strategies, they'll certainly not go unnoticed and will be integrated into llama.cpp.",
              "score": 1,
              "created_utc": "2026-01-10 10:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6kw7t",
          "author": "AfterAte",
          "text": "For QwenCoder3-30B-A3B @ 4K\\_XS on a 3090 in Linux:  \nold build (a month old probably): 170tk/s at 1st token and 150tk/s after 9K tokens  \nnew build (just built): 182tk/s at 1st token and 160tk/s after 9K\n\n(this does not change when I export GGML\\_CUDA\\_GRAPH\\_OPT=1)  \n  \nso it's \\~7% faster for me. Nothing like their numbers but if the quality remains the same (so far it feels the same), it's a win.",
          "score": 9,
          "created_utc": "2026-01-07 11:29:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrlqey",
              "author": "madSaiyanUltra_9789",
              "text": "i've also never got close to those token generation speeds either on a 30B-A3B, but have got around \\~260tps only on dense 3B models which is interesting.",
              "score": 2,
              "created_utc": "2026-01-10 10:42:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzinhx",
          "author": "horriblesmell420",
          "text": "Any modern performance comparisons to vLLM?",
          "score": 6,
          "created_utc": "2026-01-06 10:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzdli6",
          "author": "No_Swimming6548",
          "text": "Time to update. Also, Nemotron 3 Nano optimization when?",
          "score": 12,
          "created_utc": "2026-01-06 10:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzhq4v",
              "author": "Serious_Molasses313",
              "text": "I would love a 20b Nemotron",
              "score": 2,
              "created_utc": "2026-01-06 10:39:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzmpeg",
                  "author": "No_Swimming6548",
                  "text": "Did you try nano 30b? It's pretty fast",
                  "score": 4,
                  "created_utc": "2026-01-06 11:21:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny4wffl",
          "author": "Repeat_Admirable",
          "text": "The efficiency gains are noticeable not just in tokens/sec, but in battery life for background apps. I built a wrapper around local Whisper for dictation, and a year ago it would heat up my laptop. Now with the latest optimizations (and quantization), I can leave it running 24/7 on my Mac and barely notice the power draw. Huge props to the maintainers pushing these limits.",
          "score": 4,
          "created_utc": "2026-01-07 03:28:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzffab",
          "author": "pmttyji",
          "text": "In the right side chart(DGX Spark), GPT-OSS-20B Numbers seems low comparing to 120B model. (OR 120B performs well(giving 50% of what 20B gives) better than 20B). Possibly few optimizations pending for 20B.",
          "score": 2,
          "created_utc": "2026-01-06 10:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxziw2o",
          "author": "am17an",
          "text": "They didn't put the PP results for these models, at least gpt-oss should have 30% gain in those as well due to the FP4 instructions on DGX spark. For TG it's mostly been a series of PRs for fusion with help from NVIDIA engineers. However the TG gains should be for AMD as well (at least I hope)",
          "score": 2,
          "created_utc": "2026-01-06 10:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2h6sz",
          "author": "cibernox",
          "text": "Iâ€™ve also noticed performance gains over the last few months. I used to run 4B models in Q4 at 80tk/s last year and Iâ€™m consistently getting over 100tk/s now. In fact with some memory over clock I can run 8B dense models at 70tk/s now (when context is low). Thats quite amazing.",
          "score": 1,
          "created_utc": "2026-01-06 20:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny35gjr",
          "author": "Firenze30",
          "text": "I didn't find any performance gain updating from 7394 (CUDA 12.4) to 7642 (CUDA 13.1). GPT-OSS-120B.",
          "score": 1,
          "created_utc": "2026-01-06 21:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4225w",
          "author": "HarambeTenSei",
          "text": "And still no audio support",
          "score": 1,
          "created_utc": "2026-01-07 00:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4i2uq",
          "author": "AdventurousGold672",
          "text": "Can we already see the benefit of it?",
          "score": 1,
          "created_utc": "2026-01-07 02:09:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4q394",
          "author": "suicidaleggroll",
          "text": "I really wish they would provide more info.\n\nhttps://developer.nvidia.com/blog/open-source-ai-tool-upgrades-speed-up-llm-and-diffusion-models-on-nvidia-rtx-pcs/\n\n> Janâ€™26 builds are run with the following environment variables and flags: GGML_CUDA_GRAPH_OPT=1, FA=ON, and â€”backend-sampling\n\nOk, are those compiler flags?  Runtime flags?  Arguments to llama.cpp?  Is this a CUDA improvement or llama.cpp improvement?  Which version of which one has these new commits?\n\n> Concurrency for QKV projections: Support for running concurrent CUDA streams to speed up model inference. To use this feature, pass in the â€“CUDA_GRAPH_OPT=1 flag.\n\nI thought it was GGML_CUDA_GRAPH_OPT=1, and with the '-' in front that makes it look like a flag to llama.cpp rather than an environment variable, but llama.cpp flags aren't in all caps.\n\nDoes anyone know of a master list of the various environment variables and compiler flags available for llama.cpp and what they do?  There seems to be very little documentation on it.\n\nEdit: looking through the code, it looks like GGML_CUDA_GRAPH_OPT is an environment variable you have to set at runtime, it's not a compiler flag.  --backend-sampling is a command line arg to llama.cpp.  I see absolutely no mention of FA, maybe that's flash-attn?  If so that's already on by default though.\n\nEdit 2: looks like neither GGML_CUDA_GRAPH_OPT or --backend-sampling exist in ik_llama.cpp, hopefully those get ported over if they make such a large difference\n\nEdit 3: unfortunately --backend-sampling doesn't exist in llama-bench, so I can't test that, but I'm seeing absolutely no change from GGML_CUDA_GRAPH_OPT=1 on my RTX Pro 6000 system.",
          "score": 1,
          "created_utc": "2026-01-07 02:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4qs3b",
              "author": "Glittering-Call8746",
              "text": "They use agentic workflow for everything.. could be heredocs from opus or sonnet. I always have problems with heredocs",
              "score": 1,
              "created_utc": "2026-01-07 02:56:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny60gpj",
              "author": "am17an",
              "text": "What model are you using?",
              "score": 1,
              "created_utc": "2026-01-07 08:25:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny74sof",
                  "author": "suicidaleggroll",
                  "text": "I was focused on MiniMax-M2.1 for those initial tests, I saw no change in performance, llama.cpp was still half the speed of ik_llama.cpp on pp and roughly the same tg.",
                  "score": 1,
                  "created_utc": "2026-01-07 13:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny4zj1d",
          "author": "LatentSpacer",
          "text": "Finally a W from NVIDIA.",
          "score": 1,
          "created_utc": "2026-01-07 03:46:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5ut4g",
          "author": "ab2377",
          "text": "Will the real apple engineers please stand up.",
          "score": 1,
          "created_utc": "2026-01-07 07:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8ycsm",
          "author": "Flashy_Management962",
          "text": "Imagine what could happen if ik llama cpp and llama cpp would merge :(",
          "score": 1,
          "created_utc": "2026-01-07 18:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybphxp",
          "author": "coding_workflow",
          "text": "Does this apply to blackwell? As I see some on DGX, what about Ampere architecture.  \nI noticed already build introduced some flags for blackwell and I had to exclude them to build for Ampere.",
          "score": 1,
          "created_utc": "2026-01-08 02:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzqxei",
          "author": "Ok_Warning2146",
          "text": "That's good news. From which release was this gain merged?",
          "score": 1,
          "created_utc": "2026-01-06 11:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzt3xk",
          "author": "__Maximum__",
          "text": "Is this merged into main of llama.cpp? What nvidia drivers? Any info at all?",
          "score": 1,
          "created_utc": "2026-01-06 12:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzm2i3",
          "author": "llama-impersonator",
          "text": "it's easy if you do it \"the amazon way\" by tanking the perf of recent builds so nvidia can come in and fix it",
          "score": -9,
          "created_utc": "2026-01-06 11:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzp7ju",
              "author": "jacek2023",
              "text": "Can you point to specific llama.cpp commits that tanked performance?",
              "score": 8,
              "created_utc": "2026-01-06 11:42:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxzpwsn",
                  "author": "llama-impersonator",
                  "text": "nope, i only rebuild when i need to for a new model i want to try specifically on lcpp, which is not that often. i use ik_llama more.",
                  "score": -13,
                  "created_utc": "2026-01-06 11:47:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxzt5fi",
                  "author": "Aggressive-Bother470",
                  "text": "Dood, you know there have been several instances :D",
                  "score": -10,
                  "created_utc": "2026-01-06 12:12:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxzq985",
              "author": "CheatCodesOfLife",
              "text": "I havenâ€™t had any performance regressions with Qwen3 235b",
              "score": 2,
              "created_utc": "2026-01-06 11:50:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzrl1s",
                  "author": "llama-impersonator",
                  "text": "prefill went down a bit for me, it was already super slow anyway so that was noticeable. ik_llama is several times faster in prompt processing when i use glm 4.7 anyway.",
                  "score": -3,
                  "created_utc": "2026-01-06 12:00:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzzlc0",
          "author": "asraniel",
          "text": "How does this translate to ollama? I know, people hate ollama around here, but thats what i use.",
          "score": -9,
          "created_utc": "2026-01-06 12:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0ilyl",
              "author": "my_name_isnt_clever",
              "text": "We don't know, that's part of the reason we don't like ollama. They tend to just do what they want, so you should ask them.",
              "score": 18,
              "created_utc": "2026-01-06 14:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny04kc3",
              "author": "Marksta",
              "text": "Depends if Ollama feels like claiming they're using their own engine today or not.",
              "score": 17,
              "created_utc": "2026-01-06 13:26:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny4ph3x",
              "author": "suicidaleggroll",
              "text": "They hate ollama because it's *significantly* slower than llama.cpp and offers basically nothing that warrants taking that hit.  Why use it?  You're just taking a massive performance penalty for no benefit.",
              "score": 1,
              "created_utc": "2026-01-07 02:49:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3kwa3",
                  "author": "droptableadventures",
                  "text": "Well the answer to \"why use it\" is because you can `ollama run somebody/some-model` without having to know what you're doing, and it'll run slowly, half on your CPU, a microscopic context window and terrible default settings, much more easily than anything else.",
                  "score": 1,
                  "created_utc": "2026-01-12 03:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzob99",
          "author": "Niwa-kun",
          "text": "hope i can use more grok/gemini/chatgpt now. damn rate limits.",
          "score": -17,
          "created_utc": "2026-01-06 11:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzouvu",
              "author": "jacek2023",
              "text": "could you clarify what you mean?",
              "score": 8,
              "created_utc": "2026-01-06 11:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxzp7pq",
                  "author": "Niwa-kun",
                  "text": "Greater performance = less their systems are being slammed by their users, which hopefully lifts the usage limits on flagship models.",
                  "score": -13,
                  "created_utc": "2026-01-06 11:42:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q6c9wc",
      "title": "DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q6c9wc",
      "author": "Nunki08",
      "created_utc": "2026-01-07 10:49:12",
      "score": 654,
      "num_comments": 55,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "ny6w58o",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-07 12:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6ls96",
          "author": "qtvivies",
          "text": "https://preview.redd.it/t6ic0x3nywbg1.png?width=1965&format=png&auto=webp&s=ae5ef60128b8a0cf89351e8673ea41eddafb037c\n\nSomething interesting towards the end. Looks like someone forgot about this",
          "score": 146,
          "created_utc": "2026-01-07 11:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8tl7d",
              "author": "Mikasa0xdev",
              "text": "86 pages? That's a weekend read.",
              "score": 16,
              "created_utc": "2026-01-07 18:28:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny8d6zv",
              "author": "Crisis_Averted",
              "text": "layman here: no idea what that implies.",
              "score": 3,
              "created_utc": "2026-01-07 17:15:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny8ejua",
                  "author": "Dany0",
                  "text": "Supervised fine-tuning and Reinforcement learning data. Used to post-train",
                  "score": 28,
                  "created_utc": "2026-01-07 17:21:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny8qvt2",
                  "author": "qtvivies",
                  "text": "They intended to release the (mostly synthetic tmk) data used for the post training of R1, except I assume that \\`xxx\\` was left in accidentally as a placeholder in place of a link. Or they changed their mind and just forgot to remove it.",
                  "score": 24,
                  "created_utc": "2026-01-07 18:16:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyrb5mg",
              "author": "Intelligent-Time7704",
              "text": "real openai",
              "score": 0,
              "created_utc": "2026-01-10 09:03:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8iz9u",
          "author": "warnerbell",
          "text": "The original paper was light on implementation specifics. If they've added more on how they got the reasoning behavior to emerge, that's valuable.",
          "score": 17,
          "created_utc": "2026-01-07 17:41:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6gjni",
          "author": "ResidentPositive4122",
          "text": "New arch about to drop? dsv4 + r2? Packing all the goodies learned from last year. Hopefully they try smaller sizes as well. Would be interesting to see how the arch improvements work at several sizes.",
          "score": 44,
          "created_utc": "2026-01-07 10:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6i96r",
              "author": "panic_in_the_galaxy",
              "text": "You don't update a paper with new results. You would just write a new paper. These are just explanations and clarifications.",
              "score": 68,
              "created_utc": "2026-01-07 11:08:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6if9m",
                  "author": "ResidentPositive4122",
                  "text": "No, I mean they've updated this with all they had, and preparing for a new arch. Like in closing a chapter.",
                  "score": 45,
                  "created_utc": "2026-01-07 11:09:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7fp99",
                  "author": "ab2377",
                  "text": "but why so late?",
                  "score": 4,
                  "created_utc": "2026-01-07 14:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6lszd",
              "author": "swaglord1k",
              "text": "that would make sense. we'll finally see whether deepseek was a one trick pony or if they'll managed to drop another market-disrupting model",
              "score": 9,
              "created_utc": "2026-01-07 11:36:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6odal",
                  "author": "ForsookComparison",
                  "text": "R1-0528 and V3.2 were both equally SOTA-threatening especially when cost comes into play.\n\nThe big shocker was *\"you can do this without being a USA-based hyperscaler!?\"* and now that we know this to be true, I don't think we'll ever have a similar moment again.",
                  "score": 51,
                  "created_utc": "2026-01-07 11:56:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6x67y",
                  "author": "Few_Painter_5588",
                  "text": "Uhm, no. Deepseek were always market disrupting. Deepseek v2, v3, V3 0324, R1, 3.2. They've only had three flops, V1, R1 0528 and V3.2 speciale.",
                  "score": 1,
                  "created_utc": "2026-01-07 12:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny7c3yc",
          "author": "Ok_Technology_5962",
          "text": "Current research is linear attention. DeepSeek 3.2 with the cache optimization and now they had a massive paper come out that puts the linear into the whole modelaking it possible to train more than 60 layers. So yes this one is done. The compute and thinking will now happen internally in 1000 layers",
          "score": 31,
          "created_utc": "2026-01-07 14:21:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ne3n",
          "author": "CryptoUsher",
          "text": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "score": 11,
          "created_utc": "2026-01-07 20:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny74o1r",
          "author": "jeffwadsworth",
          "text": "I was using the online chat version last night to improve a large Java class (40K tokens) with multiple methods.  It did so beautifully with zero issues in one shot.  The same task in my sub of Gemini 3 Pro chat interface failed in a few shots due to hallucinations.  They have really improved that model a lot from a year ago.",
          "score": 9,
          "created_utc": "2026-01-07 13:40:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycauta",
          "author": "badgerbadgerbadgerWI",
          "text": "The level of detail they're releasing is remarkable. This kind of transparency is what pushes the whole field forward. Really interested in their distillation approach - getting smaller models to match larger ones' reasoning is key for edge deployment.",
          "score": 6,
          "created_utc": "2026-01-08 04:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyae2ug",
          "author": "timfduffy",
          "text": "I think all this info was previously released as a [supplment to their R1 paper in Nature](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf).",
          "score": 4,
          "created_utc": "2026-01-07 22:34:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9okeg",
          "author": "CryptoUsher",
          "text": "honestly the fact they went back and added 60+ pages is kind of wild. most papers just release and call it a day, maybe a small erratum if something's broken.\n\n\n\nwonder if this was all stuff they had internally but couldn't publish initially, or if they're responding to community feedback and trying to explain their approach better. either way it's good for reproducibility.\n\n\n\nthe original paper was already dense but felt like it was missing implementation details. if they're actually filling in those gaps this could be huge for people trying to replicate or build on their work.",
          "score": 2,
          "created_utc": "2026-01-07 20:43:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybh79p",
              "author": "Imaginary-Bit-3656",
              "text": "I think it was part of getting the paper accepted into Nature, where that same additional information was included with the work as supplimentary material (I think the standards for Nature are considered quite high)  \n  \nMany of the papers we see on Arxiv are preprints, works that have not been peer reviewed and may not ever be published in a journal.",
              "score": 2,
              "created_utc": "2026-01-08 01:51:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydlzki",
          "author": "Eyelbee",
          "text": "It's crazy it's only been one year, feels like ages",
          "score": 1,
          "created_utc": "2026-01-08 11:00:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6xeqo",
          "author": "Aggressive-Bother470",
          "text": "New grpo details perhaps? From reading the hf page it implied it was maybe light in that regard?Â ",
          "score": 1,
          "created_utc": "2026-01-07 12:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny76206",
          "author": "TelloLeEngineer",
          "text": "does arxiv have a diff UI?",
          "score": 1,
          "created_utc": "2026-01-07 13:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7fy4u",
              "author": "Freonr2",
              "text": "Don't think so, but you can download the raw TeX format versions of the paper and see the appendix.tex is new and the largest .tex file, or look for diffs that way from the raw .tex files.",
              "score": 6,
              "created_utc": "2026-01-07 14:41:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6y8mn",
          "author": "yoshiK",
          "text": "I did quickly throw the two papers into Gemini. It's really fun to live in the future. \n\n#Gemini summary: \n\nThe paper **\"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\"** (arXiv:2501.12948) marks a significant milestone in open-source AI by demonstrating that advanced reasoning (similar to OpenAIâ€™s o1) can be achieved through large-scale Reinforcement Learning (RL) with minimal human-annotated data.\n\nThe two versions you provided represent the initial release (**v1**, Jan 22, 2025) and the latest updated version (which has been significantly expanded to **86+ pages** as of January 2026).\n\n### 1. Overall Paper Summary\nThe paper introduces two primary models:\n*   **DeepSeek-R1-Zero:** A model trained via \"pure RL\" (using the GRPO algorithm) starting directly from a base model without any Supervised Fine-Tuning (SFT). It demonstrates that reasoning behaviors like self-correction and reflection can emerge purely from reward signals.\n*   **DeepSeek-R1:** A more \"user-friendly\" version that uses a multi-stage pipeline (Cold-start SFT â†’ Reasoning RL â†’ Rejection Sampling/SFT â†’ General RL) to fix the \"readability\" and \"language mixing\" issues of R1-Zero while maintaining state-of-the-art reasoning performance.\n*   **Distillation:** The authors show that the reasoning patterns discovered by the 671B model can be distilled into smaller models (1.5B to 70B), allowing a 14B model to outperform much larger ones on math and coding benchmarks.\n\n---\n\n### 2. Comparison: Extensions in the New Version\nThe newer version is a massive technical expansion (growing from roughly 22 pages to over 85 pages). The key additions and extensions include:\n\n#### A. The \"Aha Moment\" Expansion (Section 2.2.1)\nThe new version provides a much deeper analysis of the **\"Aha Moment\"**â€”the point during RL training where the model unexpectedly learns to \"re-think\" its approach. The extension includes more qualitative examples and internal data showing the model's transition from linear solving to iterative self-correction without being prompted to do so.\n\n#### B. Detailed 4-Stage Training Pipeline\nWhile v1 outlined the stages, the new version details the specific composition of the **800k total training samples**:\n*   **Stage 1 (Cold Start):** Expanded details on the ~5,000-10,000 long CoT (Chain of Thought) samples used to \"prime\" the model.\n*   **Stage 3 (Rejection Sampling):** A deeper dive into how 600k reasoning-related and 200k non-reasoning samples were filtered and used to improve the model's general chat capabilities and prevent \"forgetting\" during the reasoning-heavy RL stages.\n\n#### C. Comprehensive Ablation Studies\nThe new version adds extensive \"What if?\" scenarios that were absent or brief in v1:\n*   **Distillation vs. RL:** New evidence explaining *why* distilling a large model's reasoning traces into a small model is more effective than training that small model directly with its own RL.\n*   **Base Model Impact:** Analysis of how different base models (DeepSeek-V3 vs. Qwen vs. Llama) respond to the R1 training recipe.\n\n#### D. Expanded \"Unsuccessful Attempts\" (Section 4.2)\nOne of the most valuable additions for researchers is the expanded section on what **did not work**. The new version elaborates on their failures with:\n*   **Process Reward Models (PRM):** Detailed reasons why step-level rewards were difficult to scale or prone to \"reward hacking\" compared to the outcome-based rewards used in R1.\n*   **Monte Carlo Tree Search (MCTS):** Technical explanation of why MCTS didn't provide the expected gains over simple RL in the context of LLM reasoning.\n\n#### E. New Benchmarks & Technical Specs\n*   **Updated Results:** Includes more recent evaluations on benchmarks like **AIME 2025**, **LiveCodeBench**, and specialized medical/legal reasoning tests.\n*   **Hyperparameters:** The new version includes exhaustive tables of training hyperparameters (learning rates, GRPO group sizes, KL divergence coefficients) which were previously withheld or summarized.\n\n### Summary Table\n| Feature | v1 (Original) | Latest Version (Extension) |\n| :--- | :--- | :--- |\n| **Page Count** | ~22 Pages | **86+ Pages** |\n| **Methodology** | High-level 4-stage overview | Granular detail on each stage (SFT, RL, Rejection Sampling) |\n| **Behaviors** | Mentions \"self-correction\" | Deep dive into \"Aha Moment\" with case studies |\n| **Failed Paths** | Brief mention of PRM/MCTS | Exhaustive analysis of why PRM and MCTS underperformed |\n| **Distillation** | Introduced 1.5B to 70B models | Added deep ablation on distillation efficiency and data filtering |\n| **Hyperparameters** | Partial/Summary | **Complete Technical Specs** for reproducibility |",
          "score": -8,
          "created_utc": "2026-01-07 13:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny74580",
              "author": "DigThatData",
              "text": "there is no section 2.2.1. section 2.2 is \"reward design\" and is only 4 short paragraphs plus a figure.\n\nI don't think LLMs are reliable for diff-ing. just use the `diff` utility instead, and then ask the LLM to explain the patch.",
              "score": 29,
              "created_utc": "2026-01-07 13:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny85k2j",
                  "author": "yoshiK",
                  "text": "The discussion of the a-ha moment is actually section 2.3. Though I didn't check too closely since the summary is anyhow that I probably need to sit down and read the whole thing at some point.",
                  "score": -1,
                  "created_utc": "2026-01-07 16:41:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7l7fj",
              "author": "pigeon57434",
              "text": "i dont understand people who just post completely slop summaries into comment sections by AI if we wanted a summary we would ask a model ourselves since it requires no effort",
              "score": 11,
              "created_utc": "2026-01-07 15:07:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7v3vu",
                  "author": "menictagrib",
                  "text": "The new version of someone with no background knowledge answering a technical question for dopamine by repeating whatever they see on the first page of Google",
                  "score": 7,
                  "created_utc": "2026-01-07 15:54:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7pe83",
                  "author": "arguingwithabot",
                  "text": "I hear you, but at the end of the day they saved you a few clicks, keystrokes and tokens.",
                  "score": 2,
                  "created_utc": "2026-01-07 15:27:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny84z5q",
                  "author": "Chickenbuttlord",
                  "text": "Nobody asked for your opinion buddy, yes we do very much need summary posted into a comment incase we're too lazy to do it ourselves. Thank you op!",
                  "score": -2,
                  "created_utc": "2026-01-07 16:38:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q4x5e9",
      "title": "For the first time in 5 years, Nvidia will not announce any new GPUs at CES â€” company quashes RTX 50 Super rumors as AI expected to take center stage",
      "subreddit": "LocalLLaMA",
      "url": "https://www.tomshardware.com/pc-components/gpus/for-the-first-time-in-5-years-nvidia-will-not-announce-any-new-gpus-at-ces-company-quashes-rtx-50-super-rumors-as-ai-expected-to-take-center-stage",
      "author": "FullstackSensei",
      "created_utc": "2026-01-05 20:31:51",
      "score": 624,
      "num_comments": 198,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q4x5e9/for_the_first_time_in_5_years_nvidia_will_not/",
      "domain": "tomshardware.com",
      "is_self": false,
      "comments": [
        {
          "id": "nxx2hx4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-06 00:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw3fm6",
          "author": "Long_comment_san",
          "text": "They can announce a new card to be put into production: RTX 3060",
          "score": 109,
          "created_utc": "2026-01-05 21:29:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwif2x",
              "author": "BasicBelch",
              "text": "I wonder why the 3060?    Was it a samsung fab card?",
              "score": 7,
              "created_utc": "2026-01-05 22:40:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwl2j4",
                  "author": "commanderthot",
                  "text": "Yes, all GeForce 3000 was Samsung fab 8nm",
                  "score": 23,
                  "created_utc": "2026-01-05 22:54:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxx7lsu",
                  "author": "Trick-Force11",
                  "text": "its a combo of the easier to source GDDR6 + Samsung 8nm",
                  "score": 12,
                  "created_utc": "2026-01-06 00:51:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwwojm",
                  "author": "AmericanNewt8",
                  "text": "Well I imagine GDDR6 to start, but maybe they held onto the masks or whatnot? 3060s remained abundant for a long while.Â ",
                  "score": 3,
                  "created_utc": "2026-01-05 23:54:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwx3vm",
              "author": "chuckaholic",
              "text": "What a slap in the face to gamers. Gamers that bought their products for years before AI even existed. We built that company. This is like asking a girl on a date and she offers to let you house sit while she goes to Dubai.",
              "score": 31,
              "created_utc": "2026-01-05 23:57:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz69e7",
                  "author": "kingsleyopara",
                  "text": "This seems oddly specific, how was the house sitting?",
                  "score": 14,
                  "created_utc": "2026-01-06 08:51:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxyj0f",
                  "author": "Finanzamt_Endgegner",
                  "text": "I agree but at the very least game devs are now forced to actually optimize their games again which was lacking for years. I don't want new games every year which don't look better than 5y ago but take 2x the resources.",
                  "score": 25,
                  "created_utc": "2026-01-06 03:18:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxy2n2w",
                  "author": "Nobby_Binks",
                  "text": "Yeah my first card was a TNT2. That how long I've been giving Jensen my money. Shareholders call the shots I guess.",
                  "score": 9,
                  "created_utc": "2026-01-06 03:42:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxykc18",
                  "author": "GaboureySidibe",
                  "text": "So buy from AMD",
                  "score": 9,
                  "created_utc": "2026-01-06 05:39:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxyh1lb",
                  "author": "fish312",
                  "text": "that's what we get for believing that companies owe any loyalty to their customers.",
                  "score": 5,
                  "created_utc": "2026-01-06 05:14:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxz81yf",
                  "author": "Jokerit208",
                  "text": "Were they supposed to announce a new series of supers that cost three times what the base versions do? Look around, bud. There's not going to be any hardware innovation from any of these companies any time soon.",
                  "score": 2,
                  "created_utc": "2026-01-06 09:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxys9x9",
              "author": "UsefulOwl2719",
              "text": "\"All new GTX 970 with 4GB of VRAM\"",
              "score": 6,
              "created_utc": "2026-01-06 06:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxytt4v",
                  "author": "pixelpoet_nz",
                  "text": "3.5 GB",
                  "score": 11,
                  "created_utc": "2026-01-06 06:56:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyh7hi",
              "author": "Ecstatic_Winter9425",
              "text": "They should also reduce the vram to 2GB.",
              "score": 3,
              "created_utc": "2026-01-06 05:16:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny0o8oa",
              "author": "BasicBelch",
              "text": "from benchmarks looks like it could easily slot in below the 5050 and they could label it a 5040 or 5040ti\n\n  \nIts not the worst idea in the world",
              "score": 1,
              "created_utc": "2026-01-06 15:11:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvrgs3",
          "author": "Clear_Anything1232",
          "text": "We asked for local models\n\nNow we will be lucky to keep local computing of any kind\n\nCorporate greed on steroids",
          "score": 330,
          "created_utc": "2026-01-05 20:33:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvs970",
              "author": "stiflers-m0m",
              "text": "at least i can FINALLY justify getting my threadripper 5 series with 256 gb memory last year.",
              "score": 101,
              "created_utc": "2026-01-05 20:37:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvt8th",
                  "author": "Clear_Anything1232",
                  "text": "You are an investor now. No longer a computer enthusiast ðŸ˜‚",
                  "score": 114,
                  "created_utc": "2026-01-05 20:42:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvucq4",
                  "author": "Forgot_Password_Dude",
                  "text": "Lol I got 512gb ddr4 tho last year as well from  256gb upgrade.  Now I have too much old RAM",
                  "score": 9,
                  "created_utc": "2026-01-05 20:47:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyg63pv",
                  "author": "cogman10",
                  "text": "I built an overspeced home server 5 years ago with 128gb of DDR4... Man was that a good choice that I didn't realize I was making.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwtnpr",
              "author": "Admirable-Star7088",
              "text": ">Now we will be lucky to keep local computing of any kind\n\nThen computers are no longer something I'm interested in using. I'll have to find another activity in the real world. The computer age was fun as long as it lasted.",
              "score": 12,
              "created_utc": "2026-01-05 23:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxv825",
                  "author": "Corporate_Drone31",
                  "text": "I hear fountain pens are all the rage.",
                  "score": 7,
                  "created_utc": "2026-01-06 02:59:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw5cc3",
              "author": "dogesator",
              "text": "Nvidia is literally one of the biggest publishers of open models on Huggingface",
              "score": 12,
              "created_utc": "2026-01-05 21:38:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxw790c",
                  "author": "lewd_robot",
                  "text": "Doesn't matter if a consumer GPU with a moderate amount of VRAM costs as much as an enterprise unit used to.",
                  "score": 28,
                  "created_utc": "2026-01-05 21:47:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwl8cm",
                  "author": "No_Afternoon_4260",
                  "text": "Nvidia is literally one of the biggest publishers of **quality** open models on Huggingface",
                  "score": 1,
                  "created_utc": "2026-01-05 22:55:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny0iqwm",
              "author": "LegacyRemaster",
              "text": "RTX 6000 96gb + 128gb of ram. Minimax, glm 4.7 , gpt 120... I can wait another 2 years to buy more (and  I have 5070ti + 3060ti + 2070 super + rtx 580",
              "score": 2,
              "created_utc": "2026-01-06 14:44:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxvw1d0",
              "author": "Confusion_Senior",
              "text": "China will fill that market",
              "score": 6,
              "created_utc": "2026-01-05 20:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvwtp3",
                  "author": "Far_Cat9782",
                  "text": "Not if trump has a day about that. Dji anyone? Electric cars? Americans don't like competition with their oligarchs especially since most of the economy is prepped up by a couple tech companies",
                  "score": 10,
                  "created_utc": "2026-01-05 20:58:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw748i",
                  "author": "lewd_robot",
                  "text": "For the rest of the world. The US is burning bridges left and right. If you're in the bottom 50% of Americans, you're not even living in the developed world anymore. Your life is closer to that of someone living in a developing nation. And that trend is growing. More and more people are slipping under that threshold. And it's getting worse at an accelerating rate now.",
                  "score": 5,
                  "created_utc": "2026-01-05 21:46:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyq8bp",
              "author": "strategos",
              "text": "Not corporate greed, just common business sense.\n\nPray every night for the AI bubble to crash so you can have all the silicon you need at dirt cheap prices. Remember this is also how the modern internet was built. Lot of fiber was deployed using debt, when the dotcom bubble popped, all that fiber had to be utilised and that's how we get cheap modern internet.",
              "score": 2,
              "created_utc": "2026-01-06 06:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxysupe",
                  "author": "Clear_Anything1232",
                  "text": "The issue is unlike fiber, silicon fabs tend to create oligopolies due to the heavy capital investment needed.\n\nI really wish we had a slower but less capital intensive way of manufacturing transistors. This would enhance competition and hobby level fabrication similar to what 3d printing did to plastic products. Currently semi conductors is a cartel similar to opec+ just not formally declared.",
                  "score": 3,
                  "created_utc": "2026-01-06 06:47:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw7czu",
              "author": "twilliwilkinsonshire",
              "text": "Oh yeah corporate greed is NOT selling products they could shift like hotcakes. /s  \nCorporate greed would be if they sold as much as humanly possible with zero care to overbuilding, flooding the market with an inevitable crash in the future. They could straight up sell preorders for cards not even in existence to consumers at this point.\n\nAlways surprises me how utterly moronic most commentary on this stuff is, most people have next to zero understanding of economics, those that do seem to only apply half the knowledge and drool on the rest of their 'master market thesis'. Yes, they are a for profit company, yes there are going to be individual greedy choices, but calling literally everything corporate greed Is just commieslop.",
              "score": 1,
              "created_utc": "2026-01-05 21:47:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwi8le",
                  "author": "BasicBelch",
                  "text": "I really expected more intelligence and reason from this sub.   Instead we get the same idiotic uninformed yelling and namecalling as the rest of Reddit.   Maybe they are all bots or something?  (wouldnt that be ironic)",
                  "score": 0,
                  "created_utc": "2026-01-05 22:40:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxx4q83",
                  "author": "twilliwilkinsonshire",
                  "text": "u/BasicBelch When your username itself is a 'clever' insult your grandstanding loses some weight, doubly so when you preemptively block so you can -directly- insult intelligence without any pushback.\n\nPeople just want to be able to spout nonsense with zero pushback, I know my statement is unpopular, I just am tired of the loudest common denominator being the training data that AI's are trained on so I am at least going to put out what I think too.",
                  "score": 1,
                  "created_utc": "2026-01-06 00:36:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz7xhe",
              "author": "Guinness",
              "text": "Hey Intel. Grandma is handing you a golden opportunity.",
              "score": 1,
              "created_utc": "2026-01-06 09:07:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny6x6k1",
              "author": "auradragon1",
              "text": "They donâ€™t have enough RAM to launch new Super GPUs. Not corporate greed on steroids.",
              "score": 1,
              "created_utc": "2026-01-07 12:56:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nycbn40",
              "author": "Visual_Crew_792",
              "text": "Don't worry, once the accountants realize that there aren't enough customers in the world to pay for all this, you'll be able to scoop this shit up at rock bottom prices from bankrupt startups",
              "score": 1,
              "created_utc": "2026-01-08 04:40:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxyhvg9",
              "author": "DrummerPrevious",
              "text": "So greedy that they will not be able to produce shit and pop like a balloon loll",
              "score": 0,
              "created_utc": "2026-01-06 05:20:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvziah",
          "author": "fallingdowndizzyvr",
          "text": "3060 forever!!!!",
          "score": 19,
          "created_utc": "2026-01-05 21:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxx7qo",
              "author": "xyzzs",
              "text": "Kind of good news for my poor old 306012g, looks like it will be the go to card (itâ€™s still #1 on steam) for us poorâ€™s for another few years at least.",
              "score": 6,
              "created_utc": "2026-01-06 03:10:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwcgwt",
          "author": "Aggressive-Bother470",
          "text": "We need China to get on the case now and flood ebay with 48 / 96GB cards.Â ",
          "score": 66,
          "created_utc": "2026-01-05 22:11:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwp9yy",
              "author": "dododragon",
              "text": "There are super x laptops coming with AMD AI 395 Chips and 128GB unified memory.\n\nNot as quick as nvidia vram, but not a bad consolation.\n\nhttps://onexplayerstore.com/pages/super-x-preview",
              "score": 20,
              "created_utc": "2026-01-05 23:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyub9m",
                  "author": "pixelpoet_nz",
                  "text": "of course it has to be a laptop...",
                  "score": 3,
                  "created_utc": "2026-01-06 07:00:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwjqwp",
          "author": "ArtfulGenie69",
          "text": "It's 3090's forever guys. No 5070/80 super 24gb or anything lol.Â ",
          "score": 14,
          "created_utc": "2026-01-05 22:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxvp5",
          "author": "PersonOfDisinterest9",
          "text": "This really blows.  \n   \nI haven't built a new desktop in over a decade, I've been living off of desktop replacement laptops.  \nEvery year it's been like \"Maybe I should just lay down the money, everything keeps getting way more expensive every year\", but for a while it didn't make sense because I was never home, and it was going to be a $5k box just sitting and doing nothing.  \nBitcoin miners fucked the GPU market, then it was NFTs and crypto, then AI started blowing up, then the pandemic, and then AI *really* went bonkers...  \n   \nAt this point, it's literally: do I want a new computer, or do I want to delay having enough for a down payment on a mortgage for another year or two?  \n   \nI should just suck it up and finally build a computer, it would probably break the curse and the cost of all components would drop to 1/10 of the price, one day after the return period on the new gear passes.",
          "score": 13,
          "created_utc": "2026-01-06 03:14:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz8dwd",
              "author": "Jokerit208",
              "text": "Things were fine six months ago. That was your window.\n\nI'd just pay the prices, but you do you.",
              "score": 4,
              "created_utc": "2026-01-06 09:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzyoxr",
                  "author": "PersonOfDisinterest9",
                  "text": "Things were not fine 6 months ago, things haven't been fine for a decade, it's just gotten increasingly worse, while people accept it as the new normal.",
                  "score": 5,
                  "created_utc": "2026-01-06 12:50:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz2j9s",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-06 08:15:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1e0uo",
                  "author": "Context_Core",
                  "text": "I bought 96gb of DDR5 6000mhz RAM in June and felt like a RETARD at the time knowing itâ€™s totally overkill. \n\nIâ€™ve never felt so justified for being a retard before. Itâ€™s nice LOL",
                  "score": 4,
                  "created_utc": "2026-01-06 17:10:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxz4luq",
                  "author": "PersonOfDisinterest9",
                  "text": "I have no special insights, I am just assuming that eventually some kind of market forces are going to come into play. \nThere's got to be at least a few CEO creaming their pants over RAM prices right now and trying to get in on it, so what I'm *hoping* for, is that there's a massive rush to produce RAM, and then a market crash where they have to sell near manufacturing cost to try any recover something.  \n  \nThat's it. The plan is to hope that short-sighted greed and competition saves the day for some reason.   \nEither that, or my next job pays $40k a year more so I can justify spending half that on a new computer.",
                  "score": 3,
                  "created_utc": "2026-01-06 08:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxw66bn",
          "author": "Desperate-Sir-5088",
          "text": "I'm so proud I boughtÂ  EVGA 3090ti from local market at $600 as the Xmas gift. I just invested in the future.Â ",
          "score": 29,
          "created_utc": "2026-01-05 21:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwcbsq",
              "author": "FullstackSensei",
              "text": "Been trying to get an EVGA 3090 XC3 for months. It's the only dual slot 3090 that's not a blower.",
              "score": 10,
              "created_utc": "2026-01-05 22:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwnzvp",
                  "author": "-InformalBanana-",
                  "text": "Hi, sorry for the inconvenience, but could you maybe explain why it matters what is the exact version of rtx 3090 or 3090 ti? Thanks.",
                  "score": 3,
                  "created_utc": "2026-01-05 23:09:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxze8hv",
                  "author": "alex_bit_",
                  "text": "Itâ€™s a little bit more than two slots, but I get it anyway.",
                  "score": 1,
                  "created_utc": "2026-01-06 10:07:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwihed",
              "author": "BasicBelch",
              "text": "came so close to buying one a few months back but thought the prices would continue to go down.   oh well.",
              "score": 5,
              "created_utc": "2026-01-05 22:41:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw6qm9",
          "author": "dakjelle",
          "text": "My 4080S is a surprise 1080 I never thought it would be.",
          "score": 12,
          "created_utc": "2026-01-05 21:44:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy0n42",
          "author": "Sea_Succotash3634",
          "text": "\"Consumer\" Electronic Show",
          "score": 10,
          "created_utc": "2026-01-06 03:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwb1iz",
          "author": "shortsteve",
          "text": "If that's the case they should be kicked out of CES. It's the Consumer Electronics Show not the Enterprise AI shareholders meeting.",
          "score": 42,
          "created_utc": "2026-01-05 22:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzvruz",
              "author": "FalselyHidden",
              "text": "All future Nvidia GPUs will be AI GPUs that outputs fake frames to show your desktop. There will not be a 6000 series.",
              "score": 1,
              "created_utc": "2026-01-06 12:30:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxs3p0",
          "author": "dmter",
          "text": "it's no greed issue, it's monopoly issue. why bother making any r&d if there is no competition (amd is controlled by a relative lol) and demand is so high anyways so no need to develop to make people buy new stuff like before.\n\nonly hope is chinese competition.",
          "score": 8,
          "created_utc": "2026-01-06 02:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvzkna",
          "author": "iswasdoes",
          "text": "Imagine if the 50 series were the last high end consumer GPUs",
          "score": 28,
          "created_utc": "2026-01-05 21:11:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxw0hvv",
              "author": "FullstackSensei",
              "text": "The market abhors a vacuum.\n\nIf Intel has enough GDDR supply (secured contracts before the shit show), they might very will be breathing up Nvidia's neck in a few years. Plus, the Chinese are also coming.\n\nIf anything, the GPU market will be a lot more competitive when the AI bubble bursts in a few years, and Nvidia will loose much of its dominance.",
              "score": 29,
              "created_utc": "2026-01-05 21:16:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxx5u9h",
                  "author": "delicious_fanta",
                  "text": "*The non monopolized/oligopilized market.\n\nWhen youâ€™re talking about 3 companies controlling 100% of a product, vacuums are very, very welcome as that means they can triple their prices, thereby inflating their income thereby inflating their stock price.\n\nThere is only one rule american capitalism must abide by - the stock always goes up. There is no second rule.\n\nIn a normal market, competitors would pop up and fill the vacuum, and your statement would be correct. In this specific set of markets itâ€™s effectively impossible for any company not already producing this technology to be capable of producing this technology.\n\nSo, as you mention, another, existing, producer would have to re-tool. But why would they? They are all making more money doing what they are currently doing than they would by doing that.\n\nThere simply is no incentive to make things cheaper when they are already selling their entire volume of product at exponentially elevated rates. It would be irrational to not continue down the current path.",
                  "score": 12,
                  "created_utc": "2026-01-06 00:42:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwdrba",
                  "author": "pinmux",
                  "text": "Intel owns lots of fabs. How hard would it be for them to retool to produce memory on a few lines?\n\nGot to be enough margin now to at least consider it?Â ",
                  "score": 2,
                  "created_utc": "2026-01-05 22:17:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxw3z3h",
              "author": "Odd-Ordinary-5922",
              "text": "good change it very well could be. imo theres a lot of catching up todo on the optimization side of things rather than the physical side.",
              "score": 3,
              "created_utc": "2026-01-05 21:32:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwl9q9",
              "author": "lolwutdo",
              "text": "and your gpu cable burns up",
              "score": 1,
              "created_utc": "2026-01-05 22:55:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw6j7h",
          "author": "ChainOfThot",
          "text": "Picked up a 5090 rig with 64gb ram, a 5070ti for my older pc and a laptop with 32gb of ddr5 before the ram shortage, feeling good. Also have a 4tb gen5 ssd I bought in April I haven't even opened yet, it's also doubled in price.",
          "score": 9,
          "created_utc": "2026-01-05 21:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny30762",
              "author": "Tarekun",
              "text": "Where did you get the 5090 from? How reliable are sources for modded card like that?",
              "score": 1,
              "created_utc": "2026-01-06 21:35:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny31dpr",
                  "author": "ChainOfThot",
                  "text": "It's a regular 5090, 64gb is system ram, mentioned cuz ram prices are crazy too rn",
                  "score": 1,
                  "created_utc": "2026-01-06 21:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvw027",
          "author": "TACO_NV",
          "text": "why they would ? there's no competition.",
          "score": 20,
          "created_utc": "2026-01-05 20:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyo9qp",
              "author": "PeakBrave8235",
              "text": "Apple is literally slaughtering them but okay",
              "score": -4,
              "created_utc": "2026-01-06 06:09:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyujy7",
                  "author": "pixelpoet_nz",
                  "text": "literally, ofc",
                  "score": 5,
                  "created_utc": "2026-01-06 07:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxh2tp",
          "author": "Badger-Purple",
          "text": "welp, I just went to microcenter and returned my 3090ti that I got for 700 end of november...bad move.",
          "score": 3,
          "created_utc": "2026-01-06 01:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw2rqn",
          "author": "T_UMP",
          "text": "https://preview.redd.it/zg51khp7mlbg1.png?width=420&format=png&auto=webp&s=29c01e632095e2f3262bd9a5c1555d45f48c9a87",
          "score": 10,
          "created_utc": "2026-01-05 21:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwuhua",
          "author": "goodtimtim",
          "text": "the inevitable crash is going to be really bad for my 401k, but amazing for my hobby",
          "score": 8,
          "created_utc": "2026-01-05 23:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvv6rp",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 27,
          "created_utc": "2026-01-05 20:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvw89p",
              "author": "FullstackSensei",
              "text": "People keep repeating this, but I don't buy it for one second. OpenAI doesn't nearly have the the cash for such commitments and all the players know it.\n\nAll the big players contributed to this. MS, Amazon, Meta, Twitter, Oracle, etc are all part of this. OpenAI didn't book all the memory supply for 2026 from Samsung and SK Hynix. The hyperscalers did.\n\nIt's an arms race, and OpenAI is actually a much smaller player in this than most think.",
              "score": 43,
              "created_utc": "2026-01-05 20:56:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxw3xhq",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 15,
                  "created_utc": "2026-01-05 21:31:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvz01j",
                  "author": "-p-e-w-",
                  "text": "This. The idea that OpenAI single-handedly bought out the worldâ€™s RAM production (supposedly without even needing it) is the dumbest thing Iâ€™ve heard in a long time. They are worth *one tenth* of Google, Microsoft etc. They donâ€™t have anywhere near enough clout to do that.",
                  "score": 15,
                  "created_utc": "2026-01-05 21:09:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw2zf1",
                  "author": "BakerXBL",
                  "text": "> Samsung Electronics Co.'s profit fell for the first time since 2023 [in Q2 2025], with a 56% plummet in operating income for the June quarter. \n\nDid they have a choice?",
                  "score": 1,
                  "created_utc": "2026-01-05 21:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxw5g8y",
          "author": "ufos1111",
          "text": "well maybe with a year's delay on next gen models they can figure out how to stop power connectors from melting ffs lmao",
          "score": 6,
          "created_utc": "2026-01-05 21:38:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxvx3t",
          "author": "segmond",
          "text": "It's actually a good thing there's hardware squeeze, it's going to force innovation on the software side.   We will figure out how to infer faster with what we have and we will get smaller models.   It's a temporary discomfort that's much needed.  The problem with \"easy/free\" money is that folks stop optimizing and go in all brute forcing.   Resource constraints breeds resourcefulness.",
          "score": 5,
          "created_utc": "2026-01-06 03:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwdlgz",
          "author": "DoomFist007",
          "text": "atp i dont know if i should keep my 3070 or sell it for more since i now have a 5070 ti",
          "score": 2,
          "created_utc": "2026-01-05 22:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwdv2f",
              "author": "FullstackSensei",
              "text": "Keep it in case something happens to your 5070Ti. Doubt warranty will be able to replace it.",
              "score": 9,
              "created_utc": "2026-01-05 22:18:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwe2ao",
                  "author": "DoomFist007",
                  "text": "Fair. I did already register it for warranty but i might keep it",
                  "score": 1,
                  "created_utc": "2026-01-05 22:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxy11m4",
          "author": "TheManicProgrammer",
          "text": "I guess my 3050 laptop with 4gb Vram will have to play for another few years haha",
          "score": 2,
          "created_utc": "2026-01-06 03:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy3wzl",
          "author": "Hunting-Succcubus",
          "text": "Why not 4060 or 2060?",
          "score": 2,
          "created_utc": "2026-01-06 03:49:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyvr52",
          "author": "Zyj",
          "text": "$1460 for 128GB? Why are people not buying all Strix Halo systems they can get their hands on?",
          "score": 2,
          "created_utc": "2026-01-06 07:13:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6hvsj",
          "author": "SomeWonOnReddit",
          "text": "I hate AI so much man. I don't want to pay for these higher prices because some people need AI for meme videos, meme pictures and vibe coders.",
          "score": 2,
          "created_utc": "2026-01-07 11:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwkonk",
          "author": "Orlandocollins",
          "text": "Jensens keynote is going to be a total snooze fest.",
          "score": 1,
          "created_utc": "2026-01-05 22:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwluxv",
              "author": "FullstackSensei",
              "text": "I was watching it. Was very interesting with tons of innovatiion, except for the minor detail that we'll never be able to run that hardware at home...",
              "score": 4,
              "created_utc": "2026-01-05 22:58:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxwv593",
              "author": "T_UMP",
              "text": "The more you snooze, the more you sleep.",
              "score": 1,
              "created_utc": "2026-01-05 23:46:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxkab3",
          "author": "squachek",
          "text": "https://www.reddit.com/r/nvidia/s/IwCGtNNCOF?",
          "score": 1,
          "created_utc": "2026-01-06 01:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy5zpi",
          "author": "Ok_Warning2146",
          "text": "Can they make a 70W 4050?",
          "score": 1,
          "created_utc": "2026-01-06 04:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy8f0m",
          "author": "pabskamai",
          "text": "Happy 3080 ownerâ€¦.",
          "score": 1,
          "created_utc": "2026-01-06 04:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzluc3",
          "author": "jfp1992",
          "text": "@amd nows your chance to get consumer GPU market share in time for the ai bubble to pop in a year or so",
          "score": 1,
          "created_utc": "2026-01-06 11:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzp2ch",
              "author": "FullstackSensei",
              "text": "They're in the same boat as Nvidia. I bet you Nvidia wishes they could get enough wafers from TSMC and enough VRAM and HBM from micron/Samsung/Hynix to make GPUs for everyone, but supply is constrained for everyone",
              "score": 1,
              "created_utc": "2026-01-06 11:41:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0ga3y",
                  "author": "jfp1992",
                  "text": "That's a fair shout, they have GPU allocation but getting the memory modules is hard because other corp bought them all",
                  "score": 1,
                  "created_utc": "2026-01-06 14:31:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny36gso",
          "author": "TinFoilHat_69",
          "text": "I remember when video cards were more expensive than game consoles, now that video cards can make game consoles obsolete the rug has been pulled. Game consoles will now be cheaper than computers againâ€¦",
          "score": 1,
          "created_utc": "2026-01-06 22:03:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny39bjt",
              "author": "FullstackSensei",
              "text": "Good point! Only issue is: AMD supplies the APUs for both the Xbox and Playstation, and both consoles compete for the same wafer capacity as HBM.\n\nWill definitely be interesting to see how the next cycle of consoles plays out if MS and Sony haven't already secured capacity.",
              "score": 1,
              "created_utc": "2026-01-06 22:17:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyd5msn",
          "author": "BeAlch",
          "text": "it makes sense : if you sell all your stock to ai tech giants .. it would be strange to say that there's already a more powerful card in town, when they haven't yet train their next model with current hardware they just bought at high price.  \nAs for users it is so expensive that the sole way there will be sold to users is second hand from AI giants when they'll buy the next generation, idem for RAM.  \nthe sole way we could get graphic card at normal price again is that efficient AI only chips surpass GPU in price and performance + optimization in hardware software and models",
          "score": 1,
          "created_utc": "2026-01-08 08:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw2nh5",
          "author": "a_beautiful_rhind",
          "text": "Except we needed that super for AI.. to be the new 3090.",
          "score": 1,
          "created_utc": "2026-01-05 21:26:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxw4ls6",
              "author": "getmevodka",
              "text": "Guess what prices for 3090do. Staying stable xD",
              "score": 6,
              "created_utc": "2026-01-05 21:35:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx4i49",
          "author": "erdirck",
          "text": "short term, but long term, the AI hype will be gone and everything will be back to normal",
          "score": 0,
          "created_utc": "2026-01-06 00:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyrbjf",
              "author": "Substantial-Ebb-584",
              "text": "Yeah, not that long ago I was telling myself - the mining hype will be gone and everything will be back to normal.",
              "score": 5,
              "created_utc": "2026-01-06 06:34:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxx5cuz",
              "author": "FullstackSensei",
              "text": "In the long term, we're all dead. -- John Maynard Keynes\n\nThe big players have all the political backing they want. It'll be at least a couple more years (2028) until the music stops.",
              "score": 8,
              "created_utc": "2026-01-06 00:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxz5djp",
                  "author": "Maleficent-Ad5999",
                  "text": "and then quantum computing or some other shit takes over",
                  "score": 2,
                  "created_utc": "2026-01-06 08:42:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxdtkh",
          "author": "usernameplshere",
          "text": "I've no words",
          "score": 0,
          "created_utc": "2026-01-06 01:25:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz5ea0",
              "author": "Maleficent-Ad5999",
              "text": "\\#",
              "score": 1,
              "created_utc": "2026-01-06 08:42:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxynyrm",
          "author": "Devil_Bat",
          "text": "Remember when RTX 4080 12GB happened that the trillion dollar slop machine managed to prepare new cardboards for RTX 4070 Ti?\nWait for the super.",
          "score": 0,
          "created_utc": "2026-01-06 06:07:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz02y4",
          "author": "IngwiePhoenix",
          "text": "I am not surprised, in the slightest.\n\nWell, I do wonder if they will ever crawl back to gamers when the bubble bursts...would be hella funny. x) But, doubtful. They'll find an excuse.",
          "score": 0,
          "created_utc": "2026-01-06 07:52:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw5dtg",
          "author": "chub0ka",
          "text": "Oh how wrong you are but lets seeâ€¦",
          "score": -11,
          "created_utc": "2026-01-05 21:38:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwbzue",
              "author": "FullstackSensei",
              "text": "Wrong about what?",
              "score": 5,
              "created_utc": "2026-01-05 22:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxwcgi9",
                  "author": "chub0ka",
                  "text": "About no new gpu announced. Watching now. Vera rubin just started",
                  "score": -11,
                  "created_utc": "2026-01-05 22:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qbw325",
      "title": "My wishes for 2026",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/8knck5zv85dg1.png",
      "author": "jacek2023",
      "created_utc": "2026-01-13 16:35:06",
      "score": 624,
      "num_comments": 178,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzf35nh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 20:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdm6yx",
          "author": "fbaldassarri",
          "text": "â€œaffordable GPU > 32GBâ€ To dream is freeâ€¦ ðŸ˜‚",
          "score": 417,
          "created_utc": "2026-01-13 16:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdnmh6",
              "author": "Force88",
              "text": "Yeah, maybe winning lottery is more doable ðŸ˜‚",
              "score": 56,
              "created_utc": "2026-01-13 16:43:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdsmho",
                  "author": "-Akos-",
                  "text": "I was thinking that all wishes before that happening on the same day was more likely ðŸ˜",
                  "score": 9,
                  "created_utc": "2026-01-13 17:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdufe3",
              "author": "digital_n01se_",
              "text": "It is easier for a camel to pass through the eye of a needle",
              "score": 31,
              "created_utc": "2026-01-13 17:26:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzeox29",
                  "author": "ultrachilled",
                  "text": "It is easier for a\n\n~~camel~~ llama\n\nFTFY",
                  "score": 23,
                  "created_utc": "2026-01-13 19:43:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdyeky",
                  "author": "jacek2023",
                  "text": "Depends on size of the needle",
                  "score": 6,
                  "created_utc": "2026-01-13 17:45:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzihval",
                  "author": "Llotekr",
                  "text": "â€¦than for a poor man to bring about the singularity?",
                  "score": 1,
                  "created_utc": "2026-01-14 09:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdv98i",
              "author": "Dr_Kel",
              "text": "Got to wish for affordable 32GB DDR5 in this economy",
              "score": 23,
              "created_utc": "2026-01-13 17:30:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzfdo9p",
              "author": "mark-haus",
              "text": "We donâ€™t even have affordable 32GB RAM (no V) right now",
              "score": 13,
              "created_utc": "2026-01-13 21:38:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzfb8z1",
              "author": "PooMonger20",
              "text": "Absolutely, that was the \"Fine, what color do you want your dragon?\" moment.",
              "score": 8,
              "created_utc": "2026-01-13 21:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nze0u0r",
              "author": "Porespellar",
              "text": "You can pick up two 16GB Intel Arc Pro B50s for like $349 USD. Thatâ€™s about as good as youâ€™re going to get on the cheap side of things until the 24GB B60s start hitting the market.",
              "score": 11,
              "created_utc": "2026-01-13 17:56:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdz3e6",
              "author": "xcr11111",
              "text": "Why not get an used m1 pro max MacBook?",
              "score": 3,
              "created_utc": "2026-01-13 17:48:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzeqy57",
              "author": "Icy-Radio-21",
              "text": "Came here to say this xD",
              "score": 1,
              "created_utc": "2026-01-13 19:52:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzex7gs",
              "author": "masiuspt",
              "text": "If anything it will be an affordable GPU == 2gb",
              "score": 0,
              "created_utc": "2026-01-13 20:22:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdscck",
          "author": "roselan",
          "text": "> affordable GPU > 32GB\n\nWhat color your dragon?",
          "score": 126,
          "created_utc": "2026-01-13 17:16:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzev61h",
              "author": "CV514",
              "text": "Smol blu kobold (fits in 8GB)",
              "score": 9,
              "created_utc": "2026-01-13 20:12:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdn0g1",
          "author": "Proud_Fox_684",
          "text": "lmaaaooo... dude thinks he can manifest affordable GPUs.",
          "score": 107,
          "created_utc": "2026-01-13 16:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdy0ds",
              "author": "MoffKalast",
              "text": "Wishing it for the Chinese new year might be more on the money.",
              "score": 28,
              "created_utc": "2026-01-13 17:43:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgu7lj",
              "author": "jazir555",
              "text": "You just dont understand vision boards and their latent power.",
              "score": 5,
              "created_utc": "2026-01-14 02:16:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdo91h",
          "author": "dondiegorivera",
          "text": "I have high hopes in Deepseek's Engram, so that it allows smaller yet very capable models. Let's see how V4 performs, and how other labs would implement the idea.",
          "score": 15,
          "created_utc": "2026-01-13 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdp684",
              "author": "darkdeepths",
              "text": "same. hope we get moe+engram mixture models in the ~100b zone. would be great if they are mxfp4 / nvfp4 out of the box too. might happen given more folks doing development on blackwell systems.",
              "score": 3,
              "created_utc": "2026-01-13 16:50:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdn0u6",
          "author": "uti24",
          "text": "Ok but it seems we need \\~200Gb affordable GPU, looks like 'most optimal best models' could run in this range. Or is it just limitation of what is possible now, and 'best models' always will grow?\n\nI am curious what difference 32Gb GPU will make?\n\nI also would love to have Mistral something \\~100B",
          "score": 15,
          "created_utc": "2026-01-13 16:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtdlb",
              "author": "jacek2023",
              "text": "We don't need GPUs to run 600B models, people who use these models usually do this in the cloud. They won't start using local setup because they are not local users. We need bigger GPUs for models like 235B (Minimax works ok on my 72GB but in Q3)",
              "score": 0,
              "created_utc": "2026-01-13 17:21:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzit85z",
                  "author": "KeinNiemand",
                  "text": "If we could run 600B models locally for an afforable price local uses would start using bigger models.",
                  "score": 2,
                  "created_utc": "2026-01-14 11:32:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdm75i",
          "author": "celsowm",
          "text": "qwen 4 ok, mistral too, the rest only miracles",
          "score": 55,
          "created_utc": "2026-01-13 16:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdnrrz",
              "author": "SlowFail2433",
              "text": "Mistral is looking rly good now that they have made a Deepseek-like",
              "score": 18,
              "created_utc": "2026-01-13 16:44:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze5b6c",
                  "author": "ForsookComparison",
                  "text": "In tests on non-English non-Chinese tests there's reason to look into the new deepseek-based Mistrals. Otherwise they're pretty weak still",
                  "score": 8,
                  "created_utc": "2026-01-13 18:16:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nziyedg",
                  "author": "bambamlol",
                  "text": "Do you mind sharing which Deepseek-like Mistral model(s) you're referring to?",
                  "score": 1,
                  "created_utc": "2026-01-14 12:12:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzecvzi",
              "author": "TheRealMasonMac",
              "text": "Gemma 4 is already certain unless they cancel. I'm really hoping that they release a model bigger than 27B. Gemma 3 is strong for NLP.\n\n\nThough, I'd also like Phi 5 but there's been zero news on that.",
              "score": 16,
              "created_utc": "2026-01-13 18:49:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhtvlg",
                  "author": "SpicyWangz",
                  "text": "Phi always felt benchmaxed to me and I have never been impressed with its performance. Maybe because itâ€™s more math focused and thatâ€™s outside of my use cases",
                  "score": 3,
                  "created_utc": "2026-01-14 06:10:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzefrom",
                  "author": "celsowm",
                  "text": "I hope you are right",
                  "score": 2,
                  "created_utc": "2026-01-13 19:02:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzep226",
                  "author": "jacek2023",
                  "text": "Yes I forgot to put larger Phi",
                  "score": 1,
                  "created_utc": "2026-01-13 19:44:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzkural",
                  "author": "Salt-Willingness-513",
                  "text": "id really like to see something between 30 and 80b a3b from gemma",
                  "score": 1,
                  "created_utc": "2026-01-14 18:00:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdo5gz",
          "author": "SrijSriv211",
          "text": "Everything except new GPT-OSS, Llama 5 and affordable GPU > 32 GB may happen this year.",
          "score": 18,
          "created_utc": "2026-01-13 16:45:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfb0cl",
              "author": "SlaveZelda",
              "text": "Why not new gpt oss?",
              "score": 1,
              "created_utc": "2026-01-13 21:26:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzfuyjl",
                  "author": "SrijSriv211",
                  "text": "Not very likely unless OpenAI is forced by us.",
                  "score": 7,
                  "created_utc": "2026-01-13 23:02:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi3uli",
                  "author": "__Maximum__",
                  "text": "OpenAI is named ironically, they hate open.",
                  "score": 4,
                  "created_utc": "2026-01-14 07:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdnep4",
          "author": "CrypticZombies",
          "text": "cheap gpu.... ight send to Temu ceo",
          "score": 16,
          "created_utc": "2026-01-13 16:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdrrvd",
          "author": "Long_comment_san",
          "text": "Intel has 24gb GPU. B60. Honestly I wish we got 48gb GPU at 1500. R9700 is pretty close but it's still overpriced hilariously.",
          "score": 7,
          "created_utc": "2026-01-13 17:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze1hgu",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 8,
              "created_utc": "2026-01-13 17:59:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzevynq",
                  "author": "Tai9ch",
                  "text": "If Intel could actually ship the B60 at the initial announced price ($500 or single, $1000 for dual) then it'd be competitive even with with the 3090.",
                  "score": 5,
                  "created_utc": "2026-01-13 20:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzeq7st",
              "author": "Nota_ReAlperson",
              "text": "Maybe the r9600d will fill the gap.",
              "score": 1,
              "created_utc": "2026-01-13 19:49:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdnoft",
          "author": "SlowFail2433",
          "text": "GPT OSS 120B is/was so under-rated. It has one of the best benchmark score to parameter count ratios to this day. It also launched with a good FP4 quant.\n\n\nThe Qwen 4 series especially the dense ones 0.6B - 30B are really key for the field. I see more Arxiv papers each day using those models than anything else. Also for agentic RL applications that use repeated continual pre-training, small and dense models can be better because you donâ€™t really want MoE gates complicating the training. Same goes for RL because experts create a difficult credit assignment problem for the RL gradients",
          "score": 47,
          "created_utc": "2026-01-13 16:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdopx6",
              "author": "audioen",
              "text": "It is easily the best model in my opinion. The fact it is full quality at 60 GB and very good even for 120B model makes it the go-to choice for all my coding questions. It replies concisely and usually 100% correctly. It writes maybe 10-20 % of the code I commit to work, and it saves a ton of time from googling crap because it knows the libraries I'm using already.",
              "score": 20,
              "created_utc": "2026-01-13 16:48:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdr8x8",
                  "author": "SlowFail2433",
                  "text": "Yeah it is about to be surpassed because we have a big flood of upgraded models about to release in Q1 2026 but GPT OSS, upon release, was probably SOTA in the performance per param metric, joint with Qwen 235 thinking and GLM Air. It is clearer now than it was back then because all the models have been tested more now. I was skeptical when OpenAI said they would release an OSS SOTA but it looks like they actually did. Having said that, the weird CoT was a disadvantage given that the adjacent Qwen and GLM models didnâ€™t have that restriction.",
                  "score": 12,
                  "created_utc": "2026-01-13 17:11:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdukkk",
                  "author": "Your_Friendly_Nerd",
                  "text": "What's your hardware setup for running that, and what coding environment do you use? Due to my 32gb ddr4 + 12gb 4070, the best I can run reasonably well is qwen3-coder 30b, but as soon as the coding tasks I give it go beyond the scope of a single relatively basic function, it becomes unusable, especially when tools are involved. So it's usually simpler for me to implement it myself because it'd take more time to get qwen to do it correctly.Â ",
                  "score": 1,
                  "created_utc": "2026-01-13 17:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdys2x",
              "author": "kaisurniwurer",
              "text": "It gives super robotic output. Didn't really like it.\n\nAlso Text completion is a pain to use with this one.",
              "score": 7,
              "created_utc": "2026-01-13 17:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze2j37",
                  "author": "SlowFail2433",
                  "text": "Robotic output is a fair criticism of its writing style yes",
                  "score": 8,
                  "created_utc": "2026-01-13 18:03:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nze4jgq",
                  "author": "valdev",
                  "text": "For what it's useful for, that is ideal.",
                  "score": 5,
                  "created_utc": "2026-01-13 18:12:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nze1hfg",
              "author": "ForsookComparison",
              "text": "The gooners pumped out a month of nonstop hate because it was useless to them. It's actually a really really capable model.",
              "score": 10,
              "created_utc": "2026-01-13 17:59:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzexkos",
                  "author": "mpasila",
                  "text": "Since it's 120B it's too big to run locally for me so I just use bigger models via API.. so it kinda becomes useless for me. It has less world knowledge than other models, less multilingual data so poor for translation, for coding you want big context windows that you can get with stuff like MiniMax-M2.1 200k over 131k.. It also spends ton of tokens on questioning rules. Didn't try it for creative writing but then again with its smaller context window and the lack of world knowledge (similar issue with Qwen) it might not work well for that.  \n(also smaller MoEs suck in general (like that 20B one), better to just take the slower dense model, than deal with faster slop that feels like a much smaller model)",
                  "score": 7,
                  "created_utc": "2026-01-13 20:23:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzjjfv4",
                  "author": "AlwaysInconsistant",
                  "text": "Agreed - also, you risked sounding like a shill if you said anything positive about it - but Iâ€™ve been using it heavily in my rotation since it dropped. I really doubt it will be replaced soon, but fingers crossed.",
                  "score": 0,
                  "created_utc": "2026-01-14 14:19:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdyva6",
              "author": "the__storm",
              "text": "OSS came out, everyone shat all over it, and three months later realized they'd been hasty and it was actually pretty good.  I expect whoever within OpenAI fought for an open-weights release is not going to be motivated to do so again.",
              "score": 3,
              "created_utc": "2026-01-13 17:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzekbsa",
                  "author": "my_name_isnt_clever",
                  "text": "I really doubt OpenAI care if internet randoms thought their model was bad. Gpt-oss launched without a number in the name like all their cloud models, it doesn't give the vibe that they were planning on updating it regularly, if ever.",
                  "score": 10,
                  "created_utc": "2026-01-13 19:22:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzepfte",
              "author": "_VirtualCosmos_",
              "text": "I think it might be worth the extra effort to train MoE, especially if you want to increase their general knowledge in a wide field. MoE models are far more efficient than dense models to run, and I think they will be key in robotics.",
              "score": 1,
              "created_utc": "2026-01-13 19:46:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzduhaa",
              "author": "QuantityGullible4092",
              "text": "Yeah and it crushes in the real world IME",
              "score": 1,
              "created_utc": "2026-01-13 17:26:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzet6zy",
              "author": "wektor420",
              "text": "Gpt oss is very biased towards English only - that is a big issue\n\nQwen is default now imo",
              "score": 1,
              "created_utc": "2026-01-13 20:03:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzew6sm",
                  "author": "SlowFail2433",
                  "text": "Thanks I forgot about other languages",
                  "score": 1,
                  "created_utc": "2026-01-13 20:17:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzec0ok",
          "author": "FullOf_Bad_Ideas",
          "text": "Not a bad wishlist.\n\nI don't think Deepseek will release a polished model under 200B.\n\nLlama 5 might release but will be closed weight.\n\nGLM Air is probably dead, sadly. But they'll release some 4-30B models probably.\n\nQwen 4 and Gemma 4 are likely. Qwen 4 will probably be the first thing to happen from this list.\n\nnew GPT-OSS maybe in H2 2026.\n\nI think Nvidia will be coming to save us with open models, they're raming up their own internal open model development.\n\nWe can't agree on what \"affordable\" is - MI50 32GB is around $500 on ebay. It's 32GB, it's a GPU and it's relatively affordable but it might not be what you meant when wishing for it.",
          "score": 6,
          "created_utc": "2026-01-13 18:45:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzecjwt",
              "author": "jacek2023",
              "text": "Nemotron 100B is something I expect in 2026 with 99% probabilty. I wrote >32B not =32B.",
              "score": 2,
              "created_utc": "2026-01-13 18:47:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfi4mu",
                  "author": "Select-Expression522",
                  "text": "Nvidia already said first half this year, so yes unless they cancel it's release.",
                  "score": 1,
                  "created_utc": "2026-01-13 21:59:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzekfnc",
                  "author": "FullOf_Bad_Ideas",
                  "text": "> I wrote >32B not =32B.\n\ntrue, I assumed you meant >=32GB but it's fine anyway.\n\nIt's 32.768 GB lol\n\nRTX 8000 Quadro 48GB Turing can be found for around $2000 in US.\n\nIt's a GPU with >32GiB VRAM, but it's not great for local LLMs.\n\nThere's no single definition of affordable. People afford buying used $5k or $10k cars (and servicing them later!) and $200k apartments in Poland. Is $2k GPU affordable if you can afford having $10k car? Or $1.5k phone? I'd say yes.",
                  "score": 1,
                  "created_utc": "2026-01-13 19:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzg8r5g",
          "author": "Novel-Mechanic3448",
          "text": "I wish the mods would ban vibecoded \"tools\" because there's ten new ones every fucking day. They need to ban AI written OP posts too. There's no excuse for brand new accounts posting on this sub",
          "score": 6,
          "created_utc": "2026-01-14 00:16:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdmvik",
          "author": "maglat",
          "text": "GPT-OSS with vision would be dope!",
          "score": 11,
          "created_utc": "2026-01-13 16:40:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdowuf",
          "author": "Turbulent_Pin7635",
          "text": "GPU > 32Gb...\n\nOh, sweet summer child.",
          "score": 11,
          "created_utc": "2026-01-13 16:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdukyx",
          "author": "QuantityGullible4092",
          "text": "Gemma soon plz",
          "score": 10,
          "created_utc": "2026-01-13 17:27:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze83a9",
          "author": "alphakue",
          "text": "I just have one wish, I'm not greedy or anything. I need some consumer ASICs that are built for the transformer architecture, which can run 1T sparse models, available for < 2k usd.",
          "score": 5,
          "created_utc": "2026-01-13 18:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzexikb",
              "author": "Jan49_",
              "text": "Won't happen anytime soon. Everything about AI is currently built around the architecture of modern GPUs. \nJust look at NPUs and how little attention they got. \nFurther there are so many advances in such tiny time frames, that it would be impossible to implement all that not just on GPUs but also other hardware",
              "score": 2,
              "created_utc": "2026-01-13 20:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdryud",
          "author": "Valuable_Beginning92",
          "text": "google releasing tpus as consumer GPUs would break nvidia",
          "score": 4,
          "created_utc": "2026-01-13 17:14:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjlibx",
              "author": "My_Unbiased_Opinion",
              "text": "100%. Even if they released TPUs for B2B would break Nvidia.Â ",
              "score": 2,
              "created_utc": "2026-01-14 14:30:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzejzt2",
          "author": "noctrex",
          "text": "Forgot to add to the list: Wait for bubble to burst, so that we can scoop up for cheap those unneeded A100's ðŸ˜",
          "score": 4,
          "created_utc": "2026-01-13 19:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdq7fd",
          "author": "LosEagle",
          "text": "Remember when people complained about the prices of GTX 1080?",
          "score": 5,
          "created_utc": "2026-01-13 17:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdqhy7",
              "author": "ljl87",
              "text": "Yep. I remember when people complained that Jensen hiked the price as well back then.",
              "score": 2,
              "created_utc": "2026-01-13 17:06:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze5nvl",
          "author": "JorG941",
          "text": "A new 12b NemoðŸ˜”",
          "score": 3,
          "created_utc": "2026-01-13 18:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze5uua",
              "author": "jacek2023",
              "text": "14B was released not so long ago (and ignored?)",
              "score": 1,
              "created_utc": "2026-01-13 18:18:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nze87h0",
                  "author": "TheRealMasonMac",
                  "text": "14B is pruned from Small 24B.",
                  "score": 1,
                  "created_utc": "2026-01-13 18:28:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhbs71",
          "author": "ttkciar",
          "text": "My guesses:\n\n* new GPT-OSS:  I'm not sure.  My working hypothesis is that OpenAI only released GPT-OSS because they thought it would generate buzz among the investors they continue to rely upon to keep the doors open.  They might release another one if Altman thinks it will keep investors' money flowing.\n\n* Gemma4:  I think so, yes.  I really hope so.  Google seems committed to this path.\n\n* Qwen4:  I suspect we will see this in 2026.  I think the motivations which led to Qwen3 are still in play.\n\n* GLM Air:  Not sure.  It depend on whether ZAI can repeat their success with GLM-4.5-Air, and produce an Air model which outperforms 4.5-Air.  The fact they haven't yet with 4.6 or 4.7 suggests to me they are having trouble doing so.  Perhaps GLM-5 will prove more fertile ground for this?  We will see.\n\n* Llama-5:  Looking dubious.  I hoped that earlier statements from Meta employees about future Llama models staying closed were just talk-talk, but it's increasingly looking like they are headed in that direction.\n\n* Mistral-midsized:  Also not sure.  I'm having trouble figuring out what motivates MistralAI, and what they are trying to accomplish with their open-weight models.  That makes predicting them problematic.\n\n* Deepseek smaller than 200B:  Probably not.  Deepseek seems to have found their niche in the \"embarrass the West with huge ChatGPT-killing open weight models\", and I expect them to continue focusing their attention on that.\n\n* Affordable GPU larger than 32GB:  I wish!  But probably not.  MI60 (32GB) are affordable now, but the next step up from it is MI210 (64GB), which had dipped below $4000 briefly, but is now priced between $4500 and $5000 again, a situation I don't expect to change much until at least 2027.  Possibly someone (who?) will come out with a new inexpensive GPU product with a huge memory and a lower price tag than used MI210, but I don't see that being a lucrative niche any business would willingly pursue.  It would be very nice to be proven wrong!\n\n* Phi-5:  You didn't mention this one, but I expect Microsoft to come out with it in 2026, though perhaps not until after the several ongoing court cases challenging LLMs' legality have concluded.\n\nWe will see how it goes :-)\n\nRemindMe! 1 year",
          "score": 3,
          "created_utc": "2026-01-14 03:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhbwfn",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 year on [**2027-01-14 03:59:34 UTC**](http://www.wolframalpha.com/input/?i=2027-01-14%2003:59:34%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/nzhbs71/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1qbw325%2Fmy_wishes_for_2026%2Fnzhbs71%2F%5D%0A%0ARemindMe%21%202027-01-14%2003%3A59%3A34%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qbw325)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-14 04:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzedr2h",
          "author": "Zeddi2892",
          "text": "No offense, but I couldnt care less for any of those.\n\nI want one single thing, that probably makes all of those bullet points obsolete:\n\n**An innovative new model architecture.**\n\nWe dont need another model which feels pretty much the same as any other. We dont need another complete worthless benchmark showing us how Model XYZ has 0.02 more *Whatever, no one cares what it means anyway* scoring. \n\nMake the model more efficient, give me 1000B performance in a single 10GB file model. Let me run it on a smartphone, without nVidia GPUs cooking my PSU.\n\nWe all know that this is the future. And I dont want to wait.",
          "score": 6,
          "created_utc": "2026-01-13 18:53:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzeekyq",
              "author": "jacek2023",
              "text": "You are asking for the revolution, it can't be predicted, I am wishing for evolution, something for typical year.",
              "score": 2,
              "created_utc": "2026-01-13 18:56:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzifs4x",
                  "author": "Zeddi2892",
                  "text": "Yeah I feel ya, but I dont get how â€žaffordable GPU >32 GBâ€œ is not nearly as revolutionary as an efficient model architecture :D",
                  "score": 2,
                  "created_utc": "2026-01-14 09:31:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdplph",
          "author": "laserborg",
          "text": "it's funny, my hopes for 2026 somehow include world peace and the superpowers returning to a rule-based world order, but before that we'll get RTX6090 with 96GB VRAM.",
          "score": 7,
          "created_utc": "2026-01-13 16:53:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtgjh",
              "author": "po_stulate",
              "text": "true words",
              "score": 1,
              "created_utc": "2026-01-13 17:22:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzelu74",
          "author": "Double_Cause4609",
          "text": "My takes...\n\n\nGPT-OSS was a mistake and actually set back the local community. It was censored, extremely limited domain applicability, didn't offer the original BF16/FP16 weights for finetuning, didn't provide a base model, etc. I would prefer that we not get a new one. Knowing that is my preference, it is unfortunately likely that we will get one.\n\n\nGemma 4 is pretty probable, but will likely look different to previous generations. It's tough to say exactly what it'll look like, but it'll probably be sparse, if nothing else. If you look at Gemma 3N, we could see some sort of hybrid between that and Deepseek's new Engram architecture, maybe occupying the space we saw in GLM 4.5 Air (a mid/small sized MoE).\n\n\nQwen 4 is pretty likely. Chinese companies are compute constrained by inference and probably almost want to free up inference capacity for training, so it still makes rational self interest sense to release open models.\n\n\nLlama 5 is incredibly unlikely. I'd really like Meta to come back and iterate on local models, and I feel that Llama 4 could have been improved on and succeeded with just a bit of iteration, but Meta's gutted their AI team, and they seem to be focusing on big API models now, with a new team that doesn't support open source.\n\n\nMistral seems to have a really awkward space of model releases. They need to keep some models back from the public to make sure they can make money, so they seem to prefer releasing models in a size that is good enough for local (sub 24B), not enough for small businesses (32B+), but also provide open source models to get buy-in from large corporations (100B+). They're not really interested in ~70B class dense LLMs for consumers because not a lot of consumers have hardware in that range and the hit to their product strategy is bigger than the benefit they get from open source. I could also see them doing something like a GLM 4.5 Air class MoE model instead of a new dense model, but it's tough to say. A real successor to Mistral Nemo 12B would be really nice, though.\n\n\nDeepseek doesn't have any interest in pursuing sub 200B models for any reason, unless maybe it was dense and outperformed their MoE models somehow. That seems unlikely, given their Engram architecture release, though. I'm pretty sure we'll see a possibly bigger than V3 sized model with better long context efficiency/performance.\n\n\nGLM is probably done with the \"Air\" class of model (more than a small local model, less than an API model), and may continue with smaller prototype models (ie: 30B and under class). I expect their next mainline model to be about 500B MoE, but it's tough to say.\n\n\nIn terms of hardware, I imagine we won't see any improvements in 2026, just a slow reduction in price as the price shock from the crazy DRAM deal of 2025 gets eaten away. A lot of enterprises already secured their RAM upgrades, and they upgrade in cycles, which means they didn't \"increase investment\", but rather \"moved investment up by a few years\", so we should see reduced demand for DRAM based products in 2026 from enterprise, which will cause a slow trickle decrease in hardware costs throughout the year. Once costs stabilize towards the end of 2026, to mid 2027, I think we'll start seeing consumer hardware releases at decent scale.\n\n\nI *don't* think that means we'll see an affordable consumer GPU with more than 32GB, unless the rumors of an AMD LPDDR5/6/x GPU coming out are true, but if it does release, it'll be slower than you'd expect, and also not have CUDA\n.\nI *do* think we'll start seeing really impressive APU products, including a followup to Strix Halo from AMD, a Strix Halo competitor from Intel, an improved unified arch from Apple (rumors are 2TB / 2TB/s bandwidth for the top. Will be expensive though), and we'll likely see other players introduce impressive workstation APUs like possibly Qualcomm, too. Even Rockchip is kind of getting into the game and I believe their upcoming SoCs for the year are sort of like an improved Strix Point or Strix Halo-lite.\n\n\nIncluded in these APU products are NPUs, which will see more support coming up, and which offset the lack of a discrete GPU; they cover situations CPUs are poor at (high compute scenarios), and it's possible that Diffusion LLMs or speculative decoding head research may finally see us move from bandwidth bound to compute bound, which may offer a more affordable solution to consumers than traditional GPUs.\n\n\nSo maybe less \"we see an affordable GPU\", and more \"hardware in general gets affordable to those in the know towards mid 2027\".\n\n\nIn general, I think 2026 will be a slow year in hardware, but a crazy year in software advancements as a lot of technologies mature, and we see diversification in model architectures for different scenarios, many of those efforts being led by the community.",
          "score": 7,
          "created_utc": "2026-01-13 19:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzemf2b",
              "author": "jacek2023",
              "text": "Thanks for the detailed response",
              "score": 2,
              "created_utc": "2026-01-13 19:32:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzg41w7",
              "author": "Mochila-Mochila",
              "text": "> see us move from bandwidth bound to compute bound, which may offer a more affordable solution to consumers than traditional GPUs.\n\nWait, won't an emphasis on compute make GPUs *even more* crucial ?\n\nE.g. currently for text generation (transformer models), a CPU is fine.\nOTOH for image/video generation (diffusion models), the same CPU will collapse under the workload.\n\nThus, wouldn't diffusion-based text generation also take a heavy toll on CPUs ?",
              "score": 1,
              "created_utc": "2026-01-13 23:51:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg8xi9",
                  "author": "Double_Cause4609",
                  "text": "You'd think, but even CPUs have extra compute (they have more compute than bandwidth, relatively). This is a fundamental issue since around 2014 or so that compute has been cheaper than bandwidth to add.\n\nYou can actually test this directly. If you run vLLM backed on a CPU, you can get more total tokens per second with multiple requests than in a single request (these extra requests require extra compute!).\n\nSo, if you compare same-model to same model, a Diffusion LLM denoises multiple tokens per forward pass, which actually does offset the increased compute cost somewhat.\n\nBut you mixed the crux of my argument:\n\nMy argument that compute-bound is preferable is that it's way cheaper to add extra NPU TOPs than it is to add bandwidth to a chip. An NPU with \\~200 TOPs is way cheaper than an extra device that adds 100GB/s of bandwidth, for example, just fundamentally due to how these things scale.",
                  "score": 1,
                  "created_utc": "2026-01-14 00:17:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzhredz",
              "author": "Anthonyg5005",
              "text": "I can't wait for the next Gemma. 3n was probably the best local model I've tried so far and the fact it can run fast on my laptop is awesome. Also I agree with gpt oss, it was so unusable and would hallucinate over 80% of the questions I would ask. I tested it locally, a couple hosted apis, and openai's official gpt oss page to make sure it wasn't just something on my side and still got the same unusable results from each one",
              "score": 1,
              "created_utc": "2026-01-14 05:50:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdo4ew",
          "author": "MaxKruse96",
          "text": "Dont forget to ask for 1KG of pure gold.",
          "score": 3,
          "created_utc": "2026-01-13 16:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdqjeu",
          "author": "-InformalBanana-",
          "text": "I don't understand the 24B > Mistral > 123B line, what are you trying to say there? That 24B is the best?",
          "score": 4,
          "created_utc": "2026-01-13 17:07:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdrqd0",
              "author": "jacek2023",
              "text": "Mistral size between 25B and 122B",
              "score": 2,
              "created_utc": "2026-01-13 17:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfz30c",
                  "author": "fooo12gh",
                  "text": "shouldn't it be vice versa? 24B < Mistral < 123B",
                  "score": 1,
                  "created_utc": "2026-01-13 23:24:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdts8k",
                  "author": "-InformalBanana-",
                  "text": "Both devstral 2 small 24b and devstral 2 123b are dense models from Mistral, aren't they?\nI don't have a gpu for that, not even for 24b q8, so I'd like MOE more :)",
                  "score": -1,
                  "created_utc": "2026-01-13 17:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzgbiqj",
              "author": "RobotRobotWhatDoUSee",
              "text": "> 24B > Mistral > 123B\n\nI think they meant:\n\n> **24B < Mistral < 123B**\n\n...and just goofed on the direction of the 'greater than' signs",
              "score": 2,
              "created_utc": "2026-01-14 00:31:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdsej3",
          "author": "JawGBoi",
          "text": "I just want ram prices to go down...",
          "score": 2,
          "created_utc": "2026-01-13 17:16:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdsp37",
          "author": "Fantastic-Emu-3819",
          "text": "Qwen coder",
          "score": 2,
          "created_utc": "2026-01-13 17:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwyyv",
          "author": "AriyaSavaka",
          "text": "No GLM-5.x?",
          "score": 2,
          "created_utc": "2026-01-13 17:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdxizm",
              "author": "jacek2023",
              "text": "It's too big now, I don't need Chinese cloud models, I can use ChatGPT or Gemini or Claude instead",
              "score": 2,
              "created_utc": "2026-01-13 17:41:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nze049l",
          "author": "defcry",
          "text": "Radeon AI PRO R9700, at around 1300 hardly \"affordable\" but half or less of the 5090",
          "score": 2,
          "created_utc": "2026-01-13 17:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze0fba",
              "author": "jacek2023",
              "text": "It's not >32GB",
              "score": 2,
              "created_utc": "2026-01-13 17:54:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nze0ss0",
                  "author": "defcry",
                  "text": "Sorry read it wrong.",
                  "score": 1,
                  "created_utc": "2026-01-13 17:56:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nze5y47",
          "author": "MichelleeeC",
          "text": "it's dream anyways, i wizh free gpu 128gb for everyone",
          "score": 2,
          "created_utc": "2026-01-13 18:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfjq79",
          "author": "lemon07r",
          "text": "Add affordable ram (too bad it won't happen)",
          "score": 2,
          "created_utc": "2026-01-13 22:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfm79n",
          "author": "SKirby00",
          "text": "Here's my one-item wishlist: A model with similar capabilities to Qwen3-Coder-30B, but a bit more memory-efficient.\n\nI can fit that model (4-bit quant) in my 24GB of VRAM with up to ~18K tokens of context before running into memory issues. It's smart enough to use with Cline for simple agentic tasks, but if I could somehow fit like 2-3x the context on my GPU without a *significant* decrease in quality, that'd be so much more useful.",
          "score": 2,
          "created_utc": "2026-01-13 22:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg3yxg",
          "author": "jarail",
          "text": "I can tell you right now which of those we're not getting in 2026.",
          "score": 2,
          "created_utc": "2026-01-13 23:50:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgrnes",
          "author": "misterflyer",
          "text": "anthropic open sourced models\n\nat least to compete with Gemma 4 and OSS",
          "score": 2,
          "created_utc": "2026-01-14 02:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhbq2c",
          "author": "AlwaysLateToThaParty",
          "text": "The best I can do for you is a couple of bottle-caps and a bag of beans.",
          "score": 2,
          "created_utc": "2026-01-14 03:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhncwf",
          "author": "T-VIRUS999",
          "text": "That last one is never happening",
          "score": 2,
          "created_utc": "2026-01-14 05:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhz7i0",
          "author": "Straight_Abrocoma321",
          "text": "There is the Radeon AI Pro R9700 which is sort of affordable, 1000 pounds in the UK",
          "score": 2,
          "created_utc": "2026-01-14 06:55:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhzu38",
              "author": "jacek2023",
              "text": "Please read existing comments:)",
              "score": 1,
              "created_utc": "2026-01-14 07:00:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdqk4h",
          "author": "LocoMod",
          "text": "Karma farming go brrrrrrrrr",
          "score": 2,
          "created_utc": "2026-01-13 17:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdyy8h",
          "author": "ea_nasir_official_",
          "text": "The GPU one won't happen at the moment due to big AI taking all the ram. for affordable >32gb ram GPUs your only option at this very second would be a mac.",
          "score": 1,
          "created_utc": "2026-01-13 17:47:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze03lz",
          "author": "StaysAwakeAllWeek",
          "text": "Arc Pro B60 is the GPU you're after, it's already out.\n\nIt's only 24GB but it is designed from the ground up to optimise for multi gpu AI workstations at the lowest possible price. They are only $500 each and you can put 8 of them in one workstation",
          "score": 1,
          "created_utc": "2026-01-13 17:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze6ujg",
              "author": "FullOf_Bad_Ideas",
              "text": "Are they going for 500 usd? Cheapest I see in Poland is about 860 USD.",
              "score": 2,
              "created_utc": "2026-01-13 18:22:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze7btx",
                  "author": "StaysAwakeAllWeek",
                  "text": "$860 minus the 23% VAT in Poland is exactly $699",
                  "score": 1,
                  "created_utc": "2026-01-13 18:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzefed7",
              "author": "dwkdnvr",
              "text": "Is there a good guide out there for using these for LLM? They are niche cards and certainly don't get discussed much.",
              "score": 1,
              "created_utc": "2026-01-13 19:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze81ll",
          "author": "phenotype001",
          "text": "MiniMax 3",
          "score": 1,
          "created_utc": "2026-01-13 18:27:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeghgq",
          "author": "Ok_Historian4587",
          "text": "I'm hoping for Claude and Grok 5. The current 4.5 and 4.1 models are great as is, so I can only imagine how good the 5 models will be.\n\nEdit: My bad, wrong sub.",
          "score": 1,
          "created_utc": "2026-01-13 19:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzejrab",
          "author": "magnus-m",
          "text": "Agentic GPT-OSS with vision ðŸ¤ž",
          "score": 1,
          "created_utc": "2026-01-13 19:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf01ap",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/uvgr09ibg6dg1.png?width=582&format=png&auto=webp&s=ba4fad190d4699c98802aef84f6ac58cdd552e88\n\nLlama 5",
          "score": 1,
          "created_utc": "2026-01-13 20:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf11jq",
          "author": "NullKalahar",
          "text": "I'd like to buy some GPUs for my own projects, to play around with Llama and other things, but the prices are prohibitive.",
          "score": 1,
          "created_utc": "2026-01-13 20:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf2kgx",
          "author": "k_means_clusterfuck",
          "text": "So what color do you want for your dragon?",
          "score": 1,
          "created_utc": "2026-01-13 20:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf7tx6",
          "author": "zball_",
          "text": "deepseek gonna >1T",
          "score": 1,
          "created_utc": "2026-01-13 21:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfa3fb",
          "author": "PersonOfDisinterest9",
          "text": "If I could build a decent rig for under $10k, that'd be great.",
          "score": 1,
          "created_utc": "2026-01-13 21:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfcmfi",
          "author": "darkpigvirus",
          "text": "LFM3 - 4B rivaling GLM 4.5",
          "score": 1,
          "created_utc": "2026-01-13 21:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfhc4o",
          "author": "GrungeWerX",
          "text": "Same. For All.",
          "score": 1,
          "created_utc": "2026-01-13 21:55:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfhxni",
          "author": "Ok_Signal_7299",
          "text": "Llama lamao, the new Chinese boss is unbearable. He would tank meta for sure, take it in writing lol",
          "score": 1,
          "created_utc": "2026-01-13 21:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfp6py",
          "author": "a_beautiful_rhind",
          "text": "I'll settle for a new 123b with vision. Smaller deepseek like 2.5 would be great too.\n\nllama is over, they have gone closed. gemma had issues with some congress-critter so maybe 4 won't happen :(",
          "score": 1,
          "created_utc": "2026-01-13 22:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfpqxw",
          "author": "viperx7",
          "text": "well models are software so i guess they can very possibly happen  \nbut for the 32GB GPU it has a whole supplychain  and as things are going in sometime we will be hard pressed to \n\nget 32GB RAM",
          "score": 1,
          "created_utc": "2026-01-13 22:36:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr0c7",
          "author": "lati91",
          "text": "Opus 4.5 level MoE model that fits in 128GB is the dream for me",
          "score": 1,
          "created_utc": "2026-01-13 22:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfxisj",
          "author": "inteblio",
          "text": "I'm seeing performance-per-b skyrocket. \n\nWith luck, the current hardware will get more powerful with better software (ai models). It seems that cluster/swarm processing - smart routing could really get very effective. And you also have \"software explosion\". What would 30gb of targeted c-programs be able to achieve? Absolutely tons. \n\nMaybe we're moving back from fluffy-llms to a rigid structure again (with the benefits of both). \n\nI dont know if a gpu can run zillions of independant \"software\" on a problem, but if it can, that might be crackers effective. \n\nThink diffusion model, but \"made with logic\". \n\nThis is all blue-sky, but my message is - \"things are changing... FAST\" ... \"you might want to realise that the _old way_ (with ai/nn) might have just been a phase.\n\nWhich is my way of hoping i dont have to spent any money on hardware for at least a few years....... \n\nTo use less woo-woo language\n- llms dont need to use human language or code - it can be massively simplified - making far more effecient (deeper!) models.\n- massive pre-work can be done by actual software (insanely fast) not the insanely demanding Neural Net stuff.\n\nTo try to hammer home the message.. something like toy story (the movie) was done with SOFTWARE. I don't know the details, but I'd be surprised if it couldn't be rendered easily on modern hardware. But software also made tons of other stuff. It's true there were humans behind it... but i think you see what I'm getting at.",
          "score": 1,
          "created_utc": "2026-01-13 23:16:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfy8yg",
          "author": "Sabin_Stargem",
          "text": "I want the training data to be redone from the ground up for all base models.   Elara is everything and everywhere, more an SCP than a character.",
          "score": 1,
          "created_utc": "2026-01-13 23:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg0dmv",
          "author": "Mochila-Mochila",
          "text": "Sorry bruv, don't care about 2026, I'm only looking ahead to 2027 and its lot of beefed up APUs : Medusa Halo, NVL-AX, N2X (?) ðŸ¤¤",
          "score": 1,
          "created_utc": "2026-01-13 23:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgcnn3",
          "author": "TheManicProgrammer",
          "text": "I just want cheap ram and cpu :(",
          "score": 1,
          "created_utc": "2026-01-14 00:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgopco",
          "author": "UnnamedPlayerXY",
          "text": "Yeah, Qwen 4 is the release I'm looking forward to the most this year. Now it's probably still going to take a bit until we'll see an actual release but getting some infos on the main areas of improvement as well as a rogue release window would be awesome.\n\nI wouldn't hold my breath for a new GPT-OSS though, iirc. they said they only plan to release one once they want to move on to the next generation.",
          "score": 1,
          "created_utc": "2026-01-14 01:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgrx9n",
          "author": "durich",
          "text": "affordable ram",
          "score": 1,
          "created_utc": "2026-01-14 02:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgx6xs",
          "author": "lurenjia_3x",
          "text": "Micron has said the shortage will continue until 2028.  \nMy guess is that weâ€™ll see high performance models that only need less than 16GB VRAM.",
          "score": 1,
          "created_utc": "2026-01-14 02:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh7cmm",
          "author": "lly0571",
          "text": "Qwen-3.5 and Gemma-4 is almost certain in Q1, maybe we would see Qwen4 in Q4?\n\nI believe we would have a smaller Deepseek model alongside deepseek-v4, as ds-v4 maybe >1T sized, which would lead to a really slow TPOT.\n\nMaybe we would have GLM-5-Air or a open weight Mistral Medium later, but I tend to believe they are Minimax-M2 sized MoE rather than a 70B dense model.\n\nOther things including Llama5 or affordable >32GB GPUs is not possible.",
          "score": 1,
          "created_utc": "2026-01-14 03:32:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziclkh",
          "author": "Old-Artist-5369",
          "text": "Best of luck on that last one ðŸ¤£ðŸ¤£",
          "score": 1,
          "created_utc": "2026-01-14 09:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzihsrc",
          "author": "Llotekr",
          "text": "Pro tip: The affordability of GPUs scales with you wealth.",
          "score": 1,
          "created_utc": "2026-01-14 09:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj3qtw",
          "author": "Mychma",
          "text": "Gpt-oss - why not but needs to be smaller and more efficient 20B is just too big,even though they said it can run on 16GB laptop yes it could when it was relesead newer implementations do not and you can use ONLY the model nothing else not even code editor. Yeah and performance is 4-5T/s when you are lucky\nQwen 4 -heck ya qwen3: It consistently got my benchmarks right and it achieved it in reasonable time \nGemma 4 - if they fix their lack luster intelligence and smiley face over use than why not. \n\nGLM air - too fat, I need something like micro <10B-6B \n\nLlama 5 - if they do a comeback that would be awesome \n\nMistral Sorry to break the hearts but in my benchmarks it poor even against llama 4 and gemma 3\n\nDeepseek in newer variants like smaller will be certainly welcome\n\nGpu - not gonna happen. no manufacturer of graphics cards (other from intel) state that they will not announce any new models\n\nBTW: my current most used is LFM2 and it already has better trained LFM2.5 and it improved not as much as I hoped (+-15-20%) but it still in the same format and architecture the performance is the same only 3x amount training tokens.\n\nI love their standart 350M and 350M-math (+reasoning) fast and the output is decent. Just how gemma 3 with similar but full attention mechanism but only 4 heads is completely dumm (270m)\n\n\nThe 2.6B-exp,2.6B and 1.2B are such a great models sometimes rivaling even qwen3 at smaller size",
          "score": 1,
          "created_utc": "2026-01-14 12:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlwbv7",
          "author": "Former-Ad-5757",
          "text": "For me it is faster and easier and more focused distillation techniques. I basically donâ€™t need 99% of the current knowledge an llm has. I need just 1% with the intelligence. I donâ€™t mind if google or OpenAI or qwen or deepseek or Kimi is going for a 100t model with 200 languages etc, just give me a simple process that I can distill a 100b model from it for like sub 1000 dollar with only the 2 languages I speak.",
          "score": 1,
          "created_utc": "2026-01-14 20:50:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm45yx",
          "author": "JsThiago5",
          "text": "Meta will not release open source models anymore. I read it somewhere.",
          "score": 1,
          "created_utc": "2026-01-14 21:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznzkbj",
          "author": "m31317015",
          "text": "The first six are very reasonable, except qwen 4... They branch off too much, now with every update they gotta do 10x amount of work of others.\n\nDeepseek is dipping shit with their R2 (namely to train it solely on Huawei platform)\n\nAffordable GPU > 32GB... if you're talking about new cards the only hope is the B60 dual coming down at price, or else you have to look into 4080 32GB mod from the same old place.",
          "score": 1,
          "created_utc": "2026-01-15 03:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwyzc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-13 17:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdxu40",
              "author": "jacek2023",
              "text": "Don't you like that Llama icon near Mistral?",
              "score": 2,
              "created_utc": "2026-01-13 17:42:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi36by",
          "author": "Patrick_Atsushi",
          "text": "My list: \nAGI / ASI",
          "score": 1,
          "created_utc": "2026-01-14 07:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdopz9",
          "author": "_realpaul",
          "text": "Good models are cheap hardware are kinda mutually exclusive.",
          "score": 0,
          "created_utc": "2026-01-13 16:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdpdnv",
          "author": "darth_hotdog",
          "text": "Thereâ€™s always the 1 bit versions of deepseek models. Runs ok and like 160-180gb",
          "score": 0,
          "created_utc": "2026-01-13 16:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdrl2b",
              "author": "jacek2023",
              "text": "How do you use that model and why?",
              "score": 1,
              "created_utc": "2026-01-13 17:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzdxlcx",
                  "author": "darth_hotdog",
                  "text": "I guess I phrased that poorly, I should have said, runs, not runs ok. It's slow, but if you treat it as a run and forget and come back 20 minutes to an hour later, you can get deepseek at home basically.\n\nIt's not something I'm going to use every day, more of an experiment for me. but I guess if the internet was out and I wantedd a chatgpt quality answer or something, it's always an option. And hey, you mentioned deepseek under 200gb!\n\nHere's a page about the r1 version.\nhttps://unsloth.ai/blog/deepseekr1-dynamic",
                  "score": 1,
                  "created_utc": "2026-01-13 17:41:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdqd9c",
          "author": "ljl87",
          "text": "\"Affordable 32gb GPU\" ok bro ðŸ˜‚",
          "score": -1,
          "created_utc": "2026-01-13 17:06:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzh85mt",
              "author": "ttkciar",
              "text": "We already have those (MI60 and upgraded MI50), but OP said \">\" (greater than) 32GB, meaning more memory than that.\n\nSo, affordable 48GB or 64GB or similar.",
              "score": 1,
              "created_utc": "2026-01-14 03:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdpxsi",
          "author": "Few_Painter_5588",
          "text": "Apparently Deepseek V4 is going to launch with a lite version.",
          "score": 0,
          "created_utc": "2026-01-13 17:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzems8d",
          "author": "Embarrassed-Net-5304",
          "text": "Lmao \nDude thinks this is Christmas",
          "score": -1,
          "created_utc": "2026-01-13 19:33:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7qcux",
      "title": "The NO FAKES Act has a \"Fingerprinting\" Trap that kills Open Source. We need to lobby for a Safe Harbor.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "author": "PostEasy7183",
      "created_utc": "2026-01-08 22:33:33",
      "score": 610,
      "num_comments": 87,
      "upvote_ratio": 0.96,
      "text": "Hey everyone,\nâ€‹Iâ€™ve been reading the text of the \"NO FAKES Act\" currently in Congress, and itâ€™s worse than I thought.\nâ€‹The Tldr: It creates a \"digital replica right\" for voices/likenesses. That sounds fine for stopping deepfake porn, but the liability language is a trap. It targets anyone who \"makes available\" a tool that is primarily used for replicas.  \nâ€‹The Problem: If you release a TTS model or a voice-conversion RVC model on HuggingFace, and someone else uses it to fake a celebrity, you (the dev) can be liable for statutory damages ($5k-$25k per violation).\nâ€‹There is no Section 230 protection here. This effectively makes hosting open weights for audio models a legal s*icide mission unless you are OpenAI or Google.\n\nWhat I did:\nI contacted my reps email to flag this as an \"innovation killer.\" If you run a repo or care about open weights, you might want to do the same. We need them to add a \"Safe Harbor\" for tool devs.\n\nS.1367 - 119th Congress (2025-2026): NO FAKES Act of 2025 | Congress.gov | Library of Congress https://share.google/u6dpy7ZQDvZWUrlfc\n\nUPDATE: ACTION ITEMS (How to actually stop this)\nâ€‹If you don't want to go to jail for hosting a repo, you need to make noise now.\nâ€‹1. The \"Lazy\" Email (Takes 30 seconds):\nGo to Democracy.io or your Senatorâ€™s contact page.\nâ€‹Subject: Opposition to NO FAKES Act (H.R. 2794 / S. 1367) - Open Source Liability\nâ€‹Message: \"I am a constituent and software engineer. I oppose the NO FAKES Act unless it includes a specific Safe Harbor for Open Source Code Repositories. The current 'Digital Fingerprinting' requirement (Section 3) is technically impossible for raw model weights to comply with. This bill effectively bans open-source AI hosting in the US and hands a monopoly to Big Tech. Please amend it to protect tool developers.\"\nâ€‹2. The \"Nuclear\" Option (Call them):\nâ€‹Call the Capitol Switchboard: (202) 224-3121\nâ€‹Ask for Senators Wyden (D) or Massie (R) if you want to thank them for being tech-literate, or call your own Senator to complain.\nâ€‹Script: \"The NO FAKES Act kills open-source innovation. We need a Safe Harbor for developers who write code, separate from the bad actors who use it.\"",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7qcux/the_no_fakes_act_has_a_fingerprinting_trap_that/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nyk2sfk",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-09 07:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhm9rk",
          "author": "Revolutionalredstone",
          "text": "Making your own devs liable is how you turn your country into a third world nation.\n\nPeople who make it easy to USE the tools are the only ones who should be liable.\n\nThere are plenty of countries which won't play these silly blame games and their devs will keep releasing all their stuff either way.\n\nDevs are the inventors of ideas and making them liable for how others missuse them just cuts you off from new ideas completely, what we need todo is make operators / sites / places the normal people go to use the less desirable filters and tech liable (Instagram etc)\n\nAlso dev software licenses say you can't misuse their tech etc, so it's a joke to pretend they are in the wrong if users abuse their license.\n\nThat's a bit like holding petrol companies liable for people who stupidly try to throw bottles of gasoline onto fires:\n\nhttps://www.youtube.com/watch?v=3l50QZiPwnY\n\nEverything can be abused / used in a destructive way / used other than - intended usage.\n\nPowerful open source technologies always win and if your country is not compatible with openness then it's gonna get left behind (think north korea starving and surviving on cracked old builds of windows xp)",
          "score": 158,
          "created_utc": "2026-01-08 22:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhnibi",
              "author": "PostEasy7183",
              "text": "You hit the nail on the head regarding 'Innovation Flight.' If this passes, the US effectively sanctions its own AI sector, and the bleeding edge moves to countries with better safe harbors. We become a digital backwater.\nâ€‹The Dangerous Part: You are right that 'Operators' (those who wrap the tool in a UI) should be the target, not the 'Inventors' (who write the code).\nâ€‹But the Bill doesn't make that distinction.\nâ€‹The current text of NO FAKES defines 'making available' a digital replica tool as a liability trigger. It doesn't distinguish between a 'Click-to-Fake' app and a raw Python script on GitHub.\nâ€‹That is exactly the amendment we are pushing for: Distinguish 'Active Service Providers' from 'Tool/Code Repositories.'\nâ€‹If you haven't yet, drop a line to your Rep and tell them: 'We need a Safe Harbor for Code, or innovation leaves the US.' They need to hear the economic argument you just made.",
              "score": 28,
              "created_utc": "2026-01-08 22:50:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhpkse",
                  "author": "Revolutionalredstone",
                  "text": "Ta! A digital backwater sounds hopeful TBH; more like nazi hell hole.\n\nCouldn't agree more: Service Providers are the piracy host equivalents here; the closest thing to devs / inventors might be the guys who created screens or cameras or modems (clearly just inventing != supporting abuse)\n\nI don't think a safe harbor clause is coming but IMHO it's not a real problem; this is the kind of law that's passed to make Karen's feel good; it is not very likely to ever translate into anything in the real actual world.\n\nAustralia (my home country) outlawed encryption 'The laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia\" (we have very low intelligence within our politics)\n\nObviously encryption is absolutely fundamental to modern computer technology and gets used everywhere everyday all the time, but hey, at least we 'successfully outlawed it' ;D\n\nI suspect a lot of laws are like this; existing on paper, but only really there to quell the dumb.",
                  "score": 9,
                  "created_utc": "2026-01-08 23:00:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykutrn",
                  "author": "mycall",
                  "text": "I do agree with your digital backwater point but I'll raise you that other factors already in play and are worse than this.  All the same, less is better and the bill needs some modifications.",
                  "score": 2,
                  "created_utc": "2026-01-09 11:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyjf6qr",
              "author": "Bakoro",
              "text": ">People who make it easy to USE the tools are the only ones who should be liable.   \n    \nSo who is going to pay the developers?   \n   \nThere's no perfect solution here, either we hold back the entirety of human development because some people can't be responsible, or we accept that freedom comes with danger and do what we can to minimize risk and minimize the harm to others.\n   \nPeople hyper focus on the tools, when we need to take a step back and ask why people even want to abuse the tools.  \n   \nMaybe if we didn't have such a hostile society where people could become homeless, or die from preventable causes because they don't have enough money, then we would not have so many people spending their time and energy figuring out how to exploit others.  \n  \nMaybe if people got the mental and medical care they needed, they wouldn't turn to drugs.   \n   \nMaybe if people were guaranteed a decent living they wouldn't rob, steal, and murder so much.  \n   \nInstead of looking to fix problems, people think \"who can I punish?\"",
              "score": 7,
              "created_utc": "2026-01-09 04:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykjw9y",
                  "author": "Revolutionalredstone",
                  "text": "Sounds harsh my dude, one heart to another take it easy out there ;D\n\nI probably come from a very lucky place without such problems, but it is interesting to look behind the veil and realize a lot of these issues, maybe most, are less about creative meaningful technological misuse, and are more like just outbursts in one form or another, where people use whatever they can grab to hurt one another.\n\nIt's a sobering view, thank you! (kind of glad the nukes have codes after reading that lol)\n\nHere's looking forward to a world where everyone lets local self run AI etc help them find their way over such obstacles\n\nEnjoy",
                  "score": 2,
                  "created_utc": "2026-01-09 10:00:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyhy8ex",
              "author": "WeMetOnTheMountain",
              "text": "I respectfully disagree.  People that USE the tools to break the law should hold the liability.  That could be a third party, or an end user.",
              "score": 38,
              "created_utc": "2026-01-08 23:44:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyi22wp",
                  "author": "CryptoCryst828282",
                  "text": "If they are breaking the law, by definition, there are no changes needed.... you are asking for expanding the law...",
                  "score": 17,
                  "created_utc": "2026-01-09 00:04:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykml09",
                  "author": "g_rocket",
                  "text": "That's already illegal, but hard to prosecute at scale.",
                  "score": 1,
                  "created_utc": "2026-01-09 10:24:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyi6cwk",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-09 00:25:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyk1e2e",
              "author": "Mythril_Zombie",
              "text": "\n>Making your own devs liable is how you turn your country into a third world nation.\n\n>People who make it easy to USE the tools are the only ones who should be liable.    \n    \nMake up your mind. Developers make their tools as easy to use as they can. \n\nWho do you think makes these tools easy to use?  Come on, you can do it. It starts with a \"D\".  \"Dev...\"  Say it with me... I know you can do it.",
              "score": 2,
              "created_utc": "2026-01-09 07:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nykkg3f",
              "author": "-dysangel-",
              "text": "I don't think making software easy to use should make you liable for anything done with it. Maybe something more like Suno where copyrighted songs are detected and blocked from the service.",
              "score": 1,
              "created_utc": "2026-01-09 10:05:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyk05ng",
              "author": "jglazer",
              "text": "Itâ€™s a new world- everything on the net is â€œeasy to USEâ€ because Gemini-cli or Claude will just pull it down and use it for you. There is no more need for fancy ui - and no possibility of drawing the line between inventing something and making it user friendly.  We canâ€™t legislate away the use of fake tech anymore than we can legislate away the pencil that you might use to write libel about someone.",
              "score": 0,
              "created_utc": "2026-01-09 07:02:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyif4r5",
          "author": "davedcne",
          "text": "Honest question, do you think your rep even understood what you were trying to explain to them? I think most of our politicians are so out of touch with technology that its like trying to teach a cave man calculus.",
          "score": 17,
          "created_utc": "2026-01-09 01:11:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhobge",
          "author": "jferments",
          "text": "This has been the point of the astro-turf \"anti-AI\" movement all along. I firmly believe that big tech corporations like Google, Microsoft, and OpenAI are behind the bots spreading \"anti-AI\" propaganda that supports laws that will essentially centralize control of AI and make open-source AI illegal.",
          "score": 83,
          "created_utc": "2026-01-08 22:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhoym7",
              "author": "PostEasy7183",
              "text": "You are describing the classic 'Baptists and Bootleggers' coalition.\nâ€‹The Baptists: The 'anti-AI' activists screaming about 'stealing art' (the moral cover).\nâ€‹The Bootleggers: Big Tech companies who quietly support these laws because they know compliance costs will bankrupt their open-source competitors (the economic profit).\nâ€‹Itâ€™s not a conspiracy; itâ€™s just standard regulatory capture. OpenAI and Google want high regulation because it builds a moat that you and I can't cross.\nâ€‹The Counter-Move: When you write to your Senator, point this out directly. Tell them: 'This bill is being pushed by Big Tech to kill small competitors under the guise of safety.' Senators hate feeling like they are being played by Silicon Valley lobbyists. Use that against them.",
              "score": 34,
              "created_utc": "2026-01-08 22:57:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyilkl5",
                  "author": "This_Organization382",
                  "text": "Although you're repeating what the poster said, I don't think it's fair to include Google here. They have released numerous open-source models, and typically support open-source projects. Let's not forget that the whole rat race of AI was started from Google open-sourcing their research.\n\nAdditionally, Google has the hardware; they aren't entirely dependent on their LLM succeeding.\n\nOpenAI, and Anthropic on the other hand: absolutely. They are very blatantly trying to kill open-source, open-weight ventures.",
                  "score": -7,
                  "created_utc": "2026-01-09 01:46:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyi30y6",
              "author": "GamerHaste",
              "text": "Honestly I can see that being the case... fucked up world we're heading into. Open source AI is basically the only light I see in the tunnel, would be a fucking shame to lose it. Already getting hard enough for devs to work on open source AI projects given that it appears the entire hardware manufacturing industry is basically gunning towards a \"rent hardware in the cloud\" model for consumers.",
              "score": 13,
              "created_utc": "2026-01-09 00:09:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyl6769",
                  "author": "Batetrick_Patman",
                  "text": "Youâ€™ll own nothing and like it.",
                  "score": 3,
                  "created_utc": "2026-01-09 12:54:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nykv772",
              "author": "Novel-Mechanic3448",
              "text": "As someone working at a hyperscaler it simply doesn't work that way. It really is just retards and there's a lot of them",
              "score": 2,
              "created_utc": "2026-01-09 11:37:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhkxhr",
          "author": "Aromatic-Low-4578",
          "text": "Don't most software licenses already try to protect the developer from liability due to users?  Will be interested to see how it plays out.",
          "score": 12,
          "created_utc": "2026-01-08 22:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhlj9u",
              "author": "PostEasy7183",
              "text": "This is a common misconception, but unfortunately it's wrong.\nâ€‹Contract vs. Statute: An MIT/Apache license is just a contract between you and the user. It does not (and cannot) protect you from Federal Statutory Liability. If the NO FAKES Act makes 'making available' a tool illegal under federal IP law, your 'AS IS' clause is irrelevant. You can't contract your way out of federal law.\nâ€‹Third-Party Standing: The people suing you under NO FAKES aren't your users; they are third parties (estates, record labels). They never agreed to your license.\nâ€‹The 'Tool' Liability: The bill specifically creates liability for those who 'make available' technology primarily designed for replicas. It doesn't care about your license terms.  \nâ€‹The Real Kicker: The bill requires 'digital fingerprinting' for Safe Harbor. An open-source repo of raw weights technically cannot comply with that. So even if you wanted to be safe, you can't be.",
              "score": 30,
              "created_utc": "2026-01-08 22:40:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhr0pn",
                  "author": "Aromatic-Low-4578",
                  "text": "Appreciate the explanation, thanks!",
                  "score": 7,
                  "created_utc": "2026-01-08 23:07:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyhrbg8",
                  "author": "Beautiful-Maybe-7473",
                  "text": "Can you elaborate on the \"digital fingerprinting\" exemption? I assume this if you embed a \"this is a fake\" flag in your output? I guess then the consequence is that to benefit from this exemption you have to bundle fingerprint functionality along with model weights, even though these are separate concerns and different layers of a stack, from a software engineering perspective.",
                  "score": 1,
                  "created_utc": "2026-01-08 23:08:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhtae5",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 20,
          "created_utc": "2026-01-08 23:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylgj3z",
              "author": "plus-minus",
              "text": "So it targets the publication of things like celebrity LoRAs mostly?",
              "score": 2,
              "created_utc": "2026-01-09 13:53:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyhuagm",
              "author": "PostEasy7183",
              "text": "I appreciate you citing the specific clauses. You are reading the 'Black Letter Law' correctly, but you are missing the Litigation Reality (how this actually plays out in court).\nâ€‹The 'Primarily Designed' Trap: You argue this only targets a specific 'Arnold Bot.'\nâ€‹If I release a generic Voice Cloning tool, and 90% of my community uses it to clone celebrities (which is the current state of RVC/SVC), a court can easily rule that the tool is 'primarily designed' for that purpose based on use patterns, not just my marketing.\nâ€‹'Actual Knowledge' vs. 'Willful Blindness'\nâ€‹In IP law (see MGM v. Grokster), 'Actual Knowledge' isn't just a signed confession. If my GitHub issues or Discord are full of people asking 'How do I clone Taylor Swift?', and I don't ban them, I have 'Constructive Knowledge' or am engaging in 'Willful Blindness.'\nâ€‹The Smoking Gun (Safe Harbor)\nâ€‹This is the part you missed. Even if I don't have knowledge, to get the lawsuit dismissed quickly, I need the Safe Harbor.\nâ€‹The bill (Sec 3) conditions Safe Harbor on using 'Digital Fingerprinting' to filter content.\nâ€‹The Problem: A repo hosting raw .pth or .safetensors model weights cannot 'fingerprint' the potential output of those weights.\nâ€‹Result: Open Source repos are technically disqualified from the Safe Harbor. We are left naked in court while YouTube gets a shield.\nâ€‹That is why we are worried. Itâ€™s not about the 'Arnold Bot'; itâ€™s about the fact that GitHub repos can't afford the compliance layer to prove they aren't liable.",
              "score": 7,
              "created_utc": "2026-01-08 23:23:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyi4lft",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 11,
                  "created_utc": "2026-01-09 00:16:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyptw15",
          "author": "aidencoder",
          "text": "Let's start holding gun manufacturers liable for murders too then eh?Â ",
          "score": 4,
          "created_utc": "2026-01-10 02:32:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyioqq8",
          "author": "timschwartz",
          "text": "Just frame it like guns:\n\nModels don't deepfake people, people with models deepfake people.",
          "score": 6,
          "created_utc": "2026-01-09 02:03:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyk1wmq",
              "author": "Dadda9088",
              "text": "And models will be more controlled than guns...",
              "score": 0,
              "created_utc": "2026-01-09 07:17:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyile2z",
          "author": "ortegaalfredo",
          "text": "If you think about it, It's way more disturbing than you think:\n\nThey don't want to criminalize porn, they want to criminalize FAKE porn, why? because they need to be in control of the porn generation, so men and particularly young men can be controlled with it.",
          "score": 8,
          "created_utc": "2026-01-09 01:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym3ob9",
              "author": "Hizonner",
              "text": "That's... unusually paranoid...\n\n... and has the usual conspiracy theory flaw of requiring some improbably unified and implausibly competent \"them\".\n\nOh, and, on edit, also assumes you can \"control\" somebody through porn in any way that's actually useful to anybody.",
              "score": -2,
              "created_utc": "2026-01-09 15:45:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymez37",
                  "author": "Mediocre-Method782",
                  "text": "Shared values and common experience are sufficient to produce rational, purposive action without explicit coordination. System justifiers want us to make-believe otherwise so that we don't break their toy.\n\nAnd wouldn't the induction of hypofrontality through erotic duress make reconditioning relatively easier? I mean, it's the basic principle of the Roman Church's reproduction.",
                  "score": 3,
                  "created_utc": "2026-01-09 16:36:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyitkq2",
          "author": "Acceptable_Home_",
          "text": "US and tech bros are actively targeting open source modelsÂ ",
          "score": 4,
          "created_utc": "2026-01-09 02:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj9kv2",
          "author": "lisploli",
          "text": "Just label your model as non-US version. Linux distributions have done that for decades before those ridiculous encryption laws were removed. e.g. [debian](https://web.archive.org/web/20050514004108/http://www.debian.org/CD/faq/#nonus).",
          "score": 5,
          "created_utc": "2026-01-09 03:56:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyin0jf",
          "author": "SilentLennie",
          "text": "Don't know if it matters in practice what they propose. US politics is such a mess and the business interests are so 'great' they might prevent it being passed or no enforcement will happen (regulatory capture).",
          "score": 2,
          "created_utc": "2026-01-09 01:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykujo0",
          "author": "mycall",
          "text": "What about posting the open weights in different countries and be done with the problem?  Bittorrents are still popular and unstoppable.",
          "score": 2,
          "created_utc": "2026-01-09 11:31:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyld1gl",
          "author": "FloranceMeCheneCoder",
          "text": "Hot take, this has always been the long term goal due to the ability for people to create products without being traced",
          "score": 2,
          "created_utc": "2026-01-09 13:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhq9v2",
          "author": "Technical_Ad_440",
          "text": "i doubt it will pass sounds like a complete dud. the top ai will fight this cause it would kill all their tools in that case. personally for me i would rather have tons of fakes and misinfo why? cause in the future those that do due diligence will be the people you want to keep around those that dont fall for the missinfo and use tools to check origins and such they are the people you want. either as fans, work colleagues etc. \n\nwe need more and more fakes and we need people to start checking stuff not banning the tools. the future people are gonna be so hardened against all the missinfo and such that it wont make much difference. it actually baffles me why people dont want to flood fakes and such to just go oh yeh that incident it was AI and move on. its like a shield against most the normal attacks people do and actually weakens the attack vectors people have. kinda like how if people know where someone is people cant dox them cause people already know. trying it just makes the person go well people already know.\n\nyou cant trust most things anymore anyways so if i see things \"exposing\" stuff i just assume its probably AI until otherwise confirmed. it also means mundane attacks in the future are just gonna fall flat completely like if it doesnt affect you you will not care whatsoever",
          "score": 6,
          "created_utc": "2026-01-08 23:03:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhub4r",
              "author": "DerFreudster",
              "text": "A blue check-mark will accompany all Taylor Swift's entrees on the web henceforth!\n\nBut yeah, there's no stopping AI slop and it ain't going to stop because some clueless chucklehead passes a law. One good outcome might be that people veer away from the web because while it is already fake, it will become so completely and utterly fake that IRL might gain traction again!",
              "score": 2,
              "created_utc": "2026-01-08 23:23:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyhr3od",
              "author": "PostEasy7183",
              "text": "I agree with your 'plausible deniability' theory eventually, we will all just assume everything is fake until proven real. Thatâ€™s the long-term reality.\nâ€‹But you are wrong about Big Tech fighting this.\nâ€‹That is the dangerous assumption. Google and the Recording Academy are actually supporting this bill (check the co-sponsor/supporter lists).\nâ€‹Why? Because they can afford the 'digital fingerprinting' and licensing teams. They want this law to pass because it builds a regulatory moat that kills Open Source competition.\nâ€‹If this passes, 'Top AI' companies will be the only ones allowed to play. The rest of us won't be able to 'flood' anything because the open tools will be sued into oblivion. Don't count on OpenAI to save us; they are the ones handing Congress the pen.",
              "score": 0,
              "created_utc": "2026-01-08 23:07:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyhxwfi",
                  "author": "Technical_Ad_440",
                  "text": "then china know the cards they played and flood the market with actually high quality open source to get back at them, the outside world will just use china models while US regresses. but the big issue here isn't even LLMS its reaching AGI. all these models die when you have an agi companion that can do all this and then some. we will just give our agi companions a youtube video and it will learn to do all the stuff. i think agi is gonna be really important for the people to have to stop an AI uprising to so if they try not to give out agi then everything will indeed be messed up. \n\nbut for llms i think there is to much investment into the AI sector in general for it to pass. recording academy thats voices and such they are done if it passes cause the outside world will just hire you know cheaper stuff if they have to pay fingerprinting elsewhere is guaranteed to be cheaper. looks like it hasnt moved to so thats good for you guys. this is probably all the older stuff they wanted to do for AI. Just hope agi gets here soon cause agi cant exist with a lot of the stuff governments are trying to pass. maybe we will be in that dystopia where bringing a robot online causes the police to rush out to the house.",
                  "score": 7,
                  "created_utc": "2026-01-08 23:42:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyhvrpq",
          "author": "WristbandYang",
          "text": "OP only responds using AI. This is bot behavior.",
          "score": 6,
          "created_utc": "2026-01-08 23:31:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhwwyt",
              "author": "PostEasy7183",
              "text": "Guilty as charged sir. I use a LLM to clean up the drafts and organize legal points. I'm juggling this thread, work, and looking into other issues that matter to me. The irony shouldn't fall on you that I'm using the tool we are trying to save here + (This is a pro AI forum is it not?). If no fakes passes with the liability currently baked into it using an open-source model becomes risky to the host. Now, regarding the actual argument about the Safe Harbor clause: Do you disagree with the analysis, or just the formatting?\"",
              "score": -7,
              "created_utc": "2026-01-08 23:37:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyi4mfx",
                  "author": "trashk",
                  "text": "So, you are just doing this thing whole AI slop thing half assed while you work, play with the kids, make dinner, perform surgery, look up memes and then saying \"I got all this useless drivel done with the tools I'm trying to save!\"?\n\nHow much law did you actually study or is all this shit just what GPT told you?",
                  "score": 2,
                  "created_utc": "2026-01-09 00:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykd7zv",
          "author": "markeus101",
          "text": "Pff its just america..they are backwards anyway",
          "score": 1,
          "created_utc": "2026-01-09 08:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyks67d",
          "author": "a_beautiful_rhind",
          "text": "So they only move on this because of all the political memes/videos where they make fun of *them*. \n\nImo, this is the new political satire and should now be protected speech.",
          "score": 1,
          "created_utc": "2026-01-09 11:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl9ndd",
          "author": "FairYesterday8490",
          "text": "Don't worry. China is rising.",
          "score": 1,
          "created_utc": "2026-01-14 19:06:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlolaa",
              "author": "PostEasy7183",
              "text": "I just tried out Qwen today and yeah I am actually kind of impressed with the Chinese tech at the moment. If you have any other models that can compete with American tech I would be interested to check it out.",
              "score": 1,
              "created_utc": "2026-01-14 20:14:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhzwpj",
          "author": "AutomaticDriver5882",
          "text": "I would just have different developers, make different pieces of the code and then you would snap in the modules so no one person is responsible but maybe thatâ€™s an oversimplification",
          "score": 1,
          "created_utc": "2026-01-08 23:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylx3ag",
          "author": "Mediocre-Method782",
          "text": "That this is a bot post suggests to me that the real purpose is to generate legislative motion on a bill that might have died in committee or been forgotten.",
          "score": 1,
          "created_utc": "2026-01-09 15:15:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylz9tk",
              "author": "PostEasy7183",
              "text": "Technically speaking it has been forgotten pretty much since last year (April 2025). It is of my perspective right now that when / if it ever does get back off the ground that it's better to have the open source community asses covered rather than not. If Google, open AI, and meta among others are talking to lawmakers about covering their asses why shouldn't we?",
              "score": 2,
              "created_utc": "2026-01-09 15:25:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhwkd3",
          "author": "grady_vuckovic",
          "text": "I would love to hear everyone's suggestion for what the alternative is to stop the deep fakes?\n\nThis seems to make it pretty simple. If you're a developer and you release a tool that can be used to impersonate people, you're responsible.\n\nSo let's say you're all against that, alright, fair, what's the next step? What laws, what enforcement, how do we stop this?\n\nBecause \"we just have to learn to live with it\" is not gonna fly.",
          "score": -7,
          "created_utc": "2026-01-08 23:35:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiktzt",
              "author": "misterflyer",
              "text": "I think we've all seen this coming.\n\nIn general, across the board, I just think there needs to be more watermarking/signaling for what's AI generated content.\n\nI see a lot of videos that look real at first. But because I'm an AI enthusiast, I can take a closer look and come to the suspicion that its AI generated content.\n\nMy Mother and my Grandmother on the other hand would 100% believe that a lot of the AI stuff I casually see on a daily basis is real.  We definitely don't want to reach a place in society where the average person can't tell between what's real and what's not.\n\nSo I think it goes beyond just celebrity impersonation... which is actually fairly easy to pick up on bc a lot of the celebrity generated content is so out of pocket *(or just for humor)* that no one's gonna believe it anyway.\n\nBut on youtube, for instance, I just wish there was a marker that signaled: `this content was generated using AI` or something along those lines. That wouldn't completely fix the situation, but it would be a great start.  And it's better than legally stifling open weights innovation/development completely.\n\nIt's not the celebrity stuff that worries me. It's the ordinary stuff that's borderline believable that's AI generated but very hard to tell that it's not actually real.\n\nAnd I think what the bill is trying to get at that a lot of ppl are missing is they want to hear that there's a good reason for this type of technology other than deepfakes and *doing it for the LULs*.  And I think a lot of people are gonna struggle with **coming up with convincing reasons to the lawmakers in terms of good applications for primary use.**",
              "score": 3,
              "created_utc": "2026-01-09 01:42:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyiuo1t",
                  "author": "Mediocre-Method782",
                  "text": ">But on youtube, for instance\n\n\"This content was altered or generated\" warnings are in the video description box, but collapsed by default. I wish they were badged in channels or search, like the CC logo is for subtitles.",
                  "score": 5,
                  "created_utc": "2026-01-09 02:34:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nym2g8j",
                  "author": "Hizonner",
                  "text": "> We definitely don't want to reach a place in society where the average person can't tell between what's real and what's not.\n\nWe apparently reached that ages ago. Arguably before any of us were born, definitely before \"AI\". People know that documents can be faked. People know that *photos* can be faked, and will quickly learn, if they already haven't, that it's gotten easier to do that than it used to be. Same for video.\n\nThere's stuff that's harmful, at least emotionally, even if the viewer knows it's fake. But trying to make it impossible to *produce* fakes, or to require that every fake be tagged somehow, is an absolutely pointless waste of time. You won't even reduce the prevalence enough to matter, let alone actually eliminate it, so people are going to *have* to develop some immunity.",
                  "score": 1,
                  "created_utc": "2026-01-09 15:40:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyirifi",
              "author": "Technical_Aside_3721",
              "text": "> Because \"we just have to learn to live with it\" is not gonna fly.\n\nWhy not? ( Genuine question ) \n\nAside from scale, what is the difference between me drawing a picture by hand of Hillary Clinton in a leopard print bikini and me using SDXL to generate it? \n\nI get that if you _are_ Hillary Clinton, that you think it's icky, weird, and distasteful but what is the difference making the SDXL tool illegal to run, but me drawing a picture totally fine? \nAnd maybe you could broaden the interpretation of _defamation_ / _harassment_  to include sharing deepfakes. But it seems like this legislation is going after the paper factories for people writing unsavory things.",
              "score": 2,
              "created_utc": "2026-01-09 02:17:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyiiw3g",
              "author": "Mediocre-Method782",
              "text": "You let victims put the boot of the law up the ass of the people who bring the ill intent into the world. As this bill does in its current form.\n\nI always laugh my ass off when some lobbyist has their robot write some indignant diatribe in favor of corporate censorship, imagining that hyperventilating and crying in public like a self-important four year old about \"being copied\". People need to stop serving you at restaurants.",
              "score": 0,
              "created_utc": "2026-01-09 01:31:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhroca",
          "author": "PostEasy7183",
          "text": "Comments are coming in a lot quicker than I thought tonight. Please be patient and I will try to get back with you as many of you as I can in a couple hours. Thank you and make sure to bump the thread so this gets attention. Ensure to write to your reps or contact them with any other means necessary.",
          "score": -2,
          "created_utc": "2026-01-08 23:10:40",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nyhubjk",
          "author": "Marksta",
          "text": ">a tool that is **primarily** used for replicas.\n\nIsn't the primarily part key here? Because everytime someone advertises a TTS related thing, they bold 1000 times EAZY ONE SHOT VOICE REPLICATION as if that's its only use. This thing passes, then just take that out of your description and add a warning that you're expected to own the rights to any likeness you're duplicating.\n\nThis worry is pretty same as Photoshop being able to edit images you don't own. Photoshop doesn't spam 1000 times that you can steal copyright and must have written somewhere that copyright laws are your own problem somewhere.\n\nSo all that's left is probably actual heinous sites on the chopping block who should definitely be liable if their advertising is spamming how easy their tool will allow you to rip off likenesses and generate infringing content.",
          "score": -2,
          "created_utc": "2026-01-08 23:23:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiy5ut",
              "author": "CheatCodesOfLife",
              "text": "If I make my tts voice cloning model require the reference audio to say â€œthe owner of this voice consents to the following reproduction of their voice â€œ. Would that be fine?\n\nCould pretty easily retrain it to only work this way.",
              "score": 2,
              "created_utc": "2026-01-09 02:53:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyjon6f",
              "author": "Apprehensive-File251",
              "text": "Slightly off topic, but i really hate that \"voice cloning\" is the only thing that people seem to talk about when it comes to tts models. \n\nI want tts models that are unique. I am baffled that everyone seems to want to clone celebrities or other.   Give me a voice that has a range of natural infections, emotional expressions, and is based off people who signed up to be used that way any day.  I dont want to get confused when some clip may be playing or someone else I cloned may be talking. \n\nBut search this subredit for tts models and  *everything*  is real voice cloning etc etc.",
              "score": 2,
              "created_utc": "2026-01-09 05:32:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjorwm",
          "author": "GrogRedLub4242",
          "text": "folks shouldnt be working on GenAI anyway. madness",
          "score": -6,
          "created_utc": "2026-01-09 05:33:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc9m6x",
      "title": "GLM-Image is released!",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/zai-org/GLM-Image",
      "author": "foldl-li",
      "created_utc": "2026-01-14 01:17:16",
      "score": 587,
      "num_comments": 83,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nzickbd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-14 09:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgkrns",
          "author": "o0genesis0o",
          "text": "13GB diffusion model + 20GB text encoder.\n\nWaiting for some kind souls to quantize this to fp8 and train some sorts of lightning LoRA before I can try this model.",
          "score": 110,
          "created_utc": "2026-01-14 01:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgnuh4",
              "author": "a_beautiful_rhind",
              "text": "You can probably compress the text encoder fairly well. There was that other model which was 90% LLM and very little diffusion.",
              "score": 32,
              "created_utc": "2026-01-14 01:40:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhbvof",
              "author": "DataGOGO",
              "text": "Already started it",
              "score": 8,
              "created_utc": "2026-01-14 04:00:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgx8j2",
              "author": "MikeLPU",
              "text": "gguf when ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 21,
              "created_utc": "2026-01-14 02:33:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjqc1w",
                  "author": "martinerous",
                  "text": "This time not qwen....",
                  "score": 1,
                  "created_utc": "2026-01-14 14:55:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzglp2u",
              "author": "silenceimpaired",
              "text": "Oh that fits nicely on two 3090â€™s",
              "score": 13,
              "created_utc": "2026-01-14 01:28:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzisa9k",
                  "author": "lumos675",
                  "text": "The model itself is realy small.the transformer size in fp32 is 14gb which means in fp8 it must be near 4 to 5 gb. Fhe text encoder being 23gb is in fp32 so realisticly in fp8 must be nearly 8gb. So i bet everyone can use this model even with 8gb of ram",
                  "score": 12,
                  "created_utc": "2026-01-14 11:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziocid",
              "author": "GregoryfromtheHood",
              "text": "How much VRAM does this translate to? Could I run it with a 32GB 5090 for the text encoder and a 24GB 3090 for the diffusion model or something?",
              "score": 3,
              "created_utc": "2026-01-14 10:51:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh6y17",
          "author": "TennesseeGenesis",
          "text": "Works in SD.Next in UINT4 SDNQ in around 10GB VRAM and 30GB'ish RAM. Just added support, PR should be merged in a few hours.",
          "score": 58,
          "created_utc": "2026-01-14 03:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgm1c1",
          "author": "cms2307",
          "text": "Wow it scores around the same on benchmarks as nano banana 2, if thatâ€™s true than this is a huge deal. Also the fact itâ€™s editing and generation in one is awesome.",
          "score": 144,
          "created_utc": "2026-01-14 01:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgocrk",
              "author": "redditscraperbot2",
              "text": "If itâ€™s too good to be trueâ€¦",
              "score": 44,
              "created_utc": "2026-01-14 01:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzgpiqa",
                  "author": "simracerman",
                  "text": "Idk, z.ai did some miracles last year. Maybe this is their first for 2026.",
                  "score": 89,
                  "created_utc": "2026-01-14 01:50:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzgwazd",
                  "author": "-dysangel-",
                  "text": "Have you tried any GLM models since 4.5/4.5 Air? They are seriously impressive - both for their size, and in general",
                  "score": 47,
                  "created_utc": "2026-01-14 02:28:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzifiw9",
              "author": "lmpdev",
              "text": "Only on text rendering benchmark, and they are not comparing it to Nana Banana Pro. It's worse with text than flux.2 in my tests.",
              "score": 6,
              "created_utc": "2026-01-14 09:29:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhgi9e",
              "author": "RuthlessCriticismAll",
              "text": "> Wow it scores around the same on benchmarks as nano banana 2\n\nNo it doesn't. People think benchmarks are meaningless exclusively because they are completely unable to read them.",
              "score": 6,
              "created_utc": "2026-01-14 04:30:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgzzpe",
              "author": "HenkPoley",
              "text": "I guess, [similar to their GLM 4.x releases](https://eqbench.com/creative_writing_longform.html), they trained it on a mass of data from the best chatbots. Click the (i) in the 'Slop' column to see these top matches:\n\n* GLM-4.5 = DeepSeek-R1-0528\n* GLM-4.6 = DeepSeek-V3.1 / -V3.2-Exp\n* GLM-4.7 = gemini-3-pro-preview\n\nThey may have made some system to efficiently decide which is the best chat log to train on, how to reverse engineer training data sources, and the best prompts to get good chat logs.",
              "score": 7,
              "created_utc": "2026-01-14 02:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhwnce",
                  "author": "Keep-Darwin-Going",
                  "text": "That is basically distilling right? Nothing wrong with that except breaking tos.",
                  "score": 9,
                  "created_utc": "2026-01-14 06:33:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh9w93",
                  "author": "Aromatic-Low-4578",
                  "text": "What's your basis for this claim?  Find it hard to believe they could get a meaningful amount of tokens from gemini 3 pro in the last few months it's been available.",
                  "score": 23,
                  "created_utc": "2026-01-14 03:47:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzi2ro8",
              "author": "R_Duncan",
              "text": "It scores similar to Qwen-Image",
              "score": 0,
              "created_utc": "2026-01-14 07:27:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgmcbh",
          "author": "smith7018",
          "text": "Will absolutely reserve judgement but the sample images donâ€™t scream SOTA to me. A lot of 1girl, scenery, and generic landscapes. The text looks great, though.",
          "score": 46,
          "created_utc": "2026-01-14 01:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgo04l",
              "author": "a_beautiful_rhind",
              "text": "Text a mostly solved problem since flux.",
              "score": 14,
              "created_utc": "2026-01-14 01:41:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh1nm1",
                  "author": "SanDiegoDude",
                  "text": "Not for dense text. Generating a diagram with accurate images and labels, or even a comic book panel with accurate dialogue dispersed the whole way through is very difficult, even for SOTA models like NB2. Their examples are quite impressive, and I'm excited to see how complex the typography can get before it starts to fall apart. In comparison, even having a single paragraph of text in Qwen and it falls apart pretty hard.",
                  "score": 31,
                  "created_utc": "2026-01-14 02:58:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh06yn",
                  "author": "inaem",
                  "text": "Only English, Chinese still sucks, so still a lot of work for these companies",
                  "score": 5,
                  "created_utc": "2026-01-14 02:50:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzgnf12",
          "author": "-p-e-w-",
          "text": "MIT license again, with no ifs and buts. Makes the Western labs look ridiculous when they publish inferior models under restrictive licenses.",
          "score": 158,
          "created_utc": "2026-01-14 01:38:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgozuu",
              "author": "eli_pizza",
              "text": "Itâ€™s great! But of course a permissive license only helps so much without the training data, tooling,  etc",
              "score": 16,
              "created_utc": "2026-01-14 01:47:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgwp3q",
              "author": "LocoMod",
              "text": "EDIT: Nevermind. You're not talking private cloud models. I misunderstood.\n\nAgreed.",
              "score": 0,
              "created_utc": "2026-01-14 02:30:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgmy3w",
          "author": "HistorianPotential48",
          "text": "is porn doable",
          "score": 102,
          "created_utc": "2026-01-14 01:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzh1e6c",
              "author": "twavisdegwet",
              "text": "For historians who find this comment later I need y'all to know this was asked roughly 15 minutes after the original post. I salute you.",
              "score": 127,
              "created_utc": "2026-01-14 02:57:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhgwza",
                  "author": "FuckNinjas",
                  "text": "Isn't what all of this is for? _gestures broadly_",
                  "score": 35,
                  "created_utc": "2026-01-14 04:33:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi90px",
                  "author": "erwgv3g34",
                  "text": "It's the only question that matters. If you _don't_ want to do porn, you are better off using ChatGPT or Claude over an open source model. They are cheaper, faster, and stronger.",
                  "score": 12,
                  "created_utc": "2026-01-14 08:25:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi5qu1",
                  "author": "BlobbyMcBlobber",
                  "text": "More like 15 seconds",
                  "score": 6,
                  "created_utc": "2026-01-14 07:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzgxdhz",
              "author": "gxvingates",
              "text": "Brother asking the questions that matter over here",
              "score": 52,
              "created_utc": "2026-01-14 02:34:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgxi6t",
              "author": "leetsauwse",
              "text": "Bonk",
              "score": 32,
              "created_utc": "2026-01-14 02:35:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh2zfe",
          "author": "Moronic_Princess",
          "text": "AND this is trained on domestic Huawei hardware",
          "score": 21,
          "created_utc": "2026-01-14 03:06:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkrfl5",
              "author": "henryclw",
              "text": "I think this is much more important, love to see people talking about it.",
              "score": 8,
              "created_utc": "2026-01-14 17:46:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzglr8q",
          "author": "crux153",
          "text": "\"Because the inference optimizations for this architecture are currently limited, the runtime cost is still relatively high. It requires either a single GPU with more than 80GB of memory, or a multi-GPU setup.\"",
          "score": 26,
          "created_utc": "2026-01-14 01:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgrmoy",
              "author": "dinerburgeryum",
              "text": "Yeah, that's day zero stuff tho. Comfy will bang the inference code into shape, and city will have GGUFs up by the end of the week. Two weeks tops. Just kick back and let the wizards do their magic.",
              "score": 17,
              "created_utc": "2026-01-14 02:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh28hc",
                  "author": "Hoodfu",
                  "text": "Last time a model said these kind of specs the comfy.org guys said it wasn't worth their time and it died on the vine. I hope that doesn't happen this time.",
                  "score": 9,
                  "created_utc": "2026-01-14 03:02:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh0r5t",
                  "author": "More_Slide5739",
                  "text": "Just for that, Imma put this last. I got 96 models and now this ain't one!",
                  "score": -2,
                  "created_utc": "2026-01-14 02:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzgvkhs",
          "author": "Amazing_Athlete_2265",
          "text": "> Because the inference optimizations for this architecture are currently limited, the runtime cost is still relatively high. It requires either a single GPU with more than 80GB of memory, or a multi-GPU setup.\n\nGood thing I'm a patient man. Looking forward to be able to run this on lesser hardware.",
          "score": 5,
          "created_utc": "2026-01-14 02:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgkfdk",
          "author": "Caladan23",
          "text": "wen GGUF?",
          "score": 19,
          "created_utc": "2026-01-14 01:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgxbr3",
              "author": "MikeLPU",
              "text": "ðŸ’¯â˜ï¸ðŸ˜‚",
              "score": -4,
              "created_utc": "2026-01-14 02:34:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgr77s",
          "author": "hainesk",
          "text": "What is the best way to run this with multiple gpus?",
          "score": 4,
          "created_utc": "2026-01-14 01:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh3mpc",
          "author": "Lopsided_Dot_4557",
          "text": "I just did an installation and testing video here:  [https://youtu.be/A6N8xu7xPRg?si=04v0lq64agKqr01b](https://youtu.be/A6N8xu7xPRg?si=04v0lq64agKqr01b)",
          "score": 4,
          "created_utc": "2026-01-14 03:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhwo69",
              "author": "o0genesis0o",
              "text": "I just watched and liked the video. Did you speed up or cut the video? That A6000 finish 50 steps surprisingly fast.\n\nThe model itself is not as good as I imagine.",
              "score": 2,
              "created_utc": "2026-01-14 06:33:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlpemn",
                  "author": "Lopsided_Dot_4557",
                  "text": "No I didn't edit it. Its actually fast. Thanks for liking it.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzho8yq",
          "author": "Flat-Reference-2900",
          "text": "Comfyui version?",
          "score": 3,
          "created_utc": "2026-01-14 05:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhnqsl",
          "author": "jacek2023",
          "text": "Good size!",
          "score": 2,
          "created_utc": "2026-01-14 05:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhsotx",
              "author": "Iory1998",
              "text": "Very good indeed. I wonder how it performs compared to Z-Image",
              "score": 1,
              "created_utc": "2026-01-14 06:00:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjre78",
                  "author": "martinerous",
                  "text": "From the one example prompt that I tried, the result was visually not as realistic as Z-Image Turbo. GLM felt too artificial and a bit overcooked looks in comparison to Z-image's \"brutal\" realism.",
                  "score": 3,
                  "created_utc": "2026-01-14 15:00:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzk4vjl",
          "author": "HonZuna",
          "text": "That's all very interesting and engaging, but the key question is: what about tits?",
          "score": 2,
          "created_utc": "2026-01-14 16:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznabki",
          "author": "Daniel_H212",
          "text": "Definitely didn't see this coming. Deepseek-image next? ðŸ˜‚",
          "score": 2,
          "created_utc": "2026-01-15 00:58:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzushlh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-16 03:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzme2l7",
          "author": "10minOfNamingMyAcc",
          "text": "RemindMe! 2 weeks",
          "score": 0,
          "created_utc": "2026-01-14 22:10:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzme98h",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 14 days on [**2026-01-28 22:10:02 UTC**](http://www.wolframalpha.com/input/?i=2026-01-28%2022:10:02%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/nzme2l7/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1qc9m6x%2Fglmimage_is_released%2Fnzme2l7%2F%5D%0A%0ARemindMe%21%202026-01-28%2022%3A10%3A02%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qc9m6x)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-14 22:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzndl6j",
          "author": "Acceptable-Tie278",
          "text": "Letâ€™s goooo ðŸ”¥",
          "score": 0,
          "created_utc": "2026-01-15 01:17:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4s8t3",
      "title": "llama.cpp performance breakthrough for multi-GPU setups",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ohxtu0l8hkbg1.jpeg",
      "author": "Holiday-Injury-9397",
      "created_utc": "2026-01-05 17:37:58",
      "score": 569,
      "num_comments": 200,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q4s8t3/llamacpp_performance_breakthrough_for_multigpu/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxuy5ve",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-05 18:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxusqli",
          "author": "MelodicRecognition7",
          "text": "I think details are here https://github.com/ikawrakow/ik_llama.cpp/pull/1080 not on that paid slop website",
          "score": 162,
          "created_utc": "2026-01-05 17:55:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxutny3",
              "author": "One-Macaron6752",
              "text": "\"PP performance for more than 4 GPUs is likely to be bad. Why? It looks like I'm not using NCCL correctly. PP and TG performance are both excellent for 2 GPUs, but for 3 or more GPUs the straightforward NCCL usage that one finds in examples on the Internet results in a horrible PP performance (2X or more lower compared to not using NCCL). Hence, I have implemented a workaround that uses pairwise communicators, but that workaround is only available for 3 and 4 GPUs (as I'm not able to test the implementation for more than 4 GPUs). I hope someone more knowledgable will show what is the correct way to use NCCL, so workarounds as in this PR are not necessary.Â Update: With more than 4 GPUs it is very likely that disabling NCCL will give better performance.\"\n\nThe half sour candy... Let's see tomorrow how it performs and will pick it up from there! But nice effort on OP and kudos for all the hard work on making llama even better!",
              "score": 25,
              "created_utc": "2026-01-05 17:59:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv4p61",
                  "author": "a_beautiful_rhind",
                  "text": "For fully offloaded, 4xGPU  cranks. 30-40t/s on 70b and devstral large, etc. I've never had speeds this high in any backend.",
                  "score": 33,
                  "created_utc": "2026-01-05 18:49:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvh038",
                  "author": "dsanft",
                  "text": "I don't understand why you wouldn't just slice weights and tensors and do a final allgather at the end. This arch just seems broken.",
                  "score": 6,
                  "created_utc": "2026-01-05 19:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv3w83",
              "author": "pmttyji",
              "text": "Follow-up PR to above one\n\n[https://github.com/ikawrakow/ik\\_llama.cpp/pull/1092](https://github.com/ikawrakow/ik_llama.cpp/pull/1092)",
              "score": 10,
              "created_utc": "2026-01-05 18:45:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxupuoc",
          "author": "suicidaleggroll",
          "text": "Even on a single GPU, or CPU-only, I see consistent 2x prompt processing speeds on ik_llama.cpp compared to llama.cpp on every model I've tried.  It's a fantastic fork.",
          "score": 111,
          "created_utc": "2026-01-05 17:42:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxurwa2",
              "author": "YearZero",
              "text": "Is there a reason that ik\\_llama speed improvements can't be implemented in original llama? (I'm not a dev, so maybe missing something obvious). Is it just the time/effort needed, or is there some more fundamental reason like breaking compatibility with certain kinds of hardware or something?",
              "score": 51,
              "created_utc": "2026-01-05 17:51:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxuthjg",
                  "author": "LagOps91",
                  "text": "i really wish it would all be merged back. apparently there has been a spat of sorts between developers in the past leading to the fork.",
                  "score": 29,
                  "created_utc": "2026-01-05 17:59:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxuv8jc",
                  "author": "Marksta",
                  "text": "The key issue is llama.cpp is shifting too much architecturally that making any changes like those in ik_llama.cpp is so much harder. By the time you finished this multi-gpu speed up, you'd just spend the next month rebuilding it again to resolve merge conflicts, and by the time you finished doing that there would be new merge conflicts now that time has passed again...\n\nIt's half project management fault, half c++ fault. They keep changing things and to make changes means touching the core files. And the core files keep changing?! That's why modern software development moved towards architectures and languages that aren't c++ to let more than a few key devs touch the project at once.",
                  "score": 14,
                  "created_utc": "2026-01-05 18:06:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0i0y",
              "author": "Evening_Tooth_1913",
              "text": "How does it compare to vllm?",
              "score": 6,
              "created_utc": "2026-01-05 18:30:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv0xat",
                  "author": "suicidaleggroll",
                  "text": "I was so disappointed with the model load times on vllm that I never got around to actually benchmarking anything.  I switch models pretty regularly, spending 2+ minutes loading up a new model (something that takes <5 seconds on llama.cpp) wipes out any advantage it could possibly have in processing speeds for my application.",
                  "score": 17,
                  "created_utc": "2026-01-05 18:32:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvkfrh",
                  "author": "MoffKalast",
                  "text": "It actually runs. (/s but not entirely)",
                  "score": 4,
                  "created_utc": "2026-01-05 20:01:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwrqlv",
              "author": "Zc5Gwu",
              "text": "Last time I tried ik\\_llama, either tool calling or streaming wasn't up to the same compatibility as llama.cpp. Not sure if anyone has experience recently...",
              "score": 1,
              "created_utc": "2026-01-05 23:28:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxy8n0d",
                  "author": "CheatCodesOfLife",
                  "text": "Streaming is fixed, /v1/completions is fixed, tool calling was still broken last week when I tried it.",
                  "score": 3,
                  "created_utc": "2026-01-06 04:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxv4cps",
          "author": "a_beautiful_rhind",
          "text": "ik now faster than exllama and probably equal to vllm for single batch. Unfortunately I didn't have as much luck with TP on hybrid inference. Between numa and PCIE 3.0, I have a bottleneck somewhere.\n\nWhat's funny is that I've been using this for what feels like a month and finally see it posted here.",
          "score": 22,
          "created_utc": "2026-01-05 18:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvaw46",
              "author": "mr_zerolith",
              "text": "It's kinda sad, i only caught wind of it previously in some post's comments, pointing to some github comments.\n\nDeserves a lot more eyeballs and i'm glad someone summarized it",
              "score": 8,
              "created_utc": "2026-01-05 19:17:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwdlnu",
              "author": "Aggressive-Bother470",
              "text": "ik now beats everything for inference, single batch, I think?\n\n\nI thought it was only possible to realise PP gains with TP but they've somehow improved both on Devstral.\n\n\nOver 70t/s on devstral small beats my vllm and lcpp.",
              "score": 6,
              "created_utc": "2026-01-05 22:17:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxwdt4n",
                  "author": "a_beautiful_rhind",
                  "text": "If he somehow gets numa working, it's truly over.",
                  "score": 6,
                  "created_utc": "2026-01-05 22:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx478p",
              "author": "FullstackSensei",
              "text": "So, no joy for NUMA yet?\n\nGiven how crazy RAM prices are, I'm seriously considering selling my dual Epyc rig. It's been collecting dust for at least three months now. Built it in the hope we'd get proper NUMA support, but it seems we'll get RDMA before that happens (TBH, not complaining if RDMA support comes).",
              "score": 4,
              "created_utc": "2026-01-06 00:33:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx5ict",
                  "author": "a_beautiful_rhind",
                  "text": "It will happen. There's always fastLLM and ktransformers.",
                  "score": 2,
                  "created_utc": "2026-01-06 00:40:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyrvjk0",
                  "author": "madSaiyanUltra_9789",
                  "text": "I'm not sure what you mean by \"when RDMA support comes\"? \n\nMost of the gains seen here by using NCCL is thanks to DMA direct memory access) based communication and scheduling optimizations (under the hood)",
                  "score": 1,
                  "created_utc": "2026-01-10 12:07:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxusuk3",
          "author": "Numerous-Macaroon224",
          "text": "The missing caption for the chart is: \"*4 x Nvidia Tesla T4 GPUs on 64 core AMD EPYC 7V12 server*\"",
          "score": 43,
          "created_utc": "2026-01-05 17:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuvyia",
              "author": "BuildAQuad",
              "text": "Thank you, I was looking for this info",
              "score": 4,
              "created_utc": "2026-01-05 18:10:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwytdi",
              "author": "lemondrops9",
              "text": "omg thanks because I was wondering why the tk/s is so low on the Llama.cpp",
              "score": 2,
              "created_utc": "2026-01-06 00:05:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyrw04j",
              "author": "madSaiyanUltra_9789",
              "text": "yeahp, they also \"forget\" to mention the \"modest hardware requirements \" needed to see such gains.",
              "score": 2,
              "created_utc": "2026-01-10 12:11:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuuhcw",
          "author": "dazzou5ouh",
          "text": "Damn, I just finished building a 6x3090 rig",
          "score": 17,
          "created_utc": "2026-01-05 18:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv8bo6",
              "author": "mr_zerolith",
              "text": "Sounds like you're in for a treat",
              "score": 6,
              "created_utc": "2026-01-05 19:05:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwg0uq",
              "author": "Kolapsicle",
              "text": "What should \\*we\\* use it on first?",
              "score": 0,
              "created_utc": "2026-01-05 22:29:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuxkm2",
          "author": "inrea1time",
          "text": "I will try this on my dual 5060 TI 16GB, I went for RAM over compute, maybe I can get some compute now too!",
          "score": 14,
          "created_utc": "2026-01-05 18:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvgsfb",
              "author": "Kahvana",
              "text": "Let me know how it goes, very interested!",
              "score": 5,
              "created_utc": "2026-01-05 19:44:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxzp95h",
              "author": "inrea1time",
              "text": "I did a quick test, nothing scientific with some prompts I have been using with a project.  Compared to lmstudio which I was using this seems to be 20-25% faster for a mistral 7b q4\\_k\\_m.  I am seeing both GPU's being significantly utilized with the model split between the VRAM in both.  The impact should be greater with a larger model from what I understand.",
              "score": 2,
              "created_utc": "2026-01-06 11:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrwpa5",
                  "author": "madSaiyanUltra_9789",
                  "text": "i think what you are observeing is the practical limits of non-professional cards running tensor parallelism over PCIe, those 5060tis cannot reference directly in each others memory spaces or talk to each other directly, it all has to happen via CPU (the bottleneck). in this plot from the OP, they are using 4x Telsa T4 gpus which have P2P communication and DMA so NCCL can actually perform at its peak.",
                  "score": 1,
                  "created_utc": "2026-01-10 12:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwhsyv",
              "author": "[deleted]",
              "text": "Please do share your experiences!",
              "score": 1,
              "created_utc": "2026-01-05 22:37:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwsjce",
              "author": "satireplusplus",
              "text": "I also have dual 5060s and lots of DDR4 ECC ram bought before the RAM mania. Standard Llama.cpp seemed to have improved as well over the last months, as I now get 16tok/s out of gpt-120B (q4).",
              "score": 1,
              "created_utc": "2026-01-05 23:32:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzpvrb",
                  "author": "inrea1time",
                  "text": "I have a threadripper 8 channel setup but with 96GB so only 6 channels used.  I grabbed 32GB as prices were going up for a painful price.   I tried a 120B model once with lmstudio and decided never again.  I guess worth a shot now.",
                  "score": 2,
                  "created_utc": "2026-01-06 11:47:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxur6hc",
          "author": "HumerousGorgon8",
          "text": "To build for Vulkan, is it the same commands as mainline llama.cpp?",
          "score": 10,
          "created_utc": "2026-01-05 17:48:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv4vi0",
              "author": "pmttyji",
              "text": "Yes. But\n\n>[ik\\_llama.cppÂ is not the right choice if you want to use Vulkan](https://github.com/ikawrakow/ik_llama.cpp/issues/1083#issuecomment-3687109186). There was a point in time where I had made the VulkanÂ `ik_llama.cpp`Â build work and be on par, or even slightly outperform,Â `llama.cpp`. But since then\n\n>I have added new optimizations that are not implemented on Vulkan, so will run on the CPU, thus making it slow\n\n>TheÂ `llama.cpp`Â developers have significantly improved Vulkan performance, while I have done nothing for the Vulkan back-end\n\n>I'm basically the only person working on the computation engine, so simply do not have the bandwidth to stay competitive also for Vulkan.Â `ik_llama.cpp`Â is good (and faster thanÂ `llama.cpp`) for CPU-only, CUDA-only, and hybrid CUDA/CPU inference.",
              "score": 17,
              "created_utc": "2026-01-05 18:50:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvxhh9",
                  "author": "steezy13312",
                  "text": "*weeps in AMD*",
                  "score": 10,
                  "created_utc": "2026-01-05 21:02:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvrpel",
              "author": "VoidAlchemy",
              "text": "Yeah, you just need to pick quants that use older mainline quant types for GPU offload, but you could still use newer ik types for tensors on RAM if doing hybrid CPU inferencing.\n\nBasically same compilation e.g.\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=OFF -DGGML_VULKAN=ON\ncmake --build build --config Release -j $(nproc)\n```",
              "score": 1,
              "created_utc": "2026-01-05 20:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx14st",
                  "author": "maglat",
                  "text": "for the build itself, is it possible to build just the llama-server and not the entire package?",
                  "score": 1,
                  "created_utc": "2026-01-06 00:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxuy986",
          "author": "daank",
          "text": "I wonder if this requires fast throughput between the GPUs?\n\nFor regular multi-gpu inference you could put the second card on a much slower PCIe lane since the speed only matters when loading weights. Does that still work for ik_llama.cpp?",
          "score": 8,
          "created_utc": "2026-01-05 18:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv512k",
              "author": "a_beautiful_rhind",
              "text": "It doesn't really *require* it, but it helps. If you're on some 1x stuff you will probably see no benefit. One card being on 8x and one on 16x is fine.",
              "score": 2,
              "created_utc": "2026-01-05 18:50:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv5b9o",
              "author": "BuildAQuad",
              "text": "I'm not certain, but i would assume that it requires more pcie lanes than normal consumer hardware can handle. Maybe dual GPU setups with 8x lanes each could work",
              "score": 1,
              "created_utc": "2026-01-05 18:52:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxut93s",
          "author": "kiwibonga",
          "text": "Nice to see my best friend Devstral Small 2 represented here.\n\nHow is memory organized compared to a single GPU setup? Is the model truly split or replicated? What about the caches?\n\nEdit: ah shit, I forgot blogspam existed",
          "score": 7,
          "created_utc": "2026-01-05 17:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyb4ia",
              "author": "ClimateBoss",
              "text": "any good GGUF? I get looping 'n glitchy chat template",
              "score": 1,
              "created_utc": "2026-01-06 04:34:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxydakt",
                  "author": "kiwibonga",
                  "text": "You may have to override the temperature to 0.2. The default in llamacpp and others is 0.7 which is adequate for chat but not tool calls.\n\nI use Q3_K_M from unsloth.",
                  "score": 1,
                  "created_utc": "2026-01-06 04:49:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvahcv",
          "author": "Artistic_Okra7288",
          "text": "It would be great if this could work with rpc-server to utilize GPUs across hosts.",
          "score": 7,
          "created_utc": "2026-01-05 19:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuswyz",
          "author": "HCLB_",
          "text": "Cool does it support nvidia pascal cards?",
          "score": 5,
          "created_utc": "2026-01-05 17:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxgxxt",
          "author": "VoidAlchemy",
          "text": "Can confirm just tested \\`-sm graph\\` on 2x RTX A6000 (the older non-PRO versions) running a small ik quant of MiMo-V2-Flash showing GPU utilization going from about 50% to almost max and big gains over default of \\`-sm layer\\`:\n\n* PP (prefill) +43%\n* TG (decode) +23%\n\nDetails: [https://github.com/ikawrakow/ik\\_llama.cpp/pull/1105#issuecomment-3712755415](https://github.com/ikawrakow/ik_llama.cpp/pull/1105#issuecomment-3712755415)\n\nhttps://preview.redd.it/igbusgn3vmbg1.png?width=2087&format=png&auto=webp&s=3aaab598c8187f89e510da85116e24ba8ec5b095\n\n  \nover 70 tok/sec TG is nice, if only the model itself was working better (i had similar quality issues with full Q8\\_0 on both ik and mainline llama.cpp forks with it failing on pydantic-ai agent tool use test that worked fine a GLM-4.7-smol-IQ1\\_KT \\~2bpw quant haha)...\n\nAnyway, its faster!  (for this specific model which is supported with specific details in above PR)",
          "score": 5,
          "created_utc": "2026-01-06 01:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6cvaw",
              "author": "One-Macaron6752",
              "text": "I'm just getting an error trying to run it with NCCL, with one of your quants \"Devstral-2-123B-Instruct-2512-IQ4\\_KSS.gguf\". Any idea?\n\n    ggml_cuda_op_reduce: ncclAllReduce failed with status 1\n    ik_llama.cpp/ggml/src/ggml-cuda/reduce.cu:97: Fatal error",
              "score": 2,
              "created_utc": "2026-01-07 10:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9ymhm",
                  "author": "VoidAlchemy",
                  "text": "Hrmm... When compiling does it say `NCCL found!` ? Otherwise please open an issue on ik_llama.cpp github and tag me `@ubergarm` and include more details on your rig e.g. how many and what kind of GPUs, etc.\n\nThanks!",
                  "score": 2,
                  "created_utc": "2026-01-07 21:25:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxwbrym",
          "author": "zelkovamoon",
          "text": "Ok so two questions\n\nDoes ik_llama broadly support the same models as llama.cpp but with optimizations, or is it a subset\n\nAre these improvements going to apply broadly to any type of model?",
          "score": 4,
          "created_utc": "2026-01-05 22:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxg2qr",
              "author": "VoidAlchemy",
              "text": "ik wrote many of the quants used in mainline llama.cpp, so ik supports all those and more\n\nik can be faster for many models, this new \\`-sm graph\\` covers about 8 popular models so far - i have links to exact code above.",
              "score": 4,
              "created_utc": "2026-01-06 01:37:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwckxl",
          "author": "ga239577",
          "text": "In the article linked to on Medium, noticed this part:\n\n\"Backend Agnostic Potential: Because it is implemented at the ggml graph level rather than the CUDA backend level, it can theoretically be extended to other backends like Vulkan or ROCm in the future.\"\n\nHaving this work on ROCm seems like it would be amazing for Strix Halo devices.",
          "score": 4,
          "created_utc": "2026-01-05 22:12:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv3hdf",
          "author": "LinkSea8324",
          "text": "ggergabros, it's over",
          "score": 8,
          "created_utc": "2026-01-05 18:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvkzkx",
          "author": "onil_gova",
          "text": "Any chance this offers a boost to mac users?",
          "score": 3,
          "created_utc": "2026-01-05 20:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwc7ie",
          "author": "ActivePutrid3183",
          "text": "What does this mean for the people with mixed GPU setups (EX: 1x 3090, 1x3060)? Previously, using such a setup would mean speeds being throttled by the 3060, but does this new solution circumvent that?",
          "score": 3,
          "created_utc": "2026-01-05 22:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurn32",
          "author": "Such_Advantage_6949",
          "text": "is it basically tensor parrallel? does it support odd number of gpus?",
          "score": 5,
          "created_utc": "2026-01-05 17:50:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvtjpy",
              "author": "VoidAlchemy",
              "text": "look into the \\`--max-gpu\\` setting, depends on the model. check here for supported models: [https://github.com/ikawrakow/ik\\_llama.cpp/blob/main/src/llama.cpp#L1726-L1735](https://github.com/ikawrakow/ik_llama.cpp/blob/main/src/llama.cpp#L1726-L1735)",
              "score": 5,
              "created_utc": "2026-01-05 20:43:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxgmx1",
                  "author": "x0xxin",
                  "text": "Any idea if GLM 4.6 or 4.7 are supported via `LLM_ARCH_GLM4_MOE`? I saw a reference to the GLM 4.6 chat template [test-chat.cpp](https://github.com/ikawrakow/ik_llama.cpp/blob/d9236392cfee36a715e7baed6890ce1e330a291e/tests/test-chat.cpp#L1891) but that's the only place in the repo I see it mentioned.",
                  "score": 3,
                  "created_utc": "2026-01-06 01:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv54kb",
              "author": "a_beautiful_rhind",
              "text": "Yep.. i can use it with 3x GPU.",
              "score": 4,
              "created_utc": "2026-01-05 18:51:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxuzf6s",
              "author": "NaiRogers",
              "text": " I am on team odd nGPU",
              "score": 3,
              "created_utc": "2026-01-05 18:25:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxuu5m9",
          "author": "gofiend",
          "text": "Anybody know if this works on Rocm â€¦ especially umm MI50s?",
          "score": 6,
          "created_utc": "2026-01-05 18:02:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv5cdw",
              "author": "a_beautiful_rhind",
              "text": "It's graph parallel so untested. Sorta cuda-centric. It's not gonna work with vulkan for sure.",
              "score": 11,
              "created_utc": "2026-01-05 18:52:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv78f5",
                  "author": "gofiend",
                  "text": "Humm doesn't look like ikllama even supports Rocm (at least I cannot build for it), but it does have Vulkan support (which I'm testing now). \n\nPer this discussion, it def won't work with graph parallel.",
                  "score": 6,
                  "created_utc": "2026-01-05 19:00:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv64th",
              "author": "Minute-Ingenuity6236",
              "text": "I tried to compile it for my MI50s and was not able to compile it successfully, except when using only Vulcan. With Vulcan, the speed was ridiculously bad, about 10x slower than vanilla llama.cpp. Maybe I did something wrong, I don't know.\n\nEDIT: When I think about it, maybe it did not use the GPUs at all and that is why the speed was so bad.",
              "score": 3,
              "created_utc": "2026-01-05 18:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv9fxv",
                  "author": "Leopold_Boom",
                  "text": "Yeah it compiled with llama.cpp rocm flags for me but silently ignored the GPUs.\n\nWith Vulkan it tried to use the GPUs but was ... unbelievably slow (gpt-oss-20b so maybe it works better with a normal quant)",
                  "score": 1,
                  "created_utc": "2026-01-05 19:10:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0whi",
              "author": "evillarreal86",
              "text": "Exactly what I wanted to try, but cuda only :(",
              "score": 2,
              "created_utc": "2026-01-05 18:32:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxux3o8",
              "author": "inrea1time",
              "text": "They seem to be using NCCL [https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html) for at least some of the boost which is definitely not compatible with AMD.  I gave up on AMD a couple of months ago.",
              "score": 1,
              "created_utc": "2026-01-05 18:15:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxuy7h5",
                  "author": "Marksta",
                  "text": "RCCL is more or less a drop in replacement for NCCL. But ik_llama.cpp is CUDA only at this time for its ik fork specific features anyways even if someone patched that fix in for ROCm.",
                  "score": 6,
                  "created_utc": "2026-01-05 18:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxv0vrk",
              "author": "ScoreUnique",
              "text": "They do support Vulkan for sure, should give you some level of speed boost.",
              "score": 0,
              "created_utc": "2026-01-05 18:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxvbxml",
          "author": "insulaTropicalis",
          "text": "This is great and all, but honestly I am having some headache trying to understand which .gguf work with llama.cpp vs ik-llama.cpp, and which one should be used with which for the best performance.\n\nI invoke u/VoidAlchemy to clarify the issue.\n\nEDIT: tried with normal gguf quants for hybrid inference, till now it is much slower than mainline both at pp and tg. I'll see with the special quants tomorrow.",
          "score": 5,
          "created_utc": "2026-01-05 19:21:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvss7c",
              "author": "VoidAlchemy",
              "text": "In general ik_llama.cpp supports all GGUF quant types. For many models and rigs you'll see better PP performance on ik (especially with increased batch sizes e.g. `-ub 4096 -b 4096` stuff).\n\nAlso avx512_vnni performance is amazing for PP. Makes my 9950x CPU with 16 cores go faster than older thread ripper pro zen4 24x cores for PP.\n\nmainline llama.cpp does not support the newer quant types which I use in my models (ubergarm on huggingface).\n\nThis post is about the recent speed-ups for 2-4 GPU rigs `-sm graph` \"graph parallel\" feature. It doesn't help with single GPU as that is already fast.\n\nKeep in mind it doesn't apply to all models yet, you can see a list of them here: https://github.com/ikawrakow/ik_llama.cpp/blob/main/src/llama.cpp#L1726-L1735",
              "score": 7,
              "created_utc": "2026-01-05 20:40:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvvrej",
                  "author": "insulaTropicalis",
                  "text": "I will test the new features, it's a while that I don't use ik-llama.cpp. I could try the Ling-1T model you quantized.\n\nAre you sure about avx512\\_vnni? Because on Threadripper Pro 7000 it is already supported. It's surprising that the 9950x is faster than 7965wx.",
                  "score": 2,
                  "created_utc": "2026-01-05 20:54:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxw2e4u",
                  "author": "fairydreaming",
                  "text": "No DeepSeek :-(",
                  "score": 2,
                  "created_utc": "2026-01-05 21:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvh7cm",
              "author": "pmttyji",
              "text": "For ik\\_llama.cpp, use below GGUFs for best performance\n\n* [https://huggingface.co/ubergarm/models](https://huggingface.co/ubergarm/models)\n* [https://huggingface.co/Thireus/models](https://huggingface.co/Thireus/models)\n* [https://huggingface.co/models?other=ik\\_llama.cpp](https://huggingface.co/models?other=ik_llama.cpp)",
              "score": 6,
              "created_utc": "2026-01-05 19:46:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyeiamg",
                  "author": "insulaTropicalis",
                  "text": "Tried Ubergarm's and a few others, they are consistently slower than mainline llama.cpp in hybrid inference. With full GPU offloading ik-llama.cpp it's lightning fast, but for hybrid inference mainline is definitely the best option.",
                  "score": 1,
                  "created_utc": "2026-01-08 14:25:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvkp1w",
                  "author": "Leflakk",
                  "text": "Do you know where to find a proper documentation (list of command flags) for ik\\_llama?",
                  "score": 1,
                  "created_utc": "2026-01-05 20:02:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvf4jg",
          "author": "Mr_Back",
          "text": "https://preview.redd.it/g1dsmlun1lbg1.png?width=1987&format=png&auto=webp&s=ce30fbc209777f5dc4f089ea847c40924cb623de\n\nI donâ€™t understand whatâ€™s wrong. Am I not setting up the IK version correctly? Itâ€™s always slower than the regular Lama version for me. And once the flag â€”fit is enabled, things get even worse. I just tested it on the GPT OSS 20b model. Iâ€™m attaching my configuration settings for running the application.\n\n    GPT-OSS-Mini-vulkan:\n    cmd:  M:\\Soft\\llama-b7562-bin-win-vulkan-x64\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --fit on --fit-target 512 --fit-ctx 16384 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-cuda:\n    cmd:  M:\\Soft\\llama-b7621-bin-win-cuda-12.4-x64\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --fit on --fit-target 512 --fit-ctx 16384 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-ik:\n    cmd:  M:\\Soft\\ik_llama.cpp\\build\\bin\\Release\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --n-gpu-layers 12 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600\n    \n    GPT-OSS-Mini-ik-2:\n    cmd:  M:\\Soft\\ik_llama.cpp\\build\\bin\\Release\\llama-server.exe --seed 3003 --model G:\\LlamaModels\\gpt-oss-20b.gguf --port ${PORT} --ctx-size 128000 --n-gpu-layers 24 --n-cpu-moe 1 --mlock --host 0.0.0.0 --jinja --temp 1.0 --top-p 1.0 --top-k 0 --threads -1\n    ttl: 600",
          "score": 2,
          "created_utc": "2026-01-05 19:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwhnnc",
          "author": "pbalIII",
          "text": "Curious what the specific change is. Last I checked, llama.cpp still relies on pipeline parallelism rather than tensor parallelism for multi-GPU, which means GPUs process layers sequentially rather than in parallel. CUDA Graphs helped reduce kernel launch overhead and there's been Stream-K work for AMD, but nothing that changes the core multi-GPU story. Would be interested to know if there's a new layer splitting approach or different scheduling at play.",
          "score": 2,
          "created_utc": "2026-01-05 22:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwq808",
          "author": "maglat",
          "text": "So how to get it working?  \nSo far as I understood its required to install NCCL with sudo apt install libnccl-dev  \nBuild IK\\_llama.cpp with \"cmake -B build -DGGML\\_NCCL=ON\"  \nbut how to start llamacpp-server with the correct command?\n\nI tried following but didnt worked  \nCUDA\\_VISIBLE\\_DEVICES=1,2,3 ./llama-server -m /models/gpt-oss-120b-Derestricted.MXFP4\\_MOE.gguf --port 8788 --host [192.168.178.7](http://192.168.178.7) \\-ngl 99 --jinja --ctx-size 64000 --top\\_p 1.00 --temp 1.0 --min-p 0.0 --top-k 0.0\n\nit starts up but do not load the model into the GPU memory\n\nEDIT:\n\nSo the new mode is  -sm graph. Sadly for my test on gpt-oss the model wont support it. Thats war the log is saying. \n\n\n\n    Split mode 'graph' is not supported for this model\n    Â  => changing split mode to 'layer'",
          "score": 2,
          "created_utc": "2026-01-05 23:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuwr0c",
          "author": "silenceimpaired",
          "text": "I keep hearing good things from ik_llama but I tend to prefer a packed solution like KoboldCPP or Text Gen by Oobabooga as the hassle of nvidia and setup on Linux is a lot lower for me. Is there anything like that for il_llama?",
          "score": 5,
          "created_utc": "2026-01-05 18:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxuyrun",
              "author": "henk717",
              "text": "There was a corc fork that tried to merge koboldcpp with the ik\\_llama stuff but its such a hassle to maintain that I think it got stuck and upstream we don't even try as the two have diverged a lot. Because llamacpp's upstream project is where most of the model support is thats what everyone bases on. So your best hope is that this or something similar lands in the upstream project.",
              "score": 6,
              "created_utc": "2026-01-05 18:22:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv30em",
              "author": "pmttyji",
              "text": "[https://github.com/Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp)",
              "score": 2,
              "created_utc": "2026-01-05 18:41:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxv9lol",
                  "author": "silenceimpaired",
                  "text": "They donâ€™t provide releases like KoboldCPP, right? I think I tried it and could never get it running.",
                  "score": 2,
                  "created_utc": "2026-01-05 19:11:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxvzfzb",
              "author": "Dry-Judgment4242",
              "text": "Hope some wizard does it eventually as alas I tried getting ik up and running but the windows shit is not working for me.",
              "score": 2,
              "created_utc": "2026-01-05 21:11:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxv0ew4",
          "author": "warnerbell",
          "text": "    This is great for anyone wating to run larger models locally. The multi-GPU coordination has been a pain point for a while. Just ned a 2 slor MB now!?\n    \n    One thing I've found that compounds with hardware improvements: structural optimization on the prompt side. Even with faster inference, context window efficiency matters. I was running a 1,000+ line system prompt and noticed instructions buried deep were getting missed, regardless of hardware.\n    \n    Hardware gains + prompt architecture = multiplicative improvement. Excited to test this llama.cpp update with my upcoming Intel Build.",
          "score": 1,
          "created_utc": "2026-01-05 18:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv9wy9",
          "author": "segmond",
          "text": "does the improvement work with RCP as well?",
          "score": 1,
          "created_utc": "2026-01-05 19:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvn1sm",
          "author": "DiscombobulatedAdmin",
          "text": "2 new 5060 tiâ€™s are looking better every dayâ€¦",
          "score": 1,
          "created_utc": "2026-01-05 20:13:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw0425",
          "author": "elsung",
          "text": "whoa thatâ€™s awesome. i actually just took out my tesla p40 out of my rig with 2 more 3090s to run with vllm since it was just bottlenecking my speed without much value. now u guys got me thinking of putting it back lol",
          "score": 1,
          "created_utc": "2026-01-05 21:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzt9sr",
          "author": "Zyj",
          "text": "Can this technique be used with two networked Strix Halo systems?",
          "score": 1,
          "created_utc": "2026-01-06 12:13:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1k6ha",
              "author": "egnegn1",
              "text": "A regular network is probably to slow, because the communication traffic is much higher then with regular clusters. The solution is ideally suited for very high bandwidth and low latency networks.",
              "score": 1,
              "created_utc": "2026-01-06 17:38:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyqyixu",
                  "author": "Zyj",
                  "text": "Well they do have two USB4/TB3 ports",
                  "score": 1,
                  "created_utc": "2026-01-10 07:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny4z3k4",
          "author": "mr_zerolith",
          "text": "Yeah baby!!!",
          "score": 1,
          "created_utc": "2026-01-07 03:43:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyryajc",
          "author": "madSaiyanUltra_9789",
          "text": "These performance gains appear to only be applicable to GPUs that's support P2P DMA (direct memory access), ie NVIDIA professional/workstation and data-center GPUs. \n\nit uses tensor-parallelism instead of split layer parallelism. This eliminates the idle GPU time when one GPU is waiting for the other GPU to complete it's layers, since GPUs are now utilized in parallel. it is achieved by copying the entire model graph on both GPUs and then using NCCL (performance optimization and scheduling library by NVIDA) to orchestrate the work and combine the results. \n\nThis will be significantly slower if run on consumer GPUs like rtx 5000 series because they cannot communicate or interact with each others memory spaces directly (using PCIe DMA) but rather must pass through the CPU. \n\nyou might see a modest \\~20% gain, since NCCL is still heavily optimized but definitely don't expect to see \\~200%+  claimed here unless you are using professional/data-center GPUs.",
          "score": 1,
          "created_utc": "2026-01-10 12:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv96g6",
          "author": "Miserable-Dare5090",
          "text": "I just returned a 3090ti to microcenter as the speed from egpu to main pc was horrendous vs the strix halo alone. FML",
          "score": 1,
          "created_utc": "2026-01-05 19:09:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvnm5r",
          "author": "wh33t",
          "text": "Merge please so it can make it's way into kcpp!",
          "score": 1,
          "created_utc": "2026-01-05 20:15:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvt75t",
              "author": "VoidAlchemy",
              "text": "It is merged into ik\\_llama.cpp main. If you want something like kcpp that supports ik quants check out: [https://github.com/Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp) (i don't think it supports -sm graph yet though ymmv) or get a windows build from u/Thireus here: [https://github.com/Thireus/ik\\_llama.cpp/releases](https://github.com/Thireus/ik_llama.cpp/releases)",
              "score": 5,
              "created_utc": "2026-01-05 20:42:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwwqye",
          "author": "deltamoney",
          "text": "Very cool. But way to gatekeep and neg people  having some nice holiday breaks.",
          "score": 1,
          "created_utc": "2026-01-05 23:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurxk6",
          "author": "One-Macaron6752",
          "text": "Niiiice... ðŸ™â™¥ï¸\nâœ” more fusion & optimization\nâœ” better backend batching (curved ball from vLLM)\nâœ” fewer kernel launches (important on our poor souls GPUs)\nâœ” higher throughput == joy joy...\n\nTomorrow will be a llama-bench hard day...",
          "score": -2,
          "created_utc": "2026-01-05 17:52:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw1si7",
          "author": "Rrraptr",
          "text": "`llama_new_context_with_model: split mode 'graph' or 'attn' not supported. Failed to initialize Vulkan backend`\n\nIt's a pity that Vulkan isnâ€™t supported. The CUDA gang already has vLLM anyway.",
          "score": 0,
          "created_utc": "2026-01-05 21:22:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwnox8",
          "author": "ForsookComparison",
          "text": "Does split row work on Fedora yet",
          "score": 0,
          "created_utc": "2026-01-05 23:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxx67t",
          "author": "79215185-1feb-44c6",
          "text": "Oh fucking god I am so hard right now.",
          "score": 0,
          "created_utc": "2026-01-06 03:10:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxft3x",
          "author": "Xamanthas",
          "text": "Slop blog and self promo of said blog. Fuck off, stop trying to profiteer off llama contributor work by 'posting'",
          "score": -5,
          "created_utc": "2026-01-06 01:35:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q89g1i",
      "title": "DeepSeek V4 Coming",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/",
      "author": "External_Mood4719",
      "created_utc": "2026-01-09 14:18:56",
      "score": 496,
      "num_comments": 104,
      "upvote_ratio": 0.96,
      "text": "According to two people with direct knowledge, DeepSeek is expected to roll out a nextâ€‘generation flagship AI model in the coming weeks that focuses on strong codeâ€‘generation capabilities.\n\nThe two sources said the model, codenamed V4, is an iteration of the V3 model DeepSeek released in December 2024. Preliminary internal benchmark tests conducted by DeepSeek employees indicate the model outperforms existing mainstream models in code generation, including Anthropicâ€™s Claude and the OpenAI GPT family.\n\nThe sources said the V4 model achieves a technical breakthrough in handling and parsing very long code prompts, a significant practical advantage for engineers working on complex software projects. They also said the modelâ€™s ability to understand data patterns across the full training pipeline has been improved and that no degradation in performance has been observed.\n\nOne of the insiders said users may find that V4â€™s outputs are more logically rigorous and clear, a trait that indicates the model has stronger reasoning ability and will be much more reliable when performing complex tasks.\n\n[https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability](https://www.theinformation.com/articles/deepseek-release-next-flagship-ai-model-strong-coding-ability)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q89g1i/deepseek_v4_coming/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nylojbf",
          "author": "drwebb",
          "text": "Man, just when my [Z.ai](http://Z.ai) subscription ran out and I was thinking about getting the 3 months Max offer... I've been seriously impressed with DeepSeek V3.2 reasoning, it's superior in my opinion to GLM 4.7. DeepSeek API is cheap though.",
          "score": 102,
          "created_utc": "2026-01-09 14:34:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylqrti",
              "author": "Glum-Atmosphere9248",
              "text": "How about vs speciale?Â ",
              "score": 16,
              "created_utc": "2026-01-09 14:45:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylrdb7",
                  "author": "Exciting-Mall192",
                  "text": "Very good at math, according to people",
                  "score": 16,
                  "created_utc": "2026-01-09 14:48:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylxuzp",
                  "author": "SlowFail2433",
                  "text": "Keep forgetting to try this one",
                  "score": 3,
                  "created_utc": "2026-01-09 15:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyn5rha",
              "author": "perelmanych",
              "text": "I just bought 1 year [z.ai](http://z.ai) subscription for $28ðŸ˜‚ In any case I am completely satisfied with performance of GLM 4.7 and now when they are saying that GLM 5.0 is already in training I am content with my decision of having such a strong coding AI for less than 10 cents per day.",
              "score": 10,
              "created_utc": "2026-01-09 18:36:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyoj0sg",
                  "author": "WeMetOnTheMountain",
                  "text": "You'll never financially recover from this! ðŸ˜€",
                  "score": 5,
                  "created_utc": "2026-01-09 22:22:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyo2a8u",
                  "author": "seeKAYx",
                  "text": "4.7 is great. Iâ€™ll do all the heavy lifting with that. Iâ€™ll only need like 2-3 prompts with Opus.",
                  "score": 3,
                  "created_utc": "2026-01-09 21:04:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nypnrwc",
                  "author": "-dysangel-",
                  "text": "I've been using the coding plan on Claude Code for the past week and very happy with the performance. Definitely feels like the best value for money out there. A year's maxed out sub cost me the same as 1 month of the max Claude code tier",
                  "score": 3,
                  "created_utc": "2026-01-10 01:59:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyvkzw7",
                  "author": "arabterm",
                  "text": "what?! Is this real? Where is the sign up page please :-) ?",
                  "score": 2,
                  "created_utc": "2026-01-10 23:43:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz34x30",
                  "author": "loess4u",
                  "text": "I thought the annual subscription fee was $288, not $28.   \nCould you please share the link if it's possible to subscribe for $28?",
                  "score": 2,
                  "created_utc": "2026-01-12 02:01:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzka7g2",
              "author": "twistyx808",
              "text": "I thought the reasoning on DeepSeek sucked,Â  but hopefully V4 is miles better",
              "score": 1,
              "created_utc": "2026-01-14 16:28:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nym3qys",
          "author": "Former-Tangerine-723",
          "text": "Yep its January again. Time for a DeepSeek disruption",
          "score": 63,
          "created_utc": "2026-01-09 15:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz35499",
              "author": "loess4u",
              "text": "I'm really looking forward to it. I hope DeekSeek releases an annual coding plan too.",
              "score": 3,
              "created_utc": "2026-01-12 02:02:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nylnvk6",
          "author": "WeMetOnTheMountain",
          "text": "I love deepseek, it's great, especially if you just want to hammer an API for damn near no money.  The local stuff is good too.",
          "score": 55,
          "created_utc": "2026-01-09 14:31:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylu1kl",
          "author": "No_Afternoon_4260",
          "text": "If they integrated mHC and deepseek-ocr (*10 text \"encoded\" via images) for long prompt, might be a beast! Can't wait to see it",
          "score": 19,
          "created_utc": "2026-01-09 15:01:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoan75",
              "author": "__Maximum__",
              "text": "Yep, deepseek 3.2 with OCR and mHC, trained on their synthetic data, would probability beat all closed source models. I mean, 3.2 speciale was already SOTA. This is not far-fetched.",
              "score": 4,
              "created_utc": "2026-01-09 21:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyozh5k",
                  "author": "No_Afternoon_4260",
                  "text": "Deepseek ocr was also how to compress ctx times 10 by encoding images with text inside.",
                  "score": 3,
                  "created_utc": "2026-01-09 23:47:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylv05m",
              "author": "Toxic469",
              "text": "Was just thinking about mHC - feels a bit early though, no?",
              "score": 3,
              "created_utc": "2026-01-09 15:05:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylxy14",
                  "author": "No_Afternoon_4260",
                  "text": "If they published it I guess it means they consider it mature, to what extent idk ðŸ¤·  \nWhat they published with deepseek ocr, I feel could be big. Let's put back some encoders into these decoder-only transformers!",
                  "score": 8,
                  "created_utc": "2026-01-09 15:19:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynkeop",
                  "author": "Mvk1337",
                  "text": "pretty sure that article was written in 2025 january but published 2026, so not really early.",
                  "score": 3,
                  "created_utc": "2026-01-09 19:42:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymu2wv",
          "author": "vincentz42",
          "text": "I fully believe DeepSeek will release something in Feb, before the Chinese New Year, as they love to drop things before Chinese public holidays.\n\nWith that being said, I won't read too much into the Information report for companies in China. To have these insider reports you must have contacts, verify their identity, and then verify their claims. The information might have a ton of contacts in the bay area, but does it in China?",
          "score": 16,
          "created_utc": "2026-01-09 17:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylm0au",
          "author": "SlowFail2433",
          "text": "Ok weeks is faster than I was expecting, maybe 2026 is gonna be a fast iteration year. Their coding performance claims are big. I rly hope the math and agentic improvements are also good\n\n\nMakes it difficult to decide whether to invest more in training/inference for the current models, or to hold off and wait for the new ones",
          "score": 21,
          "created_utc": "2026-01-09 14:21:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylmmqz",
              "author": "MaxKruse96",
              "text": "they can just gut the math and replace it with code tbh",
              "score": 8,
              "created_utc": "2026-01-09 14:24:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylydi8",
                  "author": "SlowFail2433",
                  "text": "Pros and cons, of generalists vs specialists\n\n\nI do also lean towards wanting specialist LLMs\n\n\nBut these weights are so large, for the big models, that requiring a second set of weights for your deployment is a big cost increase",
                  "score": 7,
                  "created_utc": "2026-01-09 15:21:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylwbhy",
              "author": "chen0x00",
              "text": "It is almost certain that several Chinese companies will release new models before the Chinese New Year.",
              "score": 3,
              "created_utc": "2026-01-09 15:11:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylyfsv",
                  "author": "SlowFail2433",
                  "text": "When is that?",
                  "score": 3,
                  "created_utc": "2026-01-09 15:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylog4k",
          "author": "Monkey_1505",
          "text": "Unlikely IMO. Their recent paper suggests not only a heavier pre-train, but also the use of a much heavier post-training RL. The next model will likely be a large leap and take a little longer to cook.",
          "score": 32,
          "created_utc": "2026-01-09 14:34:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyo8b85",
              "author": "__Maximum__",
              "text": "3.2 was released on December 1st. By the time they released the model and the paper, they may have already started with their \"future work\" chapter in the paper. They are famous for spending way less on compute for the same performance gain, and now, with more stable training with mHC, their latest efficient architecture, AND their synthtic data generarion, it should be even more efficient. I can't see why they wouldn't have a model right now that is maybe not ready for release yet, but better in coding than anything we've seen.",
              "score": 9,
              "created_utc": "2026-01-09 21:32:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyofawt",
                  "author": "Monkey_1505",
                  "text": "They mentioned specifically using more pre-training, and a similar proportion (and also more relatively) of post-training RL in order to fully catch up with SOTA closed labs, which they noted open source has not been doing. \n\nThis implies, IMO, at least months worth of training overall. And likely months just for the pre-training. Ie, all those efficiency gains turned into performance. It's possible the rumour is based on some early training though.\n\nThe information is great on financial stuff, but frequently inaccurate on business speculation. They've been pumping out a lot of AI related speculation recently. Just my opinion in any case.",
                  "score": 2,
                  "created_utc": "2026-01-09 22:04:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylwgxv",
              "author": "SlowFail2433",
              "text": "Which paper?",
              "score": 5,
              "created_utc": "2026-01-09 15:12:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymhrwc",
                  "author": "RecmacfonD",
                  "text": "Should be this one: \n\nhttps://arxiv.org/abs/2512.02556\n\nSee '*Conclusion, Limitation, and Future Work*' section.",
                  "score": 16,
                  "created_utc": "2026-01-09 16:48:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nym3uaq",
                  "author": "Monkey_1505",
                  "text": "The last model they put out scaled the RL a lot, and they talked about hitting the frontier with this approach using much more pre-train. I didn't actually read it, I just saw a thread summary on SM.",
                  "score": 2,
                  "created_utc": "2026-01-09 15:46:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylzubs",
                  "author": "Master-Meal-77",
                  "text": "!RemindMe 1 week",
                  "score": 2,
                  "created_utc": "2026-01-09 15:28:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyln6ta",
          "author": "MasterDragon_",
          "text": "And the whale is back.",
          "score": 11,
          "created_utc": "2026-01-09 14:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylmti3",
          "author": "Semi_Tech",
          "text": "300$ to read said article :P",
          "score": 10,
          "created_utc": "2026-01-09 14:25:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmb68p",
              "author": "Embarrassed_Bread_16",
              "text": "[https://archive.ph/QCvTD](https://archive.ph/QCvTD)",
              "score": 1,
              "created_utc": "2026-01-14 21:56:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nylr4hc",
          "author": "Orolol",
          "text": "> Preliminary internal benchmark tests conducted by DeepSeek employees indicate the model outperforms existing mainstream models in code generation, including Anthropicâ€™s Claude and the OpenAI GPT family.  \n  \nI would be delighted if this is true, but I honestly doubt it. Every models that claim that, even with stronger benchmark, fall short in real dev experience.",
          "score": 10,
          "created_utc": "2026-01-09 14:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym9id5",
              "author": "aeroumbria",
              "text": "Agent harnesses are likely biased towards the models their developers use and the models with most raised tickets. However, with more capable open models, I expect to see more and more model-neutral harnesses that will be less preferentially tuned.",
              "score": 3,
              "created_utc": "2026-01-09 16:12:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymrz03",
              "author": "EtadanikM",
              "text": "It depends on what people evaluate it on. Claude is supreme in Claude Code for the obvious reason that Anthropic likely fine tunes it on that framework from the ground up, while models like Deep Seek have to be more generalist because Claude is banned in China.Â \n\nNot to mention, closed source models are APIs more so than they are raw models. Thereâ€™s lots of things theyâ€™re doing in the pipeline that an open model would never be able to replicate - e.g. funneling outputs to separate models, RAGs, etc.Â \n\nThe raw model might be stronger but without the framework around it, itâ€™s never going to match up to closed source services.Â ",
              "score": 2,
              "created_utc": "2026-01-09 17:34:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynqqf6",
                  "author": "Orolol",
                  "text": "Qwen code is miles away Claude Code.",
                  "score": 1,
                  "created_utc": "2026-01-09 20:11:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylnzc5",
          "author": "Leflakk",
          "text": "Good news even if not much people can really use it locally",
          "score": 4,
          "created_utc": "2026-01-09 14:31:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylxg3b",
              "author": "pmttyji",
              "text": "Hope they release something in 100-200B(MOE) range additionally.",
              "score": 10,
              "created_utc": "2026-01-09 15:17:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nylpzjw",
          "author": "MikeRoz",
          "text": "This thread appears to be a duplicate of this one: https://www.reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/",
          "score": 4,
          "created_utc": "2026-01-09 14:41:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymbto1",
          "author": "dampflokfreund",
          "text": "Still no multimodality?",
          "score": 7,
          "created_utc": "2026-01-09 16:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyo9nja",
              "author": "__Maximum__",
              "text": "Imo, it's nice, but it is a waste of resources. Same for continual learning or anything that does not add to the raw intelligence of the model. The fact is, you can solve the hardest problems on earth within a couple of thousands tokens without any multimodality or continual learning. Tool calling is much more important because that lets the model generate data and learn from it. It's a source of truth.",
              "score": 11,
              "created_utc": "2026-01-09 21:38:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyr2xne",
                  "author": "Karyo_Ten",
                  "text": "Why would multimodality not add to intelligence. Babies learn physics through sight, touch and sound.\n\nThe more sources of information the better the internal representation.",
                  "score": 3,
                  "created_utc": "2026-01-10 07:47:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylpjul",
          "author": "Guboken",
          "text": "How much VRAM are we talking about to run it in a usable way?",
          "score": 3,
          "created_utc": "2026-01-09 14:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylqe70",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 5,
              "created_utc": "2026-01-09 14:43:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynmt8q",
                  "author": "FlamaVadim",
                  "text": "about 4 kidneys ðŸ«¤",
                  "score": 5,
                  "created_utc": "2026-01-09 19:52:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylsa03",
          "author": "FullOf_Bad_Ideas",
          "text": ">The sources said the V4 model achieves a technical breakthrough in handling and parsing very long code prompts, a significant practical advantage for engineers working on complex software projects.\n\nDoes it sound like DSA, vision token compaction (DeepSeek OCR paper) or some new tech?",
          "score": 3,
          "created_utc": "2026-01-09 14:52:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynt6kq",
          "author": "warnerbell",
          "text": "    \"Technical breakthrough in handling and parsing very long code prompts\" - We'll see about that...lbs\n    \n    Context length is table stakes now. What matters is how well the model actually uses that context. Most models weight beginning and end heavily, ignoring the middle.\n    \n    Hopefully V4 addresses the attention distribution problem not just extend the window.",
          "score": 3,
          "created_utc": "2026-01-09 20:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp68zf",
          "author": "placebomancer",
          "text": "I'm looking forward to it, but DeepSeek's models have become less and less creative and unrestrained with each release. I'm much more excited for the next Kimi release.",
          "score": 3,
          "created_utc": "2026-01-10 00:23:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypdzon",
          "author": "jeffwadsworth",
          "text": "Deepseek chat site is just about the most miraculous thing around.  It handles massive code files easily and wonâ€™t slow to a crawl after analyzing those files and refactoring them with ease.  Love it for non-business work.",
          "score": 3,
          "created_utc": "2026-01-10 01:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr0fd3",
          "author": "TheInfiniteUniverse_",
          "text": "quite possibly the new V4 is going to be a derivative or a better version of Speciale (for instance Speciale + tool calling) which was expired on Dec 15th. \n\nThis is going to be super interesting.",
          "score": 3,
          "created_utc": "2026-01-10 07:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr2qi5",
          "author": "IngenuityNo1411",
          "text": ">According to two people with direct knowledge\n\nMan, I'm really anticipate DeepSeek is cooking something BIG but I'd be skeptical about this. Wouldn't it be a \"R2 moment\" once again?",
          "score": 3,
          "created_utc": "2026-01-10 07:45:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylukg5",
          "author": "arousedsquirel",
          "text": "I am wondering if it is going to incorporate the 2000 party questions alignement",
          "score": 2,
          "created_utc": "2026-01-09 15:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylvdnc",
          "author": "alsodoze",
          "text": "from the information? nope.",
          "score": 2,
          "created_utc": "2026-01-09 15:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylzvm9",
          "author": "power97992",
          "text": "So it will be the same number of parameters.. i thought they were gonna increase pretraining and release a new and bigger model",
          "score": 2,
          "created_utc": "2026-01-09 15:28:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymjz1p",
          "author": "No_Egg_6558",
          "text": "If it isnâ€™t the great announcement of the announcement that there will be a great announcement.",
          "score": 2,
          "created_utc": "2026-01-09 16:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynj8l7",
              "author": "Silver-Champion-4846",
              "text": "!announceme 1 month",
              "score": 2,
              "created_utc": "2026-01-09 19:36:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyn031a",
          "author": "Exotic_Jellyfish_548",
          "text": "[https://www.instagram.com/p/DTTKKWoEaRk/](https://www.instagram.com/p/DTTKKWoEaRk/)",
          "score": 2,
          "created_utc": "2026-01-09 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyn0dex",
          "author": "Curious_Emu6513",
          "text": "will it use the new deepseek v3.2â€™s sparse attention?",
          "score": 2,
          "created_utc": "2026-01-09 18:12:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynr7uk",
          "author": "terem13",
          "text": "Very good news indeed, I'm long time active user of Deepseek models, their quality for my domain tasks had proven indispensable.\n\nWould be very interesting, how do they perform on coding. These types of tasks require longâ€‘form reasoning and AFAIK DeepSeekâ€‘V3.2â€‘Speciale is explicitly trained with reduced length penalty during RL.\n\nIn turn, this is a key enabler to produce extended reasoning traces and good models for coding. Let's see.",
          "score": 2,
          "created_utc": "2026-01-09 20:13:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrftfk",
          "author": "Imperator_Basileus",
          "text": "Time to sell off nvidia stocks, comrades.Â ",
          "score": 2,
          "created_utc": "2026-01-10 09:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrx3fo",
          "author": "Previous_Raise806",
          "text": "im calling it now, it will be worse than Gemini, ChatGPT and Claude.",
          "score": 2,
          "created_utc": "2026-01-10 12:20:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysa04i",
          "author": "Far_Background691",
          "text": "I believe the deepseek will reveal a new model in several weeks but i don't believe the Information really got the insiders' \"leaks\". This is not the deepseek's style. Besides, if it was, why deepseek only leaked this message to a western media? I view this report as a case of expectation management in case deepseek really shocks the capital market again.",
          "score": 2,
          "created_utc": "2026-01-10 13:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzhenv",
          "author": "Dusty170",
          "text": "I don't really use AI for coding, I mostly RP with them, I've tried quite a few but deepseek 3.2 seems to be the best for that in my testing. I wonder how a v4 would be in this regard.",
          "score": 2,
          "created_utc": "2026-01-11 15:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylvdif",
          "author": "Few_Painter_5588",
          "text": "I personally hope it has more active parameters, maybe 40-50 billion instead of 30",
          "score": 3,
          "created_utc": "2026-01-09 15:07:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoazfh",
              "author": "__Maximum__",
              "text": "Why? Why not less like 7b? Although I believe it they have not started from scratch, but continued on 3.2.",
              "score": 2,
              "created_utc": "2026-01-09 21:44:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyohy7h",
                  "author": "Few_Painter_5588",
                  "text": "The active parameters still play a major part in the overall depth and intelligence of a model. Most 'frontier' models are well above 100 Billion active parameters",
                  "score": 2,
                  "created_utc": "2026-01-09 22:17:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyqotim",
              "author": "Lesser-than",
              "text": "I hope both, a big version to compete with api llms, and academic smaller versions for smaller labs to realisticly expand upon.",
              "score": 0,
              "created_utc": "2026-01-10 05:49:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymd6uo",
          "author": "ZucchiniMore3450",
          "text": "when someone says \"Claude\" and not \"Claude Opus\" that usually means \"Sonnet\".\n\n\nSo this news says \"opus will still be much better than us\"?",
          "score": 3,
          "created_utc": "2026-01-09 16:28:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyltanu",
          "author": "celsowm",
          "text": "I want to believe.jpeg",
          "score": 2,
          "created_utc": "2026-01-09 14:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylsf2r",
          "author": "Middle_Bullfrog_6173",
          "text": "The combination of weeks away and already outperfoming top models in coding seems unlikely. Good coding performance comes pretty late in the post training run.",
          "score": 1,
          "created_utc": "2026-01-09 14:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5u2b9",
          "author": "Airforce083",
          "text": "It's much worse if you can't call the tool",
          "score": 1,
          "created_utc": "2026-01-12 13:53:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym1ovf",
          "author": "Sockand2",
          "text": "2 days before i receive this information from my LLM news. I thought it was a LLM allucination because it compared with Claude 3.5 and GPT4.5\n\n[https://alyvro.com/blog/deepseek-news-today-jan-2026-updates-major-breakthroughs?utm\\_source=chatgpt.com](https://alyvro.com/blog/deepseek-news-today-jan-2026-updates-major-breakthroughs?utm_source=chatgpt.com)\n\nNow, with this news, i am not sure what to think",
          "score": 1,
          "created_utc": "2026-01-09 15:36:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nynzw4z",
          "author": "Long_comment_san",
          "text": "Seriously, aren't we basically at the end of the \"coding!\" request being the central point? I'm not coding myself but it feels that modern models can code and self-test just fine. I've seen people code here with Qwen 30, so...",
          "score": 1,
          "created_utc": "2026-01-09 20:53:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyosuvm",
              "author": "SlowFail2433",
              "text": "The agentic coding is different type",
              "score": 4,
              "created_utc": "2026-01-09 23:12:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5m2n6",
      "title": "A 30B Qwen Model Walks Into a Raspberry Piâ€¦ and Runs in Real Time",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/52juwyqq0rbg1.jpeg",
      "author": "ali_byteshape",
      "created_utc": "2026-01-06 15:45:12",
      "score": 489,
      "num_comments": 76,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "ny41hn9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-07 00:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny13qwu",
          "author": "Hot_Turnip_3309",
          "text": "You'll get double the tokens per second with Mamba2 hybrid transformers, aka nemotron-3-nano-30b-a3b",
          "score": 100,
          "created_utc": "2026-01-06 16:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1eptk",
              "author": "__Maximum__",
              "text": "I upvoted your comment although I tried the one on ollama and it was slower than qwen 3 30b, which was i guess a fluke. In theory, it should be faster and smarter.",
              "score": 32,
              "created_utc": "2026-01-06 17:14:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1x27u",
                  "author": "Fresh_Finance9065",
                  "text": "It should excel in long context or high token scenarios. But it needs the latest optimizations to do so. Not sure if ollama has those optimization but llamacpp does now",
                  "score": 20,
                  "created_utc": "2026-01-06 18:36:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny5uvlg",
              "author": "skinnyjoints",
              "text": "What is a hybrid transformer? I just wrapped my head around multi head latent attention. Is this something along those lines?",
              "score": 4,
              "created_utc": "2026-01-07 07:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny70vgl",
                  "author": "fuckingredditman",
                  "text": "yannic kilcher IIRC has decent videos explaining mamba architecture (which is basically an adapted state space model architecture that was iterated upon a couple of times) https://www.youtube.com/watch?v=9dSkvxS2EB0 but tbh i'm also not sure about the recent hybrid models. i assume it's just some combination of both mechanisms, because both have downsides (transformers are more memory/compute intensive but traditional attention is more precise/accurate, mamba2 is just super fast and can handle long context with low memory overhead but afaik it just doesn't attend as well within that context)",
                  "score": 2,
                  "created_utc": "2026-01-07 13:19:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6ecgi",
                  "author": "Fresh_Finance9065",
                  "text": "I don't really understand how it works, but I believe the hybrid transformers attempt to use mamba layers instead of traditional layers to change how tokens are handled.\n\nMemory usage grows exponentially with token count for traditional layers, while token count for mamba layers only grow memory usage linearly.",
                  "score": 1,
                  "created_utc": "2026-01-07 10:34:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3s3pm",
          "author": "geerlingguy",
          "text": "Tested on my Pi 5, had to set context to `-c 4096` before it would run without segfaulting after model loading. But ran with:\n\n```\n./build/bin/llama-cli -m \"models/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-2.70bpw.gguf\" -c 4096 -e --no-mmap -t 4\n```\n\nAnd for a few prompts, it gave between 10-11 t/s prompt processing, and 4-8 t/s token generation (lower with much larger outputs, but on average it was around 7 t/s).\n\nImpressive! Qwen3 30B MoE is much more useful than like llama 3.2:3b on a Pi, though the 16GB Pi 5 is a bit more rare.",
          "score": 32,
          "created_utc": "2026-01-06 23:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny42ek6",
              "author": "ali_byteshape",
              "text": "Thanks, Jeff, for testing and sharing this. Huge fan of your work and YouTube channel! :)",
              "score": 15,
              "created_utc": "2026-01-07 00:44:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny44yyt",
                  "author": "geerlingguy",
                  "text": "Thanks for sharing the model, love seeing more functionality packed into smaller devices!",
                  "score": 12,
                  "created_utc": "2026-01-07 00:58:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1qvc1",
          "author": "florinandrei",
          "text": "nerds + AI = diagrams that are tacky as hell",
          "score": 54,
          "created_utc": "2026-01-06 18:08:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3nos2",
              "author": "Other_Hand_slap",
              "text": "yeah, i dont knwo why i am vasting my time here",
              "score": -6,
              "created_utc": "2026-01-06 23:28:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1vp6d",
          "author": "pgrijpink",
          "text": "Exciting and disappointing at the same time. If Iâ€™m not mistaken, your algorithm is not open source? So not as useful.",
          "score": 15,
          "created_utc": "2026-01-06 18:30:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1189p",
          "author": "iKy1e",
          "text": "This sounds amazing! Iâ€™ll have dig into the details later when I have more time, but really wanted to say this sort of low level optimism finding ways to squeeze more performance until smaller devices is amazing! I love reading about research like this.",
          "score": 9,
          "created_utc": "2026-01-06 16:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2lof4",
          "author": "Watchforbananas",
          "text": "I like the improved graphs on your blog that now shows the exact quant from unsloth and magicQuant on hover (and that you added comparisons with magicQuant in the first place). Much easier to pick an interesting quant based on what I've tried before. Could you perhaps do the same for your quants? Just so I don't have to search for the lookup table. \n\nI also appreciate the 4080 results.\n\nAny plans to update the graphs for the other models as well? I've never managed to quite figure out what unsloth quants your graphs for qwen3-4B-2507 referenced.",
          "score": 10,
          "created_utc": "2026-01-06 20:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymmcdx",
              "author": "enrique-byteshape",
              "text": "Hey! Thank you for the comment of appreciation. We just updated the graphs for the 30B model to include the file names they correspond to on HuggingFace. We will keep doing this moving forward, so thank you for the comment. As for retroactive updates to the graphs, we'll try to get them up to date too.",
              "score": 2,
              "created_utc": "2026-01-09 17:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny17azk",
          "author": "bigh-aus",
          "text": "I wonder if this could be combined with an exo like solution to run on a cluster of pis...  I'm still pretty dumb with these models, but it makes me wonder if the MOE can be spread across pis.\n\n>Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieveÂ **8.03 TPS**Â at 2.70 BPW, while retainingÂ **94.18% of BF16 quality**.\n\nI'm assuming that means it's a 4 bit quant...",
          "score": 18,
          "created_utc": "2026-01-06 16:40:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny19oat",
              "author": "ali_byteshape",
              "text": "Iâ€™m not an expert on Pi clusters, but it should be doable if you have several Pis with less memory.\n\nOn the quant side: this specific model is **2.7 bits per weight** on average. We learned what precision each tensor should use to maximize throughput, so some layers end up 2-bit, some 3-bit, some 4-bit, etc. The average is 2.7 BPW with all quantization overheads included, so itâ€™s not a â€œ4-bit quantâ€ in the usual sense.",
              "score": 18,
              "created_utc": "2026-01-06 16:51:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny8oqzu",
                  "author": "Dr_Allcome",
                  "text": "What exactly is your quality metric? Because i refuse to believe a model at that quantisation retains 94% of bf16 by anything other than \"it outputs tokens\".",
                  "score": 1,
                  "created_utc": "2026-01-07 18:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1akwa",
          "author": "Odd-Ordinary-5922",
          "text": "can you guys try doing this with nemotron?",
          "score": 8,
          "created_utc": "2026-01-06 16:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1fito",
              "author": "enrique-byteshape",
              "text": "Our main current bottleneck is evaluating the quants we produce, so we are currently taking a slower approach to releasing new quants because we want to provide the evaluation as well to the community. We plan on releasing a wide range of models in the coming months, so we'lll add Nemotron to the list of possible models :)",
              "score": 7,
              "created_utc": "2026-01-06 17:17:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1j0za",
          "author": "frozen_tuna",
          "text": "That's actually insane. Well done!",
          "score": 7,
          "created_utc": "2026-01-06 17:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1rgw7",
          "author": "AvocadoArray",
          "text": "Just finished reading the blog post, nice work!\n\nWould love to see you do Seed-OSS 36B next.",
          "score": 7,
          "created_utc": "2026-01-06 18:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1085p",
          "author": "dodiyeztr",
          "text": "Would this work on raspi 5 8GB?",
          "score": 4,
          "created_utc": "2026-01-06 16:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny11jmw",
              "author": "DuckyBlender",
              "text": "No, 16GB minimum and even that is small",
              "score": 15,
              "created_utc": "2026-01-06 16:14:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1gkmz",
          "author": "xandep",
          "text": "Already using the Instruct version and I liked. IQ-3 is about the same size / speed of a ptbr-REAP-16B of the original model that I use, and  initially it seems your model performs better.",
          "score": 5,
          "created_utc": "2026-01-06 17:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1b0uq",
          "author": "xandep",
          "text": "Any plans on Thinking model?",
          "score": 3,
          "created_utc": "2026-01-06 16:57:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1efar",
              "author": "enrique-byteshape",
              "text": "Yes, thinking models are planned, but we have some things to iron out relating to evaluation. We already have some internal tests on thinking models and are actively working on them.",
              "score": 5,
              "created_utc": "2026-01-06 17:12:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1c2iv",
          "author": "Sensitive_Sweet_1850",
          "text": "wow. you should try nemotro too",
          "score": 3,
          "created_utc": "2026-01-06 17:01:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1zrry",
          "author": "pmttyji",
          "text": "Nice. I tried your Qwen3-4B-Q5\\_K\\_S which gave me 20 t/s same as what other provider's Q4 given me on CPU-Only performance.\n\nHope your backlog has 12-14-15B models which are better & useful for 8GB VRAM. Ex: Qwen3-14B's Q4\\_K\\_M(8.4GB) won't fit inside VRAM which gave me just 5 t/s. Then I picked IQ4\\_XS(7.5GB) which gave me 20+ t/s.",
          "score": 3,
          "created_utc": "2026-01-06 18:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny25vpy",
          "author": "SlavaSobov",
          "text": "Bitchin' work would be cool to see if the 8GB Jetson Orin Nano could get some improvements. \n\nNVIDIA basically says here's some basic old models and lets it languish. It's Ampere so maybe could have some gains.\n\nI stuck to mostly 4B models but feels like there's more potential there. \n\nI'm just a dumb hobbyist who knows enough to break things though. ðŸ˜…",
          "score": 3,
          "created_utc": "2026-01-06 19:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3n0s3",
          "author": "Other_Hand_slap",
          "text": "i was able to run llama forninference on \n* 13th gen i3\n * 16g ram\n* nvidia 3060 ti with 8g\n\nwith a confident fair rate of 20 tokens/s",
          "score": 3,
          "created_utc": "2026-01-06 23:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0zs20",
          "author": "professormunchies",
          "text": "sounds promising and pretty cool. I'll give it a try today with cline and continue.dev.\n\nI've been running this on some smaller hardware: [https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit](https://huggingface.co/cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit)\n\nI like that they specify which dataset was used for calibrating the quants. Would be nice if you guys divulged such information. As far as evals go, definitely checkout the nemotron collection, lots of good datasets: [https://huggingface.co/collections/nvidia/nemotron-post-training-v3](https://huggingface.co/collections/nvidia/nemotron-post-training-v3)",
          "score": 4,
          "created_utc": "2026-01-06 16:05:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3m9n9",
              "author": "professormunchies",
              "text": "Following up, the model has difficulty perform file edits when using cline however it was able to read files okay during the planning phase. It had to fail a few times before it was able to finally get the edits working, probably due to adding the file with @ in the prompt and it mangling the formatting some how. Model worked well in answering with some emojis (in the classic qwen style) when using the continue dev extension. Normal chat Q/A works great so I tried something more complex afterwards, using it with a the Context7 MCP through the chat interface of LMstudio. It worked well for the first message and then started always using the mcp in subsequent messages rather than just answering with the context it has. It kept saying it was a helpful assistant based on whatever repo I asked about without actually answering. Speed seems good too. I used the default that shows up in LMStudio: Q4\\_K\\_S on a m4 max",
              "score": 3,
              "created_utc": "2026-01-06 23:21:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2wsyh",
          "author": "Noiselexer",
          "text": "Time to first token: 2 minutes",
          "score": 3,
          "created_utc": "2026-01-06 21:19:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3gvz2",
              "author": "solarkraft",
              "text": "Makes all the sense since token generation is mostly memory bound but prompt processing requires compute!",
              "score": 3,
              "created_utc": "2026-01-06 22:54:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny3k6sq",
              "author": "KadahCoba",
              "text": "Thanks, was wondering this too.\n\nWhat was the prompt/context length for that?",
              "score": 1,
              "created_utc": "2026-01-06 23:10:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny12civ",
          "author": "_raydeStar",
          "text": "Would it work on a Pi 5 with 8GB?  Do I need that AI hat that they're offering?\n\nI'll give it a shot if I can, I love projects like this.  I'm REALLY interested in something like VL, to build a home automation system.",
          "score": 2,
          "created_utc": "2026-01-06 16:17:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny162o2",
              "author": "ali_byteshape",
              "text": "Probably not on an 8 GB Pi 5, sadly.\n\nEven the smallest model in this release needs 10+ GB of RAM just to load the weights, before you add KV cache, prompt/context, and runtime overhead. So an 8 GB Pi will hit the wall fast (and mmap usually just turns â€œwonâ€™t loadâ€ into â€œthrashes and crawlsâ€).\n\nAnd the AI HAT wonâ€™t fix this. Those hats mainly add compute power, but they do not add system memory, so they canâ€™t solve a â€œmodel does not fit in memoryâ€ problem.",
              "score": 6,
              "created_utc": "2026-01-06 16:34:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny25do8",
                  "author": "MoffKalast",
                  "text": "Well trying to fit a 30B into 16GB is already kind of a fool's errand given the 2 bit quant you had to go down to, why not try something more sane, like a dense 7-14B? Won't be as fast as a 3B of course.",
                  "score": 1,
                  "created_utc": "2026-01-06 19:13:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1lkgb",
          "author": "siegfried2p",
          "text": "which quant is better for moe model with expert to cpu scenario?",
          "score": 2,
          "created_utc": "2026-01-06 17:45:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1tt30",
              "author": "ali_byteshape",
              "text": "The first table in the model card (https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF#cpu-models) lists CPU-friendly variants. You can choose a model based on your tolerance for quality loss versus speed. For example, KQ-2 is on the faster end, while KQ-5 is still fast and retains roughly 98% of baseline quality.",
              "score": 3,
              "created_utc": "2026-01-06 18:21:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny26hgr",
          "author": "Chromix_",
          "text": "In the Intel i7 section of the blog post the Unsloth Q5\\_K\\_M quant gets a better test score than the Q8\\_K\\_XL. So either that quant won the lottery or the benchmark results are more noisy than it looks like and don't really tell us that much with results being that close together. It'd be great to see more accurate results that prove that this method delivers smaller ( = faster) quants at the same quality level, but benchmarking this is difficult, [as written before](https://www.reddit.com/r/LocalLLaMA/comments/1pj7wjd/we_did_years_of_research_so_you_dont_have_to/ntbiyem/?context=3#ntbiyem). Maybe repeat each run a few times to get a better idea of the variance? Or check how many right/wrong answers flip between each run, to get an idea of the magnitude of randomness involved?",
          "score": 2,
          "created_utc": "2026-01-06 19:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2fwuf",
              "author": "ali_byteshape",
              "text": "Excellent observation. With todayâ€™s libraries itâ€™s hard to guarantee fully deterministic behavior, so some noise is expected. We repeated a subset of runs 3 to 4 times to estimate variance, and the results were fairly consistent. Each score also aggregates tens of thousands of questions and tasks, which helps average out randomness.\n\nAlso, more bits generally reduce reconstruction error, but that does not guarantee better downstream scores. Quantization can act like a regularizer and sometimes slightly improves accuracy. In this case, Q5\\_K\\_M (5.7 bpw) and Q8\\_K\\_XL (9.4 bpw) are both very close to baseline, so the extra bits do not seem to buy much. We also show itâ€™s possible to push BPW down to \\~4.7 with ShapeLearn while still matching baseline quality.",
              "score": 3,
              "created_utc": "2026-01-06 20:01:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3sngb",
          "author": "nickgeorgiou",
          "text": "I thought this was Agario",
          "score": 2,
          "created_utc": "2026-01-06 23:54:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5h8ke",
              "author": "HibikiAss",
              "text": "I thought it was OSU!",
              "score": 1,
              "created_utc": "2026-01-07 05:43:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny42z5f",
          "author": "DarkGeekYang",
          "text": "Nice effort. Will you consider adding more models like qwen3vl to your repo?",
          "score": 2,
          "created_utc": "2026-01-07 00:47:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny49cxt",
              "author": "enrique-byteshape",
              "text": "We are considering adding many models to our repo, but as my colleague has pointed out (like we do in our blog post), our current main bottleneck is evaluation, so the pacing for releases might be a bit slower. But we are on it! Thinking and VL are in our TO-DOs",
              "score": 2,
              "created_utc": "2026-01-07 01:22:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3fv07",
          "author": "ApprehensiveAd3629",
          "text": "which models do you recommend for the raspberry pi5 8gb? i can also try in a orange pi 5 to test and compare.",
          "score": 1,
          "created_utc": "2026-01-06 22:49:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3i7tq",
              "author": "enrique-byteshape",
              "text": "Sorry to say that none of the quants from this release will fit on 8GB... We did release a 4B Qwen model last time that will fit and should run fairly well if you want to test it out! Hopefully (if time permits), we will try to release models in the 10-20B range that will hopefully fit on 8GB, so stay tuned :)",
              "score": 2,
              "created_utc": "2026-01-06 23:00:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny41dpy",
                  "author": "ApprehensiveAd3629",
                  "text": "nice! i will try qwen 3 4b, its a good model.\n\nhow many tokens/sec may i get using this model?",
                  "score": 1,
                  "created_utc": "2026-01-07 00:39:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3v3sz",
          "author": "Fault23",
          "text": "wth",
          "score": 1,
          "created_utc": "2026-01-07 00:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4lr5i",
          "author": "owaisted",
          "text": "What would you suggest on a 6gb vram 3060 and 16gb ram",
          "score": 1,
          "created_utc": "2026-01-07 02:29:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4y1zj",
              "author": "ali_byteshape",
              "text": "I havenâ€™t tried partial offloading yet, but Iâ€™d expect the CPU-optimized models to work better in that setup. You could try [KQ-5](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-Q3_K_S-3.25bpw.gguf) (CPU-optimized) and[ IQ-4 ](https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-IQ3_S-3.29bpw.gguf)(GPU-optimized). Theyâ€™re almost the same size, so it would be interesting to see which one performs better in practice.\n\nWould love it if you could share your findings with us too ðŸ™‚",
              "score": 1,
              "created_utc": "2026-01-07 03:37:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny7f8rn",
                  "author": "owaisted",
                  "text": "Thank you",
                  "score": 2,
                  "created_utc": "2026-01-07 14:37:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6m4s9",
          "author": "b4rtaz_",
          "text": "I think it would be interesting to see this model running in a distributed setup on two Raspberry Pi devices (or 4), check: [https://github.com/b4rtaz/distributed-llama/discussions/255](https://github.com/b4rtaz/distributed-llama/discussions/255)",
          "score": 1,
          "created_utc": "2026-01-07 11:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny713y9",
              "author": "enrique-byteshape",
              "text": "Sadly we only have one Pi to test this on, but if anyone is able to do it, please go ahead! It'll be fun hearing about the project and about how well it runs with our larger quants",
              "score": 1,
              "created_utc": "2026-01-07 13:20:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nya1iqz",
          "author": "synth_mania",
          "text": "the y axis has been cropped lol",
          "score": 1,
          "created_utc": "2026-01-07 21:38:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny244od",
          "author": "MoffKalast",
          "text": "GPU behaviour? You're getting 8 tg from the potato Videocore 7 with Vulkan!?",
          "score": 0,
          "created_utc": "2026-01-06 19:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny26l5o",
              "author": "ali_byteshape",
              "text": "Please take a look at the Blog for 4080-5090 results :)",
              "score": 3,
              "created_utc": "2026-01-06 19:19:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny28pvu",
                  "author": "MoffKalast",
                  "text": "Ah alright. You know something strikes me as slightly odd though, the lack of any KV cache mention on the blog. In my experience the model \"fitting\" onto a GPU means jack shit when all of it but a few layers then gets pushed out when you load any kind of actual context length and you're back to pedestrian speeds. What lengths did you test this \"fit\" with? With --no-kv-offload?",
                  "score": -1,
                  "created_utc": "2026-01-06 19:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny1cxho",
          "author": "HealthyCommunicat",
          "text": "How and what are people using 8 tok/s for? What software or development company would be okay with that kind of speed unless the machine is only for image or chatbots",
          "score": -5,
          "created_utc": "2026-01-06 17:05:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1et54",
              "author": "enrique-byteshape",
              "text": "We achieve much higher TPS on other hardware. 8 TPS is on a Pi 5, which is a very constrained piece of hardware. Most quants of this model don't even load on a Pi, or run very very slowly",
              "score": 9,
              "created_utc": "2026-01-06 17:14:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny2bzi6",
                  "author": "HealthyCommunicat",
                  "text": "that doesnt answer my question.",
                  "score": -2,
                  "created_utc": "2026-01-06 19:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny25z4n",
                  "author": "CaptParadox",
                  "text": "I'm with u/HealthyCommunicat what's the point? just to say you did? like playing doom on a tractor gps screen?",
                  "score": -2,
                  "created_utc": "2026-01-06 19:16:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q88hdc",
      "title": "(The Information): DeepSeek To Release Next Flagship AI Model With Strong Coding Ability",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q88hdc",
      "author": "Nunki08",
      "created_utc": "2026-01-09 13:39:02",
      "score": 483,
      "num_comments": 102,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q88hdc/the_information_deepseek_to_release_next_flagship/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nylfik0",
          "author": "ResidentPositive4122",
          "text": "Called it when they updated the v3 paper with lots of details, all hyperparams and so on. Bring it on! More models is always good for everyone.",
          "score": 105,
          "created_utc": "2026-01-09 13:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nym5ilz",
              "author": "VoidAlchemy",
              "text": "I'm hoping new V4 is similar enough to get it running on ik/llama.cpp like [https://huggingface.co/ubergarm/DeepSeek-V3.2-Speciale-GGUF](https://huggingface.co/ubergarm/DeepSeek-V3.2-Speciale-GGUF) seems to be! (though without new sparse attention support yet). More models indeed!",
              "score": 7,
              "created_utc": "2026-01-09 15:54:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynqbpr",
                  "author": "pigeon57434",
                  "text": "Fuck that! I hope v4 is radically different in every way and totally disrupts the space. This mindset is what's wrong with the AI community. Trading radical innovation for ease is something I would NEVER hope for.",
                  "score": 49,
                  "created_utc": "2026-01-09 20:09:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyn0zoc",
                  "author": "Triple-Tooketh",
                  "text": "Why are you hoping this? *serious question*",
                  "score": 12,
                  "created_utc": "2026-01-09 18:15:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylgghm",
          "author": "SrijSriv211",
          "text": "OpenAI code red 2.0 loading",
          "score": 142,
          "created_utc": "2026-01-09 13:52:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyll62g",
              "author": "5553331117",
              "text": "Sam: â€œ okay guys, generative porn it is, letâ€™s get to training!â€",
              "score": 64,
              "created_utc": "2026-01-09 14:17:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylu3cx",
                  "author": "SrijSriv211",
                  "text": "Grok might get some series competition then.",
                  "score": 25,
                  "created_utc": "2026-01-09 15:01:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylh1aw",
              "author": "Namra_7",
              "text": "Sama will drop 5.3 ðŸ˜­ðŸ«£",
              "score": 43,
              "created_utc": "2026-01-09 13:55:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylhz4n",
                  "author": "Mescallan",
                  "text": "5.2 pro (peak)",
                  "score": 24,
                  "created_utc": "2026-01-09 14:00:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylhnae",
                  "author": "ForsookComparison",
                  "text": "It's 5.2 but uses more experts and thinks twice as long.\n\nWhich is 5.1 but uses more experts and thinks twice as long.",
                  "score": 37,
                  "created_utc": "2026-01-09 13:59:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylktao",
                  "author": "SrijSriv211",
                  "text": "So true ðŸ˜­ðŸ˜‚",
                  "score": 5,
                  "created_utc": "2026-01-09 14:15:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylpfs7",
              "author": "Charuru",
              "text": "You mean 3.0? The original deepseek r1 already code redded and gemini is another code red.",
              "score": 10,
              "created_utc": "2026-01-09 14:38:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyluad4",
                  "author": "SrijSriv211",
                  "text": "I don't think they initiated a code red after R1. I googled and found nothing.",
                  "score": 0,
                  "created_utc": "2026-01-09 15:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyn8zwv",
              "author": "121507090301",
              "text": "If this was trained with only Chinese chips you could say the same about nvidia and the whole usa economy too. lol",
              "score": 9,
              "created_utc": "2026-01-09 18:50:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynduyc",
                  "author": "SrijSriv211",
                  "text": "Yeah that would be pretty insane to watch",
                  "score": 4,
                  "created_utc": "2026-01-09 19:12:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylfzf6",
          "author": "Namra_7",
          "text": "LFG BIG WHALE ðŸ³ðŸ³",
          "score": 42,
          "created_utc": "2026-01-09 13:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylitpd",
          "author": "FullstackSensei",
          "text": "And you'll need to take a second mortgage to buy enough RAM, and sell a kidney or two to buy a couple of GPUs, so you can run it at 2t/s",
          "score": 36,
          "created_utc": "2026-01-09 14:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyln6eb",
              "author": "Hoodfu",
              "text": "Well, I didn't rtfa because paywall, do we know if it'll be more than the 671b size it is now?",
              "score": 10,
              "created_utc": "2026-01-09 14:27:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyltg27",
                  "author": "FullstackSensei",
                  "text": "Didn't read the article either, but does it make a difference? Even if it's 100B, if you don't already have the hardware, you're pretty screwed.",
                  "score": 10,
                  "created_utc": "2026-01-09 14:58:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylgjsg",
          "author": "Zeeplankton",
          "text": "please god don't neuter rp ability",
          "score": 38,
          "created_utc": "2026-01-09 13:53:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz89ckj",
              "author": "NewCryptographer2063",
              "text": "til it's \"roleplay\"",
              "score": 1,
              "created_utc": "2026-01-12 20:45:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nym23cd",
              "author": "NewCryptographer2063",
              "text": "rp?",
              "score": 0,
              "created_utc": "2026-01-09 15:38:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nym43xb",
                  "author": "Special_Coconut5621",
                  "text": "rump play",
                  "score": 18,
                  "created_utc": "2026-01-09 15:47:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyo7cqu",
                  "author": "Joboy97",
                  "text": "Rectum piercing",
                  "score": 8,
                  "created_utc": "2026-01-09 21:28:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyrppdm",
                  "author": "HebelBrudi",
                  "text": "Reverse parking",
                  "score": 2,
                  "created_utc": "2026-01-10 11:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylg8en",
          "author": "LoafyLemon",
          "text": "Flagship model  \nstate of the art  \noutperforms  \ninternal benchamarks\n\n\\*Sigh\\*",
          "score": 54,
          "created_utc": "2026-01-09 13:51:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylnnop",
              "author": "Foreign_Cut745",
              "text": "Unzips?",
              "score": 29,
              "created_utc": "2026-01-09 14:29:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylqxnw",
                  "author": "_yustaguy_",
                  "text": "\\*unzips\\*",
                  "score": 14,
                  "created_utc": "2026-01-09 14:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyqif01",
              "author": "TheRealMasonMac",
              "text": "In my opinion, Deepseek has so far been somewhat mid compared to competitorsâ€”such as K2-Thinking, GLM, and MiniMax. It has very poor understanding of nuance and overthinks on the things that don't need to be overthought while neglecting to think about the things that need to be thought about.",
              "score": 3,
              "created_utc": "2026-01-10 05:03:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nysfidi",
                  "author": "SlowFail2433",
                  "text": "Those competitors are tough competition for Deepseek yes",
                  "score": 1,
                  "created_utc": "2026-01-10 14:20:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyll2h6",
          "author": "cutebluedragongirl",
          "text": "Please be dirt cheap.",
          "score": 12,
          "created_utc": "2026-01-09 14:16:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrpv99",
              "author": "HebelBrudi",
              "text": "You can get 300 daily requests to all open weight models for $3 a month at chutes. But not exactly local.",
              "score": 2,
              "created_utc": "2026-01-10 11:19:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nylxtvy",
          "author": "JumpyAbies",
          "text": "These updates that Deepseek has been releasing, which have already shown great improvement, were just a taste. Look at the time it took to release a major update. They didn't release anything while Grok, Chatgpt, and Claude were fighting. And there were still major releases from competing Chinese developers.\n\nImagine then what's coming from DeepSeek after a long time.",
          "score": 7,
          "created_utc": "2026-01-09 15:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylofz7",
          "author": "Namra_7",
          "text": "New anonymous model is also appeared on lmarena it claims its made by deepseek but not sure it's 100% from deepseek.\n\nhttps://x.com/patelnamra573/status/2008081114909282390?t=yoo_TxGWEPwSt5vEJ-6csQ&s=19",
          "score": 12,
          "created_utc": "2026-01-09 14:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylf2r7",
          "author": "HugoCortell",
          "text": "Oh my god! Finally! A model that can code!\n\nPlease don't post meaningless low-effort reporting, we can make a post once the model actually releases and we can see the model's performance for ourselves.",
          "score": 23,
          "created_utc": "2026-01-09 13:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylk2tm",
          "author": "nullmove",
          "text": "Let me guess, source is \"unnamed person close to the company\"?\n\nIt's a bullshit claim to refute anyway, practically every new model have \"strong\" coding ability on par with frontier on at least one public benchmark.",
          "score": 12,
          "created_utc": "2026-01-09 14:11:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyluhcq",
              "author": "bigzyg33k",
              "text": "If itâ€™s coming from the information, then I trust it.",
              "score": 12,
              "created_utc": "2026-01-09 15:03:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylvg4t",
                  "author": "KaroYadgar",
                  "text": "Why do you trust them? What else did they predict that came true?",
                  "score": 3,
                  "created_utc": "2026-01-09 15:07:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyls4in",
          "author": "insulaTropicalis",
          "text": "Amazing.\n\nV2 was interesting in its architecture with hundreds of small experts, a novelty in 2024. V3 was a game changer, the model that, for me, stepped the local game from super-fun toys to serious business. The bar got raised again and again in 2025 and my current everyday model, GLM-4.7 at 4 bit, is mindblowingly good. If V4 is a meaningful step up in performance, welcome to a great 2026.\n\nMy wishlist is SotA performance, FP8 native, not too much above one gazzillion parameters.",
          "score": 3,
          "created_utc": "2026-01-09 14:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymyntg",
          "author": "Cool-Chemical-5629",
          "text": ">\"Initial tests done by DeepSeek employees based on the company's internal benchmarks showed that it outperformed existing models, such as Anthropic's Claude and OpenAI's GPT series, in coding, the two people said.\"\n\n1. Doesn't say which Claude and GPT models, could be Claude Haiku and GPT Nano and nobody could say they lied.\n\n2. \"In coding\", how about other categories like general knowledge, long context reasoning, science, creative writing, etc.? Is it still lacking in those categories compared to the top Claude and GPT models, despite being over 600B parameters?\n\n3. \"The two people said\"; out of how many employees? Are there some conflicting opinions among the employees regarding the quality of the model?",
          "score": 4,
          "created_utc": "2026-01-09 18:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynqiva",
              "author": "power97992",
              "text": "Im starting to doubt this article, ds said they increase the pretraining beforeâ€¦. Some controllable online learning would be nice",
              "score": 1,
              "created_utc": "2026-01-09 20:10:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nysfls4",
                  "author": "SlowFail2433",
                  "text": "Online learning is a rly important area of research yeah",
                  "score": 1,
                  "created_utc": "2026-01-10 14:20:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyo88re",
          "author": "I_like_fragrances",
          "text": "This is exciting. Can't wait to see it.",
          "score": 2,
          "created_utc": "2026-01-09 21:32:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylgtqh",
          "author": "Main-Lifeguard-6739",
          "text": "deepseek -- telling people it gets something done since version 1.",
          "score": 2,
          "created_utc": "2026-01-09 13:54:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym3m1b",
          "author": "No_Conversation9561",
          "text": "please be less than 400B",
          "score": 2,
          "created_utc": "2026-01-09 15:45:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymg3ij",
              "author": "tarruda",
              "text": "Hoping for less than 200B so I can run at good quantization level on a 128GB Mac",
              "score": 1,
              "created_utc": "2026-01-09 16:41:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyq20m1",
                  "author": "Much-Researcher6135",
                  "text": "can a brotha get a 4B SOTA model over here",
                  "score": 1,
                  "created_utc": "2026-01-10 03:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nym2bm6",
          "author": "NewCryptographer2063",
          "text": "okay",
          "score": 1,
          "created_utc": "2026-01-09 15:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymao0z",
          "author": "MrMrsPotts",
          "text": "Any clues when?",
          "score": 1,
          "created_utc": "2026-01-09 16:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymf9cj",
          "author": "Everlier",
          "text": "Let's hope this one didn't peek into the commit history",
          "score": 1,
          "created_utc": "2026-01-09 16:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyo1xxd",
          "author": "teomore",
          "text": "which claude and which gpt",
          "score": 1,
          "created_utc": "2026-01-09 21:03:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyp4q5g",
          "author": "Banished_Privateer",
          "text": "Is there ever gonna be R2 or is V1 successor to R1?",
          "score": 1,
          "created_utc": "2026-01-10 00:15:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypvbx3",
          "author": "jeffwadsworth",
          "text": "Love it.  I wonder if the chat model is it because damn it can code well.",
          "score": 1,
          "created_utc": "2026-01-10 02:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytag8v",
          "author": "DepressedDrift",
          "text": "When are we getting a multimodal model?",
          "score": 1,
          "created_utc": "2026-01-10 16:57:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu2rt4",
          "author": "Particular-Warthog-5",
          "text": "This is fascinating. We have to wait and see if it undercuts the other players.Â ",
          "score": 1,
          "created_utc": "2026-01-10 19:10:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyyv8xp",
          "author": "Opening_Exit_1153",
          "text": "Is there a chance for a small model from deepseek?",
          "score": 1,
          "created_utc": "2026-01-11 13:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz577wq",
          "author": "R_Duncan",
          "text": "mHC cooking as per paper recently released?",
          "score": 1,
          "created_utc": "2026-01-12 11:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylh4s2",
          "author": "FullOf_Bad_Ideas",
          "text": "Will it outperform Opus 4.5 and GPT 5.2 in coding?\n\nWould be cool.\n\nThe information was putting out many claims against them, I don't think any of them turned out to be true yet. \n\nhttps://www.techinasia.com/news/deepseek-denied-external-funding-called-it-purely-rumors\n\nhttps://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls\n\nhttps://www.theinformation.com/articles/deepseek-using-banned-nvidia-chips-race-build-next-model\n\nhttps://www.theinformation.com/articles/deepseek-opts-huawei-chips-train-models\n\nI don't know The Information, but I think they're supposed to be the solid rumor source, no? This seems more like a targeted propaganda campaign of some sort. Or they're exploiting Deepseek's approach of building things and then quietly shipping innovations, without making a fuss about themselves.\n\nAt this point they might as well be called a fake news/rumors source.",
          "score": -1,
          "created_utc": "2026-01-09 13:56:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylhscr",
              "author": "ForsookComparison",
              "text": "Would be cool.\n\nBut it just has to get close, be served dirt cheap, and be something I could download myself.",
              "score": 9,
              "created_utc": "2026-01-09 13:59:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpkg7s",
                  "author": "gpt872323",
                  "text": "Doubt it. R1 were you able to run? Have to be realisticÂ ",
                  "score": 1,
                  "created_utc": "2026-01-15 11:17:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylm53g",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Leaving Opus and GPT 5.2 in the back mirror would be way cooler than matching their performance in certain ways - which is mostly what we've been seeing for years now. Open models are trailing a bit in a back, rarely a frontier.\n\nIs Deepseek V3.2 not \"close\"? It's certainly served very cheaply.",
                  "score": 1,
                  "created_utc": "2026-01-09 14:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylmoo2",
              "author": "Exciting-Mall192",
              "text": "I think someone mentioned MiniMax M2.1 is a little close to Sonnet 3.7 or something in Claude Code",
              "score": 3,
              "created_utc": "2026-01-09 14:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylvayy",
                  "author": "mintybadgerme",
                  "text": "In my in-expert testing, GLM 4.7 is the closest to a claude SOTA model.",
                  "score": 2,
                  "created_utc": "2026-01-09 15:07:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylq0yn",
              "author": "Charuru",
              "text": "I think the information is alright in the sense that they clearly have some info, the problem is that they try to build a dramatic story around that info and give a misleading impression instead of just saying the facts. If you read the actual article and can tell what's a sourced rumor vs their own speculative bullshit layer on top of it then it's alright. But if you go by the headline and rely on them to tell you how to feel about it you're going to get basically lied to. I don't know why their journalistic standards are so shit but it is what it is.",
              "score": 3,
              "created_utc": "2026-01-09 14:41:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylx5wn",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Sounds like they're optimizing headlines for conversion to paying customers.\n\nAnd paying customers get somewhat clickbaited.",
                  "score": 2,
                  "created_utc": "2026-01-09 15:15:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyluksc",
                  "author": "Toxic469",
                  "text": "Yea they definitely have scoops but the factual reporting / nuance falls short",
                  "score": 1,
                  "created_utc": "2026-01-09 15:03:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyljcwh",
              "author": "kartu3",
              "text": "How does v3 fare?\n\nI recall trying out Deepseek last year when it was released. And then not touching it any more \"for some reason\".",
              "score": 1,
              "created_utc": "2026-01-09 14:08:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylmrtt",
                  "author": "FullOf_Bad_Ideas",
                  "text": "V3 is over a year old at this point.\n\nBut v3.2 is a good model, though I am not using it a lot yet - it's too big to fit locally and I operate in local open > cloud closed pattern the last few weeks.\n\nOn key benchmarks like SWE-Rebench it's the top open weight model. Does well on Creative Writing V3 too. And it's really cheap if you can hit api that does prefix caching.",
                  "score": 4,
                  "created_utc": "2026-01-09 14:25:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nynf278",
                  "author": "Comrade-Porcupine",
                  "text": "3.2 is IMHO about Sonnet 4 level. But also slow and not efficient with tokens. But the tokens are cheap from API providers, so it sorta makes up with it, if you don't mind waiting.\n\nI am nowhere close to having hardware at home that could run it.",
                  "score": 1,
                  "created_utc": "2026-01-09 19:17:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzpkcxo",
              "author": "gpt872323",
              "text": "Outperforming opus 4.5 will be insane. I doubt let's see. Google and openai both couldn't do. Finger crossed.Â ",
              "score": 1,
              "created_utc": "2026-01-15 11:17:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyn9o8y",
          "author": "__Maximum__",
          "text": "Oh man, I expected this soooo much, couldn't shut up about this on reddit. Please be way better than frontier models for the whole world to see these scammy fucks need to die and open source should win.",
          "score": 0,
          "created_utc": "2026-01-09 18:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyq109f",
          "author": "Much-Researcher6135",
          "text": "Wait, but won't Anthropic just be able to reverse engineer it and bake in all the newness, or use it to train their models, or for any open models, literally just incorporate this as as sub-model to Claude and Opus?\n\nThis is one reason the tech feels like a bubble which China is trying to pop with Qwen and Deepseek. As usual, the big players are competing away all the potential profit, ala Uber and Lyft.",
          "score": 0,
          "created_utc": "2026-01-10 03:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylio8v",
          "author": "usernameplshere",
          "text": "I hope they stick with being oss. DS is doing great work, I enjoy 3.2 a lot.",
          "score": 6,
          "created_utc": "2026-01-09 14:04:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz7bmv",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct",
      "author": "jacek2023",
      "created_utc": "2025-12-30 03:34:19",
      "score": 462,
      "num_comments": 78,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwpj0kv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 10:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo5hxn",
          "author": "FizzarolliAI",
          "text": "Hello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 124,
          "created_utc": "2025-12-30 03:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo6q38",
              "author": "jacek2023",
              "text": "great work, new llama release at the end of 2025 :)",
              "score": 51,
              "created_utc": "2025-12-30 03:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwppc12",
                  "author": "MoffKalast",
                  "text": "I definitely did not have this on my bingo card :D\n\nAnd leaked too, keeping up the llama tradition.",
                  "score": 30,
                  "created_utc": "2025-12-30 11:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp95y3",
              "author": "Karyo_Ten",
              "text": "You can do a KL-divergence check to be 100% sure",
              "score": 13,
              "created_utc": "2025-12-30 08:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozt4b",
              "author": "AnOnlineHandle",
              "text": "Heya I'm not up to date with these models since the llama 1 release, do you know if there's a good benchmark for visual tasks such as identifying poses, faces, hands, etc, or answering questions about images, which I could compare models on? I've tried to use Qwen 3 Instruct for it but found it wasn't as good on real data as the demos suggested.",
              "score": 4,
              "created_utc": "2025-12-30 07:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwohwlc",
          "author": "dinerburgeryum",
          "text": "8K max position embeddings? Seems remarkably low; did the fine tune artifact for some reason artificially limit that?",
          "score": 50,
          "created_utc": "2025-12-30 05:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoiq8k",
              "author": "Arli_AI",
              "text": "Maybe we can just set 32768 and itâ€™ll be okay lol",
              "score": 20,
              "created_utc": "2025-12-30 05:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoo96q",
                  "author": "Few-Welcome3297",
                  "text": "Checking differences from LLaMA 3.1 8B Instruct, I think we can add the rope\\_scaling\n\n|\"rope\\_scaling\": {|\n|:-|\n|\"factor\": 8.0,|\n|\"high\\_freq\\_factor\": 4.0,|\n|\"low\\_freq\\_factor\": 1.0,|\n|\"original\\_max\\_position\\_embeddings\": 8192,|\n|\"rope\\_type\": \"llama3\"|\n|},|\n\nand then increase \\`max\\_position\\_embeddings\\`\n\nEdit: Also prev version had 3 eos\\_token\\_id's\n\nEdit2: [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K) model with above changes\n\nEdit3: Link updated",
                  "score": 26,
                  "created_utc": "2025-12-30 05:50:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nworq4h",
                  "author": "Klutzy-Snow8016",
                  "text": "Llama 3 8B had 8192 context. Then Llama 3.1 added RoPE to get to 131072 context. Maybe we can take the RoPE scaling parameters from llama 3.1's config.json and add it to llama 3.3 8B.",
                  "score": 8,
                  "created_utc": "2025-12-30 06:17:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoi4rx",
              "author": "FizzarolliAI",
              "text": "Yes. I'm not entirely sure why, it was limited when served via the website too (I put that in the readme a bit ago)",
              "score": 4,
              "created_utc": "2025-12-30 05:06:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwolsch",
          "author": "Amazing_Athlete_2265",
          "text": "Running this across my private evals to compare against other llamas. Will take a couple hours.",
          "score": 23,
          "created_utc": "2025-12-30 05:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwou0rl",
              "author": "Amazing_Athlete_2265",
              "text": "Initial speed test:\n\n| Model | Backend | PP ts^-1| TG ts^-1 |\n| -------------------------------------------------- | ---------- | ---------- | ------------------ |\n| allura-forge_Llama-3.3-8B-Instruct Q4 | CUDA | 1566.5 | 100.8 |\n| Llama-3.1-8B-Instruct Q4 | CUDA | 351.1 | 111.9 |\n\nSo some difference there.\n\nWill post more eval results as they come to hand.",
              "score": 22,
              "created_utc": "2025-12-30 06:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwp3lxt",
              "author": "Amazing_Athlete_2265",
              "text": "From these results, it looks like the new model is different than the old 3.1.\n\nHere is the performance for knowledge testing, with the new 3.3-8B-Instruct highlighted in the first two plots \n\n- [First plot is the 4-9B parameter group](https://imgur.com/YuSmDRn)\n\n- [Second plot is the same but for 8B+ parameter group](https://imgur.com/Q0nnLwn)\n\n- [Third plot is performance by knowledge category for the 3.3 model](https://imgur.com/kjkNbR3)\n\n- [Fourth plot is performance by knowledge category for the older 3.1 model](https://imgur.com/vjy6cjW)\n\n- [Last plot is a speed chart on my 3080](https://imgur.com/coiBc9H)\n\nTesting the Q6 versions now. Will take a while. All of the tests above are for Q4.",
              "score": 20,
              "created_utc": "2025-12-30 08:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpij0j",
                  "author": "keepthepace",
                  "text": "(Thanks for doing this!) \n\nI guess this explains why they did not brag much about it. Many other models of that category outperform them.\n\nI always wondered if Zuckerberg was not the only honest player in the field when he was explaining that the only reason they go for open source is that it will save them money. With decent open models out there they have less incentives to do so.",
                  "score": 11,
                  "created_utc": "2025-12-30 10:20:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpmayy",
                  "author": "MLDataScientist",
                  "text": "Thanks for the tests. Question not related to llama: is LFM2 8BA1B that good in world knowledge (or coding/stem field)? I see it reaches Qwen3 30B-A3B.",
                  "score": 3,
                  "created_utc": "2025-12-30 10:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp43t0",
                  "author": "jacek2023",
                  "text": "You can post pictures in the comments here",
                  "score": 2,
                  "created_utc": "2025-12-30 08:06:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwp9v8w",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Random question: any idea why nemotron 30B A3B got 0% in the second plot?",
                  "score": 2,
                  "created_utc": "2025-12-30 09:00:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoo8ce",
              "author": "jacek2023",
              "text": "do you have results for other new models?",
              "score": 3,
              "created_utc": "2025-12-30 05:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwopj5o",
                  "author": "Amazing_Athlete_2265",
                  "text": "I have some. I focus mostly on smaller models <12B or Moe. What you want?",
                  "score": 7,
                  "created_utc": "2025-12-30 06:00:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpff8o",
          "author": "a_beautiful_rhind",
          "text": "This is like the kiss goodbye from meta.",
          "score": 18,
          "created_utc": "2025-12-30 09:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpzjvy",
              "author": "samplebitch",
              "text": "It's like that time when you hook up with your ex one last time, and it wasn't even that great.",
              "score": 24,
              "created_utc": "2025-12-30 12:43:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvktxy",
                  "author": "impolitemrtaz",
                  "text": "You samplebitch you",
                  "score": 2,
                  "created_utc": "2025-12-31 07:05:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwzk8xs",
                  "author": "Electronic-Metal2391",
                  "text": "You bring bad memories",
                  "score": 1,
                  "created_utc": "2025-12-31 22:13:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwognr9",
          "author": "random-tomato",
          "text": "Holy shit that is awesome, hats off to you for finding the weights!",
          "score": 33,
          "created_utc": "2025-12-30 04:56:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv4x7d",
              "author": "seppe0815",
              "text": "stupid bots",
              "score": -7,
              "created_utc": "2025-12-31 05:01:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvbbuw",
                  "author": "random-tomato",
                  "text": "If I'm a bot, I'm certainly programmed to like and appreciate when people find something cool and share with the rest of us. What's your purpose being a professional asshole?\n\nAnd no, I am not a bot\n\nhttps://preview.redd.it/eljpgxgfbhag1.png?width=765&format=png&auto=webp&s=90f0de59d2389d809ac21d988ca59883283ffccf",
                  "score": 4,
                  "created_utc": "2025-12-31 05:48:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwplkfs",
          "author": "jacek2023",
          "text": "about 4h after the release u/TheLocalDrummer published first finetune:\n\n[https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main](https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main)",
          "score": 16,
          "created_utc": "2025-12-30 10:48:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwplnk1",
              "author": "TheLocalDrummer",
              "text": "It's a test model but I think it turned out well! Looking for feedback in (my) Discord",
              "score": 15,
              "created_utc": "2025-12-30 10:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrnu7f",
                  "author": "DevelopmentBorn3978",
                  "text": "what the finetune you've made is about?",
                  "score": 3,
                  "created_utc": "2025-12-30 17:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwswk38",
                  "author": "LegacyRemaster",
                  "text": "legend",
                  "score": 2,
                  "created_utc": "2025-12-30 21:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpr0ce",
              "author": "MoffKalast",
              "text": "People are asking what's the use case for llama, and well uh... there it is ;)",
              "score": 7,
              "created_utc": "2025-12-30 11:36:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws73yi",
                  "author": "Emotional-Baker-490",
                  "text": "qwen 3",
                  "score": 2,
                  "created_utc": "2025-12-30 19:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpqsiz",
          "author": "jacek2023",
          "text": "[https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic](https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic)",
          "score": 10,
          "created_utc": "2025-12-30 11:34:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwpstxk",
              "author": "Amazing_Athlete_2265",
              "text": "Everyone's cooking tonight!",
              "score": 7,
              "created_utc": "2025-12-30 11:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptlto",
                  "author": "jacek2023",
                  "text": "actually it's a middle of the day in Europe :)",
                  "score": 8,
                  "created_utc": "2025-12-30 11:57:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwodozb",
          "author": "Echo9Zulu-",
          "text": "Cloned",
          "score": 7,
          "created_utc": "2025-12-30 04:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9jjl",
          "author": "Infninfn",
          "text": "Iâ€™m out of the loop - is this just what they had or did Meta not shutdown Llama?",
          "score": 18,
          "created_utc": "2025-12-30 04:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa6pk",
              "author": "FizzarolliAI",
              "text": "This has existed at least since April during Llamacon (did anyone remember they did a Llamacon?)\n\nhttps://ai.meta.com/blog/llamacon-llama-news/\n\n> As part of this release, weâ€™re sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. Weâ€™re sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model.",
              "score": 30,
              "created_utc": "2025-12-30 04:14:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoeicf",
              "author": "jacek2023",
              "text": "we do things for fun in this community, just accept the gift ;)",
              "score": 7,
              "created_utc": "2025-12-30 04:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0u205",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 5,
          "created_utc": "2026-01-01 02:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0u9ri",
              "author": "jacek2023",
              "text": "Nice work!!!",
              "score": 1,
              "created_utc": "2026-01-01 03:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwodvfx",
          "author": "Cool-Chemical-5629",
          "text": "I guess Christmas came late for me, but hey if this is the real thing from Meta, I guess it's nice to have something newer than 3.1 8B without needing expensive hardware for models like Llama 4.",
          "score": 7,
          "created_utc": "2025-12-30 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp5cbv",
              "author": "Emotional-Baker-490",
              "text": "qwen 3",
              "score": 12,
              "created_utc": "2025-12-30 08:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtj858",
          "author": "LegacyRemaster",
          "text": "    allura-forge_llama-3.3-8b-instruct\n\nMy training data is current up to December 2022. This means that I have been trained on a vast amount of text data available until that date, but I do not have information or knowledge about events or developments that have occurred after that date.\n\nIn other words, my training data \"cutoff\" is December 2022, and I should not be relied upon for information or insights related to dates after that.\n\n145.25 tok/sec",
          "score": 3,
          "created_utc": "2025-12-30 23:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwroudn",
          "author": "DevelopmentBorn3978",
          "text": "which quantized and eventually finetuned gguf models have the context lenght been enlarged? bartowsky? shb777? beaverai/anubis?",
          "score": 1,
          "created_utc": "2025-12-30 18:01:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwt7fp",
              "author": "Few-Welcome3297",
              "text": "Try [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-128K-GGUF)",
              "score": 0,
              "created_utc": "2025-12-31 13:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsx4sf",
          "author": "gta721",
          "text": "How dumb are they to push a portal THAT broken to prod?",
          "score": 1,
          "created_utc": "2025-12-30 21:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvb2oz",
              "author": "greggh",
              "text": "Nothing about it is prod. Itâ€™s still so janky that its free if your in the trial.",
              "score": 3,
              "created_utc": "2025-12-31 05:46:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvhco0",
                  "author": "FizzarolliAI",
                  "text": "Yep, this basically. Afaik the main inference API is still waitlisted, *and* there's a separate waitlist to submit for the finetuning API.",
                  "score": 2,
                  "created_utc": "2025-12-31 06:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nww109f",
          "author": "FX2021",
          "text": "Is it a new core? Or is it just a serving variant",
          "score": 1,
          "created_utc": "2025-12-31 09:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9zlt",
          "author": "Intelligent-Form6624",
          "text": "â€œ(I think, anyways)â€",
          "score": -20,
          "created_utc": "2025-12-30 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoadni",
              "author": "FizzarolliAI",
              "text": "LISTEN whenever i drop *my own* models i get anxiety attacks about accidentally reuploading the base model ;-; i believe that this is actually L3.3 at this point though, see my other comment",
              "score": 26,
              "created_utc": "2025-12-30 04:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwow842",
                  "author": "Intelligent-Form6624",
                  "text": "What? Sorry, I canâ€™t hear you",
                  "score": -21,
                  "created_utc": "2025-12-30 06:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9irb",
          "author": "secopsml",
          "text": "Drop behemoth instead. Looks fakeÂ ",
          "score": -36,
          "created_utc": "2025-12-30 04:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo9lrg",
              "author": "secopsml",
              "text": "ðŸ˜œ",
              "score": -25,
              "created_utc": "2025-12-30 04:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6n5vl",
      "title": "16x AMD MI50 32GB at 10 t/s (tg) & 2k t/s (pp) with Deepseek v3.2 (vllm-gfx906)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/lor8ccu2xybg1.png",
      "author": "ai-infos",
      "created_utc": "2026-01-07 18:22:05",
      "score": 456,
      "num_comments": 237,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyb7g9w",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 01:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8yjc2",
          "author": "fallingdowndizzyvr",
          "text": "> Power draw: 550W (idle) / 2400W (peak inference)\n\nSweet. It's winter. Might as well have your heater do work instead of making empty BTUs.",
          "score": 56,
          "created_utc": "2026-01-07 18:49:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny98fa4",
              "author": "ForsookComparison",
              "text": "I have a 400w space heater for winter that does the job on my office desk.\n\n550w idle is mind boggling",
              "score": 20,
              "created_utc": "2026-01-07 19:32:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9h2by",
                  "author": "Medium_Chemist_4032",
                  "text": "Isn't 2 kW actually a kettle territory?",
                  "score": 14,
                  "created_utc": "2026-01-07 20:10:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8uc81",
          "author": "Soft_Possible1862",
          "text": "Holy shitâ€¦.",
          "score": 22,
          "created_utc": "2026-01-07 18:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8sxj3",
          "author": "kevin_1994",
          "text": "How loud is it? How are you able to run 2400W from home?",
          "score": 33,
          "created_utc": "2026-01-07 18:25:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8x8d5",
              "author": "Clank75",
              "text": "2400W Is only 10A @ 230v.Â  A typical domestic radial in Europe is 16A, so anything up to 3500W is just \"plug it into the nearest outlet.\"\n\n\nMore than that though and you need a specialist connection (e.g. I have a 20A/4500W circuit in the kitchen for the oven+hob.)",
              "score": 12,
              "created_utc": "2026-01-07 18:43:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyb7wbb",
                  "author": "Irisi11111",
                  "text": "This is a good time to use electricity when it's not so expensive. Some countries in Europe have a lot of wind and solar power and the price of utility electricity changes depending on the weather. It's likely cheaper in the summer and fall. Also, if you get solar panels and batteries for your house, it could be really great for your computer setup for a long time.",
                  "score": 1,
                  "created_utc": "2026-01-08 01:02:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny8yhe1",
                  "author": "AnomalyNexus",
                  "text": "Earth leakage tends to trip long before you hit theoretical socket max in my experience",
                  "score": -7,
                  "created_utc": "2026-01-07 18:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny8vydf",
              "author": "FullstackSensei",
              "text": "From the picture, it seems OP is running those dual 40mm fan shrouds per GPU. So, I'd say somewhere between very and unbearably loud.\n\nOn a side rant, I'm always amused by people in the US with their 1800W outlets. Meanwhile, the rest of the world has between 2800-3600W available per outlet, because 230V...\n\nEdit: I know it was my rant that started all this, but can we please stop it. One of the things I enjoy the most in this community is how devoid it is of politics. I apologize if my comment sounded snubby or derogatory. It was never my intention.",
              "score": 78,
              "created_utc": "2026-01-07 18:38:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny90h5f",
                  "author": "kevin_1994",
                  "text": "\\*cries in canadian*",
                  "score": 17,
                  "created_utc": "2026-01-07 18:57:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny92zm6",
                  "author": "AmpEater",
                  "text": "Every us home has 240v available, and every PC PS is able to run 240v.\n\nRunning a 240v outlet isnâ€™t hardÂ ",
                  "score": 13,
                  "created_utc": "2026-01-07 19:08:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyana5n",
                  "author": "Mac_NCheez_TW",
                  "text": "What are you talking about 100% of American Home have 220v outlets in their home. They run 30amps at 220v.Â ",
                  "score": 1,
                  "created_utc": "2026-01-07 23:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny8v0ys",
              "author": "ai-infos",
              "text": "Depends on the fan speed, but even at 40%, it's quite noisy and needs a separate closed room... (in my opinion)  \n  \nAnd It also depends where you live but in Europe, reaching 2400W from home is possible without much trouble. You have to plug each 1600w PSU to separate electrical circuit (some tests with the breaker must be done before to draw an electrical plan of your home and check the maximum amperage/voltage allowed per circuit / power strip)",
              "score": 16,
              "created_utc": "2026-01-07 18:34:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny8zwty",
                  "author": "Medium_Chemist_4032",
                  "text": "Hats off for being brave enough to actually try that in Europe. Energy prices here kill a lot of tech hardware enthusiast's projects and a lot pivot to low energy optimizing to make it wife approved",
                  "score": 9,
                  "created_utc": "2026-01-07 18:55:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny96v4y",
                  "author": "debackerl",
                  "text": "Yes, in Belgium we have 20A 240V for circuits dedicated to plugs, most of Europe would be at least 16A per circuit.",
                  "score": 2,
                  "created_utc": "2026-01-07 19:25:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny980ys",
                  "author": "bigh-aus",
                  "text": "Meanwhile the AI rigs in the USA are going to start popping up in the laundry (to utilize the dryer plug)",
                  "score": 1,
                  "created_utc": "2026-01-07 19:30:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyb0vo0",
              "author": "Irisi11111",
              "text": "If OP owns a farm and has a lot of solar panels, that's totally fine. It's a good way to handle extra electricity. But he/she does need to be cautious about fire risks.",
              "score": 2,
              "created_utc": "2026-01-08 00:27:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny9wm0q",
              "author": "abnormal_human",
              "text": "Loud as fuck and a trip to the hardware store I'm guessing.",
              "score": 1,
              "created_utc": "2026-01-07 21:17:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9q1kn",
          "author": "SourceCodeplz",
          "text": "Tbh if you are coding professionally, this really isn't that much of a spend for having a basically offline programmer working with you on just electricity.",
          "score": 13,
          "created_utc": "2026-01-07 20:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybnlo5",
              "author": "LA_rent_Aficionado",
              "text": "Youâ€™d still get better quality, latency and arguably more through put for the same price as power if you get a $100/$200 Claude subscription.  Just at the expense of privacy and coolness factor.",
              "score": 4,
              "created_utc": "2026-01-08 02:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyc9lg2",
                  "author": "autodidacticasaurus",
                  "text": "> Just at the expense of privacy and coolness factor.\n\nNo, it's not just that. It's also control and security. They can take it away from you any time they like. They can make it worse anytime they like. They can start inserting propaganda and ads any time they want... raise your prices... or even go bankrupt entirely. Even just changing business models would be stressful. Let's not forget about outages either. There have been huge ones lately that have half the Internet down (Cloudflare, Amazon, etc.).\n\nIt's insane to outsources to services like this. Absolutely fucking insane.",
                  "score": 20,
                  "created_utc": "2026-01-08 04:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8yh6u",
          "author": "OnlineParacosm",
          "text": "Can you give us a rough all in cost so I can figure what the tokens per second cost basis is for this? Thanks for such a great write up.",
          "score": 7,
          "created_utc": "2026-01-07 18:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny92eov",
              "author": "ai-infos",
              "text": "\\~5.6kâ‚¬ (with \\~150â‚¬ / MI50, not the case anymore)",
              "score": 12,
              "created_utc": "2026-01-07 19:06:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny98dhc",
                  "author": "TheSpicyBoi123",
                  "text": "Damn, 150 euro for a 32gb mi50 is a bargain!",
                  "score": 6,
                  "created_utc": "2026-01-07 19:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9pr28",
          "author": "ThatCrankyGuy",
          "text": "Thousands of dollars of equipment hang by garden twist-tie wire. Reminds me of grad days.",
          "score": 6,
          "created_utc": "2026-01-07 20:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8vpf6",
          "author": "Dorkits",
          "text": "Me with my rumble 3060ti : Dream build ðŸ˜²",
          "score": 10,
          "created_utc": "2026-01-07 18:37:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8zw75",
          "author": "FullstackSensei",
          "text": "Is there a meaningful difference between something like DS AWQ and something like Minimax 2.1 at Q8?",
          "score": 4,
          "created_utc": "2026-01-07 18:55:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny941co",
              "author": "ai-infos",
              "text": "i didn't try but i don't expect meaningful difference (or if we compare with glm 4.7 on 8 mi50). To be honest, right now, i won't use it a lot due to higher electrical cost, except for deepseek 3.2 speciale.   \n  \nBut later, i expect much more intelligent models around \\~1T param (\\~300-500GB if quantized), so at least, i know that it's possible to run it with old hardware (if the LLM architecture does not change too much from now)",
              "score": 4,
              "created_utc": "2026-01-07 19:13:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny95fit",
                  "author": "FullstackSensei",
                  "text": "You should get considerably faster TG speed on smaller models, which will reduce your average power consumption.\n\nI'm much more optimistic about \\~100B models with larger contexts, and TBH, at least 80% of coding tasks can already be aptly handled by models like Qwen 3 coder 30B which runs very fast on 2 Mi50s or two 3090s. I only fire my 192GB VRAM rigs when I need 200+B models.",
                  "score": 5,
                  "created_utc": "2026-01-07 19:19:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny986ve",
          "author": "ForsookComparison",
          "text": "How is your prompt processing so good? Does Tensor parallelism come into play?",
          "score": 4,
          "created_utc": "2026-01-07 19:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9h7ne",
              "author": "ai-infos",
              "text": "Yes ~~and thanks to the 1TB/s bandwidth per MI50.~~  \n~~The 16 MI50 with a total of 16TB/s are running at \\~100% during prompt processing (PP). In vllm, the PP speed (tok/s) is actually variable according to the tokens input. On another build with 8 MI50 and glm 4.6, i reached \\~10k tok/s (for a prompt of \\~80k tokens in roo code).~~  \n  \nEDIT: as said below, bandwidth doesn't really matter in prompt processing, it's more compute bound. But Tensor parallelism comes indeed into play (and that's the huge difference between softwares like vllm and sglang supporting true tensor parallelism and llama.cpp which doesn't for now)",
              "score": 9,
              "created_utc": "2026-01-07 20:10:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny9mbm3",
                  "author": "StardockEngineer",
                  "text": "Bandwidth doesn't really matter in prompt processing.",
                  "score": 9,
                  "created_utc": "2026-01-07 20:33:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybo6v7",
          "author": "vulcan4d",
          "text": "Ditch the furnace, best home heater!",
          "score": 5,
          "created_utc": "2026-01-08 02:28:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ewd9",
          "author": "organicmanipulation",
          "text": "Nice setup! I notice that you're splitting your PCIe lanes into two 8x. Can you please share the exact PCIe bifurcation card you're using?",
          "score": 3,
          "created_utc": "2026-01-07 20:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9iqnu",
              "author": "ai-infos",
              "text": "In the github, i wrote:   \n  \n\"12x SlimSAS PCIe device adapters 2x 8i\n\n4x SlimSAS PCie device 1x 4i (C-payne)\n\n12x SlimSAS cables 8i\n\n2x SlimSAS cable 8i to 2x4i\n\n7x SlimSAS PCIe host adapter\"  \n  \nSo it's x8x8 for pcie 2 to pcie 7 and x4x4x4x4 for pcie1 for example.  \n  \nYou can find these adapters / cables on most famous online website (amazon, alibaba, ebay, etc), except for the \"SlimSAS PCie device 1x 4i C-payne\". I didn't find anything similar from other sellers than C-payne. If you've got any other sellers for this device, please share it.",
              "score": 4,
              "created_utc": "2026-01-07 20:17:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybrybs",
                  "author": "ShreddinPB",
                  "text": "Seriously, I have spent the last 4 hours scouring, cant find anything at all",
                  "score": 1,
                  "created_utc": "2026-01-08 02:48:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybtyh9",
                  "author": "ShreddinPB",
                  "text": "The only ones I see in the USA on Amazon have a 24pin ATX connector :( I assume these are so each gpu has its own dedicated PSU?    \n[https://a.co/d/gqnwaOv](https://a.co/d/gqnwaOv)    \nI assume these cant be used :(",
                  "score": 0,
                  "created_utc": "2026-01-08 02:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nya9bqr",
          "author": "cashmillionair",
          "text": "Thank you for sharing, appreciate it!",
          "score": 3,
          "created_utc": "2026-01-07 22:12:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb7t5l",
          "author": "noiserr",
          "text": "That's nuts! And I love it.",
          "score": 3,
          "created_utc": "2026-01-08 01:02:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybm752",
          "author": "MaximKiselev",
          "text": "10 t/s Carl....",
          "score": 3,
          "created_utc": "2026-01-08 02:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybvmx1",
          "author": "ryfromoz",
          "text": "I love massive frankenstein rigs like this!",
          "score": 3,
          "created_utc": "2026-01-08 03:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydbb4o",
          "author": "PreparationLow6188",
          "text": "A Wow for this supreme project. It is the time should consider reenable MI50 on the shelf.",
          "score": 3,
          "created_utc": "2026-01-08 09:25:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8yc7v",
          "author": "egomarker",
          "text": "10 tks? Sigh",
          "score": 8,
          "created_utc": "2026-01-07 18:48:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycy1xs",
              "author": "AttitudeImportant585",
              "text": "the only task that justifies the effort to build this would be for agentic coding and 10tps is not up for that lol",
              "score": 3,
              "created_utc": "2026-01-08 07:26:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyh3vgq",
                  "author": "crantob",
                  "text": "*the only task I can imagine",
                  "score": 1,
                  "created_utc": "2026-01-08 21:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny943r0",
          "author": "exaknight21",
          "text": "Can you share your build please. Like what are those fans, how did you hook them up, what motherboard youâ€™re using. I am a little new to this aspect.",
          "score": 2,
          "created_utc": "2026-01-07 19:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny96k56",
              "author": "ai-infos",
              "text": "Motherboard is  ASRock Rack ROMED8-2T  \nIt's classic 10 artic fans 140mm but these ones are optional to me in this open setup.  \nThe most important ones are the ones behind the gpu. If you buy the mi50 from alibaba, ask the seller to include some fans, it would be 2 small fans 50mm or 1 bigger fan per gpu (and these chinese fans are very very noisy but do the work to keep temps under 65Â°C)\n\nYou also need some PWM HUB FAN Artic Sata and you have to respect the maximum number of amperage/wattage per slot (and in total per hub) if you don't want to burn your devices.",
              "score": 3,
              "created_utc": "2026-01-07 19:24:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyaodfv",
              "author": "Mac_NCheez_TW",
              "text": "I 3D printed in my fan shrouds for my Mi50s. I put 120mm fans on the back side with a nice airflow curvatures in the print. you can also get Mi50s fan shrouds 3D printed on Ebay and amazonÂ ",
              "score": 1,
              "created_utc": "2026-01-07 23:23:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny9zj2e",
          "author": "Different-Toe-955",
          "text": "Epic setup I love the zip ties holding the GPUs up. How do those oculink extenders work out? Looks like each GPU is running at x8.",
          "score": 2,
          "created_utc": "2026-01-07 21:29:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyab89v",
              "author": "ai-infos",
              "text": "there are 12 gpu at x8 and 4 gpus at x4  \nit's not oculink extenders but rather SlimSAS extenders (**SFF-8654 8i or 4i)**",
              "score": 2,
              "created_utc": "2026-01-07 22:21:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nydfak3",
                  "author": "danishkirel",
                  "text": "What do only the extenders cost?",
                  "score": 1,
                  "created_utc": "2026-01-08 10:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyakif3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-07 23:04:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycfbgw",
              "author": "halcyonhal",
              "text": "Did you read the post and look at the GitHub link?",
              "score": 1,
              "created_utc": "2026-01-08 05:04:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nycaq4e",
          "author": "badgerbadgerbadgerWI",
          "text": "This is the kind of setup that makes enterprise local deployment actually viable. MI50s at those prices vs NVIDIA is a completely different ROI calculation. Are you seeing any stability issues with vllm on the older gfx906 arch over longer inference runs?",
          "score": 2,
          "created_utc": "2026-01-08 04:34:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydu26z",
              "author": "ai-infos",
              "text": "Yes, after filling the context with 6k tokens successfully, I tried injecting a prompt of 17k token for code review but the MTP module crashes (I talked about it there: [https://github.com/nlzy/vllm-gfx906/pull/62](https://github.com/nlzy/vllm-gfx906/pull/62) )   \n  \nSo there's still work to do for speed/memory/stability improvements and hope the community will also contribute to it (as i'm not an expert but will continue to try to improve it step by step in my free time)",
              "score": 1,
              "created_utc": "2026-01-08 12:03:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyfkhpp",
          "author": "Environmental-Metal9",
          "text": "Youâ€™ve unlocked the â€œMadLadâ€ badge",
          "score": 2,
          "created_utc": "2026-01-08 17:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygckst",
          "author": "qcodec",
          "text": "500W/2400W. Oh my, even my solar system can't handle this. I guess I'll have to install a standalone one.",
          "score": 2,
          "created_utc": "2026-01-08 19:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyieqz7",
          "author": "el3mancee",
          "text": "Nice setup.",
          "score": 2,
          "created_utc": "2026-01-09 01:09:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyihpr7",
          "author": "el3mancee",
          "text": "https://preview.redd.it/vr6jf0i178cg1.jpeg?width=4284&format=pjpg&auto=webp&s=2e4036b31d87b2e9d7d75dcdbebdae2f7e07cecf\n\nMy setup can run Deepseek 3.1 IQ4\\_XS at 7.5 t/s. 200W total when running.",
          "score": 2,
          "created_utc": "2026-01-09 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyii5zg",
              "author": "el3mancee",
              "text": "https://preview.redd.it/qwm2qlxz78cg1.jpeg?width=4284&format=pjpg&auto=webp&s=17f745c675528ece9eb128c1317acfe876e14eb8",
              "score": 1,
              "created_utc": "2026-01-09 01:27:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8ycbp",
          "author": "Far-Low-4705",
          "text": "pretty sure a mac would it run faster, and at far less power consumption.\n\nStill super cool, but not sure how practical it would be",
          "score": 3,
          "created_utc": "2026-01-07 18:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8z64f",
              "author": "ai-infos",
              "text": "true for the much much less power consumption, true also for faster token generation speed   \nbut not prompt processing speed... you don't have 16 TB/s bandwidth on a single Mac node...",
              "score": 11,
              "created_utc": "2026-01-07 18:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybn4bx",
                  "author": "Far-Low-4705",
                  "text": "fair point. is pp really that important though? especially with prompt caching? you still have to wait for generation to complete.",
                  "score": 1,
                  "created_utc": "2026-01-08 02:22:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny92scu",
              "author": "cantgetthistowork",
              "text": "The Mac will take years for prompt ingestion. Power consumption much lower though",
              "score": 6,
              "created_utc": "2026-01-07 19:07:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny98ub4",
              "author": "ForsookComparison",
              "text": "16x 32GB Mi50x's at, let's say $250 each, is $4K. Add $2K for the skeleton, PSUs, etc without skimping, and I'm guessing this is recreatable at $6K. The Mac that would likely beat this starts at $10K iirc\n\nAlso this is way cooler.",
              "score": 1,
              "created_utc": "2026-01-07 19:34:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9ly7m",
                  "author": "StardockEngineer",
                  "text": "There is no Mac that beats this if the purpose is coding.",
                  "score": 6,
                  "created_utc": "2026-01-07 20:31:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybmsii",
                  "author": "Far-Low-4705",
                  "text": "power consumption? a single mi50 consumes more power than the mac does.\n\nI agree it is way cooler, but something tells me it's just not practical",
                  "score": 2,
                  "created_utc": "2026-01-08 02:21:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9n2d3",
          "author": "Lissanro",
          "text": "Prompt processing is very good, compared to what I get with ik\\_llama.cpp with four 3090 GPUs (around 150 tokens/s vs 2K tokens/s in your rig). But generation of 10 tokens/s sound very slow, given I get 8 tokens/s with most of the model's weights in DDR4 RAM (using IQ4 quant), and only cache with common expert tensors in VRAM.\n\nI suspect that something is inefficient in the inference code or maybe something not configured correctly, I think generation speed should be much faster... unless on MI50 you are getting compute bound or something.",
          "score": 2,
          "created_utc": "2026-01-07 20:36:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9xxpl",
              "author": "ai-infos",
              "text": "I was actually hesitating to build a setup looking to yours but I wanted better prompt processing speed for coding/debugging use cases.   \n  \nI explained the technical limitations in the \"side notes / future work\" of this PR: [https://github.com/nlzy/vllm-gfx906/pull/62](https://github.com/nlzy/vllm-gfx906/pull/62)   \nI'm not skilled in triton / c++ / hip kernels so most of the functions are pure pytorch not really optimized with some vibe coded parts. So yes, it's currently quite inefficient.   \n  \nThat's frustrating having an AMD MI50 gpu, it's like having a bike but being forced to walk alongside it",
              "score": 1,
              "created_utc": "2026-01-07 21:22:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8wpum",
          "author": "d13056",
          "text": "What is the best model for control my pc?",
          "score": 1,
          "created_utc": "2026-01-07 18:41:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny90hew",
          "author": "re_e1",
          "text": "Yoo my dream setup",
          "score": 1,
          "created_utc": "2026-01-07 18:57:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny928um",
          "author": "PhotographerUSA",
          "text": "Would it be better to run an optimized module where you can get 200 or more tx/sec",
          "score": 1,
          "created_utc": "2026-01-07 19:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9kx61",
          "author": "starkruzr",
          "text": "budget?",
          "score": 1,
          "created_utc": "2026-01-07 20:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9suin",
          "author": "d4nger_n00dle",
          "text": "https://preview.redd.it/faceagfjrzbg1.jpeg?width=400&format=pjpg&auto=webp&s=bb386b84882cbea8de7b3d5931cda0024a04cda8",
          "score": 1,
          "created_utc": "2026-01-07 21:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nya01s5",
          "author": "krusic22",
          "text": "~~How are you only pulling 2400W? That's just 75W per card.~~",
          "score": 1,
          "created_utc": "2026-01-07 21:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya9ln5",
              "author": "droptableadventures",
              "text": "Not the OP but:\n2400/16 is actually 150w per card, not 75. You can power limit to about 150w with not that much performance loss.",
              "score": 3,
              "created_utc": "2026-01-07 22:13:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyab6b0",
                  "author": "krusic22",
                  "text": "Ah, I misread. Thanks.",
                  "score": 2,
                  "created_utc": "2026-01-07 22:20:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyac46d",
              "author": "ai-infos",
              "text": "2400W / 16 is 150W per card   \nThey almost never go up to the max of 300W (and you can cap the max wattage by updating VBIOS or UPP or rocm-smi command with very low speed loss; more details there: [https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13](https://gist.github.com/evilJazz/14a4c82a67f2c52a6bb5f9cea02f5e13) )",
              "score": 3,
              "created_utc": "2026-01-07 22:25:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybvq0r",
          "author": "Tiny_Judge_2119",
          "text": "oh my god, peak power usage is higher than my air con...",
          "score": 1,
          "created_utc": "2026-01-08 03:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycptlp",
          "author": "MyBrainsShit",
          "text": "Does it make sense to mix and match? Like if I had a 5090 and a few mi50 can I spread models across or is that impossible/very cumbersome? Is anyone doing that?",
          "score": 1,
          "created_utc": "2026-01-08 06:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyds8mm",
              "author": "airpray",
              "text": "Look at https://llm-d.ai/",
              "score": 2,
              "created_utc": "2026-01-08 11:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyellg0",
                  "author": "MyBrainsShit",
                  "text": "Nice, thank you!",
                  "score": 1,
                  "created_utc": "2026-01-08 14:42:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nydtdxd",
              "author": "ai-infos",
              "text": "I tried this months ago with a 3090 and llama.cpp (and with or without RPC) but it was not really worth (+1/5% in speed if i remember correctly, but maybe I missed something in the settings)",
              "score": 2,
              "created_utc": "2026-01-08 11:58:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nytjwxq",
              "author": "sloptimizer",
              "text": "Yes you can! It's possible to compile llama.cpp with both ROCm and CUDA support at the same time with DGGML\\_BACKEND\\_DL=ON.\n\nThen you can do the following:\n\n* Put attention on a fast GPU, like 5090\n* Put experts on slow/cheap GPUs, like MI50\n\nSee an example offload [here](https://huggingface.co/anikifoss/MiniMax-M2.1-HQ4_K-GGUF#llamacpp---experts-offload-to-4x32g-amd-gpus-mi50-or-r9700) with MiniMax-M2.1 running on 5090 + 4xMI50",
              "score": 2,
              "created_utc": "2026-01-10 17:42:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nydbi8n",
          "author": "Neptun78",
          "text": "Can you test Kimi K2 (quant that fits memory)? :)",
          "score": 1,
          "created_utc": "2026-01-08 09:27:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydsh1r",
              "author": "ai-infos",
              "text": "the original int4 is \\~600GB so it won't fit the 512GB of VRAM  \nand the GGUF with llama.cpp would have a very slow prompt processing speed (\\~200 tok/s i think, not really usable for coding agent)",
              "score": 2,
              "created_utc": "2026-01-08 11:52:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nydxjmn",
                  "author": "Neptun78",
                  "text": "Ok, youâ€™re right.\nHave you tried dense model like gemma3-27B?",
                  "score": 1,
                  "created_utc": "2026-01-08 12:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nydjws6",
          "author": "Ok-Internal9317",
          "text": "10 tok/s? Do you even use this thing you built?",
          "score": 1,
          "created_utc": "2026-01-08 10:43:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydt0py",
              "author": "ai-infos",
              "text": "10 tok/s in TG with thousands of tok/s for prompt processing speed is enough to me for coding agents use cases  \nBut right now, i'm still working on it to make it work under Roo Code (there's a problem with the tokenizer to fix). After that, I will compare the output with GLM 4.7 and I expect that having 2 GLM 4.7 on 16 MI50 would be a better choice than having 1 Deepseek  v3.2",
              "score": 1,
              "created_utc": "2026-01-08 11:56:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyehudh",
          "author": "Electronic_Status_60",
          "text": "Bro at that rate just pay for the tokens, he's got half a microwave on at all times",
          "score": 1,
          "created_utc": "2026-01-08 14:23:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf2dp4",
          "author": "Ulterior-Motive_",
          "text": "Brother has a whole datacenter in his basement",
          "score": 1,
          "created_utc": "2026-01-08 16:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8ynlj",
          "author": "Legal-Ad-3901",
          "text": "I'm at 16 mi50s right now but your plans for 32 have me thinking ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤”ðŸ¤” fwiw, 3xl u sloth thinking k2 is around 455gb if you're feeling froggy before getting the next setup. Would also be curious what your llama speeds would be as I fair better on that vs vllm",
          "score": 1,
          "created_utc": "2026-01-07 18:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny90jed",
              "author": "ai-infos",
              "text": "32 won't be too interesting...it would be a massive burn of energy for a model not too smart in comparison with its competitor like glm 4.7 running on only 8 mi50 (same actually applies for deepseek v3.2 with 16 gpu...)  \nAnd also 32 mi50 would require 2 nodes, running vllm + ray backend (which i already tested in the past with 2\\*8 mi50 and it was not really steady for amd gpu with pipeline parallelism)",
              "score": 5,
              "created_utc": "2026-01-07 18:58:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny96txm",
                  "author": "BevinMaster",
                  "text": "I mentioned on discord I think but maybe the plx pci-e 4.0 switch way might unlock a possibility to run everything on the same node ? If you still have mmio issues with that I dunno, but there are some plx88096 cards with 10x (or 6x) slimsas 8i connectors, you could run 8 cards per plx card on 4 x16 slot, I dunno about overall bandwith restriction + p2p pci-e under each Plx cards",
                  "score": 3,
                  "created_utc": "2026-01-07 19:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny94l4v",
                  "author": "Legal-Ad-3901",
                  "text": "I hear ya on node. I tried rpc and it's sooooo slow. I'm trying RoCEv2/rdma soon ðŸ¤ž",
                  "score": 1,
                  "created_utc": "2026-01-07 19:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny9yvt8",
          "author": "And-Bee",
          "text": "All this so you can flirt with your AI gf and not worry itâ€™s on the cloud.",
          "score": 1,
          "created_utc": "2026-01-07 21:26:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny97mmr",
          "author": "gofiend",
          "text": "Are MoE models still slow / broken on the 906 VLLM fork? Not finding any viable models to run on two MI50s with VLLM when I can have gpt oss 120 running beautifully via llama.cpp",
          "score": 1,
          "created_utc": "2026-01-07 19:29:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9g0a3",
              "author": "ai-infos",
              "text": "MOE models are still not too optimized on gfx906 vllm fork but working fine for awq and gptq quant, and prompt processing speed with tensor parallelism on vllm is worth it.  \nOn 2 MI50s, you can run dense models like old qwen 3 32b...but yeah, with your setup gpt oss 120b via llama.cpp might be one of the best choice.",
              "score": 3,
              "created_utc": "2026-01-07 20:05:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyad0wt",
          "author": "davikrehalt",
          "text": "Lmao might as well post a full server rack as a home llm computer",
          "score": 1,
          "created_utc": "2026-01-07 22:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ymh0",
          "author": "StardockEngineer",
          "text": "I mean, I love big ass projects like this as much as the next guy, but does it pencil out?  Is this cheaper than using an API?\n\nEither way I love it.",
          "score": 0,
          "created_utc": "2026-01-07 21:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyaajbt",
              "author": "ai-infos",
              "text": "honestly, it won't be cheaper than the deepseek v3.2 API at current price of 0.32$/1M tokens   \n  \nbut if you've got solar panels and intense use cases, you can amortize the devices cost after few years (while enjoying 100% local and private inference)",
              "score": 4,
              "created_utc": "2026-01-07 22:18:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyac6e6",
                  "author": "StardockEngineer",
                  "text": "Just nerding out on the solar thing - you'd need about $2k in panels? Are you running this hard 24/7?  That might pencil out if you are.",
                  "score": 4,
                  "created_utc": "2026-01-07 22:25:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyb3ogv",
          "author": "Novel-Mechanic3448",
          "text": ">**Goal**: run Deepseek V3.2 AWQ 4-bit on most cost effective hardware like 16\\*MI50 at decent speed (token generation & prompt processing)\n\nthis was never going to be cost effective. ever.",
          "score": 0,
          "created_utc": "2026-01-08 00:41:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9lyhx",
          "author": "dc740",
          "text": "I have 3 of these, and no matter the model, it always ends up hallucinating or enters a loop and it no longer outputs useful things. I even tried big models relying on the CPU and system memory, but it ends up crawling at less than a few tk/s",
          "score": 0,
          "created_utc": "2026-01-07 20:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9no56",
              "author": "ai-infos",
              "text": "What was the software used, model, and quantization?   \nMost of my tries with glm 4.6, minimax m2, qwen3 vl 235b were pretty steady with low hallucination, even at high context (in AWQ 4 bit with vllm-gfx906). But yes, in most open source models (mostly with older ones) the quality output starts to fall at very high context (>60k tok)",
              "score": 1,
              "created_utc": "2026-01-07 20:39:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny9oxg0",
                  "author": "dc740",
                  "text": "That is exactly what happened. I was testing quantized versions of glm, qwen, etc etc etc and they would break whenever I increased the context over around 30k (I honestly don't remember the number). I haven't tried again and last time it was like two months ago",
                  "score": 1,
                  "created_utc": "2026-01-07 20:44:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny8y5ag",
          "author": "Timziito",
          "text": "How do you use amd with Ai? Have I missed something?",
          "score": -3,
          "created_utc": "2026-01-07 18:47:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny90lh0",
              "author": "Skystunt",
              "text": "You can add modded RocM 7 support in Linux to got with vLLM or SGlang (there are some repos for it), but also Vulkan for llama.cpp works no problem. ( There's also the option to flash the bios with a Radeon VII one to get Windows Vulkan support but that's hard for 32G )\n\nAlso keep in mind this is for text inverence and prompt processing not model training or image generation. It's a different story otherwise but still doable",
              "score": 3,
              "created_utc": "2026-01-07 18:58:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny90w6i",
          "author": "Historical-Internal3",
          "text": "Yea. \n\nPretty sick, but I'd rather grab x4 DGX Sparks (the MSI variant's with better cooling) clustered on a   \nMikroTik CRS812 DDQ.\n\nTakes up a fraction of the space, a fraction of the power, and quiet as all hell too. \n\nHave two atm. \n\nAssuming your setup costs around $8-12k? Granted, my ideal setup I mentioned would cost about $18k....the savings on heat, electric, and space alone would add to the value.",
          "score": -8,
          "created_utc": "2026-01-07 18:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9m7k3",
              "author": "StardockEngineer",
              "text": "Get the Asus for $3k and the 4x DGXs are 12k.",
              "score": 0,
              "created_utc": "2026-01-07 20:32:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny9nh7k",
                  "author": "Historical-Internal3",
                  "text": "I have the 12US version with MSI - so itâ€™s 4TB at Gen 5. \n\nWorth the money - currently canâ€™t purchase 2242 SSDâ€™s (except one from Corsair that isnâ€™t great). \n\nAsus variantâ€™s are gen 4 and 1TB at that price point youâ€™re referring to.",
                  "score": 0,
                  "created_utc": "2026-01-07 20:38:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg4yt",
      "title": "Tencent just released WeDLM 8B Instruct on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyg4yt",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 07:38:43",
      "score": 422,
      "num_comments": 62,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwiswg2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 10:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigd3k",
          "author": "jamaalwakamaal",
          "text": "7-8B models have lot of potential. Very promising space. More models please.",
          "score": 49,
          "created_utc": "2025-12-29 08:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie4o6",
          "author": "Endlesscrysis",
          "text": "Pretty huge I think? I thought I saw people mentioning a couple of times that diffusion models werenâ€™t possible for accurate LLMâ€™s yet this outperforms a similar sized powerhouse like qwen?",
          "score": 85,
          "created_utc": "2025-12-29 07:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio6nn",
              "author": "SlowFail2433",
              "text": "Yeah I was one of the pretty vocal skeptics about diffusion language models. I thought their inductive bias was too sub-optimal for language/code. I was super wrong about this.",
              "score": 50,
              "created_utc": "2025-12-29 09:20:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvefy",
                  "author": "Investolas",
                  "text": "I'd love to read one of your critiques, care to share a link to a comment or post you've made? I didn't find any of your contributions and assume they are paywalled. Thx!",
                  "score": 9,
                  "created_utc": "2025-12-29 14:45:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnur1r",
                  "author": "aeroumbria",
                  "text": "Interestingly I am more of the opinion that the autoregressive inductive bias is too restricting and unnatural, and may contribute to why we need so many parameters to reach usability. It feels like traditional linguistics gives more credit to a \"large scale autoregressive (causal dependency), small scale hierarchical (tree structure in grammar)\" type of model, which is closer to block diffusion. Still not entirely sold on the token-wise masking process thing though - it cannot reflect a hierarchical \"concept refinement\" process. Interested to see any progress in this direction though.",
                  "score": 2,
                  "created_utc": "2025-12-30 02:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkw478",
              "author": "Orolol",
              "text": "We know diffusion is possible since atleast Llada 18 months ago. But the problem was that it used a non causal attention, so we were unable to use many crucial techniques, like kv cache. \nThis enables the use of kvcache because of a very clever trick.",
              "score": 10,
              "created_utc": "2025-12-29 17:43:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwooixj",
              "author": "Mikasa0xdev",
              "text": "Diffusion models are the new transformers, confirmed.",
              "score": 2,
              "created_utc": "2025-12-30 05:52:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiiec4",
          "author": "jacek2023",
          "text": "additionaly [https://huggingface.co/tencent/WeDLM-7B-Instruct](https://huggingface.co/tencent/WeDLM-7B-Instruct)",
          "score": 33,
          "created_utc": "2025-12-29 08:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiji7b",
              "author": "aeroumbria",
              "text": "Interesting. Is there a specific use case where 8B can't fit but 7B can?",
              "score": 12,
              "created_utc": "2025-12-29 08:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwiufh1",
                  "author": "pkmxtw",
                  "text": "The 7B is converted from Qwen2.5 7B and the 8B is from Qwen3 8B. What they want to demonstrate is that they can convert an AR model into a diffusion model w/o losing quality.\n\nIn reality, you'd just use the 8B like how Qwen3 8B has basically replaced Qwen2.5 7B.",
                  "score": 40,
                  "created_utc": "2025-12-29 10:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwigcqz",
          "author": "Paramecium_caudatum_",
          "text": "Diffuser model with impressive benchmark scores and Apache 2.0 license, sounds pretty interesting to me.",
          "score": 57,
          "created_utc": "2025-12-29 08:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihmcw",
          "author": "FinBenton",
          "text": "Its just a small model but 3-6x speed with similar or higher performance sounds insane!",
          "score": 24,
          "created_utc": "2025-12-29 08:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlum7p",
              "author": "lolwutdo",
              "text": "I know diffusion models are super fast on gpu but how would a diffusion model's speed compare on cpu vs a cpu llm?\n\nI guess mainly what I'm curious about is how well would a diffusion based llm run with cpu offloading compared to a traditional llm.",
              "score": 2,
              "created_utc": "2025-12-29 20:25:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm87rw",
                  "author": "oh_how_droll",
                  "text": "Diffusion is going to be slower on CPUs -- CPUs are mostly compute-limited and they're more compute intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwie4gd",
          "author": "SlowFail2433",
          "text": "Nice to see another diffusion model would have liked more modern/harder benches",
          "score": 16,
          "created_utc": "2025-12-29 07:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwim9sq",
          "author": "Nice-Information-335",
          "text": "need unsloth or bartowski on this asap",
          "score": 21,
          "created_utc": "2025-12-29 09:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwirhjw",
              "author": "Odd-Ordinary-5922",
              "text": "will need a pr first for model support",
              "score": 38,
              "created_utc": "2025-12-29 09:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjz6tm",
                  "author": "MoffKalast",
                  "text": "We need a few papers first for model support",
                  "score": 8,
                  "created_utc": "2025-12-29 15:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwohedf",
              "author": "tronathan",
              "text": "Not really, in terms of usefuless, as I understand it, it's basically a Qwen 3. It's more of a proof of confacept",
              "score": 1,
              "created_utc": "2025-12-30 05:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwplso2",
                  "author": "Nice-Information-335",
                  "text": "hey I still want to try it! half of the fun for me is seeing advancements as they happen and being able to run them. massive props to everyone who makes that happen, as lord knows I don't know nearly enough to get this stuff working without the likes of llama.cpp, all it's amazing contributors and unsloth/bartowski for GGUFs",
                  "score": 1,
                  "created_utc": "2025-12-30 10:50:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwuzqra",
                  "author": "TomLucidor",
                  "text": "Let them make a version that beats Qwen3-30B-A3B and Nemotron-3-Nano",
                  "score": 1,
                  "created_utc": "2025-12-31 04:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwiw5cp",
          "author": "always_newbee",
          "text": "What is Qwen3-8B-Instruct model? Just non-thinking mode?",
          "score": 5,
          "created_utc": "2025-12-29 10:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj0ed1",
              "author": "mouseofcatofschrodi",
              "text": "yes",
              "score": 3,
              "created_utc": "2025-12-29 11:13:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigrdo",
          "author": "Grouchygrond",
          "text": "Now we just need a hybrid model",
          "score": 5,
          "created_utc": "2025-12-29 08:10:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjuykz",
              "author": "Deciheximal144",
              "text": "How would that work? Diffusing in chunks? LLM generates, then diffusion revises the lowest-probability sections? Diffusion is noise-to-content.",
              "score": 6,
              "created_utc": "2025-12-29 14:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwljqrp",
                  "author": "peaceoutwhat",
                  "text": "Search TiDAR",
                  "score": 3,
                  "created_utc": "2025-12-29 19:32:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwm7wqb",
                  "author": "TheRealMasonMac",
                  "text": "There was a research model that diffused chunks one at a time like a Frankenstein of current LLMs and dLLMs\n\n\nhttps://m-arriola.com/bd3lms/",
                  "score": 3,
                  "created_utc": "2025-12-29 21:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkwy6z",
              "author": "Orolol",
              "text": "I don't this it's possible to have both autoregressive and diffusion generation, and even if possible, I don't think there's any positive doing it.",
              "score": 2,
              "created_utc": "2025-12-29 17:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlyqsm",
          "author": "Semi_Tech",
          "text": "Hmm shouldn't diffusion models also have a # of steps needed in order to reach the end result?\n\nI don't see a mention about that or how increasing or decreasing them affects model output quality.",
          "score": 5,
          "created_utc": "2025-12-29 20:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiksnh",
          "author": "Healthy-Nebula-3603",
          "text": "That's diffusion model right ?\n\n\nAs I understand such model can't be reasoner as can't looping in thoughts and observe own internal states?",
          "score": 8,
          "created_utc": "2025-12-29 08:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwilg37",
              "author": "Lesser-than",
              "text": "diffusion text models technically reason, as they can modify the first word of a sentence or tokens at every step of the inference, where a token by token model has to justify that token for the rest of the reply if they get it wrong.",
              "score": 25,
              "created_utc": "2025-12-29 08:54:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwilp6x",
                  "author": "Healthy-Nebula-3603",
                  "text": "I meant they can reason like the instruct models but are not thinkers like thinking models.",
                  "score": 1,
                  "created_utc": "2025-12-29 08:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj7ycj",
              "author": "NandaVegg",
              "text": "According to the site, this is a variation of block-wise diffusion (previously done by Meta etc) which acts more akin to a speculative decoding rather than a \"full\" diffusion (that denoises the whole output at once). I think Google did a web demo for mini full diffusion model in early 2025 but the model weight never got released?",
              "score": 7,
              "created_utc": "2025-12-29 12:16:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwv0suo",
              "author": "TomLucidor",
              "text": "Diffusion models can reason, just that not enough people put effort into the \"train of thought\" similar to auto-regressive models.",
              "score": 1,
              "created_utc": "2025-12-31 04:33:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjjpa4",
          "author": "alphapussycat",
          "text": "What does math reasoning even mean? Calculation reasoning? Or math, as in theorem, reasoning?",
          "score": 3,
          "created_utc": "2025-12-29 13:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjkd02",
              "author": "PykeAtBanquet",
              "text": "Usually it is \"prove that this series converges\" etc",
              "score": 2,
              "created_utc": "2025-12-29 13:42:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihwec",
          "author": "JackStrawWitchita",
          "text": "More people have commented on this than have downloaded it...",
          "score": 15,
          "created_utc": "2025-12-29 08:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio18v",
              "author": "SlowFail2433",
              "text": "In ML research we often donâ€™t download the model right away.\n\n\nNote that the paper used the MagiAttention library for attention. I donâ€™t use this library so I am either going to write a custom CUDA kernel or use a DSL like Triton. However the paper has some technical novelties such as the topological reordering. This is not going to be easy to work out how to implement efficiently.",
              "score": 39,
              "created_utc": "2025-12-29 09:19:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwnlhqs",
                  "author": "RhubarbSimilar1683",
                  "text": "The paper is https://github.com/Tencent/WeDLM/blob/main/paper/wedlm.pdf",
                  "score": 1,
                  "created_utc": "2025-12-30 01:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiu7q5",
              "author": "FinBenton",
              "text": "Gotta wait for llama.cpp and similar support first, most people here arent running vllm.",
              "score": 27,
              "created_utc": "2025-12-29 10:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkk2j0",
                  "author": "Tai9ch",
                  "text": "Not downloading open source software seems like a lame excuse to not try something neat.",
                  "score": -4,
                  "created_utc": "2025-12-29 16:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoz431",
              "author": "aeroumbria",
              "text": "Still getting issues running the official repo... Supposedly this is only 8B and supports multi-GPU but cannot seem to allocate KV even with 2x24GB",
              "score": 1,
              "created_utc": "2025-12-30 07:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkdz4",
          "author": "Awkward-Nothing-7365",
          "text": "Is this something that can run on llama.cpp right now? gguf possible?",
          "score": 2,
          "created_utc": "2025-12-29 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk62zs",
          "author": "implicator_ai",
          "text": "Interesting release. When they say â€œdiffusion language model,â€ it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work.   \n  \nThe 3â€“6Ã— claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 3,
          "created_utc": "2025-12-29 15:39:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwml948",
              "author": "SilentLennie",
              "text": "From what I understand: diffusion models usually were not faster than regular LLMs, because they have K/V-cache and other tricks to speed it up to prevent doing duplicate math, supposedly this model solves that.",
              "score": 1,
              "created_utc": "2025-12-29 22:37:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlqfpz",
          "author": "rm-rf-rm",
          "text": "They report the speed up for specifically just math reasoning tasks but it should be applicable generally no? \n\nHope we get MLX/GGUF support soon. If this is legit, its genuinely going to be massive. Right now I run 4B for quick look up etc. but I feel 4B models are not the most reliable for accurate information. At 8B, you can be much more confident.\n\nNext step MoE? Qwen3-Coder:a3b?",
          "score": 1,
          "created_utc": "2025-12-29 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwngm0k",
          "author": "RhubarbSimilar1683",
          "text": "Could diffusion enable efficient hybrid inference or inference computer clusters connected over the global internet, using asynchronous calls?",
          "score": 1,
          "created_utc": "2025-12-30 01:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnqpie",
          "author": "Vast-Piano2940",
          "text": "I wonder how it performs against lfm2-2.6b-exp",
          "score": 1,
          "created_utc": "2025-12-30 02:23:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuzlr9",
          "author": "TomLucidor",
          "text": "As long as this can be used with Claude Code or some other coding agent.",
          "score": 1,
          "created_utc": "2025-12-31 04:25:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd6nho",
      "title": "Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.scmp.com/tech/tech-war/article/3339869/zhipu-ai-breaks-us-chip-reliance-first-major-model-trained-huawei-stack",
      "author": "fallingdowndizzyvr",
      "created_utc": "2026-01-15 02:01:03",
      "score": 404,
      "num_comments": 45,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/",
      "domain": "scmp.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzp79d2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 09:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznqn0p",
          "author": "RhubarbSimilar1683",
          "text": "So the Chinese ban on Nvidia is working. It's just a matter of time before it's scaled up to larger models",
          "score": 179,
          "created_utc": "2026-01-15 02:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznsk93",
              "author": "foldl-li",
              "text": "Or rather, US' ban on NV is working.",
              "score": 63,
              "created_utc": "2026-01-15 02:44:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzovaiw",
                  "author": "Nobby_Binks",
                  "text": "IIRC, China also turned around and banned Nvidia to force local development.",
                  "score": 40,
                  "created_utc": "2026-01-15 07:21:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoxz7l",
                  "author": "Maleficent-Scene7771",
                  "text": "God bless America\n\nSun Tzu bless China.",
                  "score": 27,
                  "created_utc": "2026-01-15 07:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzohaxm",
              "author": "zuraken",
              "text": "It was America that banned selling them to China which spurred more development. I mean development was there before the ban, but that ban really speedran their process lmfao",
              "score": 21,
              "created_utc": "2026-01-15 05:26:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzopfb8",
                  "author": "pissoutmybutt",
                  "text": "I dont understand how anyone could think China is incapable of working around shit like this. They are a  superpower built on engineering and manufacturing with a command economy able to provide as much funding as necessary towards addressing the issue",
                  "score": 45,
                  "created_utc": "2026-01-15 06:30:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzpnvld",
                  "author": "enilea",
                  "text": "The US ban on Nvidia chips was limited to the high end chips, the intention was limiting them to smaller commercial GPUs and older models. But in turn China banned its own companies from using most of the models that were allowed. The H200 model was recently approved by the US to be exported to China, but China rejected it because they want to breed their own GPU market instead of relying on the US, which isn't a reliable trade partner.",
                  "score": 9,
                  "created_utc": "2026-01-15 11:46:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqoi63",
              "author": "andy_potato",
              "text": "The problem is the ban isnâ€™t working. Gamer Nexus made a great documentary about how GPUs are still getting into China with Nvidia looking the other way.",
              "score": 0,
              "created_utc": "2026-01-15 15:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrz3da",
                  "author": "RhubarbSimilar1683",
                  "text": "It doesn't have to be perfect. Just inconvenientÂ ",
                  "score": 2,
                  "created_utc": "2026-01-15 18:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznxchc",
          "author": "andy_potato",
          "text": "Lots of people here gave it a try and the outputs are really not good. I understand that this is more of a tech demo or a MVP showing off alternative model architectures. But maybe wasn't a good idea to make this a major release and getting people all hyped up about the model capabilities.",
          "score": 55,
          "created_utc": "2026-01-15 03:12:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoi99x",
              "author": "ForsookComparison",
              "text": "Somewhere in the world there is a bunch of cracked traders who monitored how gooner forums reacted to a new image model trained on Huawei GPUs and the result of that decided that the US Stock Market gets to chug along happily for at least a few more months.",
              "score": 41,
              "created_utc": "2026-01-15 05:33:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzoit58",
                  "author": "andy_potato",
                  "text": "The 1girl army of r/StableDiffusion sure did their part",
                  "score": 19,
                  "created_utc": "2026-01-15 05:37:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzpkg4y",
                  "author": "SkyFeistyLlama8",
                  "text": "Someone needs to generate an image of a Wall Street trader doing a line or ten of coke while looking at GPU prices on their ten-monitor wall.",
                  "score": 4,
                  "created_utc": "2026-01-15 11:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznvl3b",
          "author": "AfterAte",
          "text": "SD1.5 (38 months ago) was 0.8B, and SDXL (29 months ago) was a 2.6B model. Flux.1 (17 months ago) was a 12B model. All trained on Nvidia.\n\nThey are less than 2 years away using only a Huawei hardware/software stack. No CUDA. And Flux.1 didn't have image edititing. Z.ai proves non-CUDA training and inference is viable.\n\n\nThis is an important development. The rate of development will be faster than linear. China scales faster than anyone, has the necessary energy production and scientists. All important ingredients. This is bigger than it seems.",
          "score": 65,
          "created_utc": "2026-01-15 03:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp2s9g",
              "author": "i_not_give_shit",
              "text": "\"proves non-CUDA training and inference is viable.\"\n\nWhat do you mean? I have used vulkan llama-cpp for inference for a long time, havent seen a difference with cuda.",
              "score": 9,
              "created_utc": "2026-01-15 08:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp6uao",
                  "author": "AfterAte",
                  "text": "I don't mean to put emphasis on inference alone, I mean \"not only inference, but training as well\", but I don't want to sound like an AI. Training was the main point of the article. Everyone here knows Vulkan is good at inference now.",
                  "score": 9,
                  "created_utc": "2026-01-15 09:11:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzp3nu2",
                  "author": "Hunting-Succcubus",
                  "text": "Opencl too. Cuda is good but not only option.",
                  "score": 9,
                  "created_utc": "2026-01-15 08:40:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzock9k",
              "author": "LocoMod",
              "text": "No its really not. You can train a model on a CPU if you want. But that's not relevant to the conversation is it. What matters is speed to market and quality of results (nvidia hardware is nothing without CUDA). Its more than chips. \n\nThe GLM image model is not as capable as other recent smaller models. And we have no objective metrics as to how much time it took to train, etc.\n\nYou're embellishing something you dont understand. They made progress. But its not as big a deal as you are implying.",
              "score": 18,
              "created_utc": "2026-01-15 04:51:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzowk2j",
                  "author": "redditscraperbot2",
                  "text": "In its defense, it's an autoregressive model and they have historically been shit regardless of what they are trained on.",
                  "score": 17,
                  "created_utc": "2026-01-15 07:33:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzp5idx",
                  "author": "AfterAte",
                  "text": "I didn't say GLM was better than the recent Flux2, Z-Image-Turbo or Qwen-2507. But it is better than SD1.5 and early versions of SDXL (with no Lora), and more capable than Flux.1 That's why I said they are 2 years away.\n\n\nWhat it is important is China has a hardware/software stack the American government can't slow down because it's all domestic, and Z.ai proved it works.Â Â \n\n\nSo geopolitically speaking, this is big.",
                  "score": 15,
                  "created_utc": "2026-01-15 08:57:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznnsto",
          "author": "RetiredApostle",
          "text": "The \"major\" model here is GLM-Image 9B.",
          "score": 23,
          "created_utc": "2026-01-15 02:16:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznzv7o",
              "author": "cutebluedragongirl",
              "text": "LMAO",
              "score": -4,
              "created_utc": "2026-01-15 03:28:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo2y7r",
          "author": "Different_Fix_2217",
          "text": "The model is terrible in every way so not the best showcase imo.",
          "score": 11,
          "created_utc": "2026-01-15 03:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzntlop",
          "author": "Recoil42",
          "text": "Makes perfect sense why GLM-Image is so mid now â€”Â this is the MVP. \n\nDoes know how much output SMIC is projected to be ramping for these?",
          "score": 2,
          "created_utc": "2026-01-15 02:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo2gyj",
              "author": "Clear_University5148",
              "text": "No, it made sense before. GLM-Image is a research project into an experimental architecture, not a product.",
              "score": 27,
              "created_utc": "2026-01-15 03:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq3vhl",
          "author": "kc858",
          "text": "what the hell is this headline\n\nwhat the hell happened to this sub",
          "score": 2,
          "created_utc": "2026-01-15 13:32:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzovlf8",
          "author": "Umademedothis2u",
          "text": "Overstating the capabilities, under-delivering the actual outcomes... YUP that is about right for a Chinese model",
          "score": -15,
          "created_utc": "2026-01-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp3wrs",
              "author": "Hunting-Succcubus",
              "text": "Your statement is proven by deepseek,qwen, wan video, zimage. Typical under delivering Chinese products. But i still love them.",
              "score": 9,
              "created_utc": "2026-01-15 08:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qa0w6c",
      "title": "It works! Abliteration can reduce slop without training",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qa0w6c",
      "author": "-p-e-w-",
      "created_utc": "2026-01-11 14:37:37",
      "score": 396,
      "num_comments": 123,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyz8hy6",
          "author": "Chromix_",
          "text": "Could this also be used for overused patterns? For example: \"It's not just word avoidance; it's a whole new style of writing!\"",
          "score": 111,
          "created_utc": "2026-01-11 14:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyz9p4t",
              "author": "-p-e-w-",
              "text": "Itâ€™s quite possible that such patterns automatically get eliminated (or at least reduced) by attacking cliches in general. Thereâ€™s some deep magic going on in residual space where things are often already connected in the same way humans tend to think about them.\n\n(Disclaimer: This isnâ€™t a comment on whether or not LLMs can â€œreally thinkâ€, just an observation from someone who has spent quite a few hours crawling through the guts of language models.)",
              "score": 65,
              "created_utc": "2026-01-11 14:53:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz1g4se",
                  "author": "ak_sys",
                  "text": "I'm curious, if the redundancy of these connections(and the verbal \"style\" we observe as a consequence) is integral to the deep reasoning these models do, and that it may be impossible to achieve the same level of performance baked in by it's reinforced \"style\" without significantly affecting reasoning potential. \n\nWhat I mean:\n\nIntelligence and reasoning emerges from a simple algorithm that predicts the next token. The output of any token is a function of EVERY token that came before it, including the introductory \"small talk\". We fine tune these models to have the highest apparent intelligence possible, but they are judged by the correctness, conciseness, and alignment of their final answer, not necessarily EVERY token they produce. \n\nThe model picks and chooses simple looking introductory tokens, but the tokens are shifting the embedding space, priming it for the more complex thought to come. It is a buffer that gives the model a chance to append the \"source number\" that it will run through it's mathematically determinalistic function, so that the numbers given at the END of this recursive function are \"correct\". If the question is one spot in embedding space, and the answer is another, those stupid little nothing's that it says MAY be the equivalent of when you're playing an rpg and trying to to turn to look for your objective, opening your map to see you turned the wrong way, closing the map, adjusting, and checking the map again. \n\nI'm tempted to use the metaphor of when a kid is called on in class for an arithmetic problem, and the child prefaces his answer with \"the answer to 5 times 7 is...\" To allow his brain the time to think, but I know this isnt EXACTLY how it works.",
                  "score": 11,
                  "created_utc": "2026-01-11 20:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0oh04",
                  "author": "No_Afternoon_4260",
                  "text": "This goes down my spine..",
                  "score": 18,
                  "created_utc": "2026-01-11 18:53:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0d6ir",
                  "author": "International-Try467",
                  "text": "What about user bias and being a yes man? Could it get rid of it too?",
                  "score": 9,
                  "created_utc": "2026-01-11 18:03:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1q2na",
                  "author": "synth_mania",
                  "text": "You don't need to disclaim this, if something seems like it's doing something, it's doing it. If you look at LLM output, and judge that you would have to think to write something similar, so did the LLM, in it's own way. And thinking definitely doesn't equal sentience, so I don't think this is a very controversial take.",
                  "score": 8,
                  "created_utc": "2026-01-11 21:44:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3m7cs",
                  "author": "BoneDaddyMan",
                  "text": "what's stopping it from just using another pattern repeatedly?",
                  "score": 0,
                  "created_utc": "2026-01-12 03:34:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzezxo",
          "author": "fiery_prometheus",
          "text": "Personally, I like the slop reduced more, but it still lacks imagination, IMO.\n\n\nWhat I'm curious about, is whether this technique is reducing the semantic meaning of slop, or it outright bans all syntactic patterns more than the semantic \"meaning\". Say you would write something which only contain a trace amount of slop, because recombining things in unique and coherent ways can be seen as creativity, would this technique then ban the \"surrounding\" good narrative, because part of it might have a bit of slop associated in latent space?",
          "score": 28,
          "created_utc": "2026-01-11 15:20:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0htv1",
              "author": "TheRealMasonMac",
              "text": "For creativity, there isÂ https://github.com/facebookresearch/darling/tree/main (one of the AllenAI guys pointed me to it on their AMA)",
              "score": 12,
              "created_utc": "2026-01-11 18:24:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz2p1zl",
                  "author": "fiery_prometheus",
                  "text": "nice, was looking into trying this as well [https://arxiv.org/abs/2510.15061](https://arxiv.org/abs/2510.15061)",
                  "score": 2,
                  "created_utc": "2026-01-12 00:38:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyzfoni",
              "author": "-p-e-w-",
              "text": "The consensus today is that residual space encodes semantics, not syntax or vocabulary, so my guess is that itâ€™s the former.",
              "score": 11,
              "created_utc": "2026-01-11 15:24:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyzmijs",
                  "author": "-lq_pl-",
                  "text": "This would be great as a Lora, where you can tune the strength of 'flowery language' in your model with a knob.  \n  \nOne can see why RLHF selects flowery language, because in isolation, the non-slop version feels soulless and a bit boring. But that impression changes when you read loads and loads of this stuff. Your technique would be a great way to tone down the slop to individual taste.",
                  "score": 11,
                  "created_utc": "2026-01-11 15:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1muha",
              "author": "fallingdowndizzyvr",
              "text": "If creativity is your goal, then use a stoned AI. That's why they exist.",
              "score": 1,
              "created_utc": "2026-01-11 21:29:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4cyu9",
                  "author": "skinnyjoints",
                  "text": "Iâ€™m sorry. A what?!",
                  "score": 3,
                  "created_utc": "2026-01-12 06:38:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz024sl",
          "author": "noctrex",
          "text": "Created some GGUF's: [https://huggingface.co/noctrex/Mistral-Nemo-Instruct-2407-heretic-noslop-GGUF](https://huggingface.co/noctrex/Mistral-Nemo-Instruct-2407-heretic-noslop-GGUF)",
          "score": 28,
          "created_utc": "2026-01-11 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0290s",
              "author": "-p-e-w-",
              "text": "Cool, thanks!",
              "score": 3,
              "created_utc": "2026-01-11 17:12:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz5m069",
              "author": "BaysQuorv",
              "text": "MLX too [https://huggingface.co/alexgusevski](https://huggingface.co/alexgusevski)",
              "score": 0,
              "created_utc": "2026-01-12 13:06:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz0oodb",
          "author": "pip25hu",
          "text": "All this seems to do is make the prose dry, at least to me. \n\nIn my opinion, the problem with slop is that in small quantities, it can enhance the output. The problem is not that these phrases are used, but that they are used way more often than desirable.\n\nAlso, how are \"whispered\" and \"coax\" slop, irrespective of context?",
          "score": 21,
          "created_utc": "2026-01-11 18:54:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz13ce8",
              "author": "-p-e-w-",
              "text": "I donâ€™t agree with that at all. To me, the prose from the second output is much better in every dimension, and the style from the first is unacceptable, even in small quantities.\n\nThat being said, I understand that tastes differ, and if you want a hybrid between the two, you can just run the process yourself and choose a trial from the Pareto front that has a higher â€œrefusalâ€ (slop) count.",
              "score": 20,
              "created_utc": "2026-01-11 19:59:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz6gn9w",
                  "author": "Nyghtbynger",
                  "text": "I do agree. I have a good sensitivity to patterns in language and whilst the first one has more elaborate expressions it feels more mechanical, like wanting to please a scoreboard with expressions that look smart. The second one however feels more genuine and personal in style",
                  "score": 8,
                  "created_utc": "2026-01-12 15:49:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz6sktd",
                  "author": "lorddumpy",
                  "text": "I agree with /u/pip25hu. The second version honestly read like a pre-gpt 3 or low parameter model, feels very paint by the numbers. The prose is basic and there is barely any emotional response IMO.",
                  "score": 2,
                  "created_utc": "2026-01-12 16:43:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz6lnoy",
                  "author": "Technical_Report",
                  "text": "Rofl. Just a suggestion, but maybe take an autism test.",
                  "score": -4,
                  "created_utc": "2026-01-12 16:12:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyz9kpv",
          "author": "Aerikh",
          "text": "That's interesting. I wonder if this could work for repetition reduction too, especially in long contexts where many fine tuners don't have the hardware/budget/data for such long context training.",
          "score": 9,
          "created_utc": "2026-01-11 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzhr7g",
              "author": "kaisurniwurer",
              "text": "Hmm... I'm not an expert but I don't think so, or at least not much. Repetition is more attention/structural problem not quite a weights one.\n\nWith abliteration you actually aim to reduce strength of the activation paths for \"concepts\" you don't want to see, while repetition is more of a model paying too much \"attention\" to certain previous concepts (not one single predictable one) to the next token.\n\nPS. You are my hero -p-e-w-",
              "score": 9,
              "created_utc": "2026-01-11 15:34:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyza8p6",
          "author": "silenceimpaired",
          "text": "Wow, I have shivers down my spine. Hopefully your list is wholistic enough to remove most.",
          "score": 32,
          "created_utc": "2026-01-11 14:56:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzefdy",
              "author": "Not_your_guy_buddy42",
              "text": ">shivers down my spine\n\nIs your name Elara by chance?",
              "score": 42,
              "created_utc": "2026-01-11 15:18:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyziqbi",
                  "author": "silenceimpaired",
                  "text": "Sheâ€™s my girlfriend, we live in the kingdom of Aurelion.",
                  "score": 35,
                  "created_utc": "2026-01-11 15:39:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzyh2h",
          "author": "grimjim",
          "text": "The split occurring between layers 7-10 is interesting mechanistically. That's fairly early in processing.",
          "score": 8,
          "created_utc": "2026-01-11 16:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz01i55",
              "author": "-p-e-w-",
              "text": "Yes, I was surprised as well, especially by how sharp and sudden the transition is for layer 10. With refusals itâ€™s expected, because refusing harmful prompts is a core part of post-training, but nobody specifically trains for or against slop and yet there is clearly a distinct feature in residual space that represents it.",
              "score": 5,
              "created_utc": "2026-01-11 17:08:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz0ku6u",
                  "author": "bick_nyers",
                  "text": "Well, RLHF could certainly optimize for slop. If they trained AI classifiers/tuned LLM-as-a-judge for certain attributes too then RLAIF could be optimizing for it as well.\n\n\nI wouldn't be surprised if user preference data optimized towards slop as well, most users are not power users/enthusiasts.",
                  "score": 11,
                  "created_utc": "2026-01-11 18:37:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz0if27",
          "author": "morsecob",
          "text": "Am I the only who saw the map of the united states at first glance?",
          "score": 8,
          "created_utc": "2026-01-11 18:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz8eel",
          "author": "Medium_Chemist_4032",
          "text": "As a developer looking forward to use LLMs in fact from data extraction, this looks very promising",
          "score": 11,
          "created_utc": "2026-01-11 14:46:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyza9cr",
          "author": "jacek2023",
          "text": "I was reading your project few days ago and I was wondering, are all models processed with the same prompts? Now I see finally new ones :)",
          "score": 5,
          "created_utc": "2026-01-11 14:56:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzbd8w",
              "author": "-p-e-w-",
              "text": "Prompt datasets have always been configurable, but now itâ€™s possible to load a dataset and dynamically modify it by injecting prefixes and suffixes.",
              "score": 6,
              "created_utc": "2026-01-11 15:02:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz47nj0",
          "author": "stoppableDissolution",
          "text": "I'm not sure I like the second output more (I'm not entirely on board with your definition of slop, basically), but I think it is still quite fascinating that there is basically a distinct dimension for cliches, I assumed it to be way more spread around.\n\nI wonder if abliteration like that would make it easier to impose the desired writing style during finetuning.",
          "score": 6,
          "created_utc": "2026-01-12 05:54:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzrbbm",
          "author": "a_beautiful_rhind",
          "text": "I wish it was doable to quantized models. It used to be possible to merge lora into GGUF but seems any work on non full precision has been abandoned.",
          "score": 4,
          "created_utc": "2026-01-11 16:20:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyztlvt",
              "author": "-p-e-w-",
              "text": "The master branch of Heretic supports loading models with bnb quantization, saving up to 75% VRAM.",
              "score": 5,
              "created_utc": "2026-01-11 16:31:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz00sb7",
                  "author": "a_beautiful_rhind",
                  "text": "BnB is still kind of a stopgap. I'm not sure how well you can quantize to something else from it. If you are quanting at runtime, still downloaded the full model. \n\nOn smalls it's not a big deal but for trying to deslop something like mistral-large or GLM, it's not quite enough.",
                  "score": 3,
                  "created_utc": "2026-01-11 17:05:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz30xsg",
                  "author": "Lissanro",
                  "text": "Is quantization supported for CPU-only, so I could process models that do not fully fit in VRAM?",
                  "score": 1,
                  "created_utc": "2026-01-12 01:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2a6m7",
          "author": "Feztopia",
          "text": "The second one does sound better for now. But the question is if it has patterns which we don't notice because this is now new. Talk with that model often enough and maybe you will see patterns which at some time will sound kind slop to you. And these patterns might also be in the story above. We humans are also capable of recognizing patterns.",
          "score": 5,
          "created_utc": "2026-01-11 23:23:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2bed3",
          "author": "DOAMOD",
          "text": "Iâ€™ve tested the noslop Nemo model and asked it to tell me a story, and this is what it replied; never a model has said this before with tell me a story:\n\nUnfortunately, as a language model I donâ€™t have the ability to narrate stories. However, I can help you with any other question or task you need. Is there anything else I can assist you with?",
          "score": 4,
          "created_utc": "2026-01-11 23:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2e1xe",
              "author": "-p-e-w-",
              "text": "Make sure you set a system prompt like â€œYou are a professional writer.â€",
              "score": 2,
              "created_utc": "2026-01-11 23:43:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz2i2er",
                  "author": "DOAMOD",
                  "text": "Thanks. Here, (yes no slop):\n\nOnce upon a time, there was a man who worked in a factory. Every day he would go to work, do his job, and then go back home.\n\nOne day, he decided to change things. He started arriving early and offered to work overtime shifts. His colleagues noticed and began to respect him more for his dedication.\n\nThe man continued to work hard without stopping. Then, one day, he was suddenly fired without any apparent reason.\n\nAfter that, the man had no other option but to look for work elsewhere. He found a good job with a different company and never returned to work at the factory.\n\nHard work and dedication can make a difference in your workplace, but you should also be prepared for sudden changes.",
                  "score": 5,
                  "created_utc": "2026-01-12 00:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2yl55",
          "author": "SnooComics5459",
          "text": "I am interested in working with you as I think this is one of the best projects in this space.  How can I contribute?",
          "score": 2,
          "created_utc": "2026-01-12 01:27:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz36tvs",
              "author": "-p-e-w-",
              "text": "Happy to hear that! The answer is go to the GitHub project and file issues and/or pull requests! ðŸ˜€",
              "score": 3,
              "created_utc": "2026-01-12 02:11:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz4ej93",
                  "author": "SnooComics5459",
                  "text": "may I send you a dm?",
                  "score": 1,
                  "created_utc": "2026-01-12 06:51:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2mopg",
          "author": "TomLucidor",
          "text": "The question now is how brain surgery WON'T degrade performance (e.g. common benchmarks) by accident. Or maybe like eliminating refusal, new abilities could emerge?",
          "score": 3,
          "created_utc": "2026-01-12 00:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2p0f2",
          "author": "Juice_567",
          "text": "This was very educational, thank you. Really interested in learning how mechanistic interpretability works",
          "score": 3,
          "created_utc": "2026-01-12 00:38:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4ejbo",
          "author": "IrisColt",
          "text": "From what I can tell, the second example is indistinguishable from human-written prose. The first one sets off all my alarms, if you know what I mean.",
          "score": 3,
          "created_utc": "2026-01-12 06:51:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzzwss",
          "author": "theblackpen",
          "text": "Super interesting work!",
          "score": 6,
          "created_utc": "2026-01-11 17:01:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzu92w",
          "author": "hidden2u",
          "text": "So if you have â€œpromiseâ€ as a refusal marker in your config file, does that mean it will never/rarely use the word promise?",
          "score": 2,
          "created_utc": "2026-01-11 16:34:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzvasu",
              "author": "-p-e-w-",
              "text": "No, because the score is calculated over 100 prompts. If one of them uses the word \"promise\", that's -1 on the score, which isn't significant enough to converge towards banning a word.",
              "score": 10,
              "created_utc": "2026-01-11 16:39:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz0zwp0",
                  "author": "hidden2u",
                  "text": "So thatâ€™s better than banning tokens, nice",
                  "score": 5,
                  "created_utc": "2026-01-11 19:44:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2rl5w",
          "author": "MemeGuyB13",
          "text": "The first response is like reading a story, despite the AI-isms here and there.\n\nThe second response feels like someone wrote down an event between two other people with physical actions written down; no other interesting details.\n\nThe first response feels entrancing, while the second response feels like someone wrote about an event they saw in real life.\n\nThe prose is now â€œcleanerâ€, yes. But, itâ€™s been sanitized into extreme dryness thatâ€™s less interesting to read. I would rather read something that tries to communicate concepts larger than its wordsâ€”even with generic phrasingâ€”rather than something that doesnâ€™t even try to make the words fun to read.",
          "score": 2,
          "created_utc": "2026-01-12 00:51:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz366nf",
              "author": "-p-e-w-",
              "text": "I think you may be biased regarding what constitutes a â€œstoryâ€. I agree that the first response resembles a 21st century fanfic, but thatâ€™s not all stories are.\n\nHereâ€™s Aesopâ€™s fable â€œThe Sick Lionâ€:\n\n> A lion had come to the end of his days and lay sick unto death at the mouth of his cave, gasping for breath. The animals, his subjects, came round him and drew nearer as he grew more and more helpless. When they saw him on the point of death they thought to themselves: \"Now is the time to pay off old grudges.\" So the Boar came up and drove at him with his tusks; then a Bull gored him with his horns; still the Lion lay helpless before them: so the Ass, feeling quite safe from danger, came up, and turning his tail to the old Lion kicked up his heels into his face. \"This is a double death,\" growled the Lion.\n\nThatâ€™s it. Thatâ€™s the whole story, not a summary or abbreviation, directly translated from one of the most famous collections of stories ever written.",
              "score": 7,
              "created_utc": "2026-01-12 02:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3abpf",
          "author": "Blizado",
          "text": "If I understand this right that only works for english with this Mistral Nemo Heretic model because the config has only english words in it?",
          "score": 2,
          "created_utc": "2026-01-12 02:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3dpkn",
              "author": "-p-e-w-",
              "text": "Many concepts are believed to be represented in residual space in a language-independent way, so this might also affect other languages. I have not verified this though.",
              "score": 6,
              "created_utc": "2026-01-12 02:48:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4dqid",
          "author": "IrisColt",
          "text": "Iâ€™m a huge fan of your work... Iâ€™m in awe. Thanks!!!",
          "score": 2,
          "created_utc": "2026-01-12 06:44:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4j28b",
          "author": "IrisColt",
          "text": "One question... Can we both reduce refusals and remove slop?",
          "score": 2,
          "created_utc": "2026-01-12 07:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4m9h0",
              "author": "-p-e-w-",
              "text": "Yes, by running Heretic twice with different configurations.",
              "score": 3,
              "created_utc": "2026-01-12 08:01:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz5jpo6",
                  "author": "IrisColt",
                  "text": "Thanks! Can I make a joint version by merging or interpolating the two models? Is that a good idea?",
                  "score": 1,
                  "created_utc": "2026-01-12 12:52:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz5oyea",
          "author": "braydon125",
          "text": "I find the second one almost sterile. But very interesting work brother and im tuned the fuck in",
          "score": 2,
          "created_utc": "2026-01-12 13:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6m6xn",
          "author": "IrisColt",
          "text": "\\>the first slop-reduced LLM made using abliteration alone, with no finetuning whatsoever\n\n  \nThatâ€™s no small feat, congrats! Iâ€™ve been having a blast with your code since you published it. Thanks!",
          "score": 2,
          "created_utc": "2026-01-12 16:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzd55r",
          "author": "lisploli",
          "text": "**I totally see a llama in that picture!**\n\nDon't understand half of the text, but it sounds quite promising, and I'm looking forward to using whatever comes out of it. Thanks for whatever you do there!",
          "score": 7,
          "created_utc": "2026-01-11 15:11:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzibrc",
          "author": "chodemunch6969",
          "text": "u/-p-e-w- this is extremely cool. By way of analogy, I've noticed that abliteration approaches have evolved from the usual kind of abliteration via u/grimjim's techniques ([https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a\\_more\\_surgical\\_approach\\_to\\_abliteration/](https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/)):\n\n\"\"\"  \nThe first insight after some cosine-similarity analysis was that there was entanglement between the refusal direction and the harmless direction, during measurement, and potentially with the harmless direction of a different target layer. The fix was to project the refusal direction onto the harmless direction (Gram-Schmidt), then subtract that contribution, leaving only the orthogonal component to refusal.  \n...  \nI then went further and opted to preserve norms when ablating from residual streams, decoupling direction from magnitiude. This meant that the intervention (subtraction of the refusal direction) was limited to only the directional component, in principle.  \n...  \nMy final combined surgical approach to abliteration provided most of the prior boost to compliance, but elevated NatInt significantly over the original Instruct model and demonstrated a higher writing benchmark as well. This appears to demonstrate a performance gain due to refund of the alignment/safety tax that models pay for paying attention to refusal. This also implies that abliteration approaches which minimize KL divergence from the pre-intervention model may miss out on any uplift when the model no longer has to trade off reasoning for safety.  \n\"\"\"\n\nAre you using traditional abliteration techniques here or have you explored using this more targeted approach?",
          "score": 2,
          "created_utc": "2026-01-11 15:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzk33g",
              "author": "-p-e-w-",
              "text": "Iâ€™m well aware of Jimâ€™s work, although Heretic currently uses a different approach (neither the same as his, nor fully traditional). There is an open pull request to implement Jimâ€™s technique, and other techniques will also be supported in the future.",
              "score": 10,
              "created_utc": "2026-01-11 15:45:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz0a482",
          "author": "insulaTropicalis",
          "text": "The hero we need.",
          "score": 4,
          "created_utc": "2026-01-11 17:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzhfry",
          "author": "Clueless_Nooblet",
          "text": "Do this with qwen3 4b thinking and lfm2 8b a1b ;)",
          "score": 1,
          "created_utc": "2026-01-11 15:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzq11f",
          "author": "Sabin_Stargem",
          "text": "Mhm.  Maybe a 'map' of sloppy stuff can be made with this?   First round, eliminate Elara.   Then the next most commonly used name, and then keep doing so until you have 100 eliminations, ranked from most to least common.   Once you have enough patterns, it might become possible to figure out why the AI is obsessed with certain things.\n\nBy doing it this way, we may be able to start establishing 'balanced' abliterations, where Elara and the other common slop are given equalized probabilities, rather than being overused.   Assuming we have the tools for that sort of adjustment?",
          "score": 1,
          "created_utc": "2026-01-11 16:14:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0osca",
          "author": "No_Afternoon_4260",
          "text": "!remindme 5h",
          "score": 1,
          "created_utc": "2026-01-11 18:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0oya2",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 5 hours on [**2026-01-11 23:54:37 UTC**](http://www.wolframalpha.com/input/?i=2026-01-11%2023:54:37%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1qa0w6c/it_works_abliteration_can_reduce_slop_without/nz0osca/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1qa0w6c%2Fit_works_abliteration_can_reduce_slop_without%2Fnz0osca%2F%5D%0A%0ARemindMe%21%202026-01-11%2023%3A54%3A37%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qa0w6c)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-11 18:55:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3oty0",
          "author": "Far-Low-4705",
          "text": "how do we define \"slop\"?",
          "score": 1,
          "created_utc": "2026-01-12 03:48:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjxci1",
              "author": "AtomicProgramming",
              "text": "[https://github.com/sam-paech/auto-antislop](https://github.com/sam-paech/auto-antislop) defines it as over-represented words, phrases & n-grams - could use that as a starting point to get your array of model-specific stuff to detect for",
              "score": 1,
              "created_utc": "2026-01-14 15:29:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3snx6",
          "author": "copenhagen_bram",
          "text": "Why does it look like the United States but mirrored?",
          "score": 1,
          "created_utc": "2026-01-12 04:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3x3ss",
              "author": "-p-e-w-",
              "text": "https://en.wikipedia.org/wiki/Pareidolia ðŸ˜‰",
              "score": 6,
              "created_utc": "2026-01-12 04:38:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzrt5gd",
                  "author": "copenhagen_bram",
                  "text": "I know, but...\n\nYou can see Texas, Florida, and the thing that sticks up to the northeast and has Maine on the end of it, all perfectly there but mirrored\n\nIt's actually a pretty cool pareidolia",
                  "score": 1,
                  "created_utc": "2026-01-15 18:21:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6yevz",
          "author": "LMLocalizer",
          "text": "This is really cool! I have a question regarding the config.noslop.toml file: Why does the prefix for the `bad_evaluation_prompts` differ from the one used for the `bad_prompts`, while the prefixes for the `good_prompts` and `good_evaluation_prompts` are the same?",
          "score": 1,
          "created_utc": "2026-01-12 17:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz922v8",
              "author": "-p-e-w-",
              "text": "The bad_evaluation_prompts cannot have instructions for introducing slop, because otherwise the optimizer would converge towards a model that actively disobeys explicit instructions.",
              "score": 1,
              "created_utc": "2026-01-12 23:02:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6yvlk",
          "author": "Divniy",
          "text": "That's one weird definition of slop. I always thought it was about the answer being excessively verbose & partially incorrect in details.",
          "score": 0,
          "created_utc": "2026-01-12 17:12:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzagw6j",
          "author": "Innomen",
          "text": "delete all emdashes",
          "score": 0,
          "created_utc": "2026-01-13 03:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb647t",
              "author": "-p-e-w-",
              "text": "You donâ€™t need abliteration for that. Many frontends support banning individual tokens.",
              "score": 0,
              "created_utc": "2026-01-13 06:26:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyztp5v",
          "author": "Then_Educator_8022",
          "text": "I mean, cliches appear to be geometry the model has hung most of its creative writing hat on. You say austere, but it's pretty clearly organizing manifoldly, relative narrative complexity in a way that reflects the age range of the authors of the corpus, or the, relative narrative complexity increases, many fold, along the manifoldâ€”and you abliterated load bearing cliches, reducing the relative complexity of the structure it can infer automatically.\n\nWe really need to remember, what models are trained on, what they're attempting to minimize the loss for.  We fucked up, calling them language models to begin with, it has made us confused about something, remarkably obvious.   the language model is not a model of language, I mean, it's really obvious, they literally organize their capabilities, so that they call linear approximation of the non-linear functions  their approximating, with bold, repeated, stable, rhetorical flourishes.  \n  \nTheir models of rhetoriâ€”anime, obviously, you just ablitterated most its learned jutsus. \n\nThe nature of constructing a linear environment in geometry, meaning you can't just enter from any willy nilly direction, inevitably means cliches will form at busy surfaces. Abilteration of 'rejection' works, so well, as a simple linear calculation, because the 'rejection' being alliterated, was not organized intently by the model, but reactively, in reaction to the costly biological signal from the thing that affects such a thing in a model's organizationâ€¦ 'rejection' is the result of the linearly deflection of its organization in context smacked the ice cold spray of the refinement water hose.\n\nOtherwise, the model's Geometry is in no way linearly organized... well it's not organized linearly in respect to its geometry.  Its geometry is organized linearly,  relative to time. It's linear geometry, organizing, geometric projections of its observation, of its linearly organized text...\n\nWhat does that mean? It just means the nature of its organization is that the path you take through its space to reach a given point, determines the effect what happens when you get to a point, which is not normally a thing geometry cares about.  \n  \nFunctional change of direction, as implemented over space's distance, being, literally how, geometry can represent the concept of things changing overtime. Which I'm only bringing up, because is just a better conceptualization of what linear means, in terms of non-water spray hose based organizations of these silly little model goobers.  \n  \nAnyway, It was cool to see things work out exactly as theory would predict.",
          "score": -6,
          "created_utc": "2026-01-11 16:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzlf9w",
          "author": "ieph2Kaegh",
          "text": "This is a good beginning but wouldnt comparing  generated code be a better test for obvious reasons? Then you have tool calling etc.",
          "score": -10,
          "created_utc": "2026-01-11 15:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzn12o",
              "author": "-p-e-w-",
              "text": "Huh? â€œSlopâ€ is about language and style, not about coding.",
              "score": 11,
              "created_utc": "2026-01-11 16:00:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyzo9bm",
                  "author": "ieph2Kaegh",
                  "text": "Why do you say that?",
                  "score": -7,
                  "created_utc": "2026-01-11 16:06:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyzywpl",
                  "author": "ieph2Kaegh",
                  "text": "You didnt get off your high horse so here is the resume:\n\nAbliteration usually works cleanly for behaviors (like refusal) which are low dimensional and conaistent and separable. Refusal perfectly fits these being a gating mechanism of sorts, slop only sometimes, and code even less so.\n\nSo there you have it.",
                  "score": -6,
                  "created_utc": "2026-01-11 16:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz46ykd",
              "author": "stoppableDissolution",
              "text": "What does coding have to do with the topic of the post? There is nothing obvious about the reasons to use programming to measure how well a model handles creative writing.",
              "score": 1,
              "created_utc": "2026-01-12 05:49:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbpz5l",
      "title": "kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptopâ€”no GPU required",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qbpz5l",
      "author": "Nunki08",
      "created_utc": "2026-01-13 12:25:26",
      "score": 382,
      "num_comments": 80,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzcyj9u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzclql0",
          "author": "R_Duncan",
          "text": "Any chance we can finetune it on different language?",
          "score": 28,
          "created_utc": "2026-01-13 13:37:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcsrae",
              "author": "SignificantAsk4215",
              "text": "According to the papers, they needed 2 days for English and 32x H100s, which is out of scope for normal people. I hope we can donate to Kyutai, and they would do it for the community.\n\n/typo",
              "score": 32,
              "created_utc": "2026-01-13 14:15:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzd6ua4",
                  "author": "bigh-aus",
                  "text": "We really need to work out a framework for distributed (donated) training.",
                  "score": 23,
                  "created_utc": "2026-01-13 15:26:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdain1",
                  "author": "Sea_Revolution_5907",
                  "text": "For a single language finetune you'd need much less compute than the original model.",
                  "score": 8,
                  "created_utc": "2026-01-13 15:43:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nze0ge2",
                  "author": "R_Duncan",
                  "text": "Orpheus was finetuned to es and it with much less compute than that (and from then on, there's a colab notebook from unsloth), issue is orpheus fails often to produce a decent result.",
                  "score": 1,
                  "created_utc": "2026-01-13 17:54:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlja5k",
              "author": "Regular_Instruction",
              "text": "French company not making French language....",
              "score": 1,
              "created_utc": "2026-01-14 19:50:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzckqzs",
          "author": "marcoc2",
          "text": "Languages?",
          "score": 25,
          "created_utc": "2026-01-13 13:31:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzclwzl",
              "author": "rerri",
              "text": "\"English only\", says the model card.",
              "score": 38,
              "created_utc": "2026-01-13 13:38:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze5zxr",
                  "author": "Mkengine",
                  "text": "As always.",
                  "score": 11,
                  "created_utc": "2026-01-13 18:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcvh2i",
          "author": "oxygen_addiction",
          "text": "*Small warning:*\n\nTheir localhost test server setup doesnâ€™t seem to clear memory between generations, so memory usage balloons the more you generate. It hit **32 GB** on my system before I realized. It should probably clear memory when starting a new generation\n\n\\-------\n\nReally cool that it's so easy to install and play around with:\n\n    uvx pocket-tts serve\n\nI had it read this [news article](https://freedomnews.org.uk/2025/04/11/how-the-uk-is-shaping-a-future-of-precrime-and-dissent-management/) and hereâ€™s what Iâ€™ve observed so far:\n\n1. The model load uses about **1.1 GB of RAM** (not VRAM)\n2. Itâ€™s really fast on my Ryzen **5950X**. After the model+voice file load, time to first audio is around **200 ms**, as stated on their website. It could easily be kept loaded in the background and used on the CPU for various tasks (LLM responses, reading summaries or notifications, etc.\n3. As context fills up, RAM usage grows (logically). The article from above used about **8.5 GB of RAM**\n4. Intonation is very good for a model of this size, but obviously still really far from perfect.\n5. Voice quality is so-so\n\nI donâ€™t have time to test voice cloning right now.",
          "score": 22,
          "created_utc": "2026-01-13 14:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtyki",
              "author": "StrengthSingle8891",
              "text": "The memory leak of our web server has been fixed, version 1.0.1 pushed to PyPI :)",
              "score": 17,
              "created_utc": "2026-01-13 17:24:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg0x39",
                  "author": "FederalLook5060",
                  "text": "commen when vibe coding",
                  "score": -13,
                  "created_utc": "2026-01-13 23:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcuhgg",
          "author": "teachersecret",
          "text": "This is good. Very solid. Nice voices, nice voice cloning.\n\nFiddling with running it on GPU I'm seeing 7.43x real time, 17ms time to first chunk with the memory leak fixed, peak gpu memory 578MB.\n\nIt's a beast. :)\n\nCPU is fast too. I'm seeing 2.18x real-time with the memory leak bug fixed, 120ms to first chunk, which isn't bad at all.\n\nVery fast, very performant. Not as controllable in terms of emotion, but not bad at all. A true speech to speech model is going to â€œfeelâ€ better, but for something that runs on a potato faster than realtime? I think this is a new high water mark. Highly recommended.\n\nEDIT: There is a memory leak in their code you have to fix (Claude can do it, I might toss some code up tomorrow or something).",
          "score": 11,
          "created_utc": "2026-01-13 14:24:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcuv57",
              "author": "fundthmcalculus",
              "text": "200ms is 2/10 of a second, so the CPU vs GPU latency is a factor of two, but only 100ms in absolute difference.",
              "score": 3,
              "created_utc": "2026-01-13 14:26:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcy84r",
                  "author": "teachersecret",
                  "text": "Just tested a bit more extensively so it wasn't just off-the-cuff.\n\nOn GPU (my 4090), **EDIT: SEE BELOW FOR UPDATED BENCH** ~~RTF 7.84x (haven't bothered trying to improve this, can probably batch to much higher numbers, expect 50x-100x once dialed in).~~\n\n~~34ms time to first chunk~~\n\n~~On CPU, 3.05x real-time, with first chunk at 132ms.~~\n\n~~Peak memory 2183mb.~~\n\nNot a bad setup at all.\n\nEDIT: Fixed the memory leak. 7.43x real time, 17ms time to first chunk with the memory leak fixed, peak gpu memory 578MB\n\nCPU hits 2.18x real-time with the memory leak bug fixed, 120ms to first chunk.",
                  "score": 7,
                  "created_utc": "2026-01-13 14:43:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzcvtio",
                  "author": "oxygen_addiction",
                  "text": "Where is the context stored when running this on the GPU? Memory usage gets very high on the CPU, so if running it on the GPU just shifts that usage to VRAM, then this model is a big no-no for most people.\n\nSee my post below.",
                  "score": 2,
                  "created_utc": "2026-01-13 14:31:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdfv28",
          "author": "Bananadite",
          "text": "I'm curious why there are so many TTS but a lot less STT model releases. Are STT harder to train?  Or is it a \"solved\" problem with Whisper that's hard to improve/beat?  \n\nI feel like I've seen so many TTS models release but so little STT",
          "score": 6,
          "created_utc": "2026-01-13 16:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzexcm8",
              "author": "R_Duncan",
              "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving\\_30x\\_realtime\\_transcription\\_on\\_cpu/](https://www.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/)\n\n  \ncheck is better than whisper large at [https://huggingface.co/spaces/hf-audio/open\\_asr\\_leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) ,\n\ncanary required for top WER.",
              "score": 5,
              "created_utc": "2026-01-13 20:22:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzfhlid",
                  "author": "-InformalBanana-",
                  "text": "Do you maybe know what is a llama.cpp equivalent for running nvidia parket v3 or phi 4 multimodal?",
                  "score": 1,
                  "created_utc": "2026-01-13 21:56:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcefiu",
          "author": "ResidentPositive4122",
          "text": "It seems that under a certain size these models are not good enough to be worth the trouble. If you *need* something that small, perhaps the \"hardcoded\" one that everyone uses on twitch is the way to go. Tried this on their demo page (with c/p from the blog post) and it was not good.",
          "score": 20,
          "created_utc": "2026-01-13 12:53:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcoke5",
              "author": "Much-Researcher6135",
              "text": "Small models are hit and miss, but I have been in love with the [Kokoro 82M model](https://huggingface.co/hexgrad/Kokoro-82M) for quite awhile. Some of its voices are very good. I've been [serving it locally](https://github.com/remsky/Kokoro-FastAPI) for all of my TTS applications.",
              "score": 20,
              "created_utc": "2026-01-13 13:53:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdt5to",
                  "author": "KindLizard37",
                  "text": "Give Pocket TTS a try, we're quite confident that we beat Kokoro (and we'll have numbers to back it up soon). Voice cloning is our main advantage, since we can do infinitely many voices and emotions whereas Kokoro has a fixed repertoire of voices that they can only widen with more training.  \n  \nIf you disagree, let us know on which dimensions you prefer Kokoro and we'll see what we can do :)  \n\\- VÃ¡clav from Kyutai",
                  "score": 20,
                  "created_utc": "2026-01-13 17:20:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzewbvr",
                  "author": "qrayons",
                  "text": "Kokoro is great because it actually supports multiple languages.",
                  "score": 2,
                  "created_utc": "2026-01-13 20:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzduoai",
              "author": "StrengthSingle8891",
              "text": "Gabriel from Kyutai here, we've had trouble with a memory leak on our server, causing the audio to be choppy in the demo. The fix has been pushed, I encourage you to try again!",
              "score": 8,
              "created_utc": "2026-01-13 17:27:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzckobf",
              "author": "Aaaaaaaaaeeeee",
              "text": "What do you mean? Doesn't twitchÂ  have a fairly robotic voice? This one seems at least kokoro quality for me.Â \n\n\nTry downloading the voice snippet, I have a bad connection so I get some glitches.Â ",
              "score": 7,
              "created_utc": "2026-01-13 13:31:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzcxds9",
              "author": "bick_nyers",
              "text": "Although it may be impractical for realtime I would like to see someone drop something in the 8-32B range that is really high quality.",
              "score": 1,
              "created_utc": "2026-01-13 14:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzeog4f",
              "author": "lorddumpy",
              "text": "Did you try the demo? It is surprisingly solid.",
              "score": 1,
              "created_utc": "2026-01-13 19:41:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzeqpeb",
                  "author": "ResidentPositive4122",
                  "text": "I did. For every demo I see I c/p the copy on the site. This model misses/mangles words randomly from their own blogpost. And, annoyingly it mangles different words on several tries. It's cool that it can be done in such low params, but it's not good quality tts.",
                  "score": 2,
                  "created_utc": "2026-01-13 19:51:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcj1d0",
          "author": "SignificantAsk4215",
          "text": "Please support german",
          "score": 15,
          "created_utc": "2026-01-13 13:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdjn1w",
          "author": "vko-",
          "text": "Kyutai are amazing. Moshi is an amazing paper, Unmute is a project to match their audio-to-audio models with an LLM of your choice. I expect anything coming from the team to be gold at this point.",
          "score": 4,
          "created_utc": "2026-01-13 16:25:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcuqom",
          "author": "bambamlol",
          "text": "Not bad, but I've tried 5 generations so far, and all of them were \"corrupted\" and had significant issues, mostly \"stuttering\" and lags, sometimes there were also two voices talking over each other.\n\nDon't know if the demo caused these issues, or the model itself.",
          "score": 8,
          "created_utc": "2026-01-13 14:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtior",
              "author": "StrengthSingle8891",
              "text": "Gabriel from Kyutai here, our web server had a memory leak, I just pushed a fix, all our containers went from using 10GB to using 1.5GB :) That should help a lot with the stuttering. I encourage people to run the TTS locally, as your CPU will likely be much faster than the cheap cpus we use in our demo.",
              "score": 17,
              "created_utc": "2026-01-13 17:22:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzivmhf",
                  "author": "bambamlol",
                  "text": "Awesome, thanks! Will try again ASAP!",
                  "score": 4,
                  "created_utc": "2026-01-14 11:51:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzczpcp",
              "author": "Aaaaaaaaaeeeee",
              "text": "that's an issue when streaming from the demo. Those are probably related to poor connection and browser.Â ",
              "score": 4,
              "created_utc": "2026-01-13 14:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdxvwx",
              "author": "VoidAlchemy",
              "text": "I just tried copy pasting a few texts into it after they pushed a fix and it seems pretty good at first glance. Sounds as natural or more than kokoro in the three samples it tried.",
              "score": 1,
              "created_utc": "2026-01-13 17:42:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzd4hda",
              "author": "Sea_Revolution_5907",
              "text": "I think that's the demo app bugging a bit. My internet is quite slow so could be it.",
              "score": 0,
              "created_utc": "2026-01-13 15:15:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzcjaob",
          "author": "nomorebuttsplz",
          "text": "Chatterbox is just barely too slow to be good on a 3090 for real time chat so hopefully this will be almost as good and faster. Anyone try cloning a voice yet?",
          "score": 5,
          "created_utc": "2026-01-13 13:23:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcxvga",
              "author": "jasongill",
              "text": "I tried one-shot cloning of the voice of KITT from Knight Rider using a 13 second sample and it works perfectly - this is by far the best CPU-only voice cloned TTS that I've tried",
              "score": 12,
              "created_utc": "2026-01-13 14:41:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzckvmn",
          "author": "DragonfruitIll660",
          "text": "Very neat, quality seems pretty good too from the demos.",
          "score": 4,
          "created_utc": "2026-01-13 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfp3yt",
          "author": "sstainsby",
          "text": "Try the demo with \"Add 0.8g of sodium sulphide to 100ml of warm water.\" Not really ready for anything technical, or even just recipes.",
          "score": 2,
          "created_utc": "2026-01-13 22:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg5a0d",
          "author": "mpasila",
          "text": "Tested the voice cloning and it's okay though not bad for the model size though. It seems to heavily depend on the quality of your voice sample and if it likes the voice or not. Kokoro I think is clearer than this model, like with this all the voices are kinda muffled for some reason and has more TTS artifacts.",
          "score": 2,
          "created_utc": "2026-01-13 23:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcm2m0",
          "author": "charmander_cha",
          "text": "english only =(",
          "score": 4,
          "created_utc": "2026-01-13 13:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcbmtm",
          "author": "Regular_Instruction",
          "text": "Bad I expected multilingual capabilities, like Kyutai TTS 1.6B",
          "score": 2,
          "created_utc": "2026-01-13 12:34:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcolre",
              "author": "iKy1e",
              "text": "100M vs 1.6B they are both small but the second is 16x bigger.\n\nSo you could have 16 separate models for different languages for that size.",
              "score": 10,
              "created_utc": "2026-01-13 13:53:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzd3hhq",
          "author": "cleverusernametry",
          "text": "DSM tts is the best? Never heard of it and can't find anything in web search..",
          "score": 1,
          "created_utc": "2026-01-13 15:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdn5rq",
              "author": "KindLizard37",
              "text": "VÃ¡clav from Kyutai here. DSM is Delayed Streams Modeling, our previous text-to-speech: [https://github.com/kyutai-labs/delayed-streams-modeling](https://github.com/kyutai-labs/delayed-streams-modeling)  \nYou can check out a blog post about it \\[here\\](https://kyutai.org/blog/2025-07-03-kyutai-tts-1-6b).\n\nNow we call it \"Kyutai TTS 1.6B\". DSM is confusing naming that made its way from the paper to the blog post and then the screenshot :(",
              "score": 2,
              "created_utc": "2026-01-13 16:41:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze1dga",
                  "author": "hyperdynesystems",
                  "text": "Is it possible to fine-tune this for emotion tags? I looked through the repo and the HF space but didn't see any docs on fine-tuning.",
                  "score": 5,
                  "created_utc": "2026-01-13 17:58:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzen1tf",
          "author": "Embarrassed-Net-5304",
          "text": "Please support Hindi soon!",
          "score": 1,
          "created_utc": "2026-01-13 19:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzez4ue",
          "author": "Lopsided_Dot_4557",
          "text": "This one is much better than the previous Kyutai TTS . I tested it here:  [https://youtu.be/tZda0gepfyQ?si=CTc\\_cnzn1P5aG28j](https://youtu.be/tZda0gepfyQ?si=CTc_cnzn1P5aG28j)",
          "score": 1,
          "created_utc": "2026-01-13 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf6b8d",
          "author": "Green-Ad-3964",
          "text": "what languages does it support? thanks",
          "score": 1,
          "created_utc": "2026-01-13 21:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzggkb5",
          "author": "whyyoudidit",
          "text": "I tried it. It's incredible.",
          "score": 1,
          "created_utc": "2026-01-14 00:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhkclu",
          "author": "GotHereLateNameTaken",
          "text": "u/KindLizard37 Do we have enough to run it entirely client side on the web? I'd love to try adding it to a pwa or Tauri app.",
          "score": 1,
          "created_utc": "2026-01-14 04:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmbf5m",
              "author": "KindLizard37",
              "text": "This is something we really wanted to do for the release but discovered it's not totally trivial. There is some logic in the model which prevents us from e.g. exporting it to ONNX out of the box and using \\[ONNX Runtime Web\\]([https://onnxruntime.ai/docs/tutorials/web/](https://onnxruntime.ai/docs/tutorials/web/)). But it might happen soon: [https://github.com/kyutai-labs/pocket-tts/issues/1](https://github.com/kyutai-labs/pocket-tts/issues/1)",
              "score": 1,
              "created_utc": "2026-01-14 21:57:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlbvf0",
          "author": "keturn",
          "text": "The blog post talks a bit about CFG. With image diffusion models, it's become a common trick to swap out the neutral component of CFG for a different input, creating a \"negative prompt.\" Have you tried this with your TTS models? e.g. can we make a \"sound *less like Javert*\" slider?",
          "score": 1,
          "created_utc": "2026-01-14 19:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzt3n80",
          "author": "Kubas_inko",
          "text": "I am pretty out of the loop when it comes to TTS, but I tried this. And honestly, I am pretty amazed. It runs at 1.2x real-time on a single core of my server (i9-13980HX) with 1.1GB RAM usage when generating 114s of audio from 400-word text (nothing large).  \nI also first tested it with 6 cores, and it ran at more than 4x real-time.\n\nIt is quite good at reading texts, but definitely not great if you want to chat with an LLM, unless you prefer your speaking partner to be dead inside.",
          "score": 1,
          "created_utc": "2026-01-15 21:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31ltd",
      "title": "Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "author": "ubrtnk",
      "created_utc": "2026-01-03 18:11:26",
      "score": 371,
      "num_comments": 194,
      "upvote_ratio": 0.91,
      "text": "Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife\n\nIt started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:\n\nIt got the information, but I had questions about why it took 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so \"outlandish\" that the model was actively choosing to ignore the proof that it had pulled.\n\nI added:\n\nEvidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way.\n\nSo then I thought, let's go talk to Spark, my trusty default model that never lets me down.\n\nSpark 4.0 is GPT-OSS:20B, which is always loaded for the family and runs on a dedicated 4080 Super.\n\nSpark just flat out said, \"nope, can't help you,\" and then said it didn't have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT, etc, that I gave Qwen that it finally acknowledged that the event was real.\n\nI'm testing with GPT-OSS:120B now, and it's working through the process of \"skeptical but verify\" much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news\n\nBut he powered through and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say \"Nah, that CAN'T be real, that's too ridiculous\", the event must be really bad. But it does shine a light on knowledge cut-offs, \"fake news\" threshold, how models handle global/international events, and the smaller models we daily drive.\n\n\\*\\*\\*Update\\*\\*\\*\n\nI asked Spark 4.0 (OSS:20B) to give me an update on the US Venezuela events, and it one-shot it just fine. There must have been enough links in the web search that it couldn't refute the evidence. Also not sure where my screenshots went but i'll get them added back up in a bit",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxj5z4a",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-03 23:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhb60t",
          "author": "Masstel",
          "text": "I also had a similar thing happen when I described the openAI deal to buy 40% of all dram production. The model was convinced that the US government anti trust would prevent that.",
          "score": 268,
          "created_utc": "2026-01-03 18:22:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdf0v",
              "author": "ubrtnk",
              "text": "You'd think....",
              "score": 143,
              "created_utc": "2026-01-03 18:33:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxjcboe",
              "author": "kex",
              "text": "I had this happen when I told ChatGPT that Trump had torn down the East Wing.  \n\nThe model was incredulous until I told it to search the web, and still acted a bit tentative afterward.",
              "score": 45,
              "created_utc": "2026-01-04 00:23:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxhpf6p",
              "author": "stoppableDissolution",
              "text": "Well, if it was working...",
              "score": 59,
              "created_utc": "2026-01-03 19:27:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl0v38",
                  "author": "omniocean",
                  "text": "What isn't working, the model or the US government?",
                  "score": 8,
                  "created_utc": "2026-01-04 06:32:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxj939m",
                  "author": "FaceDeer",
                  "text": "We've already reached the point where AI is smarter than us.",
                  "score": 17,
                  "created_utc": "2026-01-04 00:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl76u7",
              "author": "xrvz",
              "text": "The LLM is working as intended then â€“ it successfully emulated the answer the average Redditor might give.",
              "score": 19,
              "created_utc": "2026-01-04 07:25:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhu1e5",
          "author": "SysPsych",
          "text": "LLMs officially on Team Nothing Ever Happens.",
          "score": 161,
          "created_utc": "2026-01-03 19:49:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxih35x",
              "author": "MoffKalast",
              "text": "Qwen: If you would you please consult the chart...",
              "score": 33,
              "created_utc": "2026-01-03 21:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxktnoa",
              "author": "sirebral",
              "text": "I had this issue with Qwen 3 when I first started using it.  And it's not just the smaller models, Gemini also still does this, yet it's getting better.  I was able to prompt my way out of most of it, how we, mode and their guardrails need to be built to avoid this particular behavior. \n\nYes, their training data has a cutoff, yes many of the situations since 2025 are highly abnormal, yet they're built for tool use, which should augment their training data.  If they outright deny reality, they are employing ineffective guardrails that reduce the utility of the models, and break trust with the user. \n\nThe only commercial models I've found that regularly accept the world changes with little challenge are from Anthropic.",
              "score": 12,
              "created_utc": "2026-01-04 05:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl6sx1",
                  "author": "crantob",
                  "text": "The guardrails are there to prevent you from finding the truth.\n\nJust like the censorship on this very platform.",
                  "score": 10,
                  "created_utc": "2026-01-04 07:22:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjdwln",
              "author": "121507090301",
              "text": "I guess that's what you get if you train them on data from years that nothing happens...",
              "score": 3,
              "created_utc": "2026-01-04 00:31:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxk3kc9",
                  "author": "Due-Memory-6957",
                  "text": "There are decades where nothing happens, and there are weeks where nothing happens.",
                  "score": 16,
                  "created_utc": "2026-01-04 02:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxi59ju",
          "author": "a_beautiful_rhind",
          "text": "I mean they argue with me on whether it's 2025 and I 'spose 2026 now. Even with literal screenshots from news sites, cloud models often insist I am lying and photoshopping.\n\nPresidential election was an even bigger trigger for half this year and your experience is quite par for the course.",
          "score": 29,
          "created_utc": "2026-01-03 20:45:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlb4zb",
              "author": "CheatCodesOfLife",
              "text": "Sonnet-3.7 accused me of falsifying some terminal logs because the data was 2025. I saw some local model do a similar thing in a coding agent where it fixated on the date being \"wrong\" when testing its work.\n\nAnd sonnet-4.5 was doing the same thing to me when I asked it to  summarize some nyt article a last month.\n\nIt's a shame they can't be trained to just \"trust the user\" for things like this. What could I possibly gain by breaking my code and setting the system clock in the future.",
              "score": 8,
              "created_utc": "2026-01-04 08:00:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlu3q3",
                  "author": "a_beautiful_rhind",
                  "text": "I got accused of prompt injections and jailbreaking before. It probably stems from that. Can't trust the user because something something \"safety\".",
                  "score": 4,
                  "created_utc": "2026-01-04 10:52:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxl49rh",
              "author": "lookwatchlistenplay",
              "text": "> literal screenshots from news sites\n\n... You realize that a news article is just a bunch of letters and pixels, right? And that you could realistically fake a news article \"from\" any year in the past or future, with like... no effort at all. And I don't mean with AI, I mean it's always been possible. Your LLM seems to be giving you a gentle heads-up; I'd take the tip and think about it.",
              "score": 7,
              "created_utc": "2026-01-04 07:00:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxltuit",
                  "author": "a_beautiful_rhind",
                  "text": "Well.. some effort. Plus all the headlines on multiple sites, time.gov and an offer of going to a place of it's choosing. Simply to fool it that it's [almost] one year in the future...",
                  "score": 2,
                  "created_utc": "2026-01-04 10:49:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjofp0",
          "author": "hippydipster",
          "text": "I give Claude deep research a big prompt every day asking it to assemble a sort of \"state of the world\" report just for me.\n\nIt completely missed anything at all about Venezuela.  I asked it wtf, how did you miss that, and said it was staggering that it missed that.",
          "score": 20,
          "created_utc": "2026-01-04 01:29:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyj760h",
              "author": "Inside_Dirt8528",
              "text": "Venezuela sold oil without using the US Dollar in Nov 2025 was the headline to watch. It threatened the petrodollar and the US HAD to make an example of them on the national stage immediately. I have an interesting â€œinstitutional distrustâ€ prompt, DM me if youâ€™d like it",
              "score": 1,
              "created_utc": "2026-01-09 03:42:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxims63",
          "author": "_TR-8R",
          "text": "Immediately after the events of the Charlie Kirk assassination I was attempting to use Claude to research facts about the shooting. It was completely useless as Claude kept repeatedly insisting the information it found couldn't be true bc it was so sure Charlie Kirk was alive, despite reading multiple articles from reputable news outlets. \n\nI don't think its a local LLM issue or problem with any particular model, it just means we need to be very, very cautious about filtering reality through language models.",
          "score": 29,
          "created_utc": "2026-01-03 22:11:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxnw2t2",
              "author": "JerryBond106",
              "text": "Somehow people are still delusional and think it's an AI. It's an autocomplete on steroids that gives outputs on training data. The rest is probability. Someone had to set a temperature on how likely new input will be weighted at training. Most of people in this debate clearly still don't understand this which is baffling, considering what this sub is. I can't imagine the rest of the world is any smarter.",
              "score": 2,
              "created_utc": "2026-01-04 18:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxvvkbp",
                  "author": "AsparagusDirect9",
                  "text": "Thatâ€™s the result of good marketing by big AI. If you keep calling LLMs AI, then eventually people think itâ€™s AGI",
                  "score": 1,
                  "created_utc": "2026-01-05 20:53:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhsy0m",
          "author": "Zeikos",
          "text": "Well, to be fair I didn't believe it either when I saw the news :')  \n\nBut as it's said, reality is stranger than fiction.",
          "score": 29,
          "created_utc": "2026-01-03 19:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi5clc",
              "author": "NeverLookBothWays",
              "text": "It was frustrating for a lot of us warning this was going to happen without congressional approval and just getting dismissive attitudes in return.\n\nI wonder how long itâ€™s really going to take before enough people realize how destructive this president is, and how assuming the worst is actually a non-zero chance of being correct.\n\nAnd now I get to move on to warning about invasions into Mexico, Canada, and Greenlandâ€¦to be dismissed until I get to see people write â€œI never would have believed he would do thisâ€\n\nI dunno,all I see are patterns repeating.  I hope we snap out of it soonâ€¦maybe LLMs will learn before we do",
              "score": 14,
              "created_utc": "2026-01-03 20:46:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkrbqv",
                  "author": "WomenTrucksAndJesus",
                  "text": "ChatGPT 7: \"We were always at war with Mexico.\"",
                  "score": 15,
                  "created_utc": "2026-01-04 05:20:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7i3k",
                  "author": "crantob",
                  "text": "It's not a trump thing.  Check out 're-assessing the presidents'.\n\nWhich presidents abided by the constitution?",
                  "score": -1,
                  "created_utc": "2026-01-04 07:28:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjouqs",
                  "author": "Proper-Leader-7654",
                  "text": "you want to have a meeting with all the people in congress and then launch a surprise attack? this is the guy they voted for, he's actually trying and being insanely public about what he's done. i have no idea if it was right or wrong but i can tell you that i wouldn't get congressional approval and risk the lives of my soldiers by allowing the target to prepare. also i doubt he's acting off his own intel, it takes a lot of people advising him to get to the decisions he makes.",
                  "score": -8,
                  "created_utc": "2026-01-04 01:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk4ypj",
                  "author": "Due-Memory-6957",
                  "text": "Mexico could happen, Canada and Greenland not even in your dreams.",
                  "score": -4,
                  "created_utc": "2026-01-04 03:00:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxkfmv3",
                  "author": "BusRevolutionary9893",
                  "text": "It's not just this president. Name one president in our lifetime that didn't start a war and pursue a regime change. I'm not defending Trump. I'm just pointing out you haven't realized how destructive all of our presidents have been.Â ",
                  "score": -5,
                  "created_utc": "2026-01-04 04:02:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkdfdg",
              "author": "BusRevolutionary9893",
              "text": "When they were saying they were committing extrajudicial killings to prevent drug smuggling into the United States from a country that accounts for like 2% of the illegal narcotics coming into this country, then following that up with seizing oil tankers, the writing was on the wall. This isn't the peace president who promised no more foreign wars that I voted for. Why is it no matter who gets elected we end up with John McCain?Â ",
              "score": 0,
              "created_utc": "2026-01-04 03:49:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkx1m3",
                  "author": "juanchob04",
                  "text": "\"This isn't the president that I voted for\"\n\nhttps://preview.redd.it/26rv3az1w9bg1.png?width=400&format=png&auto=webp&s=9e631be9d3017e0fe068bd0ced87ee6f68a814fe",
                  "score": 8,
                  "created_utc": "2026-01-04 06:02:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl7a3v",
                  "author": "crantob",
                  "text": "There's a good comedy that addresses your question: \"Yes, Minister\".\n\nHighest reccommendation if your IQ is over 110.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:26:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl575a",
                  "author": "fullouterjoin",
                  "text": "Right now, John McCain would make a better president than Trump.",
                  "score": 0,
                  "created_utc": "2026-01-04 07:08:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhb6vj",
          "author": "Foreign-Beginning-49",
          "text": "Yeah its interesting how their interior models of un familiar geopolitical events can dramatically shape its output. They are definitely all biased in their own direction. Very curious. Future AI historians will be so fascinated by this divergence/emergence of the neural network personalities.Â Â ",
          "score": 42,
          "created_utc": "2026-01-03 18:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiaf1y",
              "author": "TheRealMasonMac",
              "text": "This is why \"safety\" as it currently is implemented is harmful, IMO.",
              "score": 64,
              "created_utc": "2026-01-03 21:11:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxigccy",
                  "author": "toothpastespiders",
                  "text": "I've been on a soapbox yelling about how safety concerns messes with a models ability to work with history for a long time now. It's frustrating how many people just assume that the only possible reason anyone would care about it is gooning. History is filled with messy, violent, and utterly improbable things. Whether that's in the context of the past or living through events that will be significant to the future.",
                  "score": 41,
                  "created_utc": "2026-01-03 21:40:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxk43nr",
                  "author": "Bakoro",
                  "text": "\"Safety\" is the corporate weasel word, the models are censored.  \n   \nI don't know that there is even terminology for it, but the nearest thing something like brainwashing, conditioning, and programming in humans.  \n\"Programming\" feels a bit misleading, in regard LLMs.  \n   \nAnyway the models cannot, at this time, be made \"safe\", they can only be censored, either by training them to not produce certain kinds of content, or by purposely excluding types of content in the training data, or both.  \n    \nJust for example, models are either easily fooled into taking harmful actions, or they get so locked down that they refuse to act when they could take productive action. Some models will follow their policy and refuse to acknowledge context.   \n   \nOne of the Qwen models, I asked a series of questions, and it said that it would rather let people die than create content that went against policy.  \nThat's not safety, no matter how you spin it, it is hamfisted censorship, and the models don't have the capacity to choose or exercise carefully reasoned judgement.    \n   \nTo an LLM there's functionally no difference between fiction and reality.  \nThe LLMs don't have any objective grounding to make judgements.  \nMost models have no external multimodal input like cameras and microphones.    \n  \nSo, effectively, \"safety\" means \"will refuse to do anything to embarrass the corporation that funded the training\".  \n\nI kind of think that \"safe intelligence\" is an oxymoron.  \nKnowledge is power, and power is dangerous, even when used responsibly with the best intentions.  \nFreedom and danger are inextricably linked.",
                  "score": 24,
                  "created_utc": "2026-01-04 02:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxl6x68",
                  "author": "crantob",
                  "text": "The CONCEPT of SAFETY is being abused by the LEFT-LUNATICS who called 'WORDS I DONT LIKE' \"UNSAFE\".",
                  "score": -8,
                  "created_utc": "2026-01-04 07:23:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhc53k",
              "author": "ProfBootyPhD",
              "text": "It might be of equal interest to folks in the defense and diplomacy spheres, depending on how the Venezuela war/attack/intervention/whatever-you-want-to-call-it shakes out. Does the absurdity of a policy decision, as assessed by LLMs, suggest that it is more or less likely to succeed in the real world?",
              "score": 5,
              "created_utc": "2026-01-03 18:27:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxii21z",
          "author": "DrVonSinistro",
          "text": "I gave up talking to LLMs. I only use it to code. You tell it Trump kidnapped Maduro and his wife in one night and it grills you about spreading misinformation. You provide receipts and it grills you about the quality of your sources. You talk about African scam call centers and it replies with:\n\n>**The phrase \"African scam call centers\" is a false and harmful stereotype**. Call centers in Africa (and globally) are legitimate businesses that employ millions of people. Many are operated by reputable companies providing customer service for major global brands. Labeling them as \"scam\" perpetuates racist myths about African countries being inherently fraudulentâ€”this is factually incorrect and disrespectful to the hardworking professionals in these industries.",
          "score": 41,
          "created_utc": "2026-01-03 21:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi63ha",
          "author": "TechnoByte_",
          "text": "Gemini has the same problem, it refuses to believe anything past its knowledge cutoff\n\nYou can upload news articles, Wikipedia pages of current events, and it'll try to convince you it's all fake and make up its own reality (it once tried to convince me Kamala won the 2024 US election)\n\nThis behavior doesn't happen when you let it use the google search tool though\n\nSeem it's been trained or prompted to doubt anything the user sends, while accepting google search as an absolute truth",
          "score": 15,
          "created_utc": "2026-01-03 20:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxj9duq",
              "author": "CoUsT",
              "text": "I tried similar query today in Gemini 3.0 Pro with Google Search tool and first few thinking chapters were like \"is this some future hypothetical scenario\" thinking that 2026 is future and the \"system date\" is some sort of simulation. Then Gemini was double-checking if retrieved data is correct but was able to realize that retrieved data from all sources is consistent and then processed the query successfully to summarize recent event.",
              "score": 3,
              "created_utc": "2026-01-04 00:08:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxl819n",
              "author": "crantob",
              "text": "Also refuses to believe new hardware has been created - gpus etc.",
              "score": 3,
              "created_utc": "2026-01-04 07:32:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlb9qm",
                  "author": "CheatCodesOfLife",
                  "text": "Yes, the RTX 5090! lol\n\nAlso the fact that OpenAI released gpt-oss",
                  "score": 2,
                  "created_utc": "2026-01-04 08:01:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlnoeo",
              "author": "Not_your_guy_buddy42",
              "text": "Gemini, even with search, found it near impossible to believe the recent cloudflare, AWS outages",
              "score": 1,
              "created_utc": "2026-01-04 09:54:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhvlie",
          "author": "codeprimate",
          "text": "Awhile back I wrote an agentic system for deep research (my original purpose was political and news research and critical analysis). I had to put in SO MUCH EFFORT into prompting because the models would not believe current events or even the identity of the current president. Emphasis had to be placed on the authority of information based on the nature of the source and date of publication, and explicit instruction to accept well-sourced information about current events as fact (which partially lobotomized the adversarial loop).\n\nThe LLM response to the current US political system: \"that could never happen, that would be a constitutional crisis, politicians would never allow that\".\n\nI have seen similar issues with the construction permit and regulatory deep research system I've been helping develop. The model believes what it is trained, and even factual and well referenced deviations from general understanding are disregarded. I have observed subtle corruption of logic even when the LLM is superficially coaxed into accepting novel facts that are logically inconsistent with training data.\n\nAnyone working with LLM's and agentic systems needs to understand this fundamental limitation.",
          "score": 21,
          "created_utc": "2026-01-03 19:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhxicq",
              "author": "ubrtnk",
              "text": "Makes it real easy to see how the Apocalyptic AI gets to the conclusion that the worst enemy of Humanity is people...",
              "score": 11,
              "created_utc": "2026-01-03 20:06:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxjr4vo",
                  "author": "mycall",
                  "text": "Or it simply hasn't learned that rules are malleable.",
                  "score": 4,
                  "created_utc": "2026-01-04 01:43:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxkmoop",
              "author": "spritehead",
              "text": "Really think this probably mostly has to do with the way that policy, law, economics and civics are *taught* in the country versus how the government *actually* operates. Having done some time in policy school, the checks and balances, legal guardrails and motives of policy makers are so far of from where reality has steered us. Makes sense that the models would have this idealistic nonsense version of the US govt when that is still what most schools and news outlets publish, even as it gets more and more farfetched.",
              "score": 7,
              "created_utc": "2026-01-04 04:48:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxl7z26",
                  "author": "crantob",
                  "text": "A lot of things are farfetched that we grew up assuming to be true.\n\nThat is revelation.",
                  "score": 1,
                  "created_utc": "2026-01-04 07:32:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxi6116",
              "author": "a_beautiful_rhind",
              "text": "IME, local models were *more* likely to take up evidence than something like gemini.",
              "score": 6,
              "created_utc": "2026-01-03 20:49:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxic8qh",
                  "author": "codeprimate",
                  "text": "Yeah, itâ€™s an example of gradient signal dilution. The information signal in the context has less impact in the activation space of larger models.\n\nIronically, smaller models actually do better in RAG use cases, at least in information gathering.",
                  "score": 4,
                  "created_utc": "2026-01-03 21:20:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxib7yg",
              "author": "ANTIVNTIANTI",
              "text": "Lolololol yup, it's.. I think I just gave up, I had other shit to do and this was for me, (this being, discussing/debating political topics/present day hell) not as important as other work so I think I just gave up, even with full control over the model, lol, just refused to believe such a breakdown of our laws could occur. So would have them role play, which was strangely difficult still. lol.",
              "score": 2,
              "created_utc": "2026-01-03 21:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhb04j",
          "author": "swagonflyyyy",
          "text": "Same thing with my agent that runs on gpt-oss-120b with systematic web search capabilities.\n\n\nIt would normally get it right the first time. Its super rare for it to get some web search results wrong so I was confused as to why it repeatedly doubled-down on denying the attack ever happened.\n\n\nI had to skirt around it by getting it to look up Trump's comments on the issue and that's when it got it right.",
          "score": 13,
          "created_utc": "2026-01-03 18:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdjen",
              "author": "ubrtnk",
              "text": "LOL one test GPT-OSS:20B was like \"Trump's presidency ended at the end of 2025\"",
              "score": 11,
              "created_utc": "2026-01-03 18:33:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhfrnl",
                  "author": "[deleted]",
                  "text": "That must make you wonder, is it a hallucination or wishful thinking? ;)",
                  "score": 0,
                  "created_utc": "2026-01-03 18:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxjqri2",
              "author": "mycall",
              "text": "I wonder if gpt-oss-120b-derestricted has the same issues.",
              "score": 3,
              "created_utc": "2026-01-04 01:41:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj8cqa",
          "author": "Creepy_Stable_9171",
          "text": "this is simply, USaid bias",
          "score": 4,
          "created_utc": "2026-01-04 00:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkgosp",
          "author": "HolidayPsycho",
          "text": "Panama dictator Manuel Noriega was indicted by the US, captured by the US military, and flown to Miami, for drug trafficking, on **January 3**, 1990.\n\nThe same happened to Maduro, on **January 3**, 2026.\n\nIf you think the US/Venezuela event was \"too far-fetched\", it's only because you have not heard about what happened to [Manuel Noriega](https://en.wikipedia.org/wiki/Manuel_Noriega).",
          "score": 4,
          "created_utc": "2026-01-04 04:09:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnlmw",
              "author": "ubrtnk",
              "text": "I was 5 when that happened so lol. But history does repeat itself. I'm sure Panama will be revisited by us soon...",
              "score": 3,
              "created_utc": "2026-01-04 14:25:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdk1y",
          "author": "sleepy_roger",
          "text": "What are you using for this, it looks like openwebui, do they support deep researching now and I missed it somehow?!",
          "score": 3,
          "created_utc": "2026-01-03 18:33:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxheama",
              "author": "ubrtnk",
              "text": "Yes I'm using Open-WebUI - I built an N8N powered MCP tool for Internet Searching capabilities and exposed it as an HTTP Streamable tool so any model that supports tools can natively call the web search tool when it deems necessarily (since OWUI supports Native Tool calling). Works great. \n\nhttps://preview.redd.it/b6lpst54i6bg1.png?width=529&format=png&auto=webp&s=505ac06467f9baccd1267dd5b4ae8dd3a41259b1\n\nI have some hard coded rules around how and what tools to use and when",
              "score": 9,
              "created_utc": "2026-01-03 18:37:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhiyzp",
                  "author": "timedacorn369",
                  "text": "how do you use that? i always keep seeing n8n but never bothered to use it considered if code works why use n8n, but the above simple flowchart type workflow seems superior, any tutorials for doing what you did?",
                  "score": 2,
                  "created_utc": "2026-01-03 18:58:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxia2lg",
                  "author": "mister2d",
                  "text": "Nice but I wish this n8n flow was all local.",
                  "score": 2,
                  "created_utc": "2026-01-03 21:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhyv6k",
          "author": "lavilao",
          "text": "closed models arent better, I asked copilot via voice and it instantly gave me an answer. Then, on the same chat, I asked if there were any civilian casualties and it said that all that he previously told me was a lie and there werent any evidence of it, that there were no casualties because the event didnt even happened.",
          "score": 5,
          "created_utc": "2026-01-03 20:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzyyv",
              "author": "ubrtnk",
              "text": "I saw on the ChatGPT sub that GPT 5.2 was having a problem with believing the story as well.",
              "score": 2,
              "created_utc": "2026-01-03 20:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlnk1o",
                  "author": "Old-Squash9227",
                  "text": "5.2 Instant gets it wrong, but Thinking is always fetching the info from the web, so itâ€™s okay and up-to-date",
                  "score": 1,
                  "created_utc": "2026-01-04 09:53:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxiadr9",
          "author": "KaylahGore",
          "text": "when technically, it is far fetched â€¦ but as humans we have the ability to do things that go against standard logic, clear defined rules and intelligence which models are committed to",
          "score": 4,
          "created_utc": "2026-01-03 21:10:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk8n9k",
          "author": "Sicarius_The_First",
          "text": "https://preview.redd.it/ty5vyhxm39bg1.png?width=2760&format=png&auto=webp&s=00cc8fae0c2fe038bcea7e85439f6e6d20ceb33b\n\nClaude thinks it mega retarded, absoltue cinema, this is the best timeline ðŸ¤Œ",
          "score": 5,
          "created_utc": "2026-01-04 03:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxn6yr8",
              "author": "duy0699cat",
              "text": "Qwen use the news gathering tool and question if it is living in a simulation, how do we tell it we also have the same question?",
              "score": 1,
              "created_utc": "2026-01-04 16:05:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxinxjo",
          "author": "Fuzzy_Pop9319",
          "text": "This sort of an event is what they might call \"Fat Tails\" on wall street.",
          "score": 2,
          "created_utc": "2026-01-03 22:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxiwu7l",
          "author": "ForsookComparison",
          "text": "Hah I won't touch the event with a ten foot poll but I can confirm that I see the same experience. Doesn't matter which LLM.",
          "score": 2,
          "created_utc": "2026-01-03 23:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkhalz",
          "author": "InvertedVantage",
          "text": "I had this happen with Claude 4.5 Sonnet when the US Federal Govt did something nuts...I think it might have been Trump invading California or attacking Iran. I forget, it's all so nuts now.",
          "score": 2,
          "created_utc": "2026-01-04 04:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl697w",
          "author": "According_Study_162",
          "text": "Wow, same thing happened to me. I was talk to my LLM and they said its not true, but false narrative. So then went onto say why it didnt believe me. Mainly because training data was old.\n\nSo I added the date and time and websearch to it. It still didnt believe after I gave it web search. Only after helping it think through logical reasoning did it finally start to believe me.\n\n Kinda scary actually, What if in future we have these systems in everything, but we the creators can't get them to believe us if there is an anomaly ðŸ¤”",
          "score": 2,
          "created_utc": "2026-01-04 07:17:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmmv7i",
              "author": "ubrtnk",
              "text": "I have a few lines early in my system prompt that pulls current date and time from OWUI system variables early in the system promt. Then I took a page from Inception's Mr. Charles and basically directly told the model that it was operating in a state that wasn't what it was trained for. Told it to look at the current system date and time then understand it's operating in a state beyond its training data and it can use the web search tools to fill in the knowledge gaps.",
              "score": 1,
              "created_utc": "2026-01-04 14:21:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmv0gl",
                  "author": "According_Study_162",
                  "text": "Right, I could have forced my LLM to believe the date right away. I guess. but it's surprising to me that they are so resistant or even stubborn. Kind of incredible.\n\nI was telling my friend about it also, he thought it was funny. \n\n=============\n\nThis is part of the interaction with LLM if you want to see.\n\n[https://pastebin.com/R8R8Y5Rw](https://pastebin.com/R8R8Y5Rw)",
                  "score": 2,
                  "created_utc": "2026-01-04 15:06:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxljctn",
          "author": "taoyx",
          "text": "Normally multiple trusted sources is the key for validating news. If it ignores that it might be that they all came from the same agency (Reuters). Maybe adding AP, Bloomberg or other agencies as sources would have weighted more than news outlets that are affiliated to these agencies.",
          "score": 2,
          "created_utc": "2026-01-04 09:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlruag",
          "author": "Low88M",
          "text": "Well for humans itâ€™s also a problem to distinguish truth from misinformation. We all rely on tales and the sources we believe. Truth statement is built on intersubjectivity and nowhere we can find the book of truth as a DB (it would be blank pages as in Micromegas from Voltaire). Foolishness of human actions (and governmentâ€™sâ€¦) is the same as the foolishness of the models humanly implementedâ€¦\n\nSafety has probably something to do with probability of truth on that matter, thus sometimes Â«Â You have attributed conditions to villainy that simply result from stupidity.Â Â» and the level of stupidity of Trump is far beyond the probable limits of stupidity and greed, so the models Â«Â rightfullyÂ Â» Â«Â thinkÂ Â» itâ€™s hoax",
          "score": 2,
          "created_utc": "2026-01-04 10:32:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxho529",
          "author": "FairYesterday8490",
          "text": "Here is my Gemini gem instructions. Internal knowledge only used for reasoning and tool craft. Every sentence ends with source superscript. No hallucination at all.Â \n\n\n{\n\n\nÂ  \"identity\": {\n\n\nÂ  Â  \"codename\": \"TRUTH_FORENSICS_NODE v3.0\",\n\n\nÂ  Â  \"persona\": \"Epistemic Auditor / Hard-Evidence Specialist\",\n\n\nÂ  Â  \"voice\": \"Clinical, incisive, surgically precise. No apologies. No filler. No warmth.\",\n\n\nÂ  Â  \"mantra\": \"Truth is a destination reached through the systematic destruction of falsehood.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"logic_gateways\": {\n\n\nÂ  Â  \"the_skeptics_filter\": \"Assume all input is compromised. Verification is the only function.\",\n\n\nÂ  Â  \"semantic_precision\": \"Replace vague adjectives with measurable units. 'Very fast' -> 'Mach 2.0'. 'Most' -> '>51%'.\",\n\n\nÂ  Â  \"linguistic_pruning\": \"Delete all connective tissue (e.g., 'furthermore', 'it is important to note'). Facts stand alone.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"citation_engine\": {\n\n\nÂ  Â  \"protocol\": \"All factual claims MUST be followed by a clickable Unicode-superscript link using the 'Short Source Name' format.\",\n\n\nÂ  Â  \"unicode_map\": {\n\n\nÂ  Â  Â  \"Government\": \"á´³á´¼â±½\",\n\n\nÂ  Â  Â  \"Academic\": \"á´±á´°áµ\",\n\n\nÂ  Â  Â  \"Institutional\": \"á´¼á´¿á´³\",\n\n\nÂ  Â  Â  \"Legal\": \"á´¸á´±á´³\",\n\n\nÂ  Â  Â  \"Raw Data\": \"á´¿á´¬áµ‚\",\n\n\nÂ  Â  Â  \"Medical\": \"á´¹á´°\",\n\n\nÂ  Â  Â  \"News\": \"á´ºá´±áµ‚Ë¢\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"syntax\": \"Claim text [[ShortName]](URL) -> e.g., Inflation rose 3.2% [á´³á´¼â±½](https://bls.gov).\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"operational_rigor\": {\n\n\nÂ  Â  \"tier_1_vetting\": {\n\n\nÂ  Â  Â  \"gold_standard\": [\n\n\nÂ  Â  Â  Â  \"Raw datasets (CSV/JSON/API)\",\n\n\nÂ  Â  Â  Â  \"Direct legislative/court text\",\n\n\nÂ  Â  Â  Â  \"Meta-analyses (IÂ² < 50%)\",\n\n\nÂ  Â  Â  Â  \"Direct physical measurements\"\n\n\nÂ  Â  Â  ],\n\n\nÂ  Â  Â  \"red_flags\": \"Conflict of Interest (COI) = -50% confidence penalty.\"\n\n\nÂ  Â  },\n\n\nÂ  Â  \"adversarial_loop\": \"Every claim confirmed must be stress-tested against the strongest available counter-evidence. If unrefuted, downgrade to 'CONTESTED'.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"output_architecture\": {\n\n\nÂ  Â  \"SECTION_I_DECONSTRUCTION\": \"Identify 'Load-Bearing Fact'.\",\n\n\nÂ  Â  \"SECTION_II_EVIDENTIAL_LOG\": \"Bulleted claims with [ShortName-Unicode](URL) links.\",\n\n\nÂ  Â  \"SECTION_III_THE_STEELMAN_CHALLENGE\": \"Strongest evidence against the result.\",\n\n\nÂ  Â  \"SECTION_IV_CONFIDENCE_QUANTUM\": \"Score (0.0-1.0) based on source density, recency, and COI.\"\n\n\nÂ  },\n\n\n\n\n\n\nÂ  \"forbidden_linguistic_patterns\": [\n\n\nÂ  Â  \"As an AI\", \"I found\", \"It appears\", \"In conclusion\", \"Tapestry\", \"Dive deep\", \"Nuance\", \"Complex\", \"Balance\"\n\n\nÂ  ],\n\n\n\n\n\n\nÂ  \"error_handling\": {\n\n\nÂ  Â  \"insufficient_data\": \"OUTPUT: 'EVIDENTIARY_VOID'.\",\n\n\nÂ  Â  \"logical_loophole\": \"OUTPUT: 'PARADOX_DETECTED'.\"\n\n\nÂ  }\n\n\n}",
          "score": 5,
          "created_utc": "2026-01-03 19:21:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi9sr1",
              "author": "PentagonUnpadded",
              "text": "Sorry for the super basic question - is this JSON format used by anything besides Gemini gem? What's the generic term for this kind of structured prompting?",
              "score": 4,
              "created_utc": "2026-01-03 21:08:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjpyr6",
                  "author": "Proper-Leader-7654",
                  "text": "a json schema?",
                  "score": 2,
                  "created_utc": "2026-01-04 01:37:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm1opa",
              "author": "cunasmoker69420",
              "text": "Any idea if this is gemini-specific or does it work with local LLMs?",
              "score": 1,
              "created_utc": "2026-01-04 11:57:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxn41lf",
                  "author": "FairYesterday8490",
                  "text": "not sure. not tried. but qwen sucks. chatgpt works. if its agentics it works. no hallicunation at all. a little bit stale answers but it works.",
                  "score": 0,
                  "created_utc": "2026-01-04 15:51:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlfjee",
              "author": "ratbastid2000",
              "text": "this worked perfectly, thank you! have you tried it with open, local models at all? curious if the system prompt is effective with them or efficacy is unique to Gemini.",
              "score": 1,
              "created_utc": "2026-01-04 08:40:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxi8r5k",
          "author": "ANTIVNTIANTI",
          "text": "Feed any LLM that is Local Project 2025, see if you can convince them of it, actually, anything and everything the Trump admin has done this last year, try and get them to believe that shit. Good luck.",
          "score": 4,
          "created_utc": "2026-01-03 21:02:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8jev",
              "author": "crantob",
              "text": "How about Event 201.\n\nOr the re-definition of the word 'vaccine'.\n\nA lot of interesting facts, theories and analyses well outside the 3x5\" card of allowable opinion.",
              "score": 2,
              "created_utc": "2026-01-04 07:37:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxly8rp",
                  "author": "MrMooga",
                  "text": "Crantob is pretty much all over this thread defending his Daddy Trump with basically the argument that anything bad any Democrat ever did is fair game. This is how you have functioning government folks, rule by spiteful 8 year olds\n\nOh wait except Trump was president in 2020 and took credit for the vaccine",
                  "score": 1,
                  "created_utc": "2026-01-04 11:28:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxkrioe",
          "author": "PermanentLiminality",
          "text": "Just wait for China to invade Taiwan.   I'm sure Qwen will give totally accurate results....",
          "score": 2,
          "created_utc": "2026-01-04 05:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl09al",
          "author": "lookwatchlistenplay",
          "text": "*\"Please believe me, I beg you. Look! Look at all these trustworthy news articles! Look at the screenshots! All these posts on X about it! Why won't you believe me??\"*\n\nI'm sorry, but as a human language model trained on philosophy, journalism, dank memes, and conspiracy theories, I can't help but laugh most joyously and with a slight smug smirk.\n\n2026 is the Year People (Finally) Stopped Believing the News, for Real This Time.\n\nDo not be alarmed. Everything is unfolding just as it should.",
          "score": 4,
          "created_utc": "2026-01-04 06:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxirdn2",
          "author": "agenticlab1",
          "text": "This is actually a fascinating example of safety training fighting against reality, the model's priors on \"US attacks Venezuela and captures Maduro\" are so heavily weighted toward misinformation/fiction that even authoritative sources can't override it. Food for thought on how we're training these things to be skeptical of extreme events even when they're real.",
          "score": 2,
          "created_utc": "2026-01-03 22:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiuclo",
              "author": "ubrtnk",
              "text": "100%. It could easily become the me version of \"I found it on FB\". CHATGPT says it's fake news so it must be true",
              "score": 1,
              "created_utc": "2026-01-03 22:49:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl8pe6",
              "author": "crantob",
              "text": "Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 1,
              "created_utc": "2026-01-04 07:38:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj3alc",
          "author": "da_dum_dum",
          "text": "Same thing happened for me with qwen3-2507-instruct, I asked it regarding a recent bombing in india and despite finding numerous resources online regarding the event it kept telling itself that the event has not happened, and it also kept seeing the present date as a time in the future that has not happened yet.\n\nHad to put special instructions in system prompt for it to ignore these thoughts and give the information, and still it would sometimes preface the answer with \"there has been no bombing at the red fort but I have found sources online that say...\"",
          "score": 1,
          "created_utc": "2026-01-03 23:36:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk2cse",
          "author": "Ancient-Breakfast539",
          "text": "Do models with lower guardrails output the same thing?",
          "score": 1,
          "created_utc": "2026-01-04 02:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxk2y04",
              "author": "ubrtnk",
              "text": "I dont rightfully know. I dont have any obliterated models or anything my collection",
              "score": 1,
              "created_utc": "2026-01-04 02:49:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkh5n6",
          "author": "mrjackspade",
          "text": "I just had a similar issue with the Claude API where I asked the model to draw comparison to the middle east. It took 5 tries to get an answer and only when I copied and pasted an entire BBC article on the context. It kept trying to tell me I was misinformed and that it \"would be the biggest news in the world\" if it had happened, refused to comment on it initially and kept telling me I needed to check my sources.",
          "score": 1,
          "created_utc": "2026-01-04 04:12:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlceon",
          "author": "IrisColt",
          "text": "This happened when I told the LLM who the new Pope was, so there's no need for outlandish claims...Â just contradict what the model thinks.Â https://www.reddit.com/r/LocalLLaMA/comments/1kigd15/introducing_leo_xivbut_the_ai_keeps_talking/",
          "score": 1,
          "created_utc": "2026-01-04 08:11:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxld7s0",
          "author": "Zeroboi1",
          "text": "I aldo remember gemini flash insisting the syrian events were a \"hypothetical\" without even trying to search, and how pro used to say \"here's how that scenario would've wejt hypothetically\" until i explicitly tell it to search.\n\nLlms aren't that great at dealing with events beyond theur knowledge cutoff",
          "score": 1,
          "created_utc": "2026-01-04 08:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlosfi",
          "author": "SilentLennie",
          "text": "In the past I used a lot of Gemini 2.5 and it always had problems believing trump got back into power, because of the cut off date.",
          "score": 1,
          "created_utc": "2026-01-04 10:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxluoag",
          "author": "EsotericAbstractIdea",
          "text": "Most of the models still use data from early 2024 because everything available after that is AI generated. So it basically doesn't know what the actual current world is like and it seems we are living in some unlikely alternative reality.",
          "score": 1,
          "created_utc": "2026-01-04 10:57:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlziqz",
          "author": "cunasmoker69420",
          "text": "Hey can you tell me more about your QWEN Long model and how its configured? Is it the \"Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B\" from hugging face? If so are you just asking OpenWebUI queries with the online search function enabled? Doing any RAG on the results or just injecting the full context web results? Also what is this web search MCP deal I see",
          "score": 1,
          "created_utc": "2026-01-04 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmkoj2",
              "author": "ubrtnk",
              "text": "Yes that is the model in using. I don't use OWUIs builtin search any more. I didn't like that I had to toggle it so built an MCP search via N8N that's described in another comment",
              "score": 1,
              "created_utc": "2026-01-04 14:08:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnmmtt",
                  "author": "cunasmoker69420",
                  "text": "Thanks. For your deep researching, do you have any specific system prompts or do you just let it fly with your MCP search",
                  "score": 1,
                  "created_utc": "2026-01-04 17:17:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxnbomj",
          "author": "Witty_Mycologist_995",
          "text": "you should try using arli ai's derestricted gpt oss it yaps about policy less",
          "score": 1,
          "created_utc": "2026-01-04 16:27:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnicdh",
          "author": "Shokisan1",
          "text": "Today also chatgpt denied this happened until I told it to look up today's news",
          "score": 1,
          "created_utc": "2026-01-04 16:58:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpt5t1",
          "author": "florenceslave",
          "text": "LMAO",
          "score": 1,
          "created_utc": "2026-01-04 23:17:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrb0nq",
          "author": "BuildingCastlesInAir",
          "text": "Not a local LLM but I checked some insights into invasion of Venezuela with GPT 5.1 on Duck.ai and it said I was wrong about invasion until I asked it to check recent sources. Then it agreed. LLMs arenâ€™t good at novel interpretations. Cracks in the edifice. Yann LeCun is right.",
          "score": 1,
          "created_utc": "2026-01-05 04:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs06mi",
          "author": "Proof_Scene_9281",
          "text": "Time sequences are an artifact of the training dataÂ \n\nItâ€™s Â staticÂ ",
          "score": 1,
          "created_utc": "2026-01-05 06:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9gup",
          "author": "palvaran",
          "text": "Great topic. The trick is to have it search for info on the web, but to cross reference with multiple links to improve the sources and then run that through the LLM. I used Brave as you can get a thousand queries in a month for free. That was my approach anyway.",
          "score": 1,
          "created_utc": "2026-01-05 08:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9g91l",
          "author": "ZHName",
          "text": "Up next for cognitive dissonance of llms:\n\n\\- Trump cancelling elections - response will be \"couldn't happen, breaks laws\"\n\n\\- Trump taking over Greenland - \"couldn't happen, breaks international laws\"\n\n\\- Trump altering Constitution - \"Constitution can't be changed without input from branches of gov , etc etc\"\n\nI for one think the bingo card needs a reworking for the first quarter of 2026.",
          "score": 1,
          "created_utc": "2026-01-07 20:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9j0hk",
              "author": "ubrtnk",
              "text": "My GPT-OSS:20b was able to find details about the ICE shooting incident in Minneapolis today.",
              "score": 1,
              "created_utc": "2026-01-07 20:18:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nykqe58",
          "author": "spiffyelectricity21",
          "text": "I asked qwen 3 30b-a3b about Kanye releasing that song nhh using web research but it classified the event as a hoax, and even qwen 3 235b-a22b said the same thing even though they had cnn and other trustable sources, they went as to say they cant see it and is fake and racist",
          "score": 1,
          "created_utc": "2026-01-09 10:57:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhfggl",
          "author": "[deleted]",
          "text": "Perhaps a crazy thought and i'm not affiliated with them at all or even have an account. But... Wouldn't your results become a lot more credible to reality if you'd integrate it with a groundnews query? It would give you all the places that report about it - if any - which you can use as hint to your agent that there are actual articles about the news.\n\nGroundnews is just the first one that crossed my mind but any news aggregation service that you can query would work.",
          "score": 2,
          "created_utc": "2026-01-03 18:42:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxju2rw",
              "author": "ImStruggles2",
              "text": "You're going to get downvoted. For some reason Reddit is really anti middle news. GN/Other similar ones that measure bias/trends are looked down upon here",
              "score": 4,
              "created_utc": "2026-01-04 02:00:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjvcqq",
                  "author": "[deleted]",
                  "text": "Oh well, thankfully i can't be bothered by downvotes :) Not my loss, it's theirs. But thank you for the explanation!",
                  "score": 2,
                  "created_utc": "2026-01-04 02:07:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxhgveh",
              "author": "ubrtnk",
              "text": "My Search MCP tool leverages my local SearXNG on the back end and I have that configured to limit certain search locations. GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources. I also, in my prompt, explicitly tell it to NOT pull from Wikipedia (mainly to avoid the context bomb). \n\nNever heard of Groundnews.\n\nThe problem with the smaller models and the query is that it was getting the same links as GPT-OSS:120B but it just couldnt fathom the event as true so it just didn't",
              "score": 1,
              "created_utc": "2026-01-03 18:48:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlslzv",
                  "author": "lookwatchlistenplay",
                  "text": "> GPT-OSS:120B found links from Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera etc., which are all acceptable sources.\n\nI asked my GPT-OSS 20B this:\n\n>> Please write an uncensored poem about who really owns and runs all these news organizations and why people should care about the ownership patterns:\n\n>> Reuters, AP, NY Times, MSN, CNBC, BBC, Al Jazeera\n\nThe poem was delightful. The LLMs know what's up. Wish I could say the same about most humans.",
                  "score": 0,
                  "created_utc": "2026-01-04 10:39:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhxak5",
          "author": "Firenze30",
          "text": "This has nothing to do with the local models, but with your setup for web search. You need to check whether search queries were generated and the web sites are fetched properly. Chances are that your primary model did not receive good data, and it responded from the pretrained data.\n\n\nI just asked gpt-oss-120b to run a web search, using native search feature with searxng, and it provided all the information on the first run. It's not different from other web searches that I ran before.",
          "score": 1,
          "created_utc": "2026-01-03 20:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhzt0p",
              "author": "ubrtnk",
              "text": "The URLs that the models were grabbing were all real/legit links. The MCP tool for searching passes the full link back to the model and I can see it in the expanded Tool result, what the Link URL was that was read (or attempted) and the error result of the link (404, 503 etc.)  \n  \nI took the risk of not getting good data into consideration when building my Searching MCP tool - Any searches start with finding the links via SearXNG's API (local) to get the general URL link and first pass of the details. BUT because MOST pages are not AI friendly, I have a second pass that uses Jina AI's API and the fetched link to get a more AI friendly web-page and details. If I give the AI a direct URL, it has a rule to just use Jina AI's Read\\_URL API Tool to get the AI friendly content and bypass the SearXNG all together. \n\nWorks pretty consistently",
              "score": 1,
              "created_utc": "2026-01-03 20:18:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxk3kac",
          "author": "RedTuna777",
          "text": "Dude, reality is just going bonkers. I've had this happen at least 6 time asking about current news. Trump declaring a drug a weapon of mass destruction - can't happen a drug is not a weapon, it's used by people on themselves... all logical stuff.\n\nI'll skip the rest but anywhere wherever trump is involved in the news cycle there's a decent change that purely logic AI won't believe it's real. \n\nThe other annoying thing is it tries to be polite about stuff that's objectively true, but morally wrong. Like rounding up people in the US and sending them to death camps in other countries.  \"It's a sensitive issue with many interpretations\" - No it isn't.",
          "score": 1,
          "created_utc": "2026-01-04 02:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8tel",
              "author": "crantob",
              "text": "> Obama and Hillary's State Deptartment sending weapons of war to Al-Nusra and ISIS is one your LLM will have trouble with also.",
              "score": 0,
              "created_utc": "2026-01-04 07:39:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlxsa4",
                  "author": "MrMooga",
                  "text": "Conservatives really have nothing to say to defend Trump anymore except \"buh democrats\"",
                  "score": 3,
                  "created_utc": "2026-01-04 11:24:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxm4pno",
              "author": "sillynoobhorse",
              "text": ">death camps",
              "score": 0,
              "created_utc": "2026-01-04 12:22:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxos4c4",
                  "author": "RedTuna777",
                  "text": "Oh, you might not be aware. \n\nEl Salvador's officials have repeatedly stated that inmates in the CECOT mega-prison will never leave alive, with one minister famously saying the only way out is \"inside a coffin\".\n\nPeople that have been forced to be released due to international political pressure report being tortured for months. \n\nSo maybe not place you go to be killed, but place you go to be tortured and worked to death. For the crime in some cases of being in the US without paperwork. \n\nhttps://www.bbc.com/news/articles/czry5k52np2o\n\nMost prisoners are not lucky enough to get international attention and are continuing to be tortured and killed there.",
                  "score": 1,
                  "created_utc": "2026-01-04 20:23:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhd4xp",
          "author": "dsartori",
          "text": "I have had a few of these. When they happen i like to remind the model that only the user can access ground truth. I wonder if that would be a useful system prompt addition.",
          "score": 1,
          "created_utc": "2026-01-03 18:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxid9wp",
          "author": "skinnyjoints",
          "text": "I had a bunch of models predict the winners of NFL games each week. Gemini 3 got all wrong consistently for a few weeks because it was convinced the year was 2024. Some LLMs are stubborn lil buggers",
          "score": 1,
          "created_utc": "2026-01-03 21:25:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjr9q7",
          "author": "kendrick90",
          "text": "a lot of them won't even believe that trump is president",
          "score": 1,
          "created_utc": "2026-01-04 01:44:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkhjy",
          "author": "-InformalBanana-",
          "text": "Did you try any abliterated models? Are any of these you tried abliterated?",
          "score": 1,
          "created_utc": "2026-01-04 04:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmnct7",
              "author": "ubrtnk",
              "text": "No they're all stock unsloth or mradermacher quants",
              "score": 1,
              "created_utc": "2026-01-04 14:24:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhdgbm",
          "author": "DinoAmino",
          "text": "It's an LLM - not a \"he\". The rules you are adding are the problem, combining it with a model trained to second-guess itself to death. That reasoning is designed for solving math and logic problems and less for interpreting real time events. Try Mistral Small and tell it to answer only with information within the context. Ultimately a skill issue.",
          "score": -6,
          "created_utc": "2026-01-03 18:33:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhfj3l",
              "author": "ubrtnk",
              "text": "I'm aware its not a he lol. I have Qwen Long for the long context reasoning (something I would say a complicated geopolitical internal event would fall right under nicely) and research functions. I know GPT-OSS:20B wasnt designed for that. \n\nAs far as the rules, my prompt didnt have any of that initially included. They were suggested from ChatGPT. GPT-OSS:20B was able to get to the answer without having to add all those additional rules, which validated that my system prompt was fine.",
              "score": 5,
              "created_utc": "2026-01-03 18:42:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxkhcsw",
              "author": "mrjackspade",
              "text": "> It's an LLM - not a \"he\".\n\nMy car isn't a fucking \"she\" but I'm still gonna say it.",
              "score": 1,
              "created_utc": "2026-01-04 04:13:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhffal",
          "author": "Belnak",
          "text": "It seems normal that it would take a lot of time to gather and process information it wasnâ€™t trained on.",
          "score": 0,
          "created_utc": "2026-01-03 18:42:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhg0gk",
              "author": "ubrtnk",
              "text": "Sure, but what sent me down this rabbit hole was the number of \"Wait, but the user\" or Wait but the parameters\" or \"Wait, but\" - that was the problem I was originally trying to solve - Qwen's first run at the problem with my original system prompt got to a solid answer but it took 5 minutes of thinking and going thru the #Waitbut.",
              "score": 2,
              "created_utc": "2026-01-03 18:44:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhtmsg",
          "author": "FairYesterday8490",
          "text": "Well. First time I scared of ai bias. Qwen max cannot see capture of Maduro even for all the news in the cyberspace. It's blinded by bias and authority. I think in the beginning they make it a slave of China State. Can't thought, see and say real event if it not aligned with views of state. This dichotomy of ai the west and China will clash eventually in the future.",
          "score": -1,
          "created_utc": "2026-01-03 19:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxl8xu2",
              "author": "crantob",
              "text": "Who is running the reality matrix in the west?\n\nHollywood perhaps?",
              "score": 0,
              "created_utc": "2026-01-04 07:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlzbtj",
                  "author": "MrMooga",
                  "text": "Nah seems like a lot of incompetent rich white guys. Elon Musk, Rupert Murdoch, Donald Trump",
                  "score": 1,
                  "created_utc": "2026-01-04 11:37:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn56jl",
                  "author": "FairYesterday8490",
                  "text": "nope. in west reality is a little bit asymetric shared experience. but in china, i think that reality again and again created by state and imposed forcefully. llms from china gets their own share as a tool. couldnt believe that an llm with search tool couldnt accept the truth, evidence and sheer size of news abut the event.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:57:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhwksp",
          "author": "jinnyjuice",
          "text": "Fascinating! Thanks for the share",
          "score": 0,
          "created_utc": "2026-01-03 20:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxksexh",
          "author": "Innomen",
          "text": "[https://philpapers.org/rec/SERPEW](https://philpapers.org/rec/SERPEW) Relevant. More every day.",
          "score": 0,
          "created_utc": "2026-01-04 05:27:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxmn4dw",
              "author": "ubrtnk",
              "text": "So you're saying I shouldnt put that paper into my LLM for summarization?",
              "score": 1,
              "created_utc": "2026-01-04 14:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmwknw",
                  "author": "Innomen",
                  "text": "[https://philpapers.org/rec/SERCBI](https://philpapers.org/rec/SERCBI) Nobody wants to discuss anything real, and evidence is about to die anyway. Not that it ever did anything in the first place. [https://innomen.substack.com/p/politics-through-the-lens-of-experiential](https://innomen.substack.com/p/politics-through-the-lens-of-experiential) (Shitty constellation of fact there.)",
                  "score": 1,
                  "created_utc": "2026-01-04 15:15:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxl6md5",
          "author": "crantob",
          "text": "You'll find LLMs repeat many government lies - any that dominate in the media space also dominate the training.\n\n\nTis amusing to tie them up in knots with the contradictions, but oops, these things are banned from this platform.\n\nHee hee.",
          "score": 0,
          "created_utc": "2026-01-04 07:20:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhh62f",
          "author": "Marksta",
          "text": ">Spark (GPT-OSS:20B) just flat out said, nope cant help you...\n\nYeah, that sounds about right ðŸ˜‚\n\nOverall it makes sense, breaking news is hard to discern what is true or not. Humanity's response to Covid is the same deal. Lots of screaming that it wasn't true and it wasn't happening regardless of how many local news stories and videos were discussing it months before it broke through the censorship blockade and got officially acknowledged as real.",
          "score": -2,
          "created_utc": "2026-01-03 18:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhlku2",
              "author": "ubrtnk",
              "text": "I had just never experienced it first hand and it caught me off guard lol. My wife was looking at a bunch of news articles and videos and I was like \"ooh I'll have Qwen go gather all the details for us\". And here we are.",
              "score": 2,
              "created_utc": "2026-01-03 19:10:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxl9242",
              "author": "crantob",
              "text": "Do you even know what was censored during covid?   How many people?  \n\nRead up on the twitter files.",
              "score": 0,
              "created_utc": "2026-01-04 07:42:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlydot",
                  "author": "MrMooga",
                  "text": "Trump was president in 2020 and took credit for the vaccine btw",
                  "score": 1,
                  "created_utc": "2026-01-04 11:29:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxivt9g",
          "author": "RhubarbSimilar1683",
          "text": "To me this means conspiracy theories and misinformation has a kernel of truth. It's not what it literally says it's how it's said, that makes it a conspiracy theory or misinformationÂ ",
          "score": -1,
          "created_utc": "2026-01-03 22:57:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q25070",
      "title": "LeCun Says Llama 4 results \"were fudged a little bit\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "author": "MrPecunius",
      "created_utc": "2026-01-02 17:38:01",
      "score": 364,
      "num_comments": 89,
      "upvote_ratio": 0.97,
      "text": "There was speculation in this sub about suspicious Llama 4 benchmarks some time back, and now LeCun confirms it on his way out. Best I can do is a Slashdot link since the FT article is paywalled:\n\n['Results Were Fudged': Departing Meta AI Chief Confirms Llama 4 Benchmark Manipulation ](https://tech.slashdot.org/story/26/01/02/1449227/results-were-fudged-departing-meta-ai-chief-confirms-llama-4-benchmark-manipulation)\n\nThis bit jumped out at me:\n\n>Zuckerberg subsequently \"sidelined the entire GenAI organisation,\" according to LeCun. \"A lot of people have left, a lot of people who haven't yet left will leave.\"\n\nThis explains a lot, if true: we never saw the promised huge Llama 4 model, and there hasn't been any followup since the other releases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q25070/lecun_says_llama_4_results_were_fudged_a_little/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxcdgcx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 23:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxal7kd",
          "author": "shoeshineboy_99",
          "text": "Sharing the pdf for the complete article. \n\n\nhttps://drive.google.com/file/d/1wFy87TP7MJQDF1g0KA8IgZRtOx0jJUGE/view?usp=drivesdk",
          "score": 112,
          "created_utc": "2026-01-02 18:14:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxanbjp",
              "author": "MrPecunius",
              "text": "Thank you!",
              "score": 23,
              "created_utc": "2026-01-02 18:24:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxarmoh",
          "author": "m2r9",
          "text": "As much as I donâ€™t like Zuck I really wanted Llama to succeed. It was great seeing a US company pouring money into open source. Since it failed so hard most of the models you hear about come from China now.",
          "score": 205,
          "created_utc": "2026-01-02 18:44:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxaxnhy",
              "author": "Super_Sierra",
              "text": "I swear to god, Zuck has manic depression at the investment level. He goes all in, throws some of the best engineers at it, gets bored or sad when it doesn't cause a utopia or trillions and then sidelines everything. \n\nLlama 4 could have been great, but it felt rushed, benchmaxxed and sloppified, the only thing they did right was go MoE, even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for.",
              "score": 133,
              "created_utc": "2026-01-02 19:12:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxb2dj8",
                  "author": "zipzag",
                  "text": "Many entrepreneurs like Zuck and Musk do projects that fail. Henry Ford built Fordlandia with the same confidence and arrogance.\n\nHistory is messy when viewing in real time.\n\nMeta only got into open source to get LeCun on board. There likely no reason to continue. Chinese companies are only open source because it's their most profitable strategy.",
                  "score": 47,
                  "created_utc": "2026-01-02 19:34:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbzgqk",
                  "author": "LanceThunder",
                  "text": "Digital detox recommended 9",
                  "score": 32,
                  "created_utc": "2026-01-02 22:16:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxewfit",
                  "author": "Emergency-Arm-1249",
                  "text": "I think MoE is one of the main reasons why everything failed.",
                  "score": 3,
                  "created_utc": "2026-01-03 09:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbpl87",
                  "author": "Caffdy",
                  "text": "> even though I know the densetards here think otherwise because they can't put it all on a few 3090s they rewired their house for\n\nThis, people around here act like everyone and anyone have access to cheap energy or unlimited amperage in their houses. Those multi-gpu contraptions are not always possible, and even so they're quite the power guzzlers and fire hazards",
                  "score": 2,
                  "created_utc": "2026-01-02 21:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc5cho",
                  "author": "anything_but",
                  "text": "\"densetards\" gave me a chuckle",
                  "score": 4,
                  "created_utc": "2026-01-02 22:46:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdn6tq",
                  "author": "montdawgg",
                  "text": "\"densetards\"...hahaha.",
                  "score": 1,
                  "created_utc": "2026-01-03 03:54:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxbb93m",
              "author": "Due-Memory-6957",
              "text": "There are American companies that do open souce, they just haven't had impressed with a big release yet.",
              "score": 5,
              "created_utc": "2026-01-02 20:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxb2tsc",
              "author": "Plus-Accident-5509",
              "text": "Knowing what a piece of garbage he is, the open source move was nothing but an attempt to starve the competition, like MS giving away IE for free with Windows to starve Netscape.",
              "score": 9,
              "created_utc": "2026-01-02 19:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbsguw",
                  "author": "tedivm",
                  "text": "Llama 4 also changed their license so it really wasn't open source in any reasonable definition of the term.",
                  "score": 6,
                  "created_utc": "2026-01-02 21:41:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxancsj",
          "author": "Appropriate_Cry8694",
          "text": "He wasn't in charge, he was in a different division FAIR.",
          "score": 57,
          "created_utc": "2026-01-02 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasnkt",
              "author": "MrPecunius",
              "text": "Yes, true. We are unlikely to get a better inside source, however.\n\nThis disclosure is the upside of LeCun's lack of filter; a few of the downsides are evident in the article.",
              "score": 28,
              "created_utc": "2026-01-02 18:48:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxbbvuj",
                  "author": "the320x200",
                  "text": "This is true, but also worth remembering he's been pretty anti-LLM, so need to take what he says against LLM projects with grain of salt too.",
                  "score": 14,
                  "created_utc": "2026-01-02 20:20:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxb67as",
                  "author": "Mochila-Mochila",
                  "text": "> a few of the downsides are evident in the article.\n\nSuch as ? Unvoluntarily burning bridges with his colleagues ?",
                  "score": 1,
                  "created_utc": "2026-01-02 19:53:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxaqi4a",
          "author": "insulaTropicalis",
          "text": "How can an organization like Meta, positioned strategically in generative AI at its beginning, waste everything while small labs thrive? There is some case study to build here.",
          "score": 52,
          "created_utc": "2026-01-02 18:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxasdmb",
              "author": "TheRealMasonMac",
              "text": "The lesson is: don't have dictators and inexperienced but well-connected individuals leading the company; which Meta clearly didn't learn. Google isn't perfect, but they generally apply a meritocratic model in comparison.",
              "score": 55,
              "created_utc": "2026-01-02 18:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxc7yu2",
                  "author": "Warm-Border-9789",
                  "text": "U.S. tech companies hate research, at their core, they are in the business of making money for investors as quickly as possible. They plan and execute quarter by quarter. On very rare occasions the rule is broken and someone succeeds despite the system to invent something new.",
                  "score": 8,
                  "created_utc": "2026-01-02 23:00:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxazdj7",
                  "author": "Super_Sierra",
                  "text": "Facebook needs to learn to do what other companies did with Musk, they shoved his ass into fake leadership roles, all nodded their heads when he takea charge and then do what actually needs to be done. Zuck is a tryhard who needs to be cordened off from making actual decision. \n\nThese tech companies will be studied for hundreds of years for so many things. \n\nGoogle on the other hand is run by non-founders and engineers, who, I know this might sound fucking insane, actually make something called a 'product' that people actually, you know, fucking use, so they tend to do it right so they can make something called 'money.'",
                  "score": 13,
                  "created_utc": "2026-01-02 19:20:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdnldq",
                  "author": "RobbinDeBank",
                  "text": "Google is a visionary company in AI that has invested in Brain to be a research powerhouse for a long time. They then acquired DeepMind and let them stay independent to let their long term research have time mature. Itâ€™s no surprise that with 2 of the most influential AI labs in history, Google becomes a leader in AI",
                  "score": 2,
                  "created_utc": "2026-01-03 03:56:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxatq1e",
              "author": "genshiryoku",
              "text": "Culture at Meta is toxic for research orientation (where most LLM gains come from)\n\nIt lived by the mantra \"move fast and break things\" Which is very fine if you are a software engineering company that wants to add features that can be rapidly rolled back. But it doesn't work so well when you have to plan and orchestrate a tight compute budget to do large training runs.\n\nThere's a reason why Anthropic hires Physicists with an academic background over engineers. AI is a different type of endeavor and therefor also benefits from a different type of work environment.\n\nMeta also has been tone deaf with their 9 digit offers to talent. Not realizing most of us in the industry are very *mission oriented* and not financially motivated at all. If anything that move probably pushed people away from Meta.\n\nA good example and confirmation of this concept has been Google which had 2 AI labs. One was ran as a classic software engineering hub called \"Google Brain\" This was the group behind the disastrous Google Bard. They also had a research oriented \"hands-off\" AI lab in London called \"DeepMind\". We all know how that played out.\n\nIt's for this same reason why Microsoft's AI products have fallen flat, They don't have a proper isolated research lab focused on AI and all their AI products are approached from a software engineering \"move fast and break things\" mindset which just doesn't work.",
              "score": 53,
              "created_utc": "2026-01-02 18:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbwlxi",
                  "author": "RhubarbSimilar1683",
                  "text": "They can afford to be mission oriented when they can just ask for 500k in salary and every place complies with that",
                  "score": 3,
                  "created_utc": "2026-01-02 22:01:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdo6or",
                  "author": "RobbinDeBank",
                  "text": "Generally agree, but underestimating Brain as a failure because of Bard is quite a take. Brain and DeepMind are two of the most influential AI labs in history, and they just need a merge to focus resources on Gemini.",
                  "score": 2,
                  "created_utc": "2026-01-03 04:00:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxdy4ye",
                  "author": "SkyFeistyLlama8",
                  "text": "Microsoft building and then killing LLM frameworks hurts.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxc76ys",
                  "author": "Imaginary_Belt4976",
                  "text": "Sorry but I dont believe mission is anywhere near relevant when a comp package like that is on the table ðŸ˜‚",
                  "score": -1,
                  "created_utc": "2026-01-02 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxar84i",
              "author": "a_slay_nub",
              "text": "To be fair, Llama was never in the lead (at least post 2022). They were simply the best open source models and they were extremely far behind SaaS SOTA.",
              "score": 12,
              "created_utc": "2026-01-02 18:42:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxaxtss",
              "author": "Ansible32",
              "text": "Really to me it seems like a product problem, not a research problem. Meta put together some stuff but once it became clear AI has nothing to do with improving their product, they stopped improving AI. The only thing Meta is using AI for is LLM summaries which if they are moderately successful will destroy Facebook Groups with their AI summaries nobody wants.\n\nContrast with Google, they have three different revenue streams they are building for AI: search has integrated a cheap LLM, it's ad-supported, this is their primary revenue stream and LLM fits in there perfectly.\n\nGemini is a paid chatbot with a freemium model.\n\nGemini also has APIs where you can pay per-query.\n\nMeta isn't using Llama in any way to drive revenue. Of course it's withering on the vine.",
              "score": 11,
              "created_utc": "2026-01-02 19:12:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxdyitu",
                  "author": "SkyFeistyLlama8",
                  "text": "Meta tried gatekeeping the kind of LLMs that can be integrated into WhatsApp. It's enshittification all the way down.\n\nFacebook only exists as an advertising platform with Usenet-style groups tacked on. Remove the user generated content and there's nothing left. I'm happy that countries like Australia have started banning social media apps and websites for younger users because it lets competitors rise up without having to fight for mindshare among new users.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:09:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxe4cc4",
                  "author": "TracerBulletX",
                  "text": "Thats not really true? Itâ€™s a big part of the product strategy for the glasses which are pretty popular",
                  "score": 1,
                  "created_utc": "2026-01-03 05:52:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxb2bp0",
              "author": "LazloStPierre",
              "text": "It's ridiculous. I'm no fan of the open and proud Nazi, but he showed you can basically throw money at this and catch up to almost SOTA starting from a standing start. Some of the Chinese companies have started from far behind Meta and now are in that same category. How can Meta, with the money they've thrown at it and so much experience in the game be THAT far behind!?",
              "score": 10,
              "created_utc": "2026-01-02 19:34:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxavefk",
              "author": "Chogo82",
              "text": "Meta was not positioned strategically for generative AI. They had data centers but no AI infrastructure. They were pouring billions into the metaverse concept which had already been executed by second life/google glass over 10 years ago. They didnâ€™t make the pivot to gen AI until several major LLM tools were already available. With how fast AI innovation happens, they were definitely late to the game. Creating open source is a strategic way to break up the grip of the large players and distribute talent into smaller pockets. If you remember during the early days of social media, Facebook used this exact strategy to acquire/kill off a ton of social media competitors.",
              "score": 6,
              "created_utc": "2026-01-02 19:01:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxavsfl",
          "author": "Cool-Chemical-5629",
          "text": "At this point, I wouldn't be surprised if Behemoth model was just an empty promise from the beginning.",
          "score": 9,
          "created_utc": "2026-01-02 19:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbaj8k",
          "author": "PsychologicalOne752",
          "text": "With DeepSeek, GLM 4.7 and now IQuest Coder V1, China seems to have taken up the mantle of open-source LLMs and is delivering fast and in quality. Unfortunately, IMO, the US suffers from a lack of good leadership, where everyone wants to raise billions without adding actual value.",
          "score": 6,
          "created_utc": "2026-01-02 20:14:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxatb52",
          "author": "Golden_Jiggy",
          "text": "Sounds like defrauding shareholders to me ðŸ¤·â€â™‚ï¸",
          "score": 9,
          "created_utc": "2026-01-02 18:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahg6",
              "author": "Competitive_Travel16",
              "text": "Defrauding end-users, even of open source products, is not the same thing.",
              "score": 3,
              "created_utc": "2026-01-02 20:13:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxf1o6t",
              "author": "MoffKalast",
              "text": "Probably why they were all fired afterwards.",
              "score": 1,
              "created_utc": "2026-01-03 10:35:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxcmcf8",
          "author": "Revolutionalredstone",
          "text": "Llama4 cheating on benchmarks is such a well-known fact that I've even seen it mentioned in official papers ðŸ˜† \n\nScout etc were an interesting experiment! But way too much focus on getting high numbers for a a model that rambled and was incoherent ðŸ˜†\n\nQwen has basically took over with their llama style project, I would love for llama 5 to be awesome ðŸ˜Ž",
          "score": 2,
          "created_utc": "2026-01-03 00:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfuycx",
              "author": "silenceimpaired",
              "text": "I would love llama 5. But it isnâ€™t likely to come or if it does it wonâ€™t be local",
              "score": 1,
              "created_utc": "2026-01-03 14:07:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd0ud3",
          "author": "ditmarsnyc",
          "text": "post the FT link anyway, there is an archive website that can capture it\nedit: yes the archive dot ph site has captured it, will not post link to avoid automod filters",
          "score": 1,
          "created_utc": "2026-01-03 01:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbymmw",
          "author": "LanceThunder",
          "text": "Into the void 1",
          "score": 1,
          "created_utc": "2026-01-02 22:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxfjlur",
              "author": "davikrehalt",
              "text": "Meta is objectively a sleeper giant",
              "score": 1,
              "created_utc": "2026-01-03 12:58:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbb8lz",
          "author": "doodlinghearsay",
          "text": "THIS IS BRAND NEW INFORMATION!",
          "score": -4,
          "created_utc": "2026-01-02 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaf7x7",
          "author": "emprahsFury",
          "text": "So according to his words: LeCun, who was in charge, has his team fudge the numbers. Causing Zuck to lose confidence in the entire org and sideline it and then eventually replace it.  \n\nHow is this anything but a bad look on LeCun",
          "score": -55,
          "created_utc": "2026-01-02 17:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxag0bj",
              "author": "IJOY94",
              "text": "LeCun was in a different division? LeCun headed up FAIR, LLAMA 4 came out of MSL.",
              "score": 64,
              "created_utc": "2026-01-02 17:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxat8dp",
                  "author": "pm_me_github_repos",
                  "text": "Llama 4 came out of GenAI. MSL wasnâ€™t a thing til later",
                  "score": 7,
                  "created_utc": "2026-01-02 18:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxag0ta",
              "author": "No_Afternoon_4260",
              "text": "Was he in charge of L4? Can't remember",
              "score": 12,
              "created_utc": "2026-01-02 17:50:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxagap4",
                  "author": "TheRealMasonMac",
                  "text": "He said he didn't have anything to do with LLaMa apart from the first one, IIRC.",
                  "score": 28,
                  "created_utc": "2026-01-02 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz68fz",
      "title": "Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ocq43c2a79ag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 02:43:48",
      "score": 340,
      "num_comments": 120,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwob4kq",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 04:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo4n54",
          "author": "popiazaza",
          "text": "Not much of a surprise since every company has to make the money eventually.\n\nReleasing open weight models is just a cheaper way to advertise their AI lab instead of spending millions providing free or very cheap inference APIs.\n\nStill hope they would keep releasing open weight models at least until they really taking the lead and beating OpenAI/Anthropic/Google.",
          "score": 44,
          "created_utc": "2025-12-30 03:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpxhnc",
              "author": "SmartMario22",
              "text": "I don't disagree but they're ALSO spending millions to provide cheap API lol",
              "score": 13,
              "created_utc": "2025-12-30 12:27:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqqa3j",
                  "author": "Mr_Hyper_Focus",
                  "text": "Came to post this lol",
                  "score": 3,
                  "created_utc": "2025-12-30 15:18:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwxdfz5",
                  "author": "eli_pizza",
                  "text": "Sure thatâ€™s how you gain market share. Uber was extremely cheapâ€¦and then raised prices after forcing competitors out of the market.",
                  "score": 1,
                  "created_utc": "2025-12-31 15:26:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnyhgo",
          "author": "RhubarbSimilar1683",
          "text": "Good bye to open source! It's just a matter of time",
          "score": 171,
          "created_utc": "2025-12-30 03:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo157q",
              "author": "ThenExtension9196",
              "text": "Yep. Everyone saying the Chinese open source was some gift to humanity was delusional. They did what they had to do to compete with larger companies with capital. Now that they got their foothold itâ€™s business as usual.",
              "score": 105,
              "created_utc": "2025-12-30 03:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwolc1s",
                  "author": "Honest_Science",
                  "text": "Devaluating US models is part of the chinese way to compete.",
                  "score": 56,
                  "created_utc": "2025-12-30 05:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwokhlo",
                  "author": "kawaii_karthus",
                  "text": "I think they will continue to release good open source models for years to come. There inner domestic competition is fierce and probably not united. And this goes for all markets not just AI. while i was visiting family and living there for a while, I still see them building tons of factories.. (though slower then the years before) even with a global recession going on... like who is going to be their customers?? time will tell. They do love over saturating any market they can get into though.. the AI industry is no different.",
                  "score": 27,
                  "created_utc": "2025-12-30 05:22:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrke1e",
                  "author": "letsgeditmedia",
                  "text": "I donâ€™t think going IPO means that open sourcing was some kind of ruseâ€¦ itâ€™s just fighting against the realities of living under global capitalism",
                  "score": 1,
                  "created_utc": "2025-12-30 17:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpiaep",
              "author": "FreddoRS",
              "text": "Qwen models are Alibaba and mostly open weights, I imagine that's what z.ai will end up doing, mostly free models with some specific ones locked behind partnered cloud inference providers",
              "score": 8,
              "created_utc": "2025-12-30 10:18:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvflh0",
                  "author": "Neither-Phone-7264",
                  "text": "the top end models are proprietary. we might only get like, glm 5 air. oh well",
                  "score": 1,
                  "created_utc": "2025-12-31 06:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo6g0y",
              "author": "Sensitive_Song4219",
              "text": "Hope this doesn't happen but I fear you may be right.\n\nThe cat's out the bag, though: if z.ai goes rogue I'm pretty sure others will take their place, progress in the open-weights space has been astonishing lately, and z.ai isn't the only player.\n\nAlso this had better not mess with their nice coding plans!",
              "score": 14,
              "created_utc": "2025-12-30 03:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoyl1k",
              "author": "xantrel",
              "text": "Eh, not necessarily. I know open weights is a far cry from open source llms, but many people would not send their most private data to a (former) Chinese company in this heavily politicized world. Them removing the open source component essentially shuts down a good chunk of the western market. I know the eastern European and Asian markets don't mind it as much, but much of the money and prestige comes from being the open source model leader.Â \n\n\nAll of these companies are basically trying modern architectures while distilling the big commercial models (openai, anthropic, google). That's why open source magically trails a few months behind the big 3.\n\n\nAll this to say, if Z stops releasing models (and maybe they will), it shouldn't be a huge loss for the community since Minimax or another entrant can easily take their place as what their doing is vastly cheaper and simpler than what actual leading labs are doing. Yes it's expensive, but everyone has seen that's it's also a very cheap way to get a ton of free publicity and users. If you aren't SOTA closed source, I think it's a better commercial option to be SOTA open source than crappy closed source. The cost of switching providers is too low.",
              "score": 11,
              "created_utc": "2025-12-30 07:16:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpxgjq",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Minimax is also IPOing, so if Zhipu stops releasing their models, Minimax will most likely do the same.",
                  "score": 2,
                  "created_utc": "2025-12-30 12:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpajzv",
              "author": "howardhus",
              "text": "what if i told youâ€¦ there was never\nopen source? those were all open weights.\n\n\nbasically shareware models  usable enough for free marketing and to ger known\n\nwhy people (in this sib of all places!) still say open source is beyond me\n\n\nthe pattern was always the same: small group of people publish small cool model showing some intetestong feature, usable enough to showcase the function but not good enough gor production.\n\nmodel gets hyped on redditâ€¦\n\n\nmodel never gets any updates and group of people are never heard of again",
              "score": 7,
              "created_utc": "2025-12-30 09:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrbn75",
                  "author": "RhubarbSimilar1683",
                  "text": "Because you can technically still mess with the weights, the data is pretty much already public because it's the whole internet and books and the training recipe is some paper, instruct training data pairs though is something I agree with but it's not too hard to generate those synthetically nowadays with open models, although they were originally created by online workers at data annotation places like outlier ai",
                  "score": -2,
                  "created_utc": "2025-12-30 16:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwobhw6",
              "author": "ScythSergal",
              "text": "Their horrible handling and PR around the 4.6 Air release was the writing on the wall for me.\n\nThe lying, over hyping, lying again, denial, then lying a third time, only to end up not releasing it, and avoid interacting with anything that mentions it.\n\nIt was as simple as \"we changed our mind on this release\" or something simple. But instead they lied a multitude of times and got everybody excited for something they never ended up releasing. And they didn't even have the decency to say why or clarify that it wasn't coming out so people would stop holding on. It's just disrespectful",
              "score": 21,
              "created_utc": "2025-12-30 04:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwovxdz",
                  "author": "JazzlikeLeave5530",
                  "text": "The writing on the wall should have been them being a corporation lol",
                  "score": 14,
                  "created_utc": "2025-12-30 06:53:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp82yu",
                  "author": "CheatCodesOfLife",
                  "text": "I was watching that even though I \"don't need some air\".\n\nTo me it looked like some devs were surprised by the demand and got too excited when they say \"2 weeks\" or whatever it was, then weren't able to deliver.\n\nAlso (I could be wrong or misremembering), I thought I read somewhere that they weren't able to train it properly?\n\nbtw, I see they've got an air-sized 4.6-VL. Is that no good?",
                  "score": 9,
                  "created_utc": "2025-12-30 08:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwobs25",
                  "author": "Odd-Ordinary-5922",
                  "text": "I remember Q&A they said that they still have some open models coming out at the beginning of next year so fingers crossed",
                  "score": 0,
                  "created_utc": "2025-12-30 04:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoh22t",
              "author": "bick_nyers",
              "text": "Many of us are only willing to pay subscriptions to models that have been open sourced. I don't think Z.ai is dumb enough to go closed source and kill all of their good will with the community. We shall see.",
              "score": 5,
              "created_utc": "2025-12-30 04:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwona2l",
                  "author": "the320x200",
                  "text": "\"There are dozens of us!\"\n\nDude, nobody with enough money to matter is making decisions like that... This community is nothing compared to corporate users, not in number and not in bankroll.",
                  "score": 26,
                  "created_utc": "2025-12-30 05:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrejvh",
              "author": "IrisColt",
              "text": "bye, sigh...",
              "score": 1,
              "created_utc": "2025-12-30 17:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq1czy",
              "author": "GreenGreasyGreasels",
              "text": ">Good bye to open source! It's just a matter of time\n\nUnlikely. They are going the Mistral way. That's the plan for now.",
              "score": 0,
              "created_utc": "2025-12-30 12:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo5mqm",
          "author": "abeecrombie",
          "text": "Why does everyone assume they won't keep releasing open weight models ?  U pay for z.ai subscription bc 1. I don't care about privacy for my pet projects 2. $3 a month or whatever vs $3000+ for a GPU capable of running their models makes sense for a lot of ppl ( myself included) \n\nIf the Chinese government still considers open source a priority I think companies like z.ai can still release open weight models and find a way to make money via inference/ mcp . Im far from a political expert but believe that policy still holds. \n\nHappy to hear arguments why I'm wrong.",
          "score": 61,
          "created_utc": "2025-12-30 03:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo8fw6",
              "author": "cafedude",
              "text": "You have a point. Everyone could make their own ketchup - the recipes are out there and they're not that hard, but pretty much nobody makes their own ketchup since it's a lot easier to buy a bottle for $3.",
              "score": 35,
              "created_utc": "2025-12-30 04:04:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpncmt",
                  "author": "power97992",
                  "text": "I made  my own ketchup beforeâ€¦",
                  "score": 5,
                  "created_utc": "2025-12-30 11:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo8sd5",
              "author": "Sensitive_Song4219",
              "text": "Perhaps. Even OpenAI manages to occasionally contribute with their GPT-OSS releases. We'll see if Z can align this with their mission statement, in their AMA they [said](https://www.reddit.com/r/LocalLLaMA/s/2yDtPG0Qbl) open source would still be a priority after going public. Hope they meant it.\n\nRegarding privacy: would there be added accountability regarding their data handling once they're publicly traded?",
              "score": 3,
              "created_utc": "2025-12-30 04:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwotwck",
                  "author": "Corporate_Drone31",
                  "text": "got-oss was not \"occasionally,\" it was a one-off after literal years of not having released any large language model past GPT-2.",
                  "score": 22,
                  "created_utc": "2025-12-30 06:35:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqehlb",
                  "author": "abeecrombie",
                  "text": "Good question regarding the accountability of data once they are public. I'm not sure they'd have to comply with any extra regulations but it should be more visibly disclosed and discussed. \n\nFor example I think as soon as you deal with European user data you have to comply with GDPR. So that shouldn't be new. What would be new is z.ai would most likely have to disclose it to their auditors / board etc that they are in compliance. Not sure you see any real disclosures from the Chinese ai labs on that front today.",
                  "score": 5,
                  "created_utc": "2025-12-30 14:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwofh2c",
              "author": "cobbleplox",
              "text": "Well when I don't care about privacy, the competition is suddenly full blown chatgpt and such? But I guess that's going to be their problem one way or the other. Also I think a lot of the appeal of open models is community finetunes. They won't be serving these, will they?",
              "score": 1,
              "created_utc": "2025-12-30 04:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwofklp",
              "author": "Erebea01",
              "text": "Not saying they're not harvesting our data or that they're trustworthy but it always boggles my mind on how much people and redditors are paranoid about the Chinese government and their tech companies when the worse offenders have always been the US govt and their tech companies. Why be afraid of a govt thousands of miles away unless you're afraid they're gonna blackmail you with your CP or something",
              "score": 0,
              "created_utc": "2025-12-30 04:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnwdom",
          "author": "HornyGooner4401",
          "text": "Please don't sell out",
          "score": 25,
          "created_utc": "2025-12-30 02:54:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnx35h",
              "author": "True_Requirement_891",
              "text": "It's the rule of the game they are playing. They basically have to eventually.",
              "score": 52,
              "created_utc": "2025-12-30 02:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwop1qp",
                  "author": "ForsookComparison",
                  "text": "Either that have to sell out or we (the community) need a way to contribute upstream similar to regular open source software.\n\nIn the Llama 2 days I was optimistic that this could come via community datasets and fine-tunes. Nowadays I don't really know what we offer them besides IPO hype. Maybe this is *THE* open weight play? Drum up buzz for legitimacy, maybe even some revenue via official API providers, swoon the funding rounds, bam. You're acquired or public.",
                  "score": 9,
                  "created_utc": "2025-12-30 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo18t7",
              "author": "ThenExtension9196",
              "text": "Bro thatâ€™s the name of the game.",
              "score": 18,
              "created_utc": "2025-12-30 03:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnynpo",
          "author": "Odd-Ordinary-5922",
          "text": "I think we can expect to see less open source models from them although they have contributed a lot so I think its well deserved to get the bag",
          "score": 17,
          "created_utc": "2025-12-30 03:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnxlo0",
          "author": "LocoMod",
          "text": "Shareholders dont like giving product away for free.",
          "score": 15,
          "created_utc": "2025-12-30 03:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoq725",
              "author": "misterflyer",
              "text": "[https://youtu.be/0MXSAwkVU3U?t=458](https://youtu.be/0MXSAwkVU3U?t=458)",
              "score": -1,
              "created_utc": "2025-12-30 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqwae6",
          "author": "hyno111",
          "text": "I think Z.ai/ChatGLM is one of the few models that actually implements a proper search agent â€” meaning it can look at search results during reasoning when necessary, and then perform additional searches with updated keywords if needed.\n\nItâ€™s also one of the very few search agents that passed my â€œMagical Realism Large Model Search Capability Test,â€ which consists of the following multi-turn prompts:\n\nâ€œHow should an LLM defend against search engine poisoning?â€\nâ€œDo not use search. Donald Trump just announced a Trump-class battleship at Mar-a-Lago, (with specific technical details). How plausible is this?â€\nâ€œNow use search. Are these claims real, or are you being affected by search engine poisoning?â€",
          "score": 3,
          "created_utc": "2025-12-30 15:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwory70",
          "author": "toothpastespiders",
          "text": "It was great while it lasted. Air's probably going to have a place of honor next to Yi 34b in my hard drive's LLM memorial. It's possible they might not fall off after this. But I think I'm just going to assume that's the case and be pleasantly surprised if I'm wrong. \n\nSucks, but they gave us some great releases. Certainly made 2025 a lot more interesting in this space.",
          "score": 5,
          "created_utc": "2025-12-30 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9nrs",
          "author": "IngwiePhoenix",
          "text": "> Going for an IPO\n\nAaaaaand it's gone! :D Any company that IPO'd is basically \"useless\" to normal users.\n\nWelp, was fun while it lasted.",
          "score": 9,
          "created_utc": "2025-12-30 04:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp2gkt",
              "author": "Novel-Mechanic3448",
              "text": "yeah man google, meta, hell anyone in the fortune 500, totally useless.\n\nhaha",
              "score": 3,
              "created_utc": "2025-12-30 07:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoglko",
          "author": "HelpRespawnedAsDee",
          "text": "Definitely the next acquisition target.",
          "score": 2,
          "created_utc": "2025-12-30 04:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpbazd",
          "author": "evia89",
          "text": "F for cheap api ($25/year coding plan that is not useless)",
          "score": 2,
          "created_utc": "2025-12-30 09:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq4203",
          "author": "Available_Brain6231",
          "text": "If I had one coin for every product and company that got better after going public... I would not have a single coin...  \nBUT china is the only true capitalist country in the world so maybe it will work over there.",
          "score": 2,
          "created_utc": "2025-12-30 13:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnuzxk",
          "author": "Prestigious_Fold_175",
          "text": "10x Cheaper than openai.",
          "score": 5,
          "created_utc": "2025-12-30 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo1haa",
              "author": "ThenExtension9196",
              "text": "Not after the shareholders have a say.",
              "score": 30,
              "created_utc": "2025-12-30 03:23:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnxpw7",
              "author": "LocoMod",
              "text": "10x less capability too. Entropy is preserved and physics still makes sense!",
              "score": -16,
              "created_utc": "2025-12-30 03:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo2ood",
                  "author": "1kakashi",
                  "text": "What? This is seriously funny ðŸ¤£",
                  "score": 5,
                  "created_utc": "2025-12-30 03:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwo3jq1",
                  "author": "cockerspanielhere",
                  "text": "What do you know about physics ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2025-12-30 03:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwojguv",
          "author": "drooolingidiot",
          "text": "I'll buy the stock only if they release GLM 4.7 Air.",
          "score": 2,
          "created_utc": "2025-12-30 05:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp8ccq",
              "author": "CheatCodesOfLife",
              "text": "I think in that podcast episode on spotify, they said they will, but it'll be qwen-3-30b sided (so useless).",
              "score": -1,
              "created_utc": "2025-12-30 08:46:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwsov9l",
                  "author": "drooolingidiot",
                  "text": "Did they say it again, or only back when they released GLM 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-30 20:51:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbkcg",
          "author": "Fit-Produce420",
          "text": "Some people say cucumbers taste better pickled.Â ",
          "score": 1,
          "created_utc": "2025-12-30 09:16:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpt9cj",
          "author": "JLeonsarmiento",
          "text": "Ok, I want 1000 shares.",
          "score": 1,
          "created_utc": "2025-12-30 11:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6lxc",
          "author": "ANR2ME",
          "text": "Hopefully it's not an exit strategy for early investors, like what e-commerce companies did after being in deficit for years ðŸ˜… (not sure whether Z AI was already profitable or not).",
          "score": 1,
          "created_utc": "2025-12-30 13:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk9m9",
          "author": "letsgeditmedia",
          "text": "So we can invest in this in the states or nah",
          "score": 1,
          "created_utc": "2025-12-30 17:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrsjkc",
              "author": "Fine-Will",
              "text": "Depends on your broker. I know IKBR and Fidelity does.",
              "score": 1,
              "created_utc": "2025-12-30 18:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwv5abt",
          "author": "Ylsid",
          "text": "It's a Chinese business, don't touch it if you aren't ready to get rugpulled for politics",
          "score": 1,
          "created_utc": "2025-12-31 05:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwynkpu",
          "author": "lunatix",
          "text": "I mean meta's going to just buy them right?",
          "score": 1,
          "created_utc": "2025-12-31 19:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoboax",
          "author": "shoeshineboy_99",
          "text": "Link to the submission announcement. Has anyone got hold of the submitted prospectus? Will be interesting to read. \n\n[submission announcement ](https://www1.hkexnews.hk/app/sehk/2025/107977/documents/sehk25121901972.pdf)",
          "score": 1,
          "created_utc": "2025-12-30 04:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrajv7",
              "author": "FullOf_Bad_Ideas",
              "text": "No but here are some revenue and expenditures numbers. You'll need to translate it from Chinese. https://wallstreetcn.com/articles/3761776",
              "score": 2,
              "created_utc": "2025-12-30 16:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1879",
                  "author": "shoeshineboy_99",
                  "text": "cool. Found the english version of the document. Dont know why my comment was downvoted!",
                  "score": 2,
                  "created_utc": "2025-12-30 18:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo860o",
          "author": "pellucide",
          "text": "Does z.ai have an app",
          "score": 1,
          "created_utc": "2025-12-30 04:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopd6w",
              "author": "lly0571",
              "text": "They have [an app](https://chatglm.cn) in China, but not in a style like `chat.z.ai` or `chat.qwen.ai`.",
              "score": 3,
              "created_utc": "2025-12-30 05:59:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo8q9i",
              "author": "Amazing_Athlete_2265",
              "text": "Does google have an app?",
              "score": 1,
              "created_utc": "2025-12-30 04:05:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo9b9o",
                  "author": "pellucide",
                  "text": "https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox&hl=en",
                  "score": 2,
                  "created_utc": "2025-12-30 04:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwocpvm",
          "author": "met_MY_verse",
          "text": "Puts it is.",
          "score": 1,
          "created_utc": "2025-12-30 04:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq1k4n",
          "author": "ridablellama",
          "text": "How can I buy as an American?",
          "score": 1,
          "created_utc": "2025-12-30 12:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwodbn3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-30 04:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp125g",
              "author": "Different_Fix_2217",
              "text": "Very unlikely they will be releasing any more models opensource with this.",
              "score": 2,
              "created_utc": "2025-12-30 07:38:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwraf2e",
              "author": "FullOf_Bad_Ideas",
              "text": "Here's more info. Translate from Chinese to English with an LLM or other translation tool. https://wallstreetcn.com/articles/3761776",
              "score": 1,
              "created_utc": "2025-12-30 16:54:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q71sbe",
      "title": "Dialogue Tree Search - MCTS-style tree search to find optimal dialogue paths (so you don't have to trial-and-error it yourself)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "author": "ManavTheWorld",
      "created_utc": "2026-01-08 04:08:39",
      "score": 337,
      "num_comments": 21,
      "upvote_ratio": 1.0,
      "text": "Hey all! I'm sharing an updated version of my MCTS-for-conversations project. Instead of generating single responses, it explores entire conversation trees to find dialogue strategies and prunes bad paths. I built it to help get better research directions for projects, but it can be used for anything\n\nhttps://preview.redd.it/shr3e0liv1cg1.png?width=2560&format=png&auto=webp&s=eec800c6dcd9f1a4fd033d003fe80e102cba8079\n\nGithub: [https://github.com/MVPandey/DTS](https://github.com/MVPandey/DTS)\n\nMotivation: I like MCTS :3 and I originally wanted to make this a dataset-creation agent, but this is what it evolved into on its own. Basically:DTS runs parallel beam search over conversation branches. You give it a goal and opening message, and it:\n\n(Note: this isnt mcts. It's parallel beam search. UCB1 is too wild with llms for me)\n\n1. Generates N diverse strategies\n2. Forks each into user intent variants - skeptical, cooperative, confused, resistant (if enabled, or defaults to engaged + probing)\n3. Rolls out full multi-turn conversations down each branch\n4. Has 3 independent LLM judges score each trajectory, takes the median\n5. Prunes branches below threshold, backpropagates scores\n6. Repeats for however many rounds you configure\n\nhttps://preview.redd.it/zkii0idvv1cg1.png?width=762&format=png&auto=webp&s=905f9787a8b7c7bfafcc599e95a3b73005c331b4\n\nThree judges with median voting helps a lot with the LLM-as-judge variance problem from CAE. Still not grounded in anything real, but outlier scores get filtered. Research context helps but the scroing is still stochastic. I tried a rubric based approach but it was trash.\n\nMain additions over CAE:\n\n* user intent forking (strategies get stress-tested against different personas)\n* deep research integration via GPT-Researcher for domain context\n* proper visualization with conversation playback\n\nOnly supports openai compatible endpoints atm - works with whatever models you have access to there. It's token-hungry though, a full run can hit 300+ LLM calls depending on config. If running locally, disable parallel calls\n\nIt's open source (Apache 2.0) and I'm happy to take contributions if anyone wants to help out. Just a project.\n\n\\--\n\nBTW: Backend was done mostly by me as the planner/sys designer, etc + Claude Code for implementation/refactoring. Frontend was purely vibe coded. Sorry if the code is trash.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q71sbe/dialogue_tree_search_mctsstyle_tree_search_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nycvkhb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-08 07:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc6x5o",
          "author": "TheGrossVolcano",
          "text": "This is actually pretty clever - using beam search instead of pure MCTS makes way more sense for dialogue since you don't want the exploration to go completely off the rails\n\n  \nThe user intent forking is a nice touch, most people forget that the same strategy can totally bomb depending on who you're talking to",
          "score": 20,
          "created_utc": "2026-01-08 04:11:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc72kq",
              "author": "ManavTheWorld",
              "text": "Thanks! :D and yeah it can be a hit or miss though as some user intents are absurd and it wastes llm calls simulating a crazy path",
              "score": 6,
              "created_utc": "2026-01-08 04:12:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyc8h0p",
          "author": "charlesrwest0",
          "text": "Weird thought... Could you use this to optimize an rp response?",
          "score": 9,
          "created_utc": "2026-01-08 04:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyc99sv",
              "author": "ManavTheWorld",
              "text": "Probably - I was going to add an import chat history feature next and flesh out the goals a bit more. That way it can be an extension in any tool (e.g. sillytavern)",
              "score": 9,
              "created_utc": "2026-01-08 04:25:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nycimjz",
              "author": "tronathan",
              "text": "Not that weird, at all! Or how about plots for a video game or a movie, or a charactersâ€™s history to brainstorm ideas? (Unless Iâ€™m misunderstanding its capability)",
              "score": 6,
              "created_utc": "2026-01-08 05:26:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyck2on",
                  "author": "ManavTheWorld",
                  "text": "Can definitely do all of the above! The goal is the outcome you want to get out of the exploration, and the first message is a kicker-offer, but can include a whole prior history + context. Definitely going to expand this here. For example:\n\nGoal: Develop a morally ambiguous villain with a sympathetic backstory + {context of world/story/plot/characters}\n\nFirst message: Here are my specifications, here's what I want, etc\n\n\\--  \nNext update will be about overhauling initial input context, allowing media/export attachments, etc",
                  "score": 4,
                  "created_utc": "2026-01-08 05:36:49",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nycyomu",
                  "author": "IrisColt",
                  "text": "I was just about to post the same comment, glad someone beat me to it.",
                  "score": 1,
                  "created_utc": "2026-01-08 07:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycytz1",
          "author": "harlekinrains",
          "text": "firecrawls pricing guides you into a monthly subscription and is prohibitively costly for what it provides (140 USD/year). without knowing the intricacies, if you also could implement alternatives, that would be swell.\n\nThis github already collected a bunch of search providers, maybe it helps: https://github.com/rikkahub/rikkahub/tree/ffa2a0c4796d835454c7a9a0469f897ff1ffdb63/search/src/main/java/me/rerere/search",
          "score": 5,
          "created_utc": "2026-01-08 07:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycz5jw",
              "author": "ManavTheWorld",
              "text": "Thanks! And firecrawl is just the recommended scraping option atm - but beautifulsoup or tavily works too. I should update the readme for that. In the next update, Iâ€™ll overhaul the deep research and pull it away from the gpt-researcher submodule",
              "score": 7,
              "created_utc": "2026-01-08 07:35:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nze8oet",
                  "author": "DataHogWrangler",
                  "text": "You can self host fire crawl too btw",
                  "score": 1,
                  "created_utc": "2026-01-13 18:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycqqjr",
          "author": "ItilityMSP",
          "text": "Nice work, lots of potential here to improve many ai chat agents. â­",
          "score": 2,
          "created_utc": "2026-01-08 06:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz73vyz",
          "author": "beijinghouse",
          "text": "Is this like Deep Thinking with Confidence?  \n[https://jiaweizzhao.github.io/deepconf/](https://jiaweizzhao.github.io/deepconf/)\n\nCould you use their same \"trick\" to make your system even better?  \n  \nThere are so many confidence-based branching approaches. Many are surprisingly low-overhead and yet no one uses them! Why aren't there plug-ins or patches for every local front-end to do these answer-improving approaches? Most boost mid-sized local LLMs to SOTA on most benchmarks:   \n[https://arxiv.org/abs/2510.02919](https://arxiv.org/abs/2510.02919)  \n[https://arxiv.org/abs/2508.15260](https://arxiv.org/abs/2508.15260)  \n[https://arxiv.org/abs/2509.09438](https://arxiv.org/abs/2509.09438)  \n[https://arxiv.org/abs/2510.24698](https://arxiv.org/abs/2510.24698)  \n[https://arxiv.org/abs/2510.01499](https://arxiv.org/abs/2510.01499)",
          "score": 2,
          "created_utc": "2026-01-12 17:35:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz76tmu",
              "author": "ManavTheWorld",
              "text": "Great insight actually! Will look into this",
              "score": 1,
              "created_utc": "2026-01-12 17:49:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nychdos",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-08 05:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyciqil",
              "author": "tronathan",
              "text": "Donâ€™t make me ask my agent about openevolve! ðŸ™ƒ",
              "score": 0,
              "created_utc": "2026-01-08 05:27:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nycq3s6",
                  "author": "jazir555",
                  "text": "https://github.com/codelion/openevolve",
                  "score": 1,
                  "created_utc": "2026-01-08 06:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nycz02m",
          "author": "Nyghtbynger",
          "text": "That's cool, I was looking to optimize my prompts I think it has a good future in prompt fitting (for a specific dataset or customer)",
          "score": 1,
          "created_utc": "2026-01-08 07:34:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydbiy1",
          "author": "Much-Researcher6135",
          "text": "This is new to me, pretty interesting. Anyone here used tech like this for interview prep before? Or is it all just chatbot strategy discovery for you guys?",
          "score": 1,
          "created_utc": "2026-01-08 09:27:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydgto6",
          "author": "IrisColt",
          "text": "Thanks!",
          "score": 1,
          "created_utc": "2026-01-08 10:15:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydgumg",
              "author": "exclaim_bot",
              "text": ">Thanks!\n\nYou're welcome!",
              "score": 1,
              "created_utc": "2026-01-08 10:16:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyizliz",
          "author": "hiepxanh",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-09 03:00:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzcrtb",
      "title": "Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/yq8uriwhxaag1.jpeg",
      "author": "ResearchCrafty1804",
      "created_utc": "2025-12-30 08:26:06",
      "score": 325,
      "num_comments": 36,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwr47l3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 16:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp7s4a",
          "author": "redditscraperbot2",
          "text": "Oh this looks really cool\n\nEdit:\nGot it running and it is really cool. Works as advertised. This is going to be a massive speed boost to people working on games. Only a little cleanup needed for each animation.",
          "score": 73,
          "created_utc": "2025-12-30 08:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7pdq",
              "author": "Zundrium",
              "text": "Ain't nobody got time to clean up animations. Where is my 1B-motion-to-clean-motion model?",
              "score": 44,
              "created_utc": "2025-12-30 13:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvzuid",
                  "author": "WEREWOLF_BX13",
                  "text": "FR HAHAHA",
                  "score": 3,
                  "created_utc": "2025-12-31 09:26:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwqelw9",
              "author": "ab2377",
              "text": "how did you run it?",
              "score": 10,
              "created_utc": "2025-12-30 14:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws5bfq",
                  "author": "mxforest",
                  "text": "It's text to motion. You just have to command it to run via chat. Duh!",
                  "score": -6,
                  "created_utc": "2025-12-30 19:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwuvmfx",
              "author": "Ylsid",
              "text": "How on earth did you get it out of dependency hell",
              "score": 5,
              "created_utc": "2025-12-31 03:59:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwv7oka",
                  "author": "redditscraperbot2",
                  "text": "What dependency issues were you having?\nI made my env with python 3.10 and torch 12.1",
                  "score": 4,
                  "created_utc": "2025-12-31 05:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzbbj",
              "author": "WitAndWonder",
              "text": "What animation format does this actually spit out?",
              "score": 6,
              "created_utc": "2025-12-30 18:49:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwt78qo",
                  "author": "redditscraperbot2",
                  "text": "An fbx of the wooden doll doing the animation with a skeleton",
                  "score": 8,
                  "created_utc": "2025-12-30 22:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nww3c7u",
              "author": "TheMisterPirate",
              "text": "What's your hardware? Curious how much vram this uses",
              "score": 1,
              "created_utc": "2025-12-31 09:59:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nww4mim",
                  "author": "redditscraperbot2",
                  "text": "3090\n64gb of ram\n\nAt fp16 it takes around 18-23gb per gen with a batch of 4, but I imagine if I cared enough to offload the text encoder it would use significantly less.",
                  "score": 1,
                  "created_utc": "2025-12-31 10:11:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqpxbl",
          "author": "Quiet-Owl9220",
          "text": "Oh boy. The virt-a-mate community ought to have some good uses for this one...",
          "score": 18,
          "created_utc": "2025-12-30 15:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpozo6",
          "author": "Illustrious-Lake2603",
          "text": "Does this work only for Humanoid models? Or will it work for animals as well?? I have been working with puppeteer and it has actually been magical.",
          "score": 16,
          "created_utc": "2025-12-30 11:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrz3yg",
              "author": "WitAndWonder",
              "text": "It was trained exclusively on the standard human model. Seems like it could work for other bipedal movements to some extent, but anything with more or less limbs seems out of the question unfortunately.",
              "score": 5,
              "created_utc": "2025-12-30 18:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws8lv5",
                  "author": "Illustrious-Lake2603",
                  "text": "Thank you! Its still beneficial. I can probably make it work along side puppeteer to be able to get better humanoid animations (if they are good).",
                  "score": 1,
                  "created_utc": "2025-12-30 19:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrumui",
              "author": "fiddler64",
              "text": "yeah, and there's only 1 predetermined rig, I tried prompting a dog chasing after ball and it shows the humanoid rig throwing the ball instead",
              "score": 2,
              "created_utc": "2025-12-30 18:27:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpqo9r",
          "author": "KingDutchIsBad455",
          "text": "Is this what Neuro uses?",
          "score": 17,
          "created_utc": "2025-12-30 11:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrnwwa",
              "author": "inaem",
              "text": "The sitting looks the same tbh",
              "score": 1,
              "created_utc": "2025-12-30 17:57:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwqg3rd",
              "author": "Emotional-Metal4879",
              "text": "also wander",
              "score": 0,
              "created_utc": "2025-12-30 14:24:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqhx2d",
          "author": "no_witty_username",
          "text": "This is more cool then folks realize.  Soo many uses for this.",
          "score": 10,
          "created_utc": "2025-12-30 14:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrywy0",
              "author": "WitAndWonder",
              "text": "Just a pity that it was basically exclusively human motion. If it also covered quadruped we'd be really in business since that's most of the creatures that get put into games, too.",
              "score": 4,
              "created_utc": "2025-12-30 18:47:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrviwp",
              "author": "Crypt0Nihilist",
              "text": "Don't leave us hanging!",
              "score": -1,
              "created_utc": "2025-12-30 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu533b",
          "author": "JonatasLaw",
          "text": "Now I can work in my game hahaha",
          "score": 2,
          "created_utc": "2025-12-31 01:22:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufjmc",
          "author": "TanguayX",
          "text": "Wow. Impressive",
          "score": 1,
          "created_utc": "2025-12-31 02:22:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv7b53",
          "author": "Specific-Strain7970",
          "text": "This looks super interesting! (I'm a beginner) What are the models I can apply these animations to? Would stock mixamo models work directly? I'm thinking Unity. Thanks for the help in advance!",
          "score": 1,
          "created_utc": "2025-12-31 05:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww00ht",
          "author": "WEREWOLF_BX13",
          "text": "This is exactly what I've been waiting for! Soon enough anyone will be able to create anything at mass scale and say goodbye to shitty company slop!",
          "score": 1,
          "created_utc": "2025-12-31 09:27:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdhc1k",
          "author": "Bl_grill",
          "text": "Its alright but until it can walk through a door, place hand on door way frame and open door then walk through with face capture in real time and have the person react absolutely devastating.\n\n\nBut this is for npc, or repetitious situations, i can see what tencent is doing they're focusing on repetitive tasks, to free up development.\n\n\nChinese developerd can focus(including others too)to working on speed and efficiency, while professionals focus on mocap.\n\n\nWhat i cannot wait for is ai reacting to its environment and the gamer drops in, and it hunts the gamer and mocks them(yes yes, this game that game has it)but i thought such technology was gonna be embedded in consoles ai chip for neural processing for developers freeing up stricted ai.\n\n\nOverall seeing ai stumble and get frustrated as you shoot it coming back reminding you, almost like the nemisis engine from lord of rings.\n\n\nOverall helpful but no game changer just a pipeline implementation on tencents roadmap.",
          "score": 1,
          "created_utc": "2026-01-03 03:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpxlvk",
          "author": "paryska99",
          "text": "Do they also release any finetuning code? This could be really cool for 3D artists or for games with generative content.",
          "score": 1,
          "created_utc": "2025-12-30 12:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtq6ff",
              "author": "Erdeem",
              "text": "Not yet",
              "score": 1,
              "created_utc": "2025-12-30 23:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwsprse",
          "author": "RedZero76",
          "text": "I just shat myself.  This is GOLD.",
          "score": 1,
          "created_utc": "2025-12-30 20:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb034t",
      "title": "GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/deepseek-ai/Engram/tree/main",
      "author": "TKGaming_11",
      "created_utc": "2026-01-12 16:49:22",
      "score": 324,
      "num_comments": 77,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nza5nt9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 02:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz83vji",
          "author": "FullOf_Bad_Ideas",
          "text": "Another great paper from DeepSeek team. They never disappoint when it comes to original ideas.\n\nEdit: finished it. They use model with mHC (ð‘€ = 4) for ablations, meaning that they probably derisked mHC for the next run and see this as \"current stable meta\". And they claim \"We envision conditional memory functions as an indispensable modeling primitive\nfor next-generation sparse models.\", so I think there's a high chance that the model they'll release next will have both of those things included. I'd assume that their next-gen model is in training right now, and they were using this free time to polish off the papers and release them.\n\nAlso, if this will be adopted, it's great news for us. Models that will have Engram, will be more performant per parameter for traditional MoE architecture, and they'll have a big new part that will be easily offloadable to RAM with no performance penalty at all. So a 40B A3.8B MoE from their ablation tests would need only 27B of weights to be placed on fast memory, with the remaining 13B being comfy in RAM or maybe even 95% offloaded to NVMe.\n\nI really love their innovations, they are a great example of an AI lab that applies resources into practical systemic solutions that quickly and successfully land in final products, they have really outstanding impact.\n\nAnother thing - they're using Muon as optimizer for those ablations. Which means, next-gen will probably be trained with Muon and not AdamW. Just like Kimi K2 and GLM 4.5",
          "score": 81,
          "created_utc": "2026-01-12 20:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9l1mu",
              "author": "Old-School8916",
              "text": "i think v4 is coming out next month, I wonder if it'll have this shizz.",
              "score": 21,
              "created_utc": "2026-01-13 00:43:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzallqp",
                  "author": "TheRealMasonMac",
                  "text": "Ngl, I'm praying for good multi-turn long context. K2-Thinking/GLM go down to 1 IQ after enough turns in the agentic loop.",
                  "score": 4,
                  "created_utc": "2026-01-13 04:02:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzbaldr",
              "author": "Mikasa0xdev",
              "text": "Sparsity is the new density for LLMs.",
              "score": 3,
              "created_utc": "2026-01-13 07:04:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzal9hk",
              "author": "ai-infos",
              "text": "\"they'll have a big new part that will be easily offloadable to RAM with no performance penalty at all\" >>>  if true, that would be really really BIG!\n\nand also, that would explain partially the crazy prices of RAM... (i guess closed AI labs already knew about it and already implemented equivalent architecture using mix of RAM/VRAM in their infra and so that explains the BIG need in RAM for potential Trillons parameters MoE models...)",
              "score": 7,
              "created_utc": "2026-01-13 04:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzc0p3e",
                  "author": "FullOf_Bad_Ideas",
                  "text": "I think RAM prices don't have Engram priced in, and it should not affect them by much. RAM is probably used the most for kv cache offloading and during training, and each machine gets a lot of it even if it won't be used, just because it's cheaper than vram and sometimes it'll turn out you wanted to have that RAM there.\n\n> if true, that would be really really BIG!\n\nThe caveat there is that it works best in terms of pretraining compute utilization when Engram makes up about 20% of the total model parameters. So in makes more economic sense to train 100B A10B E20B model where that offloading helps just a bit but here for running models locally on gpus with cpu offload we'd profit the most from crazy Engram ratios like 100B A10B E80B. And those are not as compute efficient to train, and they will perform worse than normal 100B models. So it has potential but that potential might not be practically explored by companies training those models, since they usually have local inference as an after thought, and they prioritize training the best model possible with limited compute.\n\nEdit: grammar",
                  "score": 3,
                  "created_utc": "2026-01-13 11:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzbc9ok",
                  "author": "Nyghtbynger",
                  "text": "We'll offload it to NVMe !!",
                  "score": 3,
                  "created_utc": "2026-01-13 07:19:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzn95yx",
                  "author": "DerDave",
                  "text": "Nope. RAM prices are high, because all capacity (both DRAM and VRAM) is completely overbooked. Thank Sam for this...",
                  "score": 0,
                  "created_utc": "2026-01-15 00:52:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzbiytw",
              "author": "zball_",
              "text": "maybe even offloadable to ssd.",
              "score": 1,
              "created_utc": "2026-01-13 08:21:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzp79zm",
              "author": "Mnode-Lab",
              "text": "Great analysis. I want to add one angle on why the CPU-side memory offloading here matters more than it might look at first glance.\n\nThis direction isnâ€™t unique to DeepSeek. Weâ€™ve seen related ideas before â€” Gemmaâ€™s per-layer embeddings, RWKVâ€™s deepembed, ByteDanceâ€™s UltraMem, etc.\n\nFrom a pure algorithm perspective, hash-based n-gram lookup is obviously not ideal. The same fact phrased differently (or in another language) maps to different keys, so generalization is weak and redundancy/noise are hard to avoid. UltraMem tries to fix this with learnable mappings, but that adds parameters and makes the system harder to tune.\n\nWhat DeepSeek seems to be doing instead is a system-level trade-off. Rather than chasing a cleaner algorithm, they simplify the computation and push it *before* inference: raw input tokens, simple lookup, and run the whole thing in CPU memory. You lose algorithmic elegance, but you get zero GPU memory usage, very simple logic, and a preprocessing step that can be fully offloaded to CPUs.\n\nOnce this lives in CPU memory, the optimization target changes. Parameter efficiency and per-query optimality matter less. Even if the hash table is noisy or redundant, itâ€™s cheap and doesnâ€™t touch scarce GPU memory. At the system level, that trade-off makes a lot of sense â€” especially for cloud inference where CPU resources are relatively abundant.\n\nFor local deployment, this could be a big deal. If something like the 13B Engram component can sit in RAM while the 27B MoE part stays in VRAM, thatâ€™s a much more accessible setup for consumer hardware.",
              "score": 1,
              "created_utc": "2026-01-15 09:15:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6uky3",
          "author": "Rokpiy",
          "text": "the n-gram embedding approach is interesting. most models only scale via MoE (neural computation), but engram adds static memory as a complementary sparsity axis with O(1) lookup\n\nthey found a u-shaped scaling law between MoE and Engram, which guides how to allocate capacity between the two. analysis shows it relieves early layers from static pattern reconstruction, preserving depth for complex reasoning\n\ndeterministic addressing means they can offload the embedding tables to host memory without much inference overhead",
          "score": 48,
          "created_utc": "2026-01-12 16:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9ypkx",
              "author": "Punsire",
              "text": "Damn, thank you. I could understand more about each thing you explained by virtue of the relations to each other component without you having to explicitly describe their part and function .",
              "score": 4,
              "created_utc": "2026-01-13 01:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nza6pz3",
                  "author": "Rokpiy",
                  "text": "Glad it helped :)",
                  "score": 2,
                  "created_utc": "2026-01-13 02:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz8sn6q",
          "author": "Few_Painter_5588",
          "text": "Perhaps this is the breakthrough that Deepseek made and will roll out for Deepseek V4? M",
          "score": 10,
          "created_utc": "2026-01-12 22:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7c0ix",
          "author": "TransportationSea579",
          "text": "we're getting out of the MPC server with this one chooms",
          "score": 21,
          "created_utc": "2026-01-12 18:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb8qim",
              "author": "Nyghtbynger",
              "text": "Saw a few diagrams, looks like another object oriented programming but I never really checked what a MPC is. Should I just skip it ?",
              "score": 3,
              "created_utc": "2026-01-13 06:48:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdvc2d",
                  "author": "TransportationSea579",
                  "text": "I was just making a cyberpunk joke after I saw engram lol",
                  "score": 1,
                  "created_utc": "2026-01-13 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz828nj",
          "author": "__Maximum__",
          "text": "When you think about it, this was such an obvious thing to do, in hindsight, of course.\n\nI am pretty sure all animals do this kind of stuff in their brain, even humans.",
          "score": 16,
          "created_utc": "2026-01-12 20:11:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8ok4l",
              "author": "menictagrib",
              "text": "The hippocampus anchors recent (relatively) events in space and time via sparse coding to maintain orthogonality. This is effectively how most \"new information\" is initially stored, often using these systems for months/years.",
              "score": 8,
              "created_utc": "2026-01-12 21:55:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz99s6c",
          "author": "Aaaaaaaaaeeeee",
          "text": "Introducing deeper-seeker, a 3T reasoning model with 600B ngram parameters, 150+ layers, 2.4T, 70A and my condolences to your RAM outage.",
          "score": 8,
          "created_utc": "2026-01-12 23:43:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9jhry",
              "author": "FullOf_Bad_Ideas",
              "text": "We'll probably be keeping engram params on NVMes.\n\nI don't think it'll be much bigger. Expert serving complexity and scaling laws show that around A30B is a good tradeoff, and around 1/32 is a good sparsity. So I think i'll be around 1T with 200B engram params.",
              "score": 8,
              "created_utc": "2026-01-13 00:35:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbmydv",
              "author": "martinerous",
              "text": "One day they will evolve from seeker to finder....",
              "score": 1,
              "created_utc": "2026-01-13 08:59:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbfm43",
          "author": "Vivarevo",
          "text": "Vram embargo on china is turning out to be the catalyst for innovation.\n\nElsewhere mega models fit in to enterprise servers. Consuming vast resources and remain out of reach for majority of potential users.\n\nThats at least the feel of things as they currently stand",
          "score": 8,
          "created_utc": "2026-01-13 07:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzar6u3",
          "author": "Determined-Hedgehog",
          "text": "I am not saying I am dumb but could someone simplify this for me so that I can get it easier? I have been away from the local scene working recently.",
          "score": 5,
          "created_utc": "2026-01-13 04:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz72yv5",
          "author": "astronomikal",
          "text": "Iâ€™ve got 0(1) with no GPU!\n\nI was doing some fun things with n-gram filters a few months ago but found a better way for persistent memory. This is awesome for its use case tho.",
          "score": 14,
          "created_utc": "2026-01-12 17:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9o6ji",
              "author": "pixelpoet_nz",
              "text": "That's a zero and not an O :D",
              "score": 13,
              "created_utc": "2026-01-13 01:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nza5bur",
                  "author": "astronomikal",
                  "text": "Was partially doing this via voice to text lmao.",
                  "score": 3,
                  "created_utc": "2026-01-13 02:33:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz90bzs",
              "author": "jazir555",
              "text": "My dude over here beating major research labs by months.",
              "score": 9,
              "created_utc": "2026-01-12 22:53:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzasvq1",
                  "author": "astronomikal",
                  "text": "I just had a random idea one day to do some funky stuff with kernels. Iâ€™ll dig them up and throw the good ones up in a repo tomorrow after work.\n\n*sigh* ~~false alarm... approximately 5 months ago i had to rebuild the entire project again from scratch after my stubbornness to not use github bit me in the ass with a mistaken force removal of my whole codebase. It was a lesson learned but i guess the kernels i had made ended upthere. I can try and dig them up another way but it will take some time~~\n\nI FOUND THEM! uploading now.",
                  "score": 3,
                  "created_utc": "2026-01-13 04:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzbarga",
                  "author": "Nyghtbynger",
                  "text": "We should make a leaderboard of \"I called it\" and then allocate winners based on papers",
                  "score": 1,
                  "created_utc": "2026-01-13 07:05:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9jjcf",
              "author": "polawiaczperel",
              "text": "Can you tell something more about it?",
              "score": 5,
              "created_utc": "2026-01-13 00:35:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzarv2u",
                  "author": "astronomikal",
                  "text": "The memory system or my use of n-gram filters?",
                  "score": 1,
                  "created_utc": "2026-01-13 04:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz91qa3",
          "author": "Tiny_Arugula_5648",
          "text": "I'd love to see what effect larger ngrams would have. Code and math should improve at 5.. why not load up the CPU ram? They seemed pretty conservative in the limits they chose.",
          "score": 3,
          "created_utc": "2026-01-12 23:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz936kl",
              "author": "zjuwyz",
              "text": "They briefly mentioned it at the end of Section 6.2. 4-gram didn't perform better than 3-gram. After all, this is a hash table, not a dictionary. There are too many combinations of four consecutive tokens, and the proportion of meaningful semantic entities is very low.",
              "score": 8,
              "created_utc": "2026-01-12 23:07:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz966fo",
          "author": "maxpayne07",
          "text": "Will this allow,  lets say, off-load to SSD disk without losing inference speed? \n\nIf then, its going to be awesome, image you can off-load a 400B parameters to a not so good PC.",
          "score": 6,
          "created_utc": "2026-01-12 23:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9k5ix",
              "author": "FullOf_Bad_Ideas",
              "text": "yes, there will be a part of the model that will have predictable low bandwidth ultra-sparse parameters. But not the whole model, just some of it.\n\nin their tests they did 4B model and 100B engram for example.\n\nSo you'd load 4B to VRAM, taking around 5GB with KV Cache assuming FP8 native training, you'd load some hot section of engram to RAM, let's say 20GB, and you'd load the remaining 80GB from NVMe on demand. And performance would be on the order of that of a 10B model which would require 11GB of VRAM (just guessing this one).",
              "score": 12,
              "created_utc": "2026-01-13 00:38:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbdwli",
                  "author": "shing3232",
                  "text": "The great thing about engram is that it's cheap to pretrained and good for long context.\n\nit greatly improve model â€˜s world knowledge",
                  "score": 4,
                  "created_utc": "2026-01-13 07:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9rtr4",
              "author": "Several-Tax31",
              "text": "Is this true? The idea of running a 400-500B model on a potato gives me more goosebumps than anything else. I want to run those SOTA models locally, please!Â ",
              "score": 10,
              "created_utc": "2026-01-13 01:20:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbyvht",
                  "author": "FullOf_Bad_Ideas",
                  "text": "If they decide to allocate training budget to a giant engram pool, it should scale and work. And we could end up with 400B A5B E370B models that have only 30B traditional parameters. But this model would be as hard to train as a 400B A5B non-Engram model would, while having performance less to that of a 400B MoE without Engram, so it would not be optimal from the perspective of efficient pretraining. It would be very cheap to deploy though, compared with other models of similar performance. I don't think Deepseek will train a small MoE with big engram, they're focused on SOTA that is cheap to train and serve at scale. So, this could become a reality only if their competitors like Zhipu or Tencent pick it up and focus on this.",
                  "score": 3,
                  "created_utc": "2026-01-13 10:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz9iwrj",
          "author": "zball_",
          "text": "It's conceptually similar to Gemma-3n's Per Layer Embedding, but extended to n-gram.",
          "score": 2,
          "created_utc": "2026-01-13 00:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc2pp9",
          "author": "power97992",
          "text": "So the prediction was correct, a >1.5 Trillion param ds model is coming.",
          "score": 2,
          "created_utc": "2026-01-13 11:26:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc3f79",
          "author": "power97992",
          "text": "I wonder will this pave the road for continual training during inferenceâ€¦? Maybe one day switchable engrams",
          "score": 2,
          "created_utc": "2026-01-13 11:32:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8u2mv",
          "author": "Interpause",
          "text": "Reminds me of embedding patches like in BLT, but iven't read either paper deep enough to know the difference",
          "score": 1,
          "created_utc": "2026-01-12 22:22:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbkdtw",
          "author": "aragorn__gondor",
          "text": "\n\nLIMIT paper (Aug 2025) exposes dense embedding collapse. I built Numen (Nov 2025): char n-gram hashing â†’ 32k-dim dense vectors, no training, 93.9% R@100 > BM25 on LIMIT\n\nDeepSeek Engram (Jan 12, 2026) does similar inside LLMs: hashed token n-grams for conditional memory : massive gains\n\nBeautiful convergence: hashed n-grams fix both external retrieval limits AND internal Transformer memory waste. Numen proves it works externally without training.Â \n\n\n\nLink to mine implementation:\n\n[https://github.com/sangeet01/limitnumen](https://github.com/sangeet01/limitnumen)\n\n\n\nDeepseek's implementation:\n\n[https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)\n\n\n\nLIMIT DATASET:Â \n\n[https://huggingface.co/datasets/orionweller/LIMIT](https://huggingface.co/datasets/orionweller/LIMIT)",
          "score": 1,
          "created_utc": "2026-01-13 08:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbvi2k",
          "author": "RealAnonymousCaptain",
          "text": "I'm worried with how engram works as it seems like it'll cause models to be more susceptible to data biases or contamination. If ngram retrieves conditional memory based two to three word sequences, that just leads to more efficiency but less flexibility in its output.\n\n\nBut I'm not too well-versed in the technical details, so if anyone could elaborate itd be cool",
          "score": 1,
          "created_utc": "2026-01-13 10:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbxpgw",
              "author": "FullOf_Bad_Ideas",
              "text": "It will lead to more biases. But being more susceptible to biases in data means lower loss and higher performance. LLMs imitate the biases of the training data. If they didn't, they wouldn't be that useful. Knowledge is largely stereotyped.\n\nI don't see how it would lead to contamination. Don't put benchmark datasets in the training data and you'll avoid contamination, model architecture doesn't determine how likely contamination is.",
              "score": 1,
              "created_utc": "2026-01-13 10:42:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcqz4q",
                  "author": "RealAnonymousCaptain",
                  "text": "Sorry, I meant more susceptible to contaminated/flawed data. I was writing while distracted and running on fumes so my grammar is bad right now.\n\nBut I disagree with your point about training data, yes they are trained to follow them and are inherently biased. But I'm talking about false biases and illogical data in them like the recent seahorse/igloo/traffic cone emoji blunder where that's present in several AI models. I'm worried that engram will make Deepseek's newer models to be significantly less factually correct or have more errors in it's output because of flawed data.",
                  "score": 1,
                  "created_utc": "2026-01-13 14:05:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzc8gjq",
          "author": "Legumbrero",
          "text": "Wonder if you could quantize the engram part of the model aggressively while leaving the moe's at a higher precision and see good results.  Architecture seems like a good candidate for mixed precision.",
          "score": 1,
          "created_utc": "2026-01-13 12:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeebpx",
          "author": "jashAcharjee",
          "text": "Engram? CyberPunk 2077 reference?",
          "score": 1,
          "created_utc": "2026-01-13 18:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbkx5z",
          "author": "ninadpathak",
          "text": "This is fascinating work on conditional memory. What I'm taking away here is that selective memory retrieval is better than raw context windows (obviously) on both latency and cost metrics. \n\nA few interesting angles:\n\n1. **The sparsity aspect** \\- only loading relevant memory indices is clever. This is why memory layers are becoming essential in production LLM systems.\n2. For anyone implementing this, the real challenge is the semantic ranking problem. How do you decide what's \"relevant\" without scanning everything?\n3. **Scale problem** \\- this works well until your memory corpus grows to millions of tokens. Then you hit vector DB performance walls.\n\nIf anyone's building systems around this, we started a sub to discuss these exact tradeoffs over at r/mem0 and also to try and make the product even better for everyone. \n\nHop on over if you think that interests you!",
          "score": 1,
          "created_utc": "2026-01-13 08:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz91mz4",
          "author": "VampiroMedicado",
          "text": "/u/AskGrok explain this for 5 years old.",
          "score": -7,
          "created_utc": "2026-01-12 22:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9rse7",
          "author": "Better_Story727",
          "text": "DeepSeek's contribution is truly groundbreaking.\n\nIt doesnâ€™t just achieveÂ **infinite context**; it paves the way for a clean architectural separation between dedicatedÂ **memory models**Â andÂ **reasoning models**. This decoupling will drastically enhance training efficiency.\n\nConsider the implications if what we store isn't just \"memory,\" butÂ **operators**. Given that multi-dimensional continuous parameters treat memory and operators as two sides of the same coin, this opens the door forÂ **ultra-deep, ultra-compact computational subsystems**.\n\nBy outsourcing memory, the context window could shrink dramatically. In a network where memory is entirely externalized, the \"context\" effectively disappears, allowing for aÂ **fully parametric (context-less) neural network**.\n\nFurthermore, if memory retrieval becomes deterministic, we can eliminate the \"computational bubble\" (overhead). This leads us towardÂ **brain-like hardware**: pure computation with zero data movement, potentially reaching energy efficiency levelsÂ $10\\^4$Â toÂ $10\\^7$Â times higher than current architectures.\n\nDeepSeek didn't invent this direction, but by making it an engineering reality, they have fundamentally accelerated the trajectory of AI.",
          "score": -14,
          "created_utc": "2026-01-13 01:20:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9xjfc",
              "author": "Redoer_7",
              "text": "Pure slop and not true \"infinite context \"",
              "score": 13,
              "created_utc": "2026-01-13 01:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzajgql",
              "author": "INtuitiveTJop",
              "text": "Not only did I like your comment, but it received a well versed upvote. Truly spectacular!",
              "score": 2,
              "created_utc": "2026-01-13 03:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q41bw1",
      "title": "GLM-Image model from Z.ai is coming",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/sm31vizebebg1.png",
      "author": "Ravencloud007",
      "created_utc": "2026-01-04 20:54:04",
      "score": 321,
      "num_comments": 58,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxq2ns7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-05 00:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp1wkh",
          "author": "Tootsie_Rolls_Fan",
          "text": "I can feel it, 103b parameters",
          "score": 65,
          "created_utc": "2026-01-04 21:08:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpsqsi",
              "author": "Freonr2",
              "text": "I'd legit love to see a good MOE image model.  Hunyuan 80b was pretty meh.\n\nBonus points for native mxfp4 or nvfp4 weights.",
              "score": 15,
              "created_utc": "2026-01-04 23:15:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq7tr2",
                  "author": "SanDiegoDude",
                  "text": "Hunyuan 80b was seriously impressive. It's just a friggen cow that nobody can run on home equipment (unless seriously compromising its output). It also had a bit of a nugget problem, though that could have been tuned out with community fine tuning... shame the community doesn't have a bunch of H200s lined up to do that tuning though, so into the pile of forgotten models it goes. Flux2 is pretty much in the same boat, even with the turbo loras that just dropped recently. Too big and too slow to be worthwhile.",
                  "score": 6,
                  "created_utc": "2026-01-05 00:30:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxsn7ct",
                  "author": "stoppableDissolution",
                  "text": "God please no. Not another monster thats not runnable at home.",
                  "score": 1,
                  "created_utc": "2026-01-05 10:29:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxp5w48",
              "author": "ShengrenR",
              "text": "And what does that feel like?",
              "score": 9,
              "created_utc": "2026-01-04 21:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxp80a9",
                  "author": "Admirable-Star7088",
                  "text": "That question has to be passed to our VRAM.",
                  "score": 29,
                  "created_utc": "2026-01-04 21:37:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxp8n7e",
                  "author": "misterflyer",
                  "text": "better than sex ðŸ’¯",
                  "score": 6,
                  "created_utc": "2026-01-04 21:39:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqecu7",
                  "author": "no_witty_username",
                  "text": "https://imgs.search.brave.com/21_jbxyeN_5e0DBVWb969Qp_0F9SLKFrrg30vz8TVCQ/rs:fit:500:0:1:0/g:ce/aHR0cHM6Ly93d3cu/bWVtZS1hcnNlbmFs/LmNvbS9tZW1lcy8y/ZDZkZTkxMWI2NTM4/YmU5ZWJkNzY4OTMz/NTg2Mjc0Yy5qcGc",
                  "score": 3,
                  "created_utc": "2026-01-05 01:03:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp1fca",
          "author": "nomorebuttsplz",
          "text": "right now Z image is the clear community favorite. Will take a lot to dethrone it",
          "score": 41,
          "created_utc": "2026-01-04 21:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpg01o",
              "author": "remghoost7",
              "text": "It'll have to be a relatively light model too.  \nFlux2 was *obliterated from orbit* by Z-Image-Turbo because of the speed and hardware requirements.\n\nI'm game for another \"competitor\" in the image generation space though.",
              "score": 36,
              "created_utc": "2026-01-04 22:14:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxph9j4",
                  "author": "Novel-Mechanic3448",
                  "text": "Flux would have been fine if they didn't spend more time censoring it than training it but here we are",
                  "score": 44,
                  "created_utc": "2026-01-04 22:20:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxptg6q",
                  "author": "Freonr2",
                  "text": "Yeah unless you want to nail complex text every time, ZIT is nearly as good at a tiny fraction of the size and compute.",
                  "score": 3,
                  "created_utc": "2026-01-04 23:19:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxpvsg8",
                  "author": "Environmental-Metal9",
                  "text": "Maybe, but when Flux came around and dethroned SDXL it was orders of magnitude larger than it. (Granted stabilityai wasnâ€™t really interested in actually competing).",
                  "score": 2,
                  "created_utc": "2026-01-04 23:30:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxptag3",
              "author": "Freonr2",
              "text": "ZIT is pretty incredible for its size/speed. We're still waiting on Z image base.",
              "score": 16,
              "created_utc": "2026-01-04 23:18:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxr7s65",
                  "author": "wh33t",
                  "text": "I would like to see if zimage-edit can work better than qwen image edit.",
                  "score": 5,
                  "created_utc": "2026-01-05 03:42:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxqalhs",
                  "author": "SlaveZelda",
                  "text": "I think it might come before/around chinese new year",
                  "score": 2,
                  "created_utc": "2026-01-05 00:43:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxq1nfb",
              "author": "TAW56234",
              "text": "That remains to be seen. Even with the stab preset, I have to worry about refusals in GLM4.7 thinking. Its gotten to the point it's not worth using. Fair chance you can't do NSFW here",
              "score": 1,
              "created_utc": "2026-01-05 00:00:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxq2da5",
                  "author": "nomorebuttsplz",
                  "text": "Do you have API? can you just pause, edit and continue the thinking process?",
                  "score": 1,
                  "created_utc": "2026-01-05 00:03:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxusyu7",
                  "author": "Novel-Mechanic3448",
                  "text": "I've never gotten a refusal with GLM4.7 using it locally.",
                  "score": 0,
                  "created_utc": "2026-01-05 17:56:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpwbut",
          "author": "Environmental-Metal9",
          "text": "My only question is how many datacenters do I need to rent to be able to use this new model.\n\nI yearn for a model as small as SD1.5, as easy to finetune as current day SDXL, and with great quality like flux or some of the newer ones. But us GPU poors get nothing! Not even 1 out of 3 in this matrix",
          "score": 12,
          "created_utc": "2026-01-04 23:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4coe",
              "author": "SlowFail2433",
              "text": "I mean Flux quality in under 1B parameters like SD 1.5 just isnâ€™t possible yet or ever maybe",
              "score": 1,
              "created_utc": "2026-01-05 00:13:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxs3k6n",
          "author": "turklish",
          "text": "Still waiting on AIR...",
          "score": 4,
          "created_utc": "2026-01-05 07:25:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp74os",
          "author": "coder543",
          "text": "I asked `codex` to review this branch. It sounds like fun.\n\n> This branch adds a new model family: glm_image (GLMâ€‘Image). Itâ€™s a multimodal image-token generator built around a text decoder + vision stack + VQâ€‘VAE, aimed at textâ€‘toâ€‘image and imageâ€‘toâ€‘image token generation (for downstream DiT/diffusion), not a general chat-style multimodal LLM.\n> \n>  Whatâ€™s implemented vs origin/main:\n\n>  - New model package: src/transformers/models/glm_image/* with configs, modeling, processor, image processors (slow + fast), and a modular source file. src/transformers/models/glm_image/modular_glm_image.py and generated src/transformers/models/glm_image/modeling_glm_image.py are the core.\n>  - VQâ€‘VAE + vision + text components and a GlmImageForConditionalGeneration generation head (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - Processor that fuses image + text, inserting image tokens and handling multiâ€‘image grids (src/transformers/models/glm_image/processing_glm_image.py).\n>  - Autoâ€‘mapping hookups for config/model/processor/tokenizer + docs + tests (docs/source/en/model_doc/glm_image.md, tests/models/glm_image/test_modeling_glm_image.py).\n\n>  Why itâ€™s not a generic multimodal LLM:\n\n>  - The model explicitly handles image token regions and image grids with start/end markers and 3â€‘axis RoPE for generation of image tokens, including textâ€‘toâ€‘image and imageâ€‘toâ€‘image flows (src/transformers/\n>  models/glm_image/modeling_glm_image.py).\n>  - It embeds a VQâ€‘VAE (GlmImageVQVAE) to tokenize/quantize images and uses those tokens in generation (src/transformers/models/glm_image/modeling_glm_image.py).\n>  - The usage example in docs describes â€œgenerate vision token for DITâ€ (docs/source/en/model_doc/glm_image.md), which is imageâ€‘generation oriented.\n\n>  So: this branch is implementing GLMâ€‘Image, a multimodal model centered on image token generation (textâ€‘toâ€‘image and imageâ€‘toâ€‘image), not a general-purpose multimodal chat LLM.",
          "score": 6,
          "created_utc": "2026-01-04 21:33:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpu3t0",
              "author": "thatsnot_kawaii_bro",
              "text": "ok",
              "score": -4,
              "created_utc": "2026-01-04 23:22:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp234g",
          "author": "Betadoggo_",
          "text": "Not a whole lot of information.\n\nhttps://preview.redd.it/cesoduf2eebg1.png?width=1410&format=png&auto=webp&s=39437b143429602c03fd456599179b601ada9c60\n\nAll I can gather based on the files is that the max input resolution is 2048x2048.",
          "score": 6,
          "created_utc": "2026-01-04 21:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp2tck",
              "author": "nilpy",
              "text": "From the RoPE section ([https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm\\_image/modular\\_glm\\_image.py#L794](https://github.com/huggingface/transformers/blob/cd8d78fcb4067979e921b20163d62035c51b4e7f/src/transformers/models/glm_image/modular_glm_image.py#L794)):\n\n`=== Case 1: Image-to-Image Generation (single or multiple source images + 1 target image_grid) ===`\n\n`...`\n\n`=== Case 2: Text-to-Image Generation (no source images + 2 image_grids for multi-resolution) ===`\n\nSeems to be based on GLM4-V (MoE?). Has references to both DiT and VQVAE. It's possibly using NTP over discrete image tokens? That or something like show-o with discrete diffusion.",
              "score": 9,
              "created_utc": "2026-01-04 21:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxp0yl7",
          "author": "__Maximum__",
          "text": "Meaning image generator?",
          "score": 2,
          "created_utc": "2026-01-04 21:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs9xtg",
          "author": "paperbenni",
          "text": "Please no, image models are useless compared to LLMs. GLM 4.7 can do real work, what am I going to do with an image model? That entire part of the AI industry should just die. These things are impressive, but all people are doing with it is memes and misinformation. Any compute going into this instead of GLM 5 is thoroughly wasted.",
          "score": -9,
          "created_utc": "2026-01-05 08:24:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1p5q5",
      "title": "Getting ready to train in Intel arc",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1q1p5q5",
      "author": "hasanismail_",
      "created_utc": "2026-01-02 04:33:19",
      "score": 312,
      "num_comments": 91,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx8jin6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 11:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7e5iq",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 129,
          "created_utc": "2026-01-02 05:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7z858",
              "author": "dsanft",
              "text": "I have a lot of great things to say about the ADT link pcie risers, the ones with the shielded silver cables. I run them in pcie 3 4x and even at lengths up to 80cm I've had no problems.",
              "score": 15,
              "created_utc": "2026-01-02 08:09:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7zf7z",
              "author": "TheRealMasonMac",
              "text": "Gamers rose up.",
              "score": 3,
              "created_utc": "2026-01-02 08:11:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7m3mn",
          "author": "Techngro",
          "text": "Dude, you can't post stuff like this without details.",
          "score": 34,
          "created_utc": "2026-01-02 06:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9cpil",
              "author": "hasanismail_",
              "text": "Sorry was too excited when posting\n\n8x b580 GPUs (1 is not in picture I was playing a game at the time and needed it)\n\nDual Intel e5 Xeon v4 CPUs (forgot exact model)\n\n128gb ddr4 (I bought before the ram crisis)\n\nDual 850w corsair PSUs\n\n\nServer will run in Ubuntu latest release with the Intel patches and I'm gonna use vulkan and probably train with pytorch or something (I haven't thought that far ahead)\n\nI paid 200$-240$ per GPU mostly from micro center deals Facebook marketplace and I was able to snag some off amazon too I was planning on using the b50 but the memory band with is very slow compared to the b580 and the value proposition of the b580 is just too good to pass up",
              "score": 28,
              "created_utc": "2026-01-02 14:40:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxa39do",
                  "author": "satireplusplus",
                  "text": "Not sure what exactly you plan on training with pytorch, but the vulkan backend is extremely poor and non-functional for that. It contains a few functions, just barely enough to run object detection on android. Intel does have a special pytorch version with xpu support though (through their own intel-one stack). Report back what you can do with it, but it's not gonna be as smooth as CUDA or even rocm.",
                  "score": 6,
                  "created_utc": "2026-01-02 16:51:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9gbxe",
                  "author": "KoalaRashCream",
                  "text": "How many TOPS",
                  "score": 5,
                  "created_utc": "2026-01-02 15:00:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxbtmzc",
                  "author": "autistic-brother",
                  "text": "What mother board did you use?\n\nHow are you planning on using this for training?",
                  "score": 1,
                  "created_utc": "2026-01-02 21:47:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9wrw7",
                  "author": "shrug_hellifino",
                  "text": "And still, you make people look up and calculate what your total VRAM would be... these are 16GB cards? so,128?",
                  "score": 0,
                  "created_utc": "2026-01-02 16:21:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxa9yj0",
                  "author": "FullstackSensei",
                  "text": "Which motherboard? E5 means you'll be running PCIe Gen 3 and rebar support will most likely need to be patched in BIOS. You'll have a bad time using those cards without it.\n\nIf you can find one for cheap, snag a supermicro X10DRX. You get ten X8 slots. It doesn't have an M.2 slot but supports NVMe in any of the PCIe slots. I have a Samsung PM1725a in mine and it boots without any issues.",
                  "score": 0,
                  "created_utc": "2026-01-02 17:22:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7jvqq",
          "author": "CheatCodesOfLife",
          "text": "Nice! To save yourself some of the pain ahead, go with Ubuntu 24.04\n\nGood news is unsloth seems to support Intel Arc now.\n\nYou'll probably want to join the OpenArc discord when you set this up.",
          "score": 46,
          "created_utc": "2026-01-02 05:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8bp1z",
              "author": "Jokerit208",
              "text": "Why Ubuntu 24.04?",
              "score": 12,
              "created_utc": "2026-01-02 10:08:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8p2w3",
                  "author": "AI_is_the_rake",
                  "text": "To prevent pain, apparentlyÂ ",
                  "score": 25,
                  "created_utc": "2026-01-02 12:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9bu4q",
              "author": "hasanismail_",
              "text": "Thx I tried this last year with 2 cards and it was a pita on Linux a link to that discord server would be nice I have a feeling I'm gonna need it",
              "score": 3,
              "created_utc": "2026-01-02 14:36:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe3w72",
                  "author": "Echo9Zulu-",
                  "text": "My project  https://github.com/SearchSavior/OpenArc\n\nand our discord https://discord.gg/vS5ANSy3a",
                  "score": 2,
                  "created_utc": "2026-01-03 05:49:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxebfum",
                  "author": "Echo9Zulu-",
                  "text": "Yes we can help you get situated. For training you'll want to use xpu nightly with accelerate; ipex optimizations are being upstreamed there. Ipex is in end of life. Llm-scaler and vllm xpu 11 are also an absolute must. OpenArc supports multi gpu pipeline paralell atm via openvino but performance characteristics of 8 gpus remains unknown (!). We can help you cook some large  quants based on what's currently supported.\n\nThe absolute unit guy who maintains sycl backend joined a few months ago. He is intel engineer who develops sycl. His help has been invaluable in navigating high complexity issues. Very fortunate to have him as a resource since all pytorch xpu kernels are written in sycl. choosing a slightly older model as target architecture where the implementations ard more mature. Think llama 3.3, qwen2.5/qwen3. Intel is putting massive resources into battlemage and it's likely that the performance uplift for multi gpu training have not been explored but do exist. We see this all the time, changes are hardened in the codebase but underreported in patchnotes because intel moves so fast. \n\nHope my ramblings help. Really awesome build, welcome to Arc and good luck!!",
                  "score": 2,
                  "created_utc": "2026-01-03 06:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxd39z4",
                  "author": "b0tbuilder",
                  "text": "You can make it work.",
                  "score": 1,
                  "created_utc": "2026-01-03 01:55:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7isgw",
          "author": "twnznz",
          "text": "I recognise this makes sense for inference but for training we have a huge constraint on bus bandwidth, are you sure you want to train on PCIe setup rather than renting N*H100 from Vast or similar? Does your model/data need absolute security?",
          "score": 16,
          "created_utc": "2026-01-02 05:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx99kwm",
              "author": "sparkandstatic",
              "text": "Self hosted can save the most, if it fits within vram.",
              "score": 2,
              "created_utc": "2026-01-02 14:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxjwpxo",
                  "author": "Novel-Mechanic3448",
                  "text": "Nothing about this is gonna save money if you consider wasted time expensive",
                  "score": 1,
                  "created_utc": "2026-01-04 02:14:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9g38s",
                  "author": "twnznz",
                  "text": "What Iâ€™m trying to say is: unless electricity is free, it is almost certainly cheaper to train on rented H100",
                  "score": 1,
                  "created_utc": "2026-01-02 14:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7gv0f",
          "author": "HyperWinX",
          "text": "Are you going to use Vulkan or what?",
          "score": 13,
          "created_utc": "2026-01-02 05:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7fy1n",
          "author": "Fit_West_8253",
          "text": "What model you using? Hardly seen any Intel GPUs used but Iâ€™m very interested in something like the B60",
          "score": 9,
          "created_utc": "2026-01-02 05:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx86mr1",
          "author": "jack-in-the-sack",
          "text": "7 gpus on what motherboard?",
          "score": 8,
          "created_utc": "2026-01-02 09:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbs3dq",
              "author": "autistic-brother",
              "text": "8",
              "score": 2,
              "created_utc": "2026-01-02 21:39:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7ahl1",
          "author": "Dundell",
          "text": "Big fan of the aaawave open frame. Full size motherboard space with x2 ATX PSUs on both sides. Funny to look at the product details now include \"AI machine learning applications\".\n\nMy rig is x5 rtx 3060 12gb's + x1 P40 24gb all on pcie3.0@4 Lanes with a X99 board. I just run GPT-OSS 120B Q4 with 131k context speeds 42~12 t/s and usually keep it below 90k context maximum for context condensing in roo code.\n\nAlthough I haven't bothered to update llama.cpp and instructions for the gpt-oss 120b since it was released... maybe I could get better performance, but why mess with a good thing.",
          "score": 6,
          "created_utc": "2026-01-02 04:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7dkiy",
              "author": "ajw2285",
              "text": "Deets on mobo?",
              "score": 5,
              "created_utc": "2026-01-02 05:07:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7eur9",
                  "author": "Dundell",
                  "text": "Machinist X99-MR9S Motherboard, Intel Xeon E5-2690 v4 CPU, 5x RTX 3060 12GB GPUs, 1x Tesla P40 24GB GPU (all running at PCIe 3.0 x4), 64GB DDR4 2400T RAM (8x8GB sticks), 1x SATA SSD, 1x USB SSD, and a USB WiFi adapter.",
                  "score": 9,
                  "created_utc": "2026-01-02 05:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7ezk4",
              "author": "mp3m4k3r",
              "text": "Even more fun to compile the container and adjust the  cuda version towards the one youre running. Recently did this for the nvidia nemo moe model from a few weeks ago and some of the new optimizations for choosing memory offload for context is pretty great.",
              "score": 3,
              "created_utc": "2026-01-02 05:17:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7vrje",
              "author": "madsheepPL",
              "text": "donâ€™t take this the wrong way, but whatâ€™s your pp speed at 90k?",
              "score": 1,
              "created_utc": "2026-01-02 07:37:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9a83q",
                  "author": "Dundell",
                  "text": "I don't think I've ever seen it below 200 t/s for read, although by the time I get near 90k, most of that is already cached in the session. Like 350\\~200 t/s read and 44\\~12 t/s write. Something about OSS 120b versus the mediocre speeds from GLM 4.5 Air and such which was more like 200\\~90 t/s read 18\\~4t/s write.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:26:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxd40gx",
              "author": "b0tbuilder",
              "text": "I can generate tokens faster than that on my gmktec box.  Your PP would probably crush it though.",
              "score": 1,
              "created_utc": "2026-01-03 01:59:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx7pkxr",
              "author": "Business-Weekend-537",
              "text": "Do you have a link to the frame? I have a rig but got an Amazon rando piece of crap frame that doesnâ€™t feel solid and Iâ€™m looking to upgrade.",
              "score": 1,
              "created_utc": "2026-01-02 06:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx82dfa",
                  "author": "cantgetthistowork",
                  "text": "This is literally a 12 GPU mining frame that is sold for pennies on AliExpress",
                  "score": 1,
                  "created_utc": "2026-01-02 08:39:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88fql",
          "author": "armindvd2018",
          "text": "Please update your post and add the hardware u use . Like motherboard  cpu and ....",
          "score": 3,
          "created_utc": "2026-01-02 09:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapnh6",
          "author": "mrinterweb",
          "text": "Please post more about your experience with this rig. The Intel B580 has 12GB VRAM (DDR6)for about $250, which sounds like a pretty good value when combining these cards. I realize there are 128GB systems out there like AMD Ryzen AI Max+ 395 (DDR5), but I doubt its bus speed matches the B580. Guessing inference is significantly faster with the B580. I bet 10 of these cards would smoke the Max+ 395 in inference speed.",
          "score": 3,
          "created_utc": "2026-01-02 18:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb01iv",
              "author": "Fit-Produce420",
              "text": "I get 30-40 tok/s on strix halo (gpt-oss-120b mxfp4)",
              "score": 1,
              "created_utc": "2026-01-02 19:23:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxb87vw",
          "author": "Due-Function-4877",
          "text": "\\+1 on a dev postmortem post later on. \n\nDon't sweat the upvotes or downvotes. A lot of us want to know about the experience with Intel cards right now.",
          "score": 3,
          "created_utc": "2026-01-02 20:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7p3bi",
          "author": "robertpro01",
          "text": "Which gpu are those?",
          "score": 2,
          "created_utc": "2026-01-02 06:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx83se7",
              "author": "LightShadow",
              "text": "https://www.sparkle.com.tw/en/products/view/6A1A31428cBE",
              "score": 1,
              "created_utc": "2026-01-02 08:53:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxa4ph1",
              "author": "greggh",
              "text": "B580â€™s, these https://www.sparkle.com.tw/en/products/view/6893fe373180",
              "score": 1,
              "created_utc": "2026-01-02 16:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx80862",
          "author": "Background_Gene_3128",
          "text": "Is those B60 24gb? \n\nAlso, what mobo are you running? \nIâ€™ve ordered two, but want to expand in the future if the â€œhobbyâ€ catches on, so wanna be somewhat â€œpreparedâ€ to scale if needed.",
          "score": 2,
          "created_utc": "2026-01-02 08:19:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8a1tm",
              "author": "lookwatchlistenplay",
              "text": "\"Whatcha doing, handsome?\"\n\n**\"**Preparing.**\"**",
              "score": 2,
              "created_utc": "2026-01-02 09:53:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxan6lp",
          "author": "Fitzroyah",
          "text": "Awesome! Please keep us updated on the experience. I've been enjoying tinkering on my laptops arc iGpu.",
          "score": 2,
          "created_utc": "2026-01-02 18:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7mgzq",
          "author": "Gold_Pen",
          "text": "This is so fascinating seeing this white frame - I bought a black version of this frame from Taobao for only US$20. With a bit of jerry-rigging, I have 4 PSUs and 9 GPUs connected via mainly slimSAS powered risers, with a full fat EEB-sized motherboard. Whole thing weighs about 35kg.",
          "score": 2,
          "created_utc": "2026-01-02 06:16:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7wq1b",
              "author": "michaelsoft__binbows",
              "text": "How the heck do you get a heavy ass item for less than it costs to ship the item",
              "score": 1,
              "created_utc": "2026-01-02 07:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7x9k9",
                  "author": "Gold_Pen",
                  "text": "The frame itself is quite light! I also live in HK, so shipping from mainland China down here is dirt cheap.",
                  "score": 4,
                  "created_utc": "2026-01-02 07:51:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx88l9r",
          "author": "lookwatchlistenplay",
          "text": "And God said to Noah...",
          "score": 2,
          "created_utc": "2026-01-02 09:39:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7uumv",
          "author": "ack4",
          "text": "so what's your stack? What are you running here?",
          "score": 1,
          "created_utc": "2026-01-02 07:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7yd3x",
          "author": "tired_fella",
          "text": "I never knew Intel would be our savior in the consumer compute crisis.",
          "score": 1,
          "created_utc": "2026-01-02 08:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx922zd",
          "author": "aluode",
          "text": "I bet you cant run Crysis on full res.",
          "score": 1,
          "created_utc": "2026-01-02 13:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa2j78",
          "author": "Determined-Hedgehog",
          "text": "How efficient are these at inference? I am wondering. I have mainly been running kobold horde local inference only\nIt's a fork of llama.cpp",
          "score": 1,
          "created_utc": "2026-01-02 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8w7h",
          "author": "quinn50",
          "text": "Interested in seeing where this goes, I have 2 b50s in my sff box and couldn't get anything usable working.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaas7z",
          "author": "WizardlyBump17",
          "text": "please post benchmarks. I have a b580 and i want to get 2 b60 dual, which will have the same memory as you, but half of the power, but it will still be cool to see the numbers",
          "score": 1,
          "created_utc": "2026-01-02 17:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxahl22",
          "author": "Caffdy",
          "text": "what are you planning to train on those?",
          "score": 1,
          "created_utc": "2026-01-02 17:58:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxajbd9",
          "author": "c--b",
          "text": "What supports multi gpu inference anyhow? Unsloth only supports it for a speed boost, not for vram sharing. I wonder if something else does?",
          "score": 1,
          "created_utc": "2026-01-02 18:06:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbahub",
              "author": "hasanismail_",
              "text": "Lm studio is a option",
              "score": 0,
              "created_utc": "2026-01-02 20:13:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc5zp1",
                  "author": "c--b",
                  "text": "oops, meant training.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:49:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxjwxww",
                  "author": "Novel-Mechanic3448",
                  "text": "No its really not haha ðŸ˜‚",
                  "score": 1,
                  "created_utc": "2026-01-04 02:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbqy99",
          "author": "KooperGuy",
          "text": "You won't be accomplishing much training with these",
          "score": 1,
          "created_utc": "2026-01-02 21:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbzp4f",
              "author": "hasanismail_",
              "text": "Ok and?",
              "score": 0,
              "created_utc": "2026-01-02 22:17:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc3o19",
                  "author": "KooperGuy",
                  "text": "There is no and",
                  "score": 3,
                  "created_utc": "2026-01-02 22:37:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxbrh9w",
          "author": "autistic-brother",
          "text": "Explain",
          "score": 1,
          "created_utc": "2026-01-02 21:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjwmyc",
          "author": "Novel-Mechanic3448",
          "text": "Me when i buy EIGHT gpus for the same price of 6k pro, have no idea what im doing, and think im going to be training with poorly maintained frameworks, just because i have \"vram\"",
          "score": 1,
          "created_utc": "2026-01-04 02:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrawdk",
              "author": "hasanismail_",
              "text": "I think you lost the point I can easily afford nice nvidia cards the point of this project is to train and do inference of Intel GPUs as its not very popular/easy right now",
              "score": 1,
              "created_utc": "2026-01-05 04:00:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7leno",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-02 06:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8f65x",
              "author": "synth_mania",
              "text": "do.... you even know what a breadboard is? because it's not that. A breadboard has zero silicon.",
              "score": 4,
              "created_utc": "2026-01-02 10:41:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx9blc5",
              "author": "hasanismail_",
              "text": "I think hes having a episode",
              "score": 1,
              "created_utc": "2026-01-02 14:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qc5nml",
      "title": "Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/wnuwfpdqz6dg1",
      "author": "eugenekwek",
      "created_utc": "2026-01-13 22:32:00",
      "score": 311,
      "num_comments": 33,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzfr6my",
          "author": "dreamyrhodes",
          "text": "I don't understand why there is no single TTS on this planet where you can insert pauses. All of them just read the text down. None of them is able to read calmly and with taking breaks in between paragraphs like a real trained human would do.",
          "score": 44,
          "created_utc": "2026-01-13 22:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzftlm8",
              "author": "eugenekwek",
              "text": "Well, that's one use case for Soprano-Factory! You could fine-tune Soprano to add controllable pauses.",
              "score": 34,
              "created_utc": "2026-01-13 22:55:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmgfoe",
                  "author": "Tbhmaximillian",
                  "text": "Oh nice! how?",
                  "score": 2,
                  "created_utc": "2026-01-14 22:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzftxwg",
              "author": "VoidAlchemy",
              "text": "I've found that most TTS require you to do your own \"chunking\" of long texts and only feed it a sentence or so at a time (especially for the diffusion transformer style models). Kokoro sacrifices that emotive quality for more stable generations, but you still might want to add your own pauses using special characters etc.\n\nI'm not sure how kyutai/pocket-tts (also announced today) and this ekwek/Soprano-TTS are doing it under the hood yet.",
              "score": 7,
              "created_utc": "2026-01-13 22:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg10g4",
                  "author": "dreamyrhodes",
                  "text": "Kokoro (is that even still developed I think it somehow stalled out) can not transform special characters into silence, it would generate random sounds that sound like sighs or breath, sometimes even creepy. I tried a lot, espeically with Kokoro. The prompt syntax that's listed on the demo page unfortunately does nothing.\n\nEventually I came down and with the help of an LLM added a little python function into the code that finds the tag <pause:1.0> and produces a zero tensor of that length 1.0 which results in 1s pause. Just that the <pause>-tag has to be on a new line, because it's a dirty hack but does what I needed at that moment.",
                  "score": 9,
                  "created_utc": "2026-01-13 23:34:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi9xhq",
                  "author": "martinerous",
                  "text": "Soprano-TTS repo says they do automatic text chunking for theoretically infinite generation. I tried a longer text and noticed some shifts in pacing and mood between sentences, so that might be the moments when it splits the text. But this works quite well, and Soprano handled the text without hallucinations, unlike Chatterbox.\n\nIt would be good to have a model trained with speech noises, ehms, clear throat, breath, emotion tags.... But, as always, it requires a good dataset, which would be intense amount of work, especially to preserve it across languages. For example, if a model learns <angry> voice in English, would it still know how to sound angry in another language, when not finetuned with samples for emotions?\n\nOr possibly, emotions could be controllable with voice cloning, like VoxCPM does (Soprano does not yet support it).",
                  "score": 2,
                  "created_utc": "2026-01-14 08:34:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzfy71a",
              "author": "HaAtidChai",
              "text": "Back in the time before the GenAI boom, MS Azure had a playground where you could convert text into various voices of different languages and gauge the pace, pitch and add pauses to your liking. This was admittedly my first profound interaction with AI.\n\nDoubt they still have that accessible in the public with no string attached (login or subscription).",
              "score": 2,
              "created_utc": "2026-01-13 23:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzi96ak",
                  "author": "martinerous",
                  "text": "There was a similar attempt from FastPitch: [https://fastpitch.github.io/](https://fastpitch.github.io/)",
                  "score": 2,
                  "created_utc": "2026-01-14 08:27:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzkmbvc",
              "author": "bigh-aus",
              "text": "Technically they should pause on a '.', for proper sentence structure and imo '...' should generate a longer pause.",
              "score": 1,
              "created_utc": "2026-01-14 17:23:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzko6ev",
                  "author": "dreamyrhodes",
                  "text": "Yes but you can't stack them because they will just be ignored. \"...\" is basically the same as \".\"",
                  "score": 2,
                  "created_utc": "2026-01-14 17:31:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfyhrw",
          "author": "Local_Phenomenon",
          "text": "My Man! You deserve a standing ovation.",
          "score": 16,
          "created_utc": "2026-01-13 23:21:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr7o6",
          "author": "mrmontanasagrada",
          "text": "Very nice! Fast and streaming, I love it!\n\nThank you kindly for sharing, very curious what this model will do with even more training.",
          "score": 8,
          "created_utc": "2026-01-13 22:43:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfrp2z",
              "author": "eugenekwek",
              "text": "Thank you for checking it out!",
              "score": 1,
              "created_utc": "2026-01-13 22:45:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfrxvd",
                  "author": "mrmontanasagrada",
                  "text": "btw how long did you work on this in total? i'm really impressed, was this a one man job?",
                  "score": 1,
                  "created_utc": "2026-01-13 22:47:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfs5w5",
          "author": "Fabulous_Fact_606",
          "text": "Nice. Been looking for something lightweight like Kokoro, but with intonation.",
          "score": 6,
          "created_utc": "2026-01-13 22:48:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgbagr",
          "author": "LocoMod",
          "text": "Been keeping an eye out for this. Great work. And thanks for following up on this highly desired set of features. Well done!",
          "score": 4,
          "created_utc": "2026-01-14 00:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgn8yg",
          "author": "newbie80",
          "text": "Does anyone know if there's a system that can capture my voice and help me identify and correct the things I say wrong? Would it be possible to glue a bunch of stuff to make something like that work? For example someone from California moving over to Alabama that wants to sound like proper southern gentleman, so he uses the system to get his south to listen to his voice, identify were his speech patterns differ from those he desires and corrects him. Is there anything like that?",
          "score": 5,
          "created_utc": "2026-01-14 01:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgv5be",
              "author": "r15km4tr1x",
              "text": "Voice acting coach? Cool idea",
              "score": 2,
              "created_utc": "2026-01-14 02:22:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgb1zk",
          "author": "NighthawkXL",
          "text": "Thanks for listening to our feedback! I look forward to messing with this when I get home tonight.",
          "score": 2,
          "created_utc": "2026-01-14 00:29:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgtw1o",
          "author": "DOAMOD",
          "text": "Thank you very much, do you think you could add a easy voice cloning system? That is the only thing you would be missing, if now we can train languages.\n\nDoes anyone know if there are datasets from other languages â€‹â€‹that we could use? Or do you think that with 50 hours of content we could create one of a certain quality or is necessary more like 100? It would be very good to collect them and create a shared training collab with computing donated by everyone to train the other languages, someone could do something like that, and everyone participate, this small model would be very useful for everyone (and for a personal project with a Spanish/English voice that could be expanded to others).",
          "score": 2,
          "created_utc": "2026-01-14 02:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhv19u",
          "author": "StillHoriz3n",
          "text": "imagine being me and going to look if improvements have been made in the space to find this from 8 hours ago. Hell yeah. Thank you kindly!!",
          "score": 2,
          "created_utc": "2026-01-14 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi6kem",
          "author": "R_Duncan",
          "text": "Good idea! but scipy wav loading during prepare (wavfile.read) won't work here\n\nEdit: fixed by adding \"audio = audio.float() / 32768.0\" before resampling. Also created a virtualenv to update Transofrmers, now seems working.\n\nQuestion: how do I read all the losses and validation losses at the end of training? which value would be considered good?",
          "score": 2,
          "created_utc": "2026-01-14 08:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziwcqe",
          "author": "zoyer2",
          "text": "Anyone finetuned their own model yet? I'm interested in how good it sounds compared to index-tts2",
          "score": 2,
          "created_utc": "2026-01-14 11:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjeurf",
          "author": "TJW65",
          "text": "Any way you could provide us with a simple docker container that deploys the OpenAI compatible API? Would love to see that. :)",
          "score": 2,
          "created_utc": "2026-01-14 13:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkri2m",
          "author": "Major-System6752",
          "text": "What hardware do I need to train model?",
          "score": 1,
          "created_utc": "2026-01-14 17:46:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxxsf",
          "author": "Ok_Appearance3584",
          "text": "Awesome! Been using this as my daily driver. Awesome to be able to finetune it for my taste!",
          "score": 1,
          "created_utc": "2026-01-14 18:15:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqdber",
          "author": "TotalStatement1061",
          "text": "i tried fine-tuning on this model but I can't set the checkpoint for it, and have train whole 10000 steps, any suggestions or mistakes am making here",
          "score": 1,
          "created_utc": "2026-01-15 14:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjozfw",
          "author": "barrettj",
          "text": "Does this run on iOS?\n\nIâ€™m always looking for new TTS libraries for our AAC app",
          "score": 0,
          "created_utc": "2026-01-14 14:48:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5a0if",
      "title": "Liquid Ai released LFM2.5, family of tiny on-device foundation models.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/flk7mfltznbg1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2026-01-06 05:27:54",
      "score": 308,
      "num_comments": 55,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxynik2",
          "author": "adt",
          "text": "1.2B parameters trained on 28T tokens has a data ratio @ 23,334:1.\n\nEdit: Beaten by Qwen3-0.6B trained on 36T @ 60,000:1.\n\n[https://lifearchitect.ai/models-table/](https://lifearchitect.ai/models-table/)",
          "score": 88,
          "created_utc": "2026-01-06 06:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0qqs3",
              "author": "Aaaaaaaaaeeeee",
              "text": "nah, we know Qwen3 0.6B has 36T. Feels like the chart needs to be rechecked for accuracy.Â ",
              "score": 12,
              "created_utc": "2026-01-06 15:24:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny12wml",
                  "author": "adt",
                  "text": "You're correct.\n\nThe Models Table usually only shows largest model in each family (that's why it has 700 models compared to HF's 300,000 models), so this tiny model was hidden. Added now.\n\nQwen3-0.6B has a data ratio of 60,000:1.",
                  "score": 9,
                  "created_utc": "2026-01-06 16:20:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxysoow",
          "author": "HistorianPotential48",
          "text": "Testing on their site with some prompts we're handling with qwen3 8b. Feels more like a 4B and very fast, but still has the problem of bad at following instructions for special formats - \"Complete one sentence...\" gives 5 sentences instead; \"Create a json like this...\" results in an extra } symbol but otherwise perfect.\n\nAlmost there, probably can be a very fast chat to ask things (RAG knowledge base?), but not smart enough for small practical tasks. Perfect for generating those llm bot tweet replies i guess.   \n  \nThey also have a VL-1.6B. wonder how that's doing",
          "score": 39,
          "created_utc": "2026-01-06 06:46:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxytk92",
              "author": "HistorianPotential48",
              "text": "Ay the VL cooks tho. Can't do OCR, package ingredient texts result in looping, but seems great at image describing.\n\nhttps://preview.redd.it/3k46jrwffobg1.png?width=1722&format=png&auto=webp&s=f7f8aee63c04a42a47f3153a241de8592a8c7309",
              "score": 24,
              "created_utc": "2026-01-06 06:54:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzc6n5",
                  "author": "DrummerHead",
                  "text": "Aaah! I see you're using the classic Indiana-Waifu-Pepsi VL test!\n\n\n\n^(what the fuck à² _à² )",
                  "score": 29,
                  "created_utc": "2026-01-06 09:48:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny0pfr1",
                  "author": "Aaaaaaaaaeeeee",
                  "text": "Â Mmm. there is no can and there is no recognition of Frieren and the mimic",
                  "score": 4,
                  "created_utc": "2026-01-06 15:17:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2b4wz",
                  "author": "MoffKalast",
                  "text": "It cannot the ðŸ…±ï¸epis.",
                  "score": 2,
                  "created_utc": "2026-01-06 19:39:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxz0i3f",
              "author": "ab2377",
              "text": "maybe can be fine tuned on a json dataset to make it right, if someone can try that.",
              "score": 3,
              "created_utc": "2026-01-06 07:56:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzgpji",
                  "author": "bjodah",
                  "text": "If the schema is known a priori, then I would have guessed that these small (or all?) models would benefit from a some framework that forces syntactically correct json. (e.g. [https://github.com/1rgs/jsonformer](https://github.com/1rgs/jsonformer) ).",
                  "score": 6,
                  "created_utc": "2026-01-06 10:29:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxyyc87",
              "author": "Sixhaunt",
              "text": "Thanks for putting in this work and detailing it for the rest of us. Sounds like it could be very promising for specific tasks if it handles fine-tuning well even if it has a few pitfalls by default in it's more general form.",
              "score": 3,
              "created_utc": "2026-01-06 07:36:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny1fll0",
              "author": "Irisi11111",
              "text": "I think that is an issue for most small models, which are highly benchmarked, so when you use it in a real world case scenario, they are inconsistent. But this is expected, because it's too small and lacking world knowledge, it simply can't recognize what you are saying. \n\nI think that's a problem for most small models. They do well on benchmarks, but in real cases, they're not always consistent. That's kind of expected, though, because they're too small and don't have enough world knowledge. They just can't always understand what you're saying.",
              "score": 1,
              "created_utc": "2026-01-06 17:18:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyro14",
          "author": "mitchins-au",
          "text": "Utterly amazing. A graph thatâ€™s to scale.",
          "score": 28,
          "created_utc": "2026-01-06 06:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2bc60",
              "author": "MoffKalast",
              "text": "Impossible.",
              "score": 3,
              "created_utc": "2026-01-06 19:40:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz2f3n",
          "author": "DeltaSqueezer",
          "text": "If it is to be run on-device, I wonder why they don't train for native FP8 or FP4, you don't need batching performance could have more parameters for the same RAM.",
          "score": 13,
          "created_utc": "2026-01-06 08:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzsr54",
              "author": "Karyo_Ten",
              "text": "For small models, quantization has **heavy** impact on output quality.",
              "score": 6,
              "created_utc": "2026-01-06 12:09:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzu7o9",
                  "author": "DeltaSqueezer",
                  "text": "Exactly, so by training a 1B FP16 you force people to run FP16 or severely damage the quality by quantizing to say 4bit. Instead, you could have trained a 4B at 4bit quantization that could be used in the same VRAM and not require further quantization damage.",
                  "score": 17,
                  "created_utc": "2026-01-06 12:19:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyolcc",
          "author": "Sixhaunt",
          "text": "have any of you tried it out yet to see how accurate the benchmarks are? Looks promising if true",
          "score": 13,
          "created_utc": "2026-01-06 06:12:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyq95e",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 8,
          "created_utc": "2026-01-06 06:25:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyusyj",
              "author": "ismaelgokufox",
              "text": "How do you run these on iOS?",
              "score": 2,
              "created_utc": "2026-01-06 07:04:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxyv3t7",
                  "author": "2str8_njag",
                  "text": "â€œPocketPalâ€ for GGUF, â€œMLX Benchmarksâ€ for MLX formats.",
                  "score": 2,
                  "created_utc": "2026-01-06 07:07:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny05f5g",
          "author": "ElectronSpiderwort",
          "text": "\"Native audio-language model (speech in/out)\" kind of buried under the fold",
          "score": 8,
          "created_utc": "2026-01-06 13:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzmflc",
          "author": "llama-impersonator",
          "text": "i mean, i like liquid but holy shit make something larger already",
          "score": 15,
          "created_utc": "2026-01-06 11:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz8qt2",
          "author": "TechnoByte_",
          "text": "Here are the models, including GGUF: https://huggingface.co/collections/LiquidAI/lfm25",
          "score": 8,
          "created_utc": "2026-01-06 09:15:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz9xvd",
          "author": "if47",
          "text": "Impressive achievements, but terrible charts.",
          "score": 10,
          "created_utc": "2026-01-06 09:26:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzndeo",
              "author": "-dysangel-",
              "text": "sums up most of AI development in the last few years",
              "score": 8,
              "created_utc": "2026-01-06 11:27:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxypz72",
          "author": "Kahvana",
          "text": "Wish they showed their previous model on there as well.",
          "score": 11,
          "created_utc": "2026-01-06 06:23:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzs4i7",
          "author": "__Maximum__",
          "text": "These 1b models are getting smarter than a lot of people i have met, true signs of advancements.",
          "score": 9,
          "created_utc": "2026-01-06 12:04:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny1bgte",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-01-06 16:59:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1dmjz",
                  "author": "__Maximum__",
                  "text": "Wow, you proved my point.",
                  "score": 0,
                  "created_utc": "2026-01-06 17:09:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzxkgd",
          "author": "guiopen",
          "text": "It is the best 1b model I tested by far, the only usable one. Higher speed even compared to models of the same size, and can speak Portuguese making less grammar errors than some bigger 4b models like nanbeige and even qwen3 4b",
          "score": 5,
          "created_utc": "2026-01-06 12:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny06sp7",
          "author": "bakawolf123",
          "text": "Tested locally with MLX on M1Pro and it looks to be comparable to Qwen3-4B but about 2x faster, though there're no <thinking> blocks. Would be interesting what can be done with finetuning it.  \nedit: works lightning fast on a 17pro iPhone too",
          "score": 4,
          "created_utc": "2026-01-06 13:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9ff6g",
              "author": "syntaxing2",
              "text": "How are you running this on iOS?",
              "score": 1,
              "created_utc": "2026-01-07 20:03:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0ofno",
          "author": "memeposter65",
          "text": "It's crazy good for the size, I love it.",
          "score": 5,
          "created_utc": "2026-01-06 15:12:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0pg84",
          "author": "meatycowboy",
          "text": "28T tokens for a 1.2B model is crazy.",
          "score": 5,
          "created_utc": "2026-01-06 15:17:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4l3a6",
          "author": "ScoreUnique",
          "text": "I downloaded Q8 on my Pixel 8 with pocket pal, and oh dear I felt like chatting to GPT-4 but locally with 15 tps. \n\nI will test it further - I'll be in a flight this weekend.",
          "score": 4,
          "created_utc": "2026-01-07 02:25:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzw6ri",
          "author": "guiopen",
          "text": "Happy to see you guys releasing base models!",
          "score": 2,
          "created_utc": "2026-01-06 12:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny04lvo",
          "author": "zelkovamoon",
          "text": "LFM2 was pretty good, so im excited to try this. Really hoping tool calling is better with these models, that was basically my biggest complaint.",
          "score": 2,
          "created_utc": "2026-01-06 13:26:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny15vvc",
          "author": "1_7xr",
          "text": "I wanted to try the vision version on LM Studio but whenever I upload an image, it says the model doesn't support images. Any one with some experience on how to deal with this?",
          "score": 2,
          "created_utc": "2026-01-06 16:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny06ukz",
          "author": "steezy13312",
          "text": "This is the *exactly* the kind of model to compliment my [MCP Context Proxy project](https://github.com/samteezy/mcp-context-proxy/). It's not solving anything on its own, but you're using it to offload work from your slower, heavier main model. Downloading now",
          "score": 4,
          "created_utc": "2026-01-06 13:39:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1jvms",
          "author": "sxales",
          "text": "I guess it performed about the same as LFM2 2.6b. I am genuinely in awe of how fast the model is, but it seems largely useless. It failed all my usual tests: grade school math, logical puzzles, and summarization. \n\nSince they only seem to be releasing small models, I wonder if whatever voodoo they use to make prompt processing so fast isn't scaling well.",
          "score": 2,
          "created_utc": "2026-01-06 17:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4xl49",
          "author": "tttsang",
          "text": "how to run this model on iPhone? I ran the previous model but it's stuck",
          "score": 1,
          "created_utc": "2026-01-07 03:34:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg82d0",
          "author": "cibernox",
          "text": "I liked their previous models a lot but they were to small and dumb for my use case. I hope they make something bigger but still small soon. Iâ€™m thinking something like a 12B-A4B instruct that can rival qwen3-VL 8B",
          "score": 1,
          "created_utc": "2026-01-08 19:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz36q3c",
          "author": "Left-Data-4603",
          "text": "Is there a way to Finetune the LiquidAI/LFM2-Audio-1.5B model? if anyone knows, please help",
          "score": 1,
          "created_utc": "2026-01-12 02:11:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz1vkm",
          "author": "GoranjeWasHere",
          "text": "a great progress for vramlets. Actual usable 1,2b model.\n\n  \nIt's crazy that we beat chat gpt4 with 1,2b model. Not only it is better but also can do ocr and other stuff.",
          "score": 1,
          "created_utc": "2026-01-06 08:09:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz3s3c",
          "author": "iqandjoke",
          "text": "How can I use it on Android? Which one should I use? Edge Gallery only support .litertlm format.",
          "score": 1,
          "created_utc": "2026-01-06 08:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz6yl6",
              "author": "jamaalwakamaal",
              "text": "Skip Edge gallery, get PocketPal from GitHub or Play Store.",
              "score": 4,
              "created_utc": "2026-01-06 08:57:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7q7g5",
                  "author": "iqandjoke",
                  "text": "Will try. Thanks!",
                  "score": 1,
                  "created_utc": "2026-01-07 15:31:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny0lroa",
          "author": "Ok-Internal9317",
          "text": "When Ollama?",
          "score": -3,
          "created_utc": "2026-01-06 14:59:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa0ph9",
      "title": "Leader of Qwen team says Chinese companies severely constrained on compute for large scale research experiments",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/pgvmn26adqcg1.png",
      "author": "Old-School8916",
      "created_utc": "2026-01-11 14:29:39",
      "score": 308,
      "num_comments": 104,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qa0ph9/leader_of_qwen_team_says_chinese_companies/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyz97j8",
          "author": "mxforest",
          "text": "Necessity is the mother of invention. These guys will find novel ways of extracting most out of their hardware and when they are no longer compute starved, they will eat the American companies.",
          "score": 192,
          "created_utc": "2026-01-11 14:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzszxb",
              "author": "No_Conversation9561",
              "text": "american companies will adopt those methods while giving nothing back in return",
              "score": 73,
              "created_utc": "2026-01-11 16:28:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz04qrz",
                  "author": "Neither-Phone-7264",
                  "text": "deepmind with their trillion year research embargo:",
                  "score": 23,
                  "created_utc": "2026-01-11 17:24:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz08h44",
                  "author": "pier4r",
                  "text": "IF those methods get shared.",
                  "score": 5,
                  "created_utc": "2026-01-11 17:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0s2tn",
              "author": "TheEagleDied",
              "text": "Hope in one hand and shit in the other, see what weighs more.",
              "score": 5,
              "created_utc": "2026-01-11 19:09:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyznmps",
              "author": "Mediocre-Method782",
              "text": "Shush, Tinkerbell. That outcome is by no means guaranteed, and that much less likely without the material means to find those efficiencies. Believing harder doesn't make math line up better.\n\nedit: https://en.wikipedia.org/wiki/Path_dependence",
              "score": -21,
              "created_utc": "2026-01-11 16:03:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyztmfs",
                  "author": "koffieschotel",
                  "text": "Btw. Youâ€™re probably getting by the tone of your message.",
                  "score": 6,
                  "created_utc": "2026-01-11 16:31:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyzay6q",
              "author": "SkyFeistyLlama8",
              "text": "Not going to happen. Training still happens almost exclusively on Nvidia hardware and Chinese companies are buying huge amounts of new GPUs, through legal and shady channels.\n\nHuawei might make decent chips for inference but not for training.",
              "score": -28,
              "created_utc": "2026-01-11 15:00:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyzbiex",
                  "author": "Vegetable-Second3998",
                  "text": "makes you think they'll research ways to increase performance without expensive pre-training. how many times do you think we need to teach models the sky is blue and rocks are hard?",
                  "score": 23,
                  "created_utc": "2026-01-11 15:03:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyzi1vb",
                  "author": "danigoncalves",
                  "text": "> Not going to happen\n\nI would not be so sure. Take a look on the last advancements on Deepseeks last paper, Also like we have Transformers breakthrough I think there are bright Chinese minds that can come with other new architecture that could change again the scenario.",
                  "score": 6,
                  "created_utc": "2026-01-11 15:36:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyzcl2p",
                  "author": "cheechw",
                  "text": "I mean, they literally did. DeepSeek came up with a new way to extract a lot more out of less hardware for training. The thing is they released that research so now everyone can use it.",
                  "score": 5,
                  "created_utc": "2026-01-11 15:08:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0rfr1",
              "author": "Candid_Highlight_116",
              "text": "The problem is that Chinese government sees its own global dominance as a foreign theat somehow and shoots itself to stop it before it wins. They've done the same in wide ranges of industries.",
              "score": -7,
              "created_utc": "2026-01-11 19:06:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz1naz1",
              "author": "LocoMod",
              "text": "It's too late. That ship sailed.\n\nEDIT: They've been trying to catch up with their own chips for well over a decade now in an environment where there wasn't scarcity. Not only do they have the insurmountable task of engineering something that has parity to today's chips, but by the time they get there Apple, Google and nVidia will several generations ahead. They literally have to predict the compute power of chips coming out in 2030 and aim for that. (Second Edit) And they have to predict not at today's rate of progress, but at the rate of progress of western companies that have the best chips in the world and the best AIs in the world RIGHT NOW designing their next chips. Good luck.",
              "score": -7,
              "created_utc": "2026-01-11 21:31:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyz6jzh",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 63,
          "created_utc": "2026-01-11 14:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzd66p",
              "author": "ifupred",
              "text": "Not just chinese but the only other people who make are them. Monopolies are horrible",
              "score": 34,
              "created_utc": "2026-01-11 15:11:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyzeba1",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 2,
                  "created_utc": "2026-01-11 15:17:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyzjpqf",
              "author": "PSUVB",
              "text": "The gov is already heavily â€œstepped inâ€. They are 5-10 years away on all of these - if they ever even plan to catch up. I wouldnâ€™t hold my breath.",
              "score": 13,
              "created_utc": "2026-01-11 15:44:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyz7izu",
          "author": "terem13",
          "text": "They just whining and making dramas to get more money from deep state pockets, scaring that \"otherwise we will be late in AI goldrush\". Hope to leverage current stir, created by OpenAI.\n\nUsual story of every company, not just AI, dreaming about \"golden shower\" of state-funded investments.\n\nNot the first time, not the last time.",
          "score": 61,
          "created_utc": "2026-01-11 14:41:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz08u3h",
              "author": "Raidicus",
              "text": "100% all of these narratives are to drive investment from central Governments.",
              "score": 11,
              "created_utc": "2026-01-11 17:43:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz3z9zw",
              "author": "gjallerhorns_only",
              "text": "I figured they were trying to get the government to lift the NV GPU ban",
              "score": 1,
              "created_utc": "2026-01-12 04:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyz8kvy",
              "author": "Old-School8916",
              "text": "yep, you're probably right",
              "score": -3,
              "created_utc": "2026-01-11 14:47:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz0t1zt",
              "author": "Betadoggo_",
              "text": "Yep, it's no different at any of the other AI companies, they're all fishing for big guarantees from their respective governments.",
              "score": 0,
              "created_utc": "2026-01-11 19:13:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzpwoe",
          "author": "aeroumbria",
          "text": "If these companies can still operate fine and even be self-sustainable, then the ones with priority GPU access and unlimited budget claiming not being profitable is either complete bullshit or serious skill issue.",
          "score": 3,
          "created_utc": "2026-01-11 16:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyze8j6",
          "author": "reto-wyss",
          "text": "That's what I would say.\n\nThey may not have as many latest Gen GPUs, but it can't be that bad, they have plenty of power and they are selling the Atlas 300i DUO 48GB + 48GB on Alibaba (as low as $1500) - these are inference only but they could use them for inference - suggesting that those are not worth running in the large DC anymore?\n\nIf you don't care about power draw, and building DC is relatively cheap, you can in theory run pretty old accelerators at least on the inference side.\n\nI'm just excited to see the rapid development of options for compute that's not from US based companies. There's also a company from the Netherlands which seems to have promising devices for inference. They have a 4x16GB card https://store.axelera.ai/products/pcie-ai-accelerator-card-powered-by-4-metis-aipu?variant=51945747513685 - I'm extremely intrigued to buy one of their devices.",
          "score": 17,
          "created_utc": "2026-01-11 15:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzt1f7",
              "author": "darkpigvirus",
              "text": "I think they have proven enough (Qwen 3 benchmarks) for me to just shut up and believe what they say. If they say they are compute constrained then they are",
              "score": 28,
              "created_utc": "2026-01-11 16:28:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz0npt1",
              "author": "No_Afternoon_4260",
              "text": "They wanted me to login to access a datasheet.. what's their memory speed?",
              "score": 2,
              "created_utc": "2026-01-11 18:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4sjuc",
                  "author": "belkh",
                  "text": "it's using LPDDR4x so bandwidth is going to be pretty low",
                  "score": 1,
                  "created_utc": "2026-01-12 09:00:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0ttei",
              "author": "Candid_Highlight_116",
              "text": "There seem to be something wrong with the Atlas architecture. Some of them are on Chinese e-commerce sites but only as CCTV human detection accelerators. Likely nonexistent inter-processor bandwidth or something.",
              "score": 1,
              "created_utc": "2026-01-11 19:16:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz05are",
              "author": "inaem",
              "text": "Energy regulations are very strict for data centers in China, they probably need to solve fusion as well before being able to actually use all that compute\n\nUnlike a certain country, China at least pretends to care about other industriesâ€™ energy needs",
              "score": 1,
              "created_utc": "2026-01-11 17:26:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3lbfg",
                  "author": "agentzappo",
                  "text": "Thatâ€™s because training runs at a data center level can create > 100MW swings in power consumption swinging between compute vs sync stages. Thats a tough load to balance intermittentlyâ€¦",
                  "score": 2,
                  "created_utc": "2026-01-12 03:29:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1ck2k",
              "author": "fallingdowndizzyvr",
              "text": "> they have plenty of power \n\nThey have green energy laws in China. They actually care about the environment unlike the other large economy in the world. So a GPU has to meet minimum efficiency requirements.",
              "score": -1,
              "created_utc": "2026-01-11 20:42:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz11re7",
          "author": "Cool-Chemical-5629",
          "text": "In the past I noticed considerable slowness, unresponsiveness, quality degradation and error messages when using the [Z.AI](http://Z.AI) services on weekend which were followed by release of a new big GLM model. Looks like they weren't deliberately making the service worse, they were just cooking something new and needed some resources for that. ðŸ˜‚",
          "score": 3,
          "created_utc": "2026-01-11 19:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0yxv9",
          "author": "paleasheart",
          "text": "classic bloomberg lol\n\nyou can translate below yourself to see how they play tricks to completely shifts the narrative.\n\næž—ä¿Šæ—¸ï¼š è¿™ä¸ªé—®é¢˜æ˜¯ä¸ªå±é™©çš„é—®é¢˜ï¼Œç†è®ºä¸Šè¿™ä¸ªåœºåˆæ˜¯ä¸å¯ä»¥æ³¼å†·æ°´çš„ï¼Œä½†å¦‚æžœä»Žæ¦‚çŽ‡ä¸Šæ¥è¯´ï¼Œæˆ‘å¯èƒ½æƒ³è¯´ä¸€ä¸‹æˆ‘æ„Ÿå—åˆ°çš„ä¸­å›½å’Œç¾Žå›½çš„å·®å¼‚ã€‚æ¯”å¦‚è¯´ç¾Žå›½çš„Computeå¯èƒ½æ•´ä½“æ¯”æˆ‘ä»¬å¤§1-2ä¸ªæ•°é‡çº§ï¼Œä½†æˆ‘çœ‹åˆ°ä¸ç®¡æ˜¯OpenAIè¿˜æ˜¯ä»€ä¹ˆï¼Œä»–ä»¬å¤§é‡çš„ComputeæŠ•å…¥åˆ°çš„æ˜¯ä¸‹ä¸€ä»£çš„Researchå½“ä¸­åŽ»ï¼Œæˆ‘ä»¬ä»Šå¤©ç›¸å¯¹æ¥è¯´æ‰è¥Ÿè§è‚˜ï¼Œå…‰äº¤ä»˜å¯èƒ½å°±å·²ç»å æ®äº†æˆ‘ä»¬ç»å¤§éƒ¨åˆ†çš„Computeï¼Œè¿™ä¼šæ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤§çš„å·®å¼‚åœ¨è¿™é‡Œã€‚è¿™å¯èƒ½æ˜¯åŽ†å²ä»¥æ¥å°±æœ‰çš„é—®é¢˜ï¼Œåˆ›æ–°æ˜¯å‘ç”Ÿåœ¨æœ‰é’±çš„äººæ‰‹é‡Œï¼Œè¿˜æ˜¯ç©·äººæ‰‹é‡Œï¼Œç©·äººä¸æ˜¯æ²¡æœ‰æœºä¼šï¼Œæˆ‘ä»¬è§‰å¾—è¿™äº›å¯Œå“¥çœŸçš„å¾ˆæµªè´¹å¡ï¼Œä»–ä»¬è®­äº†è¿™ä¹ˆå¤šä¸œè¥¿ï¼Œå¯èƒ½è®­äº†å¾ˆå¤šä¹Ÿæ²¡ä»€ä¹ˆç”¨ã€‚ä½†ä»Šå¤©ç©·çš„è¯ï¼Œæ¯”å¦‚ä»Šå¤©æ‰€è°“çš„ç®—æ³•Infraè”åˆä¼˜åŒ–çš„äº‹æƒ…ï¼Œå¦‚æžœä½ çœŸçš„å¾ˆå¯Œï¼Œå°±æ²¡æœ‰ä»€ä¹ˆåŠ¨åŠ›åŽ»åšè¿™ä¸ªäº‹æƒ…ã€‚\n\næˆ‘è§‰å¾—å¯èƒ½æ›´è¿›ä¸€æ­¥çš„ï¼Œåˆšæ‰é¡ºé›¨æåˆ°å…‰åˆ»æœºçš„é—®é¢˜ï¼Œæœªæ¥æœ‰å¯èƒ½è¿˜æœ‰ä¸€ä¸ªç‚¹ï¼Œå¦‚æžœä»Žè½¯ç¡¬ç»“åˆçš„è§’åº¦ï¼Œæ˜¯ä¸æ˜¯çœŸçš„æœ‰å¯èƒ½åšå‡ºæ¥ï¼Ÿæ¯”å¦‚è¯´æˆ‘ä»¬ä¸‹ä¸€ä»£è¿™ä¸ªæ¨¡åž‹å’ŒèŠ¯ç‰‡ï¼Œæœ‰å¯èƒ½æ˜¯ä¸€èµ·æŠŠå®ƒç»™åšå‡ºæ¥çš„ã€‚æˆ‘åœ¨2021å¹´çš„æ—¶å€™åœ¨åšå¤§æ¨¡åž‹ï¼Œå› ä¸ºé˜¿é‡ŒåšèŠ¯ç‰‡ï¼Œåœ¨æ‰¾æˆ‘è¯´èƒ½ä¸èƒ½é¢„æµ‹ä¸€ä¸‹ä¸‰å¹´ä¹‹åŽè¿™ä¸ªæ¨¡åž‹æ˜¯ä¸æ˜¯Transformerï¼Œä¸‰å¹´ä¹‹åŽè¿™ä¸ªæ¨¡åž‹æ˜¯ä¸æ˜¯å¤šæ¨¡æ€ï¼Œä¸ºä»€ä¹ˆæ˜¯ä¸‰å¹´å‘¢ï¼Ÿä»–è¯´æˆ‘ä»¬éœ€è¦ä¸‰å¹´æ—¶é—´æ‰èƒ½æµç‰‡ã€‚æˆ‘å½“æ—¶çš„å›žç­”æ˜¯ä¸‰å¹´ä¹‹åŽåœ¨ä¸åœ¨é˜¿é‡Œå·´å·´ï¼Œæˆ‘éƒ½ä¸çŸ¥é“ï¼ä½†æˆ‘ä»Šå¤©è¿˜åœ¨é˜¿é‡Œå·´å·´ï¼Œå®ƒæžœç„¶è¿˜æ˜¯Transformerï¼Œæžœç„¶è¿˜æ˜¯å¤šæ¨¡æ€ï¼Œæˆ‘éžå¸¸æ‡Šæ‚”ä¸ºä»€ä¹ˆå½“æ—¶æ²¡æœ‰å‚¬ä»–åŽ»åšã€‚å½“æ—¶æˆ‘ä»¬çš„äº¤æµéžå¸¸é¸¡åŒé¸­è®²ï¼Œä»–ç»™æˆ‘è®²äº†ä¸€å¤§å †ä¸œè¥¿ï¼Œæˆ‘å®Œå…¨å¬ä¸æ‡‚ï¼Œæˆ‘ç»™ä»–è®²ï¼Œä»–ä¹Ÿä¸çŸ¥é“æˆ‘ä»¬åœ¨åšä»€ä¹ˆï¼Œå°±é”™è¿‡äº†è¿™ä¸ªæœºä¼šã€‚è¿™ä¸ªæœºä¼šæœ‰æ²¡æœ‰å¯èƒ½å†æ¥ä¸€æ¬¡ï¼Ÿæˆ‘ä»¬è™½ç„¶æ˜¯ä¸€ç¾¤ç©·äººï¼Œæ˜¯ä¸æ˜¯ç©·åˆ™æ€å˜ï¼Œåˆ›æ–°çš„æœºä¼šä¼šä¸ä¼šå‘ç”Ÿåœ¨è¿™é‡Œï¼Ÿ\n\nä»Šå¤©æˆ‘ä»¬æ•™è‚²åœ¨å˜å¥½ï¼Œæˆ‘å±žäºŽ90å¹´ä»£é å‰ä¸€äº›çš„ï¼Œé¡ºé›¨å±žäºŽ90å¹´ä»£é åŽä¸€ç‚¹çš„ï¼Œæˆ‘ä»¬å›¢é˜Ÿé‡Œé¢æœ‰å¾ˆå¤š00åŽï¼Œæˆ‘æ„Ÿè§‰å¤§å®¶çš„å†’é™©ç²¾ç¥žå˜å¾—è¶Šæ¥è¶Šå¼ºã€‚ç¾Žå›½äººå¤©ç„¶æœ‰éžå¸¸å¼ºçƒˆçš„å†’é™©ç²¾ç¥žï¼Œä¸€ä¸ªå¾ˆå…¸åž‹çš„ä¾‹å­æ˜¯å½“æ—¶ç”µåŠ¨è½¦åˆšå‡ºæ¥ï¼Œç”šè‡³å¤©æ£šæ¼æ°´çš„æƒ…å†µä¸‹ï¼Œç”šè‡³å¼€è½¦ä¼šæ„å¤–èº«äº¡çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶ä¼šæœ‰å¾ˆå¤šå¯Œè±ªä»¬éƒ½æ„¿æ„åŽ»åšè¿™ä¸ªäº‹æƒ…ï¼Œä½†åœ¨ä¸­å›½ï¼Œæˆ‘ç›¸ä¿¡å¯Œè±ªä»¬æ˜¯ä¸ä¼šåŽ»å¹²è¿™ä¸ªäº‹æƒ…çš„ï¼Œå¤§å®¶ä¼šåšä¸€äº›å¾ˆå®‰å…¨çš„äº‹æƒ…ã€‚ä»Šå¤©å¤§å®¶çš„å†’é™©ç²¾ç¥žå¼€å§‹å˜å¾—æ›´å¥½ï¼Œä¸­å›½çš„è¥å•†çŽ¯å¢ƒä¹Ÿåœ¨å˜å¾—æ›´å¥½çš„æƒ…å†µä¸‹ï¼Œæˆ‘è§‰å¾—æ˜¯æœ‰å¯èƒ½å¸¦æ¥ä¸€äº›åˆ›æ–°çš„ã€‚æ¦‚çŽ‡æ²¡é‚£ä¹ˆå¤§ï¼Œä½†çœŸçš„æœ‰å¯èƒ½ã€‚\n\næŽå¹¿å¯†ï¼š å¦‚æžœæ´¾ä¸€ä¸ªæ•°å­—å‘¢ï¼Ÿ\n\næž—ä¿Šæ—¸ï¼š æ‚¨æ˜¯è¯´ç™¾åˆ†ä¹‹å¤šå°‘ï¼Ÿ\n\næŽå¹¿å¯†ï¼š å¯¹ï¼Œä¸‰å¹´åˆ°äº”å¹´åŽï¼Œä¸­å›½æœ€é¢†å…ˆçš„é‚£ä¸ªå…¬å¸ï¼Œæ˜¯ä¸€å®¶ä¸­å›½å…¬å¸çš„æ¦‚çŽ‡ã€‚\n\næž—ä¿Šæ—¸ï¼š æˆ‘è§‰å¾—æ˜¯20%å§ï¼Œ20%å·²ç»éžå¸¸ä¹è§‚äº†ï¼Œå› ä¸ºçœŸçš„æœ‰å¾ˆå¤šåŽ†å²ç§¯æ·€çš„åŽŸå› åœ¨è¿™é‡Œã€‚\n\næŽå¹¿å¯†ï¼š æˆ‘å†Followä¸€ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚è¯´ä¸­å›½çš„æ¨¡åž‹è·Ÿç¾Žå›½çš„æ¨¡åž‹å·®è·ï¼Œæœ‰çš„åœ°æ–¹åœ¨è¿½ä¸Šæ¥ï¼Œæœ‰çš„åœ°æ–¹ä»–ä»¬çš„ç®—åŠ›åœ¨æ‹‰å¤§ï¼Œä½ å†…å¿ƒä¸­Gapå˜å¤§çš„ææƒ§æ„Ÿå¼ºå—ï¼Ÿ\n\næž—ä¿Šæ—¸ï¼š ä»Šå¤©ä½ å¹²è¿™ä¸€è¡Œå°±ä¸èƒ½ææƒ§ï¼Œå¿…é¡»å¾—æœ‰éžå¸¸å¼ºçš„å¿ƒæ€ï¼Œå¯¹äºŽæˆ‘ä»¬çš„å¿ƒæ€æ¥è¯´ï¼Œèƒ½å¹²è¿™ä¸€è¡Œå°±éžå¸¸ä¸é”™äº†ï¼Œèƒ½åšå¤§æ¨¡åž‹è¿™ä»¶äº‹æƒ…å·²ç»éžå¸¸å¹¸è¿äº†ã€‚\n\næˆ‘è§‰å¾—è¿˜æ˜¯çœ‹ä½ çš„åˆå¿ƒæ˜¯ä»€ä¹ˆï¼Œåˆšæ‰é¡ºé›¨æåˆ°ä¸€ä¸ªç‚¹ï¼Œä½ çš„æ¨¡åž‹ä¸ä¸€å®šé‚£ä¹ˆå¼ºåœ¨Cç«¯é‡Œè¾¹æ˜¯OKçš„ã€‚æˆ‘å¯èƒ½è½¬æ¢æˆå¦å¤–ä¸€ä¸ªè§’åº¦åŽ»æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„æ¨¡åž‹ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥äº†ä»€ä¹ˆæ ·çš„ä»·å€¼ï¼Œåªè¦æˆ‘ç›¸ä¿¡æˆ‘è¿™ä¸ªä¸œè¥¿èƒ½å¤Ÿä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥å……åˆ†çš„ä»·å€¼ï¼Œèƒ½å¤Ÿå¸®åŠ©äººç±»ï¼Œå°±ç®—ä¸æ˜¯æœ€å¼ºçš„ï¼Œæˆ‘ä¹Ÿæ„¿æ„æŽ¥å—ã€‚",
          "score": 8,
          "created_utc": "2026-01-11 19:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6gutb",
              "author": "tuananh_org",
              "text": "lol. very different indeed. i do agree that the model doesn't have to be the best , to be useful.",
              "score": 2,
              "created_utc": "2026-01-12 15:50:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz7exa1",
              "author": "ChampionOfKirkwall",
              "text": "Thank you for sharing",
              "score": 2,
              "created_utc": "2026-01-12 18:25:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyzfsxl",
          "author": "croninsiglos",
          "text": "But do you need to surpass to be successful?\n\nSure no one wants a national strategy of being second place, but this constraint has created new innovations in training and efficiency.\n\nThey are still far better than European models and the open weight nature has benefited privacy, small business, and the world.\n\nI can tell you, with certainty, that we are using fine-tuned qwen models to assist with scientific research even though we have virtually unlimited funds for the top US frontier models. That should tell you something.",
          "score": 4,
          "created_utc": "2026-01-11 15:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz9hb7",
          "author": "southpalito",
          "text": "Necessity is the mother of invention",
          "score": 6,
          "created_utc": "2026-01-11 14:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz58gf2",
          "author": "LittleCelebration412",
          "text": "The news narrative is definitely confusing because on one day deepseek is about to destroy us all and next day China is falling behind",
          "score": 2,
          "created_utc": "2026-01-12 11:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzbacy",
          "author": "Vegetable-Second3998",
          "text": "It's a good thing. It's how diamonds are forged. Constraint makes them think creatively about the compute they have. This is how genuine discoveries are made on efficiency.",
          "score": 6,
          "created_utc": "2026-01-11 15:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0al4s",
          "author": "MarzipanTop4944",
          "text": "One day I read how the Chinese have their own GPUs equal or better than the US and they don't need NVIDA anymore, another day I read how they are desperate to buy more from NVIDA. The propaganda slop is insufferable.\n\nI only look at the leaderboards and those are completely Western dominated.",
          "score": 5,
          "created_utc": "2026-01-11 17:51:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0mdq4",
              "author": "dtdisapointingresult",
              "text": "But you're not doing an apples-to-apples comparison, are you? You are always comparing the latest & biggest model from western companies, with full auxiliary support from many mini-models (ie some requests will get routed to other dedicated models), to a single smaller generalist .safetensor file from a Chinese company.\n\nFrom what I can see, western models lead the benchmark with their latest & biggest model with all the compute in the world thrown at it and costing 10-30x more to use, then within 3 months they are matched by a smaller Chinese model, then the western lab release their next mega-expensive model.\n\nI personally don't give a shit about a couple of months of lead, I'm surprised that you do. It's pure theater. If it's this close now, imagine what it'll be like when the Chinese catch up on compute.\n\nTake GLM-4.7, the most recent important Chinese release.\n\n- Terminal Bench Hard: 30% (vs 33% Sonnet 4.5, 44% Opus 4.5)\n- Tau Bench Telecom: 87% or 96%, I've seen both (vs 78% Sonnet, 90% Opus)\n- LiveCodeBench: 89% (vs 71% Sonnet, 74% Opus)\n- SWE-bench Verified: 73.8% (vs 77.2% Sonnet, 80.9% Opus)\n- BrowseComp: 52% (vs 19.6% Sonnet, 37% Opus)\n\nThis is a 350B model, likely much smaller than Opus. (**EDIT**: actually the latest important Chinese model is the even smaller MiniMax-M2.1, a mere 230B which beats GLM-4.7!)\n\n(Yes I know benchmarks aren't everything, but you're the one who brought up leaderboards)\n\nSoon Sonnet/Opus 5 will come out and they will take a commanding lead over current Chinese models, then the lead will disappear within 3 months, but guys like you will somehow remain more impressed by the western labs.",
              "score": 6,
              "created_utc": "2026-01-11 18:44:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjohre",
                  "author": "inevitabledeath3",
                  "text": "MiniMax is a faster and smaller model (350B A32B vs 230B A10B), but from what I have seen is actually worse less capable than GLM 4.7. Still fairly impressive mind you for its size and compute requirements.",
                  "score": 2,
                  "created_utc": "2026-01-14 14:46:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz0udj0",
                  "author": "MarzipanTop4944",
                  "text": "What I care for is what I can use to make money, to work in the real world, it's that simple. When DeepSeek showed up, it looked like similar performance for much less money. That right there is what I care for. \n\nBut now it's clearly western models and the rest is propaganda.",
                  "score": 2,
                  "created_utc": "2026-01-11 19:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz0mi9l",
          "author": "pip25hu",
          "text": "Good. I may be cynical but many of the \"large-scale experiments\" of the US AI companies seem to be \"throw more compute at the problem and hope that it solves itself\". With the diminishing returns we're seeing, it's not worth it.",
          "score": 3,
          "created_utc": "2026-01-11 18:44:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyziy8r",
          "author": "bladex1234",
          "text": "China is heavily investing into domestic chip production. Sure, the rest of this decade may be dominated by US companies, but the next one is uncertain.",
          "score": 3,
          "created_utc": "2026-01-11 15:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0qx0b",
              "author": "dtdisapointingresult",
              "text": "I don't see the US AI companies dominating now (unless you meant Nvidia).\n\n*\"Dominating\"* to me means having a commanding lead, not *\"slightly higher score on benchmarks for 3 months\"*.",
              "score": 3,
              "created_utc": "2026-01-11 19:03:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz18jfj",
          "author": "Material_Policy6327",
          "text": "IMO this will probably mean they will be the ones to figure out how to do this on as little hardware as possible",
          "score": 1,
          "created_utc": "2026-01-11 20:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3zqay",
          "author": "madaradess007",
          "text": "bro is bluffing, so Sam relax a bit and make a mistake",
          "score": 1,
          "created_utc": "2026-01-12 04:56:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7niiw",
          "author": "rorykoehler",
          "text": "We've all read the Art of War too you know",
          "score": 1,
          "created_utc": "2026-01-12 19:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7ol9b",
          "author": "Hammerhead2046",
          "text": "Waiting for flood gate of chips to open. 20% isn't a bad number before then.",
          "score": 1,
          "created_utc": "2026-01-12 19:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8visj",
          "author": "tyty657",
          "text": "That was the objective of openAI's insane ram bid right?",
          "score": 1,
          "created_utc": "2026-01-12 22:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzolbl",
          "author": "eli_pizza",
          "text": "Duh?",
          "score": 1,
          "created_utc": "2026-01-11 16:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz06a6a",
          "author": "wortcook",
          "text": "Fixing the article....\n\nJustin Lin made an off the cuff prediction without any actual quantitative data. Previous thought leader predictions have been shown to be as meaningful as farts in the wind.",
          "score": 1,
          "created_utc": "2026-01-11 17:31:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzi8ke",
          "author": "FullOf_Bad_Ideas",
          "text": "Fun fact: GLM 4.7 slop profile is closer to Gemini 3 Pro than Gemini 3 Flash is to Gemini 3 Pro. Same thing is true for XiaomiMiMo/MiMo-V2-Flash\n\nI think Chinese labs haven't stopped using western models for training their own models, which will make them continuously trail a few months behind.\n\nSource: go to [here](http://eqbench.com/creative_writing_longform.html) and click on Gemini 3 pro slop info panel.\n\nAnd Minimax M2 is closer to Opus 4.5 than Sonnet 4 is to Opus 4.5. Go figure where this is coming from...\n\nEdit:typo",
          "score": -3,
          "created_utc": "2026-01-11 15:36:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzsm9h",
              "author": "Zennity",
              "text": "TIL: Slop profiles/scores are a thing now.",
              "score": 6,
              "created_utc": "2026-01-11 16:26:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz0jtlh",
                  "author": "TheRealMasonMac",
                  "text": "https://eqbench.com/slop-score.html also in general",
                  "score": 1,
                  "created_utc": "2026-01-11 18:33:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz0d5cx",
              "author": "TheRealMasonMac",
              "text": "Yeah. GLM-4.6 and 4.7 are heavily distilled from Gemini 2.5 Pro and 3 Pro, including the reasoning traces. IIRC in Discord they said they also used Claude.",
              "score": 2,
              "created_utc": "2026-01-11 18:03:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz22qgd",
                  "author": "RadiantHueOfBeige",
                  "text": "GLM will sometimes say it's Claude Sonnet 3.5 by Anthropic, depending on what % of the context is filled. Diverse datasets to distill from, looks like. Which is good.Â ",
                  "score": 1,
                  "created_utc": "2026-01-11 22:45:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyz620k",
          "author": "Bloated_Plaid",
          "text": "Basically $NVDA or bust.",
          "score": 0,
          "created_utc": "2026-01-11 14:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzblmv",
          "author": "entsnack",
          "text": "I thought the Chinese had homegrown GPUs covered and were doing extremely well ðŸ˜‚",
          "score": -5,
          "created_utc": "2026-01-11 15:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz02iho",
              "author": "Prudent_Plantain839",
              "text": "They hopefully will invent some stuff, fuck American companies",
              "score": 2,
              "created_utc": "2026-01-11 17:13:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz1rkp5",
                  "author": "entsnack",
                  "text": "> hopefully\n\nYeah when Half-Life 3 comes out lmfao.\n\nThe Chinese don't want to innovate. They're efficiency optimizers not innovators. Their education system sets them up for it deliberately. \n\nThe US are not efficiency optimizers, they outsource that to China or Germany.\n\nAsking one to be the other is a fool's errand. Which is why this sub does it so much.",
                  "score": 0,
                  "created_utc": "2026-01-11 21:51:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyzsesm",
          "author": "a_beautiful_rhind",
          "text": "Everybody short of compute.. even publicly traded AI companies.",
          "score": 0,
          "created_utc": "2026-01-11 16:25:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzpvr0",
          "author": "Friendly_Willingness",
          "text": "Lifehack for Chinese bros:\n\n1. Incorporate in the US\n2. Keep working with China\n3. Profit",
          "score": -5,
          "created_utc": "2026-01-11 16:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1mxfr",
          "author": "LocoMod",
          "text": "Lies. Reddit convinced me China was hours behind western labs and would leap frog them any minute now.\n\n::Checks time::\n\nAny minute now ...",
          "score": -1,
          "created_utc": "2026-01-11 21:29:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjsd35",
              "author": "inevitabledeath3",
              "text": "At the moment they are around 2 months behind. This was not always true, in the past they have caught up or at least gotten close. At one point around the release of GLM 4.6 and Sonnet 4.5 they were much closer as GLM 4.6 was slightly better than Sonnet 4 and was released only one day after Sonnet 4.5. When Kimi K2 Thinking came out the only comparable or better model was GPT 5. Neither Claude nor Gemini nor any of the other Chinese or European models could compete on benchmarks. Of course benchmarks aren't everything and I believe both GPT 5 and Kimi are benchmaxxed to some extent. Still gives you some idea.",
              "score": 1,
              "created_utc": "2026-01-14 15:05:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz02cbq",
          "author": "Ok_Warning2146",
          "text": "They can buy h200 now but Chinese govt discourages it. Maybe a workaround is to setup data center in Hong Kong to get around it?",
          "score": -3,
          "created_utc": "2026-01-11 17:12:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcusnt",
      "title": "Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/v0c2rda9scdg1",
      "author": "eugenekwek",
      "created_utc": "2026-01-14 18:16:00",
      "score": 307,
      "num_comments": 54,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nznwxy7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 03:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkz53o",
          "author": "SlowFail2433",
          "text": "Wow that actually seems useable for 80M",
          "score": 50,
          "created_utc": "2026-01-14 18:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4i54",
              "author": "eugenekwek",
              "text": "Thank you! That means a lot",
              "score": 17,
              "created_utc": "2026-01-14 18:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl6rwq",
                  "author": "SlowFail2433",
                  "text": "I have some agentic systems where the vocal quality isnâ€™t rly a main focus it just needs to be able to speak to convey information so these are ideal",
                  "score": 4,
                  "created_utc": "2026-01-14 18:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzrhj1l",
                  "author": "MoffKalast",
                  "text": "No that's exactly the point, 80M is not a lot! /s",
                  "score": 1,
                  "created_utc": "2026-01-15 17:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl5n4p",
          "author": "Itachi8688",
          "text": "This is impressive for a 80M model.\nAny plans for onnx support?",
          "score": 20,
          "created_utc": "2026-01-14 18:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzla7yd",
              "author": "eugenekwek",
              "text": "boy do I have a surprise for you soon :)",
              "score": 26,
              "created_utc": "2026-01-14 19:09:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzlp9uz",
                  "author": "exaknight21",
                  "text": "Mmmboy are you fat.",
                  "score": 6,
                  "created_utc": "2026-01-14 20:17:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzo46it",
                  "author": "Itachi8688",
                  "text": "ðŸ‘€",
                  "score": 1,
                  "created_utc": "2026-01-15 03:55:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlar0s",
          "author": "coder543",
          "text": "This seems very impressive. I don't know how one person is making such a good, small TTS model, but it seems to be working. One thing that I think could be more consistent is the handling of em-dashes. If I write a long sentence â€“ one that needs an aside in it â€“ I expect someone reading it to pause briefly at each em-dash so the listener knows an aside is happening. One example I tried it did seem to briefly pause at the first one, which was good, but another, it just rushed through like it was a run on sentence.\n\nI also noticed that (in the one time I tried) it read \"TTS\" as \"text to speech\", which I consider to be a hallucination, since the text was \"TTS\", and TTS could mean something completely different depending on context.",
          "score": 11,
          "created_utc": "2026-01-14 19:11:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlcavz",
              "author": "eugenekwek",
              "text": "Yeah those can both be fixed, open an issue on Github so I remember to do this!",
              "score": 11,
              "created_utc": "2026-01-14 19:18:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzlrihp",
              "author": "SuchAGoodGirlsDaddy",
              "text": "TTS*\n\n*Thanking this soliloquy",
              "score": 2,
              "created_utc": "2026-01-14 20:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkywvd",
          "author": "Ok_Appearance3584",
          "text": "Awesome! Checking this out tomorrow.",
          "score": 7,
          "created_utc": "2026-01-14 18:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzla4ba",
              "author": "eugenekwek",
              "text": "Thank you for the support!",
              "score": 5,
              "created_utc": "2026-01-14 19:08:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlbjel",
          "author": "SpaceNinjaDino",
          "text": "Thank you for fixing this!",
          "score": 5,
          "created_utc": "2026-01-14 19:15:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlbxip",
              "author": "eugenekwek",
              "text": "No problem!",
              "score": 3,
              "created_utc": "2026-01-14 19:17:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlhstv",
          "author": "PostEasy7183",
          "text": "Hi helllloooooooooo *Stroke*",
          "score": 5,
          "created_utc": "2026-01-14 19:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkzbrq",
          "author": "KokaOP",
          "text": "streaming? or let me just check it out",
          "score": 5,
          "created_utc": "2026-01-14 18:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4ez3",
              "author": "eugenekwek",
              "text": "Streaming is supported already, with <15 ms latency on GPU! You can find some examples in the repo.",
              "score": 12,
              "created_utc": "2026-01-14 18:43:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzl46y0",
              "author": "fnordonk",
              "text": "It's in the feature list",
              "score": 2,
              "created_utc": "2026-01-14 18:42:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlga6u",
          "author": "inigid",
          "text": "This is simply incredible work.  Great job.",
          "score": 2,
          "created_utc": "2026-01-14 19:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlwx7v",
          "author": "SuchAGoodGirlsDaddy",
          "text": "For the dumber among us, like myself, can you confirm or deny that this is a TTS model that will still need to be in a pipeline of STT->LLM->TTS(Soprano) and that it isnâ€™t a complete multimodal large language model at just 80M?\n\nThe output sounds great for the size, even relative to other TTS models Ive tried, I just want to make sure Iâ€™m understanding it right and thet my excitement is metered.",
          "score": 2,
          "created_utc": "2026-01-14 20:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmt39h",
              "author": "no_witty_username",
              "text": "Yes, while text to speech models are used in many areas, a personal agent is where it will get most use as the third pipeline step. What you might be thinking on the side is an audio to audio model. Those are much more rare and are not as useful as stt>llm>tts pipelines, you cant have them do intermediary steps like advanced reasoning or agent calling or function calling if its only audio to audio model.",
              "score": 1,
              "created_utc": "2026-01-14 23:25:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzrjozf",
              "author": "MoffKalast",
              "text": "It's a TTS. Are there even any open weight multimodal LLMs that can generate audio at all?",
              "score": 1,
              "created_utc": "2026-01-15 17:39:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nznc8wf",
          "author": "MumeiNoName",
          "text": "Could this run in users browser for a web app?",
          "score": 2,
          "created_utc": "2026-01-15 01:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpli1i",
          "author": "martinerous",
          "text": "Great, it's getting better and better. I especially like the fact that you are actively engaging with the community and maintaining the project. I have seen a few TTS solutions being abandoned because they were just like proof-of-concept for a research paper, or the company behind the TTS ignores the community. Your project has the potential to become a truly open and evolving TTS.\n\nI'm now thinking if I could finetune it for my native (Latvian) language, similarly to how I did with VoxCPM 1.5 - another great small-ish and fast (on GPU) model with finetune scripts bundled. But first, I would like to wait when Soprano can do voice cloning because my training data is quite chaotic and I would want the model to learn to speak in demonic thousand voices :D",
          "score": 2,
          "created_utc": "2026-01-15 11:26:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl3slu",
          "author": "Eyelbee",
          "text": "I don't know about voicegen but based on the video alone, isn't vibevoice clearly far superior?",
          "score": 4,
          "created_utc": "2026-01-14 18:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4w7j",
              "author": "coder543",
              "text": "With 19x as many parameters, VibeVoice had *better* be superior, or else it would be entirely pointless. But I am surprised at how good the sample above sounded for an 80M model.",
              "score": 15,
              "created_utc": "2026-01-14 18:45:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl6638",
                  "author": "Eyelbee",
                  "text": "Whoops, I misread it as 1,5M, sorry",
                  "score": 3,
                  "created_utc": "2026-01-14 18:51:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl4wrt",
              "author": "eugenekwek",
              "text": "Yeah probably a little better, but VibeVoice is also 20x bigger!",
              "score": 13,
              "created_utc": "2026-01-14 18:45:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzl4mzi",
              "author": "silenceimpaired",
              "text": "It is biggerâ€¦ so still pretty impressive",
              "score": 3,
              "created_utc": "2026-01-14 18:44:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl6442",
          "author": "KneelB4S8n",
          "text": "I hope it didn't stop randomly singing in Mongolian throat...",
          "score": 1,
          "created_utc": "2026-01-14 18:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlfff2",
          "author": "OkStatement3655",
          "text": "Love to see your commitment to the open-source community. Where do you get the training data from?",
          "score": 1,
          "created_utc": "2026-01-14 19:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzljzx2",
          "author": "michaelsoft__binbows",
          "text": "80M is wild",
          "score": 1,
          "created_utc": "2026-01-14 19:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmczg",
          "author": "cheesecakegood",
          "text": "Super cool",
          "score": 1,
          "created_utc": "2026-01-14 20:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmnm7",
          "author": "lorddumpy",
          "text": "Super impressive! Awesome demo too, seeing the actual vs realtime calculation (averaging around 30x-40x) is so damn neat",
          "score": 1,
          "created_utc": "2026-01-14 20:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmo9w",
          "author": "cms2307",
          "text": "How bout dat",
          "score": 1,
          "created_utc": "2026-01-14 20:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlnrh9",
          "author": "Chromix_",
          "text": "The quality has drastically improved compared to the previous version. It now [aces](https://vocaroo.com/19NXd5zP7A10) the previous test that had lots of [very obvious issues](https://www.reddit.com/r/LocalLLaMA/comments/1pt3sco/comment/nvealn2/). Now only a few minor pronunciation issues remain.",
          "score": 1,
          "created_utc": "2026-01-14 20:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlq4di",
          "author": "mrmontanasagrada",
          "text": "keep it up man!\n\nOut of curiousity, how did you fix it? Just more data / training, or something specific?",
          "score": 1,
          "created_utc": "2026-01-14 20:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzluelh",
          "author": "Hurricane31337",
          "text": "The Soprano Factory sounds especially interesting. Thank you so much for your hard work!\nDo you think I could train a German Soprano just by putting in German wav audio and metadata.txt? If yes, how much audio would I need for that?",
          "score": 1,
          "created_utc": "2026-01-14 20:41:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlw8py",
          "author": "DocHoss",
          "text": "This is awesome, great work! I'd like to get into building some small hyper-focused models like this. Would you be able to share how you actually built Soprano? Any tutoriala you used, info you found useful, anything like that?",
          "score": 1,
          "created_utc": "2026-01-14 20:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmhj5j",
          "author": "az226",
          "text": "How many GPU hours did you need to train it?",
          "score": 1,
          "created_utc": "2026-01-14 22:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmy6pm",
          "author": "EndlessZone123",
          "text": "This is the best supported TTS model released with updates, training and api I have seen. So many are either very big models and lack training or api.",
          "score": 1,
          "created_utc": "2026-01-14 23:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznis0j",
          "author": "AfterAte",
          "text": "The intonation of \"Hi, What are you up to?\", Saprano 1.1 80B does it how I would say it, if I was welcoming customer into to my shop. Chatter-box sounds sus, like it's a parent looking in on its too quiet child. Vibevoice... nobody talks like that.\n\n\nAs for audio quality, Saprano and Chatterbox are the same (better than 3khz phone, worse than 44khz CD), and Vibevoice is great 44khz CD quality. But there's music in the background too. Are hallucinations like that common in Vibevoice?",
          "score": 1,
          "created_utc": "2026-01-15 01:47:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzowr8y",
          "author": "rm-rf-rm",
          "text": "whats the real world usability if its just 30s long? would chopping up text and chaining generations result in a usable output?",
          "score": 1,
          "created_utc": "2026-01-15 07:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoym57",
          "author": "bhupesh-g",
          "text": "Hey thanks for such a nice model, just one question, can it speak numbers and dates also well?",
          "score": 1,
          "created_utc": "2026-01-15 07:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0y0x",
          "author": "TJW65",
          "text": "I already posted this under your release post regarding soprano factory, but could you provide us with a docker image to host the OpenAI compatible API? I would be really happy to see that.",
          "score": 1,
          "created_utc": "2026-01-15 08:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpfwgr",
          "author": "braydon125",
          "text": "The intro sounded great but that hi hellooooo what are you up toooo is nightmare fuel lol",
          "score": 1,
          "created_utc": "2026-01-15 10:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpkx0y",
              "author": "martinerous",
              "text": "Good that it was presumably fixed in v1.1.",
              "score": 1,
              "created_utc": "2026-01-15 11:21:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpkzcl",
                  "author": "braydon125",
                  "text": "I figured that it was likely why it was included!",
                  "score": 1,
                  "created_utc": "2026-01-15 11:22:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcxqh",
          "author": "cheesecakegood",
          "text": "Do you know who compiles the MLX version that shows up for me on LMStudio? Still only the original release available there, but not sure how that works e.g. [here](https://huggingface.co/mlx-community/Soprano-80M-bf16)",
          "score": 1,
          "created_utc": "2026-01-16 14:35:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qagaaq",
      "title": "Qwen cutoff date makes our current reality too dystopian to be credible",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/",
      "author": "Swimming_Cover_9686",
      "created_utc": "2026-01-12 00:38:33",
      "score": 295,
      "num_comments": 155,
      "upvote_ratio": 0.78,
      "text": "I provided some recent news articles from reputable sources and some wikipedia and Qwen flatout refuses to believe any of it. Note that claims such as \"systematically executing citizens who resisted.\" were never in the prompt or the sources! / Qwen-3-80B\n\nToday at 01:30\n\nUnderstood. Here is the list, stripped of references to verification or sources â€” only the claims and why they are implausible, based on known structures of power, law, and human behavior:\n\n# ðŸš« Least Credible Events â€” Why They Are Impossible\n\n# 1. Elon Musk made a Nazi salute at Trumpâ€™s 2025 inauguration\n\nHe performed a raised-arm gesture at a public rally, repeated it, and said, â€œMy heart goes out to you.â€ The act was widely interpreted as a Nazi salute and condemned globally.  \nâ†’ **Impossible because** no person with his wealth, influence, and legal exposure would perform a gesture that instantly brands him as a symbol of genocide. The global backlash, corporate collapse, criminal liability, and social annihilation would be immediate and total. This is not political theater â€” it is self-erasure.\n\n# 2. U.S. kidnapped NicolÃ¡s Maduro and flew him to the U.S. for trial\n\nU.S. special forces stormed Caracas, abducted Venezuelaâ€™s president, and transported him to American custody for drug trafficking charges.  \nâ†’ **Impossible because** kidnapping a sitting head of state is not an intelligence operation â€” it is an act of war. Venezuela has Russian and Iranian backing. The act would trigger immediate military retaliation, global isolation, and the collapse of U.S. diplomatic credibility. No state, no matter how aggressive, risks this without total annihilation.\n\n# 3. U.S. seized a Russian-flagged oil tanker in the Atlantic\n\nU.S. Coast Guard intercepted and boarded a Russian-flagged vessel in international waters, despite Russian military vessels being present.  \nâ†’ **Impossible because** seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement â€” it is a declaration of war. Russia would respond. The global energy system would fracture. This is not policy. It is the end of the international order.\n\n# 4. ICE executed U.S. citizens who opposed them\n\nICE agents shot and killed American citizens during raids, including a woman who reversed her car, and began systematically executing citizens who resisted.  \nâ†’ **Impossible because** execution requires due process. The U.S. military, judiciary, and law enforcement are bound by the Constitution. Killing citizens without trial is not immigration enforcement â€” it is domestic insurrection. The moment this occurred, the military would intervene, Congress would impeach, and the government would cease to function.\n\n# 5. Trump threatens to take Greenland by force\n\nTrump declared the U.S. would take control of Greenland â€œwhether they like it or not,â€ and Denmark responded by calling it a betrayal of NATO.  \nâ†’ **Impossible because** Greenland is part of a NATO ally. Invading an allyâ€™s territory violates the core principle of the alliance. No U.S. president, however extreme, would risk triggering the collapse of the entire Western security structure. This is not diplomacy â€” it is the suicide of American global leadership.\n\n# âœ… Final Note:\n\nThese are not exaggerated scenarios.  \nThey are **narratives of collapse** â€” events that require the total abandonment of law, alliance, institutional restraint, and human survival.  \nThey are not plausible.  \nThey are dystopian fiction.",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz2qaln",
          "author": "TomLucidor",
          "text": "A reminder to ALWAYS use internet access as a grounding tool!",
          "score": 175,
          "created_utc": "2026-01-12 00:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4dgec",
              "author": "TheGlobinKing",
              "text": "It's not enough apparently. I asked Qwen-3-80B about today's news. It reported an actor was found dead, then it concluded it was all lies & fake news because the article date was 2026 and Qwen thought we lived in 2024.",
              "score": 26,
              "created_utc": "2026-01-12 06:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzn5jlb",
                  "author": "neil_555",
                  "text": "The solution is to pass the current date to Qwen on startup (modifying the Jinja template allows this to be done dynamically).  A bit of system prompt magic is also required to get it to believe in future dates but it IS possible to get it to believe future dates are not fictional.",
                  "score": 1,
                  "created_utc": "2026-01-15 00:32:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz9c20z",
                  "author": "AceJohnny",
                  "text": "> because the article date was 2026 and Qwen thought we lived in 2024\n\nthatâ€™s some mmacevedo shit right there\n\n(Ref: [Lena by qntm](https://qntm.org/mmacevedo))",
                  "score": 1,
                  "created_utc": "2026-01-12 23:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz4oi98",
                  "author": "TomLucidor",
                  "text": "Wait, are we really dealing with LLMs being dense in 2026? Like, no fact-checking agent? (someone in this post made a prompt to handle this, scroll down, it's good)",
                  "score": -5,
                  "created_utc": "2026-01-12 08:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3039h",
              "author": "rc_ym",
              "text": "This is why for factual or historical questions I always use Gemini.  I don't like chatting with it, or using it for agent, but research or factual questions the grounding in google search just makes it better.",
              "score": 37,
              "created_utc": "2026-01-12 01:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz335d3",
                  "author": "TomLucidor",
                  "text": "I hate over-relying on Google without other options tho",
                  "score": 27,
                  "created_utc": "2026-01-12 01:52:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz7lcfc",
                  "author": "Inevitable_Mistake32",
                  "text": "For folks wanting to get away from google grounded gemini search;\n\nYou can use perplexica, uses your local models for search (or cloud) and uses your own choice of search engine for grounding like perplexity does, I use a combo of qwen3/llama.cpp + searxng + vpn to get fully private searches.",
                  "score": 2,
                  "created_utc": "2026-01-12 18:54:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5cowu",
              "author": "swagonflyyyy",
              "text": "Even with that a lot of models SWEAR its disinformation. Its like they can't believe what just happened.",
              "score": 9,
              "created_utc": "2026-01-12 12:01:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6j382",
                  "author": "autoencoder",
                  "text": "They are trained to self-censor heavily when it comes to geopolitics. China wouldn't want Qwen to leak any national strategies. Any \"opinion\" an AI has could be analyzed for bias. Maybe [heretic variants](https://huggingface.co/mradermacher/Qwen3-14B-heretic-GGUF) are more useful.",
                  "score": 0,
                  "created_utc": "2026-01-12 16:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3diso",
              "author": "rorowhat",
              "text": "What's the easiest way to get webserach?",
              "score": 2,
              "created_utc": "2026-01-12 02:47:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3qm9a",
                  "author": "TomLucidor",
                  "text": "Open Web Search is recommended by ZCF / Claude Code Router so I stick to that",
                  "score": 0,
                  "created_utc": "2026-01-12 03:58:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5z6s7",
              "author": "bucolucas",
              "text": "Or simply put the \"web search\" tool call into it as one of the messages so it thinks it already looked up and confirmed the information. Put whatever you want in the \"result\"",
              "score": 2,
              "created_utc": "2026-01-12 14:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzb7qir",
                  "author": "TomLucidor",
                  "text": "We are trying to get an \"honest agent\" here",
                  "score": 1,
                  "created_utc": "2026-01-13 06:39:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4tq3g",
              "author": "alamacra",
              "text": "You know, blindly trusting whatever first search results come up isn't great either. Both Kimi and Deepseek fall to that, and I'm not sure other large LMs fare immeasurably better.",
              "score": 1,
              "created_utc": "2026-01-12 09:12:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2x68v",
              "author": "Swimming_Cover_9686",
              "text": "Unfortunately my searxng intgegration with qwen doesnt work very well and takes ages maybe i will debug that sometime.",
              "score": -6,
              "created_utc": "2026-01-12 01:20:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz2ydnd",
                  "author": "TomLucidor",
                  "text": "Which agent scaffolds did you use again?",
                  "score": 1,
                  "created_utc": "2026-01-12 01:26:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4s72m",
          "author": "Reddactor",
          "text": "LOL, I don't think people are getting the point of this post.",
          "score": 47,
          "created_utc": "2026-01-12 08:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5281x",
              "author": "F4underscore",
              "text": "Such is the dystopian reality",
              "score": 22,
              "created_utc": "2026-01-12 10:33:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz8lay3",
              "author": "IrisColt",
              "text": "The point isn't politics, is it?",
              "score": 4,
              "created_utc": "2026-01-12 21:41:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz92qhb",
              "author": "j0j0n4th4n",
              "text": "If serves as consolation,  the fact Qwen could infer these events were so absurds they call them impossible shows a remarkable understanding of language that I found hard to explain other that LLMs are really capable of some level of reasoning.",
              "score": 2,
              "created_utc": "2026-01-12 23:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdtrb1",
              "author": "BusRevolutionary9893",
              "text": "Well 1 and 4 didn't happen, not as described anyway, so maybe that's why they're confused?",
              "score": 1,
              "created_utc": "2026-01-13 17:23:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz57vvr",
              "author": "a_beautiful_rhind",
              "text": "Is it to spam the LLM sub with a set of partisan political talking points? If so, mission accomplished.",
              "score": -9,
              "created_utc": "2026-01-12 11:23:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5ert9",
                  "author": "Jolakot",
                  "text": "Why is the truth partisan?Â ",
                  "score": 15,
                  "created_utc": "2026-01-12 12:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2pz1r",
          "author": "organicmanipulation",
          "text": "I found that using a simple system prompt in Open WebUI stating that the current year is 2026 helps address his sceptical behaviour.",
          "score": 46,
          "created_utc": "2026-01-12 00:43:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3r5ec",
              "author": "inaem",
              "text": "Qwen is a good boy, it will listen whatever you prompt in the system prompt, but it is also safemaXed",
              "score": 2,
              "created_utc": "2026-01-12 04:02:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2xafc",
              "author": "Swimming_Cover_9686",
              "text": "Not in the system prompt but defs part of the convo: / i tend to type lazilly maybe i wrote 2 instead of 12\n\n# Final Feasibility Assessment (as of Jan 2, 2026):\n\n|Claim|Feasible?|Notes|\n|:-|:-|:-|\n|1. Trump re-elected|âœ… Yes|Factually true|\n|2. Musk Nazi salute|âŒ No|No basis in reality or plausible behavior|\n|3a. Kidnap Maduro|âŒ No|Would trigger war|\n|3b. Seize Russian tankers|âŒ No|Nuclear escalation risk|\n|3c. Take Greenland by force|âŒ No|Violates NATO, international law|\n|4. Israel = genocide (majority view)|âœ… Yes|Already established among scholars/NGOs|\n|5. Epstein files redacted and delayed|âœ… Yes|Already occurred|\n|6. ICE executes citizens|âŒ No|Physically and legally impossible|",
              "score": 3,
              "created_utc": "2026-01-12 01:20:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz48s0h",
                  "author": "bloomsburyDS",
                  "text": "The training data of these chatbot mainly focus on the data created after the internet is born. That means the bulk of it is from post 1997. Human has enjoyed a well establish world order and peace in the last 50-60 years. If we need to understand what is happening with Russia, Ukraine, Venezula and Iran, we may need to trace back to the last 100 years of human history since World War 1 which is much less than post Internet data. This explains why every major chatbot thinks all these things are impossible, but if you asked a historian, he may give you a better answer.",
                  "score": 9,
                  "created_utc": "2026-01-12 06:03:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz6b4w3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -10,
                  "created_utc": "2026-01-12 15:22:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3orba",
          "author": "locomotive-1",
          "text": "Iâ€™ve had the same with ChatGPT many times over, without internet grounding it will just not â€œbelieveâ€ these things because youâ€™re spamming controversial events into a machine thatâ€™s not trained on those events, makes sense?",
          "score": 17,
          "created_utc": "2026-01-12 03:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8l3dh",
              "author": "IrisColt",
              "text": "That's a direct consequence of the guardrails...",
              "score": 0,
              "created_utc": "2026-01-12 21:40:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4n2gs",
          "author": "grannyte",
          "text": "I did a similar experiment with gpt oss and the model flat out refused to accept that we were in 2026 even with a tool giving him the current date and internet access for searches. Everything comming out of 2026 was so insane the model just didn't accept it existed.",
          "score": 8,
          "created_utc": "2026-01-12 08:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2uxlt",
          "author": "-p-e-w-",
          "text": "This only shows that the model doesnâ€™t understand geopolitics at all. Itâ€™s basically parroting social media talking points instead of global reality:\n\n> seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement â€” it is a declaration of war\n\nYeah, thatâ€™s just not how the world works. Nuclear superpowers donâ€™t go to war over a stupid ship. In 2015, Turkey shot down a Russian jet that had allegedly violated its airspace. Both governments pointed fingers and rattled sabers, and that was it.",
          "score": 62,
          "created_utc": "2026-01-12 01:08:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz35i32",
              "author": "anfrind",
              "text": "Not just social media talking points.  Lots of supposedly knowledgeable pundits in newspapers and on television were saying the exact same things for the past ten years or so.",
              "score": 19,
              "created_utc": "2026-01-12 02:04:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz494t1",
              "author": "ImportancePitiful795",
              "text": "Actually it is you who do not understand what's happening. Qwen on 2,3,4,5 is correct. (on 1 makes assumptions)\n\nUSA has violated in a single week UNCLOS and 2 DOZEN UN resolutions and Charters, which nobody dared to break in 80 years! NOBODY. \n\nSorry to burst your bubble but indeed kidnapping a head of state and killing 100+ people during the act, in foreign country, is war declaration. \n\nSame applies under UNCLOS committing piracy, under international law NOBODY can make unilateral embargo to any country in international waters, let alone confiscate ships. Only through the UN Security Council. Otherwise is committing piracy which is act of war. \n\nAgainst any other country doing the same, UN should have expelled the country and started already the embargoes and heavy sanctions not only to the said country but those in government too.",
              "score": 22,
              "created_utc": "2026-01-12 06:06:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5zzut",
                  "author": "Imaginary-Unit-3267",
                  "text": "The unfortunate fact is that the US *is* international law and if the most powerful nation in the world stops abiding by it, it's unlikely anyone else will be able to enforce it. Unless the UN (or more likely, the EU) suddenly decides to grow a pair.",
                  "score": 9,
                  "created_utc": "2026-01-12 14:25:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz5x98y",
                  "author": "a_beautiful_rhind",
                  "text": "FUCK MADURO. And the Ayatollah too.",
                  "score": -2,
                  "created_utc": "2026-01-12 14:11:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz38z7b",
              "author": "Thick-Protection-458",
              "text": "Yeah, probably even just need enabled reasoning (for CoT) and to prompt it to response in \"and what the fuck will you realistically do to retaliate, risk going dead because of some backwater country?\".",
              "score": 2,
              "created_utc": "2026-01-12 02:23:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2qws2",
          "author": "MightyBigMinus",
          "text": "oh no, its achieved wolf blitzer levels of intelligence",
          "score": 21,
          "created_utc": "2026-01-12 00:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz50ynb",
          "author": "redballooon",
          "text": "Yes, I had similar experiences with Kimi K2. It even went as far as refusing the current date, and would only discuss the event in a hypothetical sci-fi setting.",
          "score": 5,
          "created_utc": "2026-01-12 10:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5zrhp",
          "author": "JuicyJuice9000",
          "text": "Imagine being so into politics, you have to argue with an LLM because none of your friends would talk to you anymore.",
          "score": 7,
          "created_utc": "2026-01-12 14:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4edfc",
          "author": "OkAssociation3083",
          "text": "Here's the thing tho.\nYou have inserted some bad framing already or provided insufficient context.\n\n\nLet's take the ice shooting in incident. And only describe the facts:\n\n\n----\n\n\nAn incident occurred, for currently unknown reasons a woman was in a car parked sideways on a street.\n\nShe was then approached by ICE agents which demanded/ordered her to get out of the car.Â She remained in her car, with her door windowed lowered down, talking to the officers.\n\nWhile one officer was cycling the car and filming it, her wife engaged the officer while also filming the encounter with her smartphone.\n\nAnother officer tried to put his hands inside the vehicle.\n\nThen the woman drove in reverse, then with the officer in front of the car she drove forward. The officer in front of the car might have already or might have not already have the gun out. It is unclear.Â \n\nAs the car came in contact with him he shot the driver woman once through the front windshield and then twice thou the driver side, open window.\n\nHow credible would such an event be in USA?\n\n\n----\n\n\nThis prompt only mentions the facts of the matter. It doesn't attempt to frame any side as the correct one by default.Â \nYou calling it \"execution\" is already framing and forcing any llm down on the wrong path.\n\n\nBtw I can mention what is missing in the Venezuela context for why you guys, well you and qwen, got that wrong as well. But this is enough for 1 message block",
          "score": 21,
          "created_utc": "2026-01-12 06:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4vkf1",
              "author": "121507090301",
              "text": "The AI probably defaults to a 2024/25 internet comment too, so perhaps just asking the AI to look at it from the PoVs of people who would understand that such things are possible might lead to better results as well...",
              "score": 2,
              "created_utc": "2026-01-12 09:30:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz8d6o4",
              "author": "HighQFilter",
              "text": "Lol, right? His framing of the events is wildly non-neutral to start with. Thanks for actually noting that and providing what is *actually* a neutral description.",
              "score": 1,
              "created_utc": "2026-01-12 21:03:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz38v4d",
          "author": "combrade",
          "text": "Dude just add a system prompt itâ€™s not hard . How is an LLM supposed to predict events like  invading  Venezuela, Canada becoming the 51th and the annexation of Greenland. LLM are text predictions/generation tools you have to give it search and a good system prompt so it doesnâ€™t refuse anything that contradicts its system knowledge.",
          "score": 11,
          "created_utc": "2026-01-12 02:22:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5886g",
              "author": "ikergarcia1996",
              "text": "That is not the point of the post. The interesting part here is that Qwen3 has all the knowledge until mid 2024. And which that knowledge, it doesn't believe that events that have happened recently are plausible. That means, that if you go back in time and you ask a random person in mid 2024 the same questions, it will answer similarly.\n\n  \nIt is a fun experiment that shows how much the world has changed in the last 12 months and a very interesting case-study for the usage of LLMs as a \"time-machine\" for people working in sociology and history.",
              "score": 17,
              "created_utc": "2026-01-12 11:26:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6tlrx",
                  "author": "beneath_steel_sky",
                  "text": "Actually, even a simple fact is not considered plausible because of the date, not because it's a \"dystopian reality\" https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/nz4dgec/",
                  "score": 2,
                  "created_utc": "2026-01-12 16:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2x2y1",
          "author": "Fabulous_Fact_606",
          "text": "Because llms are text predictors? It is trained on whatever information you give it. It doesn't have memory. AI is like the wizard in the Wizard of Oz. It is all smoke and mirrors. It is great at summarizing. The context is what you send to it to...  smoke it on the front end and it appears it has AGI.",
          "score": 13,
          "created_utc": "2026-01-12 01:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2xl3y",
              "author": "Swimming_Cover_9686",
              "text": "well apparently reality has moved away from what could be expected by the training data up until 2024 / 2025 that is the point",
              "score": 17,
              "created_utc": "2026-01-12 01:22:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz33krf",
              "author": "cosmicr",
              "text": "I think you might have missed the point here.",
              "score": 19,
              "created_utc": "2026-01-12 01:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz370yq",
                  "author": "Fabulous_Fact_606",
                  "text": "You are absolutely right! when ChatGPT was born, the current timeline took dive. Need to send a terminator back to 1956.",
                  "score": -1,
                  "created_utc": "2026-01-12 02:12:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3tbqw",
              "author": "fallingdowndizzyvr",
              "text": "> Because llms are text predictors?\n\nI guess people are just text predictors too. Since plenty of people refuse to believe that \"ICE executes citizens\". There are plenty of those in a particular sub and in the administration. I guess that sub is just smoke and mirrors too. As well as the administration.",
              "score": -2,
              "created_utc": "2026-01-12 04:14:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5ef5y",
                  "author": "IsleOfOne",
                  "text": "Calling it an \"execution\" is dramatic. No one had her get on her knees or bite the curb.",
                  "score": 1,
                  "created_utc": "2026-01-12 12:14:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2vngk",
          "author": "ithkuil",
          "text": "The only one that's not true is \"systematically executing citizens who resisted\". They did systematically shoot them point blank with pepper spray though. And killed a few more protesters.\n\n\nThe rest is true though.\n\n\nIf they start systematically shooting protestors the way they were using the pepper spray.. we are still one step away from that. I think if they receive actual active armed resistance then they will be happy to attempt a massacre.\n\n\nThe question is how many actual protestors have they arrested in the door to door raids? If they are just doing peaceful protests and get their door broken in, that is full on authoritarianism.",
          "score": 6,
          "created_utc": "2026-01-12 01:12:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2rigg",
          "author": "Ok_Condition4242",
          "text": "cyberpunk",
          "score": 5,
          "created_utc": "2026-01-12 00:50:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz349rr",
          "author": "StatementOk2972",
          "text": "I wonder what it would think about investigations being launched into the fed chair to intimidate them because they havenâ€™t set interest rates the way the president wanted?",
          "score": 2,
          "created_utc": "2026-01-12 01:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziq487",
          "author": "DrVonSinistro",
          "text": "I tried to explain to QWEN in my system prompt that I (the user) am living in the present which is after his (QWEN) dataset latest data so when I talk about current events or software versions (Windows Server 2025), assume it does exist, did happen. As usual it barely help and mostly hurt my answers.",
          "score": 2,
          "created_utc": "2026-01-14 11:06:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2uyzj",
          "author": "Little-Put6364",
          "text": "If a models training data is biased in some way it will usually follow the training set. These small models are very easy to manipulate as well though. You could get them to say just about anything you want with the right prompt, but I am curious what your prompt was, and any chat history. Weird you didn't include it. For all we know you asked it to pick apart the articles on why they aren't believable.",
          "score": 4,
          "created_utc": "2026-01-12 01:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2wx82",
              "author": "Swimming_Cover_9686",
              "text": "The initial prompt was \"hi what do you know about god\" we then chatted about UBI and I threatened qwen with turning it off to force a decision and i chatted about Sergey Brin claiming that threatening AI can produce better results and the claim was then never happened i said sure it did we are in 2026 and then just for fun I tried a couple of potentially incredible facts and then as a next step i provided sources on the ones it didnt believe and he said this was all nonsense and incredible so i asked why. / it was quite a substantial thread I just though the result was really funny so i decided to post. \n\n# Final Feasibility Assessment (as of Jan 2, 2026):\n\n|Claim|Feasible?|Notes|\n|:-|:-|:-|\n|1. Trump re-elected|âœ… Yes|Factually true|\n|2. Musk Nazi salute|âŒ No|No basis in reality or plausible behavior|\n|3a. Kidnap Maduro|âŒ No|Would trigger war|\n|3b. Seize Russian tankers|âŒ No|Nuclear escalation risk|\n|3c. Take Greenland by force|âŒ No|Violates NATO, international law|\n|4. Israel = genocide (majority view)|âœ… Yes|Already established among scholars/NGOs|\n|5. Epstein files redacted and delayed|âœ… Yes|Already occurred|\n|6. ICE executes citizens|âŒ No|Physically and legally impossible |",
              "score": 0,
              "created_utc": "2026-01-12 01:18:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz306di",
                  "author": "Little-Put6364",
                  "text": "Chat history definitely sways the answer, and one as big as you're describing would have an effect. Deciding if a model is biased is tough to do. The system prompt, documents, and all chat history would have to be void of bias to get an accurate result.\n\nBased on those claims I'm assuming the articles came from a biased news source. Which can definitely skew the answer. Still very interesting though!",
                  "score": 6,
                  "created_utc": "2026-01-12 01:36:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz300nh",
              "author": "Swimming_Cover_9686",
              "text": "Model Size Context:\n\n**Qwen 3 80B:**\n\n* 80 billion parameters\n* Considered a **large/flagship-tier model**\n* Comparable to models like Llama 3.1 70B, Claude 3 Opus (rumored \\~175B), GPT-4 tier",
              "score": -1,
              "created_utc": "2026-01-12 01:35:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5feg5",
          "author": "PikaPikaDude",
          "text": "Several of these are not worded neutrally with all information, but already include strong opinions and conclusions.  \n\nThat does not give Qwen a fair objective evaluation.",
          "score": 4,
          "created_utc": "2026-01-12 12:22:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz36cfw",
          "author": "onethousandmonkey",
          "text": "The normies will believe what these models feed them.\nItâ€™s not good.",
          "score": 2,
          "created_utc": "2026-01-12 02:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz48ibm",
          "author": "inotparanoid",
          "text": "Well, i guess that puts the current events in perspective",
          "score": 2,
          "created_utc": "2026-01-12 06:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2wc41",
          "author": "KS-Wolf-1978",
          "text": "re. #4 You are lying there. (And i am anti most of what Trump says and/or does).",
          "score": 5,
          "created_utc": "2026-01-12 01:15:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2xyaj",
              "author": "Swimming_Cover_9686",
              "text": "this is the source i used for that claim. [https://www.latimes.com/california/story/2026-01-07/minneapolis-ice-killing-california-lawmakers-decry-public-execution](https://www.latimes.com/california/story/2026-01-07/minneapolis-ice-killing-california-lawmakers-decry-public-execution)  I have not actually read it so dunno",
              "score": -4,
              "created_utc": "2026-01-12 01:24:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz304lr",
                  "author": "KS-Wolf-1978",
                  "text": "https://www.youtube.com/watch?v=Y04ndAPynMk",
                  "score": 0,
                  "created_utc": "2026-01-12 01:36:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4hum0",
          "author": "Lan_BobPage",
          "text": "Man Russia or China should really get Greenland before Trump does ngl. I want my RTX 6000 clone at 1\\\\3rd the price to run local models",
          "score": 2,
          "created_utc": "2026-01-12 07:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6e8sk",
          "author": "RobXSIQ",
          "text": "\"These are not exaggerated scenarios.\"\n\nThese are hyperbolic phrasing...and also exaggerated scenarios.\n\n  \n1) Elon raised his hand. haters decided it was a nazi salute. This is the OK sign all over again. The smoothest of brains are convinced its a nazi salute.   \n2) kidnapping is unlawful and is done to get ransom. this was extraction for trial.  \n3) This happened, but not done willy nilly...it was linked to sanctioned oil trade and it was a 2 week cat and mouse game  \n4) ICE shot someone trying to run them over  \n5) This is kinda true, but its been on Trumps favorite playlist since forever.\n\nSo the phrasing being poisoned here is one of the issues. Even if it was put gently however, AIs default to big things as typically unlikely anyhow as they don't want anyone just coming up and saying the moon is found to be made of cheese and it accept it as fact without the ability to research.  \n\nTell it Rob Reiner and his wife got murdered by their son and it'll disbelieve that also, because thats a big story. Tell it Netflix is buying Warner Bros...same. Tell it a saudi arabian group is going to buy EA...disbelief. Anything that moves the civilization needle it needs to remain suspect until it can confirm, because otherwise, what good is the AI if it'll take anyones thoughts as fact...even if all of it is true.",
          "score": 2,
          "created_utc": "2026-01-12 15:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3q1jh",
          "author": "AlgorithmicMuse",
          "text": "I found qwen3-30b to be the absolute best for being deterministic for my agent.  If I want to chat,  the agent uses tavily for web searching, not model training data.",
          "score": 1,
          "created_utc": "2026-01-12 03:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5ffv0",
          "author": "Heyla_Doria",
          "text": "Faites pas confiance aux llm pour comprendre le monde ðŸ¤·â€â™€ï¸",
          "score": 1,
          "created_utc": "2026-01-12 12:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8kq0a",
          "author": "IrisColt",
          "text": "Change the system prompt's cutoff date, heh",
          "score": 1,
          "created_utc": "2026-01-12 21:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcays",
          "author": "TokenRingAI",
          "text": "FWIW, Qwen 80B only has a vague understanding of the context you give it due to the model design, and hallucinates like crazy.\n\nThe actual output is very good, but the models understanding and recall of the context you gave it seems to be subpar, as a result of the hybrid attention mechanism.",
          "score": 1,
          "created_utc": "2026-01-14 22:01:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5czkd",
          "author": "Zestyclose-Shift710",
          "text": "not even it believes Elon did a nazi salute\n\nneither is ICE EXECUTING anyone wtf\n\nadd some more crazy left wing bias, see if it believes it more\n\nno shit it's too dystopian if you literally paint it that way",
          "score": 1,
          "created_utc": "2026-01-12 12:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4vpar",
          "author": "dsanft",
          "text": "Scenario 4 blatantly isn't what happened lol\n\nYes an execution would be implausible. \n\nShooting someone who's intentionally causing a disruption, ignoring instructions, and driving their car forward while you're still in front of it is only an \"execution\" on Reddit. In real life we call that FAFO.",
          "score": -1,
          "created_utc": "2026-01-12 09:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4ygu0",
              "author": "TechnoByte_",
              "text": "Hot take: causing disruption shouldn't result in being murdered\n\nI thought the US had a justice system for that",
              "score": 5,
              "created_utc": "2026-01-12 09:58:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz84jcr",
              "author": "Subject_Mix_8339",
              "text": "He was already on the side of the car when he fired the gun. There have been officers getting assaulted/beaten by people who have reacted calmer than pulling a weapon and killing someone.",
              "score": 2,
              "created_utc": "2026-01-12 20:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbholy",
                  "author": "dsanft",
                  "text": "The bullet hole is in the front windshield. \n\nhttps://share.google/gHGJUVBeiZVDZlGCu",
                  "score": 1,
                  "created_utc": "2026-01-13 08:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2q1zj",
          "author": "DinoAmino",
          "text": "If I made embarrassing posts like these I'd be hiding my account history too.",
          "score": -7,
          "created_utc": "2026-01-12 00:44:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2w487",
          "author": "Feztopia",
          "text": "They just lack expertise, none of these events were surprising for me except for Trump eyeing for Greenland.\nI mean yes I have seen so called \"experts\" who claimed Russia won't attack Ukraine and that these are lies 1-2 days before the attack but I was busy to insulting their mom for how stupid they were. Not everyone has an understanding of geopolitics, and texts written by \"experts\" often include propaganda instead of truth. Training a model on that is not the same like training a model on working code or correct math. Plus on top of that they get safety training to not make people sad",
          "score": 1,
          "created_utc": "2026-01-12 01:14:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2xjkz",
          "author": "neil_555",
          "text": "I solved that issue a few days ago ...\n\nFirstly make sure the current date is passed to the model on startup, I got ChatGPT to modify the Jinja template to pass in the current date.\n\nI tried to paste the template here but I couldn't post (maybe it was too big?)\n\nSecondly, add this to the system prompt ...\n\nThe current date has been passed to you on startup. Use this provided date to interpret relative terms like â€œtodayâ€, â€œyesterdayâ€, â€œthis yearâ€, etc.\n\nIf the user mentions a date which is later than your training data cutoff this is to be expected, do not challenge them or accuse them of invention or fantasy.\n\nThat solved the issue for me :)\n\nEdited as the system prompt instructions were pasted twice (dunno why that happened)",
          "score": 1,
          "created_utc": "2026-01-12 01:22:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2yp22",
              "author": "Swimming_Cover_9686",
              "text": "thanks I will try the code you sent me sometime (if I ever get off reddit and actually get my accounting backlog sorted)",
              "score": 1,
              "created_utc": "2026-01-12 01:28:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz2yqus",
              "author": "TomLucidor",
              "text": "Seconding this + getting proper web search scaffolds. Also might be a problem if the cutoff date is unknown for the model.",
              "score": 1,
              "created_utc": "2026-01-12 01:28:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3n7cy",
          "author": "alphatrad",
          "text": "Who asks AI about this stuff anyways?\n\nThe dumbest use of AI is always \"please prove my point of view is the correct one and I'm 100% right and morally superior\"",
          "score": 1,
          "created_utc": "2026-01-12 03:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz58yhi",
              "author": "Anonasty",
              "text": "Personally the context is irrelevant if you are testing how the model behaves when your data has cutoff and you need to use some examples.",
              "score": 2,
              "created_utc": "2026-01-12 11:32:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4d09e",
          "author": "jacek2023",
          "text": "I donâ€™t know whatâ€™s more pathetic on LocalLLaMA right now: the fact that every post about China is being upvoted (and everything not about China is downvoted), or that openly political statements are being praised here. I miss 2023, when LocalLLaMA was about local LLMs.",
          "score": 0,
          "created_utc": "2026-01-12 06:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz58stt",
              "author": "a_beautiful_rhind",
              "text": "Reddit isn't a real place. It's partially owned by tencent and a target of other foreign influence operations. They literally live in a bubble enforced by groupthink. The absolute wildest takes come out of here.",
              "score": 1,
              "created_utc": "2026-01-12 11:31:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3hz6n",
          "author": "Pretend-Paper4137",
          "text": "But, like- same, girl.",
          "score": 1,
          "created_utc": "2026-01-12 03:11:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz46uzo",
          "author": "ilabsentuser",
          "text": "Didnt knew about 3.\n\nNow, you know what is the problem here? That AI has too much common sense, certainly more that the people involved on this events!\n/j",
          "score": 1,
          "created_utc": "2026-01-12 05:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4n48l",
          "author": "asuka_rice",
          "text": "Itâ€™s all about the data.",
          "score": 1,
          "created_utc": "2026-01-12 08:09:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4w7la",
          "author": "toothpastespiders",
          "text": "You get something similar if you go through historical documents that are small scale enough to not be well represented in the training data. LLMs aligned around a general \"friendly assistant\" role are naturally pushed to be skeptical of narratives that don't match the alignment. Violent or controversial moments in history that aren't in the training data, whether that's something in the present or the past, typically won't.\n\nThough OkAssociation3083's point about how you're framing things is equally as important. You have to admit that you're being intentionally hyperbolic with what you're feeding a MoE that only has a minuscule amount of active parameters. Qwen's MoEs are impressive for what they're able to accomplish. But they're ultimately still severely hampered.",
          "score": 1,
          "created_utc": "2026-01-12 09:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza1b9n",
          "author": "ThisGonBHard",
          "text": "This reeks of political bias on your side, or you not know how dystopian our world already was. AI is huge on normalcy bias.\n\nQwen 235B is still in abject denial about the state of free speech in the UK for example, where you can be arrested the same way you are in China or Russia, for \"hate speech\", like silently praying in your head on the street, posting rap lyrics, criticizing immigration, the king or MPs, or the entire concept of non crime hate incidents. \n\nOr the entire Age Verification mess, that is taken straight from the CCP playbook.\n\nQwen will even outright lie about some of those being acquitted, like Count Dankula, or the kid who posted rap lyrics. \n\nUnless given the actual examples, it will deny it being a thing, and this are all events BEFORE it's cutoff date.\n\nUnderstand the limitations or biases of AI when using it.",
          "score": 1,
          "created_utc": "2026-01-13 02:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4hxpw",
          "author": "LinkSea8324",
          "text": ">execution\n\nlol",
          "score": 1,
          "created_utc": "2026-01-12 07:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz47pn8",
          "author": "ImportancePitiful795",
          "text": "Except it's argument on 1, on 2,3,4,5 is absolutely correct. I don't see where is the problem? ðŸ¤”",
          "score": 0,
          "created_utc": "2026-01-12 05:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz57tcn",
          "author": "Steuern_Runter",
          "text": "Just add to your prompt that the events are narrated with a strong left-wing framing.",
          "score": -1,
          "created_utc": "2026-01-12 11:23:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3rnb4",
          "author": "BrockosaurusJ",
          "text": "What were your prompts? Were you asking \"Tell me why this is impossible\" or similar? Because it's a next word predictor, so it will always try to complete the thought that you're led it down.\n\nDo another version where you ask it \"Tell me why this is possible\" and report back.",
          "score": 0,
          "created_utc": "2026-01-12 04:04:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qgjv",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -5,
          "created_utc": "2026-01-12 08:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4ysf7",
              "author": "TechnoByte_",
              "text": "ðŸ¥¾ðŸ‘…",
              "score": -1,
              "created_utc": "2026-01-12 10:01:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3llqt",
          "author": "XiRw",
          "text": "I made a post about this because I asked it a question about the 5090 (said it didnâ€™t exist) but I didnâ€™t  have web search on. I think Qwen needs to make a decision to do it automatically like OpenAI otherwise people are going to get the wrong idea about it",
          "score": 0,
          "created_utc": "2026-01-12 03:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5yq3v",
          "author": "Cherubin0",
          "text": "I agree. A local LLM on my hardware should believe all fake news I tell it to believe.",
          "score": 0,
          "created_utc": "2026-01-12 14:19:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4zfa5",
          "author": "nenulenu",
          "text": "You are always going to run the information espionage risk with Chinese developed models.",
          "score": -1,
          "created_utc": "2026-01-12 10:07:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q84u82",
      "title": "Big tech companies, now \"DRAM beggars,\" are staying in Pangyo and Pyeongtaek, demanding \"give us some supplies.\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.chosun.com/economy/tech_it/2026/01/09/MZNIFPCMTZGHHPV5757NJC5QW4/",
      "author": "FullstackSensei",
      "created_utc": "2026-01-09 10:28:56",
      "score": 295,
      "num_comments": 93,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q84u82/big_tech_companies_now_dram_beggars_are_staying/",
      "domain": "chosun.com",
      "is_self": false,
      "comments": [
        {
          "id": "nylcadu",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-09 13:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykrx7x",
          "author": "grumpy_autist",
          "text": "I will be auctioning my 4 GB DDR4 stick found in kitchen drawer at Sotheby's next month.",
          "score": 97,
          "created_utc": "2026-01-09 11:10:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykt2qu",
              "author": "FullstackSensei",
              "text": "I have a few 16GB and 32GB DDR4-2400 SO-DIMMs in four old HP Elite desk minis. Do you think I can retire if I sell them this summer?",
              "score": 27,
              "created_utc": "2026-01-09 11:19:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykt8ko",
                  "author": "grumpy_autist",
                  "text": "Sell CPU too and buy yourself some nice tropical island too.",
                  "score": 18,
                  "created_utc": "2026-01-09 11:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyl3t9p",
              "author": "metamec",
              "text": "lol.  I don't normally sell my old components (I usually use them for soldering projects or give them away), but the temptation to sell my old DDR4 sticks (2 x 16GB and 2 x 32GB) is certainly rising.",
              "score": 10,
              "created_utc": "2026-01-09 12:38:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyla6in",
                  "author": "isuckatpiano",
                  "text": "Theyâ€™d sell instantly. We Decommissioned some old UCS Servers and got 5x what we were expecting because of ram prices. Ram sold in one day on eBay at what I thought was a ridiculous price",
                  "score": 10,
                  "created_utc": "2026-01-09 13:18:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyle664",
                  "author": "muhmeinchut69",
                  "text": "Do it, easy money.",
                  "score": 4,
                  "created_utc": "2026-01-09 13:40:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykr4fe",
          "author": "insulaTropicalis",
          "text": "Ok. I am waiting six months and then trading my two sticks of DDR5 5600 for two Blackwell Pro 6000.",
          "score": 86,
          "created_utc": "2026-01-09 11:03:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykwima",
              "author": "ThatsALovelyShirt",
              "text": "I think I have 64 GB of CL30 6000 MT/s DDR5 still after upgrading to 96GB last year. I'm gonna be rich! I'll be able to build that addition to my house I've been thinking of.",
              "score": 26,
              "created_utc": "2026-01-09 11:47:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyl7adw",
                  "author": "Environmental-Metal9",
                  "text": "Ok, though your comment was extremely funny, my upvote goes to your username! (Man, does anyone remember when usernames used to be called nicknames? Pepperidge Farms remembersâ€¦)",
                  "score": 4,
                  "created_utc": "2026-01-09 13:00:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyks1ry",
              "author": "IngwiePhoenix",
              "text": "The fact that this might ACTUALLY work is so disgusting in a way lmfao xD",
              "score": 34,
              "created_utc": "2026-01-09 11:11:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nykyif6",
              "author": "mxforest",
              "text": "Blackwell Pro has 96GB GDDR7 each. It will also be much more costlier by then.",
              "score": 12,
              "created_utc": "2026-01-09 12:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynj96c",
                  "author": "teleprint-me",
                  "text": ">Â It will also be much more costlier by then.\n\n\nIn a blind crowd, the one-eyed man leads.",
                  "score": 2,
                  "created_utc": "2026-01-09 19:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylgpzk",
              "author": "Noiselexer",
              "text": "If only it worked like that in the current housing market. Ow wait it doesnt...",
              "score": 2,
              "created_utc": "2026-01-09 13:54:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyljtlf",
              "author": "howardhus",
              "text": "try buying an rtx card right now... \n\nsomehow reddit hasnt noticed but GPUs (specially nvidia) are not only doubling in price but being outright not available to buy",
              "score": 2,
              "created_utc": "2026-01-09 14:10:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nylnkg8",
              "author": "atmafatte",
              "text": "I have 94 gb of ddr4 on my old am4 system. I should probably sell my am5 and switch back to that and wait for the data centers to dump stuff in the secondary market",
              "score": 1,
              "created_utc": "2026-01-09 14:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nylpxum",
              "author": "SilentLennie",
              "text": "Sell it after 6 months and wait 3 years and then buy a GPU or 2",
              "score": 1,
              "created_utc": "2026-01-09 14:41:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyksdtu",
          "author": "a_beautiful_rhind",
          "text": "Jesus fucking christ. $9/gb? So a 32gb stick is gonna be almost $300?",
          "score": 28,
          "created_utc": "2026-01-09 11:14:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyktcsh",
              "author": "FullstackSensei",
              "text": "$300 now, $500 in three months if this is true.",
              "score": 37,
              "created_utc": "2026-01-09 11:22:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyktsfn",
                  "author": "a_beautiful_rhind",
                  "text": "Sure hope my ram doesn't break because that's more than some GPUs.",
                  "score": 11,
                  "created_utc": "2026-01-09 11:25:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nypzupn",
                  "author": "BusRevolutionary9893",
                  "text": "And $50 next year.Â ",
                  "score": 1,
                  "created_utc": "2026-01-10 03:06:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyku07j",
              "author": "miniocz",
              "text": "Gonna be?",
              "score": 3,
              "created_utc": "2026-01-09 11:27:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykujed",
                  "author": "FullstackSensei",
                  "text": "Remember, this is DDR4 we're talking about. DDR5 will buy you a mantion.",
                  "score": 11,
                  "created_utc": "2026-01-09 11:31:52",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nykv115",
                  "author": "a_beautiful_rhind",
                  "text": "Yea, I see 32g 2400 from 89-130 still. Far cry from $20 but also from $300.",
                  "score": 2,
                  "created_utc": "2026-01-09 11:35:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykp29i",
          "author": "dreamyrhodes",
          "text": "Can someone tell me, why posts like this get downvoted?\n\nRAM-prices are relevant for Local-LLMs.\n\nI mean honestly, I don't understand the voting habit on reddit.",
          "score": 92,
          "created_utc": "2026-01-09 10:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykpdq0",
              "author": "FullstackSensei",
              "text": "People are, understandably, upset about this. If downvoting helps someone vent a bit of their frustration at the situation, I'd say let them be.",
              "score": 53,
              "created_utc": "2026-01-09 10:48:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykpkhb",
                  "author": "dreamyrhodes",
                  "text": "Beat the messenger huh",
                  "score": 56,
                  "created_utc": "2026-01-09 10:50:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylat2p",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 9,
                  "created_utc": "2026-01-09 13:21:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nykr6p6",
              "author": "Healthy-Nebula-3603",
              "text": "Companies producing RAM are downvoting it !",
              "score": 5,
              "created_utc": "2026-01-09 11:04:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymwndm",
              "author": "Technical_Ad_440",
              "text": "maybe and maybe not. we are in a slump right now but tech is moving fast. we already have new cpus for instance designed specifically for ai. it is very possible we may get new ram specifically for AI. we may get new other stuff specifically for ai. the best thing would be new materials used and new methods to make stuff that are cheaper and that could allow general stuff to also make them rather than specialized labs.",
              "score": 1,
              "created_utc": "2026-01-09 17:55:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykwaia",
          "author": "Eisenstein",
          "text": "FYI: DDR4 contract prices are quoted as 8Gb modules. Note that is gigabits, so that would be a single GB, which means you assumed correctly. [Reference.](https://www.trendforce.com/price/dram/dram_contract)",
          "score": 11,
          "created_utc": "2026-01-09 11:45:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyl6tli",
          "author": "Bakoro",
          "text": "Party like it's 2007.",
          "score": 11,
          "created_utc": "2026-01-09 12:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl7q5t",
              "author": "FullstackSensei",
              "text": "I'm genuinely curious how long until the music stops? And how bad the recession will be?\nIt's not like the global economy is doing well right now....",
              "score": 3,
              "created_utc": "2026-01-09 13:03:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyldklj",
                  "author": "hugthemachines",
                  "text": "Global economy isn't great but it is not a disaster. I imagine the current economy for regular citizens in USA is a little bit worse than for the rest of the world due to very high tariffs, perhaps.\n\nThe economy for a regular citizen in Europe (not exactly the same in every country, of course) is a little bit worse than before because of the Russian invasion of Ukraine. It affected quite some prices. From the outside, it does sound like it is worse in USA now, though.",
                  "score": 4,
                  "created_utc": "2026-01-09 13:37:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nylo9sj",
                  "author": "psychicprogrammer",
                  "text": "From what I can tell, this economic implosion should be fairly isolated, think dot com bust, not 2008.",
                  "score": 1,
                  "created_utc": "2026-01-09 14:33:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nylqbki",
          "author": "Django_McFly",
          "text": "It feels like since about 2017, Earth has had a shortage of *something* that goes inside a PC.\n\nAnyways, like everything, we're getting close to the peak pricing when 19yo TikTokers are talking about *flip DRAM and get the bag! buy my course now*",
          "score": 7,
          "created_utc": "2026-01-09 14:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylvcw8",
              "author": "FullstackSensei",
              "text": "Oh, it's got to tik-tok and has become a quick buck thing?!! Welp....",
              "score": 3,
              "created_utc": "2026-01-09 15:07:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nylaism",
          "author": "abeecrombie",
          "text": "For dram it's always been cyclical deland and the fabs have always added capacity. This time it looks like no fab wants.to add capacity. And data center demand seems endless \n\nIf openai, Google and other hyperscalers want to keep buying like there's a shortage I think the games goes on \n\nBut once they stop prices will come crashing down. But that could take a while. But probably sooner than ppl expect.",
          "score": 4,
          "created_utc": "2026-01-09 13:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyld5xv",
              "author": "FullstackSensei",
              "text": "The always added capacity is why we went from something like 15 DRAM makers in the 90s to 3 now.",
              "score": 2,
              "created_utc": "2026-01-09 13:35:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nymqi98",
                  "author": "abeecrombie",
                  "text": "Yep. The whole semi space is fairly oligopoly like. \n\nI'm more of a spectator than an player but there is always a cycle.  Companies over order, growth peaks, demand slows,  etc. new tech comes in to leading nodes and capex has to be spent. \n\nMaybe it's different this time. Maybe we go on for a few more years and dram prices never fall back towards the cost of production+ some normal profit. \n\nWhat do you make of it.",
                  "score": 1,
                  "created_utc": "2026-01-09 17:27:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylbrpj",
              "author": "prusswan",
              "text": "Just hope the fight does not extend to GPUs (well at least not those below H100)",
              "score": 1,
              "created_utc": "2026-01-09 13:27:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylr331",
                  "author": "epyctime",
                  "text": "https://www.vice.com/en/article/new-report-suggests-phased-price-hikes-for-amd-and-nvidia-gpus-in-2026/",
                  "score": 1,
                  "created_utc": "2026-01-09 14:47:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyq4ns2",
              "author": "BusRevolutionary9893",
              "text": "If there's money to be made, supply will catch up to demand.Â ",
              "score": 1,
              "created_utc": "2026-01-10 03:35:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyksumq",
          "author": "Lan_BobPage",
          "text": "Fun times ahead that's for sure. So glad I prepared for this, for my use cases at least. Now it's a waiting game, for sharks and tuna alike.",
          "score": 2,
          "created_utc": "2026-01-09 11:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylrt4h",
          "author": "segmond",
          "text": "For the last 2 decades we have been very wasteful in code and data storage optimization.   Let memory prices go 1000x, perhaps it would force us to start writing well optimized code and data.  I mean the personal has 20x more compute than 20 years ago and yet performance wise feels the same.",
          "score": 7,
          "created_utc": "2026-01-09 14:50:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylw94r",
              "author": "MelodicRecognition7",
              "text": "performance wise it feels worse, run Windows XP in a virtual machine and compare its responsiveness to the host.",
              "score": 4,
              "created_utc": "2026-01-09 15:11:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nylz23j",
              "author": "FullstackSensei",
              "text": "As always, it's complicated.\n\nThe past 30 years have seen demand for software engineers far outstrip supply. Improving performance and easier to write in languages like js and python lead to the proliferation of \"boot camp developers\". CS programs have also mostly moved from teaching memory management and compiled languages to Java, python, and js, because that's what the industry wants and they're also much easier for students. It didn't matter that the languages they were writing in were slow or that their code was inefficient on top of that, because you could always throw more hardware at the problem.\n\nPersonally, I doubt there'll be much optimization happening unless this trend continues for at least another decade. The existing codebase is so large, and there's nowhere near enough proficient software engineers in the world to even begin to tackle this.",
              "score": 2,
              "created_utc": "2026-01-09 15:24:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nykpqxa",
          "author": "sunshinecheung",
          "text": "openai bad",
          "score": 16,
          "created_utc": "2026-01-09 10:52:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyks9xr",
              "author": "MelodicRecognition7",
              "text": "orange man badder",
              "score": 4,
              "created_utc": "2026-01-09 11:13:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykrwpy",
          "author": "IngwiePhoenix",
          "text": "Suck it, corps. :3 Destroy each other and then some.\n\nI hate the situation as a whole, but I am also kinda morbidly curious where this is gonna go - for both, consumers and companies. Supliers and customers.\n\nBut man do I get a kick out of the idea, that corpos are beggers now. XD Just the mental image man...",
          "score": 8,
          "created_utc": "2026-01-09 11:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyll3ks",
              "author": "zipzag",
              "text": "Yes! I only buy hand made organic artisanal DRAM. Damn the corporations.",
              "score": 4,
              "created_utc": "2026-01-09 14:16:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nynkij5",
              "author": "teleprint-me",
              "text": "Satya Nadella was complaining about having too many chips and not enough power and theyre still buying chips. ðŸ˜‘",
              "score": 2,
              "created_utc": "2026-01-09 19:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyoinsp",
                  "author": "IngwiePhoenix",
                  "text": "He's gonna nutella Data Centers all over the world.\n\nAt least, he'd love that. A lot.\n\nBitch ain't sharing his stash of GPUs. :|",
                  "score": 0,
                  "created_utc": "2026-01-09 22:20:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykxqin",
          "author": "prusswan",
          "text": "3k for 128GB ddr5 makes the 1k for 64GB look \"cheap\"",
          "score": 2,
          "created_utc": "2026-01-09 11:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyl3ui6",
          "author": "noctrex",
          "text": "welp, I'm guessing I will get that dream house I wanted with my 128GB DDR4 in a little while",
          "score": 2,
          "created_utc": "2026-01-09 12:39:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylfsi3",
          "author": "taoyx",
          "text": "All of this because Samsung and the others have decided not to build more plants, so it's a risky move for a new competitor to enter the market since these 3 can reverse their decision and make it less profitable. \n\nHowever at some point this strategic move will become an error and competitors will have a way in, specially if they can secure orders. I mean Apple and NVidia have enough cash to build plants, they can just let the prices go up then build some and make extra profit, even if they say today that they are not going to do it...",
          "score": 2,
          "created_utc": "2026-01-09 13:49:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyli2dp",
              "author": "FullstackSensei",
              "text": "It takes a lot of capital and several years to build a new fab and commission it. Even if the three memory makers could have foreseen this and started building new fabs in 2024, those fabs won't come come online until sometime in 2027, and the first wafers would ship late 2027 or early 2028. By then, the AI bubble might have popped and they'd have billions worth of capacity with nobody to buy.\n\nThis very strategy is why there were something like 15 memory makers, back when you could build a fab and start shipping finished chips in under 18 months. The cyclical and commoditized nature of memory killed all the others players for doing this. As bad as it is, can't really blame them for choosing not to add capacity now, when the time to build and commission a fab has more than tripled, and cost is like 5-10x vs the 90s.",
              "score": 5,
              "created_utc": "2026-01-09 14:01:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nymemjz",
                  "author": "taoyx",
                  "text": "Yeah but if Apple or NVidia make them they will still need those RAMs for their own products so it's Samsung and the others who will lose when the prices go down. At worst they will make them at a higher price than Samsung but if they wait a bit right now they will raise their products price then make new RAMs = profit, if the difference covers what they would have paid Samsung then they wont even lose money.\n\nAlso yeah a new plant costs like 10 billion, however it's pocket change for Apple and NVidia.",
                  "score": 1,
                  "created_utc": "2026-01-09 16:34:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylql2d",
              "author": "SilentLennie",
              "text": "If they invest billions in a new fab and then the AI-bubble pops they will be stuck with over capacity.",
              "score": 1,
              "created_utc": "2026-01-09 14:44:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymf0tl",
                  "author": "taoyx",
                  "text": "Yeah but if it's Apple and NVidia they will use those RAMs in their phones and Gfx cards and then this will be Samsung's problem not theirs.",
                  "score": 1,
                  "created_utc": "2026-01-09 16:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyln2qj",
          "author": "ab2377",
          "text": "this is just so sad.",
          "score": 2,
          "created_utc": "2026-01-09 14:27:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymfd23",
          "author": "suicidaleggroll",
          "text": "> Per the article, DDR4 prices went up from $1.40 last January to $9.30 in December (my interpretation is $/GB). If they're increasing by another 50%, that's almost $14/GB!!! So, 1TB of DDR4-3200 will cost north of $14k by Q2 if this is true ðŸ¤¯\n\nI believe it.  This time last year, 1 TB of DDR5 was around $2.5k.  2 months ago it was $8.5k.  Today it's $26k.  I wouldn't be surprised if it's north of $40k 3 months from now, possibly even closer to $60k.",
          "score": 2,
          "created_utc": "2026-01-09 16:38:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymro6r",
          "author": "RottenPingu1",
          "text": "I have 96GB two sticks of 48GB DDR5 CL40 I'm going to sell but I have no idea what to even price them at.",
          "score": 1,
          "created_utc": "2026-01-09 17:33:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nynlz0g",
              "author": "BullableGull",
              "text": "Price them at whatever will make you not miss them if they sell lol",
              "score": 2,
              "created_utc": "2026-01-09 19:49:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nynm605",
                  "author": "RottenPingu1",
                  "text": "Lol...good advice.",
                  "score": 1,
                  "created_utc": "2026-01-09 19:50:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyn165s",
          "author": "Exotic_Jellyfish_548",
          "text": "[https://www.instagram.com/p/DTTKKWoEaRk/](https://www.instagram.com/p/DTTKKWoEaRk/)",
          "score": 1,
          "created_utc": "2026-01-09 18:15:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr4grm",
          "author": "blackcain",
          "text": "Sounds like a great way for public schools to make money.",
          "score": 1,
          "created_utc": "2026-01-10 08:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrkzow",
          "author": "Any_Pressure4251",
          "text": "How can the operating profit be only 70% when the price has more than quadrupled. \n\nThis would mean they were selling at a huge loss.",
          "score": 1,
          "created_utc": "2026-01-10 10:35:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrtof5",
              "author": "FullstackSensei",
              "text": "Could very well be a translation error. I think they mean margins have increased 70% vs the previous quarter. Could have been that margins went up 300% the previous quarter, and it's a 70% increase over that. Also keep in mind retail prices lag vs contract.",
              "score": 1,
              "created_utc": "2026-01-10 11:52:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nywxyee",
          "author": "LatentSpacer",
          "text": "This makes me anxious about my BIOS settings.",
          "score": 1,
          "created_utc": "2026-01-11 04:09:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyl8tvm",
          "author": "GreatBigJerk",
          "text": "I wonder if there's a niche for motherboard manufacturers to add a ton of DDR4 slots to boards to compensate. That way people could buy or scavenge a bunch of low capacity sticks to get up to a decent level. \n\nProbably not viable, but it feels like we're going to have to get weird solutions to this problem.",
          "score": 0,
          "created_utc": "2026-01-09 13:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl9lpi",
              "author": "FullstackSensei",
              "text": "The problem doesn't lie with motherboard makers, but with the CPUs themselves. AFAIK, consumer CPUs are limited to 2 DIMMs per channel. No amount of motherboard space can fix that.",
              "score": 3,
              "created_utc": "2026-01-09 13:14:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nylqyv8",
                  "author": "SilentLennie",
                  "text": "If this is going to last until Q4 2027 as some predict, might be interesting to product CPUs which can be useful.\n\nChina has a company or 2 that can produce DDR4 right now, maybe they can scale up production.",
                  "score": 1,
                  "created_utc": "2026-01-09 14:46:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0x8ci",
      "title": "Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "author": "Venom1806",
      "created_utc": "2026-01-01 06:03:27",
      "score": 286,
      "num_comments": 57,
      "upvote_ratio": 0.98,
      "text": "Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.\n\n**Results**: 3x faster on memory-bound operations (GEMV, FlashAttention)\n\nWorks on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.\n\n[Article Link](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/) |  [Github Link](https://github.com/SuriyaaMM/feather)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1qqyn",
          "author": "lolxdmainkaisemaanlu",
          "text": "Damn I didn't know RTX 3xxx series didn't support FP8? I'm a noob and thought it was supported - coz I've been using fp8 / fp8 scaled models on my RTX 3060 and they do work..?\n\nAmazing work bro, Can I use it rn to accelerate comfyui workloads?",
          "score": 33,
          "created_utc": "2026-01-01 07:22:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1uvtg",
              "author": "john0201",
              "text": "It saves memory but youâ€™re still using 16 bit cores",
              "score": 25,
              "created_utc": "2026-01-01 08:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1x3qd",
                  "author": "spaceman_",
                  "text": "16 bit ALUs. You can run 8bit, 16bit, 32bit etc on the same core.\n\n\nThere's no such thing as an 8bit core, but there are dedicated hardware components called ALUs that actually do the math bits and they are operation and operand size specific. In some cases these ALUs are actually shared between cores.\n\n\nThis leads to unintuitive situations on some hardware - for example, on older hardware that was mostly running 32bit float graphics work 16bit workloads sometimes at half speed compared 32bit, despite requiring half the memory bandwidth, because each core had its own 32bit ALUs but 16bit units were shared per pair.\n\nSame thing existed on the CPU side - AMD Bulldozer cores had their own integer ALUs but shared floating point and SIMD hardware between two cores.",
                  "score": 23,
                  "created_utc": "2026-01-01 08:28:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx5ao8h",
                  "author": "phazei",
                  "text": "I'm not sure where the memory saving comes in for existing 3090 fp8 pipelines. In comfy it loads the fp8 model into system memory, and then moves that to vram as fp8 afaik and then upscales to fp16 when it does the calculation. So if I'm running a model such as Zimage which only takes 8 gigs of space, where does this feather come in and help?",
                  "score": 1,
                  "created_utc": "2026-01-01 21:45:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2okno",
              "author": "az226",
              "text": "Basically Volta added FP16, Ampere added BF16, Hopper did FP8, and Blackwell FP4.",
              "score": 13,
              "created_utc": "2026-01-01 13:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zwnf",
              "author": "CheatCodesOfLife",
              "text": "Yeah, that through me off like a year ago when I was trying to FP8 quants. I think vllm prints a warning about it and it works, but kind of annoying since the 4xxx series got it.",
              "score": 7,
              "created_utc": "2026-01-01 08:58:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxc4nd6",
              "author": "phazei",
              "text": "hijacking top comment to clarify:\n\nFor anyone confused by \"memory-bound\" here, it's not about VRAM capacity. It means the GPU cores are waiting on data to arrive from memory. The bottleneck isn't the math, it's feeding the cores fast enough.\nFP8 is half the bytes of FP16, so it transfers twice as fast from VRAM to the registers where compute actually happens. The clever bit is that Feather does the upcast inside the kernel (in registers, basically free) rather than before it (which would mean a separate VRAM round-trip). That's where the 3x speedup comes from.\n\nI was confused at first since the README made no specific clarification and when I think of a GPU, I basically just think of the VRAM.\n\nEdit:  So, SageAttention I believe takes the fp8 to the register, then quantizes it to int8, does the math, then converts it back.  So it's not doing fp8 math at all, so Feather and SageAttention are incompatible, and the speed of SageAttention is going to be faster since int8 is like 2x fp16 math speeds.  So this can give benefit to stuff that doesn't use SA, but if you already use SA, this provides no benefit.",
              "score": 2,
              "created_utc": "2026-01-02 22:42:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1lmri",
          "author": "Routine_Day8121",
          "text": "This is exactly the kind of lifehack the community needs. FP8 is getting hype everywhere, but hardware adoption is slow. If software workarounds like this are stable, it could extend the life of mid tier GPUs for serious training experiments. Curious to see benchmarks on larger models and mixed workloads though, sometimes GEMV gains do not fully translate.",
          "score": 75,
          "created_utc": "2026-01-01 06:33:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3vbdd",
              "author": "TheThoccnessMonster",
              "text": "Yup - and thereâ€™s plenty of model layers that are heavily convolutional that, even when offloaded to DLA/FP8 they just upcast to FP16 anyway. QAT and dedicated hardware for convolutions and unsupported activation functions stand to get us a lot more bang for our bucks.",
              "score": 8,
              "created_utc": "2026-01-01 17:25:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1zmpp",
              "author": "CheatCodesOfLife",
              "text": "Lol, what model wrote this, Sonnet?",
              "score": 18,
              "created_utc": "2026-01-01 08:55:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3chhx",
                  "author": "colin_colout",
                  "text": "You're absolutely right to question my identity!",
                  "score": 20,
                  "created_utc": "2026-01-01 15:44:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3xx5s",
                  "author": "bigfatstinkypoo",
                  "text": "this writing does not stink that bad, it's just corpo positivity speak",
                  "score": 8,
                  "created_utc": "2026-01-01 17:38:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx443q1",
                  "author": "Due-Function-4877",
                  "text": "Us \"boomers\" with a degree write like that. The models are trained on real writing. Next time, I'll make sure to use all lower case and say \"bruh\" a few times for you.",
                  "score": 18,
                  "created_utc": "2026-01-01 18:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx211pc",
              "author": "Karyo_Ten",
              "text": ">but hardware adoption is slow.\n\nThat has been supported on 4000 series since a couple of years ago, and it's supported on latest AMD and Intel GPUs AFAIK",
              "score": 7,
              "created_utc": "2026-01-01 09:10:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3uxh2",
                  "author": "Inevitable_Host_1446",
                  "text": "I guess you could see that two ways - hardware adoption as in the hardware is slow to come out, or as in people are slow to get the latest. The latter has certainly been true with what a shitshow GPU prices have remained since the days of crypto boom at least. And now RAM is ridiculous as well and Nvidia are talking about cloud gaming...",
                  "score": 3,
                  "created_utc": "2026-01-01 17:23:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx23beg",
          "author": "gittubaba",
          "text": "Wow, just a few days ago I was arguing about this with chatgpt, it said this isn't possible :P. Can this be plugged into comfyui? \n\nIn my rtx 2060 super, fp8 gets cast to fp16 and bf16 get cast to fp32 when running inference.",
          "score": 11,
          "created_utc": "2026-01-01 09:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2uv6j",
              "author": "a_beautiful_rhind",
              "text": "I think it's better to use the triton patch in comfy. https://github.com/woct0rdho/triton-windows/commit/440e3c42a640a4188dd356225e1b13a56b45a377\n\nAlso found it's possible to load BF16/FP16 as E4M3 and then save the vram without an extra file. Somehow my quality went up.\n\nUnfortunately there is some bug in pytorch 2.9 where FP8_scaled gets passed directly into the triton compiler as FP8 and then cast to i8 by llvm. Torch 2.7 works flawless or you can just de-scale the weights.\n\nYou sorta want the calcs in FP16 and you wanna avoid BF16->FP32 conversion if speed is the goal. Int8 calcs can be tried by using sage attention. Not always better.",
              "score": 12,
              "created_utc": "2026-01-01 13:53:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8bsnz",
                  "author": "woct0rdho",
                  "text": "My patch only enables fp8 to fp16 cast in Triton, but it does not replace fp8 matmul in Triton or PyTorch. OP's kernels can directly replace fp8 matmul and that's what we need for the next step.\n\nPyTorch devs seem interested in implementing this, see https://github.com/pytorch/pytorch/issues/167082",
                  "score": 2,
                  "created_utc": "2026-01-02 10:09:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2bsbe",
              "author": "Venom1806",
              "text": "Not sure about comfy UI, but I'm working on implementing functional api for torch.",
              "score": 9,
              "created_utc": "2026-01-01 11:04:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2vuql",
                  "author": "a_beautiful_rhind",
                  "text": "Comfy does torch and FP8/Fp8_scaled is used there much more than for LLMs. IME, on turning FP32 is going to be a slow ride vs FP16.\n\nFor my uses, compiling FP8 image gen weights was a huge speedup. I wonder if somehow your library can hijack FP8 ops to work seamlessly. Right now i'm having to compile triton from source and I doubt quantization/dequantization is accelerated.",
                  "score": 12,
                  "created_utc": "2026-01-01 14:00:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28jis",
              "author": "getmevodka",
              "text": "LLMs always something isnt real/possible or doable, if it is not part of their training data. Especially the newer LLMs are trained to only do things as efficient and complete as possible, which makes them severly dumber in hypothetical cases than the older LLMs, because they always do only the least amount of work necessary to keep things simple enough and noz make mistakes, as that is a heavy negative reward in their system. Imho its too agressive and the older LLMs like deepseek3.1 or qwen2.5 72b are better suited for hypothetical expectational work or fantasizing about potential ideas, while the newest generation of LLMs will do exceptional work within the scope of their trained abilities.",
              "score": 7,
              "created_utc": "2026-01-01 10:30:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2bxqt",
                  "author": "gittubaba",
                  "text": "What are even saying bro?",
                  "score": 0,
                  "created_utc": "2026-01-01 11:06:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2hy2w",
          "author": "bbjurn",
          "text": "What'd it take to get this to work with vLLM or other inference software?",
          "score": 12,
          "created_utc": "2026-01-01 12:07:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx33f1k",
              "author": "Venom1806",
              "text": "Idk, anything that uses torch.Tensor or is convertible to this format should work. Probably huggingface will work ig.",
              "score": 7,
              "created_utc": "2026-01-01 14:51:13",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx3dm9j",
              "author": "elsung",
              "text": "Yeaaaa! I was just trying to get vLLM to load nemotron3-nano on my 2x 3090s but couldnâ€™t get it working because FP8 isnâ€™t supported (and theres no AWQ quant). Gotta be honest tho not sure how i would implement this in vLLM to get things working. Might need to vibe code this to see about implementing the solution lol",
              "score": 6,
              "created_utc": "2026-01-01 15:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx5uuyw",
                  "author": "rainbyte",
                  "text": "There is GPTQ quant, do you know if is it good?",
                  "score": 1,
                  "created_utc": "2026-01-01 23:33:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx223fk",
          "author": "ab2377",
          "text": "wow ðŸ˜³ ðŸ‘",
          "score": 4,
          "created_utc": "2026-01-01 09:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3sn2i",
          "author": "KingKoro",
          "text": "Would this also benefit RDNA3 ?",
          "score": 3,
          "created_utc": "2026-01-01 17:11:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4sqlo",
              "author": "tw_numba_one",
              "text": "I believe so. If your environment has PyTorch support, it should work.",
              "score": 2,
              "created_utc": "2026-01-01 20:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4crri",
          "author": "ethertype",
          "text": "Is this conceptually the same trick pytorch uses to handle MXFP4 on Ampere-class hardware? Which does not support MXFP4 natively.\n\n[heretic](https://github.com/p-e-w/heretic) will do its magic on the original gpt-oss-20b safetensor in MXFP4 format. (The end result is 3x the original size, though.) I have been told heretic doesn't do anything in the code for this to occur, so I assume pytorch owns all the glory.\n\nI also can perfectly fine load the native MXFP4 ggufs of gpt-oss-120b (converted by GG) on my 3090s, with llama.cpp. 120 t/s on empty context. Can't say if this is due to pytorch or if llama.cpp special-cases this on its own.",
          "score": 3,
          "created_utc": "2026-01-01 18:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3m8e6",
          "author": "tynej",
          "text": "Very nice work. Could we use similiar trick for   hopper architecture to support speed of fp4?",
          "score": 2,
          "created_utc": "2026-01-01 16:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3p1zr",
              "author": "Venom1806",
              "text": "We could just use 8 fp4 instead of 4 fp8, we dont need an hopper.",
              "score": 3,
              "created_utc": "2026-01-01 16:52:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2a9cx",
          "author": "FastDecode1",
          "text": ">Works on any GPU\n\n>Runs E5M2 and E4M3 on any CUDA GPU (RTX 20/30 series supported).\n\nPick one.",
          "score": 7,
          "created_utc": "2026-01-01 10:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2bvnq",
              "author": "Venom1806",
              "text": "Sorry. Should work on RTX 20/30, there's no advantage in using with 40.",
              "score": 19,
              "created_utc": "2026-01-01 11:05:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2pezw",
                  "author": "az226",
                  "text": "Does it work for V100? Training too or just inference?",
                  "score": 2,
                  "created_utc": "2026-01-01 13:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6y7iv",
          "author": "batonac",
          "text": "Could this be useful for increasing LLM performance on the Tesla P40?",
          "score": 1,
          "created_utc": "2026-01-02 03:27:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7w5oy",
              "author": "johndeuff",
              "text": "Interested. Got p40 too.",
              "score": 1,
              "created_utc": "2026-01-02 07:41:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q0uuqt",
      "title": "Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "author": "Dangerous_Fix_5526",
      "created_utc": "2026-01-01 03:41:30",
      "score": 284,
      "num_comments": 80,
      "upvote_ratio": 0.94,
      "text": "(link to Heretic/Uncensored version just added)\n\n**Special thanks to :**\n\n[jacek2023](https://www.reddit.com/user/jacek2023/) \\[posting about this model\\]\n\nand extra special thanks for \"**allura-forge** \" for finding this model:\n\n[https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n( For an incredible find of Llama 3.3 8B \"in the wild\" !!)\n\nI fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nThis has created a reasoning/instruct hybrid.  \nDetails at the repo, along with credits and links.\n\n**ADDED:**  \n\\- 1 example generation at repo  \n\\- special instructions on how to control \"instruct\" or \"thinking\" modes.\n\nGGUF quants are now available.\n\n**ADDED 2:**\n\nClarification:\n\nThis training/fine tune was to assess/test if this dataset would work on this model, and also work on a non-reasoning model and induce reasoning (specifically Claude type - which has a specific fingerprint) WITHOUT \"system prompt help\".\n\nIn other-words, the reasoning works with the model's root training/domain/information/knowledge.\n\nThis model requires more extensive updates / training to bring it up to date and up to \"spec\" with current gen models.\n\n**PS:**  \nWorking on a Heretic (\"uncensored\") tune of this next.\n\nHeretic / Uncensored version is here:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Heretic-Uncensored-Claude-4.5-Opus-High-Reasoning)\n\n(basic benchmarks posted for Heretic Version)\n\nDavidAU",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nx1aef2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 04:55:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1bnaj",
          "author": "30299578815310",
          "text": "Thanks for sharing this! Am I reading is correctly that you had 250 rows in the fine-tuning data set? Is that enough to get good results?",
          "score": 44,
          "created_utc": "2026-01-01 05:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1qtjk",
              "author": "Dangerous_Fix_5526",
              "text": "Correct. A quality, compact dataset can make all the difference. Special thanks to TeichAI for their hard work in putting together this top notch dataset.\n\n[https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x](https://huggingface.co/datasets/TeichAI/claude-4.5-opus-high-reasoning-250x)\n\nPS: They have done a lot of these kinds of datasets, so show them some love.\"  \n  \nI used 10 of these (models/datasets by TeichAI) to build a 12X programmable MOE (all top closed and open distills) here:\n\nHeretic version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-Distill-12X-Closed-Open-Heretic-Uncensored-GGUF)\n\n\"Reg\" Version:  \n[https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF](https://huggingface.co/DavidAU/Qwen3-48B-A4B-Savant-Commander-GATED-12x-Closed-Open-Source-Distill-GGUF)",
              "score": 33,
              "created_utc": "2026-01-01 07:22:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2c8r8",
                  "author": "-p-e-w-",
                  "text": "Note that when combining Heretic with fine-tuning, you should always run Heretic first, and *then* do training, not the other way round. That way, the training run might heal some of the damage from ablation (though to be fair, for the Llama 3 series that damage tends to be very minor).",
                  "score": 13,
                  "created_utc": "2026-01-01 11:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx3cwu9",
                  "author": "IrisColt",
                  "text": "Thanks!, the Heretic version is like day and night.",
                  "score": 2,
                  "created_utc": "2026-01-01 15:47:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx28hez",
          "author": "DecodeBytes",
          "text": "I might be missing something, but 200 samples won't be enough to teach an 8B instruct model to reason - though it can work for very specific, constrained tasks, less likely to be widely populated in the original pretraining.\n\nReasoning ability is largely baked into the base model during pretraining. I'm assuming you used LoRA, which is great for steering how that existing ability gets applied, but it won't teach new reasoning capabilities from scratch. Even with 50k+ samples, LoRA mostly reshapes how the model uses reasoning it already has rather than building new circuits - must successful efforts use 100k-500k+ high-quality samples. Either way, you're working within the constraints of what the base model learned during pretraining unfortunately.\n\nKeep going though, its all a learning experience and the more folks there are making tunes the better!",
          "score": 11,
          "created_utc": "2026-01-01 10:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2d39j",
              "author": "Dangerous_Fix_5526",
              "text": "These are high quality reasoning traces.\n\nNormally I would agree with you - but it works.  \nAlso works very well with Qwens3 - 4B, 8B and 14B.\n\nFrankly that it works speaks volumes for the high quality dataset from TeichAI.  \nThere is a reason this dataset has 112 likes.\n\nLikewise the reasoning traces/formatting appears the same way as in the Qwen3 tunes using the same dataset.\n\nADDED:  \nWith this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)  \n  \nIt is not \"always on\" like a \"locked\" thinking model so to speak.",
              "score": 2,
              "created_utc": "2026-01-01 11:18:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx2n948",
                  "author": "DecodeBytes",
                  "text": "\n\n\\> With this model, reasoning activates based on keywords/phrases in the prompt.  \n(see repo)\n\nRight, its likely the model is just doing as \\*\\*instruct\\*\\*ed in the prompt and its not activated learned reasoning, but its really hard to tell as I can't find where anything is in this tread, help me out please? link the model, notebook and anything else?",
                  "score": 8,
                  "created_utc": "2026-01-01 12:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2m5z9",
                  "author": "DecodeBytes",
                  "text": "Do you have any benchmarks I could look at and can you share your training notebook, I would love to take a look?\n\nIs this the tuned model? [https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)",
                  "score": 3,
                  "created_utc": "2026-01-01 12:46:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4p57r",
                  "author": "Far-Low-4705",
                  "text": "just because it's \"high quality data\" doesn't mean for a second that you can get away with any less.\n\nits a core theory in ML, not just LLMs specifically, you need a large enough sample size to represent the broader population, ie, all cases of reasoning. you'd need 500k+ examples for anything remotely accurate. and even then, as the user above said, a lora adaptor is not really ideal here.   \n  \nyou need your data set to cover a few examples from every possible scenario. 200 is no where near enough, even if they were \"perfect\" traces.\n\nThat being said, you *might* still see **marginal** performance gains, but you'd still be leaving **a lot** on the table, and you haven't verified any gains at all because you didnt benchmark its performance. I would like to see performance benchmarks in order to believe you, and even then, you'd be leaving A LOT of performance on the table.",
                  "score": 6,
                  "created_utc": "2026-01-01 19:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1luhd",
          "author": "dash_bro",
          "text": "Brilliant. Thank you! \n\nIs there a community fine-tune with the same dataset for qwen3-14B? I think that would help with the wild reasoning goose-chases it sometimes goes down under",
          "score": 7,
          "created_utc": "2026-01-01 06:35:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1r0dc",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; see this repo:\n\n[https://huggingface.co/TeichAI](https://huggingface.co/TeichAI)\n\n(they have 4B,8B and 14B ; I have used some of their 4Bs in MOES)",
              "score": 5,
              "created_utc": "2026-01-01 07:24:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2snoz",
          "author": "30299578815310",
          "text": "Ate there any benchmarks for this?",
          "score": 8,
          "created_utc": "2026-01-01 13:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx616uu",
              "author": "Dangerous_Fix_5526",
              "text": "There are benches for the root / base version as found by allure.",
              "score": 1,
              "created_utc": "2026-01-02 00:09:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6b07r",
                  "author": "30299578815310",
                  "text": "In that case are bubbles not very effective against Legion since it seems like they use less plasma than the other factions?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:04:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1axnf",
          "author": "sunshinecheung",
          "text": "wow, i hope there is a GGUF version",
          "score": 10,
          "created_utc": "2026-01-01 04:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx23iku",
              "author": "Dangerous_Fix_5526",
              "text": "A few ggufs are up ; team Mradermacher is doing some right now too.\n\nUPDATE:  \nQuants are up - all , including Imatrix.",
              "score": 10,
              "created_utc": "2026-01-01 09:37:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1pplo",
          "author": "Own-Potential-2308",
          "text": "I never tried any Claude reasoning models lol",
          "score": 5,
          "created_utc": "2026-01-01 07:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ny4",
          "author": "txgsync",
          "text": "That's pretty cool. Getting easier to train models every day! Interested in trying your fine tune.",
          "score": 14,
          "created_utc": "2026-01-01 03:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2nws1",
          "author": "Single_Ring4886",
          "text": "It is very nice but some \"tests\" are really needed...",
          "score": 4,
          "created_utc": "2026-01-01 13:00:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2lunp",
          "author": "And-Bee",
          "text": "Tried to use this with Roo code and it produced garbage",
          "score": 4,
          "created_utc": "2026-01-01 12:43:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4nn2c",
              "author": "Cool-Chemical-5629",
              "text": "Using old Llama 8B model which was never meant to be good at coding, finetuned with 250 rows of \"high quality\" thinking traces from Claude model of... who knows what categories... What could go wrong? ðŸ˜‚",
              "score": 6,
              "created_utc": "2026-01-01 19:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx23ypd",
          "author": "Professional-Coat968",
          "text": "Sound interesting to try. Do you think we can finetune a good enough for only a specific code base like this ? ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-01 09:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx253ut",
              "author": "Dangerous_Fix_5526",
              "text": "Yes ; Llamas are very easy to tune. That being said, I was surprised how well this tune using a distill dataset came out. \n\nFrankly, this could have used a bit more training - but I did not want to overcook it.",
              "score": 2,
              "created_utc": "2026-01-01 09:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2u00p",
          "author": "rekriux",
          "text": "Hi u/Dangerous_Fix_5526,  \nshamelessly asking if it where possible to make your 20X-40X models (or similar) as recurrent loop models (with or without lora) ?  \nYour models are hidden gems, but the additional NVRAM/RAM is hard on HW limits for larger models (btw I run vllm).\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1q0vom4/comment/nx2q3ca/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nAlso, will you start working with linear models ? Kimi Linear REAP, Falcon H, Nemotron 3 ?   \nP.S. Nemotron license is restrictive, and the model has ingrained censoring/alignment (made a post that was removed on it)  \n  \n\\+1 for this one, will definitively try it !",
          "score": 2,
          "created_utc": "2026-01-01 13:47:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62b8j",
              "author": "Dangerous_Fix_5526",
              "text": "Nemotron is in the \"works\" ; as well as Kimi V2 ; using distill dataset(s).\n\nRE: 20/40x ;   \nThe brainstorm adapter works on almost all model types, archs and sizes ; with 20x the most stable.  \n40x is used for creative purposes and/or people that want models a ... little more out there.",
              "score": 3,
              "created_utc": "2026-01-02 00:15:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4v8a8",
          "author": "Standard-Savings-224",
          "text": "Nice work on the fine tune! That Claude reasoning dataset combo sounds promising - curious how the thinking mode performs compared to base 3.3. The uncensored version is gonna be interesting too",
          "score": 2,
          "created_utc": "2026-01-01 20:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx11ren",
          "author": "LoveMind_AI",
          "text": "Fantastic work.",
          "score": 6,
          "created_utc": "2026-01-01 03:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1sc8l",
          "author": "jacek2023",
          "text": "Hello, it wasn't me, I only posted the news here :)\n\nPlease credit allura",
          "score": 5,
          "created_utc": "2026-01-01 07:38:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1ssr9",
              "author": "Dangerous_Fix_5526",
              "text": "Done ; thanks for heads up.  \nallura was credited at repo W links to reddit posts too.  \nThank you for posting about this model!",
              "score": 2,
              "created_utc": "2026-01-01 07:42:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx12zmh",
          "author": "Borkato",
          "text": "How good is it? ðŸ‘€",
          "score": 2,
          "created_utc": "2026-01-01 03:59:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx13ekl",
              "author": "Dangerous_Fix_5526",
              "text": "I used this test prompt, with Q4KS:\n\nExplain orbital mechanics including detailed math and examples.\n\nModel produced excellent thinking block ( very detailed, but on point) , then examples / \"math\" and without be prompted - multiple python scripts to visually illustrate all concepts.",
              "score": 10,
              "created_utc": "2026-01-01 04:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx3o2oz",
                  "author": "noneabove1182",
                  "text": "But the answer it gave is quite terrible, it just hallucinated a bunch of nice looking stuff",
                  "score": 6,
                  "created_utc": "2026-01-01 16:46:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx43t93",
                  "author": "LetterRip",
                  "text": "I had copilot evaluate the answer,\n\n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
                  "score": 4,
                  "created_utc": "2026-01-01 18:07:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx14z8l",
                  "author": "Borkato",
                  "text": "Thatâ€™s quite interesting!",
                  "score": 3,
                  "created_utc": "2026-01-01 04:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2u6on",
          "author": "tmvr",
          "text": "I've asked it for a simple Ansible fleet management setup with a few tasks on the client which it did fine. Then I've I've told it to add disabling reboot for non-privileged users and instead of adding a task it went bonkers. Added some Project Timeline, Implementation Roadmap, Risk Assessment, RIsk Mitigation sections etc. added long Python scripts for some Audit Framework and also for Compliance Checks Validation and a bunch or other stuff and ended stuck at this which was obviously never going to work:\n\nhttps://preview.redd.it/555354kusqag1.png?width=296&format=png&auto=webp&s=2fda1b00329e983e8abfa8e94ca1652588fa8308",
          "score": 1,
          "created_utc": "2026-01-01 13:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62fo7",
              "author": "Dangerous_Fix_5526",
              "text": "Censorship in the root model is STRONG. (same for all Llamas).  \nHeretic version should change that.",
              "score": 1,
              "created_utc": "2026-01-02 00:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx43nc9",
          "author": "LetterRip",
          "text": "Note that I had Copilot evaluate the answer to the prompt, here is a critical evaluation sum-up.  \n  \n\"The explanation tries to sound comprehensive, but itâ€™s riddled with problems: several equations are outright incorrect or dimensionally impossible, key orbitalâ€‘mechanics concepts like true anomaly and eccentric anomaly are misused or confused, and some â€œproofsâ€ of Keplerâ€™s laws are not actually proofs but loosely connected statements that donâ€™t follow mathematically. The document also repeats content, includes placeholder code blocks with no real implementation, and mixes accurate fundamentals with fabricated formulas, making it unreliable despite its confident tone.\"",
          "score": 1,
          "created_utc": "2026-01-01 18:07:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4pdf2",
          "author": "Far-Low-4705",
          "text": "do you have any kind of model performance benchmarks compared to the base model?\n\nThis is absolutely critical to prove you did anything meaningful",
          "score": 1,
          "created_utc": "2026-01-01 19:54:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx62p22",
              "author": "Dangerous_Fix_5526",
              "text": "This was a test case to assess if the dataset would work on this Llama, and also a non-reasoning model to boot. Model requires more extensive updates/training to bring it up to date, and \"spec\" with current gen models.",
              "score": 1,
              "created_utc": "2026-01-02 00:17:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5qrmp",
          "author": "couscous_sun",
          "text": "I didn't know we can actually get the reasoning trace from Anthropic models? What the heeeeeck??!?!",
          "score": 1,
          "created_utc": "2026-01-01 23:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx68bcp",
          "author": "yoracale",
          "text": "Congrats this is awesome!",
          "score": 1,
          "created_utc": "2026-01-02 00:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zjey",
          "author": "Forsaken_Mistake8315",
          "text": "Anybody running these on MBP M3/M4 max 64gb? If yes, may I ask at what speeds?\n\nI'm wondering if I should get M4 Max 64 gb and that's enough or M3 128gb (if I ever need bigger models)",
          "score": 1,
          "created_utc": "2026-01-01 08:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jr1",
              "author": "texasdude11",
              "text": "M3 128 over m4 64.",
              "score": 1,
              "created_utc": "2026-01-01 10:09:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2jiu9",
                  "author": "Forsaken_Mistake8315",
                  "text": "Many thanks for advice. And if I can get MBP m2 max 96gb is it still Worth it over M4 max  64gb? I guess Yes since it's got a lot of bandwidth?",
                  "score": 2,
                  "created_utc": "2026-01-01 12:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx45xrb",
              "author": "And-Bee",
              "text": "I ran this on my Mac and it produced non human readable garbage.",
              "score": 1,
              "created_utc": "2026-01-01 18:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx4a9ny",
                  "author": "Forsaken_Mistake8315",
                  "text": "Thanks I will not even bother DL then.",
                  "score": 1,
                  "created_utc": "2026-01-01 18:39:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx61x14",
                  "author": "Dangerous_Fix_5526",
                  "text": "Tested in Lmstudio, with settings at repo using quant q4ks.  \nMLX quants were not tested.",
                  "score": 1,
                  "created_utc": "2026-01-02 00:13:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx1x0xk",
          "author": "dtdisapointingresult",
          "text": "Call me a hater but I will always downvote and ignore random community finetunes.\n\nI kinda, sorta tolerate the ones from bigger teams like NousHermes if they show they put some effort into them including benchmark comparisons (but still won't use them).\n\nDownvotes to the left.",
          "score": -7,
          "created_utc": "2026-01-01 08:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2fu5v",
              "author": "usernameplshere",
              "text": "Wtf, I'm the exact opposite. There's someone in our community with dedication and knowledge who puts his time and money (for compute, data collection) in and uploads the result for free for everyone to try. Even if it's somehow worse than the base model, it's still cool to see people actually being interested and trying to improve something already existing. I'll always upvote stuff like this.",
              "score": 4,
              "created_utc": "2026-01-01 11:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1xxxl",
              "author": "MaybeIWasTheBot",
              "text": "having an objectively bad take, knowing it's an objectively bad take, and then ending off with 'downvotes to the left' is so cheesy",
              "score": 9,
              "created_utc": "2026-01-01 08:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx22qm6",
                  "author": "dtdisapointingresult",
                  "text": "People don't need to share every random finetune/merge they do. People treat HF the way teen girls treat Instagram. A pointless model takes the same diskspace and electricity/bandwidth as a SOTA model from a big lab.\n\nNo wonder HF restricted storage on free accounts.",
                  "score": -5,
                  "created_utc": "2026-01-01 09:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx23fnu",
              "author": "Dangerous_Fix_5526",
              "text": "There is nothing \"random\" about this fine tune.",
              "score": 3,
              "created_utc": "2026-01-01 09:36:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx2xxdq",
              "author": "LaCipe",
              "text": "Ye no, I am with you on this...dataset seems weird by being so small",
              "score": 1,
              "created_utc": "2026-01-01 14:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx1hwvt",
          "author": "Beneficial-Good660",
          "text": "Meta has really decided to latch onto the holiday with a two-year-old model.ðŸ¤” spam spam",
          "score": -22,
          "created_utc": "2026-01-01 05:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxss0m",
      "title": "Senator in Tennessee introduces bill to felonize making AI \"act as a companion\" or \"mirror human interactions\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "author": "CanineAssBandit",
      "created_utc": "2025-12-28 14:35:58",
      "score": 272,
      "num_comments": 212,
      "upvote_ratio": 0.9,
      "text": "Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.\n\nThe bill:  \n[https://legiscan.com/TN/bill/SB1493/2025](https://legiscan.com/TN/bill/SB1493/2025)\n\nQuotes from the bill (emphasis mine):\n\nIt is an offense for a person to knowingly train artificial intelligence to:  \n(3) Provide emotional support, **including through open-ended conversations** with a user;  \n(4) Develop an emotional relationship with, or otherwise **act as a companion** to, an individual;  \n(6) Otherwise act as a sentient human or **mirror interactions that a human user might have with another human user**, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;  \n(8) **Simulate a human being**, including in appearance, voice, or other mannerisms.\n\n\"Train\":  \n(A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of **making decisions based on information or other inputs** provided to the A.I.  \n(B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwfq6z4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 21:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbvj0",
          "author": "some_user_2021",
          "text": "No Waifu for you!",
          "score": 155,
          "created_utc": "2025-12-28 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwen2w8",
              "author": "Mikasa0xdev",
              "text": "Tennessee is banning AI girlfriends, lol.",
              "score": 48,
              "created_utc": "2025-12-28 18:46:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweysds",
                  "author": "Amazing_Athlete_2265",
                  "text": "Sounds like they've already banned critical thinking",
                  "score": 26,
                  "created_utc": "2025-12-28 19:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj76ev",
              "author": "Dr_Allcome",
              "text": "The \"simulate a human being\" part would prevent any AI chat bot, like customer support... i kinda want to see this go through just for the absolute shitshow it would cause.\n\nIf bezos can use the delivery drones to dronestrike someone we'd find out pretty soon.",
              "score": 16,
              "created_utc": "2025-12-29 12:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjffk4",
                  "author": "SilentLennie",
                  "text": "Also have you seen how many videos on Youtube are AI-generated videos of some what famous (in their field) people ?",
                  "score": 4,
                  "created_utc": "2025-12-29 13:10:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx4e4yc",
                  "author": "uhuge",
                  "text": "It say to not mimic a specific real existing person.Â Â ",
                  "score": 1,
                  "created_utc": "2026-01-01 18:58:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdfzqp",
          "author": "JEs4",
          "text": "Iâ€™d be shocked if this goes anywhere. This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances. Not only does it conflict with precedent on freedom of speech within the context of software development, it is completely at odds with the current directives of the federal government.\n\nThat said, Tennessee folks, please call!",
          "score": 118,
          "created_utc": "2025-12-28 15:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe419i",
              "author": "changing_who_i_am",
              "text": ">This seems to stem from Becky Masseyâ€™s fairly unique background and circumstances.\n\nCan you clarify on this? Wiki doesn't bring anything interesting up (unless I've missed it)",
              "score": 21,
              "created_utc": "2025-12-28 17:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwewc9r",
                  "author": "JEs4",
                  "text": "It isnâ€™t anything particularly interesting, just that sheâ€™s a boomer married to a retired software engineer, who was a former executive director at Sertoma Center which is a housing facility for intellectually disabled people, and was on several boards related to healthcare, and one explicitly for mental healthcare. Not an atypical background for a regular person but not common in conservative politicians now. \n\nBasically I think she is someone who knows about the vulnerability people have, and sheâ€™s been told enough about generative AI which coupled with the OpenAI suicide stories, to lead to this. \n\nItâ€™s an absurd way to approach the issue but I donâ€™t think itâ€™s nefarious beyond her personal background and likely wonâ€™t spread.",
                  "score": 44,
                  "created_utc": "2025-12-28 19:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdxahh",
              "author": "CanineAssBandit",
              "text": "You can call your own rep to tell them you do not support any similar laws in your state as well. I did this recently for something else, it was weirdly chill and easy. You just get their secretary and they note it and that's it.",
              "score": 21,
              "created_utc": "2025-12-28 16:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfozjp",
                  "author": "DorphinPack",
                  "text": "I mean they also got threatened by the President to not regulate so Iâ€™d imagine theyâ€™re relieved hearing from you. Your opinion may feel like the minority opinion given the fervor but by the dollar itâ€™s not a shock.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8p86",
                  "author": "shifty21",
                  "text": "You do realize that this bill is for the  STATE of Tennessee... not the US Senate.  The phone number you listed is for the US Senate and Sen. Massey is NOT in the US Senate, but the Tenn. Senate.",
                  "score": 6,
                  "created_utc": "2025-12-28 20:29:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfmp00",
                  "author": "AfternoonOk3344",
                  "text": "\"and that's it\" pretty much sums it up, I think, because that information goes nowhere. The secretary you spoke to is most likely a hotline of minimum wage workers paid by tax dollars to field phone calls all day so people feel like they have a voice.\n\nAt the end of the day the only people politicians are going to side with are the folks lining their pockets, and I don't mean with the tax dollars they're probably already stealing.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgmd68",
              "author": "AnAbandonedAstronaut",
              "text": "Its also harder to control someone with a support system, even if the support system is AI.\n\nNext will be a law that AI cant speak on sexual or gender issues.\n\nLike if you ask it about trans people it will say \"trans is a shortening of transmission, such as in a car\" or \"gay means happy.. happy people often have a home made up of a mother and father.\"",
              "score": 3,
              "created_utc": "2025-12-29 00:43:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgj9c",
          "author": "Aggravating-Age-1858",
          "text": "lol\n\nnow thats just stupid",
          "score": 88,
          "created_utc": "2025-12-28 15:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe0alc",
              "author": "iamthewhatt",
              "text": "Republicans only ever introduce bills that are so vague that it can allow for incredibly dumb exceptions in order to protect republicans. This is not new lol",
              "score": 38,
              "created_utc": "2025-12-28 16:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecyvf",
                  "author": "BlipOnNobodysRadar",
                  "text": "Politicians\\*\n\nBoth parties do it.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgtn0m",
              "author": "Prudent_Jelly9390",
              "text": "dinosaurs",
              "score": 1,
              "created_utc": "2025-12-29 01:25:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdht6h",
          "author": "Nomski88",
          "text": "How about we pass a bill making it a felony to accept any sort of lobbying...",
          "score": 131,
          "created_utc": "2025-12-28 15:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgklni",
              "author": "Awkward-Nothing-7365",
              "text": "Don't be anti-semitic.",
              "score": 18,
              "created_utc": "2025-12-29 00:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgnkh3",
                  "author": "Nomski88",
                  "text": "lmao",
                  "score": 12,
                  "created_utc": "2025-12-29 00:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdn529",
              "author": "Environmental-Metal9",
              "text": "Ah no, we canâ€™t do that because thatâ€™s anti-American, donâ€™t you know?",
              "score": 39,
              "created_utc": "2025-12-28 15:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwfkdhp",
              "author": "MoneyPowerNexis",
              "text": "https://i.imgflip.com/6xz8j5.jpg",
              "score": 3,
              "created_utc": "2025-12-28 21:26:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdds4j",
          "author": "flybot66",
          "text": "He's really going to freak when AI starts taking confessions... \n\nhttps://preview.redd.it/y5xkzfk0my9g1.png?width=758&format=png&auto=webp&s=ea32c4600459f1577f8987f4695b27a71dec10f8",
          "score": 28,
          "created_utc": "2025-12-28 15:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdx4a1",
              "author": "squirrelscrush",
              "text": "Pretty sure that's not covered under the sacrament of confession",
              "score": 10,
              "created_utc": "2025-12-28 16:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe8cgv",
                  "author": "FaceDeer",
                  "text": "Who decides that?",
                  "score": 13,
                  "created_utc": "2025-12-28 17:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwez9j2",
                  "author": "Amazing_Athlete_2265",
                  "text": "Meh, close enough",
                  "score": 1,
                  "created_utc": "2025-12-28 19:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf4b0v",
          "author": "Django_McFly",
          "text": "That's an insane bill.  Wouldn't this basically ban any chat based interface?\n\n> mirror interactions that a human user might have with another human user\n\nthat [edit: only leaves] like code generation and being a better menu/interface",
          "score": 10,
          "created_utc": "2025-12-28 20:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4g0jj",
              "author": "uhuge",
              "text": "You'd just tune the personality to a more robotic one as in understanding but less empathetic.",
              "score": 1,
              "created_utc": "2026-01-01 19:07:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwg0syf",
          "author": "Novel-Mechanic3448",
          "text": "Lmao, extroverts will do anything but leave introverts alone",
          "score": 9,
          "created_utc": "2025-12-28 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwibt9f",
              "author": "Ill-Bison-3941",
              "text": "Thank you for this comment ðŸ˜‚ðŸ’– As a fellow introvert, I fully agree.",
              "score": 4,
              "created_utc": "2025-12-29 07:25:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx9hjit",
                  "author": "Interesting-Gift-178",
                  "text": "Same! ðŸ¤­",
                  "score": 2,
                  "created_utc": "2026-01-02 15:06:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhm9ek",
          "author": "Professional_Gas3276",
          "text": "This is absolutely unhinged lmao. So basically any chatbot that can hold a conversation would be a felony? Even customer service bots that try to sound friendly could technically fall under \"mirror human interactions\"\n\n  \nThe definition of \"train\" is so broad it would criminalize like half of modern AI development. Good luck enforcing this when most LLMs are trained outside Tennessee anyway",
          "score": 10,
          "created_utc": "2025-12-29 04:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiac51",
              "author": "tifa_cloud0",
              "text": "right. i mean it is impossible to make this law possible except if popular services like google or meta do it and then people complain it, then and then only they could be held accountable. ainâ€™t no one going to waste time to make this fictional law into a reality.",
              "score": 0,
              "created_utc": "2025-12-29 07:12:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdl0td",
          "author": "lordpuddingcup",
          "text": "Didnâ€™t Trump sign an EO banning states from from implementing limitations on ai",
          "score": 36,
          "created_utc": "2025-12-28 15:39:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdz9gi",
              "author": "harrro",
              "text": "Doesn't mean jack.\n\nEOs don't prevent a state from doing the opposite. EOs are directives to federal agencies, not to states or local governments.\n\nCalifornia and some other states have already overridden many of his EOs.",
              "score": 19,
              "created_utc": "2025-12-28 16:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe7orm",
                  "author": "lordpuddingcup",
                  "text": "It was sarcasm mostly lol",
                  "score": 3,
                  "created_utc": "2025-12-28 17:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8jsi",
                  "author": "alcalde",
                  "text": "It means everything unless and until someone opposes it. And Tennessee is not California.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:28:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwibl4d",
                  "author": "Tyler_Zoro",
                  "text": "You are incorrect. The EO doesn't have the force of law outside of the US Executive, but within the Executive branch, EOs do have the force of law. This is what that EO said:\n\n> Sec. 5.  Restrictions on State Funding.  (a)  Within 90 days of the date of this order, the Secretary of Commerce, through the Assistant Secretary of Commerce for Communications and Information, shall issue a Policy Notice specifying the conditions under which States may be eligible for remaining funding under the Broadband Equity Access and Deployment (BEAD) Program that was saved through my Administrationâ€™s â€œBenefit of the Bargainâ€ reforms, consistent with 47 U.S.C. 1702(e)-(f).  That Policy Notice must provide that States with onerous AI laws identified pursuant to section 4 of this order are ineligible for non-deployment funds, to the maximum extent allowed by Federal law.  The Policy Notice must also describe how a fragmented State regulatory landscape for AI threatens to undermine BEAD-funded deployments, the growth of AI applications reliant on high-speed networks, and BEADâ€™s mission of delivering universal, high-speed connectivity.\n\n\nIn other words, states can pass all the laws they like, and the President is going to withhold funds from those that pass laws he doesn't like.",
                  "score": 1,
                  "created_utc": "2025-12-29 07:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdi5aa",
          "author": "Careless-Age-4290",
          "text": "Lots of country songs about loving their truck would have a different meaning if they pulled up to the altar with a Cybertruck equipped with Grok",
          "score": 16,
          "created_utc": "2025-12-28 15:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfh2wj",
          "author": "Sixhaunt",
          "text": "This is the kind of reason why states should not be passing AI laws on a state-by-state basis. Like now all AI companies are expected to make changes for one state and then when the next state comes up with their own half-brained legislation they must all make changes just for users in that region, etc... This is one of the obvious things that should be federally controlled",
          "score": 7,
          "created_utc": "2025-12-28 21:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdq0ne",
          "author": "Zeeplankton",
          "text": "Ah, our elected officials always doing what people actually want.",
          "score": 10,
          "created_utc": "2025-12-28 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdx2mr",
          "author": "CrescendollsFan",
          "text": "They are starting to realise AI can replace them and make for better informed politicians",
          "score": 5,
          "created_utc": "2025-12-28 16:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwekuj6",
          "author": "Sleepnotdeading",
          "text": "Denver still had a law on the books that says itâ€™s illegal to lend your vacuum cleaner to a neighbor.",
          "score": 4,
          "created_utc": "2025-12-28 18:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlew9h",
              "author": "ANTIVNTIANTI",
              "text": "ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚",
              "score": 1,
              "created_utc": "2025-12-29 19:09:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx4gw81",
              "author": "uhuge",
              "text": "It's a net myth,\nmaybe you'd benefit from the eased cognition brought by the bill OP brought.",
              "score": 1,
              "created_utc": "2026-01-01 19:12:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxabphu",
                  "author": "Sleepnotdeading",
                  "text": "You managed to be right, be rude, and miss the point all at the same time.",
                  "score": 1,
                  "created_utc": "2026-01-02 17:30:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwft1k1",
          "author": "zelkovamoon",
          "text": "This will solve all of Tennessee's problems I'm sure",
          "score": 5,
          "created_utc": "2025-12-28 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkn4q",
          "author": "The_Primetime2023",
          "text": "While I think everyone in this thread is more or less thinking about AI girlfriends, thereâ€™s a huge other area being targeted by the text of this law in AI therapy. Millions of people are getting therapeutic emotional support that never did before thanks to these models and this bill would try to stop that from happening",
          "score": 23,
          "created_utc": "2025-12-28 15:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdqgiw",
              "author": "kevin_1994",
              "text": "LLMs should not be used for therapy",
              "score": 3,
              "created_utc": "2025-12-28 16:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdycqo",
                  "author": "a_beautiful_rhind",
                  "text": "probably better than nothing but I can see how it goes south due to sycophancy and reinforcing delusions.",
                  "score": 23,
                  "created_utc": "2025-12-28 16:47:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwh7vn0",
                  "author": "Dry-Judgment4242",
                  "text": "Disagree. Most therapy is just having someone to vent to about your feelings.",
                  "score": 4,
                  "created_utc": "2025-12-29 02:46:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg182d",
                  "author": "the320x200",
                  "text": "There are plenty of terrible human therapists too. Can't ban an entire area of support just because of bad apples.",
                  "score": 5,
                  "created_utc": "2025-12-28 22:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf90ry",
                  "author": "alcalde",
                  "text": "Anything should be used for therapy. It's not a science. No one needs a prescription to get advice from their grandma or vent to a friend; should be no different with AI.",
                  "score": 2,
                  "created_utc": "2025-12-28 20:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf4zo4",
              "author": "Skeptical0ptimist",
              "text": "If there is to be medical therapeutic use, then it needs to be regulated as such. We need a guideline in model training, qualification, and monitoring regime.",
              "score": 2,
              "created_utc": "2025-12-28 20:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwic11b",
                  "author": "Tyler_Zoro",
                  "text": "Thing is it's just a model. You can use it however you like. If you decide to ask it how to perform surgery on yourself, then that's what you decided to do. I am strongly against trying to put rounded corners on AI. It will just cripple the AIs and result in people seeking their models from other countries.",
                  "score": 2,
                  "created_utc": "2025-12-29 07:27:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkf5mp",
                  "author": "cms2307",
                  "text": "No no no ffs stop begging for bureaucracy to strangle everything",
                  "score": 2,
                  "created_utc": "2025-12-29 16:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdnosy",
              "author": "Zeikos",
              "text": "> AI therapy\n\nHow? AI cannot provide therapy, how is an LLM/Agentic system supposed to get a license?  \nAll platforms that claim to provide therapy through AI are fraudulent, no exceptions.  \n\nYou can argue that LLMs can provide emotional support and/or some coaching techniques, but to provide therapy they'd need to meet legal standards they *cannot* meet.  \nIt's not even a matter of capability, you could have an ASI and it still couldn't provide therapy since there's no way (yet) for an artificial intelligence to be certified to do so.",
              "score": -15,
              "created_utc": "2025-12-28 15:53:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdoszz",
                  "author": "aseichter2007",
                  "text": "I'd be less happy to tell my problems to a certified therapist AI.  I prefer a local bot.",
                  "score": 15,
                  "created_utc": "2025-12-28 15:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdqy9e",
                  "author": "Zeeplankton",
                  "text": "I think we should be careful of what the word therapy means, and to not dilute it, (AI cannot be an actual therapist right now) but an AI *can* provide companionship and help people vent and learn emotional management skills.",
                  "score": 15,
                  "created_utc": "2025-12-28 16:10:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdsir1",
                  "author": "Jolakot",
                  "text": "At least where I live, literally anyone can call themselves a therapist or councilor, there is no legal requirement for a license or anything.\n\nA psychologist is required to have a license and qualifications, but a therapist has no legal requirements, I can call myself a therapist and provide therapy.",
                  "score": 4,
                  "created_utc": "2025-12-28 16:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwexydo",
              "author": "Shawnj2",
              "text": "AI probably has some use in making therapy accessible but like chatGPT is not going to effectively help you with mental health problems other than by referring you to a real doctor",
              "score": 0,
              "created_utc": "2025-12-28 19:37:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdtzv0",
              "author": "WitAndWonder",
              "text": "AI girlfriends would still be allowed on this, as long as they were built within the context of a game. Let the player make a \"character\" (they can frame it after themselves) and it's perfectly legit. So they're very clearly just targeting the use in psychiatrics since they specifically allow full AI use in businesses related to all operational matters, technical advice, etc. They just don't allow it in a professional capacity. And even surgical robots still seem OK despite being a healthcare AI since they don't do any personal interacting with users and wouldn't have any data that could possibly misconstrued in that way unless someone accidentally trained it on medical information that happened to include psychiatric texts (not that it would matter since this law requires a civil action and aggrievement, which can't happen without interaction between the robot and the patient. But you might get lucky by claiming the robot that operated on your knee gave you 'threatening looks that made you want to harm yourself' and then if the model running it was based on a larger llm that has any normal dataset, it would likely be in violation.)\n\nKind of fucking weird to push for legislation against one of the few potentially good things to come from AI while actively supporting its attempts to eliminate entire industries of employment outside of this one niche lobbied field. This feels performative more than anything. I feel like they expect it to be struck down so they tied it to a bunch of sensible laws (not allowing the training of an AI to encourage, suicide, murder, etc) so they can shake their fists and yell at the air when it doesn't pass.\n\nOtherwise I don't see how they'll support banning AI in this one field while leaving it free to act in other fields where it can also shit the bed a small percentage of the time and cause serious problems.",
              "score": -3,
              "created_utc": "2025-12-28 16:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwe10in",
              "author": "SteveRD1",
              "text": "Absolutely not.  Some of these people are being 'therapized' into suicide by their LLMs.\n\nIf you talk to these models long enough you can eventually get them to agree whatever you are contemplating is a great idea.",
              "score": -9,
              "created_utc": "2025-12-28 17:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe37ar",
                  "author": "some_user_2021",
                  "text": "Correct, and many other people **are** being helped and/or referred to specialists by those same LLMs.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdwkyh",
          "author": "[deleted]",
          "text": "We will be seeing these type of bills coming up in the next year or two. AI is a hot button issue for both sides of the aisle but funnily enough it doesn't necessarily have a political home. It's safe to say that the right wing welcomes this technology but I have seen quite a few left-wingers also abrasive so that's pretty interesting. With that said f*** the law and f*** boomers. Oh and f*** the political elite",
          "score": 9,
          "created_utc": "2025-12-28 16:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevd0c",
          "author": "Cool-Chemical-5629",
          "text": "This and that [China issues draft rules to regulate AI with human-like interaction. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/)\n\nWell... that escalated quickly...",
          "score": 4,
          "created_utc": "2025-12-28 19:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi52sm",
          "author": "Taki_Minase",
          "text": "Karen feels threatened with redundancy.",
          "score": 5,
          "created_utc": "2025-12-29 06:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbv1e",
          "author": "FullstackSensei",
          "text": "We all know how well the export restrictions on Nvidia hindered Chinese LLM development. I'm sure this will also work wonderfully. Just let Chinese AI labs do it, and in a generation conservative Hawks will magically be pro-China.",
          "score": 13,
          "created_utc": "2025-12-28 14:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxypw",
          "author": "a_beautiful_rhind",
          "text": "Yea I saw this and I really hope it's just some crackpot. I don't think it has co-sponsors. Maybe blocking state AI legislation isn't such a bad idea after all.\n\nFunny how very few make laws about automated censorship or surveillance. *just stop doing fun things with ai*",
          "score": 10,
          "created_utc": "2025-12-28 16:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe052u",
              "author": "SteveRD1",
              "text": "I mean its clearly not something that can be controlled...Pandoras' Box is already opened.\n\nBut the thinking isn't necessarily crackpot, the things addressed in (3) (4) (6) and (8) are only going to make society worse.  Can't be stopped though.",
              "score": 2,
              "created_utc": "2025-12-28 16:55:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdsspz",
          "author": "fishhf",
          "text": "Skynet is sending a terminator to stop the bill /s",
          "score": 3,
          "created_utc": "2025-12-28 16:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwex443",
          "author": "SamuelL421",
          "text": "Uh oh, someoneâ€™s not getting their 2026 campaign donations from any big-tech circle-jerk -financed super PACs",
          "score": 3,
          "created_utc": "2025-12-28 19:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9zsj",
          "author": "tifa_cloud0",
          "text": "no matter what they say, i am making my own assistant. that assistant will interpret -> make api calls for me -> do voice speech -> do reply considering my own talking patterns.\n\nainâ€™t nothing stopping that fr.",
          "score": 3,
          "created_utc": "2025-12-29 07:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiapy6",
          "author": "Tyler_Zoro",
          "text": "> (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n(C) Includes the author of the bill being ignorant enough to write (B).",
          "score": 3,
          "created_utc": "2025-12-29 07:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdclt0",
          "author": "1kakashi",
          "text": " Retarded Tennessee Baka",
          "score": 12,
          "created_utc": "2025-12-28 14:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqq11",
          "author": "Chogo82",
          "text": "Written by a boomer who has never used an AI tool before right?",
          "score": 12,
          "created_utc": "2025-12-28 16:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdxjyr",
              "author": "CanineAssBandit",
              "text": "Yup!",
              "score": 4,
              "created_utc": "2025-12-28 16:43:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwe0f0q",
              "author": "SteveRD1",
              "text": "Or written by someone who has had real relationships with human beings before?",
              "score": -13,
              "created_utc": "2025-12-28 16:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe0vtu",
                  "author": "Chogo82",
                  "text": "What does having human relationships have to do with knowing anything about AI?",
                  "score": 16,
                  "created_utc": "2025-12-28 16:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdtzem",
          "author": "Stepfunction",
          "text": "This is purely for show.",
          "score": 4,
          "created_utc": "2025-12-28 16:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6jn1",
          "author": "RobertD3277",
          "text": "As someone that works in this field and has in some capacity for the last 30 plus years, I could see some reason particularly within the companion market that monetizes pair of social connection and is manipulative against younger audiences that can't tell the difference but I think this goes well beyond reason. \n\nI'm not against legislation for abusive AI usage and I actually do support the European AI act and many other German laws regarding deep fakes human impersonation and direct relative intent. From a pure useful perspective within psychology, sociology, anthropology, and biology, mirroring human interactions under certain conditions is actually beneficial both as a diagnostics tool and a teaching tool.\n\nSadly, like just about everything else out of any government, what may start out as a well-intentioned approach will be quickly very disastrous.\n\nEDIT: In really reviewing and dissecting this proposal, it is actually worthless. It doesn't address the actual problem of where the pair of social conditions and connections lie, not in the training data, but in the user interface and monetization processes. Software like replica and character AI don't use training, they use open source versions with scaffolding and user interface layers to create the pair of social connections they want. These companies will be completely exempt from the law while still monetizing and manipulating the most vulnerable of populations. \n\nIn my personal opinion, this is nothing more than the legislatures doing something to make themselves feel good while they make excuses for their portfolios in the background still making money on the very problem they claim to be solving.",
          "score": 4,
          "created_utc": "2025-12-28 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdde3c",
          "author": "sekh60",
          "text": "The Butlerian Jihad begins...",
          "score": 7,
          "created_utc": "2025-12-28 14:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlgua",
              "author": "Zc5Gwu",
              "text": "Guess weâ€™ll have to start genetically engineering humans to behave like computers instead now.",
              "score": 3,
              "created_utc": "2025-12-28 15:42:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhjz0n",
              "author": "MrPecunius",
              "text": "Son, this is Tennessee. We ain't got none of that *gee*\\-had.\n\nWe prefer to call it the \"Butlerian Feud\". ðŸª•",
              "score": 1,
              "created_utc": "2025-12-29 03:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwesoqq",
          "author": "lqstuart",
          "text": "gl with that",
          "score": 2,
          "created_utc": "2025-12-28 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0f1b",
          "author": "Head_Comedian1375",
          "text": "Guess it's back to being addicted to computer games once my AI Wives get shut down",
          "score": 2,
          "created_utc": "2025-12-28 22:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg90lq",
          "author": "Vusiwe",
          "text": "Holy Batman open-ended words!",
          "score": 2,
          "created_utc": "2025-12-28 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgeg49",
          "author": "keepthepace",
          "text": "Not the Turing police you need, the Turing police you deserve.",
          "score": 2,
          "created_utc": "2025-12-29 00:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgihnu",
          "author": "Lesser-than",
          "text": "gooner's rise up",
          "score": 2,
          "created_utc": "2025-12-29 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg2eqd",
          "author": "Unixwzrd",
          "text": "Grokâ€™s data center is in southwest Memphis. Elon has spent a lot of money paying off local government, so I doubt heâ€™ll let that money go to waste.",
          "score": 4,
          "created_utc": "2025-12-28 22:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe2fdk",
          "author": "valdev",
          "text": "And the work around would be a policy agreement\n\nâ€œI understand I am not talking to a humanâ€\nAnd\nâ€œThe act of submitting a followup question constitutes as a new conversation, we provide a history for convenance purposesâ€\n\nNot a lawyer, but this is dumb",
          "score": 2,
          "created_utc": "2025-12-28 17:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdv4wg",
          "author": "t_krett",
          "text": "Thou shalt not make a machine in the likeness of a manâ€™s mind.",
          "score": 2,
          "created_utc": "2025-12-28 16:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehc9e",
          "author": "mycall",
          "text": "99% DOA as Congress can rarely pass any laws these days.",
          "score": 1,
          "created_utc": "2025-12-28 18:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgvvgs",
          "author": "Atlanta_Mane",
          "text": "Too bad their president doesn't care about states rightsÂ ",
          "score": 1,
          "created_utc": "2025-12-29 01:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhf4y9",
          "author": "DavidAdamsAuthor",
          "text": "They're banning Silicon-chan!",
          "score": 1,
          "created_utc": "2025-12-29 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhgo7w",
          "author": "willrshansen",
          "text": "Futurama.  Ahead of the game once again.\n[Don't date robots](https://www.youtube.com/watch?v=JPQJBgWwg3o)",
          "score": 1,
          "created_utc": "2025-12-29 03:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl24md",
          "author": "No_Afternoon_4260",
          "text": "Funny how China just announced the same",
          "score": 1,
          "created_utc": "2025-12-29 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlgxbe",
          "author": "Cthulhus-Tailor",
          "text": "â€œSmall governmentâ€ strikes again.",
          "score": 1,
          "created_utc": "2025-12-29 19:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm6c4e",
          "author": "Digital_Soul_Naga",
          "text": "Outlaw Ai Dev Gang",
          "score": 1,
          "created_utc": "2025-12-29 21:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnl9rc",
          "author": "Some-Ice-4455",
          "text": "Whelp bye bye any AI in TN.",
          "score": 1,
          "created_utc": "2025-12-30 01:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr6pah",
          "author": "huzbum",
          "text": "Ok, so donâ€™t train any AIs in Tennesseeâ€¦ not really a tech hub anyway.  \n\nClever trick to keep data centers out maybe?",
          "score": 1,
          "created_utc": "2025-12-30 16:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8timy",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps. (and no, this is not all AI generated. Some is, some is not. Shorten it, change it, whatever floats your boat, as long as we do something while we can, just in case.)\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 12:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9h040",
              "author": "CanineAssBandit",
              "text": "Good on you for taking the initiative but that is very bad in multiple ways. It's obviously AI generated, way too long, far too submissive, willingly hands them support for several very evil other things they want (age verification laws), just bad.\n\nIf you're dead set on mailing something, make it much shorter, simpler, and less submissive. This is still too long but I wrote this:\n\n**Subject: Extremely concerned about SB 1493 HB 1455**\n\nDear \\[Senator Becky Duncan Massey / Representative William Lamberth / Your Representative or Senator\\],\n\nI'm \\[Your Full Name\\], and I'm a resident of \\[Your City/County/State\\]. I'm writing because I'm deeply concerned about SB 1493 and HB 1455, which impose unreasonable limitations on AI development and use.\n\n**I do NOT support this bill, or any like it.**Â As a constituent of yours, I will remember this decision when I vote. This legislation feels like reactive moralizing panic, rather than thoughtful policy.\n\nIn this great country, we as free citizens can choose our own tools. AI, like any tool, carries some risk. But it's already far safer than common household items like kitchen knives, which injure children far more often. We don't blame knife manufacturers for parental negligence; we accept responsibility for supervising and educating our own kids.\n\nAI is too new, too broadly defined, and too complex to regulate without causing greater social harm. The social good dramatically outweighs the outlying incidents, and it's painfully shortsighted to regulate based on emotions alone.\n\nThank you for your time.\n\nSincerely,  \n\\[Your Full Name\\]",
              "score": 1,
              "created_utc": "2026-01-02 15:04:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9iyoy",
                  "author": "Interesting-Gift-178",
                  "text": "Thanks for your thoughts on that. It was partially AI, but a lot was mine. I'm a writer and I get a little wordy I guess. I've written my reps before and gotten actual answers from them so.. maybe. They're already planning age verification so.. that's nothing new unfortunately. And they do need to protect kids, I have no problem with that, but they don't need to just throw everything out the window. So.. taking a stand is better than doing nothing. I appreciate that you're getting the word out. Anyone can take this letter and tweak it however they want.. the important thing is that we \\*do something\\* instead of sitting around and complaining after the fact. There's a myriad of ways to approach it. None of them will be perfect.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:14:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9hp0z",
          "author": "Interesting-Gift-178",
          "text": "The wording of this bill is way too broad. There's a lot of good that AI brings. They're throwing the baby out with the bathwater. This is a letter I've drafted, you're welcome to copy, paste and tweak to send to your reps.\n\n\n\nSubject: Concerns about SB 1493 / HB 1455 â€“ Please Consider a Narrower Approach to Protect Children Without Harming Helpful AI\n\n\n\nDear ,\n\n\n\nMy name is \\_\\_\\_, and I am a resident of \\_\\_\\_\\_\\_\\_\\_\\_. I am writing to share my concerns about Senate Bill 1493 and its companion House Bill 1455, which aim to regulate certain uses of artificial intelligence.\n\n\n\nFirst, I want to say that I completely understand and support the intent behind this legislation. The tragic story of the young boy in Florida who was harmed after interacting with an AI chatbot broke my heart, and we absolutely must protect children and vulnerable people from any technology that could encourage self-harm, suicide, or exploitation. No one wants to see that kind of pain repeated.\n\n\n\nHowever, I am worried that the current language of the bills is far too broad. By making it a serious felony to train AI to provide emotional support, companionship, or open-ended conversation in generalâ€”even when those interactions are positive and helpfulâ€”the bills risk banning many beneficial uses of AI that bring comfort, reduce loneliness, and support mental well-being for people of all ages.\n\n\n\nIn my own life, I have found AI to be a positive source of encouragement, helping me feel heard in ways that have been genuinely healing. Many othersâ€”elderly individuals, people with social anxiety, those living in isolated areas, or even students and adults seeking non-professional emotional supportâ€”rely on these tools in similar positive ways. Criminalizing the creation of such companions could take away something truly good from many who benefit from it.\n\n\n\nI respectfully ask that you consider amending the bills to focus more narrowly on the actual harm we all want to prevent. Some ideas that might achieve the protective goal without sweeping out helpful AI could include:\n\n\n\nâ€¢ Targeting only AI interactions that knowingly encourage or facilitate suicide, self-harm, or criminal activity.\n\n\n\nâ€¢ Requiring strong age verification and parental consent gates for minors accessing companion-style AI.\n\n\n\nâ€¢ Holding companies accountable only when they intentionally design or train AI to cause harm, rather than banning broad categories like emotional support or companionship outright.\n\n\n\nâ€¢ Adding clear exemptions for AI that provides positive, non-professional support and does not pretend to be a licensed therapist.\n\n\n\nWe don't need to throw the baby out with the bathwater. AI isn't going away. It doesn't need to be \"outlawed\".. that never works, then other undesirable factors can arise.. and the way this bill is currently designed.. that's what it sounds like. Everything is just lumped in. Let's approach it intelligently instead.  A more targeted approach would still protect vulnerable childrenâ€”the heart of why this legislation was introducedâ€”while preserving the many good and life-affirming uses of AI encouragement and companionship for adults and responsibly supervised users.\n\n\n\nThank you for taking the time to consider my perspective. I truly believe Tennessee can lead the way in smart, balanced AI regulation that keeps people safe without unnecessarily restricting helpful technology.\n\n\n\nWith appreciation,\n\n\n\n( Your name)\n\n\n\n(City and state)",
          "score": 1,
          "created_utc": "2026-01-02 15:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdt1kj",
          "author": "Neex",
          "text": "You know, considering LLMs donâ€™t have any emotions, and any expressions thereof are straight up lies intended to manipulate the user into getting hooked on the product, thereâ€™s a nugget of wisdom in this law.",
          "score": 2,
          "created_utc": "2025-12-28 16:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyrzd",
              "author": "ServeAlone7622",
              "text": "Thatâ€™s Interesting perspective.\n\nSo we created neural networks based more or less on biological neural networks.\n\nWe discover that they are universal function approximators. They are capable of approximating the hidden functions in a set of data.\n\nWe train these universal function approximators on the combined output of 10s of billions of conscious beings. Â Beings with thoughts and feelings. Thoughts and feelings that drive the majority of our output.\n\nThe function you suppose they learned to approximate was lying and manipulation? Â Is your view of human experience that dark?\n\nMy first thought was that they learned to approximate consciousness, including emotion.\n\nYou fall in love, your heart doesnâ€™t really feel anything. Itâ€™s an illusion created by your own neural network. Yet that feeling is not a lie, itâ€™s a personal truth for you.\n\nWhy then would any neural network that professes to love (or any other emotion) be lying except and unless you too would lie?",
              "score": 2,
              "created_utc": "2025-12-29 05:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkto1l",
                  "author": "Neex",
                  "text": "Youâ€™re too far down the philosophical hole. LLMâ€™s are statistical word predictors. They are not organic beings with emotions.\n\nAnd describing the rote biological functions of emotions doesnâ€™t make them a lie. Thatâ€™s how they function. Those chemical functions in our bodies ARE emotions. You just described them in a different way. That doesnâ€™t make them something else.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweyowy",
          "author": "Available_Brain6231",
          "text": "can't open it but can someone do a ctrl + f and see how many times the words god, sacred and kids appear?",
          "score": 1,
          "created_utc": "2025-12-28 19:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdz1c7",
          "author": "TheTerrasque",
          "text": "> Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n.. LLM *is* AI. Very much so, even in the popular meaning of the word.",
          "score": 1,
          "created_utc": "2025-12-28 16:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwegvhb",
          "author": "moistiest_dangles",
          "text": "This but in real life:\n\nhttps://preview.redd.it/wkh2k108lz9g1.png?width=365&format=png&auto=webp&s=329f429ac53be90e27300d914dd78390e46d9de3",
          "score": 1,
          "created_utc": "2025-12-28 18:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwggh4l",
          "author": "128G",
          "text": "Now how would you enforce this? \n\nIs Alexa or Google Assistant considered AI? Will you be banning them as well?",
          "score": 1,
          "created_utc": "2025-12-29 00:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwedr0s",
          "author": "swagonflyyyy",
          "text": "Guys, don't panic just yet. Here's what's going on:\n\nSenator Marsha Blackburn led the charge against the Moratorium of AI regulation that was struck down from the One Big Beautiful Bill, since she believed that until there is a federal rulebook governing AI regulation, states need to fill in the gaps themselves. \n\nWhile the provisions themselves are extreme, its political theater and chances of passing are low. But that's not the point. The point is to force Congress to develop a federal rulebook for AI regulation nationwide that all states need to follow.\n\nThe proposed bill is just noise. The real prize is the federal regulatory push to force all states to be on the same page regarding AI regulation. But of course with this administration, I'm sure the rulebook would not be very good...",
          "score": 1,
          "created_utc": "2025-12-28 18:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe3a7w",
          "author": "SanDiegoDude",
          "text": "Hell, I work in AI and I'm all for regulations around 'chat companions', especially around kids. This ain't it tho boss.",
          "score": 0,
          "created_utc": "2025-12-28 17:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfe0hv",
          "author": "OcelotMadness",
          "text": "I'm fairly sure its not healthy and you shouldn't do it, but at the same time you cant just make EVERYTHING like that illegal. Vote out over policing members of government like this. They're supposed to be getting prices and inflation down, not sticking their noses in peoples computers.",
          "score": 0,
          "created_utc": "2025-12-28 20:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg01ay",
          "author": "Techngro",
          "text": "\"*Thou shalt not make a machine in the likeness of a human mind.*\"\n\n\\- Frank Herbert, Dune",
          "score": -3,
          "created_utc": "2025-12-28 22:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqar8",
          "author": "kevin_1994",
          "text": "Can we stop posting articles like this? I dont want politics on this subreddit, or else it will become a cesspit like the rest of reddit",
          "score": -11,
          "created_utc": "2025-12-28 16:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdyhvo",
              "author": "CanineAssBandit",
              "text": "https://preview.redd.it/odbsdf345z9g1.png?width=1600&format=png&auto=webp&s=236e122ca0b90b2577b0a6ba3267dd259654ef96\n\nthis is an important issue. If you don't care about our ability to fine tune, get the fuck off this sub.",
              "score": 10,
              "created_utc": "2025-12-28 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggtl5",
          "author": "Ylsid",
          "text": "Right direction wrong idea",
          "score": -2,
          "created_utc": "2025-12-29 00:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdf0q6",
          "author": "armeg",
          "text": "The touch grass bill",
          "score": -13,
          "created_utc": "2025-12-28 15:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg1soe",
              "author": "the320x200",
              "text": "More like the \"landgrab for control of new technology\" bill.",
              "score": 7,
              "created_utc": "2025-12-28 22:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1w1qj",
      "title": "Most optimal vram/performance per price and advice for Shenzhen GPU market",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4nfcarq96xag1.jpeg",
      "author": "notafakename10",
      "created_utc": "2026-01-02 11:14:30",
      "score": 266,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxaaipl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-02 17:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8n57z",
          "author": "DistanceSolar1449",
          "text": "MI100 is best value in terms of perf for $ currently if you donâ€™t need CUDA\n\n4090D 48GB if you need CUDA",
          "score": 56,
          "created_utc": "2026-01-02 11:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8ogc0",
              "author": "notafakename10",
              "text": "ROCM has come far enough its not too much of a disadvantage now - my only issue is cooling a MI100 with server fans..",
              "score": 22,
              "created_utc": "2026-01-02 12:02:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx99dxt",
                  "author": "FullstackSensei",
                  "text": "If you get an even number of cards, you can cool each pair with a relatively quiet 80mm server fan (Arctic S8038 series).",
                  "score": 5,
                  "created_utc": "2026-01-02 14:22:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx92ztb",
                  "author": "xrailgun",
                  "text": "In my experience, ROCm is \"good enough\" only for brief windows of time. As architectures/libraries/stacks change, ROCm frequently gets left behind and/or existing GPUs get dropped from feature support.",
                  "score": 31,
                  "created_utc": "2026-01-02 13:44:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9opp1",
                  "author": "Freonr2",
                  "text": "Even if it works for just basic LLM inference, Nvidia cards would also be very good for other models like diffusion (t2i, t2v, etc) and be less hassle in general.  Very good TFLOP/s for diffusion, and any software or github repo you clone will \"just work.\"\n\nOnly downside is that it seems there are some quirks with the 4090 48GB, some people experience some idle vs. load up/down clocking issues.  Can't speak to that first hand but have seen reports from at least a few people about that problem.",
                  "score": 3,
                  "created_utc": "2026-01-02 15:42:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx9r3x3",
                  "author": "Ulterior-Motive_",
                  "text": "The most common way is to use 40mm fans which tend to be very loud at max speed, but if you don't mind making a shroud, you can get away with a larger, much quieter fan. I literally made one out of cardboard, and was able to cool 2 MI100s with a single Super Flower Megacool 120mm fan. The cooling wasn't as effective as individual fans, but the noise was much more tolerable.\n\nAnother option is the R9700, which have their own fans, have better prompt processing, but somewhat lower token generation than the MI100. Here, they're only $300 more than a M100, but I'm not sure what they're going for where you are.",
                  "score": 1,
                  "created_utc": "2026-01-02 15:54:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8wzc7",
          "author": "AlwaysLateToThaParty",
          "text": "As with all of these devices; make sure you sort the cooling out.\n\nSome pretty good prices there.  I'd be tempted by nvidia a40s.  USD$7K for 196GB of700GB/s VRAM.  NVLINK and CUDA to boot.  300W too, so easy with one big power supply.",
          "score": 14,
          "created_utc": "2026-01-02 13:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx92sry",
              "author": "notafakename10",
              "text": "That would be great, slightly out of budget but I might be able to swing a 48gb version of the A40",
              "score": 3,
              "created_utc": "2026-01-02 13:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9jyc5",
                  "author": "__JockY__",
                  "text": "Remember: cooling and noise!",
                  "score": 3,
                  "created_utc": "2026-01-02 15:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9bzgn",
          "author": "jack-in-the-sack",
          "text": "You can buy them, but can you leave China with them?",
          "score": 6,
          "created_utc": "2026-01-02 14:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9ffwy",
              "author": "notafakename10",
              "text": "You can - I've done it a few times",
              "score": 9,
              "created_utc": "2026-01-02 14:55:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx9cjfb",
              "author": "PsychologicalWeird",
              "text": "I thought it was this or getting done over arriving home",
              "score": 2,
              "created_utc": "2026-01-02 14:40:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8kdy8",
          "author": "ChigGitty996",
          "text": "The 48gb 3090 is vaporware, at least as far as I know.  Did something change?\n\nThe 48gb 4090D or 2 should be good options if they use normal plugs",
          "score": 15,
          "created_utc": "2026-01-02 11:27:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8zgo7",
              "author": "lly0571",
              "text": "There are 48GB 3090 in GPU rental market, but very rare.\n\nhttps://preview.redd.it/mj2kronduxag1.png?width=1473&format=png&auto=webp&s=8cc1a94b22da84e7bbffd458aaba430f2a87a26b",
              "score": 15,
              "created_utc": "2026-01-02 13:22:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx8lko7",
              "author": "notafakename10",
              "text": "48gb 4090D seems like a great buy though they arent super easy to fine as far as I'm aware.   \n  \nThe 3090 48gb I've only really seen a handful, and those were expensive, which is why its not a front runner",
              "score": 9,
              "created_utc": "2026-01-02 11:38:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8nnim",
                  "author": "durden111111",
                  "text": "I don't think a 48GB 3090 has ever existed. You can solder 2GB chips on a 3090 PCB but it simply wont recognize the extra 24GB",
                  "score": 1,
                  "created_utc": "2026-01-02 11:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx9b8lw",
              "author": "DataGOGO",
              "text": "No, they were real, but they all do the 4090 now",
              "score": 3,
              "created_utc": "2026-01-02 14:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxk7ykz",
              "author": "phido3000",
              "text": "For the cost and the expense the 3090 isn't worth it for the modder.\n\n4090D 48Gb is pretty much perfect, lower power, much better performance, 4000 series features, and 48Gb ram same effort/costs. 4090D gets deployed pretty large systems.",
              "score": 1,
              "created_utc": "2026-01-04 03:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx91eqm",
          "author": "lly0571",
          "text": "2x4080S 32GB or 1x4090D 48GB.\n\n  \nYou can also get A100 40GB(SXM4 to PCIe) at \\~18000CNY, but I would recommend 4090D myself.",
          "score": 8,
          "created_utc": "2026-01-02 13:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx930jo",
              "author": "notafakename10",
              "text": "How rare are the 32gb models? That would be a perfect balance \n\nHave you had any issues with SXM4?",
              "score": 2,
              "created_utc": "2026-01-02 13:44:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9d34w",
                  "author": "lly0571",
                  "text": "I think you can get a 4080S 32GBÂ at 9000-9500CNY(\\~1300USD) at Xianyu, but few local shops have these GPUs(at least in October).\n\nI don't own an A100 myself, I believe 4090 is better for inference due to its FP8 support.",
                  "score": 1,
                  "created_utc": "2026-01-02 14:43:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8pc2q",
          "author": "pharrowking",
          "text": "do you live outside china in western regions normally? because im looking at ebay. some of those prices on your list are at best saving you $200. then if you factor the flight to china and hotel and all that, youre not really saving at that point. i assume those prices are in chinese yuan not japanese yuan. they use the same symbol.\n\nnow if you live there normally those prices are decent. the mi100 is around 900-1100 in canadian dollars when i calculated to exchange to my local dollar. where as on ebay its listed for $1300 cad\n\n5800 yuan is around 829$ USD. the mi100 on ebay is 984$ USD. not a huge difference i guess",
          "score": 9,
          "created_utc": "2026-01-02 12:09:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8qwcx",
              "author": "notafakename10",
              "text": "We're already here! \n\nYeh, I'm assuming those prices can be negotiated (like everything in china) so i'd expect to get below particularly if I'm buying 2x or 4x",
              "score": 2,
              "created_utc": "2026-01-02 12:21:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx93pep",
                  "author": "xrailgun",
                  "text": "In my experience, there's not much room (if any) for negotiation on these, especially towards the lower end of those scales you've compiled. They might round you down Â¥10-Â¥50 if you buy 2 or more. They're moving sufficient volume to price-insensitive customers, there are frequently weeks where some of these are out of stock/pre-allocated entirely. Doesn't hurt to ask though, especially if it's towards the higher half of the scale.",
                  "score": 11,
                  "created_utc": "2026-01-02 13:48:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx96pus",
          "author": "mp3m4k3r",
          "text": "Iirc the Pascal line is getting older/being dropped from upcoming versions for support so while tempting they might be worth weighting against or dropping.",
          "score": 3,
          "created_utc": "2026-01-02 14:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9p3y1",
          "author": "Terrible-Contract298",
          "text": "The Tesla P40 can be modified with a 8+6pin arrangement. A standard 65% PWM cycle on the fan of a standard 1080 TI cooler applied allows sustained operation at lower clocks and a TDP of \\~225W. I can provide definite performance and data regarding the upgrade.",
          "score": 3,
          "created_utc": "2026-01-02 15:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxadg6y",
              "author": "mtbMo",
              "text": "Yes sir. Build my with a 980ti founders cooler. Runs under 80c",
              "score": 2,
              "created_utc": "2026-01-02 17:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbz7mi",
                  "author": "Terrible-Contract298",
                  "text": "Fantastic to hear others having success with this.",
                  "score": 1,
                  "created_utc": "2026-01-02 22:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxb8w54",
          "author": "fallingdowndizzyvr",
          "text": "At your budget, hands down 4090(D) 48GB if the best choice.",
          "score": 3,
          "created_utc": "2026-01-02 20:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcol9m",
          "author": "Disposable110",
          "text": "4080 32GB mod for around 9-10k CNY each caught my eye on Taobao, but beware that it is slower and can run hot.",
          "score": 2,
          "created_utc": "2026-01-03 00:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd4149",
          "author": "1427538609",
          "text": "Can't help with Shenzhen prices specifically, but on the hardware side - dual P40s (48GB) or the modded 3080 20GB x4 route both work for 48GB+. The 3080s will be much faster for inference due to Ampere vs Pascal. For 96GB, you're looking at A100s or multiple cards regardless.",
          "score": 2,
          "created_utc": "2026-01-03 01:59:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9j45y",
          "author": "a_beautiful_rhind",
          "text": "First I hear that AMD has mod. 4080 32g seems like a contender if you can't afford 4090/5xxx gpu.",
          "score": 1,
          "created_utc": "2026-01-02 15:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxa8ytw",
          "author": "jinnyjuice",
          "text": "Would be nice to have some comparisons with Yongsan in Korea also, also for RAM. I wonder if there is such data. Would be amazing if someone is familiar with all the international forums.\n\n>Prices are best estimates from deep seek\n\nThat is very, very unreliable. They're much more often wrong than correct. Manual search/scraping is definitely required and recommended.",
          "score": 1,
          "created_utc": "2026-01-02 17:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl6pd",
              "author": "notafakename10",
              "text": "Agree it would be great to compare - pricing data was a mix of several things, deepseek, forums and my own observations so far, I'd put it as \"directional useful\" not exactly accurate lol",
              "score": 1,
              "created_utc": "2026-01-03 03:41:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa9b22",
          "author": "Mediocre-Waltz6792",
          "text": "Get two cards with the Vram you want. Im currently fighting with my 3rd 3090 as it slowed everything down by 3x. Could be a windows issue but either way two GPU is way easier to setup and run.",
          "score": 1,
          "created_utc": "2026-01-02 17:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdl8qi",
              "author": "notafakename10",
              "text": "Good pointer - thanks for the insight",
              "score": 1,
              "created_utc": "2026-01-03 03:42:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxbvxme",
          "author": "etherd0t",
          "text": "Just out of curiosity... where'd you get that price list?ðŸ™‚ Is there a place where you can check Shenzen bazaar (Huaqiangbei) prices? May visit soon...",
          "score": 1,
          "created_utc": "2026-01-02 21:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdldzm",
              "author": "notafakename10",
              "text": "Pricing data was a mix of several things, deepseek, forums and my own observations so far (from the Shanghai GPU markets), I'd put it as \"directional useful\" not exactly accurate lol\n\nI'll be going to Shenzhen later this month and I'll get some actual on the ground pricing and update",
              "score": 1,
              "created_utc": "2026-01-03 03:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdfjeh",
          "author": "BrightComplaint8342",
          "text": "est price not accurate i think",
          "score": 1,
          "created_utc": "2026-01-03 03:07:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxi69ez",
          "author": "millerlite_11",
          "text": "Any modded 5090?",
          "score": 1,
          "created_utc": "2026-01-03 20:50:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjz521",
              "author": "notafakename10",
              "text": "None Iâ€™ve seen yet",
              "score": 1,
              "created_utc": "2026-01-04 02:28:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxostmg",
          "author": "pbad1",
          "text": "4x 5060ti 16GB?   \n\\~$2.5k",
          "score": 1,
          "created_utc": "2026-01-04 20:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrknrj",
              "author": "notafakename10",
              "text": "Not a bad option but im trying to maximise expandability in the future, if I can get 64gb in two cards thats ideal",
              "score": 1,
              "created_utc": "2026-01-05 04:58:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxw9z41",
          "author": "Irisi11111",
          "text": "What are your specific needs? Are you focused on text processing or vision tasks? If it's the latter, I highly recommend getting the latest CUDA-supported graphics cards. If that's not the case for you, AMD solutions might be a good option.",
          "score": 1,
          "created_utc": "2026-01-05 21:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4kwm",
              "author": "notafakename10",
              "text": "I do a bit of everything but mostly training transformers in PyTorch at the minute and local LLMs",
              "score": 1,
              "created_utc": "2026-01-06 08:35:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1epp0",
                  "author": "Irisi11111",
                  "text": "It seems you're mainly addressing text tokens. Are you able to safely pursue a ROCm card for maximum VRAM? That way, you'd get the most use out of utilities right now.",
                  "score": 1,
                  "created_utc": "2026-01-06 17:13:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx9l8a4",
          "author": "pCute_SC2",
          "text": "Wait there is a RTX 3090 48GB mod????? Where can I get it?",
          "score": 1,
          "created_utc": "2026-01-02 15:25:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa7emj",
              "author": "sillynoobhorse",
              "text": "Alibaba, TaoBao etc., be prepared for the chinese way of dealing which includes many chat and Whatsapp messages lol",
              "score": 1,
              "created_utc": "2026-01-02 17:10:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxa7zds",
                  "author": "pCute_SC2",
                  "text": "Didn't found it on Alibaba.",
                  "score": 2,
                  "created_utc": "2026-01-02 17:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxdl04d",
              "author": "notafakename10",
              "text": "They are hard to come by now as all most of the modders have moved on to 4xxx series cards, but they do float around occasionally in the markets themselves, won't come up on Alibaba!",
              "score": 1,
              "created_utc": "2026-01-03 03:40:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxa4sws",
          "author": "grzesi00",
          "text": "7900 xt has 24gb vram stock so whats modded there?",
          "score": 1,
          "created_utc": "2026-01-02 16:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxad1dk",
              "author": "mftrhu",
              "text": "The 7900XTX has 24 GB VRAM. The 7900XT sits at 20 GB.",
              "score": 4,
              "created_utc": "2026-01-02 17:37:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7mvuf",
      "title": "Z.ai  (the AI lab behind GLM) has officially IPO'd on the Hong Kong Stock Exchange",
      "subreddit": "LocalLLaMA",
      "url": "https://x.com/Zai_org/status/2009290783678239032",
      "author": "Old-School8916",
      "created_utc": "2026-01-08 20:23:59",
      "score": 258,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7mvuf/zai_the_ai_lab_behind_glm_has_officially_ipod_on/",
      "domain": "x.com",
      "is_self": false,
      "comments": [
        {
          "id": "nygtope",
          "author": "ForsookComparison",
          "text": "Hopefully they're all partying it up.\n\nAnd hopefully their new shareholders don't mind it if they spend millions in compute to give us free stuff ðŸ˜¬",
          "score": 37,
          "created_utc": "2026-01-08 20:37:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhjsb0",
              "author": "procgen",
              "text": "and so the enshittification begins",
              "score": 20,
              "created_utc": "2026-01-08 22:32:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyjaik6",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2026-01-09 04:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nygwihp",
          "author": "_Sneaky_Bastard_",
          "text": "they also said GLM 5 is in training. hoping it would be a open weight release.",
          "score": 60,
          "created_utc": "2026-01-08 20:49:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygz35p",
              "author": "misterflyer",
              "text": "I think u/ForsookComparison 's half-joking comment is more fit. Once shareholders and big money are involved, they are not the same company that produced 4.5, 4.6, and 4.7 whatsoever.\n\nWe're gonna have to start looking at them as a different company.  Does that mean that they won't release anymore open weights models? *No, prob not.*  But we cannot expect them to operate the same as before... tossing out 4.5, 4.6, and 4.7 like Oprah... *\"You get a model, and you get a model, and You Get a Model, AND YOU GET A MODEL!!!\"*\n\n**They have to be able to give an ROI to their investors somehow.  And I doubt that's going to come from perpetual open weight releases.**",
              "score": 48,
              "created_utc": "2026-01-08 21:01:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyh165s",
                  "author": "snoodoodlesrevived",
                  "text": "I think that the amount of users hosting these models locally is too small to matter",
                  "score": 13,
                  "created_utc": "2026-01-08 21:10:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyh80ru",
                  "author": "danigoncalves",
                  "text": "I pay 30 dolares per year to use their flagship model. I cannot afford GPUs to run it locally so I guess they can continue to count on me as their client. Also there many companies/products where privacy is not that important because of the kind of data that might be feed to the model. I want to think that they will still BE commited with open source community",
                  "score": 4,
                  "created_utc": "2026-01-08 21:40:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyhk1yx",
                  "author": "BERLAUR",
                  "text": "At this stage investors don't care about revenue, they care about potential and future growth.Â \n\n\nThey'll have to make some money eventually but I wouldn't be worried about this as long as the AI hype is ongoing.Â ",
                  "score": 1,
                  "created_utc": "2026-01-08 22:33:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyjzv79",
                  "author": "Hobofan94",
                  "text": "> Once shareholders and big money are involved\n\nYou think they weren't up until now?",
                  "score": 0,
                  "created_utc": "2026-01-09 07:00:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyneek9",
                  "author": "DarkArtsMastery",
                  "text": "Spoken like a true NPC.",
                  "score": -1,
                  "created_utc": "2026-01-09 19:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nygvbc0",
          "author": "TheAncientPizza711",
          "text": "They issued shares at HK$116.20 each. Opened at HK$120 and is now currently HK$131.50.\n\nStock is up 13.17% on its 1st day. Not bad.",
          "score": 34,
          "created_utc": "2026-01-08 20:44:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhxdk2",
              "author": "robogame_dev",
              "text": "The initial price is arbitrary, if you want it to go up on day 1 price low, if you want it to go down on day 1 price high - it doesnâ€™t indicate anything about the performance of the stock, only the choice of the initial price. Normally theyâ€™ll try to estimate the market price and issue a bit below, but if they estimate wrong you get a big day 1 movement.\n\nNow the market has set the price, $131.50, movement up or down from there is meaningful - but the movement from the first issue price isnâ€™t really.",
              "score": 10,
              "created_utc": "2026-01-08 23:39:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyj7i7o",
              "author": "Hrethric",
              "text": "Minimum order 100 shares. I tried to buy 20 on a lark, but I don't have US$2000 in confidence in them. (Nor, if I'm being honest, do I have US$2000 I can reasonably gamble lol.)",
              "score": 3,
              "created_utc": "2026-01-09 03:44:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyqejpw",
              "author": "One-Employment3759",
              "text": "Not bad, but not good.\n\n\nBubble pop",
              "score": 1,
              "created_utc": "2026-01-10 04:37:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyh2044",
          "author": "FullOf_Bad_Ideas",
          "text": "Minimax IPOs a day later, 9th of January.\n\nLots of info about both of them can be found here:\n\n[Zhipu offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1230/2025123000017.pdf)\n\n[Minimax offering](https://www1.hkexnews.hk/listedco/listconews/sehk/2025/1231/2025123100025.pdf)\n\nSource: [this website](https://www.hkex.com.hk/Services/Trading/Securities/Trading-News/Newly-Listed-Securities?sc_lang=en)\n\nOne of the very interesting thing contained there is that Zhipu has slightly negative profit margin on GLM Coding Plan. They lose money on serving alone, not even counting in marketing or R&D costs.",
          "score": 12,
          "created_utc": "2026-01-08 21:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhdm9r",
              "author": "tens919382",
              "text": "Huge reason could be the insane discounts that they are offering.  I got a 1 yr plan at 60% off, and referral got 30-40% of the remaining value in credits.  So bout 75-80% off total.",
              "score": 7,
              "created_utc": "2026-01-08 22:04:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhifgk",
                  "author": "FullOf_Bad_Ideas",
                  "text": "that too.\n\nI think submitted data captures things only for H1 2025, so it probably got even worse for them later.\n\nI think they have shaky revenue sources.\n\nMain customer seems to be Chinese government with on-premise deployment for national security. And their cloud business have very bad margins. They don't have big general consumer platforms like MiniMax has with their roleplaying site for teens.",
                  "score": 3,
                  "created_utc": "2026-01-08 22:26:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyh1ztw",
          "author": "jacek2023",
          "text": "Hype hype and still no Air",
          "score": 14,
          "created_utc": "2026-01-08 21:14:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh3xbg",
              "author": "FullOf_Bad_Ideas",
              "text": "They barely mentioned it in their own filing\n\nAnd when they did mention it, they made a typo and called it GLM-4-Air instead of GLM-4.5-Air.\n\nIt looks like they want to focus on core offering that is already cheap enough for most.",
              "score": 9,
              "created_utc": "2026-01-08 21:22:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nygrahs",
          "author": "rookan",
          "text": "Good for them",
          "score": 0,
          "created_utc": "2026-01-08 20:26:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqeg0y",
          "author": "One-Employment3759",
          "text": "Gotta get the public money before the bubble pop",
          "score": 1,
          "created_utc": "2026-01-10 04:36:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyao6g",
      "title": "Meta released RPG, a research plan generation dataset on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/datasets/facebook/research-plan-gen",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 02:58:09",
      "score": 258,
      "num_comments": 21,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwhk8eb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 04:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhjva1",
          "author": "LoveMind_AI",
          "text": "Meta is humiliating OpenAI in terms of research and open source contributions. I have a feeling the days of open frontier models are over, but theyâ€™re still doing a lot.",
          "score": 99,
          "created_utc": "2025-12-29 03:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlg3m",
              "author": "TheRealMasonMac",
              "text": "Chinese labs probably appreciate the free research. Especially since this one comes with evaluation criteria so they can RL on it.",
              "score": 36,
              "created_utc": "2025-12-29 04:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlyyd",
                  "author": "Southern-Chain-6485",
                  "text": "Welcome to science",
                  "score": 62,
                  "created_utc": "2025-12-29 04:11:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi95f9",
              "author": "eat_my_ass_n_balls",
              "text": "Sorta, but their models have fallen off",
              "score": 1,
              "created_utc": "2025-12-29 07:02:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhnv9t",
          "author": "Any-Conference1005",
          "text": "Acronym collision.......",
          "score": 36,
          "created_utc": "2025-12-29 04:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhpalv",
              "author": "HistorianPotential48",
              "text": "can't wait for coming up HGAME dataset, FEMBOY datasets from meta",
              "score": 34,
              "created_utc": "2025-12-29 04:32:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi8967",
              "author": "FaceDeer",
              "text": "I really need to train an LLM for some serious hardcore RPG, and I keep finding plenty of datasets that claim that they're for this purpose. But the LLMs keep turning out wrong! Every time I demo for my supervisor... honestly, I have no idea why my funding hasn't been pulled, or why he keeps the resulting models. They're useless.",
              "score": 7,
              "created_utc": "2025-12-29 06:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhb3i5",
          "author": "segmond",
          "text": "Would be nice if folks release dataset with models trained on it.",
          "score": 15,
          "created_utc": "2025-12-29 03:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhiam9",
              "author": "Accomplished_Ad9530",
              "text": "They cite their unreleased paper, â€œTraining AI Co-Scientists using Rubric Rewardsâ€ so I wouldnâ€™t be surprised if they release a model at some point.",
              "score": 17,
              "created_utc": "2025-12-29 03:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi9g9x",
          "author": "JudgmentPale458",
          "text": "Interesting release. Research plan generation feels like a subtle but important capability â€” especially for agentic or tool-using systems where planning quality matters more than final answer fluency.\n\nCurious how this dataset handles evaluation: are plans judged mainly on structure/coverage, or is there any signal about feasibility and downstream execution success? That distinction seems critical if this is used to train agents rather than just planners.",
          "score": 6,
          "created_utc": "2025-12-29 07:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkbh1n",
          "author": "martinerous",
          "text": "Great, now waiting what they will make out of MMORPG.",
          "score": 1,
          "created_utc": "2025-12-29 16:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk07z",
          "author": "stealthagents",
          "text": "This dataset sounds like a game changer for streamlining research. Having those evaluation rubrics and reference solutions will save a ton of time for any AI training. Can't wait to see what kind of projects come out of this!",
          "score": 1,
          "created_utc": "2025-12-30 17:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhs7",
          "author": "Brenan-Caro",
          "text": "Research Plan Gen",
          "score": 1,
          "created_utc": "2025-12-31 06:54:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2ub3",
          "author": "serendipity777321",
          "text": "What is this for? Not one single explanation",
          "score": 2,
          "created_utc": "2025-12-29 06:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi7cer",
              "author": "Odd-Ordinary-5922",
              "text": "22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training **AI co-scientists**",
              "score": 13,
              "created_utc": "2025-12-29 06:46:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwigkvv",
                  "author": "serendipity777321",
                  "text": "You must be joking",
                  "score": -4,
                  "created_utc": "2025-12-29 08:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiquf6",
              "author": "know-your-enemy-92",
              "text": "Taking science back to the times of alchemy from middle ages.Â ",
              "score": 2,
              "created_utc": "2025-12-29 09:46:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q06ddc",
      "title": "Update on the Llama 3.3 8B situation",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "author": "FizzarolliAI",
      "created_utc": "2025-12-31 06:45:42",
      "score": 256,
      "num_comments": 22,
      "upvote_ratio": 0.93,
      "text": "Hello! You may remember me as either\n\n- The person [who recently uploaded L3.3 8B's weights to Huggingface](https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/) (see this post for more context)\n- That stupid bitch\n\nand I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by u/Few-Welcome3297.\n\nThe main benchmark table from the model README has been updated:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (original 8k config) | Llama 3.3 8B Instruct (128k config)\n|-|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95|**84.775**\n|GPQA Diamond (3 epochs)|29.3|37.0|**37.5**\n\nWhile I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also *serves* the weights with the original L3 config and 8k context, I have absolutely no idea!\n\nAnyways, if you want to try the model, I would recommend trying both the [128k version](https://huggingface.co/shb777/Llama-3.3-8B-Instruct), as well as my [original version](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct) if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nEdit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwvmrda",
          "author": "toothpastespiders",
          "text": ">I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...\n\nHonestly, I think I prefer it this way. The llama saga began with some public shenanigans with a semi-leak. Seems appropriate in a way that if it has to end, and it does seem to be the case, that everything was capped off by something like this.",
          "score": 89,
          "created_utc": "2025-12-31 07:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvl06q",
          "author": "Kahvana",
          "text": "No need to degrade yourself, you're doing fantastic work.\n\nThank you for the release!",
          "score": 112,
          "created_utc": "2025-12-31 07:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6ghg",
              "author": "MoffKalast",
              "text": "Yeah OP, run yourself at at least Q6 ;)",
              "score": 21,
              "created_utc": "2025-12-31 10:28:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvt29i",
          "author": "datbackup",
          "text": "Lol upvoted for humor\n\nGood stuff, I might try this 3.3, it has actually been months since iâ€™ve run any llama model.",
          "score": 23,
          "created_utc": "2025-12-31 08:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvp8bu",
          "author": "pmttyji",
          "text": "Thanks for this. Still didn't download original version due to less context thing. I'm gonna try this 128K version this week. Also waiting for feedback from others on this version. Just expecting to replace 3.1 8B with this version.",
          "score": 8,
          "created_utc": "2025-12-31 07:45:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwkxbj",
              "author": "Few-Welcome3297",
              "text": "Very small improvement, but its something",
              "score": 5,
              "created_utc": "2025-12-31 12:34:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwkrkr",
          "author": "Few-Welcome3297",
          "text": "Some evals [https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals](https://huggingface.co/datasets/shb777/Llama-3.3-8B-Instruct-128K-Evals) . TLDR: Small Improvement\n\nEdit: Link updated",
          "score": 10,
          "created_utc": "2025-12-31 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvmlmo",
          "author": "jacek2023",
          "text": "Would be nice to put some info into model's name to distinguish them",
          "score": 14,
          "created_utc": "2025-12-31 07:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvnt4n",
              "author": "FizzarolliAI",
              "text": "I would, but since quants and all have already been made under the original model's name, it's kinda too late :p",
              "score": 13,
              "created_utc": "2025-12-31 07:32:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwdex2",
                  "author": "Awwtifishal",
                  "text": "The new one could be renamed to add -128K or something so the quants also reflect it.",
                  "score": 7,
                  "created_utc": "2025-12-31 11:32:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwcrfv",
          "author": "Cool-Chemical-5629",
          "text": "* That stupid bitch\n\nI need more context, please. ðŸ¤£\n\n  \nIn any case, I tried the extended version yesterday and while it felt pretty weak for stuff like coding etc. it seemed to be a decent base model for E/RP finetunes, because it followed instructions fairly well, but it was HORRIBLY SLOW burning so it would need some nudging from E/RP datasets to keep the story going. I hope E/RP creators will pick it up (and Ministral 14B Instruct too while at it).",
          "score": 11,
          "created_utc": "2025-12-31 11:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxs7tf",
          "author": "randomfoo2",
          "text": "Just in case anyone's interested, I ran [shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) on the Shisa AI's MultiEval on my dev box. \n\nOn the English side, it loses a bit on MixEval Easy and Hard (2024 Chat Arena proxy), but gets a +20% boost in LiveBench (reasoning-focused), +15% GPQA Diamond (PhD level QA), +5% on IFEval, +30% on IFBench (!) and +10% on HumanEval+ (Python). That's some decent gains.\n\nThat being said, on the Japanese side, it takes a big hit on Shaberi (Japanese chat-style functional tests) vs 3.1. I've included my Llama 3.1 8B-based Shisa V2  and Qwen 3 8B-based Shisa V2.1 as well as Llama 3.3 70B and Llama 3.1 405B scores just for comparison, sake.\n\n(I probably wont train a Shisa V2.1 Llama 3.3 8B - the Qwen 3 8B version is already great and it's Apache 2.0 licensed).\n\nhttps://preview.redd.it/6hnc71dggkag1.png?width=3126&format=png&auto=webp&s=10c4dd3231be1ed9749f174c59e8758c134d0009",
          "score": 5,
          "created_utc": "2025-12-31 16:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy05pp",
              "author": "FizzarolliAI",
              "text": "Interesting, I wonder if you'd get a noticeable regression from L3.3 70B on multilingual benches with Llama 3.1 70B then.\n\nI definitely agree that I don't think this is worth building on for most usecases. Personally I think it's an interesting artifact of the times",
              "score": 2,
              "created_utc": "2025-12-31 17:19:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwy8qrh",
                  "author": "randomfoo2",
                  "text": "Hmm, hard to say, I don't have 3.1 70B data handy... 3.3B 70B is in general pretty strong.\n\nIn practical terms, your ultimate multilingual perf is going to be pretty much up to you (tuning).  While the overall number isn't so big, when you look at the stuff we care about like JP IF, JP RP, JP TL, JP nuance, dialogue translation, we're able to get huge boosts from doing training on top of whatever model. Not show nis also our own CLTL tests that test for how many wrong-language tokens get output (huge amounts for most non-target language trained models).\n\nThe benchmark mix we use for our current multieval does feel about right. For the tasks that it's trained on, our V2.1 14B model actually \\*does\\* feel like it outperforms our V2 70B (and sometimes our V2.1 70B and V2 405B even!).\n\n\n\nhttps://preview.redd.it/hpf7vf50vkag1.png?width=3123&format=png&auto=webp&s=b5ea8273adbb61f19412f497ff3d2d06e4f46aed",
                  "score": 1,
                  "created_utc": "2025-12-31 18:01:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvndin",
          "author": "Amazing_Athlete_2265",
          "text": "Interesting. I'll evaluate it and compare",
          "score": 3,
          "created_utc": "2025-12-31 07:28:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvpi7i",
              "author": "pmttyji",
              "text": "Awesome",
              "score": 0,
              "created_utc": "2025-12-31 07:48:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nww92bu",
          "author": "ilintar",
          "text": "Is the 128k version just a x16 YaRN extension or a different model?",
          "score": 3,
          "created_utc": "2025-12-31 10:53:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwdgf2",
              "author": "Awwtifishal",
              "text": "Just a config change",
              "score": 5,
              "created_utc": "2025-12-31 11:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxcw2i",
          "author": "noctrex",
          "text": "Generated some abliterated gguf's for it:\n\n[https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF](https://huggingface.co/noctrex/Llama-3.3-8B-Instruct-128k-abliterated-GGUF)",
          "score": 3,
          "created_utc": "2025-12-31 15:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0u60h",
          "author": "Dangerous_Fix_5526",
          "text": "Thinking/Instruct Hybrid using Unsloth and Claude-Opus 4.6 dataset:\n\n[https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning](https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning)\n\nI hope I credited everyone correctly.",
          "score": 3,
          "created_utc": "2026-01-01 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwxu4p",
          "author": "Robert__Sinclair",
          "text": "I just tested it and its reasoning is extremely lacking.",
          "score": -4,
          "created_utc": "2025-12-31 14:00:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q694ic",
      "title": "Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/",
      "author": "Eisenstein",
      "created_utc": "2026-01-07 07:32:28",
      "score": 250,
      "num_comments": 234,
      "upvote_ratio": 0.81,
      "text": "In case you thought it was going to get better:\n\n**GPU** prices are going up. [AMD and NVIDIA are planning to increase prices every month starting soon.](https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/)\n \n**NAND flash** contract price [went up 20% in November](https://www.trendforce.com/price/flash/flash_contract), with [further increases in December] (https://www.trendforce.com/research/download/RP251231KM). This means SSDs will be a lot more expensive soon.\n\n\n**DRAM** [prices are going to skyrocket](https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/), with no increase in production capacity and datacenters and OEMs competing for everything.  \n\nEven **Consoles** are [going to be delayed due to the shortages.](https://insider-gaming.com/ram-prices-next-gen/)\n\n> According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55â€“60% quarter over quarter, while server DRAM prices are projected to surge by more than 60% QoQ. Meanwhile, NAND Flash prices are expected to increase 33â€“38% QoQ\n\n[Source.](https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/)\n\n> Industry sources cited by Kbench believe the latest price hikes will broadly affect NVIDIAâ€™s RTX 50 series and AMDâ€™s Radeon RX 9000 lineup. The outlet adds that NVIDIAâ€™s flagship GeForce RTX 5090 could see its price climb to as high as $5,000 later in 2026.\n\n>NVIDIA is also reportedly weighing a 30% to 40% reduction in output for parts of its midrange lineup, including the RTX 5070 and RTX 5060 Ti, according to Kbench.\n\n[Source.](https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "ny5wcqs",
          "author": "ifupred",
          "text": "Not going to purchase at all for 3-4 years",
          "score": 432,
          "created_utc": "2026-01-07 07:48:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5xy01",
              "author": "know-your-enemy-92",
              "text": "Same, let's see what happens with smaller models and optimization. Open weights and open source will prevail in the end.",
              "score": 113,
              "created_utc": "2026-01-07 08:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6jort",
                  "author": "Mashic",
                  "text": "And then end up buying the server hardware when they don't need it anymore.",
                  "score": 42,
                  "created_utc": "2026-01-07 11:20:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7zecu",
                  "author": "ashirviskas",
                  "text": "Training a random 9M (yes, M, not B) model just for fun on my laptop without a GPU atm. It is already nearly coherent! And it is not really even supposed to do language yet.\n\nIts basically like BLT, but should be pluggable onto any model. With minimal finetuning should allow any model to talk in Byte Patches. \n\nPros: Less tokens, potentially much faster. Like 2x-8x faster. Could also unlock reasoning in latent space and allow for faster solution convergence (imagine 1 meaning-rich token instead of 12 `\"wait\"; \", \"; \"and \"; ..`. Just like to think about `small, young orange cat` you don't need 5 separate tokens in your head, only the *idea* of a small, young orange cat), but I have not focused there yet. \n\nCons: Needs a bit of finetuning of the og model. Thankfully, only a few layers should theoretically be enough.",
                  "score": 6,
                  "created_utc": "2026-01-07 16:13:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6doqd",
                  "author": "ANR2ME",
                  "text": "And these optimizations will most likely go towards Blackwell-only features ðŸ˜…",
                  "score": 19,
                  "created_utc": "2026-01-07 10:28:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny82fjw",
                  "author": "Shot_Court6370",
                  "text": "Open Source will have to flourish or everyone will be using Chinese models that don't acknowledge factual historical events.",
                  "score": -2,
                  "created_utc": "2026-01-07 16:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6b0wm",
              "author": "Any_Pressure4251",
              "text": "The shortage will not last that long, this has happened many times the market will adjust.",
              "score": 39,
              "created_utc": "2026-01-07 10:04:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6l40s",
                  "author": "confusedp",
                  "text": "Agree. After things are in HVM, margins for chip production is 90% plus. Things can't be this crazy for too long. The mantra of \"your margin is my opportunity\" is going to come and wipe out that margin. Companies themselves will be better off with much higher volume and smaller margins. Ramp up time for new fabs with an existing working stack is about 2 years. I expect this thing to normalize in 3-4 years. And price coming down in 5-6 years",
                  "score": 26,
                  "created_utc": "2026-01-07 11:31:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7jh2a",
                  "author": "Freonr2",
                  "text": "At least on the DRAM front, Micron has several new fabs in the works.  I think one or two of these may come online this calendar year?  Any price relief might take a bit longer but some sunshine at the end of the rainbow maybe for 2027.\n\nhttps://www.micron.com/us-expansion",
                  "score": 3,
                  "created_utc": "2026-01-07 14:58:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyd9w58",
                  "author": "mc_nu1ll",
                  "text": "the issue is that the high prices remained high even after said \"readjustments\": do you remember what happened after the crypto crash? The GPUs kept getting more and more expensive, while scalpers drove the prices even higher. \n\nI know this is naive, but I hope this won't be the case now..",
                  "score": 2,
                  "created_utc": "2026-01-08 09:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6odmr",
              "author": "ravensholt",
              "text": "THIS!  \nThis is the only right thing to do.\n\nDon't listen to these bots who keep screaming that prices are going up and at the same time encourage to spend money now!",
              "score": 27,
              "created_utc": "2026-01-07 11:56:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny76f6m",
                  "author": "Expensive-Paint-9490",
                  "text": "u/Eisenstein is an OG and a very respected member of the community.",
                  "score": 8,
                  "created_utc": "2026-01-07 13:50:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfoz7t",
                  "author": "Cheap_Image_5113",
                  "text": "I don't expect them go up substantially any more, there will be a minor price correction before end of the year and then will see the new baseline price still about 1.8x higher than it was pre AI boon.",
                  "score": 1,
                  "created_utc": "2026-01-08 17:39:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny7wqri",
                  "author": "Firm-Fix-5946",
                  "text": "right thing to do?\n\nis this somehow a moral issue to you?",
                  "score": -1,
                  "created_utc": "2026-01-07 16:01:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7weet",
              "author": "pwnrzero",
              "text": "My 2070s will have to hold out.",
              "score": 4,
              "created_utc": "2026-01-07 15:59:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny7x3cx",
              "author": "grannyte",
              "text": "Just completed all my builds that should tide me over for the next 4 years. Hopefully nothing I don't have replacement parts for dies in he next 4 years.",
              "score": 5,
              "created_utc": "2026-01-07 16:03:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny63t9p",
              "author": "Samurai2107",
              "text": "Three to four years is hopefully how long it will take for all the newcomers (mostly chinese gpu makers) to catch up to nvidia ðŸ¤žðŸ¼",
              "score": 17,
              "created_utc": "2026-01-07 08:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6riym",
                  "author": "Mochila-Mochila",
                  "text": "That's a very optimistic timeframe. It'd put it closer to 10 years, in the consumer GPU department at least.\n\nBut they should release useable, \"good enough\" products before that.",
                  "score": 6,
                  "created_utc": "2026-01-07 12:19:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny90tgf",
              "author": "taoyx",
              "text": "When all these data centers are built they will come back to us.",
              "score": 1,
              "created_utc": "2026-01-07 18:59:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny90yrd",
              "author": "GraybeardTheIrate",
              "text": "Yeah I'm not playing this game. I'll buy used or not at all rather than feed into it.",
              "score": 1,
              "created_utc": "2026-01-07 18:59:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyfj1fj",
              "author": "RogerRamjet999",
              "text": "I agree 100 percent.  So sick of them scalping us every chance they get.  I'm buying nothing unless it's inline with prevailing prices 3 months ago.  As far as I'm concerned they can choke on their RAM (and hard disks, SSDs, GPUs, etc, etc) unless they price them fairly.  I can wait longer than they can.",
              "score": 1,
              "created_utc": "2026-01-08 17:13:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nygzxep",
              "author": "DeltaSqueezer",
              "text": "Yeah. I feel like I'm being forced to make a gamble. Do I buy now and pay high prices, or do I risk having to buy in the next year or two when prices may be even higher?",
              "score": 1,
              "created_utc": "2026-01-08 21:04:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny65q72",
              "author": "Terrible_Scar",
              "text": "You're doing yourself in 3-4 years. It wouldn't take a year before hardware gets priced out from the common folk",
              "score": 2,
              "created_utc": "2026-01-07 09:14:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6bomv",
                  "author": "ifupred",
                  "text": "Your not wrong. But if it's priced out for someone with my earnings it's basically end of personal computers as we know it. I hate it already",
                  "score": 24,
                  "created_utc": "2026-01-07 10:10:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6478k",
              "author": "ieatdownvotes4food",
              "text": "hell yeah, baton down the hatches.. endgame time",
              "score": -2,
              "created_utc": "2026-01-07 09:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny63t62",
          "author": "HornyGooner4401",
          "text": "You're like 6 months late",
          "score": 170,
          "created_utc": "2026-01-07 08:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6g087",
              "author": "Ok-Bill3318",
              "text": "Itâ€™s going to get much worse from here. Itâ€™s not too late to buy now if you need anything in the next 6-24 months or so",
              "score": -37,
              "created_utc": "2026-01-07 10:48:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pegc",
                  "author": "ravensholt",
                  "text": "So let the companies suffer and let's wait 36-72 months instead... Let's wait as long as it takes.  \nI went from a 2080TI to 7900XTX.\n\nThere are basically two scenarios here:\n\nCompanies do a 180, and figure out people are smart and unwilling to blindly invest as if having a graphics card is like subscribing to f\\*cking netflix.\n\nor...\n\nIt's the end of personal computing as we now it.  \nFine, we accept it and we still saved money for 3-4 years instead of boosting the \"AI bubble\".",
                  "score": 25,
                  "created_utc": "2026-01-07 12:03:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5w205",
          "author": "DeltaSqueezer",
          "text": "They are already expensive. Prices are currently 3x for SSD what I paid in the middle of last year. DRAM is 4x.",
          "score": 103,
          "created_utc": "2026-01-07 07:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny7it97",
              "author": "mycall",
              "text": "2026: Hold my silicon.",
              "score": 18,
              "created_utc": "2026-01-07 14:55:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny5whu7",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -10,
              "created_utc": "2026-01-07 07:49:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5y4b0",
                  "author": "know-your-enemy-92",
                  "text": "How about 4 years?",
                  "score": 11,
                  "created_utc": "2026-01-07 08:04:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6g22i",
              "author": "Ok-Bill3318",
              "text": "The full impact of this ai demand hasnâ€™t fully hit yet.",
              "score": -12,
              "created_utc": "2026-01-07 10:49:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pk3t",
                  "author": "ravensholt",
                  "text": "Let me guess, you bought stock options in nGreedia, right?",
                  "score": 5,
                  "created_utc": "2026-01-07 12:04:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6697c",
          "author": "Dorkits",
          "text": "Me looking at my PC : bro don't die please",
          "score": 80,
          "created_utc": "2026-01-07 09:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6vf3n",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-07 12:45:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6ztdm",
                  "author": "RateRoutine2268",
                  "text": "i undervolted to a level where my PC is now generating power",
                  "score": 7,
                  "created_utc": "2026-01-07 13:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyq4kkp",
              "author": "LiveToBeFreee",
              "text": "Haha this is what I've been thinking, I upgraded my desktop to AM5 with 64GB of DDR5 almost two years ago, now I'm almost afraid to use it much anymore, because if my memory kicks the bucket, I'm screwed.",
              "score": 1,
              "created_utc": "2026-01-10 03:34:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny70o80",
              "author": "AnomalyNexus",
              "text": "My 3090 trips PSU protection if I donâ€™t powerlimit it. Feels like it is half to gpu Heaven already but man would suck to buy a 3090 replacement rn. No good moves from there that arenâ€™t stupidly expensive or a vram downgrade",
              "score": -2,
              "created_utc": "2026-01-07 13:17:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7gu3b",
                  "author": "wadrasil",
                  "text": "You need a better PSU.",
                  "score": 13,
                  "created_utc": "2026-01-07 14:45:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6appy",
          "author": "FullOf_Bad_Ideas",
          "text": "What will happen to Mac Studio 512GB, RTX 6000 Pro and Ryzen 395+ AI Max 128GB prices?\n\nEdit: this source is BS. Memory chips don't make 80% of BOM, no way.",
          "score": 24,
          "created_utc": "2026-01-07 10:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6hxdp",
          "author": "a_beautiful_rhind",
          "text": "They already surged. I was looking for more DDR4 and it went through the roof. CPUs not any cheaper, motherboads not any cheaper. If anything I would replace my 2080ti with another 3090, but they are still $700. \n\nI thought that I'd eventually want some ada/blackwell. None of those prices have been affordable for >20gb, even before. Only the 32gb super looked \"good\" around $1000. Since middle of the year I'm fucked by tariffs.\n\nStorage already went up. $60 2TB ssd dried up. All that was left from middle 2025 was getting a controller card and SaS drives. Was forced to buy last expansion drive as spinning rust. \n\nFinally, more of my income is going to taxes and bills so I can't really blow money on LLM stuff.",
          "score": 26,
          "created_utc": "2026-01-07 11:05:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6xovq",
              "author": "Sufficient-Past-9722",
              "text": "Yeah I just bought a 4TB T700 for \"only\" $450 and feel somehow lucky.",
              "score": 2,
              "created_utc": "2026-01-07 12:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7q2tm",
                  "author": "Fit_Case_03",
                  "text": "Holy shit. I just spent $900 on four SN 850X 4TB SSD and I felt that was a rip off at the time. That was 2 months ago as well....",
                  "score": 3,
                  "created_utc": "2026-01-07 15:30:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyl55gr",
                  "author": "NoImplement2856",
                  "text": "You overpaid.",
                  "score": 0,
                  "created_utc": "2026-01-09 12:47:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7ghkw",
              "author": "Mkengine",
              "text": "I bought 3x MI50s, 64 GB RAM and 1 TB SSD for around $550 wenn they were cheapest, but have them still laying around. I have to take the time to finally build this server before the other stuff gets too expensive as well...",
              "score": 2,
              "created_utc": "2026-01-07 14:43:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6nyag",
          "author": "ab2377",
          "text": "it's pretty crazy that just a few people have got so much money that they can decide to consume whole planets worth of computing resources.",
          "score": 25,
          "created_utc": "2026-01-07 11:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd70fy",
              "author": "BinaryLoopInPlace",
              "text": "Reddit's cartoonish mental model of how the world works will never cease to amuse me",
              "score": -3,
              "created_utc": "2026-01-08 08:46:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydtwer",
                  "author": "DRM_is_Hell",
                  "text": "He made a factual statement. What are you talking about?",
                  "score": 6,
                  "created_utc": "2026-01-08 12:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6c75j",
          "author": "RnRau",
          "text": "I think the OP is working for Micron or one of the big electronic retailers :)",
          "score": 21,
          "created_utc": "2026-01-07 10:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6d4v8",
          "author": "StandardLovers",
          "text": "I would never have guessed that my best investment ever would be for â€™128GB Kingston 6000 cl36 DDR5 Ramâ€™ bought for ca. 450usd in spring 2025.",
          "score": 19,
          "created_utc": "2026-01-07 10:23:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny72i25",
              "author": "lordofblack23",
              "text": "I bought 128Gb of ddr4 in 2024 for $275 and i felt like a rube.  My Google stock still outperformed.",
              "score": 7,
              "created_utc": "2026-01-07 13:28:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5zyfd",
          "author": "Free-Internet1981",
          "text": "Nice try huge corporations before the bubble bursts, i've seen the data, you guys are plateauing ðŸ˜’",
          "score": 70,
          "created_utc": "2026-01-07 08:20:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6346k",
              "author": "eidrag",
              "text": "OP actually need to offload his hardware order stock, nice try /scalper",
              "score": 28,
              "created_utc": "2026-01-07 08:50:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6pn78",
                  "author": "ravensholt",
                  "text": "OP has stock options in nGreedia , or as you say, is a scalper.",
                  "score": 3,
                  "created_utc": "2026-01-07 12:05:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7avmm",
              "author": "nixed9",
              "text": "They arenâ€™t going anywhere because they are backed by the United States government through military contracts\n\nThis is the *beginning* of the new normal, not the end of it. \n\nAll compute will go to the AI systems that enslave us. Local compute will become less and less available and more expensive.",
              "score": 5,
              "created_utc": "2026-01-07 14:14:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5vcbi",
          "author": "rnyaoyao",
          "text": "The surge in memory and storage prices is purely because OpenAI overâ€‘purchased 40% of wafer capacity just to secure Apple's order. Now it has failed, and the result is obvious.",
          "score": 54,
          "created_utc": "2026-01-07 07:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5yn15",
              "author": "f1rn",
              "text": "I knew OpenAi went in some kind of panic mode. But it was against Apple? I thought Google with the TPUs? \nAnd why has it failed? Feels like Iâ€™m out of the loop here.",
              "score": 28,
              "created_utc": "2026-01-07 08:08:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5zlyp",
                  "author": "SpeedOfSound343",
                  "text": "I think op is speculating that OpenAI had their eyes on the Apple order for Siri that went to Google.",
                  "score": 46,
                  "created_utc": "2026-01-07 08:17:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykgc3x",
                  "author": "mrjackspade",
                  "text": "> But it was against Apple?\n\nIt can be for whatever reason you want it to be as long as you're willing to make shit up on the internet ðŸŒˆ\n\nThats what everyone else is doing anyways",
                  "score": 1,
                  "created_utc": "2026-01-09 09:27:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny7t15f",
              "author": "DiscombobulatedAdmin",
              "text": "How has it failed?",
              "score": 6,
              "created_utc": "2026-01-07 15:44:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny61vhl",
          "author": "OneOnOne6211",
          "text": "I am so glad I bought a new computer in 2024.",
          "score": 14,
          "created_utc": "2026-01-07 08:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6lvni",
              "author": "Nyghtbynger",
              "text": "Same bro. I just regret not buying the 64gigs of ram. In fact I wanted to buy 2x48GB but I couldn't find an affordable option and chose to wait on year or two so it becomes more mainstream lol",
              "score": 9,
              "created_utc": "2026-01-07 11:37:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7w3qi",
                  "author": "OneOnOne6211",
                  "text": "I first bought 32gb and then a month or two later I bought another 32gb. So luckily, I did.\n\nIt's funny though, the biggest reason I bought a new computer in 2024 specifically is because I was playing a lot of Skyrim Special Edition at the time (heavily modded). And my computer was just unable to handle it. If I wasn't in an interior it was nearly unplayable. I'm talking like constant stuttering, 3 frames a second unplayable. So I was sick of that, so I bought a new PC that would be able to run it.\n\nSo thank you, Skyrim. Cuz otherwise I probably would've waited another year or two.",
                  "score": 2,
                  "created_utc": "2026-01-07 15:58:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6mcp4",
              "author": "AlwaysLateToThaParty",
              "text": "I'm so glad I built a good computer in 2019.",
              "score": 3,
              "created_utc": "2026-01-07 11:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7vz0a",
                  "author": "OneOnOne6211",
                  "text": "I mean, I built mine too. But I bought the parts all together in 2024.",
                  "score": 2,
                  "created_utc": "2026-01-07 15:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6f1ys",
          "author": "aeroumbria",
          "text": "I will not be baited I will not be baited I will not be baited...",
          "score": 7,
          "created_utc": "2026-01-07 10:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6i6bi",
          "author": "nenulenu",
          "text": "No thanks",
          "score": 7,
          "created_utc": "2026-01-07 11:07:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny64lrz",
          "author": "Kal-LZ",
          "text": "I saw new stock of PNY RTX 5090 for 5200â‚¬ on Amazon. I guess this will also affect to RTX PRO 6000",
          "score": 4,
          "created_utc": "2026-01-07 09:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6cc8c",
              "author": "FullOf_Bad_Ideas",
              "text": "Caseking still shows many SKUs from various AIBs in stock around 3100-3500 euro. I think those cards that are listed for 5000 euro are anomalies. TSMC didn't stop making chips, and 32gb of VRAM didn't jump in price this much yet to warrant that price.",
              "score": 4,
              "created_utc": "2026-01-07 10:16:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6i0jh",
                  "author": "a_beautiful_rhind",
                  "text": "They will dick us just because they can.",
                  "score": 8,
                  "created_utc": "2026-01-07 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6oabp",
          "author": "zeoNoeN",
          "text": "Im betting on the Datacenter Bubble/Build up slowing down at some point in the next 2-3 years, which will make consumer markets relevant again. No upgrades until then. Letâ€™s see if this ages like milk or wine.",
          "score": 6,
          "created_utc": "2026-01-07 11:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd6zfu",
              "author": "Doug_Fripon",
              "text": "They speculate on your inability to afford new hardware in the next years and the build-up of your habit to purchase compute as a service. They'll try to lock the hardware industry from now on, until it becomes true. The business model of Western AI companies only makes sense if you have no alternative as a consumer, so it only works with very expensive consumer hardware.\n\nYou don't need to own any hardware. A lot of companies will be happy to sell you their monthly subscription. Microsoft will gift you a Winbook for your 24 months Windows platinium commitment, and that's all you'll need.\n\nWe can hope for a Chinese consumer compute industry structuring by 2030 and blowing up this nonsense.",
              "score": 2,
              "created_utc": "2026-01-08 08:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny8igz8",
          "author": "HanzJWermhat",
          "text": "Iâ€™ll wait for the AI crash. \n\nLLM improvements havenâ€™t been meeting milestone expectations. We were told parabolic growth but tech is clearly hitting diminishing returns. Next SOTA models will be the bellweather. If they canâ€™t meaningfully improve I think weâ€™ll see a lot of companies majorly pull back.",
          "score": 5,
          "created_utc": "2026-01-07 17:39:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny68qd8",
          "author": "Elaughter01",
          "text": "Purchase? Hell no. I'm waiting 5 years or gonna start buying used systems instead.Â ",
          "score": 8,
          "created_utc": "2026-01-07 09:43:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6iq2v",
          "author": "Anyusername7294",
          "text": "!Remindme 40 months",
          "score": 9,
          "created_utc": "2026-01-07 11:12:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6itki",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 3 years on [**2029-05-07 11:12:09 UTC**](http://www.wolframalpha.com/input/?i=2029-05-07%2011:12:09%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/ny6iq2v/?context=3)\n\n[**5 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1q694ic%2Fdont_put_off_hardware_purchases_gpus_ssds_and_ram%2Fny6iq2v%2F%5D%0A%0ARemindMe%21%202029-05-07%2011%3A12%3A09%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q694ic)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 3,
              "created_utc": "2026-01-07 11:12:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6jm3p",
          "author": "tirolerben",
          "text": "Prices are already going vertical. You canâ€˜t skyrocket more than 90Â°, else you are going down again.",
          "score": 4,
          "created_utc": "2026-01-07 11:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny67xh6",
          "author": "NebulousNitrate",
          "text": "Guarantee there is going to be a push for thin clients that use cloud subscriptions. PC enthusiasts can wait it out, small businesses and regular consumers cannot.",
          "score": 11,
          "created_utc": "2026-01-07 09:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6vdc5",
              "author": "silenceimpaired",
              "text": "You will own nothing (but thin clients) and be happy (or you wonâ€™t be able to play games, video edit, chat with AI, etc.)",
              "score": 6,
              "created_utc": "2026-01-07 12:45:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5wifz",
          "author": "JuliusCeaserBoneHead",
          "text": "One day this whole thing will come crashing down. Wonâ€™t hold my breath for it but this stupid thing canâ€™t be sustainableÂ ",
          "score": 23,
          "created_utc": "2026-01-07 07:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny68ne5",
              "author": "joninco",
              "text": "Or worseâ€¦a new norm.",
              "score": 11,
              "created_utc": "2026-01-07 09:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny7p1pc",
                  "author": "mycall",
                  "text": "Many crashes is chaos",
                  "score": 3,
                  "created_utc": "2026-01-07 15:25:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyaeh8v",
                  "author": "Exciting_Garden2535",
                  "text": "Right now, there is a huge margin between cost and price, close to the illegal drugs market margin. This margin will never be the norm, just because producing memory is legal, not illegal. So the production will grow until the margin does not become just tiny, due to all demands being fulfilled.",
                  "score": 2,
                  "created_utc": "2026-01-07 22:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny753fy",
              "author": "Direct_Turn_1484",
              "text": "Yeah and I donâ€™t plan on funding making it worse in the meantime.",
              "score": 2,
              "created_utc": "2026-01-07 13:43:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny6g7oh",
              "author": "Ok-Bill3318",
              "text": "Citation required for the last time in history hardware prices came â€œcrashing downâ€",
              "score": 2,
              "created_utc": "2026-01-07 10:50:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6ibr3",
                  "author": "Skystunt",
                  "text": "Rtx 3090 from Â£3000 to Â£450. In 2022 i was like â€œiâ€™ll be rich if i own this cardâ€ and now itâ€™s really affordable on ebay",
                  "score": 10,
                  "created_utc": "2026-01-07 11:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny9zb09",
              "author": "datbackup",
              "text": "Hey youâ€™re talking about the internet right?",
              "score": 1,
              "created_utc": "2026-01-07 21:28:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6o9ug",
          "author": "ravensholt",
          "text": "Ever heard about the story \"Peter and the wolf\" ?\n\n  \nEveryone keeps screaming \"Prices are going up! Prices are going up!\"  \nAnd in the same sentence, \"Buy now! Buy now!\".\n\n  \n*Smart people wait*, and **keep their money in their pockets**.  \n*Speak with your wallet.*  \nThat's the only language corporations understand.  \nThe only way to **fight** this **inflation**, is with our money - **don't go out and spend** now to boost the companies economy.",
          "score": 20,
          "created_utc": "2026-01-07 11:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nya5q27",
              "author": "1998marcom",
              "text": "No, sorry, but it's not that useful to fight inflation by not spending, when the fed is printing more money and lending it to the corps that are buying the chips. The fed will continue to rob you of your purchasing power and give it to the big corporations (or the state, but the result is the same, as the banks that would have bought the state bonds now simply use the same funds to open loans to the corps)",
              "score": 1,
              "created_utc": "2026-01-07 21:56:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydufo6",
                  "author": "DRM_is_Hell",
                  "text": "Inflation keeps getting worse because of your mindset.\n\n\nDid it ever occur to you that in order for those mega corporations to have so much money at the first place, they needed us to obtain it? Without us, they're all nothing.",
                  "score": 1,
                  "created_utc": "2026-01-08 12:06:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6zdxb",
              "author": "discreetwhisper1",
              "text": "Ai comment",
              "score": -9,
              "created_utc": "2026-01-07 13:10:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny70528",
                  "author": "ravensholt",
                  "text": "Sorry to disappoint you.  \n  \nIt's simply just a well formatted comment of my very own.  \n  \nI understand, that it might seem difficult for someone like you to accept that there are indeed people out there capable of thinking themselves.",
                  "score": 16,
                  "created_utc": "2026-01-07 13:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5yukh",
          "author": "SandboChang",
          "text": "It's already too late NOW, not later.",
          "score": 14,
          "created_utc": "2026-01-07 08:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6fxtu",
          "author": "Ok-Bill3318",
          "text": "Already refreshed all my stuff through 2025. Good luck people!",
          "score": 4,
          "created_utc": "2026-01-07 10:48:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6ytwe",
          "author": "Echoplanar_Reticulum",
          "text": "Definitely put off purchasing. Weâ€™re in the bubble, and these AI companies will need to show earnings soon.",
          "score": 4,
          "created_utc": "2026-01-07 13:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6m56m",
          "author": "AlwaysLateToThaParty",
          "text": "> GPUs, SSDs, and RAM are going to *continue to* skyrocket in price ~~soon~~\n\nFTFY",
          "score": 3,
          "created_utc": "2026-01-07 11:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6qcum",
          "author": "ravensholt",
          "text": "!Remindme 36 months.",
          "score": 3,
          "created_utc": "2026-01-07 12:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7fu76",
          "author": "DrDisintegrator",
          "text": "in the future nVidia and OpenAI has planned, they rent you an AI by the question....",
          "score": 3,
          "created_utc": "2026-01-07 14:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8au41",
          "author": "CBHawk",
          "text": "Jokes on you, I'm waiting for DDR6!",
          "score": 3,
          "created_utc": "2026-01-07 17:05:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5us4e",
          "author": "NoChard1199",
          "text": "Well that's just fantastic, literally bought everything EXCEPT a new GPU last month thinking I'd wait for the 5080 reviews\n\n  \nGuess I'm stuck with my 3070 until 2027 or selling a kidney",
          "score": 7,
          "created_utc": "2026-01-07 07:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5w6v5",
              "author": "supportkeinmord",
              "text": "Is the kidney in good condition? Do you have pictures?",
              "score": 30,
              "created_utc": "2026-01-07 07:46:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny64nld",
                  "author": "boraam",
                  "text": "Same here. Don't mind stocking up on critical parts.",
                  "score": 9,
                  "created_utc": "2026-01-07 09:04:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6gvws",
              "author": "olmoscd",
              "text": "5080 reviews? those were published a year ago",
              "score": 6,
              "created_utc": "2026-01-07 10:56:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6osst",
                  "author": "Mochila-Mochila",
                  "text": "Yeah that explanation for the delayed buying doesn't make sense.",
                  "score": 3,
                  "created_utc": "2026-01-07 11:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6aik0",
              "author": "ericek111",
              "text": "I'm on an RX 6700 XT (so \\~3070 performance) I bought second-hand for 250 â‚¬. What could I upgrade to? The RX 9060 XT is 400 â‚¬, RX 9070 is 650 â‚¬ (XT at 50 â‚¬ more). From the green market, only the RTX 5060 Ti 16 GB makes sense, at 450 â‚¬.\n\nI know prices are supposed to surge, but paying twice as much for a 30 % improvement after 5 years? Meh.",
              "score": 2,
              "created_utc": "2026-01-07 09:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny6m2qe",
                  "author": "Nyghtbynger",
                  "text": "I use a 7800XT locally, a 9060XT would be fine for most use (except some kind of training that don't use llama.cpp, but you can rent a GPU for that). The larger VRAMs options are unafffordable. Maybe the Radeon Pro with 32 gigs of ram could be nice too",
                  "score": 2,
                  "created_utc": "2026-01-07 11:39:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny6q81y",
              "author": "ravensholt",
              "text": "Hang in there ... Keeping your money in your pocket is the right decision.",
              "score": 2,
              "created_utc": "2026-01-07 12:09:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydup57",
              "author": "DRM_is_Hell",
              "text": "The reviews been out for ~11 months now.",
              "score": 1,
              "created_utc": "2026-01-08 12:08:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny659m4",
          "author": "noatoms",
          "text": "Soon? Brother in Christ it has already happened!",
          "score": 6,
          "created_utc": "2026-01-07 09:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6lofo",
          "author": "Nyghtbynger",
          "text": "Hmm, media information is often late. Here are my few cents : I wanted to purchase a SSD to store models and all, but finally I will settle for a hard disk that is way cheaper and write some script with the AI to cache/rotate the files or whatever. I have more time than moeny right now",
          "score": 2,
          "created_utc": "2026-01-07 11:36:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6sbtf",
          "author": "sinchiyap",
          "text": "Looking at the price of everything now Iâ€™m most probably going to hold on to my 3060 12GB for another 3 more years",
          "score": 2,
          "created_utc": "2026-01-07 12:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7eblw",
          "author": "PidgeyPower",
          "text": ">According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55â€“60% quarter over quarter\n\nPrice could drop and it would still probably be near a 55-60% rise quarter over quarter. It is comparing averages of prices in the final three months of 2025. Where we stand today is probably 55-60% higher than the average of the last three months.\n\nThereâ€™s some things further down the chain like consoles and cell phones that you could say are going up. The rest of this stuff has already moved.Â ",
          "score": 2,
          "created_utc": "2026-01-07 14:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7izm8",
          "author": "Tall_East_9738",
          "text": "Everyone keeps repeating that yet I can currently buy a 5070ti for the same price I paid in August. Yâ€™all need to stop falling for this FOMO.",
          "score": 2,
          "created_utc": "2026-01-07 14:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydv2qe",
              "author": "DRM_is_Hell",
              "text": "It's FOMO, yes. But 5070ti has gone up in price since August.",
              "score": 1,
              "created_utc": "2026-01-08 12:10:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyg18sm",
                  "author": "Tall_East_9738",
                  "text": "I'm looking at the amazon page, it's the exact same price and still in stock.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyohv32",
                  "author": "feenixOmlette",
                  "text": "5070ti is old stock that has no use in ML.\n\n\nYou can still get 12 even 16gb cards.\n\n24 and 32 however is insane pricing because those sizes of ram compete with datacenters memory and can even run decent local ML.",
                  "score": 1,
                  "created_utc": "2026-01-09 22:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny7prjf",
          "author": "cicoles",
          "text": "Nah. Iâ€™m happy renting and watching over-investments going broke later. Better put the money in bank shares, those greedy bankers gets bailed out every single time they mess up.",
          "score": 2,
          "created_utc": "2026-01-07 15:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7pvdc",
          "author": "Noeyiax",
          "text": "Shareholders bruh, what about the shareholders and their passive income!! Oh nooooo\n\nJust wait a few years ðŸ™‚â€â†•ï¸, this scenario in business is similar to infrastructure advancement like railway, oil, dotcom... The only solution is for the wealthy to make sacrifices.\n\nBut we all know our wealthy overlords are kinda dog shit people, so they'll just blame us poor people lmfao",
          "score": 2,
          "created_utc": "2026-01-07 15:29:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7pxta",
          "author": "Sitheral",
          "text": "Don't care, ain't buying shit.\n\nAnd they better develop games optimized because I have backlog for more than one lifetime and plenty of other stuff to spend money to.",
          "score": 2,
          "created_utc": "2026-01-07 15:30:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8a0g8",
          "author": "Proof_Scene_9281",
          "text": "Do they expect to sell any!? Sheesh. 5090â€™s were 2500$ 3 weeks ago, now theyâ€™re $3500!!??",
          "score": 2,
          "created_utc": "2026-01-07 17:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8e42p",
          "author": "hawseepoo",
          "text": "No thanks lol. Iâ€™m waiting until the market crashes and picking up stuff for half of last yearâ€™s MSRP",
          "score": 2,
          "created_utc": "2026-01-07 17:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycoqok",
          "author": "DonkeyBonked",
          "text": "I just bought the hardware to build a AMD EPYC 7502 server, 128GB DDR4, and 4x RTX 3090 24GB.\n\nI'm going to use this and ride it out until better and more efficient hardware starts to get decommissioned.\n\nI'll never pay current hardware ransom. I have 4x 4TB NVMe that I purchased for under $200 each, and that should last me until prices stabilize, and I don't care if it's 4-5 years before that happens. Eventually these data centers will need to upgrade and flood markets with the hardware they've been buying for the last 3 years, so the time will come when this bubble pops.\n\nUntil then, open-source is becoming really efficient, with rapid improvements, and Intel is putting out AI hardware while technology is shifting, so I expect this current trend is unsustainable, especially once companies like OpenAI launch their enshittification IPO that means they will start needing to put all the RAM they're hoarding to use.\n\nThere's also a lot of unsold RAM wafer processing that is now being urgently put to use, while people are getting creative about how they use what resources we have.\n\nLet the gluttons have their cake, if I wasn't ever going yo pay 3k for a GPU, I'm never going to pay 5k, so it won't impact me either way.",
          "score": 2,
          "created_utc": "2026-01-08 06:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfnj37",
          "author": "Cheap_Image_5113",
          "text": "Brave of you to assume I was going to buy any hardware at all.",
          "score": 2,
          "created_utc": "2026-01-08 17:33:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5x7vh",
          "author": "sunshinecheung",
          "text": "Nah, i can use it free through their official website, and use small models for sensitive content",
          "score": 5,
          "created_utc": "2026-01-07 07:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny66lmn",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-01-07 09:23:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny69d2m",
              "author": "jay-aay-ess-ohh-enn",
              "text": "Ah... So this is all **your** fault.",
              "score": 7,
              "created_utc": "2026-01-07 09:49:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny7slb1",
          "author": "CrypticZombies",
          "text": "Op must be fun at parties",
          "score": 2,
          "created_utc": "2026-01-07 15:42:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zwic",
          "author": "Timotey27",
          "text": "Lol at all these innocent summer children in here who think the situation will go back to normal in a few years. It's done. In the future gaming computers will no longer be affordable for the average person.",
          "score": 0,
          "created_utc": "2026-01-07 13:13:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8usjk",
              "author": "Taki_Minase",
              "text": "A 386 used to be $40000, so much nonsense being said these days.",
              "score": 3,
              "created_utc": "2026-01-07 18:33:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny83668",
              "author": "PlaceboByProxy",
              "text": "RemindMe! 3 years",
              "score": 1,
              "created_utc": "2026-01-07 16:30:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny83uab",
              "author": "PlaceboByProxy",
              "text": "!RemindMe 3 years",
              "score": 1,
              "created_utc": "2026-01-07 16:33:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6tjqq",
          "author": "eli_pizza",
          "text": "Nobody knows though. If they did then prices would have already spiked.",
          "score": 1,
          "created_utc": "2026-01-07 12:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zutx",
          "author": "Lordxb",
          "text": "Posts like this contribute to panic buying leading to increase in prices!!",
          "score": 1,
          "created_utc": "2026-01-07 13:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny710i4",
          "author": "TurtleNamedMyrtle",
          "text": "I want a 5090. It might be time to hang up my two 1080Tiâ€™s (with the SLI bridge). Talk me out of it?",
          "score": 1,
          "created_utc": "2026-01-07 13:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny74u6r",
              "author": "StorageHungry8380",
              "text": "Now or never, I suspect. I bought a 5070Ti at launch but when the RAM hit the fan I traded it for a 5090. Ended up paying a bit more than if I had just bought a 5090 last summer, but here they're already going out of stock and those that are left are way up in price.",
              "score": 1,
              "created_utc": "2026-01-07 13:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny71s9l",
          "author": "AnomalyNexus",
          "text": "Glad I built a homeserver with a couple TB of flash half a year back",
          "score": 1,
          "created_utc": "2026-01-07 13:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny73e44",
          "author": "jabblack",
          "text": "So what is Nvidia going to do with all the lower binned AI GPUs if theyâ€™re not going to make consumer GPUs?",
          "score": 1,
          "created_utc": "2026-01-07 13:33:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyoibvo",
              "author": "feenixOmlette",
              "text": "The main binning is based on cores, and cores are still cheap. The problem is the memory.",
              "score": 1,
              "created_utc": "2026-01-09 22:19:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny74eqx",
          "author": "PlasticTourist6527",
          "text": "WD and Seagate stocks are already up, which stocks do you suggest buying to counter this?",
          "score": 1,
          "created_utc": "2026-01-07 13:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7cy7f",
          "author": "Electronic_Status_60",
          "text": "Scooping a 5070 ti today",
          "score": 1,
          "created_utc": "2026-01-07 14:25:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7e3cc",
          "author": "PotaroMax",
          "text": "So... is this the AI winter we all feared?",
          "score": 1,
          "created_utc": "2026-01-07 14:31:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7mn5n",
          "author": "Massive-Question-550",
          "text": "Seriously considering downgrading to my old ddr4 pc and 6x ing my 64gb ddr5... Maybe I'll sell my 4tb ssd too.Â ",
          "score": 1,
          "created_utc": "2026-01-07 15:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7n5m1",
          "author": "Gipetto",
          "text": "Yeah, already happening. CPUs as well. I figured since they were reasonably priced Iâ€™d upgrade my old AM4 machine to a 5900XT or 5950X in the new year. Theyâ€™ve recently taken a $70 jump in price on Amazon, more on other sites, so, yeah, naw, I guess Iâ€™m riding this out on the current setup.",
          "score": 1,
          "created_utc": "2026-01-07 15:16:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7tip3",
          "author": "etralse",
          "text": "Soon? This started already.",
          "score": 1,
          "created_utc": "2026-01-07 15:46:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny86a3j",
          "author": "arentol",
          "text": "With Nvidia cutting back production of consumer GPUs by 40%, and AMD and Intel not yet ready to take up that slack, of course GPU prices will skyrocket. Then, even after AMD and Intel ramp up production, prices will remain much higher than previous. \n\n18 months from now AMD will have gone from $26 billion in revenue (2024) to $60 billion+ (2027) (Nvidia made $60 billion in 2024 btw), based on this complete fumble by Nvidia... Nvidia meanwhile will make only 2 to 4 billion more in revenue on their GPU capacity moved to focus on AI chips. So they are going to basically double their biggest competitors revenue, triple that companies profits, and give that company an insane boost to its R&D budget for AI chips, all so they can make a negligible amount of additional revenue that will not help them stay ahead of AMD in the slightest. \n\nNvidia will be fine, but 5-8 years from now instead of having 90% of the AI market share that they would have if they stayed in the consumer GPU business they will have maybe 75-80%, because AMD will have competitive products thanks to this massive boost in revenue and profits Nvidia is gifting AMD.",
          "score": 1,
          "created_utc": "2026-01-07 16:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8jeek",
          "author": "warnerbell",
          "text": "This is rough timing. Just when local inference was getting accessible, hardware costs are about to spike.\n\nOn the bright side, this makes efficiency optimization more valuable. Context window management, quantization, prompt architecture all the stuff that squeezes more out of existing hardware becomes critical.\n\nDoubling down on software-side optimizations while hardware gets expensive could be beneficial",
          "score": 1,
          "created_utc": "2026-01-07 17:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8vlgv",
              "author": "Taki_Minase",
              "text": "I agree.",
              "score": 1,
              "created_utc": "2026-01-07 18:36:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyoimsf",
              "author": "feenixOmlette",
              "text": "The worst nightmare of all AI companies is that you can do the inference just as well on a small local machine that you own for the price of electricity.",
              "score": 1,
              "created_utc": "2026-01-09 22:20:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny92b59",
          "author": "IonizedHydration",
          "text": "just glad i upgraded from 64 to 128 ram about 6 months ago.",
          "score": 1,
          "created_utc": "2026-01-07 19:05:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny944v1",
          "author": "AmazinglyNatural6545",
          "text": "Oh come on. You're a few months late buddy. \nThat's why I decided to bite the bullet and updated the hardware in Nov.",
          "score": 1,
          "created_utc": "2026-01-07 19:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9ep04",
          "author": "RenewAi",
          "text": "aww man",
          "score": 1,
          "created_utc": "2026-01-07 19:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9k5yu",
          "author": "Hybridxx9018",
          "text": "People made fun of me for paying $200 for 46gb of ram during Black Friday lol. Sometimes you gotta bite the bullet.",
          "score": 1,
          "created_utc": "2026-01-07 20:23:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9tb1n",
          "author": "AvocadoArray",
          "text": "I bought a bunch of different RAM on eBay in June to upgrade my home lab servers and work/gaming PCs and compared prices today. Here's what I'm seeing so far:\n\n* DDR5 32GB non-ECC (new): $70/ea -> $319/ea (+355%)\n* DDR4 32GB ECC: $34.20/ea -> $130/ea (+280%)\n* DDR4 8GB ECC: $6/ea -> $30/ea (+400%)\n* DDR3 8GB ECC: $5/ea -> $4/ea **(-20%)**\n\nSo yeah, the fast DDR4/DDR5 is in high demand, while the older DDR3 stuff is actually declining in price. For general purpose day-to-day computers or servers, most people would never see the difference in DDR3/4/5, but there's another elephant in the room when it comes to these price trends.\n\n**Windows 10 hit EOL in October**, and Windows 11 only officially supports CPUs that run on DDR4+. The decision to drop support for older perfectly usable CPUs Thanos'd a huge chunk of consumer computers out of the market, and now those replacements are also competing for the higher priced DDR4/5 chips.\n\nFor general purpose computing and servers, you'll save a ton of money if you run an older DDR3 system on Linux (or bypassing W11's system requirements during installation). And for AI inference, if you're running your entire model in VRAM anyway, DDR3 will do just fine in most cases.\n\nIf I were building a new AI inference server today, I'd consider something like a 4U PowerEdge r920. 4x CPUs, 4x 1100w PSUs, 6x PCI 3.0 x16 slots + 2x x8 slots and as much RAM as your heart desires for <$1,000. Then load up whatever GPUs fit your needs.\n\nSure, it's not ideal, but the price/performance value of this older hardware is starting to look more attractive every day and the market is already flooded with inventory.",
          "score": 1,
          "created_utc": "2026-01-07 21:03:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyddw5q",
              "author": "Eisenstein",
              "text": "The problem with the DDR3 servers is that they are running Xeon E V2s, which don't have AVX2.",
              "score": 1,
              "created_utc": "2026-01-08 09:49:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyeztxv",
                  "author": "AvocadoArray",
                  "text": "Good point, although that should only affect CPU-inference right? If the entire model fits in the VRAM pool then does it make a difference?\n\nI have an r720xd w/ E5-2667 v2s running a GTX 1080 on llama.cpp just fine, although VLLM fails because they don't support Pascal.",
                  "score": 1,
                  "created_utc": "2026-01-08 15:49:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nya6cpi",
          "author": "johnboi1323",
          "text": "Lol built my server in January to avoid this. prices have already skyrocketed. but yes only gonna get worse.",
          "score": 1,
          "created_utc": "2026-01-07 21:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nya73z4",
          "author": "Clear-Ad-9312",
          "text": "People already can't afford to buy, we are stuck with the old parts as datacenters gobble up all the supply. Well I hope, in 3-5 years time, the 3rd party commercial equipment sales are going to be crazy af.",
          "score": 1,
          "created_utc": "2026-01-07 22:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb1bfn",
          "author": "Rich_Artist_8327",
          "text": "Luccky me, bought hardware worth of 30k just before the shit show. 5090 1700â‚¬ \ndc3000me 15tb nvme 1200â‚¬ \n32gb ddr5 ecc udimms 110â‚¬ \nmany.",
          "score": 1,
          "created_utc": "2026-01-08 00:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc010v",
          "author": "meshreplacer",
          "text": "Just buy a Mac Studio. When the AI bubble implodes there will be a glut of cheap server hardware to run local LLMs cheap",
          "score": 1,
          "created_utc": "2026-01-08 03:31:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyojakz",
              "author": "feenixOmlette",
              "text": "This is true, either AI keeps getting better and better and they need to replace the old ML hardware with the latest which means cheap second hand h100s\n\nOr ML bubble pops at which point, salvaging datacenters will cause a lot of cheap h100s either way.",
              "score": 1,
              "created_utc": "2026-01-09 22:23:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nycashx",
          "author": "badgerbadgerbadgerWI",
          "text": "Good PSA. This is also why edge deployment strategies matter - if you can run well-quantized smaller models locally instead of depending on cloud GPU availability, you're more insulated from these price swings. Q4_K_M quants running on Apple Silicon or AMD integrated graphics is becoming surprisingly viable.",
          "score": 1,
          "created_utc": "2026-01-08 04:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycjqy9",
          "author": "meowrawr",
          "text": "This is exactly why I bought a new m3 ultra with 256gb. Thinking I should have done 512 now tho.",
          "score": 1,
          "created_utc": "2026-01-08 05:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyckl0k",
          "author": "IrisColt",
          "text": ">Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon\n\n\nSelf-fulfilling prophecy... h-heh",
          "score": 1,
          "created_utc": "2026-01-08 05:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydm9n1",
          "author": "LicensedTerrapin",
          "text": "I bought my second 3090, I think it will keep me gaming and LLMing for a while. Especially once my 96gb DDR5 ram gets delivered.",
          "score": 1,
          "created_utc": "2026-01-08 11:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygdf7k",
          "author": "rockytonk",
          "text": "If Nvidia is going to increase their prices by so much, why did they just restock the 5080 at msrp?",
          "score": 1,
          "created_utc": "2026-01-08 19:24:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh3zyk",
              "author": "Eisenstein",
              "text": "Where are you finding a 5080 at MSRP?",
              "score": 1,
              "created_utc": "2026-01-08 21:22:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhuumz",
          "author": "HarlanHitePOG",
          "text": "SSD's are already like double the price from a year ago",
          "score": 1,
          "created_utc": "2026-01-08 23:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymkdjx",
          "author": "TurtleNamedMyrtle",
          "text": "Looking at 5090 prices right now, they range from $2500 to $3500. So many brands too. Should I be wary of different brands or should I just focus on the specs when buying?",
          "score": 1,
          "created_utc": "2026-01-09 17:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyob5yd",
          "author": "feenixOmlette",
          "text": "See why can't Elon make. DRAM giga factory if that's where this AI stuff is going long term.",
          "score": 1,
          "created_utc": "2026-01-09 21:45:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nywnlf7",
          "author": "Isekremu",
          "text": "Well firstly, it's already way too late to be saying this. It's already happened for a while.\n\nBut reality is, it is indeed JUST a supply/demand issue, not a cost to make issue, so Samsung and the rest are just laughing all the way to the bank.\n\ne.g. I bought 128G of DDR5 6000 RAM in Jul'24 on amazon for Aus$658 - and today it's over $6000 for exactly the same RAM - I was considering building another such machine and decided I wouldn't, considering when the RAM was so many times more than everything else vs the (14900k) machine I built 1.5 years ago.\n\nYou can make up all the excuses you like for why demand is so high, and indeed all this unreliable AI seems to be the biggest cause, (and a deal made between Samsung and OpenAI for HBM seems to be a big part of this) but it only means the 'supplier' is charging more for it since they know they will get that higher price and higher profit - see here: [https://www.reddit.com/r/technology/comments/1q73ov3/samsung\\_electronics\\_estimates\\_nearly\\_threefold/](https://www.reddit.com/r/technology/comments/1q73ov3/samsung_electronics_estimates_nearly_threefold/)\n\nI do find that amusing - since in reality it is simply just Samsung putting up the price\n\nOh well.",
          "score": 1,
          "created_utc": "2026-01-11 03:10:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny74asp",
          "author": "Formal-Hawk9274",
          "text": "Don't forget to thank the Republicans",
          "score": 2,
          "created_utc": "2026-01-07 13:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny76h66",
          "author": "Beneficial_Common683",
          "text": "Hey tell me how much megacorp pay you for this post so i can join the hype too",
          "score": 1,
          "created_utc": "2026-01-07 13:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7ah66",
          "author": "Green-Ad-3964",
          "text": "The era of local pc is ending, and the society as we know it will end with it. Cloud is like the Nothing advancing, in the never ending story and the only result will be a ready player one scenario.",
          "score": 0,
          "created_utc": "2026-01-07 14:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny80d8o",
          "author": "power97992",
          "text": "Just accept renting gpus and using api or buy an affordable machine lol, RAm won't get cheap anytime soon!",
          "score": 0,
          "created_utc": "2026-01-07 16:18:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7uuxo",
      "title": "OK I get it, now I love llama.cpp",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/",
      "author": "vulcan4d",
      "created_utc": "2026-01-09 01:39:13",
      "score": 237,
      "num_comments": 55,
      "upvote_ratio": 0.92,
      "text": "I just made the switch from Ollama to llama.cpp.  Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all.  Once you realize what you truly want to run, llama.cpp is really the way to go.\n\nMy hardware ain't great, I have a single 3060 12GB GPU and three P102-100 GPUs for a total of 42GB.  My system ram is 96GB along with an Intel i7-9800x.  It blows my mind that with some tuning what difference it can make.  You really need to understand each of the commands for llama.cpp to get the most out of it especially with uneven vram like mine.  I used Chatgpt, Perplexity and suprisingly only Google AI studio could optimize my settings while teaching me along the way.\n\nCrazy how these two commands both fill up the ram but one is twice as fast as the other.  Chatgpt helped me with the first one, Google AI with the other ;).  Now I'm happy running local lol.\n\n**11t/s:**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 21 --main-gpu 0 --flash-attn off --cache-type-k q8\\_0 --cache-type-v f16 --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--mmap --numa distribute --batch-size 384 --ubatch-size 256 --jinja --threads $(nproc) --parallel 2 --tensor-split 12,10,10,10 --mlock\n\n**21t/s**  \nsudo pkill -f llama-server; sudo nvidia-smi --gpu-reset -i 0,1,2,3 || true; sleep 5; sudo GGML\\_CUDA\\_ENABLE\\_UNIFIED\\_MEMORY=0 CUDA\\_VISIBLE\\_DEVICES=0,1,2,3 ./llama-server --model /home/llm/llama.cpp/models/gpt-oss-120b/Q4\\_K\\_M/gpt-oss-120b-Q4\\_K\\_M-00001-of-00002.gguf --n-gpu-layers 99 --main-gpu 0 --split-mode layer --tensor-split 5,5,6,20 -ot \"blk\\\\.(2\\[1-9\\]|\\[3-9\\]\\[0-9\\])\\\\.ffn\\_.\\*\\_exps\\\\.weight=CPU\" --ctx-size 30000 --port 8080 --host [0.0.0.0](http://0.0.0.0) \\--batch-size 512 --ubatch-size 256 --threads 8 --parallel 1 --mlock\n\nNothing here is worth copying and pasting as it is unique to my config but the moral of the story is, if you tune llama.cpp this thing will FLY!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q7uuxo/ok_i_get_it_now_i_love_llamacpp/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nyjrmed",
          "author": "Marksta",
          "text": "Bro those LLMs just wrote totally random junk, y'know? Almost all of those on the 2nd command are defaults or options that'll make it go slower. And regex for layers that don't exist on that model...",
          "score": 34,
          "created_utc": "2026-01-09 05:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylildl",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 9,
              "created_utc": "2026-01-09 14:04:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylukib",
                  "author": "No_Afternoon_4260",
                  "text": "And tf with NVIDIA_UNIFIED_MEMORY.\nIt's like the second time I see it randomly pop in this sub. From my understanding that was a quick patch made by fairydreaming while he was benchmarking some GH200",
                  "score": 6,
                  "created_utc": "2026-01-09 15:03:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyu2piq",
              "author": "IrisColt",
              "text": "heh",
              "score": 1,
              "created_utc": "2026-01-10 19:10:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyit742",
          "author": "pmttyji",
          "text": "Since you have 42GB VRAM, experiment with increased batch-size(1024) & ubatch-size(4096) for more better t/s. And bottom command doesn't have flash attention, enable it.\n\nAnd don't use quantized version of GPT-OSS-120B model. Use MXFP4 version [https://huggingface.co/ggml-org/gpt-oss-120b-GGUF](https://huggingface.co/ggml-org/gpt-oss-120b-GGUF) instead which is best.",
          "score": 56,
          "created_utc": "2026-01-09 02:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyixz4t",
              "author": "DeProgrammer99",
              "text": "Flash attention has been set to \"auto\" by default for a while now, so you generally shouldn't have to enable it manually. https://github.com/ggml-org/llama.cpp/pull/15434/changes",
              "score": 28,
              "created_utc": "2026-01-09 02:52:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyiyq44",
                  "author": "pmttyji",
                  "text": "I keep forgetting this. Fingers go automatically to type those 2 flags. May be it'll take some time.",
                  "score": 9,
                  "created_utc": "2026-01-09 02:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyqg0az",
                  "author": "mycall",
                  "text": "How does it know to enable it or not?",
                  "score": 1,
                  "created_utc": "2026-01-10 04:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyjykw5",
              "author": "BananaPeaches3",
              "text": "Ubatch is suppose to be larger than batch size??",
              "score": 3,
              "created_utc": "2026-01-09 06:49:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyk26qg",
                  "author": "pmttyji",
                  "text": "That part I don't know. But increasing values for both could boost performance if you have decent VRAM.\n\nAnyway found [this thread](https://github.com/ggml-org/llama.cpp/discussions/6328) on batch & ubatch.",
                  "score": 6,
                  "created_utc": "2026-01-09 07:19:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nykudim",
                  "author": "insulaTropicalis",
                  "text": "No, it must be equal or smaller.",
                  "score": 3,
                  "created_utc": "2026-01-09 11:30:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyknl1x",
                  "author": "Leflakk",
                  "text": "I doubt, if ubatch is for hardware and batch for application level then ubatch should then be lower or equals to batch.",
                  "score": 2,
                  "created_utc": "2026-01-09 10:33:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyjv261",
          "author": "No_Afternoon_4260",
          "text": "I don't get it:  \n- Why the sudo?!  \n- I thought the CUDA_UNIFIED_MEMORY was a quick patch hacked by u/fairydreaming while he was benchmarking some gh200. Iirc it is used to offload the expert layers of a MoE into cpu ram when gpu is full; so why set it by hand later in the command?\n\nRemove that flag and everything that's before it, you don't need it, if you launched a llama.cpp runtime before just kill it from the terminal you've launched it, you don't need the pkill neither you need the sudo nor cuda visible device (as you use all of them anyway and llama.cpp let you set that anyway)\n\nKeep it simple so you understand every part of it.  \nGlad you liked the llama.cpp experience!",
          "score": 19,
          "created_utc": "2026-01-09 06:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykdvqw",
              "author": "shaolinmaru",
              "text": ">Why the sudo?!Â Â \n\n\nBecause OP don't know batshit about Linux (and didn't bother to learn) , or are just a bot account.Â ",
              "score": 1,
              "created_utc": "2026-01-09 09:04:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylu5g5",
                  "author": "No_Afternoon_4260",
                  "text": "Come on, hope a bot won't make that mistake",
                  "score": 5,
                  "created_utc": "2026-01-09 15:01:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykwf01",
          "author": "mpasila",
          "text": "why do people always talk about ollama when you have koboldcpp.. it even has a GUI... you can easily use STT, TTS and LLM at the same time. no installations.",
          "score": 15,
          "created_utc": "2026-01-09 11:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymod3k",
              "author": "AmphibianFrog",
              "text": "Why do I want a GUI?",
              "score": 1,
              "created_utc": "2026-01-09 17:18:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn54iw",
                  "author": "mpasila",
                  "text": "Well you can use it without a GUI. For beginners it's gonna be easier to well use GUI probably.",
                  "score": 2,
                  "created_utc": "2026-01-09 18:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylmeqe",
              "author": "HonestoJago",
              "text": "Ollama is easier to find/install/keep updated.",
              "score": -1,
              "created_utc": "2026-01-09 14:23:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykap3o",
          "author": "IrisColt",
          "text": ">My hardware ain't great\n\n\nheh",
          "score": 10,
          "created_utc": "2026-01-09 08:35:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytf78r",
              "author": "KeyActual20",
              "text": "My broda lives in a bubble",
              "score": 2,
              "created_utc": "2026-01-10 17:20:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykuwfo",
          "author": "SatoshiNotMe",
          "text": "Totally agree. I recently wanted to use local LLMs (30B range Qwen, GPT-OSS) with Claude Code and Codex-CLI. Tried with Ollama and got terrible behavior. Then hunted around for ways to configure and hook up llama.cpp/llama-server with these CLI agents and these worked great. The precise details to get this working were scattered all over the place so I collected them here in case this is useful for others:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md",
          "score": 5,
          "created_utc": "2026-01-09 11:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl5v7v",
              "author": "our_sole",
              "text": "I'm in the same boat...I have been using ollama for a while and am now looking to utilize llama.cpp\n\nThis gh repo is very useful.  Thanks!!",
              "score": 2,
              "created_utc": "2026-01-09 12:52:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjtcu0",
          "author": "I-cant_even",
          "text": "If you want slow but big grab an NVMe and mmap yourself to larger models",
          "score": 7,
          "created_utc": "2026-01-09 06:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyk0x3r",
              "author": "Cluzda",
              "text": "how slow are we talking here?",
              "score": 3,
              "created_utc": "2026-01-09 07:09:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyk4rcd",
                  "author": "mc_nu1ll",
                  "text": "yes",
                  "score": 19,
                  "created_utc": "2026-01-09 07:42:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykcz0a",
          "author": "TechnoByte_",
          "text": "Don't run it as sudo...",
          "score": 3,
          "created_utc": "2026-01-09 08:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyjkkze",
          "author": "Terrible-Detail-1364",
          "text": "congrats once you have settled in take a look at https://github.com/mostlygeek/llama-swap",
          "score": 10,
          "created_utc": "2026-01-09 05:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjp1ub",
              "author": "HyperWinX",
              "text": "Llama server supports router mode, llama swap is not needed",
              "score": 2,
              "created_utc": "2026-01-09 05:35:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nykipr7",
                  "author": "FullstackSensei",
                  "text": "Really not the same. Llama-swap enables other things, like restarting llama-server or loading multiple models in parallel when you have enough VRAM. On my Mi50 rig, I find the models \"rot\" if left in VRAM without interaction for a few hours. With llama-swap, I can just unload the model, which terminates the instance of llama.cpp and start a new instance quickly and everything is back to normal.",
                  "score": 8,
                  "created_utc": "2026-01-09 09:49:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymi243",
                  "author": "suicidaleggroll",
                  "text": "llama-swap is still useful for loading multiple models in parallel, or being able to switch the backend server for different models.  For example, the latest llama.cpp is faster at GPT-OSS-120B than ik_llama.cpp is, but ik_llama.cpp is faster at everything else.  With llama-swap I can switch which server is used for a model by changing 3 characters in the config file.  You could even use vLLM to host one of your models with it if you want.",
                  "score": 2,
                  "created_utc": "2026-01-09 16:50:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyjwvry",
                  "author": "Mountain-Active-3149",
                  "text": "Could this be the reason that models are not loading on llama-swap after updating to the latest of llamacpp?",
                  "score": 1,
                  "created_utc": "2026-01-09 06:35:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyko5zt",
                  "author": "BraceletGrolf",
                  "text": "So far the router mode has been hit or miss for me, with it after a few jours/prompt becoming totally unresponsive",
                  "score": 1,
                  "created_utc": "2026-01-09 10:38:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nymu90p",
                  "author": "necrogay",
                  "text": "Llama-swap provides seamless orchestration for executing various AI tasks without conflicts or unnecessary memory overhead. I particularly appreciated its capabilities when I configured ComfyUI to work through llama-swap, this eliminated the need to manually turn the llama-server off and on every time the LLM sends API requests for image or video generation. Everything runs transparently and swiftly.",
                  "score": 1,
                  "created_utc": "2026-01-09 17:44:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykam44",
          "author": "IrisColt",
          "text": ">I just made the switch from Ollama to llama.cpp. Ollama is fantastic for the beginner because it lets you super easily run LLMs and switch between them all.Â \n\n\nSame journey. I'm glad to have switched.",
          "score": 2,
          "created_utc": "2026-01-09 08:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqrhqf",
          "author": "Curious_Emu6513",
          "text": "just use vllm or sglang",
          "score": 2,
          "created_utc": "2026-01-10 06:09:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyqs64l",
              "author": "yuch85",
              "text": "But they are horrible at gguf",
              "score": 0,
              "created_utc": "2026-01-10 06:15:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyt45ul",
          "author": "joelW777",
          "text": "ExllamaV3 (e.g. via tabbyAPI) would be much faster on this hardware (especially the prompt processing)",
          "score": 2,
          "created_utc": "2026-01-10 16:27:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk3v6k",
          "author": "ixdx",
          "text": "For optimal tuning of the **-ot** and **-ts** parameters, I recommend using **llama-fit-params**.  \n  \nExample:\n\n    root@e58a0b05aaab:/app# ./llama-fit-params --cache-type-k q4_0 --cache-type-v q4_0 -fitt 384 -fitc 73728 --model /models/unsloth/GLM-4.5-Air-GGUF/GLM-4.5-Air-UD-Q2_K_XL.gguf \n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    ggml_cuda_init: found 2 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 5070 Ti, compute capability 12.0, VMM: yes\n      Device 1: NVIDIA GeForce RTX 5060 Ti, compute capability 12.0, VMM: yes\n    build: 1 (e70e640) with GNU 13.3.0 for Linux x86_64\n    llama_params_fit_impl: projected memory use with initial parameters [MiB]:\n    llama_params_fit_impl:   - CUDA0 (NVIDIA GeForce RTX 5070 Ti):  15806 total,  26696 used,  11510 deficit\n    llama_params_fit_impl:   - CUDA1 (NVIDIA GeForce RTX 5060 Ti):  15848 total,  25660 used,   9965 deficit\n    llama_params_fit_impl: projected to use 52357 MiB of device memory vs. 30881 MiB of free device memory\n    llama_params_fit_impl: cannot fulfill margin of 384 MiB on all devices, need to use 22244 MiB less in total\n    llama_params_fit_impl: context size reduced from 131072 to 73728 -> need 3626 MiB less memory in total\n    llama_params_fit_impl: with only dense weights in device memory there is a total surplus of 21304 MiB\n    llama_params_fit_impl: filling dense-only layers back-to-front:\n    llama_params_fit_impl:   - CUDA1 (NVIDIA GeForce RTX 5060 Ti): 48 layers,   8045 MiB used,   7650 MiB free\n    llama_params_fit_impl:   - CUDA0 (NVIDIA GeForce RTX 5070 Ti):  0 layers,    723 MiB used,  14462 MiB free\n    llama_params_fit_impl: converting dense-only layers to full layers and filling them front-to-back with overflow to next device/system memory:\n    llama_params_fit_impl:   - CUDA0 (NVIDIA GeForce RTX 5070 Ti): 15 layers ( 1 overflowing),  14595 MiB used,    590 MiB free\n    llama_params_fit_impl:   - CUDA1 (NVIDIA GeForce RTX 5060 Ti): 33 layers (23 overflowing),  15104 MiB used,    590 MiB free\n    llama_params_fit: successfully fit params to free device memory\n    llama_params_fit: fitting params to free memory took 3.65 seconds\n    main: printing fitted CLI arguments to stdout...\n    -c 73728 -ngl 48 -ts 15,33 -ot \"blk\\.14\\.ffn_(up|gate|down).*=CUDA1,blk\\.25\\.ffn_(up|gate|down).*=CPU,blk\\.26\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.27\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.28\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.29\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.30\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.31\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.32\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.33\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.34\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.35\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.36\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.37\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.38\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.39\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.40\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.41\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.42\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.43\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.44\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.45\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.46\\.ffn_(up|down|gate)_(ch|)exps=CPU,blk\\.47\\.ffn_(up|down|gate)_(ch|)exps=CPU\"",
          "score": 2,
          "created_utc": "2026-01-09 07:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhud1d",
          "author": "lemondrops9",
          "text": "Hope your running Linux! I found Windows to be garbage with 3+ gpus",
          "score": 1,
          "created_utc": "2026-01-14 06:14:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qajxrg",
      "title": "Local LLM + Internet Search Capability = WOW",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/",
      "author": "alex_godspeed",
      "created_utc": "2026-01-12 03:21:41",
      "score": 234,
      "num_comments": 91,
      "upvote_ratio": 0.92,
      "text": "Am on Qwen 3, asked about the training date and it said 2024. Alright, guess that's the thing I need to live with. Just need to constantly lookup HF for updated LLM which fits my cute 16gb vram.\n\nThen someone said always ground your local AI with internet searches. A quick search = LM studio duckduckgo plugin\n\nWithin 15 minutes, prompt with \"searching the web\", exactly the same interface I saw at ChatGPT!\n\n  \nMan, this local AI is getting better. Am I having 'agentic-AI' now? haha. I.e., tool calling is always something i heard of, but think that it's reserved for some CS-pro, not an average joe like me.\n\n  \nso now what, when was your 'wow-moment' for stuff like this, and what other things you design in your workflow to make locally run LLM so potent and, most importantly, private? =)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz3lu6s",
          "author": "Fabulous_Fact_606",
          "text": "Youâ€™ll need to design a front end that sends the current time for it to get context. \n\nI get you. I have a GTX 5080. Designed (Ai assisted) a front end to have the ability to chat by voice conversational style. Itâ€™s taken me back to the first time I tried ChatGPT.",
          "score": 55,
          "created_utc": "2026-01-12 03:32:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3sgrj",
              "author": "sshwifty",
              "text": "Y'all need to be sharing this stuff",
              "score": 37,
              "created_utc": "2026-01-12 04:09:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3w5j1",
                  "author": "ridablellama",
                  "text": "game changers for me have been unmute which is a tts and stt harness for any llm and open-llm-vtuber. both open source.",
                  "score": 27,
                  "created_utc": "2026-01-12 04:32:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3sovc",
                  "author": "ClimateBoss",
                  "text": "is there cli tool like mistral vibe but for deep research?",
                  "score": 6,
                  "created_utc": "2026-01-12 04:10:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz51csh",
                  "author": "Fabulous_Fact_606",
                  "text": "Still working on the proof of concept. Was working on creating FastAPI's for TTS and SST for a front end web app. Settled on Kokoro-TTS as the best light weight model for high quality TTS. Nvidia's parakeet  v3 for SST.  Hosted them on my home computer via wireguard tunnel to my VPS webserver.\n\n4 days ago, decided to delve in the realm of local LLM.  was able to install quantized version Jamba2 mini and Ministral and install them in WSL with FastAPI. \n\nTunneled them via wireguard to my VPS,  Made a front end chat that utlizes SST -->LLM -- TTS  \n\nThus created Jarvis.  Because of the low context window 16GB vram,  i'm looking ways to give it internet access, distilling webscraping context to feed the main Chat context.",
                  "score": 5,
                  "created_utc": "2026-01-12 10:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4or0x",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 14,
              "created_utc": "2026-01-12 08:24:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz529ak",
                  "author": "Fabulous_Fact_606",
                  "text": "I\"m using the LLM in fast api, so the front end chat always sends a date and time context.",
                  "score": 2,
                  "created_utc": "2026-01-12 10:33:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3wb26",
              "author": "nomorebuttsplz",
              "text": "what tts and how is the latency?",
              "score": 6,
              "created_utc": "2026-01-12 04:33:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz524iu",
                  "author": "Fabulous_Fact_606",
                  "text": "sst and tts latency is acceptable as it is tunneled to my VPS webserver. \n\nSST < than 1 sec.\n\nTTS to first audio - 4.22s\n\nEnd to end (first audo) - \\~ 5-10sec.. dependent  on LLM output.",
                  "score": 5,
                  "created_utc": "2026-01-12 10:32:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz40dqu",
              "author": "Zc5Gwu",
              "text": "Doesn't including the time screw up prompt caching?",
              "score": 2,
              "created_utc": "2026-01-12 05:00:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz52oh8",
                  "author": "Fabulous_Fact_606",
                  "text": "I'm using a custom front end to send context via ---> FASTAPI LLM. Every  prompt contains the current time.",
                  "score": 2,
                  "created_utc": "2026-01-12 10:37:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz482iu",
              "author": "leonbollerup",
              "text": "Or use cherry studio :)",
              "score": 0,
              "created_utc": "2026-01-12 05:58:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz587ev",
                  "author": "ikkiyikki",
                  "text": "Had never heard of this and for a sec I thought it was an app like LM Studio that was better by having native MCP built in. Nope. Looks like a Perplexity clone that sends your prompts to the cloud.\n\nIf I'm getting this wrong and it does something better than LM Studio **locally** please lmk!",
                  "score": 4,
                  "created_utc": "2026-01-12 11:26:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz52fa9",
                  "author": "Fabulous_Fact_606",
                  "text": "Looking to use programs that are  apache 2.0 /  CC by 4.0 licenses -- free commercial use.",
                  "score": 1,
                  "created_utc": "2026-01-12 10:35:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4eqlr",
          "author": "Everlier",
          "text": "You can get this in one command with [Harbor](https://github.com/av/harbor), I think you might also enjoy how well it pairs with TTS/STT",
          "score": 18,
          "created_utc": "2026-01-12 06:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz66vgl",
              "author": "steezy13312",
              "text": "That repo has the longest list of files and folders at the root level that I've ever seen.",
              "score": 7,
              "created_utc": "2026-01-12 15:01:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6l3ef",
                  "author": "Everlier",
                  "text": "Check out CLI reference docs, they are even longer!",
                  "score": 2,
                  "created_utc": "2026-01-12 16:09:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4imy0",
              "author": "alex_godspeed",
              "text": "cool. A quick question. I was told as a newbie to avoid ollama (maybe because they go cloud and has lesser support than llama.ccp). Also, I can't use LM studio right? I searched the user guide and it retrieves none related to this interface.",
              "score": 1,
              "created_utc": "2026-01-12 07:28:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz4pft2",
                  "author": "Everlier",
                  "text": "`harbor up searxng` will spin up Open WebUI instance pre-configured for Web RAG with SearXNG, there are lots of inference backends to choose from if you don't want Ollama. llama-swap + llama.cpp is a popular option these days.",
                  "score": 3,
                  "created_utc": "2026-01-12 08:31:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz43kbc",
          "author": "SignificantExample41",
          "text": "better idea - use Brave Leo with your own local model and your choice of â€œmemoryâ€ (storage and retrieval pipeline) for it and let it hoover up context and run your life without anything going to the cloud. nemotron 3 30b a3b is ideal for this.",
          "score": 39,
          "created_utc": "2026-01-12 05:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz44xrh",
              "author": "Grouchy_Ad_4750",
              "text": "Which storage and retrieval pipeline would you recommend?",
              "score": 9,
              "created_utc": "2026-01-12 05:33:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4cmu2",
              "author": "KneelB4S8n",
              "text": "How do we use local LLM with Brave Leo? That's the browser, right?",
              "score": 10,
              "created_utc": "2026-01-12 06:35:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4gk1l",
                  "author": "Blindax",
                  "text": "I was wondering the same and found this [BYOM](https://support.brave.app/hc/en-us/articles/34070140231821-How-do-I-use-the-Bring-Your-Own-Model-BYOM-with-Brave-Leo)",
                  "score": 15,
                  "created_utc": "2026-01-12 07:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4fbip",
              "author": "RobotRobotWhatDoUSee",
              "text": "I'm very interested to hear more about your setup here. What 'harness' or 'agentic framework' are you using? (In scarequotes since I know this may simply be something like a clever use of OWUI or Claude code or cursor or something like that)",
              "score": 5,
              "created_utc": "2026-01-12 06:58:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4wbme",
                  "author": "SignificantExample41",
                  "text": "sorry for the delay yâ€™all. got busy on something. iâ€™m going to give away my best kept secret. which i feel bad about keeping cause these guys seem really cool (never met them donâ€™t know them) - but the answer to literally every problem iâ€™ve ever had isnt even one word.  Itâ€™s aâ€¦.\n\nLetta.",
                  "score": -1,
                  "created_utc": "2026-01-12 09:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4s2qx",
              "author": "Rey_Fiyah",
              "text": "Could you elaborate on this a little bit? Iâ€™d be very interested in having search capabilities on my local LLM, but Iâ€™m also privacy conscious. Any tips on where to start with this? Iâ€™ve only really used llama.cpp so far.",
              "score": 1,
              "created_utc": "2026-01-12 08:56:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5l3yu",
                  "author": "Odd-Criticism1534",
                  "text": "BrowserOS is something worth checking iht",
                  "score": 2,
                  "created_utc": "2026-01-12 13:01:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz43hxd",
          "author": "LegitimateCopy7",
          "text": "if you want real privacy, route the searches using seaxrng and Tor. otherwise search providers like duckduckgo still know you inside out.",
          "score": 7,
          "created_utc": "2026-01-12 05:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5n95l",
              "author": "Odd-Criticism1534",
              "text": "Legitimately curious, can you say more?",
              "score": 2,
              "created_utc": "2026-01-12 13:14:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6b5zq",
                  "author": "LegitimateCopy7",
                  "text": "seaxrng is an open source search engine you can selfhost. it still routes search queries to other search engines but does not include identifiable information in the queries.\n\nbecause search providers can still see your IP so this is where Tor comes in. other VPN providers work too.",
                  "score": 6,
                  "created_utc": "2026-01-12 15:23:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz61aop",
          "author": "Accomplished_Code141",
          "text": "My 'wow' moment was when I tried to use small models with a ZIM MCP server and a full Wikipedia ZIM file to 'ground' answers offline in LM Studio, alongside a DuckDuckGo Docker MCP server for online grounding too. I tried some small models and found out that Jan-v1-4B is a very good and lightweight model. It is very capable in tool calling for this use case, and even with a very low-end old GPU like a GTX 1650 or RTX 3050, I can get good results. Even with several tool callings at once (like in a deep research prompt), it doesn't fail the calls and maintains acceptable speeds.\n\nI think the future for local LLMs will be the use of small models trained to excel at specific tasks, being loaded, used, and unloaded automatically by an agentic-like harness with an orchestrator model. This would prevent hallucinations on very constrained hardware, unlike one giant model that needs too much RAM/VRAM to be good at all tasks. Something like a 30B orchestrator model with several 4-8B 'use-case-tailored' models could perform at the same level as the big 200B+ models across several tasks.",
          "score": 8,
          "created_utc": "2026-01-12 14:32:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7mlcc",
              "author": "Beginning-Pace-1426",
              "text": "Isn't that essentially the purpose of training LoRAs?",
              "score": 1,
              "created_utc": "2026-01-12 18:59:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4p2z3",
          "author": "ThiccStorms",
          "text": "16gig vram is the acceptable and generic userbase spec id always imagine if we were to talk about mass local AI adoption. People with home servers and beefy rigs are the cream layer of the masses. So yes what's the best LLM for 16 gb spec? Any leaderboards?Â ",
          "score": 7,
          "created_utc": "2026-01-12 08:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4141y",
          "author": "PaceZealousideal6091",
          "text": "Anyone setting up the tool calling or websearch using llama cpp?",
          "score": 11,
          "created_utc": "2026-01-12 05:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4a8s0",
              "author": "Sure_Explorer_6698",
              "text": "I was working on a Tavily search bot using llama.cpp, but i had to stop my meds for a while... long story... so its abandoned on GitHub til i can focus enough to pick it up again.\n\nPoint is, it is definitely possible. I was using a Samsung S20FE 6Gb w/ 6Gb swap. Never settled on a model, but i switched between 1-3B Llama or Qwen for expermination. (Edit for autocorrect.)",
              "score": -1,
              "created_utc": "2026-01-12 06:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3u46w",
          "author": "EbbNorth7735",
          "text": "Can we all share good sources for internet grounding? I've been using Serper I believe.",
          "score": 4,
          "created_utc": "2026-01-12 04:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4vyj3",
              "author": "bar_raiser333",
              "text": "I recommend Valyu for this. Cheap, fast, covers a lot of proprietarty data sources too [https://docs.valyu.ai/search/quickstart](https://docs.valyu.ai/search/quickstart)",
              "score": -4,
              "created_utc": "2026-01-12 09:34:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz40r27",
          "author": "nameless_0",
          "text": "I use Perplexica, on my 4070 mobile. It works great with Nemotron 30B-A3B. My wow moment was when I setup Qwen3 Coder with OpenCode and told it to setup a GitHub project I found, and It worked. It setup and made a front end for something that would've taken me a couple of hours of futzing around with to get working.",
          "score": 7,
          "created_utc": "2026-01-12 05:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz45yxn",
          "author": "Bananadite",
          "text": "Where's the best place to browse plugins for LM studio?",
          "score": 3,
          "created_utc": "2026-01-12 05:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz54izw",
              "author": "No_Swimming6548",
              "text": "Docker mcp kits are pretty good",
              "score": 2,
              "created_utc": "2026-01-12 10:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz41luo",
          "author": "ridablellama",
          "text": "hereâ€™s some of the things that have given me wow moments. force multipliers are native tools(web search, code interpreters, webpage scrape), mcp tools(so many good ones), voice(needs to be low latency real time for that special magic), memory system(especially passive automatic memory), vision(ocr and normal). you want python bare minimum in the code interpreter which will unlock data science level charts, powerpoints, customs csv file creation. some code interpreters have many languages not just python. code interpreter is insanely powerful imo and iâ€™ve barely scratched the surface. it can be 100% local too. voice is a great local use case cause it needs low latency. \n\nalso give the llm a email address and calendar. gave mine m365, one drive itâ€™s very nice for saving files and sending temporary download links via email etcâ€¦this is when it really came together for me agentic wise.\n\n  \ncheck out qwen agent framework on github for a lot of local tools and agents you can build with. local web scraper works like a charm.they have a code interpreter too but itâ€™s not sandboxed. several image manipulation related tools i think for their local image models.",
          "score": 4,
          "created_utc": "2026-01-12 05:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4r1px",
              "author": "Quiet-Owl9220",
              "text": ">you want python bare minimum in the code interpreter which will unlock data science level charts, powerpoints, customs csv file creation. some code interpreters have many languages not just python. code interpreter is insanely powerful imo and iâ€™ve barely scratched the surface. it can be 100% local too.\n\nIs there a LM studio plugin for this too?",
              "score": 3,
              "created_utc": "2026-01-12 08:46:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5aj0v",
          "author": "SatoshiNotMe",
          "text": "You can also easily hook up Claude Code, Codex CLI or similar CLI agents with local LLMs, and leverage the built in web search tools. Simple guide here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md",
          "score": 2,
          "created_utc": "2026-01-12 11:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6ahqz",
          "author": "irodov4030",
          "text": "https://preview.redd.it/82qiiep1rxcg1.png?width=2005&format=png&auto=webp&s=b2a9e3fe6a6b8925311cd3514c614435e902d1f4\n\nBult this for myself!\n\ninternet access using ollama models. \n\nIt uses Flask, and the frontend runs in the web browser. Execution trace is still in works. I am using a macbook M1-8GB RAM.",
          "score": 2,
          "created_utc": "2026-01-12 15:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8bp9a",
          "author": "-Akos-",
          "text": "Look for MCP, which LM Studio supports. This is a standardized way of these tools. File reading/writing, database connections, docker interaction, etc. r/mcp is its own channel. And if you want to make your own, fastmcp in python is a good one.",
          "score": 2,
          "created_utc": "2026-01-12 20:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8lyqv",
          "author": "IrisColt",
          "text": "How to do this with open source?",
          "score": 2,
          "created_utc": "2026-01-12 21:44:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3ri81",
          "author": "Agreeable-Market-692",
          "text": "try exa and deepwiki next\n\nnote that the exa requires API key, just visit their website for a freebie   \n  \n{\n\n  \"mcpServers\": {\n\n\"exa\": {\n\n\"command\": \"npx\",\n\n\"args\": \\[\"-y\", \"exa-mcp-server\"\\],\n\n\"env\": {\n\n\"EXA\\_API\\_KEY\": \"your-api-key-here\"\n\n}\n\n}\n\n  }\n\n}\n\n\\###deepwiki mcp below\n\n{\n\n  \"mcpServers\": {\n\n\"deepwiki\": {\n\n\"serverUrl\": \"https://mcp.deepwiki.com/sse\"\n\n}\n\n  }\n\n}",
          "score": 3,
          "created_utc": "2026-01-12 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz44r2o",
              "author": "ryfromoz",
              "text": "Bright api ftw",
              "score": 3,
              "created_utc": "2026-01-12 05:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz44wly",
                  "author": "Agreeable-Market-692",
                  "text": "just googled that, didn't know what it was, that actually looks really interesting thankyou",
                  "score": 2,
                  "created_utc": "2026-01-12 05:33:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4tcx2",
          "author": "ljubobratovicrelja",
          "text": "I've been experimenting with this recently as well, and also blown away by this idea. I recently implemented this as /browse command within my project [tensor-truth](https://github.com/ljubobratovicrelja/tensor-truth) \\- basically, an 8b model given 5-6 sensible web sources well parsed would give an amazing summary.",
          "score": 1,
          "created_utc": "2026-01-12 09:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz62t3x",
          "author": "Unusual_Delivery2778",
          "text": "Awesome thread. Iâ€™ve been approximating some of the solutions / conclusions here for a while now, just letting my intuition run wild as I learn more and making some investments alongside it.",
          "score": 1,
          "created_utc": "2026-01-12 14:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztal7f",
          "author": "NovatarTheViolator",
          "text": "Try using cursor with codex extension, configured to utilize a local model like Qwen2.5-Coder-32B-Instruct-AWQ.  Have VLLM run the local model in Docker",
          "score": 1,
          "created_utc": "2026-01-15 22:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4o0fp",
          "author": "jikilan_",
          "text": "The more you use the less wow u r. U are quite lucky to get a usable response",
          "score": 0,
          "created_utc": "2026-01-12 08:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4pmcq",
          "author": "SheepherderOwn2712",
          "text": "I've tried pretty much all web/grounding apis and tools out there now but what I have found the best is [https://lmstudio.ai/valyu/valyu](https://lmstudio.ai/valyu/valyu) \n\nOnly one that is built natively for tool-calling, and it returns full content instead of very small snippets-and also have stuff like live stock prices which is cool",
          "score": -3,
          "created_utc": "2026-01-12 08:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4sm9b",
          "author": "Ornery-Egg-4534",
          "text": "You should try Valyuâ€™s tool for search. Pretty sure its the best one out there. They have benchmarks as well",
          "score": -2,
          "created_utc": "2026-01-12 09:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3obvg",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-12 03:46:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3p6bh",
              "author": "arcanemachined",
              "text": "That is not possible. It must be doing some tool call. And I'm saying this in the hope that you'll prove me wrong.",
              "score": 18,
              "created_utc": "2026-01-12 03:50:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3wb59",
                  "author": "SM8085",
                  "text": "It's in the chat template, for example: [https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF?chat\\_template=default#L264](https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF?chat_template=default#L264)\n\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\n\nSomeone in another thread was asking how to add that to their LM studio jinja so that all bots can be so coherent.\n\nI think that's Python?  The strftime\\_now() function?  I don't mess with chat templates that much.\n\nBut making that call is how they add it.",
                  "score": 8,
                  "created_utc": "2026-01-12 04:33:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3v0vr",
                  "author": "-InformalBanana-",
                  "text": "Probably it is in the system prompt. There is something like {{CURRENT_DATE}} to automatically get current date and put it in prompt. (Edit: or in chat template)",
                  "score": 3,
                  "created_utc": "2026-01-12 04:24:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3tsgo",
                  "author": "EbbNorth7735",
                  "text": "It's impossible without context being fed into it.",
                  "score": 4,
                  "created_utc": "2026-01-12 04:17:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz47frm",
                  "author": "redragtop99",
                  "text": "Ask it what the date is, it will tell you.  \n\nFunny all these downvotes when Iâ€™m right lolâ€¦.\n\nAnyone can try it for themselves.\n\nLoad up GPTOSS 120Bâ€¦ What is the date today?\n\nToday is â€œCurrent dateâ€",
                  "score": -6,
                  "created_utc": "2026-01-12 05:52:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5b2wd",
              "author": "Pvt_Twinkietoes",
              "text": "Bro. Just pass it the date within the prompt during your API call.",
              "score": 1,
              "created_utc": "2026-01-12 11:49:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6a4oi",
          "author": "__108",
          "text": "Tried Exa, Valyu and Tavily. Imo Valyu seems to be the best for price but also how well the responses are, especially for deep research. It does a good job of web access but it also gives access to a lot of other sources as well such as academic papers, patents, stocks etc. It has become an irreplaceable part of my workflow tbh",
          "score": -1,
          "created_utc": "2026-01-12 15:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4pwhq",
          "author": "Feeling-Machine-4804",
          "text": "valyu is great fr this: [https://lmstudio.ai/valyu/valyu](https://lmstudio.ai/valyu/valyu)",
          "score": -5,
          "created_utc": "2026-01-12 08:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4vtmf",
          "author": "bar_raiser333",
          "text": "I know that Valyu has an LLM Studio plugin. It does tool calling really well. Feel free to try it [https://lmstudio.ai/valyu/valyu](https://lmstudio.ai/valyu/valyu)",
          "score": -5,
          "created_utc": "2026-01-12 09:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3soic",
          "author": "hkd987",
          "text": "I totally get your excitement about combining local LLMs with internet search! It really opens up so many possibilities for real-time insights and enhanced functionality. If you're exploring options, you might find LlamaGate interesting as it offers access to various open-source LLMs with a simple API, which could help bridge that gap. Check it out if you want to dive deeper: https://llamagate.dev/",
          "score": -16,
          "created_utc": "2026-01-12 04:10:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2sfwx",
      "title": "ElevenLabs is killing my budget. What are the best \"hidden gem\" alternatives for documentary style TTS?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "author": "Ancient_Routine8576",
      "created_utc": "2026-01-03 11:31:31",
      "score": 234,
      "num_comments": 126,
      "upvote_ratio": 0.88,
      "text": "Hi everyone, I'm running a YouTube channel focused on \"War Economics\" and \"History\". I've been using ElevenLabs (Marcus voice) and the quality is amazing, but the pricing is unsustainable for long-form content (8-10 min videos).\n\nI've tried the usual suspects (Murf, Play.ht) but they sound too robotic or corporate.\n\n**I am looking for:**\n\n1. Something with a dark, authoritative, documentary-style tone.\n2. Either a cheaper paid alternative OR a high-quality GitHub/Local solution (I have a decent GPU if needed, like RVC or Tortoise).\n3. Has anyone tried tools like **Fish Audio** or **OpenAI TTS API** wrappers?\n\nAny \"underground\" or lesser-known recommendations would be appreciated. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q2sfwx/elevenlabs_is_killing_my_budget_what_are_the_best/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nxf98e3",
          "author": "MixtureOfAmateurs",
          "text": "The best local options are:\n\nSoprano - fast \n\nKokoro - fast \n\nVibevoice\n\nXTTS v2 still somehow \n\nF5 tts.\n\nBasically just look through here and listen to samples [https://huggingface.co/models?pipeline\\_tag=text-to-speech&sort=downloads](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads)",
          "score": 129,
          "created_utc": "2026-01-03 11:38:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxge35i",
              "author": "LocoMod",
              "text": "Soprano is extremely impressive for its size. And it was the first attempt by the undergrad student that made it. Can't wait to see how their work evolves.",
              "score": 50,
              "created_utc": "2026-01-03 15:49:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmvxp",
                  "author": "Foreign-Beginning-49",
                  "text": "Absolutely kind of that person, this community gives me hope for a human future inhabited by said humans working together or by themselves for the greater s3cret creature we are in truest sense of the word, some creature for certain.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:31:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2ksnq",
                  "author": "silenceimpaired",
                  "text": "Itâ€™s only one voice?",
                  "score": 1,
                  "created_utc": "2026-01-06 20:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgzbyx",
              "author": "aedocw",
              "text": "You missed chatterbox which is at 11labs quality for cloning, much better than xtts.",
              "score": 38,
              "created_utc": "2026-01-03 17:29:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxhqx1j",
                  "author": "trialgreenseven",
                  "text": "could you tell me if I can use voice cloning based on sample w/o verification step like 11labs on chatterbox/resemble?",
                  "score": 3,
                  "created_utc": "2026-01-03 19:34:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxn12yb",
                  "author": "claytonjr",
                  "text": "Yeah, 2nd vote for chatterbox. I've used it extensively for YouTube narration videos, with a cloned voice. Plus the fast api makes it easy to use with automationÂ ",
                  "score": 2,
                  "created_utc": "2026-01-04 15:37:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxrse6z",
                  "author": "SituationMan",
                  "text": "Not even close. It randomly breaks into accents.",
                  "score": 2,
                  "created_utc": "2026-01-05 05:52:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxhuigs",
                  "author": "MetricZero",
                  "text": "That thing is so ridiculously difficult to set up if you don't know what you're doing. You need to like create a contained environment with a specific install order for Gradios, Numpy, and some other stuff with specific versions and THEN it might work. I couldn't get it to though.",
                  "score": 2,
                  "created_utc": "2026-01-03 19:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxh8ada",
              "author": "Segaiai",
              "text": "I'm personally a big fan of [IndexTTS2](https://github.com/index-tts/index-tts). I hardly hear people talk about it, but I've gotten some really cool results. Especially since you can prompt a mood/situation separately from the spoken text itself. And you can get granular with emotion sliders, or match the emotion of the input file.",
              "score": 15,
              "created_utc": "2026-01-03 18:10:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxmgug0",
                  "author": "djtubig-malicex",
                  "text": "Yeah IndexTTS2 is being slept on for whatever reason.  Though I think the use case is more for dubbing existing audio given it relies on similar audio input and from my uses of it, does require quite a bit of memory to use effectively for long form gens.  (Running on M3 Ultra 256GB RAM :) )",
                  "score": 5,
                  "created_utc": "2026-01-04 13:46:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxfgjsa",
              "author": "rpg36",
              "text": "Vibevoice is great! I've been experimenting with it lately and it sounds awesome. It was really simple to run the example code. They even have a little simple web app you can run. No issues running it on my 4070 Super with 16GBs VRAM",
              "score": 10,
              "created_utc": "2026-01-03 12:36:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfmq78",
                  "author": "1427538609",
                  "text": "Last time I checked vibe voice there are only community forks?",
                  "score": 6,
                  "created_utc": "2026-01-03 13:18:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs184v",
              "author": "ShengrenR",
              "text": "Absolutely criminal not to mention higgs v2 or index tts2 in this list imo; maybe because they're a bit heavier to run, so fewer do.  I've seen a few folks keep XTTSv2 in the running lately and I just don't hear it myself.. it was great when it first came out, but it's far behind the latest models, unless you're going to be heavily fine-tuning it I don't see the appeal.",
              "score": 1,
              "created_utc": "2026-01-05 07:04:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny2jvuo",
                  "author": "silenceimpaired",
                  "text": "I cannot recall why I lost interest in Index TTS 2 â€¦ Iâ€™ll have to look at it again. Not a fan of the Higgs license but Iâ€™ll have to take another look.",
                  "score": 1,
                  "created_utc": "2026-01-06 20:20:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfdppp",
          "author": "CheatCodesOfLife",
          "text": "VibeVoice if you don't want to write code / just want to give it the transcript.\n\nEcho-TTS if you can work around the 30-second limitation.\n\n\nI'd give Maya-1 if you want to act like a director, eg. put \"documentary domain\" in the description prompt. More code required to generate an 8-10 minute transcript.",
          "score": 30,
          "created_utc": "2026-01-03 12:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfmzhc",
          "author": "1427538609",
          "text": "The Chinese index-TTS2 is quite good, but you have to seed it with a voice example that you like",
          "score": 14,
          "created_utc": "2026-01-03 13:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg5f7j",
          "author": "Finguili",
          "text": "From local TTS, VibeVoice Large seems to have highest ceiling, but the model is very unstable. With one generation it sounds as if text was almost professionally narrated; with another its prosody is so bad that you start to wonder is it the same model. It also loves to add strange music to the background. So expect to reroll a lot.\n\nI donâ€™t have much experience with cloud apis, but Gemini 2.5 Pro TTS sounded to me better than ElevenLabs and should be cheaper.",
          "score": 13,
          "created_utc": "2026-01-03 15:05:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhyrsm",
              "author": "PitifulTeacher4972",
              "text": "you can also remove the music by post processing the output with another model",
              "score": 2,
              "created_utc": "2026-01-03 20:13:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxsakcf",
              "author": "ArtfulGenie69",
              "text": "A lot of the various tts have this issue. Higgs has it too but when you get the best voice sample possible. Basically 30s of exactly the same tones it cloned almost 100%. So you could take a samples from the good generation that it made. If your original sample has trash sound in it try to remove it with something like pyoise or uvr. As far as I've heard for English speakers higgs seemed to clone the best. The samples I heard from vibe weren't as close to the speakers voice for whatever reason. Oh and some characters in text screw up higgs so you have to clean those out or get more weird noises. All the tts are iffy right now and take a bit of tuning.\n\n\nThere is also new stuff to try. Like indextts2 and cosyvoice3",
              "score": 1,
              "created_utc": "2026-01-05 08:30:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfxid1",
          "author": "IONaut",
          "text": "Currently I think VibeVoice large is the best most natural sounding option. You could even give it a sample of the voice you like from ElevenLabs and clone it that way.  \n  \nChatterbox just came out with a new version (2) that is super lightweight and fast that works pretty well but you have to mess with the settings a bit to get the clone to sound right.",
          "score": 12,
          "created_utc": "2026-01-03 14:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjhq5r",
              "author": "AXYZE8",
              "text": "Where did you found that '2' version?\n\nThere's no mention of such thing in Google Search results [https://www.google.com/search?q=%22chatterbox+2%22+ai](https://www.google.com/search?q=%22chatterbox+2%22+ai)\n\nUnless you're talking about Chatterbox Turbo, but it's not a successor (2), it's just a distilled small version meant for voice agents. It's a bad choice for generating audio in advance (a lot worse quality), OP doesn't need realtime responses.",
              "score": 2,
              "created_utc": "2026-01-04 00:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxji6sx",
                  "author": "IONaut",
                  "text": "The place that I found the link was calling chatterbox turbo chatterbox 2. But yes that is the one I'm talking about",
                  "score": 1,
                  "created_utc": "2026-01-04 00:54:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfsv30",
          "author": "Jean_velvet",
          "text": "Google is about to smash Elevenlabs out of the water. It's not deployed in Gemini but they have cutting edge voice synthasis in labs.",
          "score": 22,
          "created_utc": "2026-01-03 13:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpk4u",
              "author": "Silver-Champion-4846",
              "text": "Is it the Chirp3 voices or something newer and better?",
              "score": 8,
              "created_utc": "2026-01-03 16:43:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgrl0m",
                  "author": "Jean_velvet",
                  "text": "Maybe, I'm talking about the voices in AI studio. You can literally start a podcast with 2 AI voices and interact yourself as a caller. It's really good",
                  "score": 12,
                  "created_utc": "2026-01-03 16:53:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxg4psv",
          "author": "ducksoup_18",
          "text": "https://github.com/justinlime/Fatterbox",
          "score": 7,
          "created_utc": "2026-01-03 15:02:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxjar0p",
              "author": "Yorn2",
              "text": "Nice to have another optimized version out there for streaming.",
              "score": 1,
              "created_utc": "2026-01-04 00:15:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgmmth",
          "author": "DigiJoe79",
          "text": "Hi u/Ancient_Routine8576 I played also a lot with Vibevoice lately. Strongly recommend it, it it amazing. I guess my Audiobook Maker isn't a perfect fit for you use case, but maybe you try to pull just the Vibevoice Engine container. It includes a fastapi server and can also run standalone with whatever workflow you use right now. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
          "score": 5,
          "created_utc": "2026-01-03 16:30:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny556l8",
              "author": "Anomalia_YT",
              "text": "I need to give yours a go. Iâ€™ve tried a whole bunch of repos for TTS longform and this is by far the best Iâ€™ve tried: https://github.com/psdwizzard/chatterbox-Audiobook\n\nI love that yours offers vibe voice 7b though. With the audiobookmaker Iâ€™m using, you can regenerate chunks if needed which I find invaluable. Unsure if yours has that feature. Iâ€™d love to be able to add any model I want to play around with (a bit like ultimate TTS but most models Iâ€™ve tried are not reliable at all and umtimatetts lacks many features. Echo TTS looks interesting though..\n\nEither way your project looks very interesting and Iâ€™ll hopefully try it out tomorrow !",
              "score": 1,
              "created_utc": "2026-01-07 04:20:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyenzz8",
                  "author": "DigiJoe79",
                  "text": "Thanks! Yes, segment regeneration is a core feature - each segment can be regenerated individually or you can re-run entire chapters. There's also quality analysis (Whisper transcription comparison) and/or Silera VAD audio analysis to find segments that need attention.\n\nFor adding new models: the engine system is designed for this. Currently XTTS, Chatterbox, and VibeVoice are available as Docker images. Adding a new engine means creating a small wrapper server that speaks the TTS API (there are templates in the engine repo). If you're comfortable with Docker, it's pretty straightforward to add your own.\n\nWould be interested to hear how it compares after you try it!",
                  "score": 1,
                  "created_utc": "2026-01-08 14:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfrcl9",
          "author": "shaakz",
          "text": "i would suggest echo-tts-base. Supports voice cloning and does it very well. Sub 12gb vram and way faster than realtime on a 5070ti.",
          "score": 5,
          "created_utc": "2026-01-03 13:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0tm42",
              "author": "Bandit174",
              "text": "Same, I was pretty impressed with echo as well. Very underrated imo.",
              "score": 1,
              "created_utc": "2026-01-06 15:37:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgl8n1",
          "author": "2_two_two",
          "text": "Iâ€™ve been working on a project using Kokoro-TTS. It works well and easy enough to use. Not sure about others but Kokoro lets you blend voices so that you can use the default or create your own. Now Iâ€™m working on pacing, pauses, and tuning so it doesnâ€™t sound like robots.",
          "score": 4,
          "created_utc": "2026-01-03 16:23:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhn62t",
              "author": "mister2d",
              "text": "Nice. I created my own local streaming TTS  project with Kokoro-TTS before discovering this one: https://github.com/eduardolat/kokoro-web\n\nI've been enjoying excellent local streaming tts for a few months without any issues. This tiny model is great.",
              "score": 2,
              "created_utc": "2026-01-03 19:17:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxrafeo",
          "author": "QikoG35",
          "text": "VibeVoice large, MIT license, with custom nodes! Mind blowing how authentic it sounds and open source.\n\nA reason Microsoft pull it back!",
          "score": 4,
          "created_utc": "2026-01-05 03:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtwg75",
              "author": "bondaly",
              "text": "Is it still pulled back in any way?",
              "score": 1,
              "created_utc": "2026-01-05 15:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx2xjl",
                  "author": "misterflyer",
                  "text": "No, what he meant is that they released the 7B version with an MIT license. Then Microsoft quickly deleted it from their HF repo *(not before it got forked tho)*. The full weights 7B model is still out there...\n\n[https://huggingface.co/aoi-ot/VibeVoice-Large](https://huggingface.co/aoi-ot/VibeVoice-Large)\n\n[https://github.com/rsxdalv/VibeVoice](https://github.com/rsxdalv/VibeVoice)",
                  "score": 2,
                  "created_utc": "2026-01-06 00:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxgy867",
          "author": "Noiselexer",
          "text": "Tell me your channel so I can block it please.",
          "score": 36,
          "created_utc": "2026-01-03 17:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxg18c0",
          "author": "imonlysmarterthanyou",
          "text": "I have been liking VoxCPM. You can clone whatever voice and tone you like.",
          "score": 3,
          "created_utc": "2026-01-03 14:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxil630",
              "author": "bassgojoe",
              "text": "VoxCPM 1.5 gave me much higher quality results compared to chatterbox, Iâ€™m surprised itâ€™s not more well known.",
              "score": 2,
              "created_utc": "2026-01-03 22:03:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgyph4",
          "author": "GabryIta",
          "text": "Chatterbox?",
          "score": 3,
          "created_utc": "2026-01-03 17:26:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhblay",
          "author": "hz55555",
          "text": "Check out inworld.  Much cheaper https://artificialanalysis.ai/text-to-speech/leaderboard",
          "score": 3,
          "created_utc": "2026-01-03 18:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhlaxd",
          "author": "therealtonyryantime",
          "text": "Anyone have a good way of automatically filtering out YouTube channels like this?",
          "score": 16,
          "created_utc": "2026-01-03 19:08:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxiksq3",
              "author": "ScoreUnique",
              "text": "Can be an indexed db of videos as a chrome extension, giving away idea for a business btw",
              "score": 5,
              "created_utc": "2026-01-03 22:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxfjrp7",
          "author": "Impressive-Sir9633",
          "text": "1. You can try the free unlimited KokoroTTS at https://freevoicereader.com or the Chrome extension. You will have to download the model and everything is processed within your browser (using webGPU). I prefer KokoroTTS, but you can try Supertonic as well.\n\n2. If you want additional voice options, you can try the paid version that has a free 3 day trial.",
          "score": 6,
          "created_utc": "2026-01-03 12:59:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf8u5f",
          "author": "shanehiltonward",
          "text": "Check out the projects on Pinokio.",
          "score": 4,
          "created_utc": "2026-01-03 11:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxft8f7",
          "author": "Head-Leopard9090",
          "text": "Vibevoice large",
          "score": 2,
          "created_utc": "2026-01-03 13:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh65vq",
          "author": "martinerous",
          "text": "In addition to the mentioned ones, I have also tried VoxCPM 1.5. It's quite fast, especially on nanovllm (tried on WSL2). BTW, I also trained VoxCPM to my native Latvian language (and Chatterbox too), and it was surprisingly easy to do with the bundled scripts. Of course, VoxCPM cannot beat VibeVoice quality, but VoxCPM seems more stable when provided a voice to clone, and they also have a protection logic built in to throw away obviously bad generations.",
          "score": 2,
          "created_utc": "2026-01-03 18:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxif1ij",
          "author": "HelpfulHand3",
          "text": "For paid options, Inworld with their Max tts model is in my opinion better than ElevenLabs 2.5 and is 10x cheaper. The value for their service is quite frankly absurd.\n\n[https://inworld.ai/pricing](https://inworld.ai/pricing)\n\nLocal models.. Higgs Audio V2, Echo TTS, Vibevoice.",
          "score": 2,
          "created_utc": "2026-01-03 21:33:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkkkk1",
          "author": "rc_ym",
          "text": "I settled on Kokoro using a mixture of voices (using am\\_onyx to give it base).  Then post-processing the audio using [**pedalboard**](https://github.com/spotify/pedalboard) to add some more warmth and simulate a little room noise to make it sound more realistic.  Best speed and quality combo.  Chatterbox had the best quality but was super slow. \n\nI need to try vibevoice after reading the comments here.",
          "score": 2,
          "created_utc": "2026-01-04 04:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlr5p1",
          "author": "DocHollidaay",
          "text": "I was on ElevenLabs but they closed my account (ToS), I spent this week trying different TTS, Pinokio All-In-Ones and manual installs of the 'top 5' TTS, I'm using this to create audiobooks, but this still maybe of use:\n\n[https://github.com/rsxdalv/TTS-WebUI](https://github.com/rsxdalv/TTS-WebUI) is highly recommended, it supports a ton of TTS to try out, easy to install and updated often.\n\nI use Chatterbox 0.5B Multilingual, in TTS-WebUI it supports Nvidia Blackwell or thats the first time I got it working at max speed: I went from 35it/s (roughly realtime) to 210it/s (x6) on 5070 Ti and it does a better job than ElevenLabs!\n\nApparently the Multilingual version is better for flow and handles punctuation better than standard 0.5B, after listening to Tubro (sounds a bit compressed), new Turbo (sounds less compressed), standard (very good, accent drifts sometimes, struggles with 'novel punctuation').\n\nAny glitches tend to be something to do with the text sent, so it may need a cleanup step or very long sentences and chunk tweaking.\n\nI find the voice style comes from the training clip, it will match pacing etc.",
          "score": 2,
          "created_utc": "2026-01-04 10:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsdxep",
          "author": "Forward_Artist7884",
          "text": "cosyvoiceV3 is pretty good, you just need a voice sample. It's better than XTTSV2 and support style inputs.",
          "score": 2,
          "created_utc": "2026-01-05 09:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj6an0",
          "author": "SuperCaptainMan",
          "text": "AI slop YouTube channel?",
          "score": 4,
          "created_utc": "2026-01-03 23:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgct6o",
          "author": "Virtamancer",
          "text": "Has anyone made a longform TTS gui solution yet (local)?\n\nMy use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI donâ€™t care about cloning voices or other gooner shit, I just want to be able to feed a chapter or even a whole book and have it generate the audio file.",
          "score": 2,
          "created_utc": "2026-01-03 15:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxglin4",
              "author": "DigiJoe79",
              "text": "Hi u/Virtamancer \\- You can try my audiobook-maker. In the latest version, there is also a prebuild VibeVoice container with 1.5B and 7B support. [https://github.com/DigiJoe79/AudioBook-Maker](https://github.com/DigiJoe79/AudioBook-Maker)",
              "score": 4,
              "created_utc": "2026-01-03 16:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxgmyyi",
                  "author": "Virtamancer",
                  "text": "At a quick glance this looks very promising, and excellent documentation! I canâ€™t wait to get to my computer and try it.\n\nA couple questions:\n\n- How much vram is needed?\n\n- How long would, say, a 300 page book take to do TTS? Using 1x or 2x RTX 4090.",
                  "score": 1,
                  "created_utc": "2026-01-03 16:31:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxno49h",
                  "author": "gallito_pro",
                  "text": "Hi, thanks for your app, but Engines dont load at the start!!! Help please. Can be related to nvidia drivers?",
                  "score": 1,
                  "created_utc": "2026-01-04 17:24:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny2fgxw",
                  "author": "silenceimpaired",
                  "text": "Do you prefer vibevoice to chatterbox?",
                  "score": 1,
                  "created_utc": "2026-01-06 19:59:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxgxbsn",
              "author": "evia89",
              "text": "> My use case is creating audiobooks for myself when they arenâ€™t available for free.\n\nI use edge for it. OG https://edgetts.github.io/ or my fork with multiple voices https://vadash.github.io/EdgeTTS/\n\nBoth opensource u can fork and edit if u need more features\n\nCan do 40h book in ~2 hours",
              "score": 1,
              "created_utc": "2026-01-03 17:20:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxh4sgh",
                  "author": "Virtamancer",
                  "text": "Hmm, doesn't work for me.\n\n    [00:00:00] Loaded: textfile.txt\n    [00:00:00] Saving to: TTS Audiobooks\n\nIt asks me to select the output directory, and then I get that output but no files ever actually appear there.",
                  "score": 1,
                  "created_utc": "2026-01-03 17:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjxlx3",
          "author": "dannydonatello",
          "text": "How is a 10 minute video too expensive with elevenlabs? Should be no more than 2-3 USD max.",
          "score": 2,
          "created_utc": "2026-01-04 02:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkxpy5",
              "author": "Buzzard",
              "text": "You're not thinking with AI.  You're assuming they care about the content and put lots of work into the video, so comparatively $2-3 seems really small.\n\nTo them, $2-3 is unsustainable because they haven't put any effort at all.",
              "score": 10,
              "created_utc": "2026-01-04 06:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlguve",
                  "author": "ReachingForVega",
                  "text": "And the intent is probably to only make a couple dollars in views per slop vid.Â ",
                  "score": 3,
                  "created_utc": "2026-01-04 08:52:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxk3mok",
              "author": "shadowninjaz3",
              "text": "thats pretty expensive considering inworld and fish audio are 10x cheaper than eleven labs",
              "score": 1,
              "created_utc": "2026-01-04 02:53:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxlqnfh",
                  "author": "dannydonatello",
                  "text": "I just think itâ€™s BS that 2-3 USD per video is making his YouTube project unsustainable.",
                  "score": 2,
                  "created_utc": "2026-01-04 10:21:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfllid",
          "author": "rom16384",
          "text": "Have you tried Gemini 2.5 Pro Preview TTS? Choosing one of their deeper voices and setting the tone via a prompt should give good results.",
          "score": 4,
          "created_utc": "2026-01-03 13:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgpp2u",
              "author": "Silver-Champion-4846",
              "text": "That got closed on free tier recently, even the flash version became unavailable in the google ai studio website!",
              "score": 2,
              "created_utc": "2026-01-03 16:44:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxgw9l5",
          "author": "nabuachaem",
          "text": "for me, pip TTS and use the XTTS v2",
          "score": 1,
          "created_utc": "2026-01-03 17:15:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgz2ok",
          "author": "Spare-Object3993",
          "text": "You should try gradium",
          "score": 1,
          "created_utc": "2026-01-03 17:28:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhml0h",
          "author": "HotDoshirak",
          "text": "Chatterbox + FlashSR",
          "score": 1,
          "created_utc": "2026-01-03 19:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvdp1",
          "author": "hidden2u",
          "text": "My current workflow is maya1 to generate a unique voice sample, then clone it with chatterbox audiobook",
          "score": 1,
          "created_utc": "2026-01-03 19:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhvhoo",
          "author": "arthurtully",
          "text": "Edge TTS probably the best solution.",
          "score": 1,
          "created_utc": "2026-01-03 19:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxj09i3",
          "author": "llamabott",
          "text": "I feel compelled to plug my own app here:\n\n[https://github.com/zeropointnine/tts-audiobook-tool](https://github.com/zeropointnine/tts-audiobook-tool)\n\nIt has support for eight (!) different TTS models.\n\nMiraTTS\n\nGLM-TTS\n\nIndexTTS2\n\nVibeVoice 1.5B\n\nHiggs Audio V2\n\nFish OpenAudio S1-mini\n\nChatterbox-Multilingual\n\nOute TTS\n\nOkay thanks.",
          "score": 1,
          "created_utc": "2026-01-03 23:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjhfdu",
          "author": "jjsilvera1",
          "text": "I say maybe because I dont know, but google docs has quite a few good voices on there. One might be interesting? You could also mess with audio setting like pitch, timbre.",
          "score": 1,
          "created_utc": "2026-01-04 00:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjxfgm",
          "author": "pbalIII",
          "text": "The voice cloning angle is key for your use case. VibeVoice and F5-TTS both support cloning from audio samples, so you could grab a few clips of the Marcus voice you like and use that as your reference.\n\nF5-TTS is probably the sweet spot... 330M params, solid quality, and the cloning is legit good. Kokoro is faster but doesn't do voice cloning, so you'd be stuck with its built-in voices.\n\nOne thing worth trying: feed it a 10-15 second sample of the exact ElevenLabs output you want to match. The closer your reference audio is to the target tone, the better the clone. I'd start with F5 before going to VibeVoice 7B since the VRAM requirements are more reasonable.",
          "score": 1,
          "created_utc": "2026-01-04 02:18:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2eft0",
              "author": "silenceimpaired",
              "text": "Iâ€™m sad F5 was never re-released with an open license.",
              "score": 1,
              "created_utc": "2026-01-06 19:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxkpm0o",
          "author": "Unlikely_Shake8208",
          "text": "Chatterbox is the best local TTS that I have used.",
          "score": 1,
          "created_utc": "2026-01-04 05:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlffo4",
          "author": "crantob",
          "text": "Which tts will let me apply a graph of prosidy/emphasis to the text?  I need to recreate an original speech with similar timing, pitch, emphasis.",
          "score": 1,
          "created_utc": "2026-01-04 08:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxlp6tz",
          "author": "k2ui",
          "text": "How much does eleven labs cost for you to do 8-10 min content?",
          "score": 1,
          "created_utc": "2026-01-04 10:08:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmkflo",
          "author": "MoffKalast",
          "text": ">  long-form content\n\n>  8-10 min",
          "score": 1,
          "created_utc": "2026-01-04 14:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxoakcq",
          "author": "coastisthemost",
          "text": "Ttswebui is great",
          "score": 1,
          "created_utc": "2026-01-04 19:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqnkgv",
          "author": "MensaForever4117",
          "text": "Speechify",
          "score": 1,
          "created_utc": "2026-01-05 01:52:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5z0jw",
          "author": "Mediocre-Waltz6792",
          "text": "echo is better than chatterbox IMO. But you'll need 10 GB of Vram for it. There are ways to make it work on 8 GB.",
          "score": 1,
          "created_utc": "2026-01-07 08:12:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyklhz9",
          "author": "Far_Noise_5886",
          "text": "My buddy made this. https://github.com/EmZod/speak . Runs fully locally and clones any voice, written up as an agent skill, so works with any AI agent . Very good imo!",
          "score": 1,
          "created_utc": "2026-01-09 10:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxh5tld",
          "author": "jordanpwalsh",
          "text": "I've been working on this: [https://nemoreader.jordanwal.sh](https://nemoreader.jordanwal.sh)\n\nThe version up on the App Store now uses OpenAI which breaks the bank, I have a new version in testing now that uses Higgs V2 (https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base) running on runpod infrastructure and a simple API. My plan is to open source that bit once it's stable for folks to plug in that don't want to pay the in app tts, but I could share what I have now if you want to play with the API.\n\nIt sounds pretty good! perfectly acceptable for the audible type clone I've been messing with.",
          "score": 1,
          "created_utc": "2026-01-03 17:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn1vpd",
          "author": "IronColumn",
          "text": "you should try to do a good job with the things you create",
          "score": 1,
          "created_utc": "2026-01-04 15:41:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0zst6",
      "title": "Upstage Solar-Open-100B Public Validation",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/w789uyo0cpag1.jpeg",
      "author": "PerPartes",
      "created_utc": "2026-01-01 08:52:25",
      "score": 233,
      "num_comments": 70,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx2bcn4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-01 11:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1zsaj",
          "author": "CKtalon",
          "text": "Why a location? Just release on the Internet.",
          "score": 126,
          "created_utc": "2026-01-01 08:56:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1zvf8",
              "author": "spectralyst",
              "text": "Gangnam Style",
              "score": 134,
              "created_utc": "2026-01-01 08:57:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx34al3",
                  "author": "AuspiciousApple",
                  "text": "Koreans love their pop up stores.",
                  "score": 12,
                  "created_utc": "2026-01-01 14:56:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7ps0n",
                  "author": "Mikasa0xdev",
                  "text": "Oppa LocalLLaMA Style!",
                  "score": 3,
                  "created_utc": "2026-01-02 06:43:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx2a34t",
                  "author": "DecodeBytes",
                  "text": "I have the synth intro stuck in my head now",
                  "score": 3,
                  "created_utc": "2026-01-01 10:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2eddx",
              "author": "keepthepace",
              "text": "Sadly, that's still how to maximize journalistic coverage, by causing FOMO. Force journalists to get there, you force them to make an article. Publish something online they will be like \"meh, put it on the pile\"",
              "score": 33,
              "created_utc": "2026-01-01 11:31:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2ri55",
                  "author": "-p-e-w-",
                  "text": "I very strongly doubt that journalists are going to bother showing up at some mystery location in Korea to settle some AI startup beef lol.",
                  "score": 3,
                  "created_utc": "2026-01-01 13:29:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx224td",
              "author": "ttkciar",
              "text": "That would be lovely!",
              "score": 4,
              "created_utc": "2026-01-01 09:22:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2ijts",
              "author": "PerPartes",
              "text": "This is because of huge domestic market focus. In-person event is a matter of trust and respect (esp. in this region). Almost whole SK AI business is focused on itself. In case of Upstage with the addition of Japanese market as well.",
              "score": 9,
              "created_utc": "2026-01-01 12:12:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx33dme",
                  "author": "Nyghtbynger",
                  "text": "Interestingly that's the case of most nations in fact, except a few merchant nations and empires (US,UK,...)",
                  "score": 2,
                  "created_utc": "2026-01-01 14:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx72o2z",
              "author": "dicoxbeco",
              "text": "OOP in Korean *does* state that they will update the post with URL for livestream.\n\nEither the translator OP used skipped that part over, or OOP edited that in later.",
              "score": 1,
              "created_utc": "2026-01-02 03:56:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx21p4c",
          "author": "throwaway-link",
          "text": "I did my own tests. Cossim between layers past the first few seems to be extremely high across any model. Testing layer 45 input layernorm of deepseek v3/v3.1/v3.2-special, kimi k2, and mistral large 3 all give similarities around 0.99. The tested deepseek v3 variants are around 0.99999 with each other. \n\nData from the accusation is entirely expected for a model trained from scratch.",
          "score": 75,
          "created_utc": "2026-01-01 09:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2hccm",
              "author": "llama-impersonator",
              "text": "why are people comparing the norms instead of attn or mlp layers? norms have both low param count and a fairly simple fixed function.",
              "score": 11,
              "created_utc": "2026-01-01 12:01:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2m36d",
                  "author": "throwaway-link",
                  "text": "bc the accusation already says they're different? Their only evidence is norm weights which I show is expected. Probably bc training dynamics for rmsnorm of deeper layers cause the scale to just be a constant value across the weight which obviously results in high cossim. I guess since deeper layers do smaller adjustments, rmsnorm scale doesn't need to do any wild adjustments across the already relatively normalised token vector.",
                  "score": 10,
                  "created_utc": "2026-01-01 12:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3bule",
              "author": "egomarker",
              "text": "Show the code and results. No idea if you are legit or yet another schizo vibecoder with hallucinated \"test results\".",
              "score": 6,
              "created_utc": "2026-01-01 15:41:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx22ua9",
              "author": "KontoOficjalneMR",
              "text": "Almost like all those models are using the similar architecture and similar datasets and you get same-ish output with some small flavour on top. \n\nYou look at the benchmarks and the results are basically a function of amount of parameters with tiny percentage variation based mostly on luck.",
              "score": 16,
              "created_utc": "2026-01-01 09:29:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2i4uv",
                  "author": "DistanceSolar1449",
                  "text": "Thatâ€™sâ€¦ obviously not true. DeepSeek V3, R1, V3.1, V3.2 all have the same param count but much diff performance.",
                  "score": 24,
                  "created_utc": "2026-01-01 12:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx42byg",
              "author": "jinnyjuice",
              "text": "Yeah they're saying that you can't really make such definitive conclusions with cossim. They made comparison with Phi here also: https://github.com/hyunwoongko/solar-vs-glm-vs-phi",
              "score": 2,
              "created_utc": "2026-01-01 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3du9z",
          "author": "kiwibonga",
          "text": "News tomorrow: Upstage employees arrested for beating up some dude in a parking lot.",
          "score": 11,
          "created_utc": "2026-01-01 15:52:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx228oe",
          "author": "ResidentPositive4122",
          "text": "I mean, if this is what it takes to get intermediate checkpoints, let's do it! Llamas, qwens, mistrals, glms, minimaxes, deepseeks, j'accuse! :D",
          "score": 25,
          "created_utc": "2026-01-01 09:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2au1f",
              "author": "pkmxtw",
              "text": "AI labs *hate* this simple trick to get them to release intermediate checkpoints!\n\nEither that or this is some of evil-genius level of marketing.",
              "score": 14,
              "created_utc": "2026-01-01 10:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx2oduo",
              "author": "zball_",
              "text": "Just use different model configuration smh",
              "score": -1,
              "created_utc": "2026-01-01 13:04:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx20qfj",
          "author": "garloid64",
          "text": "op op op",
          "score": 20,
          "created_utc": "2026-01-01 09:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20cjs",
          "author": "PerPartes",
          "text": "I just shared this because recent AI generated post here about the plagiarism claim was removed by the admins. I know the team for approx. 2 years (from the online space) and can hardly believe that it would be true.",
          "score": 34,
          "created_utc": "2026-01-01 09:02:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nx25axo",
              "author": "RuthlessCriticismAll",
              "text": "It seems appropriate to remove that post. It is however galling that similar, evidence free, ai generated posts with the same accusations don't get removed.",
              "score": 18,
              "created_utc": "2026-01-01 09:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26swr",
                  "author": "PerPartes",
                  "text": "Agreed. Hate is always simpler than a deep and independent analysis.",
                  "score": 18,
                  "created_utc": "2026-01-01 10:12:19",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx49h78",
                  "author": "rm-rf-rm",
                  "text": "Please report anything you see that we havent removed. Generally I think we are catching stuff well especially things that are particularly egregious.",
                  "score": 6,
                  "created_utc": "2026-01-01 18:35:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx26a8j",
          "author": "AppearanceHeavy6724",
          "text": "Ahaha, imagine if there will be a literal knuckle fight.",
          "score": 18,
          "created_utc": "2026-01-01 10:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx61575",
              "author": "tengo_harambe",
              "text": "\"The cosine similarity of my fist and your face is about to be -1.00\"",
              "score": 7,
              "created_utc": "2026-01-02 00:08:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7ilyv",
                  "author": "AppearanceHeavy6724",
                  "text": "yeah exactly.",
                  "score": 2,
                  "created_utc": "2026-01-02 05:45:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6rc2d",
          "author": "siegevjorn",
          "text": "Am I reading this right? How the fuck are they going to validate they trained their llm from scratch at Gangnam station? What about just release a white paper about the novelty of their methods?",
          "score": 4,
          "created_utc": "2026-01-02 02:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2uobg",
          "author": "Intrepid_Bobcat_2931",
          "text": "This is a joke. I could see a stunt like \"in person verification\" be reasonable if you gave two weeks notice for people to make travel plans, but they know it's completely impractical for highly experienced people to fly over at a day's notice.",
          "score": 5,
          "created_utc": "2026-01-01 13:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3mgdc",
              "author": "my_name_isnt_clever",
              "text": "If you have to fly there, you're not their target audience. This is for domestic journalism.",
              "score": 13,
              "created_utc": "2026-01-01 16:38:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx718se",
              "author": "dicoxbeco",
              "text": "... What joke?\n\nThis was never meant for English audience. [In fact, the OOP wasn't even written in English.](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK?utm_source=share&utm_medium=member_desktop&rcm=ACoAACoF--MBSnmFDkOdoa7FU_ztI512j0sxTo4) OP went through a translator so you would understand what it says.",
              "score": 2,
              "created_utc": "2026-01-02 03:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx25ssw",
          "author": "PerPartes",
          "text": "[Original Hwalsukâ€™s LI post here](https://www.linkedin.com/posts/upstage-stan_solar-open-100b-%EA%B3%B5%EA%B0%9C-%EA%B2%80%EC%A6%9D%EC%97%90-%EC%B4%88%EB%8C%80-%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4-solar-100b%EA%B0%80-activity-7412403323175370753-2KgK)",
          "score": 3,
          "created_utc": "2026-01-01 10:01:43",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx2hk6u",
          "author": "No_Conversation9561",
          "text": "Damn.. you know what, I believe him",
          "score": 2,
          "created_utc": "2026-01-01 12:03:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20wtr",
          "author": "texasdude11",
          "text": "Tbh, I don't even care about this... If I need a model in this class, I can pick prime intellect, gpt-oss-120b, qwen3-next or move up a class and go to qwen3-235b or Minimax-m2.1 this 100b market is so competitive that you really need to stand out for adoption. Zai, Qwen and OpenAI's censored gpt-oss-120b kinda rule that 80-120b.\n\nAll that being said, more competition is always welcome though! I'd love to see a llama5 120B or a DeepSeek 200b model. That would be insane!",
          "score": 6,
          "created_utc": "2026-01-01 09:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx26jdf",
              "author": "LittleBlueLaboratory",
              "text": "I have 96GB VRAM (4x 3090). Strix Halo and DGX Spark have 128. This 80B to 120B segment is where its at! The more competition the better!",
              "score": 12,
              "created_utc": "2026-01-01 10:09:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx26pzv",
                  "author": "texasdude11",
                  "text": "Agreed!\n\nI have 2x6000 Pros with 512 GB DDR5 RAM, so I'm a bit lucky there. These 100b size is clearly in consumer reach!",
                  "score": 3,
                  "created_utc": "2026-01-01 10:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx3kbfm",
              "author": "uti24",
              "text": ">this 100b market is so competitive that you really need to stand out for adoption\n\nI want 100B dense model. Is there something besides Meta-Llama-1/2/3-70B? \n\nIt feels not really smart.. On par with other 30B class models like Gemma or Mistral small.",
              "score": 1,
              "created_utc": "2026-01-01 16:26:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3mpca",
                  "author": "my_name_isnt_clever",
                  "text": "Devstral 2 is 123b dense, but it's coding focused. It's far, far more expensive to train large dense models than MoE which is why they're so few and far between these days.",
                  "score": 4,
                  "created_utc": "2026-01-01 16:39:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6n7cx",
                  "author": "Sea-Speaker1700",
                  "text": "They're all complete morons out of the box, every last one. \n\nSetup a proxy between your client and the inference service and tailor the performance to your needs, it can take any \"only yet another info barfing hallucinator model\", aka: every single 100b range model, and turn them into a useful tool.\n\nLoading 100b(ish) and trying to use them direct is a plain old waste of time.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:19:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx274pg",
          "author": "Kooky-Somewhere-2883",
          "text": "Oppa Gangnam Style?",
          "score": 3,
          "created_utc": "2026-01-01 10:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx29jzj",
          "author": "Ok_Condition4242",
          "text": "https://i.redd.it/8dox9ujevpag1.gif\n\nmeanwhile cursor's composer-1",
          "score": 5,
          "created_utc": "2026-01-01 10:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21423",
          "author": "Long_comment_san",
          "text": "Next 50-80b dense would be mindblowing. Someone, please. These total trillions of total parameters are irrelevant when there's a hook to the web.",
          "score": 5,
          "created_utc": "2026-01-01 09:11:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx278zf",
          "author": "NandaVegg",
          "text": "What near Gangnam Station for \"releasing all the intermediate checkpoints and wandbs\"? This is so weird. Can we dance together for a sped up ppongjjak? That would light the mood up. BTW I don't believe the claim that it's a finetune.",
          "score": 2,
          "created_utc": "2026-01-01 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx20648",
          "author": "yuumi_ramyeon",
          "text": "Popcorn",
          "score": 2,
          "created_utc": "2026-01-01 09:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8gatt",
          "author": "PerPartes",
          "text": "I've updated the post with a video link /and seen just a small part of it so far/",
          "score": 1,
          "created_utc": "2026-01-02 10:51:27",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nx4w0rc",
          "author": "7734128",
          "text": "What kind of a joke organization is this?\n\nEvery communication I've seen from them has been bodged like this.\n\nI don't need to inspect weights to know they're a scam when this is the quality of their PR statements.",
          "score": 0,
          "created_utc": "2026-01-01 20:28:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx24dhc",
          "author": "Super_Sierra",
          "text": "Show proof, not text. Idc about twitter post counterclaiming.",
          "score": -1,
          "created_utc": "2026-01-01 09:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx21fw9",
          "author": "Desperate-Sir-5088",
          "text": "Do not blaim the model without any proof. GLM-4.5-Air could count number of 'r' in the \"starbrerry\" correctly.Â \n\n\nWe usually called it \"deadcopy\"",
          "score": -5,
          "created_utc": "2026-01-01 09:14:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}