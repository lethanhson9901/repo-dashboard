{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 756,
    "file_size_bytes": 768550
  },
  "items": [
    {
      "id": "1qp87tk",
      "title": "Kimi K2.5 is the best open model for coding",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/unxlhercm2gg1.jpeg",
      "author": "npc_gooner",
      "created_utc": "2026-01-28 10:54:13",
      "score": 693,
      "num_comments": 202,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27hwzo",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 13:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2761ie",
          "author": "seeKAYx",
          "text": "I worked on a few larger React projects with it yesterday, and I would say that in terms of accuracy, it's roughly on par with Sonnet 4.5... definitely not Opus level in terms of agentic function. My previous daily driver was GLM 4.7, and Kimi 2.5 is definitely better. Now I'm curious to see if [z.ai](http://z.ai) will top that again with GLM-5.",
          "score": 96,
          "created_utc": "2026-01-28 11:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276wsa",
              "author": "michaelsoft__binbows",
              "text": "Curious what would be a good place to get k2.5 on a coding plan. Theyre asking for $12 a month for the low tier which is like 4x what zai offers for theirs.",
              "score": 21,
              "created_utc": "2026-01-28 11:57:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o277p0l",
                  "author": "korino11",
                  "text": "Naaaahh there is a HUGE difference betwween coding plans from zai and kimi. zai -you have a limits with tokens!  Kimi -your limits =calls!\n\nIt means doesn matter 20k of tokens or you just asking smthing with 200tokens.. it all the same a ONE api -call\n\n39$ plan limits from kimi will be empty much sooner than you will use codex for 25$\n\n  \nKimi need to change their STUPID limits based on CALLS",
                  "score": 21,
                  "created_utc": "2026-01-28 12:03:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2abqho",
                  "author": "Torodaddy",
                  "text": "Id just use openrouters and pay per use",
                  "score": 5,
                  "created_utc": "2026-01-28 20:53:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27zjgj",
                  "author": "sannysanoff",
                  "text": "it sucks, unfortunately. Take kimi cli, you ask it a question it makes 5-10 turns (reading files, reading more files, making change, another change).\n\nEach turn is \"1 request\", which counts toward 200 requests / 5 hours and 2000 requests / week.\n\nGLM is definitely more.",
                  "score": 7,
                  "created_utc": "2026-01-28 14:43:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29cvoz",
                  "author": "raidawg2",
                  "text": "Free on Kilo code right now if you just want to try it out",
                  "score": 2,
                  "created_utc": "2026-01-28 18:20:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o277m63",
                  "author": "SourceCodeplz",
                  "text": "Yeah but Z is almost unusable with just 1 req / sec.",
                  "score": 4,
                  "created_utc": "2026-01-28 12:03:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28em8k",
                  "author": "momentary_blip",
                  "text": "Nano-gpt has it.Â  $8/mo for 60K requests to all the open models",
                  "score": 1,
                  "created_utc": "2026-01-28 15:52:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29b2at",
                  "author": "No-Selection2972",
                  "text": "use kimmmy to negociate the price [https://www.reddit.com/r/kimi/comments/1qn6mp6/got\\_it\\_all\\_the\\_way\\_down\\_to\\_099\\_for\\_the\\_first\\_month/](https://www.reddit.com/r/kimi/comments/1qn6mp6/got_it_all_the_way_down_to_099_for_the_first_month/) it's 0.99$",
                  "score": 1,
                  "created_utc": "2026-01-28 18:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cuw0k",
                  "author": "Grand-Management657",
                  "text": "I've been running nano-gpt for months. They have an awesome community and support Kimi K2.5 since release. 60k requests/month which is basically unlimited for me. I've been running it through opencode today and it works flawlessly and honestly on par with Sonnet 4.5 but I still really like Opus 4.5's output quality. But for $8/month, essentially unlimited Sonnet 4.5 is hard to beat. My referral if you want a small discount [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 1,
                  "created_utc": "2026-01-29 04:52:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2783hg",
                  "author": "seeKAYx",
                  "text": "That would actually be too expensive for me, considering the service. For 10$, you get 300 requests with Github Copilot. So I'm just hoping that [z.ai](http://z.ai) will deliver now. I saw somewhere on Twitter that they are already in the training process. So let's just wait and see.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:06:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jadc",
              "author": "MasterSama",
              "text": "is there an abliterated version out there yet, uncensored? the GLM4.7 was great but it gets stuck in a loop from time to time!",
              "score": 5,
              "created_utc": "2026-01-28 18:47:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2abjvy",
                  "author": "Primary-Debate-549",
                  "text": "Yeah I just had to kill a GLM 4.7 on a DGX spark that had been \"thinking\", ie. talking to itself, for about 17 hours. That was extreme, but it really likes doing that for at least 20 seconds anytime I ask it any question.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29f8gs",
              "author": "cmdr-William-Riker",
              "text": "If it's on par with sonnet 4.5, that's incredible",
              "score": 2,
              "created_utc": "2026-01-28 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27z6lz",
              "author": "SilentLennie",
              "text": "I worry GLM-5 isn't going to be open weights, because... they are now on the stock market.",
              "score": 2,
              "created_utc": "2026-01-28 14:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2by8n4",
                  "author": "Exciting_Garden2535",
                  "text": "How are these two statements: \"being in open-market\", \"non-releasing open weight models\" connected?\n\nAlibaba has been on the stock market for ages, yet their Qwen models are open weights.\n\nAnthropic is a private company and never releases even a tiny model.",
                  "score": 2,
                  "created_utc": "2026-01-29 01:41:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2azypg",
              "author": "Most-Tennis7911",
              "text": "are you using 240 gb version?",
              "score": 1,
              "created_utc": "2026-01-28 22:41:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bswmq",
              "author": "Expert_Job_1495",
              "text": "Have you played around with their Agent Swarm functionality? If so, what's your take on it?Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:11:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2crtwu",
              "author": "Which-Jello9157",
              "text": " was burning cash testing all these different apis just to compare it to qwen 3 and glm 4.7. k2.5 definitely edges out glm 4.7 for slightly more. But i heard glm 5 and deepseek v4 are both coming in Feb because quarter kpi sync and launch event politics lol, fake arms race vibes.ðŸ˜‚ btw, I've been using [atlascloud.ai](http://atlascloud.ai) and [openrouter.ai](http://openrouter.ai) for different model routing.",
              "score": 1,
              "created_utc": "2026-01-29 04:31:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e3j50",
              "author": "Dry_Natural_3617",
              "text": "GLM 5 is due very soonâ€¦. They were training it through the festive seasonâ€¦ Assuming itâ€™s better than 4.7, i think itâ€™s gonna be opus level ðŸ™€",
              "score": 1,
              "created_utc": "2026-01-29 11:10:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2elpoj",
              "author": "Which-Jello9157",
              "text": "I'm now using it viaÂ [atlascloud.ai](https://www.atlascloud.ai/models/moonshotai/kimi-k2.5?utm_medium=reply&ref=F27PTG). $0.56/2.8/M Input&Output, cheaper than most providers.",
              "score": 1,
              "created_utc": "2026-01-29 13:17:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2enoua",
              "author": "Funny_Working_7490",
              "text": "In codebase understanding and without over engineering solutions \nHow do you rate claude sonnet vs glm? \nAre glm actually good or just for vibe coding",
              "score": 1,
              "created_utc": "2026-01-29 13:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o276ztc",
          "author": "TechnoByte_",
          "text": "LMArena is nothing more than a one-shot vibe check\n\nIt says absolutely nothing about a model's multi-turn, long context or agentic capabilities",
          "score": 67,
          "created_utc": "2026-01-28 11:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28l7f4",
              "author": "wanderer_4004",
              "text": "Actually I fear models that score well on LMArena - I think this is where we got all the sycophancy from and the emojis sprinkled all over the code.",
              "score": 18,
              "created_utc": "2026-01-28 16:20:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27o3ou",
              "author": "eposnix",
              "text": "True. But Kimi is still likely the best open model for coding. LiveBench places it top 10 for coding also.",
              "score": 9,
              "created_utc": "2026-01-28 13:44:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28zh76",
              "author": "SufficientPie",
              "text": "What's a good leaderboard for coding?",
              "score": 3,
              "created_utc": "2026-01-28 17:23:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9pqe",
                  "author": "gxvingates",
                  "text": "Open router programming section, gives you an actual idea of what models are actually being used and are useful. Sort by week",
                  "score": 2,
                  "created_utc": "2026-01-29 06:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b6cnz",
              "author": "TurnUpThe4D3D3D3",
              "text": "I feel that the ranking is pretty accurate (Opus is currently #1)",
              "score": 2,
              "created_utc": "2026-01-28 23:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o272ayx",
          "author": "ExpressionWeak1413",
          "text": "What kinda set up would be needed to run this locally?",
          "score": 61,
          "created_utc": "2026-01-28 11:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2734xw",
              "author": "cptbeard",
              "text": "[https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)\n\n\"You need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM â‰¥ 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.\"",
              "score": 89,
              "created_utc": "2026-01-28 11:29:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o278qkx",
                  "author": "Antique_Dot_5513",
                  "text": "1 bitâ€¦ might as well ask my cat.",
                  "score": 244,
                  "created_utc": "2026-01-28 12:10:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274oms",
                  "author": "InevitableArea1",
                  "text": "That's cool but what's the use case for that setup? Tokens would be so slow, it'd take so long. Even if you had time to spare, power isn't free and I wonder how that cost would compare to just paying for it.",
                  "score": 16,
                  "created_utc": "2026-01-28 11:41:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o273th4",
                  "author": "MaverickPT",
                  "text": "You heard that 4070 TI? You better get ready with all your 12 GB of VRAM eheh",
                  "score": 13,
                  "created_utc": "2026-01-28 11:34:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o285ira",
                  "author": "gomezer1180",
                  "text": "With a trillion parameters and it still came in behind Google and Anthropic. Yes itâ€™s great at coding but you need a $200k setup to run itâ€¦ /s",
                  "score": 5,
                  "created_utc": "2026-01-28 15:11:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bamz8",
                  "author": "Mister_Otter",
                  "text": "Wait for the quantized version?",
                  "score": 1,
                  "created_utc": "2026-01-28 23:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o28h7hl",
              "author": "dobkeratops",
              "text": "2x 512gb M3-ultra Mac Studio, can run the 4bit quantization. It's been demonstrated on this config at 24tokens/sec.",
              "score": 7,
              "created_utc": "2026-01-28 16:03:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27d1bz",
              "author": "muyuu",
              "text": "if by \"this\" you mean the full model taking 247GB, you're going to need some really ridiculous hardware so it runs at an acceptable speed, maybe a bunch of H200s or a cluster of Mac Studios [like this one claiming 24 tps](https://xcancel.com/alexocheema/status/2016404573917683754)\n\njudging from the performance of Qwen3-Coder, it's much better to run a smaller parameter model than heavily quantising a very large one\n\nI doubt many people will run it locally vs the trusty smaller models that fit under 128GB but it will be available from many providers for a lot cheaper than the larger GPTs",
              "score": 14,
              "created_utc": "2026-01-28 12:40:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28s2hn",
              "author": "mrpogiface",
              "text": "8xH200 is the official supported size",
              "score": 1,
              "created_utc": "2026-01-28 16:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2708ou",
          "author": "WhaleFactory",
          "text": "From my experience so far, Kimi K2.5 is truly impressive. Feels more competent than Sonnet 4.5. Honestly it feels as good as Opus 4.5 to me so far.... Which is crazy given that it is like 1/5th the cost....It costs less than Haiku!",
          "score": 55,
          "created_utc": "2026-01-28 11:05:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o273k49",
              "author": "SnooSketches1848",
              "text": "not opus competitor yet, sonnet yes not opus",
              "score": 27,
              "created_utc": "2026-01-28 11:32:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27zw1m",
              "author": "Hoak-em",
              "text": "I'm using it as an orchestrator and it was very clearly fine-tuned to work well for that purpose",
              "score": 2,
              "created_utc": "2026-01-28 14:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o292jza",
                  "author": "chriskevini",
                  "text": "which models for subagents?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27n2yf",
              "author": "stonk_street",
              "text": "What's you current local setup?",
              "score": 2,
              "created_utc": "2026-01-28 13:39:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27w4wm",
                  "author": "WhaleFactory",
                  "text": "I can't run it locally. Using OpenRouter.",
                  "score": 5,
                  "created_utc": "2026-01-28 14:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o270btn",
              "author": "npc_gooner",
              "text": "True that.",
              "score": 1,
              "created_utc": "2026-01-28 11:06:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2at9vy",
              "author": "kazprog",
              "text": "On some of my benchmarks, Kimi K2.5 is the first model to beat Opus 4.5, Gemini 3 Pro + Deep Research, and Codex 5.2.  Really really impressive, I'm surprised people are getting worse results.  Kimi code is also a fairly solid agent by itself, and I'm not paying for the agent swarm or anything.",
              "score": 1,
              "created_utc": "2026-01-28 22:10:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bjamo",
              "author": "daniel-sousa-me",
              "text": "1/5 of the API cost? Does that mean it's more expensive than the subscription? ðŸ¤”",
              "score": 1,
              "created_utc": "2026-01-29 00:20:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cvqk2",
                  "author": "Grand-Management657",
                  "text": "Depends. I use nano-gpt subscription for $8/month and get 60k requests/month. That includes K2.5. Comes out to $0.00013/request and each request can be of any size (within model limits). Can't beat that. Essentially unlimited coding for me and on par with Sonnet 4.5, if not better. My referral if you want a discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 1,
                  "created_utc": "2026-01-29 04:57:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27bbn0",
          "author": "formatme",
          "text": "I dont see it on LMArena, and how does it compared to GLM 4.7",
          "score": 6,
          "created_utc": "2026-01-28 12:28:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28jtnu",
              "author": "ps5cfw",
              "text": "On real Life coding scenarios regarding awful React JavaScript code I can Say it's extremely impressive and even Better than whatever Gemini 3 pro ai studio offers.\n\n\nIt's slower but It really gets the point and respects prompt directives",
              "score": 5,
              "created_utc": "2026-01-28 16:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271jh7",
          "author": "CYTR_",
          "text": "Thanks U, npc_gooner !",
          "score": 22,
          "created_utc": "2026-01-28 11:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276lut",
              "author": "Comfortable-Rock-498",
              "text": "OG reddit vibes",
              "score": 3,
              "created_utc": "2026-01-28 11:55:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28l23w",
          "author": "SoupSuey",
          "text": "Well, I guess rising on the list to compete with Claude is a feat on its own. \n\nGoogle allegedly doesnâ€™t use your data to train the models if you are a Pro subscriber or above, is that the case with services like Kimi and z.AI?",
          "score": 5,
          "created_utc": "2026-01-28 16:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bd17v",
              "author": "TheRealMasonMac",
              "text": "There is nothing in the ToS for MoonshotAI that forbids them from training on you AFAIK. At the very least, I believe they mention that they save chat for \\`kimi.com\\`. [Z.AI](http://Z.AI) claims they don't in their ToS when you use their API or coding plan, but I believe they can see stuff on [chat.z.ai](http://chat.z.ai) too",
              "score": 2,
              "created_utc": "2026-01-28 23:47:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bncbm",
                  "author": "SoupSuey",
                  "text": "Makes sense.",
                  "score": 1,
                  "created_utc": "2026-01-29 00:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27hxok",
          "author": "shaonline",
          "text": "Lol anybody who's been trying to use Gemini 3 Pro knows that this ranking is BS, Gemini is the nuclear briefcase of coding.",
          "score": 11,
          "created_utc": "2026-01-28 13:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27svio",
              "author": "starfries",
              "text": "Wait, are you saying it's better than Claude? Or that it's awful lol",
              "score": 8,
              "created_utc": "2026-01-28 14:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27tjxp",
                  "author": "shaonline",
                  "text": "That sometimes it's REALLY awful and a good way to nuke your codebase. I've watched it add a pure virtual function/unimplemented function to a baseclass, until then good, and it progressively nuked all the classes derived from it because it could not figure that it needed to prepend \"abstract\" to the immediate subclasses that had now become abstract as well due to the unimplemented function. Thank god for source version control am I right ?",
                  "score": 18,
                  "created_utc": "2026-01-28 14:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28trq0",
                  "author": "mehyay76",
                  "text": "use something like this to shove the entire codebase into Gemini and get amazing results!\n\nhttps://github.com/mohsen1/yek\n\n\nCLI tools are greedy with context when it comes to models with 1M token context window",
                  "score": 3,
                  "created_utc": "2026-01-28 16:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27uwqw",
              "author": "bick_nyers",
              "text": "Yeah and Chat 5.2 isn't even up here",
              "score": 2,
              "created_utc": "2026-01-28 14:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27v7ka",
                  "author": "shaonline",
                  "text": "Yeah having used claude, GPT and gemini I'd say Claude and GPT are neck and neck at the top. Like what the fuck Grok and Gemini are doing up there lol there's no way.",
                  "score": 8,
                  "created_utc": "2026-01-28 14:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a3ybx",
          "author": "cheesecakegood",
          "text": "Yeah but look at the size of that interval. Two to three times that of the others. Sure the score as a point estimate is good but itâ€™s definitely going to be more unreliable! Something that I feel is lost in the discussion here",
          "score": 3,
          "created_utc": "2026-01-28 20:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c7blr",
          "author": "jonas-reddit",
          "text": "Looking forward to SWE Rebench results. \n\nhttps://swe-rebench.com/",
          "score": 3,
          "created_utc": "2026-01-29 02:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cydxv",
              "author": "Grand-Management657",
              "text": "Same here, I keep checking every day but they haven't even gotten around to GLM-4.7 Flash yet so it might be a while.",
              "score": 2,
              "created_utc": "2026-01-29 05:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271mmv",
          "author": "lemon07r",
          "text": "It's quite good. I tested in my coding eval and it scored surprisingly well. Was always a very big kimi fan.",
          "score": 5,
          "created_utc": "2026-01-28 11:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27eoqh",
          "author": "SnooCapers9708",
          "text": "Claude ðŸ”¥ðŸ”¥",
          "score": 2,
          "created_utc": "2026-01-28 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kslq",
          "author": "brennhill",
          "text": "I'm going to use your post to explain to my wife why I have to buy an M5 Max laptop when they come out.  Thank you for your contribution :D",
          "score": 2,
          "created_utc": "2026-01-28 16:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2etu",
          "author": "harlekinrains",
          "text": "164 comments!\n\n601 likes!\n\nPromoted by someones Discord commuity!\n\nNo one looked at the confidence intervall in the second column yet.\n\nWe all have come a long way. On hype alone.\n\nUsing nothing but a LLM arena ranking and three \"I've seen him!\" postings.\n\nCongratulation to Kimis post IPO Marketing Department.",
          "score": 2,
          "created_utc": "2026-01-29 05:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zjfl",
          "author": "Theio666",
          "text": "Gemini 3 pro and even 3 flash higher than GPT 5.2, very trustwordy benchmark xd.",
          "score": 13,
          "created_utc": "2026-01-28 10:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29t7hn",
              "author": "Fault23",
              "text": "https://preview.redd.it/efgjohzl65gg1.png?width=745&format=png&auto=webp&s=1a55e37a4772a999e7e3f37cf0bc9dc4a3559d4c",
              "score": 4,
              "created_utc": "2026-01-28 19:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29xjwy",
                  "author": "Fault23",
                  "text": "And for the coding benchmark, Kimi K2.5 is listed in 7th place",
                  "score": 2,
                  "created_utc": "2026-01-28 19:50:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2725js",
              "author": "kabelman93",
              "text": "Honestly I had very bad experiences with 5.2 for coding. Obviously this is just anecdotal evidence at best, but I am sure others had similar experiences.",
              "score": 12,
              "created_utc": "2026-01-28 11:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o274qhw",
                  "author": "Front_Eagle739",
                  "text": "Honestly it's my favourite. For long iterative sessions with complex single feature implementations/fixes it is far far more likely to solve in one prompt than claude code opus. Slower though.",
                  "score": 13,
                  "created_utc": "2026-01-28 11:41:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274vh6",
                  "author": "Tema_Art_7777",
                  "text": "Quite the opposite - I use codex and gpt 5.2 with coding and it is quite good.",
                  "score": 13,
                  "created_utc": "2026-01-28 11:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o271wsu",
              "author": "lemon07r",
              "text": "These are just one shots. Gemini 3 pro sucks at everything but one shots (coding wise) and is especially good at ui/webdev. So yeah, not the greatest benchmark, but still a valid one. GPT 5.2 much more useful for solving problems, or longer iterative coding (which is more realistic use). Just a matter of understanding what the benchmark is measuring.",
              "score": 4,
              "created_utc": "2026-01-28 11:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cfqbi",
                  "author": "toothpastespiders",
                  "text": ">These are just one shots.\n\nI think people get 'far' too invested in those without realizing their limitations. It basically just means that a model was trained on something and can regurgitate it. Which can be great and it often shows important differences in training data. But it's the 'start' of investigating the strength and weakness of a model not the end. What's far more important is if the model is \"smart\" enough to actually do anything with that training data besides vomit it out. Because otherwise it might as well just be a 4b model hooked up to a good RAG system.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:17:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o274vef",
              "author": "alphapussycat",
              "text": "ChatGPT is terrible for coding. It's an extreme gaslighter, and cannot understand requirement, or follow very simple logic.\n\nI feel like it was better a year ago than it is now.",
              "score": 6,
              "created_utc": "2026-01-28 11:42:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27fb19",
                  "author": "zball_",
                  "text": "That's literally Opus, not GPT.",
                  "score": 3,
                  "created_utc": "2026-01-28 12:54:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bm9og",
                  "author": "Crinkez",
                  "text": "The fact that you called it \"ChatGPT\" is quite telling. I agree that ChatGPT on the web is horrendous for coding.\n\n\nBut GPT5.2 CLI curbstomps all competition, including Opus 4.5",
                  "score": 0,
                  "created_utc": "2026-01-29 00:35:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zud3",
              "author": "Inflation_Artistic",
              "text": "All GPT models are terrible for coding",
              "score": -5,
              "created_utc": "2026-01-28 11:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270o58",
                  "author": "Alywan",
                  "text": "You clearly have no idea what you are talking about",
                  "score": 5,
                  "created_utc": "2026-01-28 11:09:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o270p4c",
                  "author": "Theio666",
                  "text": "???\n\nGpt 5.2 is better than opus if you tried using both of them on any complicated and messy codebases. And especially in ML, I'm never letting claude models on my ML projects, it's just being sloppy af in that area.",
                  "score": -2,
                  "created_utc": "2026-01-28 11:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27g963",
              "author": "Officer_Trevor_Cory",
              "text": "5.2 is terrible for agentic coding even codex. these lm arena results are irrelevant",
              "score": -4,
              "created_utc": "2026-01-28 13:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o296ut3",
          "author": "Avocados6881",
          "text": "I paid 20$ for google every month and I got better result. LocalLM takes 100k$ machine to perform similar or less. Yay!",
          "score": 2,
          "created_utc": "2026-01-28 17:54:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2at2y5",
              "author": "vmnts",
              "text": "Because it's open weights, you can instead pay any number of other companies a lot less than $20/mo to host it for you...",
              "score": 2,
              "created_utc": "2026-01-28 22:09:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28crw3",
          "author": "pab_guy",
          "text": "Opus 4.5 gets a 1539 and Sonnet 4.5 gets a 1521.  That 18 points represents the difference between an OK but still stupid model and a very capable model that can handle most coding tasks end to end on it's own.\n\nThe 30 point difference makes me think I don't want to touch open models for coding ATM.  But I have access to unlimited Opus so it's an easy call for me lol.",
          "score": 2,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cncoz",
              "author": "forgotten_airbender",
              "text": "How does one get unlimited opus?Â ",
              "score": 2,
              "created_utc": "2026-01-29 04:02:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cy5s2",
              "author": "Grand-Management657",
              "text": "If you have unlimited opus then really its a no brainer to stick to that. In my testing over a few hours, K2.5 seems to be on par with Sonnet 4.5, maybe even slightly better (big maybe). I don't care about benchmarks or points at all, in real world usage it seems to hold up well.",
              "score": 1,
              "created_utc": "2026-01-29 05:14:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bh3ru",
              "author": "Funny-Advertising238",
              "text": "These points don't represent jack shit nothing.Â ",
              "score": 1,
              "created_utc": "2026-01-29 00:09:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o273s1h",
          "author": "fugogugo",
          "text": "okay but how is its token consumption?",
          "score": 1,
          "created_utc": "2026-01-28 11:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279ko8",
          "author": "BABA_yaaGa",
          "text": "Scores are very tight for top 10",
          "score": 1,
          "created_utc": "2026-01-28 12:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cu4g",
          "author": "Ne00n",
          "text": "Doesn't fit on my 64GB DDR4 LLM server, sad.",
          "score": 1,
          "created_utc": "2026-01-28 12:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o297snv",
          "author": "horaciogarza",
          "text": "So for coding it's better than Sonnet or Opus? If so (or not) for how much is different from a scale 1-10?",
          "score": 1,
          "created_utc": "2026-01-28 17:58:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ac7e3",
          "author": "Torodaddy",
          "text": "Qwen 3 coder 30b is pretty good thats my goto for open models",
          "score": 1,
          "created_utc": "2026-01-28 20:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2acqjf",
          "author": "ortegaalfredo",
          "text": "I ran my custom benchmarks about cybersecurity and...Kimi K2.0  thinking was definitively better. I has regressed at this subject. And it's nowhere near the commercial models like gemini or even sonnet.   \nJust my datapoint. Now the performance is almost equal to that of GLM 4.7.",
          "score": 1,
          "created_utc": "2026-01-28 20:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b686l",
          "author": "TurnUpThe4D3D3D3",
          "text": "Itâ€™s fantastic at web design. Creates beautiful websites.",
          "score": 1,
          "created_utc": "2026-01-28 23:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bb7ff",
          "author": "Freki371",
          "text": "where you seeing this? my arena.ai latest update is 23 Jan.",
          "score": 1,
          "created_utc": "2026-01-28 23:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2brxdy",
          "author": "FrankMillerMC",
          "text": "Where did Minimax go?",
          "score": 1,
          "created_utc": "2026-01-29 01:05:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cu369",
          "author": "forgotten_airbender",
          "text": "Waiting for swe rebench",
          "score": 1,
          "created_utc": "2026-01-29 04:46:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cxg0e",
          "author": "Grand-Management657",
          "text": "Its 1/5 the price but even cheaper if you use it through a subscription like nano-gpt where each request comes out to $0.00013. And that's regardless of input or output size.   \n  \n$8/month for 60,000 requests is hard to beat. It's basically unlimited coding or whatever your use case is, but you can also switch models and have access to the latest models without having to change providers each time a new and better model releases. For coding K2.5 Thinking is a beast and essentially on par, if not better than Sonnet 4.5 IMO\n\nHere's my referral for a web discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
          "score": 1,
          "created_utc": "2026-01-29 05:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e78jj",
              "author": "Drizzity",
              "text": "Yeah the only problem is k2.5 is not working on nano-gpt at the moment",
              "score": 1,
              "created_utc": "2026-01-29 11:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e83th",
                  "author": "Grand-Management657",
                  "text": "Which harness are you using? I found nanocode to work fine. There was an issue with multi-turn tool calling which they are fixing right now. But otherwise it works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d20gy",
          "author": "alexeiz",
          "text": "I tried it via Ollama cloud and claude code.  If feels like Sonnet 4.5 on my tasks.",
          "score": 1,
          "created_utc": "2026-01-29 05:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2deeh7",
              "author": "goingsplit",
              "text": "how can you use any model on claude code?",
              "score": 1,
              "created_utc": "2026-01-29 07:23:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dfbw6",
          "author": "This_Lemon2165",
          "text": "wow, its amazing",
          "score": 1,
          "created_utc": "2026-01-29 07:31:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhyov",
          "author": "evilbarron2",
          "text": "I get 404 errors in goose, opencode, openwebui and anythingllm every time it tries to use a tool. Quick search shows Iâ€™m not the only one. How did you folks solve that?Â ",
          "score": 1,
          "created_utc": "2026-01-29 07:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dlzks",
          "author": "cantgetthistowork",
          "text": "/u/voidalchemy wen gguf",
          "score": 1,
          "created_utc": "2026-01-29 08:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dvgdx",
          "author": "jasonhon2013",
          "text": "I love kimi but the weight is like â€¦. To heavy",
          "score": 1,
          "created_utc": "2026-01-29 10:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e031l",
          "author": "Familiar_Wish1132",
          "text": "Okay i am surprised. GLM 4.7 was unable to find a problem that i was trying to find and fix for 2 hours, kimi k 2.5 found it in 4 prompts. Now waiting for fix :D",
          "score": 1,
          "created_utc": "2026-01-29 10:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cqi9",
          "author": "BigMagnut",
          "text": "Isn't it a trillion parameters? Doesn't seem very efficient. What am I missing here?",
          "score": 1,
          "created_utc": "2026-01-28 15:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cxv3i",
              "author": "Grand-Management657",
              "text": "It only activates 32b parameters at a time",
              "score": 1,
              "created_utc": "2026-01-29 05:12:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blibm",
          "author": "Crinkez",
          "text": "Bad benchmark site, I don't see the best coding model (GPT5.2) on it. Wouldn't trust that benchmark.",
          "score": 1,
          "created_utc": "2026-01-29 00:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275acz",
          "author": "ptear",
          "text": "Do you have the link to the leaderboard from the screenshot, I couldn't find the model listed and wanted to see that.",
          "score": 0,
          "created_utc": "2026-01-28 11:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ofxf",
          "author": "LocoMod",
          "text": "7th is the 6th loser",
          "score": 0,
          "created_utc": "2026-01-28 13:46:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o287yd5",
              "author": "SpicyWangz",
              "text": "You never know. Itâ€™s possible theyâ€™re all losers",
              "score": 4,
              "created_utc": "2026-01-28 15:22:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o291zpq",
          "author": "datosweb",
          "text": "lo estuve probando y comparando con glm 4.7 y estan muy muy parejos",
          "score": 0,
          "created_utc": "2026-01-28 17:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27914k",
          "author": "Asleep-Ingenuity-481",
          "text": "Barely open but I guess it counts. Honestly im getting tired of these models being insanely large. Though I guess we can kind of just go with it setting the bar for performance for smaller models as well.",
          "score": -4,
          "created_utc": "2026-01-28 12:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o285kwa",
              "author": "pneuny",
              "text": "I feel like the Qwen models are one of the few that target hardware people actually have in their homes, though I certainly appreciate these larger open models that move the research forward for future small models. Though right now I have free access to Gemini given the free year for students, plus AI Studio, but I still keep an eye out for when the free lunch eventually ends.\n\nThough I'd probably hook up my local models to Vertex AI Search anyways to get that web result grounding without needing to load in hundreds of webpages on a home computer.\n\nIf I had to switch now, I'd probably do Qwen 3 VL 4b hooked up to Vertex AI Search and that would probably be enough. That would fit on my 16 GB RX 9070 XT.",
              "score": 2,
              "created_utc": "2026-01-28 15:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28lan8",
                  "author": "chodemunch6969",
                  "text": "You should try GLM 4.7 Flash if you haven't already. It's next level for the 30ba3b MoE weight class that can reasonably run on your own metal.",
                  "score": 3,
                  "created_utc": "2026-01-28 16:21:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bbpnu",
                  "author": "Asleep-Ingenuity-481",
                  "text": "That's where I stand, I don't like them due to their size, but I like them because they set the bar for smaller models to aspire to. Hell im pretty sure that qwen3.5/4 will probably beat deepseek R1 at around 14-40b params.",
                  "score": 1,
                  "created_utc": "2026-01-28 23:41:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tuiv",
              "author": "z_3454_pfk",
              "text": "then train your own instead of whining",
              "score": 3,
              "created_utc": "2026-01-28 14:14:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o285fk2",
                  "author": "into_devoid",
                  "text": "But I want people to work for free and focus on MEEE! Â These models will fit on consumer hardware sometime within the next 5-10yrs most likely. Â The difference between a 30B and 600+ model are staggering in terms of intelligence and real world utility. Â Theyâ€™re probably just planning ahead instead of wasting training time on models that will never be good enough for 95% of most tasks.",
                  "score": 1,
                  "created_utc": "2026-01-28 15:11:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27cmjq",
          "author": "Niwa-kun",
          "text": "I wish i could load Grok 4 into Antigravity, it's actually not bad. I used it in-tangent with Gemini 3 Pro, together they figured out alot of stuff for my project that 1 alone would fail at--but i had to keep being the meditator for them.",
          "score": -5,
          "created_utc": "2026-01-28 12:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27i9w0",
              "author": "Equivalent_Plan_5653",
              "text": "No thanks",
              "score": 4,
              "created_utc": "2026-01-28 13:12:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27sl6e",
          "author": "eihns",
          "text": "ON MY TESTS (cli): its using 1 or 2 subagent while coding uhhhh... anyway, its 90% \\[if u ask me\\] of chatgpt 5.2 /codex, so  why bother? Maybe next version wont immediatly touch my prod.db even tho the promt and agents and everything on that whole repo told him to not touch it xD Tests are irrelevant if the f\\* thing doenst understand what youre talking about... or how to start a dev server.... (all agents were able to do it without any help, except him) xD he just waited till bash timedout and \"tought\" the sever is running did screenshots of \"not reachable\" and told me the fix is working :D so yeah, if you want to stress a developer, then just let 100 of those agents run :D",
          "score": -1,
          "created_utc": "2026-01-28 14:07:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkyex0",
      "title": "Your post is getting popular and we just featured it on our Discord!",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/",
      "author": "roculus",
      "created_utc": "2026-01-23 18:16:47",
      "score": 586,
      "num_comments": 56,
      "upvote_ratio": 0.96,
      "text": "Your post is getting popular and we just featured it on our Discord! Come check it out!\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\nI am a bot and this action was performed automatically.\n\n-----------------------------------------------------\n\nCan you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1c2iii",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-23 23:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1am1aw",
          "author": "FullstackSensei",
          "text": "The fun part will be if this post enough traction from the community that the bot will come to inform us that it was featured in the discord ðŸ˜‚",
          "score": 278,
          "created_utc": "2026-01-23 19:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c3eg6",
              "author": "Asalanlir",
              "text": "Boy do I have some news for you",
              "score": 26,
              "created_utc": "2026-01-23 23:54:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1au0v2",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 33,
              "created_utc": "2026-01-23 20:11:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1h1ugu",
                  "author": "random-tomato",
                  "text": "lmao I remember that, it was like a screenshot from reddit of a screenshot from X where the post had a screenshot of reddit",
                  "score": 4,
                  "created_utc": "2026-01-24 18:46:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1aptcp",
              "author": "Cool-Chemical-5629",
              "text": "Plot twist: That was the OP's goal all along. ðŸ˜",
              "score": 17,
              "created_utc": "2026-01-23 19:51:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1blz2j",
                  "author": "InterestTracker9000",
                  "text": "That bastard just wanted a flair this whole time, jealousy is a hell of a motivator.\n\n/s",
                  "score": 8,
                  "created_utc": "2026-01-23 22:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ejt7e",
              "author": "silvergreen123",
              "text": "Reporting in from the discord. Seeing the content of the post be the same as the message in discord perplexed me",
              "score": 2,
              "created_utc": "2026-01-24 10:18:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1antfa",
              "author": "ParadoxeParade",
              "text": "The new Discord drift ðŸ¤£ðŸ¤£",
              "score": 2,
              "created_utc": "2026-01-23 19:42:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1edl0q",
              "author": "IrisColt",
              "text": "heh",
              "score": 0,
              "created_utc": "2026-01-24 09:21:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b41h3",
          "author": "relmny",
          "text": "I fully agree, it's very annoying to me.Â \nBut there are multiple issues with this sub... one being let's see how long your post stays up...",
          "score": 85,
          "created_utc": "2026-01-23 20:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bfzz7",
              "author": "robertpro01",
              "text": "Also the name, I joined about a year ago and users hated other models talk, they all wanted to hear about was the llama, except there won't be any other llama open model.",
              "score": -8,
              "created_utc": "2026-01-23 21:54:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cjka2",
                  "author": "MrPecunius",
                  "text": "That's not at all my recollection from a year to a year and a half ago.\n\nMaybe you mean all the commercial & non-local model/service discussion?",
                  "score": 22,
                  "created_utc": "2026-01-24 01:24:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1c8tzf",
                  "author": "FaceDeer",
                  "text": "It's impossible to change the name of a subreddit once it's been created, so that's just something that must be endured. Like how /r/StableDiffusion is still a popular subreddit for discussing generative image AIs even though Stability.ai has faded into relative obscurity at this point.",
                  "score": 10,
                  "created_utc": "2026-01-24 00:24:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ekvt3",
                  "author": "Orolol",
                  "text": "I'm here nearly from the start and this never happened",
                  "score": 2,
                  "created_utc": "2026-01-24 10:28:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1cm4gt",
              "author": "N8Karma",
              "text": "It's been hand approved by mods. We welcome this sort of discussion.",
              "score": -11,
              "created_utc": "2026-01-24 01:39:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1d1ast",
                  "author": "DinoAmino",
                  "text": "Please correct me if I'm wrong, but I recall the previous mod that made this sub go dark was also a mod for r/OpenAI. Seems like a complete conflict of interest... and he was the one to initially remove the \"local only\" verbage. Just like you all decided in favor of marrying Discord, you all could change the rules back to what the majority of the users want:\n\nhttps://www.reddit.com/r/LocalLLaMA/s/5dK9C5gmUA\n\nAnd how about some minimum karma requirements in order to post in order to minimize the junk a bit?",
                  "score": 21,
                  "created_utc": "2026-01-24 03:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1apkod",
          "author": "Cool-Chemical-5629",
          "text": "So you're telling me there's a Discord server for this sub? ðŸ˜³",
          "score": 12,
          "created_utc": "2026-01-23 19:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cbdzv",
              "author": "ttkciar",
              "text": "There is, yes.  There's even a stickied post about it.",
              "score": 4,
              "created_utc": "2026-01-24 00:38:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aps84",
          "author": "jwpbe",
          "text": "They're trying to make money off the community and the 'twitter', god knows that ever since they made it that the amount of LLM slop replies has tripled",
          "score": 45,
          "created_utc": "2026-01-23 19:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e15go",
              "author": "ansibleloop",
              "text": "Jesus every fucking sub does this\n\nThis site is trash and somehow its still not as bad as the rest",
              "score": 6,
              "created_utc": "2026-01-24 07:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ik7h5",
                  "author": "ginger_and_egg",
                  "text": "Public, searchable resource? nah let's go to the private walled garden of discord",
                  "score": 6,
                  "created_utc": "2026-01-24 23:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ebk36",
          "author": "Your_Friendly_Nerd",
          "text": "Because of this, I just instinctively ignore the top comment on any reddit post lol",
          "score": 10,
          "created_utc": "2026-01-24 09:02:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fo69g",
          "author": "Feisty-Patient-7566",
          "text": "Fuck discord. Reddit doesn't require me to have a phone number to make an account but Discord does.",
          "score": 7,
          "created_utc": "2026-01-24 15:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bcnqc",
          "author": "FullOf_Bad_Ideas",
          "text": ">FullOF_Bad_Ideas | Llama 65B | 1 points | an hour ago\n\nwe also lost proper flairs, discord flair sucks. I want Llama 65B flair back.",
          "score": 10,
          "created_utc": "2026-01-23 21:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bfewb",
          "author": "T_UMP",
          "text": "\\*mIRC has entered the chat",
          "score": 4,
          "created_utc": "2026-01-23 21:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bjiri",
          "author": "jazir555",
          "text": "Upvoted, gib bot message",
          "score": 6,
          "created_utc": "2026-01-23 22:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b3eee",
          "author": "anticommon",
          "text": "Your post is getting a lot of traction!\n\nUnfortunately, this is a burnout competition VRRRRRRRRRR SQUEEEEEEEEEEEEeEEEEeeeEEE......",
          "score": 8,
          "created_utc": "2026-01-23 20:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e9s8p",
          "author": "luget1",
          "text": "https://preview.redd.it/hz0vzmfuf9fg1.png?width=1440&format=png&auto=webp&s=946872ea9b6f911e0ab0ba73bd3006b38d5857f3\n\nLmao",
          "score": 6,
          "created_utc": "2026-01-24 08:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ecm0z",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-01-24 09:12:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x04aw",
              "author": "AskGrok",
              "text": "Sorry, I encountered an error while processing your request. Please try again later.",
              "score": 0,
              "created_utc": "2026-01-26 23:25:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ig2id",
          "author": "Steuern_Runter",
          "text": "Agree, this bullshit is annoying.",
          "score": 2,
          "created_utc": "2026-01-24 22:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1a5zzl",
          "author": "jacek2023",
          "text": "you are right but there are bigger problems with LocalLLaMA since 2025 than this one, so I don't think we can do anything with that",
          "score": 4,
          "created_utc": "2026-01-23 18:21:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1aukfn",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-01-23 20:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bc0wm",
              "author": "WithoutReason1729",
              "text": "I'm managing the bot",
              "score": -9,
              "created_utc": "2026-01-23 21:36:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1drp9z",
          "author": "k_means_clusterfuck",
          "text": "Your post is getting popular and we just featured it on our Discord!Â [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a human and this action was performed manually.*",
          "score": 3,
          "created_utc": "2026-01-24 06:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dwb8c",
          "author": "Loginloolzocker",
          "text": "Yes, dear friends, I don't know why my comment keeps getting deleted and then re-approved... I can only tell you that it's true and the door is open to you. Live long and prosper. ðŸ––\nThe Architect",
          "score": -8,
          "created_utc": "2026-01-24 06:45:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoty38",
      "title": "Kimi K2.5 costs almost 10% of what Opus costs at a similar performance",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xz7okply3zfg1.png",
      "author": "Odd_Tumbleweed574",
      "created_utc": "2026-01-27 23:10:16",
      "score": 556,
      "num_comments": 106,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o269wrx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 07:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o257yrq",
          "author": "one-wandering-mind",
          "text": "It used 3x the tokens that opus does for the same tasks so cheaper, but more like 3x cheaper than 10x cheaper.Â \n\n\nThese models often use a dramatically different number of tokens to do the same thing. It should be considered for both cost and latency when you compare them.Â \n\n\nI've heard great things about the kimi models especially the last version for writing.Â \n\n\nÂ https://artificialanalysis.ai/#cost-to-run-artificial-analysis-intelligence-index",
          "score": 148,
          "created_utc": "2026-01-28 02:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26orzg",
              "author": "ozzie123",
              "text": "â€œSimilar performanceâ€ here also carries lots of asterisks",
              "score": 46,
              "created_utc": "2026-01-28 09:23:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dhcye",
                  "author": "Cergorach",
                  "text": "Similar performance does not mean equal quality output for every task.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:49:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2472ya",
          "author": "TAW56234",
          "text": "If I had a nickel for every time someone claimed the newest OSS Sota model was similar to Claude, I could generate a few prompts.",
          "score": 343,
          "created_utc": "2026-01-27 23:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gq5f",
              "author": "ForsookComparison",
              "text": "I use closed weight models at work (required) and open weight models for side projects (cost). Several hours of agent use per day.\n\nI think open weight is approaching **Sonnet 3.7** if I'm totally honest. I love this community to death but it gets drunk off of bar charts and one-shots. Sonnet 3.7 for Kimi and Deepseek prices is amazing, but *\"approaching Opus 4.5\"* just flags for me that nobody is using these things for hours at a time.",
              "score": 130,
              "created_utc": "2026-01-28 03:46:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o260qbh",
                  "author": "peculiarMouse",
                  "text": "I used Max extensively for multiple months and I tend to disagree. Claude is best at writing code, but architecture, hallucinations and creativity make it highly debatable in all other things that matter about code. \n\nGemini Pro for instance has INSANE, borderline dementia levels hallucinations, where it necessitates undoing generation and redoing with something else, but its context window and knowledge is unparalleled.\n\nClaude is very fixed on things it thinks it knows how to do. For example old libraries that have long deprecated documentation. GLM with same Claude .md has much lower rate for ignoring it altogether. It goes as far as making your code worse through unrequested changes.\n\nI would easily put Claude first as agentic powerhouse that comes with your subscription.  \nBut I tend to use other models a lot, when working with Claude, so I'd put them on Sonnet 4.6 level easily.\n\nClaude just has very good understanding of what's most important to 95% devs.",
                  "score": 38,
                  "created_utc": "2026-01-28 05:57:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25q46f",
                  "author": "KaroYadgar",
                  "text": "What do you use them for? LLMs are very spike-y so I'd like to know at what specific tasks you think they're comparable to Sonnet 3.7. Are there any tasks you think they are comparable or better at?",
                  "score": 13,
                  "created_utc": "2026-01-28 04:43:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25qhvs",
                  "author": "Virtamancer",
                  "text": "Have you used any of the current-gen top open models in OpenCode? Like, the absolute newest version of deepseek/glm/kimi/qwen/whatever, like within the last 7 days or whenever they released.\n\nIâ€™m curious how they compare running through OpenCode vs Claude Code with Opus 4.5 or Codex with GPT-5.2 set to Extra High reasoning.",
                  "score": 6,
                  "created_utc": "2026-01-28 04:46:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26h5zi",
                  "author": "FyreKZ",
                  "text": "I think you're misremembering how dumb 3.7 was lol. K2.5 is realistically probably not far off Sonnet 4.5, Opus is a stretch.",
                  "score": 1,
                  "created_utc": "2026-01-28 08:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o252exg",
              "author": "Zc5Gwu",
              "text": "Generate prompts with kimi or claude though, thatâ€™s the question.",
              "score": 27,
              "created_utc": "2026-01-28 02:28:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2avsuf",
              "author": "eikenberry",
              "text": "No reason it couldnâ€™t be true every time. This is active research with things constantly changing.",
              "score": 1,
              "created_utc": "2026-01-28 22:21:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24694d",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 68,
          "created_utc": "2026-01-27 23:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qbtx",
              "author": "DistanceSolar1449",
              "text": "Almost certainly not. [Estimates for Claude Opus 4.5 put it at 1.6T-3T params, 160b active](https://news.ycombinator.com/item?id=46038512). \n\nThat would make Kimi roughly 1/2 to 1/4 the size.",
              "score": 6,
              "created_utc": "2026-01-28 04:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24ewv8",
              "author": "neotorama",
              "text": "I would say almost similar. Opus has been degrading last 3 weeks",
              "score": -10,
              "created_utc": "2026-01-28 00:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24rlr0",
                  "author": "eli_pizza",
                  "text": "Opus hasnâ€™t changed.",
                  "score": 15,
                  "created_utc": "2026-01-28 01:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o24fo78",
              "author": "Healthy-Nebula-3603",
              "text": "Yes \n\nIs very similar",
              "score": -11,
              "created_utc": "2026-01-28 00:29:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24q0lj",
                  "author": "Bob_Fancy",
                  "text": "Iâ€™d bet a good amount that itâ€™s not.",
                  "score": 12,
                  "created_utc": "2026-01-28 01:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25snrk",
          "author": "dubesor86",
          "text": "this assumes a ton of input and will swing widely depending on use case. for me, the bulk of the cost is always the model output.\n\nin my general benchmark the cost was:\n\nKimi-K2.5 (reasoning) $1.60\n\nClaude Opus 4.5 $2.75\n\n= 42% cheaper\n\n\nin my chess benchmark the game cost was:\n\nKimi-K2.5 (reasoning) $0.87\n\nClaude Opus 4.5 $0.46\n\n= 89% more expensive\n\nAlso, obviously the performance is not \"similar\" level if you actually used these models, despite what some bars tell you.",
          "score": 23,
          "created_utc": "2026-01-28 04:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24e4e0",
          "author": "ghulamalchik",
          "text": "I'm just gonna wait for DeepSeek 4 and MiniMax M2.2\n\nI trust those from experience.\n\nI used many models in cline and DS and MiniMax were my favorite.",
          "score": 33,
          "created_utc": "2026-01-28 00:21:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gwjm",
              "author": "epyctime",
              "text": "you prefer them to glm4.7?",
              "score": 6,
              "created_utc": "2026-01-28 03:47:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25qbup",
                  "author": "ghulamalchik",
                  "text": "I haven't used GLM 4.7 a lot to form a comprehensive opinion, however when I tried it it was *slightly* worse than MiniMax M2.1 for my use cases (programming desktop GUIs). MiniMax was faster and slightly better. DeepSeek was nice when it came to debugging big issues, it's about the same in terms of general intelligence, it's much slower than MiniMax though.\n\nTL;DR\n- DS was smart but slow, good for debugging and planning big changes. It's also very cheap. The main downside is that it's slower.\n- MiniMax is good a default.\n- GLM 4.7 is in the same ballpark in terms of performance but MiniMax felt a little better for me personally.\n\nI think it all depends on what you use the models for. There's no answer that fits all.",
                  "score": 5,
                  "created_utc": "2026-01-28 04:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25c7v4",
              "author": "Zianiwarhead",
              "text": "I strongly agree.",
              "score": 2,
              "created_utc": "2026-01-28 03:21:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25wjgj",
              "author": "deadcoder0904",
              "text": "What are u using DS & MiniMax for specifically? Just coding? Frontend or backend?",
              "score": 1,
              "created_utc": "2026-01-28 05:26:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25xqoj",
                  "author": "ghulamalchik",
                  "text": "Yeah just coding. I'm mainly doing desktop GUIs in Qt (full programs not just the GUI part).",
                  "score": 2,
                  "created_utc": "2026-01-28 05:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26vtkt",
              "author": "Loskas2025",
              "text": "I use GLM 4.7 with kilocode or cline for planning and code setup. But then I switch to minimax 2.1 (all local) for writing and back to GLM 4.7 for orchestration. Used in production.",
              "score": 1,
              "created_utc": "2026-01-28 10:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2712vh",
                  "author": "evia89",
                  "text": "Dont u need smart model for design+plan? I notice that when I do that with opus it gets me at 95%. When I use glm for it I need to do ALOT of hand holding and plan (atomic step by step tasks) wont be as good\n\nHowever if I do plan (superpower skill, not build in) with opus then GLM can code it just fine",
                  "score": 1,
                  "created_utc": "2026-01-28 11:12:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25coe3",
              "author": "Peetlin",
              "text": "same here it's not getting enough love. it's fast and intelligence and agentic capability. it has brain oof 80% of gpt5.2 but that's enough for me",
              "score": 2,
              "created_utc": "2026-01-28 03:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24byuf",
          "author": "NighthawkT42",
          "text": "It's good, but not really the same level.",
          "score": 39,
          "created_utc": "2026-01-28 00:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e4mh",
              "author": "ihexx",
              "text": "yeah, it's closer to sonnet or gemini 3 flash",
              "score": 1,
              "created_utc": "2026-01-28 07:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2458eb",
          "author": "TransportationSea579",
          "text": "Is the similar performance in the room with us?",
          "score": 78,
          "created_utc": "2026-01-27 23:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24lm0k",
              "author": "Howdareme9",
              "text": "It never is lol.",
              "score": 19,
              "created_utc": "2026-01-28 00:59:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2486zq",
              "author": "illforgetsoonenough",
              "text": "K2.5 is sitting on the chair in the corner of the room",
              "score": 7,
              "created_utc": "2026-01-27 23:51:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24fur6",
              "author": "Healthy-Nebula-3603",
              "text": "From what I tested and what I saw on YouTube.... Very similar to opus 4.5 in coding.",
              "score": -10,
              "created_utc": "2026-01-28 00:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24vphu",
          "author": "Recoil42",
          "text": "Does it use the same number of tokens? I doubt it.",
          "score": 7,
          "created_utc": "2026-01-28 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o250252",
              "author": "electronicsoul",
              "text": "Exactly, and what's being cached in those token counts for a much cheaper rate and therefore faster inference is what people seem to forget. That's a big part of what makes Claude feel a level above the rest.",
              "score": 4,
              "created_utc": "2026-01-28 02:16:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25d942",
          "author": "galambalazs",
          "text": "It's more fair to compare to sonnet 4.5",
          "score": 8,
          "created_utc": "2026-01-28 03:26:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25tibf",
          "author": "Torodaddy",
          "text": "*absolutely not the same performance*",
          "score": 13,
          "created_utc": "2026-01-28 05:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e67e",
              "author": "rm-rf-rm",
              "text": "based on prejudice or something more concrete? TBH I am in agreement with you, but Im asking what your basis is for clarity",
              "score": 3,
              "created_utc": "2026-01-28 07:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a7x97",
                  "author": "Torodaddy",
                  "text": "Anecdotally, ive used both extensively and I think its not even close when it comes to something requiring some sequential logic to get the answer. Opus head and shoulders better",
                  "score": 2,
                  "created_utc": "2026-01-28 20:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o246yjt",
          "author": "LoveMind_AI",
          "text": "Kimi in the thinking era is VERY hit or miss. The hits are amazing. The misses are sad, because when itâ€™s on, it really gives the SOTA a run for its money. I still think MiniMax M2 is the best LLM outside of the West.",
          "score": 19,
          "created_utc": "2026-01-27 23:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264z3l",
          "author": "TheInfiniteUniverse_",
          "text": "obviously this would be fantastic if true. this is quite obviously not true, still way behind Opus and GPT but getting there....",
          "score": 4,
          "created_utc": "2026-01-28 06:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2668r4",
          "author": "Michaeli_Starky",
          "text": "At similar performance?  Not even close.",
          "score": 5,
          "created_utc": "2026-01-28 06:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25x2iy",
          "author": "Middle_Bullfrog_6173",
          "text": "If you just go by benchmarks, Genini 3 Flash also has similar performance for 10% cost. In reality for some use cases there's much more difference at the top than a few points suggest. And some tasks are not really captured by benchmarks.\n\n\nLimited testing so far, but K2.5 has the large model feature of great niche performance, like low resource languages. But it seems to lose coherence earlier than the big closed models as context fills up. Can't really say anything more specific yet.",
          "score": 4,
          "created_utc": "2026-01-28 05:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260dkg",
          "author": "Dazzling_Focus_6993",
          "text": "as someone who using heavily using these tools for real world tasks, I would say opus 4.5 is another level, despite benchmarks implying gpt 5.2 and gemini 3 are very close. I say the gap is huge. I cannot use any other model for relatively complex tasks. \n\nI use opensource models (e.g., gpt-oss, kimi and qwen) for relatively simple tasks mainly due to the cost. I look forward to trying kimi 2.5 but, to be frank, i do not have high hopes. I think i will continue to use my combination of opus 4.5 and cheap OS models. \n\nPS: I select models through Kilo Code\n\nAdditionally: For gemini 3, it is the opposite. I do not have any use cases for gemini 3 despite benchmarks look very high.",
          "score": 3,
          "created_utc": "2026-01-28 05:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26iuob",
              "author": "Front_Eagle739",
              "text": "weird. I use codex 5.2high and claude code side by side and both are better at different things. If I had to pick a smarter one i'd nudge to 5.2 high. Its just more likely to one shot fix a problem and less likely to get lost in circles. Claude and opus is waaay faster though so still gets used first.",
              "score": 2,
              "created_utc": "2026-01-28 08:28:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3acs",
                  "author": "Impossible_Hour5036",
                  "text": "Probably depends on prompting technique.  Can't say I've gotten Opus lost unless I was badgering it while it worked or I didn't have a clear goal.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:16:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o240qo4",
          "author": "ChainOfThot",
          "text": "How can their API costs be so cheap with what I assume is still older hardware?",
          "score": 5,
          "created_utc": "2026-01-27 23:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24us8r",
              "author": "BlueSwordM",
              "text": "Native INT4, great context handling, small MOE and not amazing speeds.",
              "score": 8,
              "created_utc": "2026-01-28 01:48:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o249wya",
              "author": "smith7018",
              "text": "Government subsidies",
              "score": 11,
              "created_utc": "2026-01-28 00:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24kjkr",
                  "author": "HateAccountMaking",
                  "text": "We(USA) also have Government subsidies. China is a peoples government thats the difference.",
                  "score": -9,
                  "created_utc": "2026-01-28 00:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o250j8u",
          "author": "electronicsoul",
          "text": "Everyone loves cheap tokens but that's a very reductionist calculation.",
          "score": 3,
          "created_utc": "2026-01-28 02:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24qgyx",
          "author": "lochyw",
          "text": "Who chose these colors? It's literally the opposite of what it should be.",
          "score": 2,
          "created_utc": "2026-01-28 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24tw1h",
              "author": "derivative49",
              "text": "you're talking about it ;)",
              "score": -3,
              "created_utc": "2026-01-28 01:43:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o261l6f",
          "author": "Cool-Selection-9275",
          "text": "Just how do they even do that? That's nuts",
          "score": 2,
          "created_utc": "2026-01-28 06:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260jz8",
          "author": "ChimSau19",
          "text": "I have problem with GLM that claude code just dont ask me if it could modify? Could it fix simply by Shift Tab, or u guy have that problem too?",
          "score": 1,
          "created_utc": "2026-01-28 05:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o265zbl",
          "author": "Crafty-Struggle7810",
          "text": "At what quant?",
          "score": 1,
          "created_utc": "2026-01-28 06:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dit1",
          "author": "korino11",
          "text": "well i making a project and codex 5.2high doesnt solved. But kimi did LOL\n\nGPT always aking about whole parameters what they should be, how need to be done wjole parts. When he have all formulas amd whole project in high math is done. But Kimi, i jut give all formulass and...hold my bear!\n\nJust 1 thing i do not like. in Coding Plans you have a limits on API -Calls. So doesnt matter your call 20k tokens or just 500 tokens...  But..i got it for 2.59  So it perfect for that price",
          "score": 1,
          "created_utc": "2026-01-28 07:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26j1gj",
          "author": "No_Afternoon_4260",
          "text": "> the first time that I feel a open source model is truly competitive..\n\nYeah I know! I feel the same every 6 months, last time was deepseek's release, then k2's.. then glm was the faster brother..\n\nCrazy times I know ðŸ˜…\n\n(As every 6 months, maybe not opus level but not far)",
          "score": 1,
          "created_utc": "2026-01-28 08:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qaqj",
          "author": "rduito",
          "text": "Wait, you said a bit better than glm. So not comparable to opus after all?\n\n\nAm a fan of glm but opus is something else",
          "score": 1,
          "created_utc": "2026-01-28 09:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ywi8",
          "author": "alexrada",
          "text": "what tests have you run?",
          "score": 1,
          "created_utc": "2026-01-28 10:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cwbv",
          "author": "Beginning_Company_85",
          "text": "I tried every models available and ended up with Claude max. Totally insane the amount of work it got me done in just 2 weeks.",
          "score": 1,
          "created_utc": "2026-01-28 15:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28z4im",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. one-wandering-mind is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 1,
          "created_utc": "2026-01-28 17:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bc6jd",
          "author": "elllyphant",
          "text": "& you can use this Synthetic promo (40% off a sub) to use kimi k2.5 w/ high rate limits [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
          "score": 1,
          "created_utc": "2026-01-28 23:43:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsl6g",
          "author": "Muted_Standard175",
          "text": "Does anyone tried using opencode with codex for planning and k2.5 for building? Is it good?",
          "score": 1,
          "created_utc": "2026-01-29 01:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbrnq",
          "author": "Spiritual_Cycle_9141",
          "text": "https://preview.redd.it/4a8nhb7kd7gg1.png?width=856&format=png&auto=webp&s=efb13080f2958c34b582af7238533243fc89edeb\n\n  \nWorked for few seconds and errored , don't waste ur time (( no real free things now )) , also they will trian on ur propmpts",
          "score": 1,
          "created_utc": "2026-01-29 02:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2da7bq",
          "author": "deparko",
          "text": "I've been using Kimi too. I've been developing a health agent, and it is very responsive and very good, but it sometimes comes off as very authoritative and occasionally hallucinates.\n\nI plan to build an agent swarm to validate, but overall I think it's one of the first open models that I don't want to stop using. A lot of the open models I'll work with, but I usually end up on a frontier model eventually. I don't feel that way with kimi.",
          "score": 1,
          "created_utc": "2026-01-29 06:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ea2su",
          "author": "DarqOnReddit",
          "text": "A bit better? Aka still shit. Sorry but neither GLM-4.7 not Minimax are good models. They may be narrowly scoped tiny requests right, but require a lot of iteration and their reasoning or should I say problem solving skills are utter trash",
          "score": 1,
          "created_utc": "2026-01-29 12:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egpkv",
          "author": "Late_Special_6705",
          "text": "How to use  kimi? Kimi coder?",
          "score": 1,
          "created_utc": "2026-01-29 12:46:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27e6xl",
          "author": "tcoder7",
          "text": "No model matches even half of Opus 4.5 full capacities.  This is hype. OSS models are good for execution. This latest model is more a competitor to Sonnet 4.5.",
          "score": 0,
          "created_utc": "2026-01-28 12:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o243jly",
          "author": "PhotographerUSA",
          "text": "Kimi has terrible programming skills. lol",
          "score": -12,
          "created_utc": "2026-01-27 23:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bh4i",
          "author": "Ashley_Sophia",
          "text": "Yeah but I just had a look. Instant ads via UI. No thx.",
          "score": -8,
          "created_utc": "2026-01-28 00:08:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn3xig",
      "title": "I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wky8vuufylfg1.jpeg",
      "author": "brandon-i",
      "created_utc": "2026-01-26 02:51:42",
      "score": 521,
      "num_comments": 156,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1rwv5b",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 06:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzd5p",
          "author": "DraconPern",
          "text": "Run 3 NextJS.",
          "score": 203,
          "created_utc": "2026-01-26 02:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r1pc2",
              "author": "brandon-i",
              "text": "Yeah that was where my logic was going because it has around 4 Terabytes of SSD and around one Petaflop of FP4 AI Performance I was thinking if I could run 4 if I really did better memory management and maybe partioned the unified memory.",
              "score": 40,
              "created_utc": "2026-01-26 03:09:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r1tk4",
          "author": "randomfoo2",
          "text": "Try going through these: [https://github.com/NVIDIA/dgx-spark-playbooks](https://github.com/NVIDIA/dgx-spark-playbooks)",
          "score": 57,
          "created_utc": "2026-01-26 03:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzr0b",
          "author": "l33t-Mt",
          "text": "What was your project at the hackathon?  Congratulations!",
          "score": 60,
          "created_utc": "2026-01-26 02:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r298o",
              "author": "brandon-i",
              "text": "My project was figuring out Social Determinants of Heath so that governments and organizations can find potentially unrelated relevant data and make actionable solutions with AI Agents. https://www.loom.com/share/375f4ba2ae9047d5911e41b763dbb4a9",
              "score": 167,
              "created_utc": "2026-01-26 03:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r2qjj",
                  "author": "Opteron67",
                  "text": "wtf",
                  "score": 75,
                  "created_utc": "2026-01-26 03:15:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r50rg",
                  "author": "ResolveSea9089",
                  "text": "That sounds lit. What's your educational background if you don't me asking? Where/how did you learn this stuff?",
                  "score": 44,
                  "created_utc": "2026-01-26 03:28:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r9t81",
                  "author": "Prigozhin2023",
                  "text": "Will you be open source it? The UI & work amazing!",
                  "score": 10,
                  "created_utc": "2026-01-26 03:56:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r83ck",
                  "author": "klenen",
                  "text": "Project alone needs its own post, very interesting/cool!",
                  "score": 4,
                  "created_utc": "2026-01-26 03:46:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r4dtl",
                  "author": "amejin",
                  "text": "So.. help me out here. Other than stt and tts, I'm assuming you have an LLM that parses the users response to some category or expected result, and the rest of this is just good old SE? It's interesting... Indo wonder if this is at risk of things like HIPAA, and where your data comes from for outreach... \n\nIt also seems you are implying that the bots are responsible for actual distribution and allocation of resources... Is that the case? Or is the idea to let humans be the arbiters of action and there is a trust factor for data aggregation and accuracy?",
                  "score": 5,
                  "created_utc": "2026-01-26 03:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r8jd7",
                  "author": "Helpful-Magician2695",
                  "text": "How much time did you spend on this?   \nYou could have programmed the entire government in a month)",
                  "score": 5,
                  "created_utc": "2026-01-26 03:48:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1trxyp",
                  "author": "hugthemachines",
                  "text": "Sounds like you really deserve the reward.",
                  "score": 2,
                  "created_utc": "2026-01-26 14:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vdm3l",
                  "author": "starkruzr",
                  "text": "hey uh. you looking for a job? ðŸ‘€",
                  "score": 2,
                  "created_utc": "2026-01-26 19:01:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rzgvy",
                  "author": "joelasmussen",
                  "text": "Rad!!!",
                  "score": 1,
                  "created_utc": "2026-01-26 06:55:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1shoq0",
                  "author": "Best-Mycologist3608",
                  "text": "I might want to implement this project, is it for sale?",
                  "score": 1,
                  "created_utc": "2026-01-26 09:35:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1tmt8h",
                  "author": "Exact-Emotion-1932",
                  "text": "Very cool! Would you mind sharing your stack / libraries you used primarily?",
                  "score": 1,
                  "created_utc": "2026-01-26 14:25:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s3uv7",
          "author": "LicensedTerrapin",
          "text": "Sell it and buy 8gb ddr5",
          "score": 38,
          "created_utc": "2026-01-26 07:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1svzzz",
          "author": "Generatoromeganebula",
          "text": "https://preview.redd.it/2ee0hcsukofg1.jpeg?width=1180&format=pjpg&auto=webp&s=1834be0ddcae3ed3928b6d99ca20b1bebc67737b",
          "score": 18,
          "created_utc": "2026-01-26 11:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rauo4",
          "author": "jadhavsaurabh",
          "text": "Try ltx video editing, u can run flux 2 also very well",
          "score": 16,
          "created_utc": "2026-01-26 04:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1riajl",
              "author": "brandon-i",
              "text": "Holy shit LTX is cool tyvm",
              "score": 11,
              "created_utc": "2026-01-26 04:50:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sa9iz",
                  "author": "jadhavsaurabh",
                  "text": "Sure. For idea I will say. Experiment. Create short films. Share on stable diffusion for inspiration",
                  "score": 3,
                  "created_utc": "2026-01-26 08:27:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1sa1lu",
              "author": "howardhus",
              "text": "> ltx\n\ncan you reccomend how to get started with video edit? im out of it..\n\ncomfy? or is it its own app",
              "score": 1,
              "created_utc": "2026-01-26 08:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1saflu",
                  "author": "jadhavsaurabh",
                  "text": "2 ways. You can direct run python scripts,\nBut comfy will be best way,\nDepending on ur ram. Vram \n\nChoose wisely, also checkout stable diffusion sub, for detail,\n\nOn civitai you will find workflows,\n\nLatest ltx is very light weight, and good competitor to wan",
                  "score": 1,
                  "created_utc": "2026-01-26 08:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rggzo",
          "author": "massive_rock33",
          "text": "Give it to me ðŸ’€",
          "score": 8,
          "created_utc": "2026-01-26 04:38:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r2z53",
          "author": "Fit-Produce420",
          "text": "On 128gb you can fine tune up to a 70B model.Â \n\n\nYou can also qlora larger models in the 120B range, like gtp-oss-120b. That's probably current best performer on that setup in terms of larger/smarter models.Â \n\n\nYou can run the big devstral 2 but it is slow being a dense model.Â ",
          "score": 18,
          "created_utc": "2026-01-26 03:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r3ky6",
              "author": "brandon-i",
              "text": "I might get sent two because they really liked my idea. So I was thinking maybe I can hook both of them up?",
              "score": 12,
              "created_utc": "2026-01-26 03:20:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rht1a",
                  "author": "thebadslime",
                  "text": "You can, they have a high speed link between the two",
                  "score": 7,
                  "created_utc": "2026-01-26 04:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rgzky",
          "author": "nofilmincamera",
          "text": "This is amazingly.  I am an idiot that sells stuff. But I have a 96 GB Blackwell,  I use for overkilled text analysis. \n\nHowever last year my Dad died waiting for an organ transplant,  my Wife was also dying, and recieved a no for posting.  I mention this because I navigated 7 different transplant evaluation processes and the amount of resources it took to get approval made it clear  that how much non medical social conditions matter more than they should. Fringe case approval or denial was proximity, support network, health insurance network. I have been wondering how that might be measured. Same kind of thing that let Steve Jobs post in every network in the country.",
          "score": 19,
          "created_utc": "2026-01-26 04:41:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rjlam",
              "author": "brandon-i",
              "text": "Btw, my last startup I exited finds fraud in healthcare bills. I can help if needed with any healthcare bills you may need adjudicated.",
              "score": 16,
              "created_utc": "2026-01-26 04:58:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rpx65",
                  "author": "nofilmincamera",
                  "text": "Luckily I am pretty good at that and had excellent insurance. But that is very kind and much needed in that industry.  I was lucky to have a network of people in the Med Industry also to pull from.  I am sure you found the solution is largely incompetence and bureaucracy. Lots of people that care.\n\nI do work now with a non profit working with families because navigating should not need experts, and essentially negotiating approval like a sales contract.\n\nYou don't sound like you need any help but if you do I could connect you with an Infomatics Program ( Nursing PHD program), could be partner opportunity. \n\nReally cool project.",
                  "score": 7,
                  "created_utc": "2026-01-26 05:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r2bcx",
          "author": "SituationMan",
          "text": "What don't you do with it?",
          "score": 5,
          "created_utc": "2026-01-26 03:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r31vj",
              "author": "brandon-i",
              "text": "Play Minecraft because itâ€™s a Linux device.",
              "score": 2,
              "created_utc": "2026-01-26 03:17:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r3ayf",
                  "author": "trichocereal117",
                  "text": "Prism Launcher would like a word ;)",
                  "score": 6,
                  "created_utc": "2026-01-26 03:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rk4i3",
          "author": "GoodbyeThings",
          "text": "https://media1.tenor.com/m/s3XrUKl2a1oAAAAd/congrats-happy-for-you.gif",
          "score": 6,
          "created_utc": "2026-01-26 05:02:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5tfy",
          "author": "Sl33py_4est",
          "text": "I have one of those\n\nI use it for video models in comfyui, huge llm/vlms in llamacpp, and semi automated student distillation projects (WIP example tiny talking head distilled from liveportrait+ditto)\n\nwhat are you gonna do with it",
          "score": 3,
          "created_utc": "2026-01-26 03:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r7an7",
              "author": "brandon-i",
              "text": "I was probably just going to create a Tailscale instance and let my friends use it via SSH.",
              "score": 6,
              "created_utc": "2026-01-26 03:41:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sh1ac",
                  "author": "Sl33py_4est",
                  "text": "i dont like my friends enough for that ðŸ˜…",
                  "score": 1,
                  "created_utc": "2026-01-26 09:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rcx1w",
          "author": "fire_inabottle",
          "text": "If you want to code, glm-4.5-Air 4-bit quantized is awesome and if you put your KV cache into fp8 you can get 128k context.  https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 3,
          "created_utc": "2026-01-26 04:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rhl1y",
          "author": "thebadslime",
          "text": "I would use it for endless finertunes",
          "score": 3,
          "created_utc": "2026-01-26 04:45:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rt5e9",
          "author": "brandon-i",
          "text": "I also, found another use case for it. I can do transfer learning and RL for some robotics hackathons. If anyone has cool ideas that they want to work together on or see come to life I would love to hear them. Send me a DM.",
          "score": 3,
          "created_utc": "2026-01-26 06:06:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1rlhyz",
          "author": "brandon-i",
          "text": "I have a serious question. How long will this DGX Spark last me? Will models get smaller and better so I can run open source ones locally that will be as good or better than Claude 4.5 Opus before this unit becomes obsolete?\n\nOr is the assumption that they will get bigger and will still not fit onto my DGX?\n\nI have heard that we will have a much better small language models that can just be as good because they are fine tuned for specific tasks like coding and my assumption is that this DGX can last a long time (assuming it doesnâ€™t brick. We had 2-3 brick at the Hackathon from some vibe coders using Claude Code)\n\nOn Friday I spoke to an engineer at Google Deepmind saying they already have â€œinfinite context lengthsâ€ or what not and I wasnâ€™t really convinced, but Iâ€™m not sure what to believe since he was google deepmind.",
          "score": 2,
          "created_utc": "2026-01-26 05:11:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rw0mo",
              "author": "Ok_Difference_4483",
              "text": "Smaller and smarter models for sure. Step3-VL-10B is a clear example that a much smaller model 10-20x compared to big MOEs that they could perform much better than what we currently have today and we have barely scratched the surface. Kimi Linear, Deepseek V3.2 DSA is also very experimental/new but we are definitely moving towards longer context/and still with good quality. Engram will accelerate this even further.\n\nI was wondering if I could inbox/speak more about this. I am looking into improving models like GPT-OSS by converting it into an MLA/DSA model/Converting it into a Diffusion model/NVFP4 KV Cache, or even just doing REAP/Pruning on the model, I have a 90B GPT-OSS model(Pruned from original 120B), and Iâ€™m trying to maybe get it down to even 60B. All in all, Iâ€™m really looking into getting more resources/Compute for doing this kind of work. Wondering if you can help/ or know anyone who I might be able to get support from?",
              "score": 2,
              "created_utc": "2026-01-26 06:28:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1se31o",
              "author": "inteblio",
              "text": "Congratulations & good work.\n\nProbably you can see both sides. Big get bigger, small get smaller. But all are getting better.\n\nMy hunch is that \"orchestration\" is the angle to watch. So, effecient C logic that becomes like \"hardwire\" between the flexible \"tentacles\" of LLMs. Bringing immense effeciency,speed, and capability boosts. AI coded, obviously. We're talking about pseudo-AI density C. I mean, in a fanciful near-future... the AI writes code to perform the tasks it needs to, right? Thus speeding up its own \"paths\". This is a cloudy \"direction\" take, to your question about the future. \n\nYour hardware likely has some good lifespan. But the AI space is going to go NUTS from here in. \n\nSolve more world!",
              "score": 2,
              "created_utc": "2026-01-26 09:02:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sfjhe",
              "author": "ProfessionalSpend589",
              "text": "If your budget allows it I would consider buying a second DGX to pair it with.\n\nIt would allow for bigger general-purpose LLMs. And just offers more RAM for tasks. Maybe NVidia offers speed ups Â when you cluster them (I vaguely remember that was the case, but do your own research).",
              "score": 2,
              "created_utc": "2026-01-26 09:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sdi2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-26 08:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uhmzr",
                  "author": "brandon-i",
                  "text": "It wasnâ€™t anything irreversible. I think they were just doing stupid shit like Claude Code on yolo mod in root. It was 2-3 out of like 25-30. I think also there was an issue with like some sort of adapters. These machines get reflashed for every event so maybe that contributes to it as well.\n\nIt didnâ€™t seem like it was any sort of over clocking issue. These things handled some crazy shit. My friend was really like putting it through the wringer.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:45:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uhnwh",
              "author": "dobkeratops",
              "text": "I agree there will be specific finetunes that will let a smaller box punch above it's weight in one domain, but also remember they can be clustered, regarding \"how long will it last\". Out of the box they can be paired up directly",
              "score": 1,
              "created_utc": "2026-01-26 16:45:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sdmm9",
          "author": "jacek2023",
          "text": "s/vllm/llama.cpp and then Nemotron will use 30GB of memory instead 100GB",
          "score": 2,
          "created_utc": "2026-01-26 08:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uitz7",
              "author": "brandon-i",
              "text": "Thanks so much for this one. I kept trying to optimize it by changing the GPU memory but only got it down to 90 GB. Should I use this for all of the other models too?",
              "score": 1,
              "created_utc": "2026-01-26 16:50:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sfj9n",
          "author": "foldl-li",
          "text": "Put it into a box, write down my address, and send it to UPS.",
          "score": 2,
          "created_utc": "2026-01-26 09:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sixif",
          "author": "Legitimate-Pumpkin",
          "text": "Itâ€™s sort of funny you won it and donâ€™t know what to do with it. Who chose the prizes for the hackathon? ðŸ¤£",
          "score": 2,
          "created_utc": "2026-01-26 09:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1um6rh",
              "author": "brandon-i",
              "text": "Dell and Nvidia. I actually didnâ€™t know what the prizes were to be honest. This is just a really hard problem that I wanted to solve and they gave me an opportunity to do it with a super computer which I usually donâ€™t get.",
              "score": 3,
              "created_utc": "2026-01-26 17:04:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1uuerq",
                  "author": "Legitimate-Pumpkin",
                  "text": "Congrats on the opportunity then. It must have been quite satisfying :)",
                  "score": 3,
                  "created_utc": "2026-01-26 17:40:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r6bat",
          "author": "brandon-i",
          "text": "Also, I was literally only able to get max 45 TKS with the nemotron 30B with VLLM. Is best solution to just local inference with Ollama?",
          "score": 1,
          "created_utc": "2026-01-26 03:35:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rwh40",
              "author": "Ok_Difference_4483",
              "text": "Try Sglang, and I think even GPT-OSS-120B is even faster and better than Nemotron 30B, I was getting around 150 TK/s on an H100 before? And around 100-120TK/s on blackwell sm120",
              "score": 1,
              "created_utc": "2026-01-26 06:32:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u3zn9",
              "author": "smflx",
              "text": "Sglang will be little faster than vllm. Well, the problem is Spark is slow in token generation due to slow memory. So, try fp8 or awq with vllm or sglang, or even smaller quantz with llama.cpp.\n\nPersonally I avoid ollama.",
              "score": 1,
              "created_utc": "2026-01-26 15:46:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u4bc6",
              "author": "smflx",
              "text": "Oh, btw congratulation to your winning hackathon",
              "score": 1,
              "created_utc": "2026-01-26 15:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uq3b7",
              "author": "Freonr2",
              "text": "It's not a blazing fast box, for LLM inference it is mostly limited by the memory bandwidth. \n\nThe trade off compared to a GPU is you get a lot more memory but it is much slower memory.",
              "score": 1,
              "created_utc": "2026-01-26 17:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rcaxx",
          "author": "fire_inabottle",
          "text": "https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 1,
          "created_utc": "2026-01-26 04:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rdm6q",
          "author": "vmspionage",
          "text": "I'm thinking about buying either a spark or jetson thor soon and using the guts for my GRiD 1520 minipc/cyberdeck.  Mostly for slow large model inference, fine tuning, other genai experimentation garbage and looking good",
          "score": 1,
          "created_utc": "2026-01-26 04:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rhlrp",
              "author": "nofilmincamera",
              "text": "I thought about a Jetsoj cyberdeck. Sounds fun",
              "score": 1,
              "created_utc": "2026-01-26 04:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rjljk",
          "author": "rm-rf-rm",
          "text": "Looks like you are SF/bay area based. If you are open to collaborating on fine tuning models with local data, DM me",
          "score": 1,
          "created_utc": "2026-01-26 04:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rm6gd",
              "author": "brandon-i",
              "text": "Iâ€™m not quite sure what the ask is, but feel free to reach out.",
              "score": 1,
              "created_utc": "2026-01-26 05:16:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rs6a5",
          "author": "Icy_Foundation3534",
          "text": "Sell it to me? No seriously just try models on hugging face.",
          "score": 1,
          "created_utc": "2026-01-26 05:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s428n",
          "author": "wildyam",
          "text": "Sell it",
          "score": 1,
          "created_utc": "2026-01-26 07:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1scedy",
          "author": "Mart-McUH",
          "text": "Hack it, obviously...",
          "score": 1,
          "created_utc": "2026-01-26 08:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1slduu",
          "author": "Maximum_Transition60",
          "text": "it's beyond me how you guys have NextJS using 60gb+ wtffff",
          "score": 1,
          "created_utc": "2026-01-26 10:09:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1smppe",
          "author": "fabkosta",
          "text": "Put it to a throughput test with an increasing number of users making requests to the model.\n\nI'm still waiting to see such things done. Everyone seem to focus on TG and PP, which is cool, but I want to know how it behaves regarding concurrency of requests.",
          "score": 1,
          "created_utc": "2026-01-26 10:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1snezg",
          "author": "FastDecode1",
          "text": "Sell it and live like a king for 20 years.",
          "score": 1,
          "created_utc": "2026-01-26 10:27:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1szqsv",
          "author": "tamal4444",
          "text": "congrats",
          "score": 1,
          "created_utc": "2026-01-26 12:09:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t8zi0",
          "author": "yensteel",
          "text": "Just don't tell your company you won one. \n\nE.g. \"Employee quits job over an Nvidia RTX 5060 â€” intern refused to hand in GPU won on an all-expense-paid business trip\"",
          "score": 1,
          "created_utc": "2026-01-26 13:11:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unjiu",
              "author": "brandon-i",
              "text": "I own my companies :)",
              "score": 1,
              "created_utc": "2026-01-26 17:10:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tec9v",
          "author": "michael554466",
          "text": "That's so awesome! Congratz!",
          "score": 1,
          "created_utc": "2026-01-26 13:41:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1th1of",
          "author": "sheikyon_",
          "text": "Run DeepSeek-R1-Distill-Llama-70B. NVIDIA even recommends running it on DGX Spark.",
          "score": 1,
          "created_utc": "2026-01-26 13:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tiqdk",
          "author": "AdDizzy8160",
          "text": "... prepare for a second hackathon, or be prepared for buying a second one ;)",
          "score": 1,
          "created_utc": "2026-01-26 14:04:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unsnk",
              "author": "brandon-i",
              "text": "Yeah! I am already registered for a few hackathons that I can win a Unitree robot dog and some other robotics hardware! I just need to figure out a cool project to do.",
              "score": 1,
              "created_utc": "2026-01-26 17:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tj9tp",
          "author": "JsThiago5",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tk3j3",
          "author": "StardockEngineer",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tklpu",
          "author": "wrath_Hog-",
          "text": "Rent me",
          "score": 1,
          "created_utc": "2026-01-26 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tmn2a",
          "author": "celsowm",
          "text": "sell it and got money",
          "score": 1,
          "created_utc": "2026-01-26 14:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tnmwt",
          "author": "TerminatedProccess",
          "text": "Nvidia just released and open source Persona talking model. Maybe you can work on interaction with humans for your project.",
          "score": 1,
          "created_utc": "2026-01-26 14:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tr1jf",
          "author": "HerrGronbar",
          "text": "Sell it and buy a car.",
          "score": 1,
          "created_utc": "2026-01-26 14:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1un6dk",
          "author": "NoWen7252",
          "text": "Sell it & buy nVidia stocks",
          "score": 1,
          "created_utc": "2026-01-26 17:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uvr92",
          "author": "NaiRogers",
          "text": "Sell it and buy a 6000 Pro!",
          "score": 1,
          "created_utc": "2026-01-26 17:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uwwr1",
          "author": "GCoderDCoder",
          "text": "Can you train nemotron to work? \n\nJkjk But seriously has anyone had consistent success on tool calls with nemotron 30b with thinking off? I got tired of experimenting and it thinks too much for me to use its speed in instruct mode.",
          "score": 1,
          "created_utc": "2026-01-26 17:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vjkev",
          "author": "reddit-369",
          "text": "Sell it and buy L40",
          "score": 1,
          "created_utc": "2026-01-26 19:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vuk4j",
          "author": "brandon-i",
          "text": "For those of you interested I just wrote a blog post about why/how I knew how to solve the problem. [https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn](https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn)",
          "score": 1,
          "created_utc": "2026-01-26 20:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1w6hh6",
          "author": "klapperjak",
          "text": " To had the shittiest project too, editing the demo video to win with a fake project oml. Also bro sshâ€™d into every other teams dgx and changed the password super anti-competitive",
          "score": 1,
          "created_utc": "2026-01-26 21:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wnh0c",
          "author": "SnowyOwl72",
          "text": "can it run crysis?",
          "score": 1,
          "created_utc": "2026-01-26 22:23:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x4x8z",
          "author": "goingsplit",
          "text": "if you have a lot of millionaire friends i think you should be able to raise enough computing resources, or?",
          "score": 1,
          "created_utc": "2026-01-26 23:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y53zc",
          "author": "Low_Cycle_4582",
          "text": "Run agent zero into it and make it your bitch ... Now u have super power to do anything ...",
          "score": 1,
          "created_utc": "2026-01-27 03:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yhrqo",
          "author": "Torodaddy",
          "text": "Sell it while people are still jazzed to buy it",
          "score": 1,
          "created_utc": "2026-01-27 04:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23rfnl",
          "author": "rc_ym",
          "text": "Give it to a random reddit commenter... Hint. Hint. :P",
          "score": 1,
          "created_utc": "2026-01-27 22:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn3qk",
          "author": "ismaelgokufox",
          "text": "Put Clawdbot on it for research purposes ðŸ˜‰",
          "score": 1,
          "created_utc": "2026-01-26 05:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rnsis",
              "author": "brandon-i",
              "text": "If youâ€™re serious I would actually do it. For science.\n\nThe only issue is I have seen what Claude Code did to other DGX sparks. We couldnâ€™t reflash them after the bricked.",
              "score": 1,
              "created_utc": "2026-01-26 05:27:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rnxs5",
                  "author": "ismaelgokufox",
                  "text": "I just did minutes ago and itâ€™s interesting! Have yet to do something specific with it but looks promising.",
                  "score": 2,
                  "created_utc": "2026-01-26 05:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1xf57e",
              "author": "bigh-aus",
              "text": "alternately use it to host models for clawdbot. I'm currently running it (day 0) on a vm, that's pointing back to ollama on my gaming rig (single 3090).  Super interesting ... but damn it's dangerous.",
              "score": 1,
              "created_utc": "2026-01-27 00:42:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rsl3v",
          "author": "Dry-Yogurtcloset4002",
          "text": "Sell it. You can barely do any serious AI works with it. Unified memory is a joke.",
          "score": 1,
          "created_utc": "2026-01-26 06:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sf9bi",
              "author": "Regular-Forever5876",
              "text": "soooo wrong!!! ðŸ¤£ðŸ¤£ðŸ¤£ I have 2 DGX and 1 Thor and there is REALLY ANYTHING better for the prince tag to thinkering!!",
              "score": 1,
              "created_utc": "2026-01-26 09:13:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s8otz",
          "author": "AVX_Instructor",
          "text": "Try running GLM 4.7 REAP (Q1/2/3/4), this is SOTA for self hosted",
          "score": 1,
          "created_utc": "2026-01-26 08:13:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sg0hp",
          "author": "Hearcharted",
          "text": "NVDA should hire/sponsor you, forever :)\n\nSo you can keep doing your thing to help society ;)",
          "score": 1,
          "created_utc": "2026-01-26 09:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ukslv",
              "author": "brandon-i",
              "text": "You know, I havenâ€™t been the best as selling myself so I donâ€™t know what the huge value add is if companies sponsored me. Because thereâ€™s the societal impact I can make, but often times businesses try to figure out how that aligns with their business strategy. For Nvidia maybe it makes sense. Iâ€™ll talk to their team.",
              "score": 2,
              "created_utc": "2026-01-26 16:58:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xe741",
                  "author": "Hearcharted",
                  "text": "ðŸ¥‡",
                  "score": 1,
                  "created_utc": "2026-01-27 00:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sgsd8",
          "author": "Easy_Kitchen7819",
          "text": "sell it and buy normal hardware for llm",
          "score": 0,
          "created_utc": "2026-01-26 09:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7f6o",
          "author": "Pretty_Challenge_634",
          "text": "Mail it to me, duh.",
          "score": -3,
          "created_utc": "2026-01-26 03:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r3f8p",
          "author": "stacksmasher",
          "text": "You better just send it to me for testing lol!",
          "score": -1,
          "created_utc": "2026-01-26 03:19:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5fo8",
          "author": "Lorelabbestia",
          "text": "Sell it to me! 2K offer",
          "score": -7,
          "created_utc": "2026-01-26 03:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rk6se",
              "author": "GoodbyeThings",
              "text": "No no, to me. 1K offer",
              "score": 3,
              "created_utc": "2026-01-26 05:02:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo595n",
      "title": "Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "author": "Kimi_Moonshot",
      "created_utc": "2026-01-27 05:39:09",
      "score": 488,
      "num_comments": 110,
      "upvote_ratio": 0.97,
      "text": "ðŸ”¹**Global SOTA on Agentic Benchmarks**: HLE full set (50.2%), BrowseComp (74.9%)  \n  \nðŸ”¹**Open-source SOTA on Vision and Coding**: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%)  \n  \nðŸ”¹**Code with Taste**: turn chats, images & videos into aesthetic websites with expressive motion.  \n  \nðŸ”¹**Agent Swarm (Beta)**: self-directed agents working in parallel, at scale. Up to **100** sub-agents, **1,500** tool calls, **4.5Ã—** faster compared with single-agent setup.  \n  \nðŸ¥**K2.5** is now live on [http://kimi.com](https://t.co/YutVbwktG0) in **chat mod**e and **agent mode**.  \n  \nðŸ¥**K2.5 Agent Swarm** in beta for high-tier users.  \n  \nðŸ¥For production-grade coding, you can pair K2.5 with **Kim**i Code: [https://kimi.com/code](https://t.co/A5WQozJF3s)\n\nðŸ”—API: [https://platform.moonshot.ai](https://t.co/EOZkbOwCN4)\n\nðŸ”—Tech blog: [https://www.kimi.com/blog/kimi-k2-5.html](https://www.kimi.com/blog/kimi-k2-5.html)  \n  \nðŸ”—Weights & code: [https://huggingface.co/moonshotai/Kimi-K2.5](https://huggingface.co/moonshotai/Kimi-K2.5)\n\nhttps://preview.redd.it/b3lldwzvwtfg1.png?width=1920&format=png&auto=webp&s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o20b0wc",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-27 13:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yv72k",
          "author": "Asleep_Strike746",
          "text": "Holy shit 100 sub-agents working in parallel sounds absolutely bonkers, definitely gonna have to test this out on some coding tasks",
          "score": 91,
          "created_utc": "2026-01-27 05:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20x5ir",
              "author": "IronColumn",
              "text": "the whole thing with sub-agents is protecting the primary model's context window from overload. But at 100 sub agents, just their reporting is going to stretch even a big context window",
              "score": 11,
              "created_utc": "2026-01-27 14:56:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o214mwe",
                  "author": "MrRandom04",
                  "text": "If they can coordinate well, they can actually accomplish much more than a single agent could for reasonably parallel tasks.",
                  "score": 7,
                  "created_utc": "2026-01-27 15:31:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21nqg7",
                  "author": "JChataigne",
                  "text": "What do you use to run several agents in parallel locally ?",
                  "score": 2,
                  "created_utc": "2026-01-27 16:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z6x58",
              "author": "Pro-editor-1105",
              "text": "https://preview.redd.it/9sku14q4gufg1.jpeg?width=1024&format=pjpg&auto=webp&s=afabc084139d09741d32972cbce3a8ae5ea16dfc",
              "score": 60,
              "created_utc": "2026-01-27 07:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yw8v4",
              "author": "derivative49",
              "text": "how are people with 1-2 gpus expected to do that ðŸ¤” (Can they?)",
              "score": 14,
              "created_utc": "2026-01-27 05:58:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z19kt",
                  "author": "claythearc",
                  "text": "You donâ€™t",
                  "score": 46,
                  "created_utc": "2026-01-27 06:37:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21o1wz",
                  "author": "Far-Low-4705",
                  "text": "you cant even run this model on 1-2 GPUs lol",
                  "score": 6,
                  "created_utc": "2026-01-27 16:55:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24wyxu",
                  "author": "newbee_2024",
                  "text": "The agent-swarm pitch is neat, but for most folks the question is: whatâ€™s the smallest â€œusefulâ€ setup locally?\nAnyone got numbers for VRAM/RAM at Q4/Q5 + decent context? Even rough ballparks help.",
                  "score": 1,
                  "created_utc": "2026-01-28 02:00:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26l92o",
              "author": "No_Afternoon_4260",
              "text": "Per today's [cooperbench](https://cooperbench.com/static/pdfs/main.pdf) (Stanford) I'm not so sure anymore",
              "score": 1,
              "created_utc": "2026-01-28 08:50:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z6fwd",
          "author": "Lan_BobPage",
          "text": "I'll download it and tinker with it in 3-4 years",
          "score": 48,
          "created_utc": "2026-01-27 07:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhycy",
              "author": "bobby-chan",
              "text": "For perspective, Llama 1 was 3 years ago.",
              "score": 43,
              "created_utc": "2026-01-27 09:05:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zqvcm",
                  "author": "Lan_BobPage",
                  "text": "I'll download it and keep it as a relic",
                  "score": 23,
                  "created_utc": "2026-01-27 10:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20uurf",
              "author": "Zyj",
              "text": "In 2-3 years we might get Medusa Halo with 256GB RAM. Not very optimistic about RAM prices. Youâ€˜d need 3-4 of them to run at Q4 with context.",
              "score": 3,
              "created_utc": "2026-01-27 14:44:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o226l36",
                  "author": "power97992",
                  "text": "In 2 years, you probably will see 5-8   trillion parameter models",
                  "score": 2,
                  "created_utc": "2026-01-27 18:15:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20xvnn",
                  "author": "Miloldr",
                  "text": "We are reaching physical and quantic limitsÂ ",
                  "score": 1,
                  "created_utc": "2026-01-27 14:59:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o229x6a",
                  "author": "Lan_BobPage",
                  "text": "Hold on I'm not THAT poor just yet",
                  "score": 1,
                  "created_utc": "2026-01-27 18:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o221579",
              "author": "Confident-Ad-3465",
              "text": "I'll download it, so my SSD doesn't feel empty inside.",
              "score": 3,
              "created_utc": "2026-01-27 17:52:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yvta6",
          "author": "Accomplished_Ad9530",
          "text": "Huh, OP u/Kimi_Moonshot was banned. Was it impersonation or a fake account or something?",
          "score": 70,
          "created_utc": "2026-01-27 05:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z48hp",
              "author": "Accomplished_Ad9530",
              "text": "Also, OP used to be an r/kimi mod, and now they're not. I wonder what's going on.\n\nhttps://preview.redd.it/bx68r5q0cufg1.png?width=2237&format=png&auto=webp&s=32ebebdd2a2e7c78eb3a2b76334966b06c94f657",
              "score": 28,
              "created_utc": "2026-01-27 07:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z49k7",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/c3v5wd24cufg1.png?width=2235&format=png&auto=webp&s=a382f13d022c1d3900797cf08240634badc9579b",
                  "score": 11,
                  "created_utc": "2026-01-27 07:02:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1z49h2",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-01-27 07:02:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z4odr",
              "author": "segmond",
              "text": "probably got auto flagged as spammer as they posted the same thing across multiple subreddits.",
              "score": 19,
              "created_utc": "2026-01-27 07:06:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yy3bm",
              "author": "eidrag",
              "text": "wait what",
              "score": 9,
              "created_utc": "2026-01-27 06:12:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzjl8",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/dhod4rt55ufg1.png?width=1607&format=png&auto=webp&s=78b61df5cf626108adf77346c0d1bc9541403496",
                  "score": 23,
                  "created_utc": "2026-01-27 06:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21objm",
              "author": "Far-Low-4705",
              "text": "of course they did, i hate reddit so much",
              "score": 7,
              "created_utc": "2026-01-27 16:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zy655",
          "author": "fairydreaming",
          "text": "I see impressive improvements in logical reasoning ([lineage-bench](https://github.com/fairydreaming/lineage-bench) [results](https://github.com/fairydreaming/lineage-bench-results/blob/main/lineage-8_64_128_192/README.md)):\n\n|Nr|model\\_name|lineage|lineage-8|lineage-64|lineage-128|lineage-192|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|moonshotai/kimi-k2.5|0.963|1.000|0.975|1.000|0.875|\n|2|moonshotai/kimi-k2-thinking|0.525|1.000|0.850|0.200|0.050|\n\nCongratulations on overcoming this hurdle and joining the elite reasoners club!",
          "score": 28,
          "created_utc": "2026-01-27 11:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ywpdn",
          "author": "-illusoryMechanist",
          "text": "1TÂ Activated Parameters 32B wow",
          "score": 51,
          "created_utc": "2026-01-27 06:01:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206jy7",
              "author": "pawofdoom",
              "text": "Same as K2 right?",
              "score": 20,
              "created_utc": "2026-01-27 12:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20763v",
                  "author": "KaroYadgar",
                  "text": "Yep",
                  "score": 6,
                  "created_utc": "2026-01-27 12:35:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zs2g9",
          "author": "Capaj",
          "text": "https://preview.redd.it/ryc3btmkevfg1.png?width=2629&format=png&auto=webp&s=2c6adae97f14b7c8d471b3bee52a0a73505e1e91\n\njust quickly tested with a prompt: write me an SVG displaying a fox riding a unicycle\n\nnot too bad",
          "score": 34,
          "created_utc": "2026-01-27 10:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zrb2v",
          "author": "MadPelmewka",
          "text": "How happy I am that itâ€™s a VL model, and such a powerful one according to the benchmarks! \n\nEarlier I made aÂ [post](https://www.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/)Â about how there are no good VL models for complex image captioning. Now there are! I'm so happy!",
          "score": 13,
          "created_utc": "2026-01-27 10:31:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z3ipo",
          "author": "polawiaczperel",
          "text": "Will they opensource their agent? It is amazing.",
          "score": 14,
          "created_utc": "2026-01-27 06:56:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zrxhd",
              "author": "adeadbeathorse",
              "text": "https://x.com/Kimi_Moonshot/status/2016034259350520226",
              "score": 11,
              "created_utc": "2026-01-27 10:37:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z38yj",
          "author": "Middle_Bullfrog_6173",
          "text": "This part is interesting: \"Kimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base.\"\n\n\nFor reference, K2 pretraining was 15.5T tokens. So almost double the pretraining, not just another SFT + RL.",
          "score": 21,
          "created_utc": "2026-01-27 06:54:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20oden",
              "author": "durable-racoon",
              "text": "is there a typo? 15.5 vs 15T? thats not double?",
              "score": 4,
              "created_utc": "2026-01-27 14:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qvyh",
                  "author": "Fit-Produce420",
                  "text": "It's trained in 30.5T, which is almost double 15.5T.",
                  "score": 10,
                  "created_utc": "2026-01-27 14:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o200pe9",
          "author": "ffgg333",
          "text": "How is creative writing?",
          "score": 9,
          "created_utc": "2026-01-27 11:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20801d",
              "author": "Cat-informer",
              "text": "Decent, good prose, grok levels of uncensored now :)",
              "score": 14,
              "created_utc": "2026-01-27 12:41:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2092du",
                  "author": "ffgg333",
                  "text": "Really, where did you test it,on theyr website or the API?",
                  "score": 3,
                  "created_utc": "2026-01-27 12:48:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20mb1c",
              "author": "Middle_Bullfrog_6173",
              "text": "Top open model in longform writing benchÂ https://eqbench.com/creative_writing_longform.html\n\n\nFrom short vibe checks also seems good.",
              "score": 11,
              "created_utc": "2026-01-27 14:01:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20ogfh",
              "author": "durable-racoon",
              "text": "Kimi K2 was better than opus for creative writing, cant wait to see how this performs",
              "score": 6,
              "created_utc": "2026-01-27 14:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yxrfq",
          "author": "ikkiyikki",
          "text": "You go Kimi! Not that I have any reason to cheer.... The Q4 version of this will still be larger than any rig this side of 20k will be able to run ðŸ˜”",
          "score": 20,
          "created_utc": "2026-01-27 06:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkv1y",
              "author": "Expensive-Paint-9490",
              "text": "A refurbished HP Z8 G4 with >600GB DDR4 is about 7k. Of course it would be extremely slow. Just six months ago it would have been 4k.",
              "score": 10,
              "created_utc": "2026-01-27 09:33:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2742j8",
                  "author": "TechExpert2910",
                  "text": "since weâ€™d need to use system ram as VRAM,Â a significantly better choice would be a 512 GB Mac Studio\n\nthe M3 Ultraâ€™s GPU is amazingly fast and is Appleâ€™s best\n\nitâ€™s probably 100x faster than running on a CPU + standard DDR5",
                  "score": 1,
                  "created_utc": "2026-01-28 11:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20vjxc",
              "author": "Zyj",
              "text": "5x Strix Halo, 640GB RAM (for q4), $10,000. It will be slow. Probably around 2.5 t/s for now. Might get speedups later on.",
              "score": 1,
              "created_utc": "2026-01-27 14:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24jfds",
              "author": "True_Requirement_891",
              "text": "It's it already int4 something?",
              "score": 1,
              "created_utc": "2026-01-28 00:48:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20yk7v",
          "author": "fragment_me",
          "text": "Seems interesting but the membership and quota details are confusing on the site. It's not clear if I get 10 requests or 10,000 per day with any membership. For example, the limits in the \"Allegretto\" plan are not clear. Can you clarify for people who are interested in the product?",
          "score": 5,
          "created_utc": "2026-01-27 15:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22tw92",
              "author": "b0307",
              "text": "same. i want to pay just to try the agent swarm but i cant find any details on how much usage I get, not even a vague description.",
              "score": 1,
              "created_utc": "2026-01-27 19:56:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ywt7k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 10,
          "created_utc": "2026-01-27 06:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yz1pb",
              "author": "misterflyer",
              "text": "[https://openrouter.ai/moonshotai/kimi-k2.5](https://openrouter.ai/moonshotai/kimi-k2.5)\n\nAnd yes, Mr. Wayne...\n\n... it *does* come in ***black***",
              "score": 22,
              "created_utc": "2026-01-27 06:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzi0d",
                  "author": "nycigo",
                  "text": "That's a bit expensive for a Chinese AI.",
                  "score": -26,
                  "created_utc": "2026-01-27 06:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20lbjw",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/dkrzkltzdwfg1.png?width=796&format=png&auto=webp&s=8c18c3e9a34bffc774baa484738e77dbb249e6c7\n\npiccolo The 1.8-bit (UD-TQ1\\_0) quant will run on a single 24GB GPU if you offload all MoE layers to system RAM (or a fast SSD). With \\~256GB RAM, expect \\~1â€“2 tokens/s.",
          "score": 3,
          "created_utc": "2026-01-27 13:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bpo6",
              "author": "uhuge",
              "text": "should I try on my newly purchased 2019 MB Pro?",
              "score": 2,
              "created_utc": "2026-01-28 00:09:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zrdr6",
          "author": "inkberk",
          "text": "SOOOOOOTTTTTAAAAAAA!!!!  \nGreat job Kimi Team!",
          "score": 6,
          "created_utc": "2026-01-27 10:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zd1g0",
          "author": "Different_Fix_2217",
          "text": "It seems really good so far. For sure best local model, need time to compare to claude / gpt 5.2.",
          "score": 5,
          "created_utc": "2026-01-27 08:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhqwj",
              "author": "ArFiction",
              "text": "what about compared to glm / m2.1?",
              "score": 5,
              "created_utc": "2026-01-27 09:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjpbr",
                  "author": "Different_Fix_2217",
                  "text": "For sure better than those but those are really small models for low level tasks locally / implementing other model's planning for cheap. Not really fair to compare imo. This is more around actual cloud models.",
                  "score": 9,
                  "created_utc": "2026-01-27 09:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zqmbp",
          "author": "Icy_Butterscotch6661",
          "text": "Whatâ€™s â€œvisual codingâ€ in this context?",
          "score": 3,
          "created_utc": "2026-01-27 10:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bjgr",
              "author": "uhuge",
              "text": "see this f'd up button? make it ðŸ”˜&ðŸŒŸ",
              "score": 2,
              "created_utc": "2026-01-28 00:08:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20bgdz",
          "author": "c00pdwg",
          "text": "Thank god they provided the legend at the top of their graph",
          "score": 3,
          "created_utc": "2026-01-27 13:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o220ffa",
          "author": "Alternative-Way-7894",
          "text": "Looks like there is new architecture here with Ktransformers and KT-Kernel where you can get heteregenous inference where about 100GB of VRAM is enough to run the model at decent speeds if you have over 600 GB system RAM! Looks to be able to get decent output with this new technology! They even tried with as little as 48GB VRAM (2x RTX 4090)\n\nVery exciting!\n\nHave a look [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md)\n\n\\*EDIT\\* If you have even more system RAM....look at this. Not bad at all!\n\n\"This achieves end-to-end LoRA SFT Throughput: 44.55 token/s on 2Ã— NVIDIA 4090 + Intel 8488C with 1.97T RAM and 200G swap memory.\"\n\nMore details refer toÂ [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT\\_Installation\\_Guide\\_KimiK2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT_Installation_Guide_KimiK2.5.md)Â .",
          "score": 3,
          "created_utc": "2026-01-27 17:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228ff3",
          "author": "ConsciousArugula9666",
          "text": "already alot of provider choices and free on nvidia: [https://llm24.net/model/kimi-k2-5](https://llm24.net/model/kimi-k2-5)",
          "score": 2,
          "created_utc": "2026-01-27 18:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25kss4",
              "author": "BuffMcBigHuge",
              "text": "Didn't know about Nvidia! Thanks! Working great",
              "score": 1,
              "created_utc": "2026-01-28 04:10:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o23l34r",
          "author": "captain_cavemanz",
          "text": "I'm not convinced, have experimented against 5.2 on coding in an established rust codebase and its not as good in my highly opinionated view.\n\nAre there standardized metrics out there to do side by side performance on a codebase?",
          "score": 2,
          "created_utc": "2026-01-27 21:58:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o206qsf",
          "author": "Aggressive_Special25",
          "text": "How do you use this? Can I run in lm studio?",
          "score": 1,
          "created_utc": "2026-01-27 12:32:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o229bna",
          "author": "newbee_2024",
          "text": "The speed of AI development is so fast that I wake up every day feeling like I'm falling behind againðŸ˜‚A brand new concept has emerged<visual coding>Will visual coding become futuristic, friends?",
          "score": 1,
          "created_utc": "2026-01-27 18:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22dt5k",
          "author": "Bloodipwn",
          "text": "How generous are the limits in the subscription plan? And did somebody already test how good it works in claude code?",
          "score": 1,
          "created_utc": "2026-01-27 18:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2420uo",
          "author": "Aggressive_Arm9817",
          "text": "This is so insanely good, has anybody tried it in real coding tasks?",
          "score": 1,
          "created_utc": "2026-01-27 23:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26bv3g",
          "author": "GosuGian",
          "text": "This is insane",
          "score": 1,
          "created_utc": "2026-01-28 07:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27twzt",
          "author": "pratiknarola",
          "text": "u/Kimi_Moonshot     \nI am hosting the model on my 8x H200 node as per huggingface model card using vllm.    \nbut i am getting \"(no content)\"   in the output content.   is this known?   any guidance on how i can fix it ?",
          "score": 1,
          "created_utc": "2026-01-28 14:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o289zoh",
          "author": "OkBottle1699",
          "text": "Kimy k2.5  amd ai",
          "score": 1,
          "created_utc": "2026-01-28 15:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z1p52",
          "author": "Hurricane31337",
          "text": "Wow, how many RTX 6000 Pro are needed to run this? ðŸ¥²",
          "score": 1,
          "created_utc": "2026-01-27 06:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zfxpr",
              "author": "power97992",
              "text": "7 Â if u dont want to offload it onto the cpu.( It is Â around 595 GB in safetensors..) Â ",
              "score": 8,
              "created_utc": "2026-01-27 08:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o207pn7",
                  "author": "KaroYadgar",
                  "text": "I flinched like an abused dog when I saw that number.",
                  "score": 9,
                  "created_utc": "2026-01-27 12:39:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20jyhq",
                  "author": "LocoMod",
                  "text": "So about $6000 in RAM alone before even discussing the rest of the hardware.",
                  "score": 3,
                  "created_utc": "2026-01-27 13:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zinh3",
              "author": "dobkeratops",
              "text": "2 x 512gb mac studio ? (connected with RDMA, a pair of them is shown to do inference at 1.8x the rate of 1)",
              "score": 10,
              "created_utc": "2026-01-27 09:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o214akz",
              "author": "Capaj",
              "text": "you only need 8 h200s :D\nYou can buy a server with this config in a single rack for like 350k USD",
              "score": 2,
              "created_utc": "2026-01-27 15:29:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o221jlt",
              "author": "Alternative-Way-7894",
              "text": "Looks like you will need only 1 if you have about 600GB system RAM",
              "score": 1,
              "created_utc": "2026-01-27 17:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z0c05",
          "author": "iamsimonsta",
          "text": "initial results indicate this model should have been named kimi2.5-preview, definitely not ready for serious use :(",
          "score": -4,
          "created_utc": "2026-01-27 06:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkexh",
              "author": "__Maximum__",
              "text": "Elaborate?",
              "score": 6,
              "created_utc": "2026-01-27 09:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22j5wz",
                  "author": "iamsimonsta",
                  "text": "A simple code review request on 120K javascript file generated garbage, quoting non existent code with odd fixation on non existent backticks.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zo65w",
              "author": "True_Requirement_891",
              "text": "People are downvoting but I'm getting buggy code and somehow it still doesn't match sonnet in quality... using it inside claude code.",
              "score": 0,
              "created_utc": "2026-01-27 10:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o227suk",
                  "author": "iamsimonsta",
                  "text": "Wow I am getting downvoting for testing it?  \n  \nI gave it the source (.js) to my current project asked it for a code review including any obvious bugs, and it hallucinated / tripped balls a list of fictional issues like.a 128K context model from 2024.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:20:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zevb3",
              "author": "zoyer2",
              "text": "ouch! sadge",
              "score": -2,
              "created_utc": "2026-01-27 08:37:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22kgpr",
          "author": "lemon07r",
          "text": "Does the Kimi for coding API use the new model now?",
          "score": 0,
          "created_utc": "2026-01-27 19:14:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnk7fq",
      "title": "transformers v5 final is out ðŸ”¥",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "author": "unofficialmerve",
      "created_utc": "2026-01-26 16:07:40",
      "score": 439,
      "num_comments": 41,
      "upvote_ratio": 0.99,
      "text": "Hey folks, it's Merve from Hugging Face ðŸ‘‹ðŸ»\n\nWe've finally released the first stable release of transformers v5 in general audience, it comes with many goodies:\n\n\\- Performance especially for Mixture-of-Experts (6x-11x speedups)\n\n\\- No more slow/fast tokenizers: way simpler API, explicit backends, better performance\n\n\\- dynamic weight loading: way faster, MoE now working with quants, tp, PEFT..\n\nWe have a migration guide on the main branch; please take a look at it in case you run into issues, we also have documented everything in release notes. We appreciate the feedbacks, so feel free to create issues if you have any! ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v9rxe",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 18:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uaagj",
          "author": "jacek2023",
          "text": "\"Performance especially for Mixture-of-Experts (6x-11x speedups)\" please explain",
          "score": 88,
          "created_utc": "2026-01-26 16:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uaknp",
              "author": "MaxKruse96",
              "text": "Best guess is that transformers was horribly slow for them before, and now is better",
              "score": 86,
              "created_utc": "2026-01-26 16:14:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1we09f",
                  "author": "kthx0",
                  "text": "If you improved performance 2x you did something clever, if you improved it 10x you stopped doing something stupid",
                  "score": 47,
                  "created_utc": "2026-01-26 21:40:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uv83j",
                  "author": "TheRealMasonMac",
                  "text": "For reference, with the same setup, GLM-4.7-Flash currently takes 7 minutes per step. Gemma 27B takes 40 seconds.\n\n\nI guess the Unsloth team was waiting for this since they promised faster MoE training in the coming week.",
                  "score": 28,
                  "created_utc": "2026-01-26 17:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1usqps",
              "author": "NandaVegg",
              "text": "Transformers v4 had rather simple for loop for MoE model experts (except GPT-OSS, which had custom code for performance from day one, I believe) which caused massive under-utilization. As well. they now have more generalized solution for custom kernels.\n\nCongrats for the release, by the way!",
              "score": 21,
              "created_utc": "2026-01-26 17:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uau3z",
              "author": "jikkii",
              "text": "hey, there are mainly two PRs responsible for this:\n\n\\- [https://github.com/huggingface/transformers/pull/43126](https://github.com/huggingface/transformers/pull/43126)\n\n\\- [https://github.com/huggingface/transformers/pull/42697](https://github.com/huggingface/transformers/pull/42697)\n\nand more coming to continue down this road. These are initial speedups, but expect more down the road as we continue improving on it, delivering specialized kernels, etc.\n\n  \nEDIT: we have a dedicated post about it if you want to check it out: [https://www.linkedin.com/posts/ilyas-moutawwakil\\_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm\\_source=share&utm\\_medium=member\\_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg](https://www.linkedin.com/posts/ilyas-moutawwakil_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm_source=share&utm_medium=member_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg)",
              "score": 33,
              "created_utc": "2026-01-26 16:16:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uikwd",
                  "author": "llama-impersonator",
                  "text": "shouldn't this be an hf blog?",
                  "score": 26,
                  "created_utc": "2026-01-26 16:49:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zcyun",
              "author": "woct0rdho",
              "text": "You know it if you've seen this https://github.com/woct0rdho/transformers-qwen3-moe-fused\n\nThe MoE support in Transformers 5 is great, and there is still a lot of room to speedup on consumer GPUs",
              "score": 3,
              "created_utc": "2026-01-27 08:19:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uco0j",
              "author": "bick_nyers",
              "text": "Less for loops is my guess.",
              "score": 3,
              "created_utc": "2026-01-26 16:23:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x9x9f",
              "author": "RainierPC",
              "text": "From the 6x to 11x statement, it sure sounds like they parallelized things, that's why the range is written like that - the speedup depends on how many experts there are.",
              "score": 3,
              "created_utc": "2026-01-27 00:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ub304",
              "author": "-philosopath-",
              "text": "https://preview.redd.it/xqivm6x4ypfg1.png?width=744&format=png&auto=webp&s=6ff49e94d3d569bd414867d17f32d220f57e8715",
              "score": -4,
              "created_utc": "2026-01-26 16:17:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vcgvo",
          "author": "sir_creamy",
          "text": "this is awesome.  updated to v5 and vllm 0.14.1 (from 0.11) and my single prompt inference speed is up 50% and 40x concurrent inference up 100%",
          "score": 9,
          "created_utc": "2026-01-26 18:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ysqjq",
              "author": "__JockY__",
              "text": "I was like \"there's no fucking way\".\n\nUpdated vLLM from 0.12 to 0.14.1 and tps went from 70/sec to 98/sec with MiniMax-M2.1 FP8 on quad 6000 Pros. Holy fucking shit. That's an IMMENSE update.",
              "score": 6,
              "created_utc": "2026-01-27 05:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yw3os",
                  "author": "sir_creamy",
                  "text": "Glad it worked out for you! Â Iâ€™m going to test using the transformers tomorrow",
                  "score": 1,
                  "created_utc": "2026-01-27 05:57:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1vkoie",
              "author": "MammayKaiseHain",
              "text": "Does vllm use transformers internally ? I thought they had their own engine",
              "score": 4,
              "created_utc": "2026-01-26 19:31:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1w0jql",
                  "author": "sir_creamy",
                  "text": "I'm not sure -- why i included that i updated vllm as well",
                  "score": 3,
                  "created_utc": "2026-01-26 20:41:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uqyax",
          "author": "Edenar",
          "text": "Ok, what does that mean for me running small-medium sized MoE locally using llama.cpp on an NVIDIA GPU or AMD igpu (ie Strix Halo) ?\n(My feeling is : it use more compute so running MoE will be less memory bandwidth bound ? Or maybe i don't understand at all...)",
          "score": 16,
          "created_utc": "2026-01-26 17:24:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v0qpq",
              "author": "Thick-Protection-458",
              "text": "Llama.cpp is a fully separated engine.\n\n\nVllm maybe reuse some transformers internals, but not llamacpp",
              "score": 11,
              "created_utc": "2026-01-26 18:07:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2h5t",
                  "author": "Edenar",
                  "text": "Thx!",
                  "score": 3,
                  "created_utc": "2026-01-26 18:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v11nw",
              "author": "the__storm",
              "text": "Nothing, `transformers` the Python library is not involved when you're running a model with llama.cpp.  It's often the \"default\" non-production way to run a new model though, before it gets support in other inference engines (llama.cpp, vllm, etc.)",
              "score": 33,
              "created_utc": "2026-01-26 18:08:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2g08",
                  "author": "Edenar",
                  "text": "Thank you!",
                  "score": 5,
                  "created_utc": "2026-01-26 18:14:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1wp380",
                  "author": "segmond",
                  "text": "In the long term it means we can borrow ideas from the transformer implementation library and improve llama.cpp",
                  "score": 3,
                  "created_utc": "2026-01-26 22:31:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uw9yd",
          "author": "Odd-Ordinary-5922",
          "text": "\"MoE now working with quants\" this didnt work before?",
          "score": 3,
          "created_utc": "2026-01-26 17:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wngbw",
          "author": "TumbleweedDeep825",
          "text": "WHAT I'VE DOOOOOONE............. oh wait, wrong transformers",
          "score": 6,
          "created_utc": "2026-01-26 22:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmlx5",
              "author": "jikilan_",
              "text": "New divine?",
              "score": 1,
              "created_utc": "2026-01-27 01:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v8fkp",
          "author": "DigThatData",
          "text": "still no movement on the mythical `.generate` refactor then I take it?\n\nhttps://github.com/huggingface/transformers/issues/30810",
          "score": 2,
          "created_utc": "2026-01-26 18:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ucb26",
          "author": "a_beautiful_rhind",
          "text": "All previous stuff still works as before?",
          "score": 3,
          "created_utc": "2026-01-26 16:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ucpdo",
              "author": "-p-e-w-",
              "text": "No, otherwise there would be no need for a migration guide.",
              "score": 27,
              "created_utc": "2026-01-26 16:24:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ut50v",
                  "author": "FullstackSensei",
                  "text": "So, maintainer of projects using HF can expect a wave of AI PRs offering to upgrade to v5?",
                  "score": 5,
                  "created_utc": "2026-01-26 17:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1vez7t",
              "author": "TokenRingAI",
              "text": "Nope, it breaks everything",
              "score": 5,
              "created_utc": "2026-01-26 19:07:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ucug7",
              "author": "jikkii",
              "text": "some of the internals are reworked to offer a more solid, faster base. Some APIs are also reworked; we recommend you read the release notes before upgrading and that you test your stack on the new version. If there's anything missing or weird, don't hesitate to open an issue and we'll work with you on resolving them",
              "score": 4,
              "created_utc": "2026-01-26 16:24:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1w5kq9",
          "author": "IulianHI",
          "text": "oh nice, the quantized cache alone saved me like 6GB on my setup which is huge. been benchmarking these improvements on r/AIToolsPerformance and the MoE speedups are wild for running stuff like Qwen3 locally. also the simpler tokenizer API was long overdue tbh",
          "score": 1,
          "created_utc": "2026-01-26 21:03:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wlsii",
          "author": "rudokazexotohatu0r",
          "text": "Great work from the team. Looking forward to benchmarking those MoE speedups.",
          "score": 1,
          "created_utc": "2026-01-26 22:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x2t5y",
          "author": "victoryposition",
          "text": "Tried it -- but get OOMs when dealing with tight VRAM margins... it has an automatic cache warmup to load models faster. But I can confirm the grouped\\_mm is much faster for calibration.",
          "score": 1,
          "created_utc": "2026-01-26 23:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xom93",
          "author": "Plenty-Aerie1114",
          "text": "Thank you!!!",
          "score": 1,
          "created_utc": "2026-01-27 01:33:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ukzl9",
          "author": "fairydreaming",
          "text": "Finally! Hopefully DeepSeek V3.2-Exp/V3.2 support will be merged soon now. Four months to support a new model arch is a bit too long. :-)",
          "score": 0,
          "created_utc": "2026-01-26 16:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vtdh3",
          "author": "pmv143",
          "text": "Dynamic weight loading is the most interesting part of this release imo.",
          "score": 1,
          "created_utc": "2026-01-26 20:09:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk8zj1",
      "title": "Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/",
      "author": "[deleted]",
      "created_utc": "2026-01-22 22:31:28",
      "score": 403,
      "num_comments": 192,
      "upvote_ratio": 0.94,
      "text": "Lately I go on Reddit and I keep seeing the same idea repeated over and over again. Another chat app, another assistant, another â€œAI toolâ€ that, in reality, already exists â€” or worse, already exists in a better and more polished form.\n\nMany of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use. Iâ€™m not saying AI is bad â€” quite the opposite, itâ€™s incredible. But there are people pouring all their money into Anthropic subscriptions or increasing their electricity bill just to build a less polished version of things like OpenWebUI, Open Code, Cline, etc",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o16tkcl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-23 05:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14sgun",
          "author": "bigattichouse",
          "text": "yep.  Part of the reason I'm working on tools. Things I've always wanted in Linux that I don't have. Just little commandline things, and virtual machines things, and testing tools... stuff that still works when the bubble pops ( And learning how to finetune open models for specific tasks.).\n\nIt's kinda like seeing customers who decided they were gonna make a facebook killer in 2008 by copying facebook.",
          "score": 109,
          "created_utc": "2026-01-22 22:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14t5v8",
              "author": "OGScottingham",
              "text": "Honestly, a 2008 version of Facebook would probably end up becoming a modern Facebook killer pretty quickly.",
              "score": 76,
              "created_utc": "2026-01-22 22:42:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o14ylz8",
                  "author": "bigattichouse",
                  "text": "hah.. kinda like duckduckgo just being \"original flavor Google\"",
                  "score": 46,
                  "created_utc": "2026-01-22 23:10:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o16y0k1",
                  "author": "camwow13",
                  "text": "Facebook circa late 2000s and early 2010s was pretty awesome. People shared what they were doing. It was fun. \n\nNowadays group chats have kind of replaced that. But it's small and islanded.\n\nFacebook started jumbling up the news feed to show more addicting things. Emphasizing the share button. Emphasizing shared content over regular content. Then pages and shit you didn't even follow. Always emphasizing the most rage inducing thing to draw in the masses. \n\nNow you have to actively jump through hoops to see what your friends posted. Nobody but your crazy and kinda dumb friends post anything. The only updates you ever see friends post is public announcements of death/disease/marriage/accidents/etc. It's just another content aggregator with gambling addiction algorithms running it.",
                  "score": 20,
                  "created_utc": "2026-01-23 06:09:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14x6au",
                  "author": "According_Tea_6329",
                  "text": "Surprised no one has tried",
                  "score": 4,
                  "created_utc": "2026-01-22 23:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ejly",
              "author": "Amazing_Trace",
              "text": "There are companies that came after the first one, copied them, and successfully killed them.\n\nUber has died in several markets to their copies.\n\ngoogle copied and killed yahoo\n\noreos copied hydrox and killed them.",
              "score": 5,
              "created_utc": "2026-01-23 08:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1e0n0i",
                  "author": "Pyros-SD-Models",
                  "text": "If you think google â€œcopiedâ€ yahoo you are either 12 and therefore never used old yahoo or you have no clue of how their searches differ and the math behind it. \n\nJust because both make cars it doesnâ€™t mean Ferrari is copying Ford.",
                  "score": -1,
                  "created_utc": "2026-01-24 07:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15a04b",
              "author": "[deleted]",
              "text": "It's practically the same use I give it it's literally the best way to create a small tool that used to take days or weeks.",
              "score": 3,
              "created_utc": "2026-01-23 00:10:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19sn67",
              "author": "bigh-aus",
              "text": "This is the best way to do it. Build things you want, and will use.  I think self hostable stuff is an excellent area of focus for projects too (replace any online service you use with something that can run on a pi).",
              "score": 3,
              "created_utc": "2026-01-23 17:20:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1a306t",
              "author": "JMowery",
              "text": "Got any specific examples? That's ultimately what I want to use a local AI for on my Linux box. Would be great to see a more detailed explanation of how one goes about approaching these types of workflow/optimization things. :)",
              "score": 1,
              "created_utc": "2026-01-23 18:07:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1axhve",
                  "author": "bigattichouse",
                  "text": "Want a todo list program, work with your AI to design, build code tests, and build it! \n\nIn the outside world they call it \"dogfooding\" - you use the tools you build.\n\nYou don't need to share them unless you want to , just build the things you need to use on a daily basis.",
                  "score": 1,
                  "created_utc": "2026-01-23 20:28:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1cxew7",
              "author": "alitadrakes",
              "text": "When the bubble popsâ€¦ what do you mean",
              "score": 1,
              "created_utc": "2026-01-24 02:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1d1vcb",
                  "author": "bigattichouse",
                  "text": "AI inferences prices (especially at the big players) are current artificially low.  At some point the true cost will come to bear. You can't just keep pouring VC money at it.  It's just like the dotCom boom 25+ years ago... eventually the bill comes due.",
                  "score": 1,
                  "created_utc": "2026-01-24 03:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u737",
          "author": "RedParaglider",
          "text": "Yes, I built a pretty fucking amazing RAG system with all sorts of badass features with the goal of leaning hard on local LLM's for enrichment tasks, and can use different computers in the house to up the processing speed with different LLM capabilities on each computer, etc.    \n  \nBut truth is that there is one open source project out there that's better than mine because it's VC funded with a huge staff of people, and there's just no way a solo dude can keep up, it is what it is.  \n  \nRight now you are seeing a lot of the same types of tools because people are building projects largely because they are trying to learn.  I'm glad I made my system, I learned a shit ton, and I built a lot of tools around it that are super useful at least to me.  HOWEVER, at the end of the day it's just another \"same thing\" system.",
          "score": 45,
          "created_utc": "2026-01-22 22:47:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15jd2p",
              "author": "MaverickPT",
              "text": "What's the open source project you're talking about?",
              "score": 4,
              "created_utc": "2026-01-23 01:00:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15wexz",
                  "author": "RedParaglider",
                  "text": "[https://github.com/vmlinuzx/llmc](https://github.com/vmlinuzx/llmc) is my rag tool.\n\nLlama Index does a lot of what my system does much better.   I have an edge on them in some ways, but mostly their project is pretty badass.  [https://www.llamaindex.ai/](https://www.llamaindex.ai/)",
                  "score": 10,
                  "created_utc": "2026-01-23 02:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o255ook",
              "author": "Fabulous_Cloud_8239",
              "text": "yeah, maybe guys just coding for practice",
              "score": 1,
              "created_utc": "2026-01-28 02:45:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14tvx9",
          "author": "dsartori",
          "text": "This is early days for an exciting new technology. Tons of enthusiasm and the barrier to entry is pretty low!\n\nI think this is going to promote shallow implementations and people re-re-rebuilding the basics. I've certainly built my share of standard POC projects this past year: an agent, a chatbot, a RAG processor. Building skills and finding the boundaries is important.\n\nHurts nobody although I think many people are a bit over their skis in terms of how much value their whatever they made actually has. There is nothing wrong with that, either! They'll learn or they'll move on.",
          "score": 142,
          "created_utc": "2026-01-22 22:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15lm05",
              "author": "MmmmMorphine",
              "text": "I generally agree and think I am experiencing something similar, anecdotally.\n\nMy main fear is whether we are actually \"learning\" much compared to other approaches. I generally feel like I am offloading cognition and my brain is losing abilities rather than gaining them.\n\nI'm becoming an expert at prompting, but is that valuable? I'm truly not sure anymore",
              "score": 30,
              "created_utc": "2026-01-23 01:12:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15p2kc",
                  "author": "Internal_Werewolf_48",
                  "text": "Itâ€™s valuable, in the sense that it will become the new table stakes skills to possess.\n\nI predict AI will be the next Excel, as a tool. People will just assume you can operate it, the bottom quartile of people will be bad at it and make messes. The middle half will use it periodically to solve some real problem. The top quartile will make their living off it.",
                  "score": 25,
                  "created_utc": "2026-01-23 01:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15xmls",
              "author": "RasPiBuilder",
              "text": "This is true, but also I think there is a changing dynamic.\n\nA couple years ago, I'd just use whatever systems existed even it if didn't exactly fit what I wanted/needed..   Sure I *could* probably have just built out my own system but that would take weeks.\n\nNow I can just quickly write up some specs, toss it to the AI and, boom, have a functional app in minutes (relatively speaking). It may not be perfect, and definitely not enterprise level.. but it's also exactly what I wanted/needed.",
              "score": 9,
              "created_utc": "2026-01-23 02:20:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o173vbu",
                  "author": "littlelowcougar",
                  "text": "Yeah itâ€™s absolutely insane for personal projects like that.  Especially the latest frontier coding models.  And if youâ€™ve got existing software engineering experience and could technically build it yourself with all that ample free time you haveâ€¦ then man itâ€™s hard not to get excited watching an agent crush it in a fraction of the time it would have taken you.",
                  "score": 1,
                  "created_utc": "2026-01-23 06:56:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o19tzny",
                  "author": "bigh-aus",
                  "text": "Totally agree - It's going to really reduce the cost of a lot of software to near zero.   People will develop something (quickly) and then maybe maintain it - give it away for free.  \n\nSelf host-able oss stuff should get a lot more popular, which will mean less people use paid services.\n\nThere's a few areas that need some focus from some ideas people though - eg email.  I'd love to have a local gmail, where i could actually add the smart features *I want - eg create a mail rule so that advertising is automatically snoozed until a specific time on the calendar = better time boxing.*",
                  "score": 1,
                  "created_utc": "2026-01-23 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15smts",
              "author": "Ill-Bison-3941",
              "text": "I agree, it's like a new hobby. It's exciting!",
              "score": 6,
              "created_utc": "2026-01-23 01:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o16tutq",
              "author": "No_Bake6681",
              "text": "An interesting plot to follow is if vibe coding brings about the de-sass-ification of tools in favor of home grown vibe coded solutions that do exactly what the business needs for a fraction of the cost of renting.",
              "score": 3,
              "created_utc": "2026-01-23 05:37:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1an1mh",
              "author": "Significant_War720",
              "text": "I think what is annoying is some MC syndrom moron trying to srll their slop because they think they are the first one thinking about the most basic of tools",
              "score": 2,
              "created_utc": "2026-01-23 19:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1anmpq",
                  "author": "dsartori",
                  "text": "Like the ones open-sourcing a revolutionary chatbot with a pretentious three-part name in a github profile with one repo in it?",
                  "score": 1,
                  "created_utc": "2026-01-23 19:41:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15xoed",
              "author": "iMakeSense",
              "text": "What's a RAG processor? I'm running an indexer locally but I haven't heard of that",
              "score": 1,
              "created_utc": "2026-01-23 02:20:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o166sqr",
                  "author": "dsartori",
                  "text": "Sorry - not very precise. A while back I built a [prototype](https://github.com/dsartori/process-briefings) to test an idea I had for preprocessing documents for RAG by attaching metadata. It did seem to improve query results for this type of document in informal testing.  \n  \nThere are better ways to load context for the stuff I do than RAG so I haven't done much more with it.",
                  "score": 1,
                  "created_utc": "2026-01-23 03:10:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u01s",
          "author": "jacek2023",
          "text": "We are in the hype stage at the moment.\n\n  \nPeople who used to be experts in cryptocurrency masturbation are now AI experts, AI coaches, and AI leaders.\n\n  \nTwenty-five years ago, everyone was an Internet expert and every business was doing the Internet, now they are doing AI.",
          "score": 78,
          "created_utc": "2026-01-22 22:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15iirv",
              "author": "spyboy70",
              "text": "Those dipshits were selling virtual land before crypto. And somewhere along the way they got into digital beanie babies (NFTs) as well.",
              "score": 15,
              "created_utc": "2026-01-23 00:55:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15o1sa",
                  "author": "roosterfareye",
                  "text": "The dipshits that  sold the virtual land to dumb dipshits.",
                  "score": 7,
                  "created_utc": "2026-01-23 01:26:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o150v0v",
          "author": "kevin_1994",
          "text": "there's a lot of ai schizos whose ai of choice tells them they're going to change the world with some random-half baked idea. back in my day, they would see the hourly rate for a decent developer, cry a little bit, and move on to some other form of grifting like crypto.\n\nnowadays, claude vomits out some slop and a lot of the time these people genuinely believe theyve done it! the world is now changed for the better, due to their genius\n\nthe problem is that these people are not experienced developers, take no effort or time to truly understand the problem theyre trying to solve, but just want attention and to appear smart and special in front of other people, because maybe, deep down, they don't really believe it when claude tells them for the 85th time\n\nif these people were only capable of even the smallest amount of introspection, the world would be a much less annoying place. the hubris to believe your 2 hour long chat with <insert llm here> solved the problem of LLM AGI is amazing to me. they think the thousands of PhDs, who have devoted their life to a particular field, together will millions or even billions of dollars of R&D budget, are no match for their weekend vibecoded ai slop project\n\nok rant over",
          "score": 40,
          "created_utc": "2026-01-22 23:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1638q8",
              "author": "EmbarrassedBiscotti9",
              "text": "so you're telling me the robot lied when it said my observation was astute? please say cap",
              "score": 9,
              "created_utc": "2026-01-23 02:51:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o170sgf",
                  "author": "MrWeirdoFace",
                  "text": "You're right to say that!",
                  "score": 5,
                  "created_utc": "2026-01-23 06:31:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o16otch",
              "author": "hidden2u",
              "text": "I dunno, ChatGPT said my LLM idea with quantum gravity wells that forced vector alignment was good",
              "score": 5,
              "created_utc": "2026-01-23 05:01:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1bxtkw",
              "author": "AggravatinglyDone",
              "text": "My favourite example of this is Borris Johnson talking about Chat GPT: https://m.youtube.com/shorts/dRE41eYkui4\n\nWe all need enough humility to know that not everything we think or say, will be brilliant.",
              "score": 5,
              "created_utc": "2026-01-23 23:24:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14vx5f",
          "author": "Old-School8916",
          "text": "the ppl working w/ AI on proprietary stuff/contexts aren't publicizing it as much. thats whats happening in enterprises.",
          "score": 13,
          "created_utc": "2026-01-22 22:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14u4eb",
          "author": "kiwibonga",
          "text": "I did spend a long time inventing local tool calls, only to realize that's the specific killer feature (and only valuable feature) of all the vibe coding apps I hadn't tried yet, because I was more interested in computer use than programming.",
          "score": 12,
          "created_utc": "2026-01-22 22:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o156zd9",
              "author": "Foreign-Beginning-49",
              "text": "Interesting seeing you in these parts..Â  lots of bots around these days......... in fact they been around a long time. Just starting to get really good these bots are......oh and image gen too lots of realistic videos being made en masse.........best wishes..........",
              "score": -2,
              "created_utc": "2026-01-22 23:54:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dbki",
                  "author": "kiwibonga",
                  "text": "Are you the person from like 6 years ago who claimed to be \"in-the-know\" about psychic experiments at Skinwalker Ranch?",
                  "score": 3,
                  "created_utc": "2026-01-23 00:27:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u8d3",
          "author": "sine120",
          "text": "The price and effort of making those tools have gone down, and now everyone's excited to make something, but lack creativity, so they all converge on similar ideas.",
          "score": 10,
          "created_utc": "2026-01-22 22:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9i5s",
              "author": "AppointmentDry9660",
              "text": "You mean my app store scraper tool isn't the unique snow flake I thought it was??\n\nJust kidding. I knew something like it must already exist, I mean I'm using libraries that already exist too. The barrier to entry has been lowered and people think that since they have an idea , it's a novel thing.. I get it \n\nI still like what I built and it's on a 12 y/o machine using local inference so all the data stays there. I like owning data, I have my own blog I built too.. but it's more about the journey than the destination that makes these projects interesting",
              "score": 1,
              "created_utc": "2026-01-23 18:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a9xp9",
                  "author": "sine120",
                  "text": "I think it's cool if you're building stuff for yourself.  Keep data to yourself and learn something.  If you're putting in as little effort as possible, trying to make a quick buck and wondering why you have no users, you're an idiot.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:38:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14ttcm",
          "author": "NaiRogers",
          "text": "I guess the people doing different stuff or something that brings in revenue are not going to be out here taking about it.",
          "score": 11,
          "created_utc": "2026-01-22 22:45:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15n66y",
          "author": "RiotNrrd2001",
          "text": "Alta Vista, Ask Jeeves, HotBox, InfoSeek, Yahoo, and Google are just the search engine names I can *remember* from 1999.  But there were many many more.  I remember seeing web pages that *only* contained icon links to search engines, and there might have been fifty or a hundred of them.\n\nThen the bubble popped.  >POP<  You could hear it clear down on Wall Street.  So many companies just out and out disappeared. [fuckedcompany.com](http://fuckedcompany.com) was an actual site that documented the explosion, collecting stories of the stupidity as it collapsed.\n\nNote that not *all* of the companies I mentioned above disappeared, though.  Bubbles popping just clear out the chaff, they don't shut down the operation.  Just as in the search engine space, there's LOTS of things being built right now, and certainly some of them will remain; there will be losers but there will also be winners.",
          "score": 6,
          "created_utc": "2026-01-23 01:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15uhp7",
              "author": "ANR2ME",
              "text": "There was also a search engine called Excite or something ðŸ¤”",
              "score": 2,
              "created_utc": "2026-01-23 02:02:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1a9yp4",
              "author": "AppointmentDry9660",
              "text": "Gonna way back this and see if I can find it...",
              "score": 1,
              "created_utc": "2026-01-23 18:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ak7ab",
                  "author": "RiotNrrd2001",
                  "text": "If you wait a little while it might start up again.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14ulqp",
          "author": "mr_zerolith",
          "text": "Yes, very much.  \nA bunch of amateurs ( or just lazy programmers ) just got their hands on a new toy and they're going down the dunning-kruger curve and they think their prototype is a finished product.\n\nWait till they find out what creating quality software actually entails.",
          "score": 12,
          "created_utc": "2026-01-22 22:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15hy06",
              "author": "cosimoiaia",
              "text": "Those 'amateurs' will kick your ass because they're learning what the new paradigms entail the hard way and fast.\n\nI'm a system engineer with 25 years of experience. I've seen the death of more quality standards than I can count.",
              "score": 4,
              "created_utc": "2026-01-23 00:52:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15r1nn",
                  "author": "mr_zerolith",
                  "text": "I'll get nervous when they actually ship something worthwhile that is something more than a poorly done, partial clone of something else.\n\nI'm a systems engineer with more than 25 years of experience.\n\nAnd also you're talking to me like i don't use local LLMs for coding or something!",
                  "score": 5,
                  "created_utc": "2026-01-23 01:43:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1570e0",
          "author": "gnnr25",
          "text": "If you lived through the Dot Com era, you're probably amused and entertained.",
          "score": 6,
          "created_utc": "2026-01-22 23:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17voj5",
              "author": "a_beautiful_rhind",
              "text": "dot com 2.0 is much more grounded. that era had a lot of \"AI pin\" style of ideas, iirc. We learned; if only a little bit.",
              "score": 1,
              "created_utc": "2026-01-23 11:06:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14sc3e",
          "author": "Lazy-Pattern-5171",
          "text": "The AI was trained to regress to the mean and now everything is built by AI so everything now is gonna regress to the mean. You and I and everyone.",
          "score": 21,
          "created_utc": "2026-01-22 22:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14t7s1",
              "author": "Combinatorilliance",
              "text": "The medium is the message!",
              "score": 9,
              "created_utc": "2026-01-22 22:42:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14s6nr",
          "author": "Firm_Meeting6350",
          "text": "I think you make a point here. But I think it's how everything starts. Keep in mind that the LLMs have been trained on things that already existed before (obviously :D), so to do the first steps in this kind of new era it's only logical to start with some \"clones\". I agree that those people shouldn't think \"woah, I made the next 1B$ startup\" and post here, though :D",
          "score": 5,
          "created_utc": "2026-01-22 22:35:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14tzhz",
          "author": "philmarcracken",
          "text": ">Many of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use\n\nIf it doesn't include the word AI, the c-suite goes flaccid",
          "score": 3,
          "created_utc": "2026-01-22 22:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o150bl8",
          "author": "CanadaSoonFree",
          "text": "I find that AIs best use case for me personally is the role of a teacher or an advisor. So damn handy to be able to bounce dumb questions off it while youâ€™re learning something new. Contextual awareness is fantastic for my learning style.",
          "score": 5,
          "created_utc": "2026-01-22 23:19:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16z42w",
              "author": "tmvr",
              "text": "The issue with that is you need to use a model where you have some level of confidence that it's not talking nonsense, which is not easy. For example at work I mainly use Claude Sonnet or Opus through the Copilot subscription in VScode. It works great for coding. Also have the Copilot app in Teams and asking something there mostly leads to anger, that is where my soul goes to die. The issue is that is states things with absolute confidence even if they are wrong and it is sticking to it no matter what. The \"personality\" they gave it is also infuriating with the whole \"great question\", \"you are absolutely right\", \"I'm totally sure now this is the solution\" etc. style while getting stuck in suggesting stuff that just does not work even after giving it full error outputs or relevant logs. I'm better off searching the web myself, because I get less angry. A huge difference to the Claude models in VScode where it it pretty much knows what I want and how to do it.",
              "score": 1,
              "created_utc": "2026-01-23 06:17:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o16rgy0",
          "author": "Graemer71",
          "text": "To echo.what ithers have said, there are a lot of people who view Aai as a get rich quick scheme -.vibe code something that already exists,  stick the word AI on the adverts and hope the money rolls in.\n\nI started learning this stuff because its interesting and if I dont learn it for my day job and understand it I will become obsolete and at risk.\n\nSo I started a project - beginning with a chatbot with chat history and persona imported from a commercial one because the company were annoying me. Then I added specific tools to help me with my writing- uploaded all of my books into it so I could ask thr chatbot questions  about events and characters. Have it evaluate full manuscripts with specific editong questions -.review this characters arc and look for plotholes. \n\nThen I refined the memory storage, moved from Langchain to Langraph, and used a three tier summary system to summarise the books. Then I created an engineer persona and did the same with the systems source code and made it auto documenting. Then I hooked it into.an ad platform and three sales platforms ao that I could see how various ad campaigns worked and advise me on whether an ad was working and whether to scale it, stop it or just leave for another week. Then I added a visual cortex so it could generate and edit images. Then I automated the royalty system.\n\nThis has taken me two months to create and yet i'm not a developer  -.i'm a software tester. So I understand the development lifecycle well enough to spec things out properly and make sure it works.\n\nAnd this is game changing because now I can build any tool to make my life easier, exactly how I want it, in days. I've got no intention of trying to sell.any of this commercially, but its automated elements of my business that used to take me hours",
          "score": 5,
          "created_utc": "2026-01-23 05:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14tgw0",
          "author": "AetherSigil217",
          "text": "It's kind of normal for people with a new shiny thing to try it everywhere.  It'll take a bit for people to work out what it is and is not good for and focus on the useful parts.",
          "score": 4,
          "created_utc": "2026-01-22 22:44:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15782x",
          "author": "GTHell",
          "text": "The simple you mentioned is not so simple in practice. â€œJust slap together an LLM and extension and theyâ€™re all the sameâ€ I kid you not but if you were to build any of this product youâ€™ll realize we are far from having a standardized chat app or AI integrated product that work like traditional software",
          "score": 4,
          "created_utc": "2026-01-22 23:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15boae",
              "author": "[deleted]",
              "text": "Not at all, it's not simple, but making a single application work with Vibecoding isn't either. My point is that if, instead of trying to create a new open-source coworking space, we improve existing tools as has traditionally been done, we can surely achieve more with less. In this case, I've achieved almost the same thing just by changing the system prompt from programmer to office worker cline.",
              "score": 1,
              "created_utc": "2026-01-23 00:19:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dtys",
                  "author": "Savantskie1",
                  "text": "The problem with the existing system is that almost everything has become monetized and no longer open. People want they donâ€™t have to pay for.",
                  "score": 3,
                  "created_utc": "2026-01-23 00:30:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17ihev",
                  "author": "Winter_Educator_2496",
                  "text": "It is quite hard to create something that satisfies all. Because you need to be able to keep it fully open source while also monetizing it while also figuring out how to match the features of closed source cloud applications while only having access to a single local machine with varying hardware.\n\nIf you have an idea to improve a closed source tool - you have to start a new one.\n\nBut even improving the existing tools is not always possible because of how limited they actually are. Even the biggest open source alternative to LLMStudio I know, receives like 10 commits a month. They're often relatively poorly monetized and therefore do not have the man power to actually compete and incorporate the ideas other people present.",
                  "score": 1,
                  "created_utc": "2026-01-23 09:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15fgqu",
          "author": "Kholtien",
          "text": "I think there will be a day soon where we live in a world of personalised software. Where basically everyone has an app that does essentially the same thing but theyâ€™re all built different differently and custom tailored to us. Itâ€™s probably not the most efficient way to do it, but it means that everything is exactly the way I want it to be rather than having to deal with workarounds because one vendor or another decides to go one way where I would prefer preferred they had gone another",
          "score": 5,
          "created_utc": "2026-01-23 00:39:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15oxpz",
              "author": "Sabin_Stargem",
              "text": "I can see this happening with Linux.   Personalized distros, as simple or complex as one would like.",
              "score": 4,
              "created_utc": "2026-01-23 01:31:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o167nas",
          "author": "Positive_Ad_313",
          "text": "Using AI , most of the people take the output at the face value rather than makes their brain think aboutÂ ",
          "score": 4,
          "created_utc": "2026-01-23 03:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14whm8",
          "author": "No-Marionberry-772",
          "text": "im busy destroying hard work ive done on my computer, dont worry.",
          "score": 3,
          "created_utc": "2026-01-22 22:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14zies",
          "author": "false79",
          "text": "Honnestly - i'm just doing the same SWE stuff. But expoentially faster.",
          "score": 3,
          "created_utc": "2026-01-22 23:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15i798",
          "author": "mystery_biscotti",
          "text": "I'm just here for the all you can eat tokens, but I don't disagree with you.",
          "score": 3,
          "created_utc": "2026-01-23 00:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18wpa8",
          "author": "corbanx92",
          "text": "I mean I took 2 9b models and put them to manage a VM with different goals. It's actually hilarious watching them work over eachother. (Example of a completely pointless use of ai)",
          "score": 3,
          "created_utc": "2026-01-23 14:53:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15uzh8",
          "author": "chuckaholic",
          "text": "The sales guys did this. \n\nOk, a quick history of AI since the transformer was invented: \n\nGoogle invented something, decided it didn't fit their business model, and shelved it. Some guys found the paper Google wrote, decided they could build a business on it, and created OpenAI. Their product had an uncanny ability to generate text that passed the Turing test and started selling it. A bunch of other people realized it had a lot more potential and developed the tech as well. Once companies started jumping on the bandwagon, the sales guys stepped in and said, \"hold my beer\" and sold more than they had ever sold before. They got trillions of dollars in investment into various AI businesses and had CEOs around the planet scared to death that their company would miss out if they didn't jump onboard. Every CEO starts prostletizing the new meta and starts delivering their products and... No one really wants them. They're  kinda cool I guess, if you want to have a conversation with something that's not really intelligent. But the real value that they sold from the beginning, the only way most of these projects can even break even, much less turn a profit, is if they somehow increase profits, which they have not done, so far. Best bet so far is people willing to pay $20 a month to use a chatbot like a search engine. The one dim light at the end of the tunnel, and the only thing that will save all these investors' money is if someone comes out with an agent that can replace a human employee. Alas, there's really not much hope for that as we have started hitting walls in performance that throwing more compute cannot solve. People are starting to notice that the latest models are scoring better and better on all the benchmarks, but they really aren't any smarter, they are just getting better at taking the tests. In my honest opinion, Qwen2.5 was smarter than Qwen3. Maybe it's just the quants that my machine can run, but 3 can't answer questions as well. It can't digest internet results and summarize as well. Its logic is weaker. My theory is that Qwen 3 has more guardrails, it's safer. But that makes it dumber. I think that's the roadblock that the companies are coming up against. Since Chat-GPT talked that guy into killing himself, companies are not comfortable releasing a product that doesn't have strict safeties but those same safeties also make LLMs dumber. So companies are throwing everything they have at solving this while training for benchmarks and hoping there's some breakthrough that can get them over the last 30% of improvement they need to unveil the ultimate product. The game changer. \n\n**THE DIGITAL EMPLOYEE**\n\nIf they don't do it soon, all the seed money will run out, the investors will demand their profits, and there won't be any. They are all treading water right now, hoping for a miracle, and the sales guys are out there promising everything and delivering nothing. \n\nI can't wait to see what happens.",
          "score": 6,
          "created_utc": "2026-01-23 02:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zobe",
              "author": "tmvr",
              "text": "I like to yapp more than the average person, but even I would have put some paragraphs in there, because holy wall of text! :D",
              "score": 1,
              "created_utc": "2026-01-23 06:22:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a0zj0",
                  "author": "chuckaholic",
                  "text": "I normally have better composition. What if there was an AI that could clean up my writing? ðŸ¤”",
                  "score": 1,
                  "created_utc": "2026-01-23 17:58:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o159qcc",
          "author": "its_a_llama_drama",
          "text": "How many people have written hello world? \n\nIt's not stupid to learn through doing. It is not stupid to emulate what has already been done to work out how it works. \n\nWould it be stupid to want a new jumper and You see one you like, but you decide instead, you'd like to make your own, so you learn to knit or sew and make your own? Is it stupid that there are jumpers for sale, made by real designers clothing manufacturers but you chose to make your own? What about if you buy a knitting starter kit which rhousands of people own, and use a template that thousands more people have used to knit a jumper. Would you whinge if the person was hapoy with what they made? \n\nIt's called a hobby. People like learning. People like making stuff. People don't always care if something already exists. People like to see what they can make.",
          "score": 4,
          "created_utc": "2026-01-23 00:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ao75",
              "author": "[deleted]",
              "text": "Good point about learning. So, do you think this period of experimentation will eventually lead to real innovations? Or is it more likely to result in a bunch of \"homemade sweaters\" that no one uses after making them?",
              "score": 2,
              "created_utc": "2026-01-23 00:13:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dlt2",
                  "author": "its_a_llama_drama",
                  "text": "Mostly, no. Lets be honest, 99%+ of the stuff people are making with ai is not going to stick. Most of it isn't even going to interest anyone as a product. \n\nBut, there is something satisfying to build something and see it work. Some people who knit jumpers do sell them, on etsy, or market stalls or whatever. i am sure there are one or two people who took up knitting or sewing and made a handmade brand out of it, or maybe scaled it into a larger business. but these people mostly get few to no buyers. Probably the same with this. \n\nI think it depends what people's expectations are. Sure, there are people who believe they are going to change the world with ai from their bedroom. They are more than likely not going to. I am sure there will be that one in however many thousand who do make something useful, popular, genuinely innovative. But mostly, people doing this are like most people who try selling their clothes making hobby on etsy.\n\nI think most people's expectations are not 'i'm going to change the world'. just 'i made this and it works' or 'this is cool, i'm proud of what I made'. Those people don't post online everytime they make something. As they are not deluded enough to think it is importaht to anyone beside themselves, so online, it looks like everyone thinks they are some revolutionary genius. \n\nBut Most people are quietly tinkering away at home and way more than 99% of the ai stuff people make never leaves the machine it was made on or gets shared with anyone other than the person who made it. \n\nOne example i have relating to ai and one person making something genuinely good. I belive openwebui was created and is maintained by one person. They did an ama somewhere on reddit a while ago and i was surprised to find out it is one person making and updating it in their spare time. I guess they are one of those one in however many thousand people.\n\nEdit: openwebui was made by one person and was maintained by that one person for a while. But they are a small team now. Still, one person made and maintained one of the most popular frontends around.",
                  "score": 4,
                  "created_utc": "2026-01-23 00:29:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15ugrx",
                  "author": "mister2d",
                  "text": "\"Homemade sweaters\" is so appropriate for my LLM coding activity. I didn't come up in the industry necessarily as a coder, but as a really good systems integrator. I always marveled at the developers I worked with that could take something from conception to a finished solution. (I used to work adjacent to the IBM Watson team over a decade ago. To get a LLM response on home hardware would take a few weeks!)\n\n\nNow I'm taking all that institutional knowledge with requirements gathering, proof of concept, integration and deployment, and am able to close the loop myself! â˜ºï¸\n\n\nAnd because I run it locally I get to keep sensitive stuff in my house, iterate as fast as I need to, and not rely on a vendor to care about my edge case.\n\n\nSome real examples of the work I found useful:\n\n\n1) forking open source projects and extending for features that I need. Did this for esp32 firmware recently.\n\n\n2) Reimagined a turn based game from the dialup BBS days that now has AI driven traders and RAG powered interactions. It's about 70% complete.\n\n\n3) Created an application to turn a small TTS model (Kokoro-TTS) into a streaming/near-realtime OpenAI endpoint for local TTS. That was until I found someone had done it better already.Â \n\n\nSo yeah they're homemade, but they get worn frequently. ðŸ˜Š",
                  "score": 2,
                  "created_utc": "2026-01-23 02:02:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15ch9r",
                  "author": "myelodysplasto",
                  "text": "Some people will have homemade sweaters and others will come up with something new.",
                  "score": 1,
                  "created_utc": "2026-01-23 00:23:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t2vq",
          "author": "MissJoannaTooU",
          "text": "Not really.",
          "score": 2,
          "created_utc": "2026-01-22 22:41:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o150ofq",
          "author": "Ztoxed",
          "text": "I think my idea for a model ( if I ever get there will be unique ) .So there is that drive as well.  \nexploring. Like first time I ever built a PC, hard as it was. It was a great thing to learn.  \nThat was decades ago, if we find something that challenges us. It wont be the same.   \nIf we do it different.",
          "score": 2,
          "created_utc": "2026-01-22 23:21:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o153l86",
          "author": "jazir555",
          "text": "The problem for me is the agents are too slow at producing code for gigantic codebases, and they still make *way* too buggy code. I'm vibe coding a gigantic project, and it's still effectively impossible to one shot anything, and I'm stuck having agents go in loops for hours until they finally fix *insert bug here* before continuing. My project will be functional when it's over, but I'm spending tons more time than should be necessary just making the AIs troubleshoot their own work, even shuttling it to other agents when the first one can't figure it out.\n\nIt's a *substantial* project that will have well over 1M lines of code, so it's definitely taking me a while. I'm legitimately rate limited by how fast these models can output tokens, even using cloud models. Once it's done, it will be a commercial product and I think it will be very disruptive to the industry I'm targeting, but the development time is just off the chain given the intricacy and complexity of what I'm building.\n\nI'm sure many devs are stuck in the same place, inching forward on their vibe coding projects and debugging. By the end of the year or early-mid next year we'll see disruptive vibe coded software products hit the market.",
          "score": 2,
          "created_utc": "2026-01-22 23:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16anm3",
              "author": "Imaginary-Unit-3267",
              "text": "How do you know this will even end up as anything useful? Are you a dev with programming experience already prior to AI?",
              "score": 1,
              "created_utc": "2026-01-23 03:33:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o15awoa",
          "author": "seymores",
          "text": "Haha, so true.",
          "score": 2,
          "created_utc": "2026-01-23 00:15:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15gvbi",
          "author": "florinandrei",
          "text": "Everyone doing the same thing is how every cycle begins.\n\nSource: previous cycles.",
          "score": 2,
          "created_utc": "2026-01-23 00:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15jb4d",
          "author": "spyboy70",
          "text": "I don't understand how all of these AI app companies start up so fast, take money from everyone, and then go under in about 18 months.  There's no actual SLA, which makes me not want to use them, or to integrate their apps into my workflows because I know they're not going to be here in a year's time.",
          "score": 2,
          "created_utc": "2026-01-23 00:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15uipq",
          "author": "brickout",
          "text": "Yeah. Someone needs to figure out how to actually use AI in the most impactful way. I have an idea.",
          "score": 2,
          "created_utc": "2026-01-23 02:03:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15zwns",
          "author": "ANR2ME",
          "text": "There is this EBM reasoning model https://sudoku.logicalintelligence.com/",
          "score": 2,
          "created_utc": "2026-01-23 02:32:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o164957",
          "author": "CSharpSauce",
          "text": "People aren't going to talk about the stuff that makes money.  You'd just be inviting new competition.",
          "score": 2,
          "created_utc": "2026-01-23 02:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1667gq",
          "author": "Ademn_iii",
          "text": "I totally get what you mean. Right now, thereâ€™s way more clutter than genuinely useful stuff in the AI tools space. But in a way, this messy phase, annoying as it is, feels like a necessary part of the ecosystem growing upâ€¦ kind of like the awkward â€œgrowing painsâ€ stage.  \nSo for developers, the real challenge and honestly, the real opportunity, is to focus on creating and championing design patterns that let the next wave of tools be more connected from the start, and cut down on all the reinventing-the-wheel.",
          "score": 2,
          "created_utc": "2026-01-23 03:07:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16kciq",
          "author": "User1539",
          "text": "Same as it ever was. \n\nWhen BBS systems were hot, everyone wrote one. \n\nWhen Webpages were hot, everyone wrote one. \n\nWhen File Sharing systems were hot, everyone wrote one. \n\nWhen chat systems were hot ... \n\nWhen video streaming ...\n\netc, etc ... \n\nThere are a lot of good reasons. It's good for developers to get familiar with the hot new thing. Everyone is trying something a little different, looking for a refinement that will move the ball forward, and a bunch of businesses need those people, because they have ideas they need developers who are familiar with the new tools to implement new ideas. \n\nIt's fine, most of it never goes anywhere, but it's good to have something to show in an interview. Besides, you might stumble on something, and start your own thing.",
          "score": 2,
          "created_utc": "2026-01-23 04:32:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1781qy",
          "author": "m31317015",
          "text": "For the less polished projects... That's what the open source devs circle been since when the entry threshold become somewhat low w/ the help of YouTube tutorials. It's the nature of devs. \n\nSome may say \"if you never try how would you find out whether you wanna go on with it or not?\", some may say \"it's an utter waste of time to reinvent the wheel\". For me I say it depends on your goal and purpose. Many devs started out building something because there's nothing on the internet rn can fulfill their own needs, maybe somebody wants a webui, live2d w/auto motions, tts and text gen all in one place. They build it, people like it, that's it.\n\nThe electricity part, I don't like it as well. Few years back we're talking 'bout bitcoin ruining environments and drawing insane amount of power w/ billions of ASIC miners\", now nobody can stop xAI using portable generators. Welp.",
          "score": 2,
          "created_utc": "2026-01-23 07:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1789bd",
          "author": "Lesser-than",
          "text": "Do one thing and do it well, leave those other things for other applications that can do them well.",
          "score": 2,
          "created_utc": "2026-01-23 07:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o183be9",
          "author": "boisheep",
          "text": "Well I'm making a RPG with hidden states and a LLM powered RPG engine where many characters can interact at once.\n\n\nSo far much superior to these character cards or silly tavern or kobold...\n\n\nSomething weird is that the emotional hidden states improve memory even in trash models. I think the fake emotions cause correlations between data. Huh",
          "score": 2,
          "created_utc": "2026-01-23 12:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18c8g0",
          "author": "frmlsshrmn",
          "text": "Most people are not original nor are they particularly creative (just look at your average Reddit comment section). LLMs have dramatically lowered the barrier to entry for creating passably functional software so everyone and their uncle is getting in on the action. That's not to say that there isn't somebody out there right now with the perfect combination of skill, passion, intelligence and discipline using AI as a force multiplier to work on something that's going to blow everyone out of the water.\n\nIf every Tom, Dick and Sally was building the next big thing, then it wouldn't really be that big.",
          "score": 2,
          "created_utc": "2026-01-23 13:05:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ryii",
          "author": "bigh-aus",
          "text": "I think it's often a learning experience for many people - lets build some achievable thing that already exists, don't push the boundaries on the ideas - learn the tooling/ prompting.\n\nI don't mind new takes on old stuff, it's when it's not maintained after it's coded (due to time constraints or inability to actually code).\n\nWhat I do mind is if everyone is trying to launch paid stuff, that's vibe coded with no knowledge backing.\n\nNew ideas are hard, much like building a complete, secure product!\n\nAlso prompting is replacing some libraries entirely. Fascinating space - I'm definitely still learning.",
          "score": 2,
          "created_utc": "2026-01-23 17:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19t7wy",
          "author": "Xatter",
          "text": "I was thinking this long before AI\n\nLike how are Okta and Auth0 even things?\n\nThey are just a single part of an application and a solved part years ago\n\nInstagram is worth a billion dollars? Itâ€™s literally the photo sharing feature from any number of photo management applications plus filters.Â \n\nSomething happened around 2010 where features became complete businesses.Â \n\nBut you know what? Iâ€™m clearly wrong because those people are so wealthy now they canâ€™t even spend all their money and I still need to work",
          "score": 2,
          "created_utc": "2026-01-23 17:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1azi0m",
          "author": "kompania",
          "text": "I feel like there are a lot of innovative projects out there, but nobody shares them due to fear of being copied. Everyone wants to commercialize their ideas.\n\nI also want to point out some interesting and unique projects that are currently happening.\n\nThe first one concerns the justice system. In my city, the Ministry of Justice is running a pilot program to implement LLMs to support prosecutors in solving complex cases.\n\nTheyâ€™ve identified 110,000 successfully concluded investigations. All court documents have been digitized, and they are currently running parallel tests comparing RAG vs. full-finetuning approaches. Whatâ€™s great about these investigations is that theyâ€™re a huge source of question-answer pairs.\n\nThe second project is being carried out by a friend of mine who is a sports bettor. His idea revolves around assessing the body language of soccer referees in the local league, or certain elements related to them during a match â€“ like their wording or decisions â€“ to find signals of corruption. I donâ€™t know if the idea is viable, but every few years a referee from that league gets convicted, so there might be something to itâ€¦\n\nI think there are currently many innovative projects, but everyone keeps them to themselves, looking for profit.",
          "score": 2,
          "created_utc": "2026-01-23 20:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14zz7o",
          "author": "Ztoxed",
          "text": "Myself, I want a challenge. Not to become and expert.   \nBut To do something and challenge myself.   \nI build Python predictions for awhile, learned some was hard, maddening at times.  \nNow I like Python. I think age gives different perspectives.   \nI want to learn, as as I am getting much older, scaling mountains is no long possible.  \nBut I can use my mind. I want more from myself. Not more from AI.",
          "score": 4,
          "created_utc": "2026-01-22 23:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15dzaz",
          "author": "mdmachine",
          "text": "You're watching the \"App Store Gold Rush\" replay, but faster and dumber.\n\n90% of these \"tools\" are just API wrappers built by people learning to code. They aren't products, they are tuition fees. The developers are paying Anthropic/OpenAI to learn how to build software that nobody needs because OpenWebUI and Cline already won the race.\n\nThe harsh truth? AI isn't a standalone product anymore. It's a feature. If your \"app\" is just a chat window that I have to Alt-Tab to, itâ€™s already dead.\n\nThe reality is those who are selling the shovels will as always be the winners.",
          "score": 1,
          "created_utc": "2026-01-23 00:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15hcty",
              "author": "manipp",
              "text": "God I hate LLM writing",
              "score": 6,
              "created_utc": "2026-01-23 00:49:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15tht3",
                  "author": "mdmachine",
                  "text": "The irony is palatable. ðŸ¤£",
                  "score": 0,
                  "created_utc": "2026-01-23 01:57:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14xg4c",
          "author": "Reasonable_Listen888",
          "text": "always can do your own research :) [https://zenodo.org/records/18332871](https://zenodo.org/records/18332871)",
          "score": 1,
          "created_utc": "2026-01-22 23:04:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15jbwf",
          "author": "cosimoiaia",
          "text": "In the early '00 everyone was building html pages on tiny webservers and some said that it would have been easier to be on geocities. Guess what happened.",
          "score": 1,
          "created_utc": "2026-01-23 00:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15ob7z",
          "author": "wivaca2",
          "text": "Not every problem in the world can be solved by AI or LLMs, and not everything that can be done with LLMs and AI is a problem.  Let's not forget we have had non-artificial intelligence for years not doing many of these things and managed to thrive.\n\nAs is typical for anybody who gets a new hammer, everything looks like a nail and most products are so commoditized that there is neither profit margin nor differentiation.  Now everyone has a new AI hammer and is trying to gin up differentiation by involving ML or LLM but they can only apply them to a narrow band of dubiously justifiable use cases for everyday devices.  AI doesn't help the refrigerator do its job, but it can be made to operate surveillance on buying habits and bring targeted advertising into the home.\n\nIf anything, in consumer products, AI has actually become more of a codeword for cloud subscription to retain functionality and surveil behaviors in the home than any actual intelligence.\n\n\"Intelligence\" in this case is the spy related definition, not the smart context.",
          "score": 1,
          "created_utc": "2026-01-23 01:28:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15s3l5",
          "author": "cmndr_spanky",
          "text": "Donâ€™t forgot the same dumbass Reddit posts over and over again. People who claimed to have invented x,y,z about models or idea",
          "score": 1,
          "created_utc": "2026-01-23 01:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15wadw",
          "author": "WillingMachine7218",
          "text": "So you're saying some people make derivative, mediocre software? What's you point here?",
          "score": 1,
          "created_utc": "2026-01-23 02:12:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15yz6r",
          "author": "basicKitsch",
          "text": "You ever make an led blink on an Arduino?",
          "score": 1,
          "created_utc": "2026-01-23 02:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15zc6l",
          "author": "cniinc",
          "text": "The truly innovative ideas are often hard to make money on. That's why Incubators exist - you need space to try it out because nobody can see the potential until there's an ecosystem to capitalize on it. Imagine if you discovered that you can make electric current, but there was no copper wire, like, anywhere. \"But I could light up a room! You just have to line the walls with hidden metal so that the electricity travels. Then you can get into a car and drive anywhere! ...but you have to get gas every 70 miles. You'll also have to pump it through a huge interconnected system of pipes throughout the country...\"\n\n\nSo instead, people try and solve what they think people want in ways that they think they want it. Ford is most famous for saying \"if I asked people what they wanted, they would have said a faster horse\" when he invented the model T.Â \n\n\nPeople also can't build things that are cool and don't immediately have a sales pitch. In this near-buesting bubble, to make anything is expensive and you need to have an ROI because nobody wants another OpenAI-sized mouth to feed with endless cash.Â ",
          "score": 1,
          "created_utc": "2026-01-23 02:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o160csu",
          "author": "ifupred",
          "text": "Well I don't know about others but it's helping my business with huge number of internal tools. Made life a lot easier. I less less value in external where everyone is fighting over the same thing",
          "score": 1,
          "created_utc": "2026-01-23 02:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o162fiw",
          "author": "Tiny_Arugula_5648",
          "text": "Fun fact about people who use AI to figure out what they should do with AI.. the models tell everyone the same things over and over again..  Of course no bothers to research what existing solutions are already out so they run off and do yet another version of the same thing everyone else is doing..",
          "score": 1,
          "created_utc": "2026-01-23 02:46:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o167aag",
          "author": "clockentyne",
          "text": "I'm building a TTS model, but with how many are now coming out I don't know if it's worth releasing it, if I get all of the kinks worked out.  I mean, I'll probably use it for my iOS app because I'll have unique voices no one else would have, but I don't know if it'll be worth open sourcing it with so many options that happened in the last month alone. It's not a vibe-coded project though and something I've been working on for months. :P",
          "score": 1,
          "created_utc": "2026-01-23 03:13:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16ba3g",
          "author": "rulerofthehell",
          "text": "Yes, machine learning has a vision problem",
          "score": 1,
          "created_utc": "2026-01-23 03:36:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16c8qw",
          "author": "Imaginary-Unit-3267",
          "text": "Meanwhile almost no one uses chatbots for the most obvious thing, which is getting them to ask YOU questions to help you clarify your OWN thoughts. Everyone tries to get it to think for them. I don't want a servant! I want a rubber duck that talks back.",
          "score": 1,
          "created_utc": "2026-01-23 03:42:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16iryh",
          "author": "chill-i-will",
          "text": "AI & plug and play platforms like zapier and n8n have made it very easy these days to create workflows and exchange data across different interfaces. Due to which I think a lot of people just out of sheer curiosity, desperation and fueled further by validation from their AI have started marketing these as products trying to make some money off it. Personally I build proof of concepts in various domains and post on LinkedIn to boost my profile visibility because if anyoneâ€™s worked in tech as I have they would know that thereâ€™s no way a single person is going make an actually functional groundbreaking application sitting alone vibe coding bunch of lines they themselves donâ€™t understand",
          "score": 1,
          "created_utc": "2026-01-23 04:22:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16jggl",
              "author": "chill-i-will",
              "text": "Also I think the root of this is less AI and more Social media. Itâ€™s made everything a lot more conspicuous so we see a lot more of everything than we did earlier. Theres just a lot more of everything on my feed but i see the complete opposite irl. But Im sure the visibility to these half baked products also motivates others to give it a try themselves and make more. Its like the bullwhip effect",
              "score": 1,
              "created_utc": "2026-01-23 04:26:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o16l43p",
          "author": "HealthyCommunicat",
          "text": "Yes. You need to have a real use case, most of the time big innovations and real things we would actually use are made simply because a person had a real strong necessity for that thing. \n\nIâ€™ve literally been tasked to try to replace as many workers as possible at my job so I think Iâ€™d like to think Iâ€™m not making anything thats â€œthe same thingâ€, (not to the more common extent at least) but actually will replace the need for 24/7 sysdba team when its all literally the near same queries and commands over and over. \n\nTo an extent Iâ€™m more than certain everyone can find a need that can be filled with LLMâ€™s, I just donâ€™t think most people are at the point to be able to start building that kinda thing though because most people kinda stop pushing for growth after a certain point where they think theyâ€™ve learned enough, or maybe they canâ€™t push to learn any further idk\n\nMost people either just do not have that strong of a use case / necessity that can be filled by it. I guess thats why its called a bubble.",
          "score": 1,
          "created_utc": "2026-01-23 04:37:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16nmzm",
          "author": "saltyourhash",
          "text": "This is what happens with hype cycles driven by algorithms for attention. People cling to the trend for content farming.",
          "score": 1,
          "created_utc": "2026-01-23 04:53:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qvk2",
          "author": "Yarafsm",
          "text": "We have half our team building cool internal automation projects which noone knows how to scale or integrate or build value in overall delivery chain.Its encouraged by leadership as its cool to do AI while client billability is old boring item.",
          "score": 1,
          "created_utc": "2026-01-23 05:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16rkld",
          "author": "bene_42069",
          "text": "This is what AI bubble is about. Most of it is just silly hype as AI, at least in the general publicity, is still a relatively hot and new technology. Just like the dot com bubble pop doesn't mean the world wide web suddenly shuts down, AI bubble pop does not mean that AI will disappear the day after, it just means that silly hype will fade and development will be more mature & steady. We'll get through the noise eventually.",
          "score": 1,
          "created_utc": "2026-01-23 05:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16rxwu",
          "author": "Bakoro",
          "text": "Building shitty versions of things makes me appreciate high quality things more.\n   \nI mean, I also don't try to sell the shitty versions of things I make from myself, so you're welcome for that.",
          "score": 1,
          "created_utc": "2026-01-23 05:23:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16shw8",
          "author": "Ylsid",
          "text": "You're absolutely right! They're all doing the same thing â€” and that's remarkable. This isn't just an insightful post â€” it's a revolution in app development.\n\nObvious botpost is obvious",
          "score": 1,
          "created_utc": "2026-01-23 05:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16u1ap",
          "author": "OccasionallyImmortal",
          "text": "Agentic AI seems promising. We keep thinking in terms of a single AI and client for each task, but it could be several per task. Even for development, an AI for architecture, one for performance analysis, one for testing, one for deployment... all working together. The testing agent can create a new agent to hunt down any bugs. A watchdog AI can determine when an AI is struggling, shut it down and replace it with a new one trained on a use case that more closely matches the problem.\n\nIt's complex. Once the framework for this interaction is created, this will take off... if we can manage it.",
          "score": 1,
          "created_utc": "2026-01-23 05:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16x4hd",
          "author": "IngwiePhoenix",
          "text": "AI itself is a token maschine - it can not \"create\", it can only \"process\". The real creativity to make something new comes from the human user.\n\nWell, let's just say, you know the Toy Story meme of the one guy in the shelf with an idea, whilst everyone in the shelf has the same?\n\nYeah, that.",
          "score": 1,
          "created_utc": "2026-01-23 06:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16xzv1",
          "author": "kidflashonnikes",
          "text": "Currently writing a paper on this. Itâ€™s not published but the title is something along the lines of a great reset. Once the AI bubble pops - it effectively nuclear bombs the entire industry horizontally and vertically - making money virutually cease to exist for funding. This reset - likely economic - will usher in a golden era of machine intelligence later on, but in the beginning people who have money to fund ai ventures will only fund ai ventures that are meaningful and create real value. Until the great reset event happens - ai slop companies and scam Altman will continue to thrive in this current env.",
          "score": 1,
          "created_utc": "2026-01-23 06:08:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16z0wv",
          "author": "rm-rf-rm",
          "text": "Amen!! I try pointing this out on their submissions but my exasperated tone is typically what gets the attention rather than the content",
          "score": 1,
          "created_utc": "2026-01-23 06:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16zaya",
          "author": "no_witty_username",
          "text": "The people that re building something special arent wasting time talking about it on reddit.",
          "score": 1,
          "created_utc": "2026-01-23 06:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1743l8",
          "author": "Defiant-Snow8782",
          "text": "Every tech product is converging to a RAG chatbot with occasionally some tools.",
          "score": 1,
          "created_utc": "2026-01-23 06:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o174ani",
          "author": "[deleted]",
          "text": "I'm not. I'm trying to develop and build something truly novel, unique and interesting. I will eventually post on here when it's ready, but it's a few more months off yet at least. It will be a genuinely clever ai that actually learns on its own and figures out how to achieve its own goals based on its own motivations and have full ability to digitally go and do anything it wants.",
          "score": 1,
          "created_utc": "2026-01-23 06:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1760gm",
          "author": "Electronic-Blood-885",
          "text": "Yes but I feel that this is also the point some will build the thing that matters and as people in the chat have said it usually the lessor items the social app that just worked the lesser google some one will make the dumb simple thing we  actual like or find useful",
          "score": 1,
          "created_utc": "2026-01-23 07:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o176sbq",
          "author": "Altruistic_Click_579",
          "text": "I think for normal personal and office use it will just be adaptable and powerful chatgpt-like chat llms. With decreasing cost and ability of local inference and open source models it will just be something you have running locally on your phone. Private and secure. \n\nNo one will make money on this use case though.",
          "score": 1,
          "created_utc": "2026-01-23 07:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17ibw7",
          "author": "Mart-McUH",
          "text": "To a degree. But how it is different from every other area? People are doing similar things (whether it is reading \\[same\\] books, watching \\[same\\] movies, programming \\[same algorithms\\], digging trenches etc.) It is rare when someone is creating something really new and unique (and if it is good, soon people are repeating it as everything else.)\n\nDIY was always with us too. So what if China can print your house in few days, people still build their own. Or on smaller scale do all kind of hobby projects which of course could be done more efficiently 'professionally', but that is not the point. Also you learn lot more when you do it yourself.",
          "score": 1,
          "created_utc": "2026-01-23 09:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17pdmx",
          "author": "PunnyPandora",
          "text": "If I have an idea that already exists in some form I still want to see if I can make it the way I want instead of using someone else's version. If I don't feel like starting from scratch, I will also often fork existing stuff and change it to be the way I want it to be, and most importantly I'm having fun during this.\n\nMy life would probably be less fun if I didn't do the things I had the ability to do, which is a lot more expanded now thanks to llms.",
          "score": 1,
          "created_utc": "2026-01-23 10:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17r0fe",
          "author": "Far_Composer_5714",
          "text": "Oh? I've been finding a lot of the AI tools that I see to be really cool.Â \n\n\n3D models, improvements in video, The introduction of audio, speech to text seems to have made some improvements.",
          "score": 1,
          "created_utc": "2026-01-23 10:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17s8hw",
          "author": "countAbsurdity",
          "text": "Man I just want LM Studio to handle image creation models and TTS models, like literally that's the only thing I want. One app to handle shit without installing a million different apps, dependencies, python, conda, git, etc.",
          "score": 1,
          "created_utc": "2026-01-23 10:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17thln",
          "author": "davew111",
          "text": "Like crypto, it's a solution in search of a problem.",
          "score": 1,
          "created_utc": "2026-01-23 10:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17wi07",
          "author": "landed-gentry-",
          "text": "I would imagine a lot of this is just people getting their feet wet, so to speak: building their first project. It reminds me of data science where everyone starts by discovering how to predict who died on The Titanic or using ML for character recognition.",
          "score": 1,
          "created_utc": "2026-01-23 11:13:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18upaw",
          "author": "eternus",
          "text": "Here's the silver lining, or how I'm allowing myself to accept all the new, same but different, applications being created...\n\n... death to the subscription model.  \n... death to the saturated market of viable software worth paying for.\n\nIt's also giving 'consumers' that version of an app they always needed, but with one extra feature. Or without all the extra features.\n\nIs it inefficient? Hell yeah! But people are learning why those other things were created, they're learning how to make and ship a product. They're being pulled from a \"I guess I'll just wait til someone else does it\" mindset into a \"I need this, I can make it myself\" mindset.\n\nMy expectation for almost 2 years now is, we'll get to the point where software doesn't exist... or the platforms of software anyway. I won't need the Adobe Suite because I'll open some version of an AI interface and explain my intentions... create a bespoke application interface for my needs, that matches \"how I handle things.\" I won't be searching the Help > Search to find Layers which has been renamed Pages in this version of the software. It'll automatically be localized, it'll automatically handle any accessibility issues I navigate around.\n\nSo yes, we're all doing the same thing... which is good. One day we won't need to do all those things, we'll just do the thing we want to be doing without having to find some 3rd party utility to help us do it.",
          "score": 1,
          "created_utc": "2026-01-23 14:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o194f1c",
          "author": "AnomalyNexus",
          "text": "Would be preferable if everyone had only novel ideas and implemented them in polished fashion but that not realistic \n\nI personally donâ€™t think this is an issue. If anything it allows everyone to have a tool that is an exact fit for their wants which is arguably better than a polished generic one",
          "score": 1,
          "created_utc": "2026-01-23 15:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19hzwh",
          "author": "eli_pizza",
          "text": "I dunno I think those are just things people make when they're screwing around with personal projects. It's fine. I'm sure people use AI to build payroll registers and expense forecasting tools, but that's not the stuff getting posted on reddit.",
          "score": 1,
          "created_utc": "2026-01-23 16:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ifv2",
          "author": "Minimum_Ad_4069",
          "text": "Without Anthropicâ€™s Sonnet, I probably wouldâ€™ve had to delay my graduation. Itâ€™s helped me so much.",
          "score": 1,
          "created_utc": "2026-01-23 16:33:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1anf7t",
          "author": "tim-tim-ai",
          "text": "Thereâ€™s a dynamic where itâ€™s sometimes easier to vibe code a fully customized version of exactly what you need that find and learn an existing tool. If you then share it and everyone has requests and ideas and the project expands the complexity comes back, itâ€™s often expanded poorly without the historical institutional knowledge of successful existing tools, and most get abandoned. It can be useful as a personal tool or learning project but Iâ€™ve long since stopped jumping on most randomly shared tools.",
          "score": 1,
          "created_utc": "2026-01-23 19:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b6ctn",
          "author": "ramendik",
          "text": "If someone is pushing out a less polished but also less monstery version of OpenWebUI please point me to them. I don't want RAG and the kitchen sink, I was a client with reliable tool calls, reliable file attachments, and a plugin architecture versatile enough to support custom memory and context compression without terrible kludges.\n\nI did start my own but I'm stuck in a lack of front end skill; if I could find a co-developer with front end skills enough to get my VERY LIGHT frontend going without the clunkiness of a vibe coded version, I'd be at 1.0 in weeks. Most functionality is in the backend and it's nearly feature complete - but requires changes to front-end that I'm just afraid to try vibe coding, given that every previous change took hours of debugging between three LLMs",
          "score": 1,
          "created_utc": "2026-01-23 21:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bhv9j",
          "author": "literallymetaphoric",
          "text": "Real AI researchers aren't limited to language models",
          "score": 1,
          "created_utc": "2026-01-23 22:03:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bqasw",
          "author": "Denial_Jackson",
          "text": "Terence Tao solved some ErdÅ‘s problems with it. A good tool in the hand of a master.",
          "score": 1,
          "created_utc": "2026-01-23 22:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f48is",
          "author": "-dysangel-",
          "text": "Yep. Well, to be honest even before AI I would sometimes build things that already exist, just to learn how they work, or to add my own spin on things. I think it's good to learn. The undertone to your point is that we could be building on the bleeding edge all the time. I think you have a point, but then again, not everyone is wired to live in that space.",
          "score": 1,
          "created_utc": "2026-01-24 13:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gnkhm",
          "author": "Pretty_Challenge_634",
          "text": "LLMs are surprisingly limited unless you have a nuclear power plant.",
          "score": 1,
          "created_utc": "2026-01-24 17:45:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1phbsn",
          "author": "rabf",
          "text": "I often see a tool or app that does something useful, only to realise that it is based on electron or Unity, is gigabyte's in size and eats memory to do a simple thing! So yeah I go ahead and rewrite my own version.",
          "score": 1,
          "created_utc": "2026-01-25 22:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rv67p",
          "author": "Aggressive-Math-9882",
          "text": "I agree and it is very confusing to me.  Why do people make things they don't want, instead of making things they do want that don't exist?",
          "score": 1,
          "created_utc": "2026-01-26 06:21:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vk110",
          "author": "lombwolf",
          "text": "Its solutions looking for problems",
          "score": 1,
          "created_utc": "2026-01-26 19:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgtqu",
          "author": "Fadelz",
          "text": "Is there any jailbroken AI that you can run locally no filter you can change how it speak because half of these things are just trying to jailbreak the AI",
          "score": 1,
          "created_utc": "2026-01-29 07:44:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14xqwt",
          "author": "segmond",
          "text": "... and what are you doing that's different?",
          "score": 1,
          "created_utc": "2026-01-22 23:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o158h96",
              "author": "[deleted]",
              "text": "Regarding the use of LLMs, I only use them to improve study materials. I've also spent three months trying to create an application to use small LLMs to generate flashcards in Rust, dividing the study material into chunks while I learn Rust. I'm not saying we have to be \"different,\" it's just that all the time and LLMs spent creating the same thing seems rather odd to me",
              "score": 1,
              "created_utc": "2026-01-23 00:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15w4nv",
                  "author": "segmond",
                  "text": "it's not the same thing, but rather similar things with various variations.  Do you think it odd that everyone is all of a sudden making electric cars.  Should it just be Tesla?  Are they not all the same thing?  This is the way technology works, new tech will mean tons of people doing the same thing, and tiny improvements from various groups will be adopted and all end up bubbling up to whomever becomes the leader.",
                  "score": 2,
                  "created_utc": "2026-01-23 02:12:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14s674",
          "author": "atineiatte",
          "text": "Another AI generated post",
          "score": -2,
          "created_utc": "2026-01-22 22:35:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15443h",
              "author": "Marksta",
              "text": "Don't know why they downvoted you, OP's post is clearly LLM tokens. OP's profile would suggest they don't even speak English. Hmm, but the body of the post may be human driven instead of the usual bot spam and an actual attempt at interacting with the users on the sub.",
              "score": 3,
              "created_utc": "2026-01-22 23:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1579jy",
                  "author": "[deleted]",
                  "text": "no puedo ser un LLM porque todos los LLM hablan ingles, le has dado en el clavo",
                  "score": 1,
                  "created_utc": "2026-01-22 23:55:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o14t03l",
              "author": "GenLabsAI",
              "text": "https://i.redd.it/jhfp6kp0bzeg1.gif",
              "score": 1,
              "created_utc": "2026-01-22 22:41:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o151g0q",
              "author": "kevin_1994",
              "text": "maybe he just likes spaced en dashes, and remembers the â€œdirectional quotesâ€ alt code!",
              "score": 1,
              "created_utc": "2026-01-22 23:25:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o15hv4d",
          "author": "Solid-Iron4430",
          "text": "Ñ Ð´Ð°Ð²Ð½Ð¾ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð» Ð½ÐµÑ„Ð¸Ð³ Ð±Ñ‹Ð»Ð¾ Ð¸Ð· Ð¾ÐºÐµÐ°Ð½Ð° Ð²Ñ‹Ð»ÐµÐ·Ð°Ñ‚ÑŒ . ",
          "score": 0,
          "created_utc": "2026-01-23 00:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o154n9a",
          "author": "BidWestern1056",
          "text": "hoping youll feel differently about incognide:\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\nit's essentially an all-in-one workspace/development environment for all kinds of work done by researchers/developers/knowledge workers. I've made it possible to edit like docx/xlsx/pptx, text files, browse the web, run terminals, read pdfs, preview markdown/html files, have multi branch chats, git integrated, ai memory/knowledge management, agent team/context management, and it also provides a way to toggle a copilot like predictive text (still a bit rough cause i dont use it too much but gonna be fixing it up more in the next few releases).",
          "score": -2,
          "created_utc": "2026-01-22 23:42:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfse6",
      "title": "Run Kimi K2.5 Locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/rxqfj5os74gg1.jpeg",
      "author": "Dear-Success-1441",
      "created_utc": "2026-01-28 16:17:45",
      "score": 396,
      "num_comments": 72,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2bui1v",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 01:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rq6e",
          "author": "Daniel_H212",
          "text": "Anyone tried this on strix halo yet to see how many seconds per token it runs at?",
          "score": 147,
          "created_utc": "2026-01-28 16:49:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28uh5w",
              "author": "IngwiePhoenix",
              "text": "> how many seconds per token\n\nFelt. XD",
              "score": 92,
              "created_utc": "2026-01-28 17:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28yf7t",
                  "author": "bobaburger",
                  "text": "definitely the right question to ask",
                  "score": 27,
                  "created_utc": "2026-01-28 17:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2978iw",
              "author": "JamesEvoAI",
              "text": "I wanted to, but the smallest quant is still 240GB",
              "score": 23,
              "created_utc": "2026-01-28 17:56:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2b3svm",
              "author": "Zyj",
              "text": "Edit: **it runs at 12.5 tokens/s with no context on dual Strix Halo**\n\nCurrently downloading on dual Strix Halo. Performance could be decent, it's 32b active parameters, at 1.8bits per parameter that's 7.2 GB of RAM.\n\nEdit:\n\nOK, by itself llama-server tries to allocate too much data on the second host running rpc-server. I have to manually specify \\`-fit off --tensor-split 48,52\\`\n\n`load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = false)`  \n`load_tensors: offloading output layer to GPU`  \n`load_tensors: offloading 60 repeating layers to GPU`  \n`load_tensors: offloaded 62/62 layers to GPU`  \n`load_tensors: CPU model buffer size = 630.00 MiB`  \n`load_tensors: ROCm0 model buffer size = 109322.47 MiB`  \n`load_tensors: RPC0[192.168.220.2:50052] model buffer size = 118481.80 MiB`\n\nUnfortunately, after loading for several minutes, llama-server crashes after reporting a malformed response from rpc-server:\n\n`llama_context: n_ctx_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized`  \n`llama_context: ROCm_Host output buffer size = 1.25 MiB`  \n`llama_kv_cache: ROCm0 KV buffer size = 279.00 MiB`  \n`llama_kv_cache: RPC0[192.168.220.2:50052] KV buffer size = 270.00 MiB`  \n`llama_kv_cache: size = 549.00 MiB ( 4096 cells, 61 layers, 2/2 seqs), K (f16): 549.00 MiB, V (f16): 0.00 MiB`  \n`sched_reserve: reserving ...`  \n`sched_reserve: Flash Attention was auto, set to enabled`  \n`sched_reserve: RPC0[192.168.220.2:50052] compute buffer size = 264.01 MiB`  \n`sched_reserve: ROCm0 compute buffer size = 362.00 MiB`  \n`sched_reserve: CPU compute buffer size = 36.01 MiB`  \n`sched_reserve: graph nodes = 4852`  \n`sched_reserve: graph splits = 3`  \n`sched_reserve: reserve took 251.87 ms, sched copies = 1`  \n`common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)`  \n`/opt/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp:669: Remote RPC server crashed or returned malformed response`  \n`recv failed (bytes_recv=0, size_to_recv=8)`\n\nHowever, on the side of rpc-server, there are no errors visible in the log.  \nTried it again with a split of 51,49. I got this on the rpc-server side:\n\n`ROCm error: unspecified launch failure`  \n`current device: -1, in function ggml_backend_cuda_synchronize at /opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2850`  \n`hipStreamSynchronize(cuda_ctx->stream())`  \n`/opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:96: ROCm error`\n\nI might be hitting a ROCm bug. I'm using Fedora 43, kernel 6.18.3 and the latest rocm nightly, updated a few minutes ago.\n\nEdit:  \nGot it working with tensor-spliit 46,54, **it runs at 12.5 tokens/s with no context.**  \nI can see the reasoning, it looks like the llama-server webgui doesn't recognize the markup that Kimi K2.5 uses to mark the reasoning or perhaps it doesn't mark it properly at all.",
              "score": 22,
              "created_utc": "2026-01-28 23:00:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bibzi",
                  "author": "Zyj",
                  "text": "It doesn't seem to be very good... more testing tomorrow",
                  "score": 6,
                  "created_utc": "2026-01-29 00:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jaip",
              "author": "RedParaglider",
              "text": "It's a pretty big lift to get a 100gb model loaded on the old girl.  My strix halo has 128gb shared vram/ram so this thing is out of bounds if I need 256gb ram. \n\nThe way it works is you use your ram to pull the model off the disk, then it unloads into the same ram pool the kernel just allocates that ram to vram.  Then you also need additional vram to utilize for context.\n\n I have yet to see another model that will make me switch of glm 4.5 air derestricted.  It works well for pretty simple agentic tasks, creative writing, looking shit up on the web, looking for race conditions in code, etc.  Speed is around 23 t/s so it's not fast, but there is overhead calling remote API's that often means it's faster for simple tasks than even Gemini flash.",
              "score": 13,
              "created_utc": "2026-01-28 18:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a0ym1",
                  "author": "JamesEvoAI",
                  "text": "Give Qwen 3 VL 30B-A3B a shot. It's been my workhorse on this machine and leaves me plenty of memory for other models/tools",
                  "score": 7,
                  "created_utc": "2026-01-28 20:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c1yja",
                  "author": "Far-Low-4705",
                  "text": "gpt-oss 120b is also very good in my experience. no idea about anything beyond STEM tho",
                  "score": 4,
                  "created_utc": "2026-01-29 02:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2971oe",
              "author": "Sufficient_Prune3897",
              "text": "Too little RAM",
              "score": 3,
              "created_utc": "2026-01-28 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29aqsh",
                  "author": "Daniel_H212",
                  "text": "Might not have been in this thread but I saw people talking about using disk space to run this, which is why I used the units of seconds per token.",
                  "score": 6,
                  "created_utc": "2026-01-28 18:11:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28rcxs",
          "author": "misterflyer",
          "text": "IQ0.2\\_XXS wen?",
          "score": 31,
          "created_utc": "2026-01-28 16:47:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28xzcd",
          "author": "Marksta",
          "text": "Thanks for the quants, gave it a spin yesterday. Q2_K_XL seemed perfectly fine as far as coherence goes. Kimi-K2 sticks to its signature style of absolute prompt adherence like a cold robot. 10/10 model, I think its style is what all non-creative focused models should really be striving for. \n\nIts creative side seems slightly better than their last model actually, but in a brute forcing via logic way. In RP scenario its thinking was like \"This character SHOULD say that, that fits that trope...\" and then proceed to deliver the right idea wrong execution of it because it's writing chops are awful ðŸ˜‚",
          "score": 22,
          "created_utc": "2026-01-28 17:16:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28zbti",
              "author": "misterflyer",
              "text": "I experienced the same thing over API with creative writing \n\nlol smh, oh well",
              "score": 3,
              "created_utc": "2026-01-28 17:22:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2945oq",
          "author": "MikeRoz",
          "text": "What is the point of Q5 and up (UD-Q5_K_XL, Q6_K) when the experts are all in int4?",
          "score": 12,
          "created_utc": "2026-01-28 17:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29nh3t",
              "author": "Lissanro",
              "text": "It depends on how quants are made. If preserving the original quality without inflating size is the goal, making Q4_X quant is the only way. Non-INT4 tensors can be kept at Q8. Dynamic quants only make sense for Q3 or lower I think. You can read discussion at https://github.com/ggml-org/llama.cpp/issues/19127 for details, in the forth message someone already upload Q4_X quant if that is what you are looking for. Unsloth quants can be good choice if you need something smaller instead.",
              "score": 6,
              "created_utc": "2026-01-28 19:05:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29yugm",
                  "author": "MikeRoz",
                  "text": "[AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) is the repo. Already downloading it. I'm always a fan of people who provide detailed instructions on how to reproduce their work.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:56:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cfhr7",
                  "author": "danielhanchen",
                  "text": "Oh yes I saw the discussion - I was planning to follow in the footsteps of that GitHub issue but didn't get time yet - will investigate later today!",
                  "score": 2,
                  "created_utc": "2026-01-29 03:15:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29hrqs",
              "author": "defensivedig0",
              "text": "Other parts of the model are not in int4",
              "score": 2,
              "created_utc": "2026-01-28 18:41:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3qiz",
                  "author": "Lissanro",
                  "text": "I checked their quants and it seems like Q4 and higher quants did not properly preserve the original quality. The proper way is to keep INT4 as modded Q4\\_0 and the rest can be kept as Q8\\_0, there is no need for quants higher than Q4\\_X since they only lose performance without any gains. How to make Q4\\_X quant for K2.5 is documented here: [https://huggingface.co/AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) (where you also can download pre-made Q4\\_X quant, it should work both with ik\\_llama.cpp and mainline llama.cpp).",
                  "score": 4,
                  "created_utc": "2026-01-28 20:18:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29yijd",
                  "author": "MikeRoz",
                  "text": "You can use HF's nifty GGUF viewer to see that the experts in their UD-Q5\\_K\\_XL are a mix of Q5\\_K and Q6\\_K, and all Q6\\_K in their Q6\\_K. If the experts were all capped at 4-bit, then difference in size between various quants 4-bit would be trivial, as seen with quants of models like GPT-OSS-120B.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:55:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28u9qv",
          "author": "IngwiePhoenix",
          "text": "Congrats to the small handful of LocalLLaMa people that have >300 <500 GB of VRAM to do this. x)\n\nI'll just keep dreaming...",
          "score": 65,
          "created_utc": "2026-01-28 17:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29aqfi",
              "author": "devvie",
              "text": "That isn't what the post says. Disk + RAM + VRAM >= 240GB, and you can run it. The poster claims 5T/s with 24GB VRAM.",
              "score": 30,
              "created_utc": "2026-01-28 18:11:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29dway",
                  "author": "juggarjew",
                  "text": "5T/s with 256GB RAM as well, large asterisks there lol",
                  "score": 37,
                  "created_utc": "2026-01-28 18:24:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2aad17",
                  "author": "ReasonablePossum_",
                  "text": "So you say a 6gb vram + 234gb ssd space? 8)",
                  "score": 4,
                  "created_utc": "2026-01-28 20:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29idcq",
              "author": "pandodev",
              "text": "Us normies over here will just be watching videos about it....",
              "score": 2,
              "created_utc": "2026-01-28 18:43:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2b0xa9",
          "author": "nonameisdaft",
          "text": "How would 64gb ram amd 24gb of vram handle this?",
          "score": 9,
          "created_utc": "2026-01-28 22:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c1s6v",
              "author": "FrodoTheExplorer",
              "text": "a",
              "score": 6,
              "created_utc": "2026-01-29 02:00:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d03xf",
                  "author": "jmellin",
                  "text": "Â ^ - The next token will arrive next week",
                  "score": 17,
                  "created_utc": "2026-01-29 05:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o297q4w",
          "author": "Own-Lemon8708",
          "text": "My 96gb vram and 128gb ram still aren't really enough to use these are they? I've tried past dynamic quants and didn't really see the value.",
          "score": 11,
          "created_utc": "2026-01-28 17:58:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o298nrz",
              "author": "RobTheDude_OG",
              "text": "If it makes you feel better, i was planning to get 256gb ram with my rx 9070 xt with 16gb vram. \n\nBut then the ram crisis hit as i ALMOST had enough money to commit to the purchase and from there i was chasing the constantly moving line until i gave up\n\nI now have a pc budget of around 2k, and i still cannot fucking buy the ram,motherboard and CPU.",
              "score": 7,
              "created_utc": "2026-01-28 18:02:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29bi2m",
                  "author": "Own-Lemon8708",
                  "text": "Thankfully I built my rig a while back. Bought some older 48gb quadros for less than a 5090 when it released. I would definitely not be building the same level of system with today's prices!",
                  "score": 3,
                  "created_utc": "2026-01-28 18:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2931om",
          "author": "Historical-Internal3",
          "text": "I have two DGX spark's clustered and I don't think I could run this in any meaningful way lol.",
          "score": 5,
          "created_utc": "2026-01-28 17:38:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6ojm",
              "author": "Zyj",
              "text": "Why not? Should work.",
              "score": 5,
              "created_utc": "2026-01-28 23:14:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29bbtw",
              "author": "Boring_Resolutio",
              "text": "this is really crazy, how much hardware is needed for SOTA",
              "score": 2,
              "created_utc": "2026-01-28 18:13:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ckuc1",
              "author": "buyurgan",
              "text": "same here, but i think offloading to SSD could work, but this document doesn't have any info about how much more memory needed for 100k context length processing, it will require more offloading then it looks.",
              "score": 1,
              "created_utc": "2026-01-29 03:47:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cl6i1",
                  "author": "Historical-Internal3",
                  "text": "Yea, most likely a ton.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ufwq",
          "author": "Sensitive_Housing_62",
          "text": "This is pretty amazing. I like it.",
          "score": 3,
          "created_utc": "2026-01-28 17:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2app5w",
          "author": "DragonfruitIll660",
          "text": "Ty for the GGUFs, always appreciated. Initial impressions for the model seem to be that its a little behind GLM 4.7 of a similar quant for RP. It appears to be more focused on coding though so that makes sense.",
          "score": 3,
          "created_utc": "2026-01-28 21:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dnfaq",
          "author": "tbwdtw",
          "text": "Step 1.\nBe rich",
          "score": 3,
          "created_utc": "2026-01-29 08:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291sqg",
          "author": "GabryIta",
          "text": "5 token/s RAM only?",
          "score": 3,
          "created_utc": "2026-01-28 17:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6tqq",
              "author": "DealingWithIt202s",
              "text": "Frontier model on your hardware tho",
              "score": 5,
              "created_utc": "2026-01-28 23:15:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bulne",
              "author": "PeakBrave8235",
              "text": "20 tokens on MacÂ ",
              "score": 2,
              "created_utc": "2026-01-29 01:20:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28vf8t",
          "author": "Long_comment_san",
          "text": "I wonder if we're gonna get hardware 2 bit precision soon with models like that.",
          "score": 2,
          "created_utc": "2026-01-28 17:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8kgf",
          "author": "_VirtualCosmos_",
          "text": "How much worse is the model with 1.8-bit quant compared to the original?",
          "score": 2,
          "created_utc": "2026-01-28 20:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2daw3f",
              "author": "Zyj",
              "text": "It seemed much worse in a 5 minute test",
              "score": 2,
              "created_utc": "2026-01-29 06:53:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cit2q",
          "author": "xxdesmus",
          "text": "Running Kimi K 2.5 via Ollama Cloud via OpenCode, and tons of hallucinations and strange output â€” \nparticularly while programming.",
          "score": 2,
          "created_utc": "2026-01-29 03:35:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cjxjv",
          "author": "Saruphon",
          "text": "Thanks, this is where by 256GB Ram come in handy. Also no i am not a millionaire. Just bought them last July before the price goes crazy.",
          "score": 2,
          "created_utc": "2026-01-29 03:41:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d423o",
          "author": "wh33t",
          "text": "I have 260GB total of (v)ram. I could run this with like 2048 context! w00t! Might have to build a swap space for it lol",
          "score": 2,
          "created_utc": "2026-01-29 05:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2db0j5",
          "author": "NeuralNexus",
          "text": "168 GB RAM in my experimental machine, not enough. Need more.",
          "score": 2,
          "created_utc": "2026-01-29 06:54:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dbwya",
          "author": "squachek",
          "text": "1 bit quant?",
          "score": 2,
          "created_utc": "2026-01-29 07:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2axmcw",
          "author": "juniperl78",
          "text": "is the q\\_1 quant even usable?",
          "score": 2,
          "created_utc": "2026-01-28 22:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhlp2",
          "author": "AkiDenim",
          "text": "Go on and use the 1-bit quantization. XD",
          "score": 1,
          "created_utc": "2026-01-29 07:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dueib",
          "author": "Distinct-Expression2",
          "text": "\"local\" if you own a small datacenter",
          "score": 1,
          "created_utc": "2026-01-29 09:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e79vj",
          "author": "siegevjorn",
          "text": "I guess the question is how good Q1.8 be I wonder",
          "score": 1,
          "created_utc": "2026-01-29 11:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f15sr",
          "author": "Disastrous-Fold7589",
          "text": "I have 4080 super 7tb harddrive and 64 gig game and 7950x3d cpu could it work for me",
          "score": 1,
          "created_utc": "2026-01-29 14:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fx5jm",
          "author": "koibKop4",
          "text": "nice!  \nI'm curious, is having more VRAM speed things up or if anything goes to RAM then speed automatically goes down? I've 128GB VARM and enough RAM so I can easily fit 1.8-bit.",
          "score": 1,
          "created_utc": "2026-01-29 17:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d14yf",
          "author": "StartupTim",
          "text": "I  have both a Strix Halo 128GB as well as a Nvidia DGX Sparc 128GB.  I haven't setup either of them and, if somebody would offer me help setting up both, I'll deploy this model and do some benchmarks!",
          "score": 0,
          "created_utc": "2026-01-29 05:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28m0ni",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Deployment Notes:\\*\\*\n\n\n\nTested Kimi K2.5 for customer support chatbot - here's what works in production:\n\n\n\n\\*\\*Resource Reality:\\*\\*\n\nâœ“ 1.8-bit quant runs on single 3090 (24GB)\n\nâœ— Latency: \\~8-12s first token (too slow for real-time chat)\n\nâœ“ Context: 128K works, but 32K sweet spot for cost/speed\n\n\n\n\\*\\*Where It Excels:\\*\\*\n\n\n\nâ€¢ Code generation for automation scripts\n\nâ€¢ Multi-turn reasoning (customer inquiry analysis)\n\nâ€¢ Vision tasks: document parsing, screenshot analysis\n\n\n\n\\*\\*Production Setup:\\*\\*\n\n\\`\\`\\`python\n\n\\# Our config\n\nmodel: kimi-k2.5-1.8bit-GGUF\n\ncontext\\_length: 32768\n\ntemp: 0.3 (more deterministic for bots)\n\nbatch\\_size: 512\n\n\\`\\`\\`\n\n\n\n\\*\\*Compared to GPT-4o:\\*\\*\n\nâœ“ Privacy: On-prem data stays local\n\nâœ— Speed: 3-4x slower inference\n\nâœ“ Cost: $0 per query after setup\n\n\n\n\\*\\*Reality Check:\\*\\*\n\nFor EU/CIS markets with GDPR requirements, local deployment justifies the latency trade-off. US markets usually prefer hosted APIs.\n\n\n\n\\*\\*Pro Tip:\\*\\* Use router pattern - simple queries â†’ fast local 7B model, complex reasoning â†’ Kimi K2.5. Cuts avg response time by 60%.\n\n\n\nAnyone running this in production for chatbots? What latency are you seeing?",
          "score": -16,
          "created_utc": "2026-01-28 16:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2917ut",
              "author": "Loyal_Rogue",
              "text": "Why all the downvotes?!?",
              "score": 3,
              "created_utc": "2026-01-28 17:30:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o295ozz",
                  "author": "MikeRoz",
                  "text": "Not even the smallest of the quants here will run on a single 3090, unless you also have 256 GB+ of system RAM. Kimi-k2.5-1.8bit-GGUF isn't a quant offered in this repo, and it's nowhere to be found in a full-text search on HuggingFace. This is hallucination with a side of spam.",
                  "score": 8,
                  "created_utc": "2026-01-28 17:49:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cm6n0",
                  "author": "CheatCodesOfLife",
                  "text": "It's a spambot. Look at all the generic incorrect shit it's saying. I don't think even one thing it's said is correct.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qni356",
      "title": "216GB VRAM on the bench. Time to see which combination is best for Local LLM",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/5ilrgdymhpfg1.jpeg",
      "author": "eso_logic",
      "created_utc": "2026-01-26 14:51:22",
      "score": 387,
      "num_comments": 102,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1tt97y",
          "author": "HugoCortell",
          "text": "If this forum is to be believed, it'll be unusable (lower token output than standard reading speed).\n\nBut I tend to use a gain of salt when reading some of the takes posted here by people who haven't actually tried, so I look forward to seeing what your actual testing discovers, OP.\n\nI'd also be interested in knowing how you're rigging that many GPUs to a single PC without a massive loss in bandwidth, most \"cheap\" server motherboards I found can only do a handful of GPUs.",
          "score": 60,
          "created_utc": "2026-01-26 14:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tu82o",
              "author": "eso_logic",
              "text": "My thoughts exactly.\n\nThe motherboard here is actually a dual socket X99 board, supermicro X10DRG-Q. These are very cheap on the secondhand market (\\~200 USD) and give you a lot of PCIe lanes to avoid throttling. I have had situations where motherboard/CPU combo does throttle performance, detailed on [da blog](https://esologic.com/gpu-server-benchmark/#hardware-updates).",
              "score": 30,
              "created_utc": "2026-01-26 15:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u31ix",
                  "author": "OutlandishnessIll466",
                  "text": "I had 4x p40 but replaced 3 with 3090's. They work perfectly fine and are definitely useable. The problem comes with increased context and bigger models. The bigger the model the slower and at 400+ GB even 3090s become unusable.",
                  "score": 14,
                  "created_utc": "2026-01-26 15:42:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uvzhl",
                  "author": "DrKenMoy",
                  "text": "Dude no joke this is so cool. Are you just planning on running llama or are there others youâ€™re considering?",
                  "score": 3,
                  "created_utc": "2026-01-26 17:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w8a0i",
              "author": "segmond",
              "text": "False, I posted a little more than $1,000 build with MI50 a while ago, 10 16gb MI50s and they produce very usable speed much better than Strix Halo.  It's pinned on my profile posts.   I was running it on x1 PCI bus with the slowest celeron CPU and ddr3 ram.  P40/P100s are better faster and on a good platform will produce awesome speed.",
              "score": 4,
              "created_utc": "2026-01-26 21:15:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wae8x",
                  "author": "HugoCortell",
                  "text": "Damn dude, you almost make me want to try it myself. Then again P40/60/100s are super expensive in Europe, for the US price the most we can get here are the K ones, which I have been resolutely told by everyone that they are garbage.\n\nA P40 is \\~$500 here. MI50s are awesome (cheaper and more memory than a P40), but the lack of support for Windows kind of kills them for me.",
                  "score": 2,
                  "created_utc": "2026-01-26 21:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wxeys",
              "author": "pharrowking",
              "text": "https://preview.redd.it/a2qvipvlzrfg1.png?width=1576&format=png&auto=webp&s=44aa68be194ba19f0436af4e375b251c37aeb418\n\nCheck out this llama-benchmark for minimax m2.1 using 8x tesla p40s. MOE models do very well on pascal hardware. in this case the model is fully loaded in gpu vram. any use of cpu ram and that speed becomes nothing. but pure gpu ram, is workable. 25 t/s is way more than read speed.",
              "score": 2,
              "created_utc": "2026-01-26 23:11:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u1gaw",
          "author": "BananaPeaches3",
          "text": "The main issue with older cards is that prompt processing will get you even if token gen speed is tolerable (and it is)\n\nIf youâ€™re using it like chat gpt then itâ€™s fine but once you start using things like cline the system prompt is allegedly 15k tokens.\n\nSo imagine youâ€™re waiting several minutes before there is even any output. At this point a DGX Spark is a better investment, it will output slightly slower than P100 but at least prompt processing will be fast.",
          "score": 13,
          "created_utc": "2026-01-26 15:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tumh6",
          "author": "iampoorandsad",
          "text": "Cooling those teslas will turn your house/lab into a plane with jet engines. Get some earplugs or a really good ANC headphones.",
          "score": 19,
          "created_utc": "2026-01-26 15:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tvdbh",
              "author": "eso_logic",
              "text": "Ha! I actually developed my own high static pressure air cooler for these cards that scales fan power with load so it's not that bad. You can see the coolers hanging off the back here:\n\nhttps://preview.redd.it/cn86bpjplpfg1.jpeg?width=6960&format=pjpg&auto=webp&s=7e4f2a22382f79834a313495286a5f1102c5676d\n\nI've written about the project as well: [esologic.com/cooler](http://esologic.com/cooler)",
              "score": 47,
              "created_utc": "2026-01-26 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzb5e",
                  "author": "gittb",
                  "text": "Super clean",
                  "score": 14,
                  "created_utc": "2026-01-26 15:25:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u7lpz",
                  "author": "pokemonplayer2001",
                  "text": "I was going to point out the cooling issues with these cards, but you not only know about it, you have a fix and docs.\n\nðŸ‘ðŸ‘ðŸ‘\n\nEdit: ok bro, no need to show off!! :)   \n[https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg](https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg)\n\nðŸ¤ŒðŸ‘Œ",
                  "score": 9,
                  "created_utc": "2026-01-26 16:02:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u1fsq",
                  "author": "iampoorandsad",
                  "text": "Well done sir, but how is that suppressing the noise at 100% load ? It looks really clean though. But even at idle temps (30C ish?) the blowers would be spinning, no? I'm not sure what would bother me most, constant noise or intermittent... anyway, earplugs it is!",
                  "score": 7,
                  "created_utc": "2026-01-26 15:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ur8u1",
              "author": "BusRevolutionary9893",
              "text": "Assuming 250-300 watts for each of the 12 GPUs that's only 3.0-3.6 kW. A one ton(3,410 watts) wall mounted split system or a small window unit is all he needs. A wall mounted split system is practically silent.Â ",
              "score": 1,
              "created_utc": "2026-01-26 17:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tvupr",
          "author": "FullOf_Bad_Ideas",
          "text": ">In gpu_box_benchmark, single GPU tests are parallelized using Docker containers. The same test is invoked inside of a docker container, one per GPU. The containers are started at the same time so each GPU is loaded at the same time.\n\nI don't think this benchmark contains any test for serving big models split across GPUs, which is the main usecase for having multiple GPUs with a lot of VRAM.\n\nI am looking forward to your results anyway.\n\nI think this project is progressing slowly since I first heard of it almost a year ago, have you faced issues with cooler design that prevented it from being finished earlier, or is it just caused by a lack of time?",
          "score": 7,
          "created_utc": "2026-01-26 15:10:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tylqq",
              "author": "BuildAQuad",
              "text": "I would assume you could get decent speeds if you run MOE models?",
              "score": 3,
              "created_utc": "2026-01-26 15:22:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1tzn8q",
                  "author": "FullOf_Bad_Ideas",
                  "text": "yeah, it could run big MoEs well. And I think that should be the benchmark.",
                  "score": 3,
                  "created_utc": "2026-01-26 15:27:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1tz34y",
              "author": "eso_logic",
              "text": "Yeah it's not clear, my bad. Multi gpu native tests like \\`llama-bench\\` and the ResNet50 tests run both the parallelized container approach *and* a single container with all GPUs. It's up to user which results are selected for comparison, in the graph on the blog it's the all GPUs result.\n\nYeap it has been a while. Day jobbing sadly. To get max cooler performance I've designed a fan manifold for each of the supported cards but must of that work had been done a long time ago.  \n\nI also had to build an [air intake system](https://github.com/open-rack-vent) for my server rack to feed my GPU node with air inside the rack. \n\nhttps://preview.redd.it/9n5dnshropfg1.jpeg?width=4238&format=pjpg&auto=webp&s=2ad2b2def2e8684a721dfe1dec5f1b253ddbedea\n\nProjects within projects lol.",
              "score": 3,
              "created_utc": "2026-01-26 15:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzcru",
                  "author": "eso_logic",
                  "text": "https://preview.redd.it/kh3oo1tvopfg1.jpeg?width=6960&format=pjpg&auto=webp&s=99cd54b9ad3f8b81a4d2041cbb87647ac615cc81\n\nAnother rack air intake shot, cool air is moved to the front from the sides.",
                  "score": 2,
                  "created_utc": "2026-01-26 15:26:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tvcyu",
          "author": "dc740",
          "text": "from those I have a P40 and an M10 (EDIT: I thought I had the M40 originally but I simply didn't remember it). The P40 no longer has support, and the M10 is just 4 maxwell 8gb gpus in a single card. The P40 runs in circles around the M10, but after lots of testing I still prefer to run 3 amd instinct Mi50 32gb (even though support was dropped from rocm a few months ago)",
          "score": 12,
          "created_utc": "2026-01-26 15:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u0b9p",
              "author": "Far-Low-4705",
              "text": "i was able to get two of the amd mi50's when they were cheap.\n\nDef not as good as 3090 or anything, but for $200 for 64Gb VRAM, really cant beat it",
              "score": 5,
              "created_utc": "2026-01-26 15:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ubu29",
                  "author": "WiseassWolfOfYoitsu",
                  "text": "Another option is v620. Newer than the mi50s and better supported, but more like 350 per instead of 100. Still not bad for the higher speed and 32gb vram per, though.",
                  "score": 5,
                  "created_utc": "2026-01-26 16:20:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1u3lne",
              "author": "TheSpicyBoi123",
              "text": "No, the m40 is a single gpu, you are confusing it with the m60 which is 4GPUs on one board. The m40 comes in two versions the 12GB one and the 24GB one. The p40 is by all means a much stronger GPU though, however the best Pascal card is by far the p100.",
              "score": 4,
              "created_utc": "2026-01-26 15:44:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u81x1",
                  "author": "dc740",
                  "text": "Oh, you are right! I was confused with the M10. I had to go look up what I had because I no longer remembered it.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uxw65",
              "author": "wh33t",
              "text": "I'm still using my P40's for LLM just fine. On driver 580. The datacenter driver should continue to support P40's for much longer still.",
              "score": 1,
              "created_utc": "2026-01-26 17:55:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ub2me",
          "author": "blazze",
          "text": "**Mark Watney**Â says:Â *\"In the face of overwhelming odds, I'm left with only one option: I'm gonna have to science the shit out of this.\"*Â  \\- *The Martian*Â (2015)\n\nYou've earned an official Martian creativity and ingenuity award.  I will study your design for a  96 GB, 6 RTX 5060TI GPU rig i want to build for LLM training.",
          "score": 5,
          "created_utc": "2026-01-26 16:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1udeue",
              "author": "eso_logic",
              "text": "Ha thanks! Let me know if you run into problems and I can try to lend a hand.",
              "score": 1,
              "created_utc": "2026-01-26 16:27:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u3tkm",
          "author": "TheSpicyBoi123",
          "text": "Have you tried bios modding the Kepler and Maxwell gpus? You can squeeze a surprising amount of headroom this way.",
          "score": 5,
          "created_utc": "2026-01-26 15:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u4mib",
              "author": "eso_logic",
              "text": "Yes I've heard of this but haven't taken the plunge yet. It would be good to A/B the benchmarking suite on a modded vs stock set of GPUs as well. I'll add this to my notes, thank you.",
              "score": 3,
              "created_utc": "2026-01-26 15:49:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7q1f",
                  "author": "TheSpicyBoi123",
                  "text": "Its worth a shot! I have a nice guide on my github on how to do it, however I only have made the bioses for K40's (single gpu version of the k80). To make the new bios for these dual gpus is a bit more involved but follow the instructions and it should work just fine. M40 should be much easier.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yies1",
              "author": "Bobby72006",
              "text": "Even on stock wattages, I was able to squeeze a lot of juice out of both the core and the VRAMs on my own M40. There's good potential here!",
              "score": 1,
              "created_utc": "2026-01-27 04:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20lzu6",
                  "author": "TheSpicyBoi123",
                  "text": "The much more interresting question is not about wattage per se but thermal headroom. Would you be willing to try some bioses if you have an m40 spare?",
                  "score": 1,
                  "created_utc": "2026-01-27 14:00:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tzas1",
          "author": "twavisdegwet",
          "text": "what interface? my t4 setup is wayyy faster after doing ik_llama with the nccl setup",
          "score": 3,
          "created_utc": "2026-01-26 15:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tzq6f",
              "author": "eso_logic",
              "text": "I'm using stock llama-bench. Can you link me your config? I can add a new test.",
              "score": 3,
              "created_utc": "2026-01-26 15:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u13gq",
                  "author": "twavisdegwet",
                  "text": "https://github.com/ikawrakow/ik_llama.cpp/\n\nHave to get cuda and NCCL enabled\nmust compile with -DGGML_CUDA=ON\n\nConfig should be unchanged outside of needing to add \"-sm graph\" to get the speed boost of ik_llama \n\nProbably better explined here but that's the gist of it \nhttps://medium.com/@jagusztinl/llama-cpp-performance-breakthrough-for-multi-gpu-setups-04c83a66feb2",
                  "score": 4,
                  "created_utc": "2026-01-26 15:33:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ufqsp",
          "author": "GoodSamaritan333",
          "text": "I can hear the noise, just by looking at this image.",
          "score": 3,
          "created_utc": "2026-01-26 16:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1urd0c",
          "author": "Ok-Internal9317",
          "text": "Havenâ€™t seen so much ewaste on one table for so long. I also have four m40s and one m60, with no support with vllm and pytorch dropping support day by day, I am becoming more pessimistic of their future\n\nBut the P100 is nice tho, planning to buy that soon",
          "score": 3,
          "created_utc": "2026-01-26 17:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1url8r",
              "author": "eso_logic",
              "text": "One man's trash...",
              "score": 5,
              "created_utc": "2026-01-26 17:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xdkvl",
          "author": "Current_Ferret_4981",
          "text": "What is your plan for avoiding the communication and memory slowdown? While 200+GB of VRAM is interesting, you are going to be compute, memory bandwidth, and comm bound long before you can load that vram up efficiently. \n\nI would take a look at the math in this article and see how it looks https://jax-ml.github.io/scaling-book/applied-training/. I suspect it's going to turn out worse than something like 1x6000 pro and certainly cost more before even considering the power and cooling costs",
          "score": 3,
          "created_utc": "2026-01-27 00:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u7yty",
          "author": "Freonr2",
          "text": "Short of someone else trying that exact setup and posting benchmarks you aren't going to know until you try. I can't recall anyone posting benchmarks for 4/8 Tesla GPU setups.",
          "score": 2,
          "created_utc": "2026-01-26 16:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ua6bf",
          "author": "a_beautiful_rhind",
          "text": "P40/P100 will be ok just full of software hassles. K/M GPU are going to be more trouble than they're worth.\n\nI'm actually curious how ik_llama.cpp would do on NCCL-TP p2p, at least on models where pascal is still working. That's where I'd start or that pascal vllm fork.",
          "score": 2,
          "created_utc": "2026-01-26 16:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud8tq",
              "author": "eso_logic",
              "text": "Docker helps with locking down the environment for comparison. Yep it looks Like I'l have to add some \\`ik\\_llama\\` tests.",
              "score": 2,
              "created_utc": "2026-01-26 16:26:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ubl75",
          "author": "PhotographerUSA",
          "text": "You don't need a lot of vram anymore. Also, you can just have AI gather all the info off the web. You don't need the big library.",
          "score": 2,
          "created_utc": "2026-01-26 16:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xj6pn",
          "author": "bigh-aus",
          "text": "I'm seriously starting to think about options for a high vram rig @ home.  \nGoal would be to run minimax m2.1 (and i guess 2.2).  Some rough calculations in cost are pretty scary :\\\\ unless I go the mac route.\n\nDepending how fast you're able to get things, maybe it's a case of buy the comptuer then run older GPUs...",
          "score": 2,
          "created_utc": "2026-01-27 01:03:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y202a",
              "author": "eso_logic",
              "text": "Yep thatâ€™s the goal of the project, to be able to answer such questions.",
              "score": 2,
              "created_utc": "2026-01-27 02:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xto16",
          "author": "MLExpert000",
          "text": "This is the kind of setup where VRAM management starts to matter more than raw capacity., once youâ€™re juggling multiple large models on local machines , the cost isnâ€™t just fitting them, itâ€™s reload and reinit churn when switching. Please do some benchmarks around  swap or reactivation latency, not just steadystate throughput",
          "score": 2,
          "created_utc": "2026-01-27 02:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xv9zt",
              "author": "eso_logic",
              "text": "Do you know of anything that exists to benchmark this? Or how would you do it",
              "score": 2,
              "created_utc": "2026-01-27 02:09:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xwnur",
                  "author": "MLExpert000",
                  "text": "There isnâ€™t really a standard benchmark for this today unfortunately. Most LLM benchmarks focus on steady-state throughput or tokens per second once the model is already hot but not on swap, reload, or reactivation latency. In practice, people tend to measure this with a simple harness that repeatedly evicts a model from VRAM and then triggers a cold or semi-cold inference and records time to first token along with GPU memory residency over time. If you want meaningful comparisons, you have to control for disk, PCIe, and host memory effects, since reactivation latency often ends up being more user-visible than raw throughput when youâ€™re switching between models.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:17:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xwu1h",
                  "author": "MLExpert000",
                  "text": "A local multi-GPU setup like this is actually well suited for that kind of testing, since you can control disk, PCIe, and host memory effects.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z5c9g",
          "author": "TooManyPascals",
          "text": "Cool! Super interested on the benchmarks! \n(I have a similar setup)",
          "score": 2,
          "created_utc": "2026-01-27 07:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usw89",
          "author": "spacenavy90",
          "text": "Sure you have a lot of VRAM but extremely slow inference. Sorry you wasted your money OP.",
          "score": 1,
          "created_utc": "2026-01-26 17:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ur3nx",
          "author": "funnyrobot10",
          "text": "Can I please get the parts for your setup? I really struggle putting together the part list for my build. Currently I have 2 Tesla V100 and some 256GB ram",
          "score": 1,
          "created_utc": "2026-01-26 17:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1urop6",
              "author": "eso_logic",
              "text": "Check out the blog",
              "score": 1,
              "created_utc": "2026-01-26 17:28:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwhl3",
          "author": "Informal_Trade_3553",
          "text": "if theyre nvidia chips youre lucky, and if the drivers for cuda work.  \nIf not, its just a nice toy burning electricity you have :P",
          "score": 1,
          "created_utc": "2026-01-26 23:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y42c7",
          "author": "KadahCoba",
          "text": "P40 are still usable, but not with everything, like vllm unless you compile it with older compute support but it will be slow. llama.cpp+gguf works well on them, I was running 3xP40's till about a month ago. I'm planning to sell the P40's soon.\n\nM40 and Maxwell in general were pretty much unusable in late 2024. A lot of things dropped support for that compute level a while ago. I have one I'll likely post for free at some point.\n\nK80 and Kepler, very useless. I bought on back in 2022 and it was unusably slow then on the few things that would work with Kepler. I have one I will give away to anybody that wants to come get it in socal.",
          "score": 1,
          "created_utc": "2026-01-27 02:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y4fg1",
              "author": "KadahCoba",
              "text": "Your cooling setup is fancy.\n\nI would have just put a 120mm server fan on each block of 3.5 cards. Overkill would be one on each end.",
              "score": 1,
              "created_utc": "2026-01-27 02:59:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1va4yy",
          "author": "FullstackSensei",
          "text": "If you upgrade to an X11 motherboard, the IPMI will detect the GPUs and regulate fan speed based on their temps.\n\nYou can also use a single 80mm fan, like the Arctic S8038 series, to cool each pair of GPUs.\n\nSupermicro motherboard fan headers are rated at 2A each, so you could hook up to four S8038-7k fans to each header.\n\nSuch a solution is not only simpler and cheaper, but also much quieter than those blower fans.",
          "score": 0,
          "created_utc": "2026-01-26 18:46:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp6rm5",
      "title": "API pricing is in freefall. What's the actual case for running local now beyond privacy?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-28 09:27:55",
      "score": 337,
      "num_comments": 359,
      "upvote_ratio": 0.84,
      "text": "K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.\n\nMeanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.\n\nI've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:\n\n1. **Privacy** â€” legit, no argument. If you're processing sensitive data, local is the only option.\n2. **No rate limits** â€” fair, but most providers have pretty generous limits now unless you're doing something unusual.\n3. **\"It's free after hardware costs\"** â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\n\nThe argument I never hear but actually find compelling: **latency control and customization**. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.\n\nWhat's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o26pje2",
          "author": "Minimum-Vanilla949",
          "text": "The offline aspect is huge for me - I travel a lot and having models that work without internet is clutch. Also call me paranoid but I don't trust these API companies to not randomly change their ToS or jack up prices once they corner the market",
          "score": 491,
          "created_utc": "2026-01-28 09:30:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vec7",
              "author": "AciD1BuRN",
              "text": "They keep changing the tos so much now i don't think it can be even called tos at this point",
              "score": 100,
              "created_utc": "2026-01-28 10:23:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o296qvc",
                  "author": "MoffKalast",
                  "text": "It's more like tng now already, or even ds9.",
                  "score": 22,
                  "created_utc": "2026-01-28 17:54:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27esl4",
              "author": "thatguy122",
              "text": "Exactly this. Don't be fooled by these 10 yr subsidized loss leader fees intended to corner the market.Â ",
              "score": 43,
              "created_utc": "2026-01-28 12:51:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27taov",
                  "author": "Icy-Pay7479",
                  "text": "Trudging through the mountains with a triple 3090 desktop â€œdeath strandingâ€ style.",
                  "score": 31,
                  "created_utc": "2026-01-28 14:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28y2ou",
                  "author": "Mkboii",
                  "text": "So isn't it better to hold off local upgrades till the market collapses. It would coincide with the demand for data centre RAM going down and then better local gpu options may become available.",
                  "score": 6,
                  "created_utc": "2026-01-28 17:16:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29gtlb",
                  "author": "CarrotcakeSuperSand",
                  "text": "Theyâ€™re not subsidized, inference has pretty solid gross margins. Itâ€™s the training and initial infrastructure buildout that causes negative cash flow.",
                  "score": 1,
                  "created_utc": "2026-01-28 18:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27mst1",
              "author": "spaceman_",
              "text": "100% this last bit. It's the same reason I use so many open source tools: I don't want to depend on a single vendor who can independently decide \"actually, you need us now\" and jack up prices massively or change the model quality I have access to as a low-end user.\n\nThink about Adobe but on steroids.",
              "score": 23,
              "created_utc": "2026-01-28 13:37:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2afcq5",
                  "author": "fenixnoctis",
                  "text": "I donâ€™t get your logic here though. \n\nOnce a vendor jacks up prices, youâ€™re free to switch. \n\nUntil then why not take advantage of it?",
                  "score": -1,
                  "created_utc": "2026-01-28 21:09:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2821zo",
              "author": "bigh-aus",
              "text": "Don't forget about being forced to give all their past data - that's the big one.  \n  \n[https://openai.com/index/response-to-nyt-data-demands/](https://openai.com/index/response-to-nyt-data-demands/)\n\nData invariably gets leaked (even accidentally).",
              "score": 19,
              "created_utc": "2026-01-28 14:55:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27aqwk",
              "author": "miken0222",
              "text": "Do you mind sharing what model you use on your offline travels? Im in a similar situation where offline is frequent but I need something during those times.",
              "score": 11,
              "created_utc": "2026-01-28 12:24:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e2be2",
                  "author": "Kahvana",
                  "text": "On my 8GB RAM laptop with Intel N5000 I use LFM2-VL 1.6B (could probably go for 3B) as it's super fast (near instant replies), low resource usage, has vision and is decent enough for toolcalling. Pair it with zim archives (kiwix) for grounding / world knowledge and it's solid. Haven't tried websearch with it.\n\nYou can also pair it with LFM2-CoBERT-350M for RAG. Probably not better than Qwen3-Embedding-0.6B and likely comparable to embeddinggemma-300M (haven't benchmarked), but it is much lighter to run.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:00:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26ui9r",
              "author": "UsernameAttempt",
              "text": "I'm not sure cornering the market is in the cards. There's too much competition, too many alternatives, and the technology is not one that can be monopolized. The best models of the biggest companies of 1 year ago are worse than the models of small companies in China today. With improvements to models slowing down, we're moving towards models as commodities - similar in performance and competing on price. I think these models today are the most expensive they'll ever be.",
              "score": 49,
              "created_utc": "2026-01-28 10:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26xnoi",
                  "author": "SeasonNo3107",
                  "text": "Cornering the market will only happen in 5 plus years with buyouts and mergers imo",
                  "score": 38,
                  "created_utc": "2026-01-28 10:43:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2798dz",
                  "author": "Conscious-Ball8373",
                  "text": "There's also too much investor money around and once it dries up someone will have to pay the bills.",
                  "score": 7,
                  "created_utc": "2026-01-28 12:14:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hxsc",
                  "author": "Icy_Foundation3534",
                  "text": "mergers happen very fast in this market",
                  "score": 1,
                  "created_utc": "2026-01-28 16:06:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27pu9s",
              "author": "Ayumu-Aikawa",
              "text": "exactly, the TOS and pricing are things that can change any day, we keep seeing stories about how these companies are not making any profit. It's clear for me they're going to have to change something sooner or later",
              "score": 8,
              "created_utc": "2026-01-28 13:53:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27prfq",
              "author": "genshiryoku",
              "text": "The trend we're seeing is a commodification of LLMs it's basically impossible to have a commodity monopoly, so over the long run costs should get lower as innovations reduce the cost of serving inference.\n\nThere is also an economy of scale going on where large server farms can simply just serve you inference at a lower cost than it takes you to simply pay for the electricity.\n\nFor me as someone owning multiple RTX 3090s the electricity costs of serving prompts are already higher than the cost of using APIs for the inference of the same model. \n\nLocal only makes sense if you have free (solar) power, *need* privacy or offline usage. Or if you have a custom fine-tuned model that you need to run, since hosting this yourself on rented GPUs is still more expensive than running it yourself unlike generic big model inference.",
              "score": 8,
              "created_utc": "2026-01-28 13:53:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28uagh",
                  "author": "MrPecunius",
                  "text": ">*need* privacy\n\nEverybody *needs* privacy.",
                  "score": 10,
                  "created_utc": "2026-01-28 17:00:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dxtia",
                  "author": "GnistAI",
                  "text": "Interestingly, anyone using electricity to heat their home can justifiably consider their GPU power consumption as free. At least during winter.",
                  "score": 2,
                  "created_utc": "2026-01-29 10:21:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2851pp",
              "author": "p3r3lin",
              "text": "Same. I live in a part of the world where stable and always available internet connection is still not guaranteed (...Germany). So having \"AI in the pocket\" would be a great thing to have. There are projects like https://locallyai.app/ that enable small models (4b, etc) running on mobile phones, but its definitely no replacement for a SOTA model of any kind.",
              "score": 2,
              "created_utc": "2026-01-28 15:09:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ao1in",
              "author": "pandodev",
              "text": "yes privacy and offline for the right things is EVERYTHING.",
              "score": 2,
              "created_utc": "2026-01-28 21:47:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27f30f",
              "author": "lambdawaves",
              "text": "â€œOnce they corner the marketâ€\n\nThey wonâ€™t corner the market. There will always be at least a handful of competitors that offer a 98% similar product (via the same API) preventing arbitrarily high price hikes",
              "score": 5,
              "created_utc": "2026-01-28 12:53:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27cj5l",
              "author": "Imaginary_Context_32",
              "text": "May I know your setups ( hardware, models, usecase?)",
              "score": 3,
              "created_utc": "2026-01-28 12:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2az6tt",
              "author": "qwerty____qwerty",
              "text": "what setup do you have? if you travel a lot - I'm assuming that's a laptop, aaaand how powerful should that laptop be to compete with gemini api?",
              "score": 1,
              "created_utc": "2026-01-28 22:37:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2boezb",
              "author": "mycall",
              "text": "They can also block countries if they were forced too",
              "score": 1,
              "created_utc": "2026-01-29 00:47:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29odo6",
              "author": "Eye-m-Guilty",
              "text": "Would love to know what ur running offline n the set up!",
              "score": 1,
              "created_utc": "2026-01-28 19:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26vrl0",
          "author": "IactaAleaEst2021",
          "text": "For my work, repeatability of results. When you download a model, you audit it and you start trusting it, you are sure the vendor does not change its behavior behind the scene.\nI am not saying they do it for malicious purposes, but in many cases they improve their product in some direction, while making it less useful in others.",
          "score": 151,
          "created_utc": "2026-01-28 10:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aj5fc",
              "author": "notRandomUsr",
              "text": "This happened to us, we were working with \"diseases\" data.  We gather lots of information for different organisms using gpt3.5 turbo, after couple months we tried the new model (gpt4 and its variants) and got terrible results. The difference was so dramatic that we went back to the previous model, using the exact same version and the results were completely different, non existent basically. It was curious to see how, with the same prompts, the same system instructions, the same format, and the same questions, we obtained scarce data or empty responses compared to what we achieved in the first run.",
              "score": 3,
              "created_utc": "2026-01-28 21:26:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26y385",
              "author": "SpicyWangz",
              "text": "I havenâ€™t encountered this with any API product. Consumer facing chat interfaces are going to continually evolve, but if youâ€™re using a tagged model on an API it shouldnâ€™t be changing",
              "score": -7,
              "created_utc": "2026-01-28 10:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27g36l",
                  "author": "TheRealMasonMac",
                  "text": "Gemini, GPT, and Claude often have undisclosed model updates.",
                  "score": 36,
                  "created_utc": "2026-01-28 12:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o272y78",
                  "author": "IactaAleaEst2021",
                  "text": "Youâ€™re right, but still if you develop a product based on consistent results, the should not must become must not.",
                  "score": 16,
                  "created_utc": "2026-01-28 11:27:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27k3ts",
                  "author": "ross_st",
                  "text": "Google recently changed the way the Gemini 3 API works behind the scenes by adding the cutoff date and an encouragement to use its chain of thought to the system instruction. That gets added even if you leave the system instruction parameter blank in the API call. Before, if you left it blank, there would just not be a system instruction block in the context window.",
                  "score": 11,
                  "created_utc": "2026-01-28 13:22:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2767yw",
                  "author": "jikilan_",
                  "text": "U will be forced to upgrade when they decomm the version of model that u r using",
                  "score": 10,
                  "created_utc": "2026-01-28 11:52:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27ja0u",
                  "author": "Significant-Heat826",
                  "text": "That's weird because I often get emails from vendors saying they've changed something in their API endpoint yet again.",
                  "score": 6,
                  "created_utc": "2026-01-28 13:18:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hg1x",
                  "author": "Eugr",
                  "text": "They can still serve a more quantized version to serve more customers during the peak hours, change their guardrails, etc.",
                  "score": 4,
                  "created_utc": "2026-01-28 16:04:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27s8rd",
                  "author": "Flamenverfer",
                  "text": "2 come to mind immediately. I can't remember exact model names from Amazon bedrocks service but the sonnet models they serve on there, the new version of sonnet was extremely token conservative when we needed it to finish a response in a consistent format. But the model would always take the lazy way out and say something to the likes of  \"All other data shall be labelled N/A\" when it would need to list every datapoint as N/A for the json file.\nAlso lets not forget chatGpt taking away 4 when they released 5",
                  "score": 3,
                  "created_utc": "2026-01-28 14:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27d3ri",
                  "author": "Imaginary_Context_32",
                  "text": "I had faced this with GPT 4 turbo â€œnot with specific modelâ€",
                  "score": 1,
                  "created_utc": "2026-01-28 12:40:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28ljq9",
                  "author": "Double_Cause4609",
                  "text": "You'd think, but providers tend to quantize their models over time, and sometimes without labeling. It's not really a \"finetune\" or change, exactly, but it can be noticeable, particularly for function calls.",
                  "score": 1,
                  "created_utc": "2026-01-28 16:22:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26pgqm",
          "author": "05032-MendicantBias",
          "text": "API pricing won't be subsidized forever. At some point venture capital will want a return. Same as the short time where Uber was subsidized.\n\nBy all means, get a millionare to subsidize your workflows, but know this is a short term deal that won't last.\n\nThe goal of venture capital is to make everything else go away, so they get a monopoly and raise prices. Maintaining a local rig as fall back, and maintaining open source tools and models, will screw that businness plan heavily ;)",
          "score": 338,
          "created_utc": "2026-01-28 09:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27wiwc",
              "author": "BonjaminClay",
              "text": "Exactly this. Enshittification comes for everything at this point and if they are giving something away or it is unrealistically cheap now then you shouldn't rely on it. I have learned this lesson too many times. I stopped buying physical media or maintaining my own copies for a long time because streaming was just easier and now there are 20 streaming services all wanting 5x more per month each. \n\nBuilding with local or on something I control the costs of means that when the AI bubble pops my stuff won't break or get unpredictably more expensive.",
              "score": 26,
              "created_utc": "2026-01-28 14:27:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28hoks",
              "author": "cleverusernametry",
              "text": "It stuns me that people are still so gullible after over a decade of SaaS and cloud. You already see people going back to on-prem from cloud because cloud pricing has become so predatory. Just wait for this arc to play out with AI APIs",
              "score": 19,
              "created_utc": "2026-01-28 16:05:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o274q5k",
              "author": "rditorx",
              "text": "Besides venture capital, if you dry out the competition, and that includes locally run AI, you gain control over the market and can ask for almost any price, as long as people and companies can afford it.\n\nBig tech has always been subsidizing its services. It always starts with great products and awesome services or any other offer you can't refuse and will use until you're depending on them, then they pull the rug.\n\nIt's so common there's a term Doctorow coined: Enshittification.",
              "score": 15,
              "created_utc": "2026-01-28 11:41:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26qgad",
              "author": "Distinct-Expression2",
              "text": "Nice point. When do you think that will happen?",
              "score": 27,
              "created_utc": "2026-01-28 09:39:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26rl9c",
                  "author": "05032-MendicantBias",
                  "text": "I have no idea. The scheme collapses when venture capital lose patience. It could be as soon as this quarter, or it might take a few years. \n\nI feel confident the bubble pop will be preceeded by OpenAI trying an IPO for 2 trillion dollars, when venture capital will try to offload their position to retail.",
                  "score": 85,
                  "created_utc": "2026-01-28 09:49:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26sm8l",
                  "author": "neotorama",
                  "text": "When they IPO",
                  "score": 15,
                  "created_utc": "2026-01-28 09:58:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xk0y",
                  "author": "Finn55",
                  "text": "When it is least convenient",
                  "score": 13,
                  "created_utc": "2026-01-28 10:42:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28cmrg",
                  "author": "rbpri",
                  "text": "Google just cut Geminiâ€™s free tier in AI studio from ~100 RPD to ~20 RPD. Itâ€™s impossible to know when exactly enshittification is going to hit but itâ€™s coming sooner rather than later.",
                  "score": 5,
                  "created_utc": "2026-01-28 15:43:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27g48e",
                  "author": "SINdicate",
                  "text": "18 to 24 months",
                  "score": 1,
                  "created_utc": "2026-01-28 12:59:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2czwts",
                  "author": "protestor",
                  "text": "OpenAI already announced the first enshittification package (ads in chat). The reason they didn't jack up prices yet is that they need to  first kill local AI - they need to maintain in people's minds this idea that local AI makes no sense financially, since cloud AI is so cheap. Also, they can't kill local AI by pushing the frontier (open weights AI is trailing frontier models by 6-8 months). They have no moat in the model itself, and all the money they pour on training will only help Chinese clones when they distill it.\n\nSo what they are currently doing is to buy up enough wafers to jack up prices for machines used in local AI. I think that's what they are more worried about, consumer GPUs that crosses the 32GB-ish VRAM barrier. They play at TSMC made GPUs more expensive and made people buy more 8GB GPUs.\n\nI think that by the time the enshittification of closed AI is completed, running local AI will be unfeasible, a distant thing in the past",
                  "score": 1,
                  "created_utc": "2026-01-29 05:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26xc94",
              "author": "johnkapolos",
              "text": "API pricing isn't subsidized. Subscriptions are subsidized.",
              "score": 13,
              "created_utc": "2026-01-28 10:40:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27b7qx",
                  "author": "cyberdork",
                  "text": "Both are subsidized.",
                  "score": 9,
                  "created_utc": "2026-01-28 12:28:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29ppmp",
                  "author": "bathamel",
                  "text": "None of these companies are remotely profitable.  Therefore they are all subsidized at the moment.",
                  "score": 4,
                  "created_utc": "2026-01-28 19:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2720nf",
              "author": "AnomalyNexus",
              "text": "Even without subsidization itâ€™s hard to beat the economics of centralized data centers that have scale.\n\nSeems unlikely that itâ€™ll ever drop to a point where local wins a like for like shootout regardless of what happens",
              "score": 20,
              "created_utc": "2026-01-28 11:20:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2b39t4",
                  "author": "KontoOficjalneMR",
                  "text": "I can absolutely see it dropping. Cloud is multiple time more expensive than setting up your own server. People pay for convinience and scaling.\n\nBut if you're short on cash and don't need scaling ... local servers are multiple times cheaper then AWS.",
                  "score": 3,
                  "created_utc": "2026-01-28 22:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o290v7c",
              "author": "cniinc",
              "text": "\"The goal of venture capital is to make everything else go away, so they get a monopoly and raise prices\" - the best succinct description of venture capital. Understand that the second you become dependent on the external model they'll squeeze you for as much as they can on that investment. While the model is cheap, learn how to do it without the API, and have the API do the parts you can't. Then slowly increase you skills until you can do it all without API. Make them subsidize your learning, not your dependence.Â ",
              "score": 5,
              "created_utc": "2026-01-28 17:29:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29ifzg",
              "author": "corruptboomerang",
              "text": "This. It's the same as when Cloud & SAAS were becoming a thing, they were price competitive even under priced... To lure customers in, knowing that once you move, once you give up your capabilities its very difficult to rebuild those.",
              "score": 5,
              "created_utc": "2026-01-28 18:44:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26q6pn",
              "author": "MitsotakiShogun",
              "text": ">API pricing won't be subsidized forever\n\nNot everyone runs their APIs at a loss. Most inference providers don't, and neither does Anthropic, likely OpenAI too, definitely not Amazon/Microsoft/Google's cloud deployments of the same models. As an example:\n\nhttps://preview.redd.it/isxl8xnd82gg1.png?width=1230&format=png&auto=webp&s=aec9db1d96b8b7ca5c11bb85ce752ae05b0200e2\n\nEven if everyone on the list \"subsidized\" their API (why?), Google doesn't have much reason to do that, right?",
              "score": 18,
              "created_utc": "2026-01-28 09:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qmfc",
                  "author": "pip25hu",
                  "text": "We should not mix up two things. Inference providers running open-weight models can definitely make a profit, no doubt about it. But companies like OpenAI and Anthropic are also hoping to recoup their costs for training their models, and no, they're not even close to breaking even.",
                  "score": 39,
                  "created_utc": "2026-01-28 09:40:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xjb6",
                  "author": "johnkapolos",
                  "text": "You are being downvoted for being right.",
                  "score": 13,
                  "created_utc": "2026-01-28 10:42:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26r6xz",
                  "author": "05032-MendicantBias",
                  "text": "OpenAI burns money like there is no tomorrow, reportedly they burn anywhere between 15 to 50 billions a year and OpenAI themselves say they won't see green until 2030. If there is a company I'm 100% certain will go bankrupt, is OpenAI. It might be as soon as this year if venture capital loses patience.\n\nThe GPU datacenters use will become obsolete in two to three years, and because of hype, and demand spike, they cost 3X to 10X their base price. Not talking about many electricity grids just not having the capacity, and datacenters using portable gas turbines, at great expense.\n\nThe amount of money a GPU must make to break even, has greatly increased. There are just so many of them, and I feel doubtful they can get the utilization to the point of breakeven. Your argument is that your API might pay for the hours you use it, when running open models, which might be true locally, until you consider the lifetime of the hardware and the overall utilization.\n\nModels are getting bigger, and more expensives, and agents burn exponentially more tokens, increasing inference cost even further.\n\nNone of this is profitable, let alone sustainable.\n\nThere might be rare exceptions, like datacenters using hardware bought at fair price, running on renewables.\n\nNow, Microsoft and Google do have a profitable businness, so they can subsidize the money losing AI inference. I love to use their free credits! (which you can be certain are at great loss)\n\nIt's the silicon valley play of subsidize it until everyone is out, and with a monopoly raise prices. They love spamming this strategy.",
                  "score": 19,
                  "created_utc": "2026-01-28 09:45:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26zv8u",
                  "author": "No_Afternoon_4260",
                  "text": "vertex providing kimi, that's a new one.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:02:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26rdnk",
                  "author": "iotsov",
                  "text": "Oh sweet summer child.",
                  "score": 2,
                  "created_utc": "2026-01-28 09:47:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26x2so",
              "author": "Anyusername7294",
              "text": "It's generally agreed that interference isn't subsidized",
              "score": 7,
              "created_utc": "2026-01-28 10:38:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27bbln",
              "author": "mumBa_",
              "text": "I understand what you're saying but the amount of competition basically means that as long as no model wins (price-performance wise), they will have to keep the prices low for competition, which will definitely be the case for the upcoming decade+. All these companies also get free training data, so it's a two way street. You need users for their data, but if your model is too expensive, no one will use it and go to the cheaper competitor. It's a race to the bottom but once you're there, you need to stay there otherwise someone else will take your market position. So yeah unless a monopoly appears (google probably), realistically it will only get cheaper with time.",
              "score": 1,
              "created_utc": "2026-01-28 12:28:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28ywj9",
              "author": "customgenitalia",
              "text": "Nailed it. Running local is the long game, the skills you learn will start to pay off when the VC money runs out. There is so much compute potential sitting idle, I think youâ€™ll soon start to see creative ways to leverage this as nodes in a distributed AI fabric of sorts, think SETI but for ASI.",
              "score": 1,
              "created_utc": "2026-01-28 17:20:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o299jj6",
              "author": "thisdude415",
              "text": "API unit pricing is profitable at all the major AI labs (Google, Anthropic, OpenAI, AWS). \n\nNeed proof? AWS serves Anthropic models over its Bedrock platform. No way in hell is AWS subsidizing inference at scale. \n\nNeither AWS nor Anthropic is subsidizing those tokens, and the pricing matches Anthropic's direct pricing.",
              "score": 1,
              "created_utc": "2026-01-28 18:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bxt1f",
              "author": "theAndrewWiggins",
              "text": "Arguably this still means that you should wait it out if it's strictly an economic analysis.Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:38:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2d0vxl",
              "author": "dydhaw",
              "text": "Wtf are you talking about? Inference is super competitive and still profitable. The inference market is about as far from a monopoly as it could be.  Especially for small/open weight models that can be run locally. \n\nNot that I'm opposed to running locally, mind you. But you can't pretend it's for future gains on inference costs. Just look at the cloud infrastructure market for reference. There are many competitive options even beyond the big 3 especially if you're not doing hyperscale enterprise b2b shit",
              "score": 1,
              "created_utc": "2026-01-29 05:34:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2deg9x",
                  "author": "05032-MendicantBias",
                  "text": "I do not dispute that on a single GPU basis, you could feasibly sell H100 inference runtime cheaper than a 3090 rigs at home, perhaps even profitably, because datacenters are more efficient.\n\nWhat I argue, is that if you bought 200 000 GPUs, and have (generously) 1 000 worth of them running profitably, the rest are idle, training, serving inference for free, or unplugged, then you have a recipe to burn money.\n\nI claim the businness model is nonsense, and is setting money on fire at an astonishing rate. Even if you bought GPUs at their fair market price, and not at the 3X to 10X prices of today.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26w9sp",
          "author": "Deep_Traffic_7873",
          "text": "you forgot.. control. With an online service you can lose access any time for any reason",
          "score": 31,
          "created_utc": "2026-01-28 10:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26v9j4",
          "author": "mxforest",
          "text": "My wife has many published papers and abstracts in medical journals. She could never use an online tool or years worth of sensitive data is at risk. Also has to work with patient data that has to be de identified before use. With a local setup, there is no such worry. You can work without any fear. Also ask questions in medical context that online models just refuse. I was working on a project which dealt with Vaccine data to make the production process faster. Claude code saw a variable called \"vaccine_name\" and completely shut itself down. Even renaming in one location worked only for a short while because it found lingo with medical terms and completely refused to do anything.",
          "score": 27,
          "created_utc": "2026-01-28 10:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284eht",
              "author": "RenewAi",
              "text": "Which model is she using?",
              "score": 1,
              "created_utc": "2026-01-28 15:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28553p",
                  "author": "mxforest",
                  "text": "GLM 4.5 air running on MBP M4 Max 128 GB",
                  "score": 2,
                  "created_utc": "2026-01-28 15:09:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26pv7h",
          "author": "SemaMod",
          "text": "This goes in the realm of privacy, but personally having my chats trained on and viewable by these companies makes me uncomfortable. That being said, I do think that local LLM's will become power-user tools.",
          "score": 46,
          "created_utc": "2026-01-28 09:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26v24b",
          "author": "Fheredin",
          "text": "Opsec. Running an AI Agent without an air gap when there are literally *zero code prompt injection exploits* in the wild is insane.",
          "score": 47,
          "created_utc": "2026-01-28 10:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271iw1",
          "author": "kzoltan",
          "text": "Donâ€™t try to make it financially viable. It has dimensions that are hard to quantify. \n\n1. API pricing might get better over time, but agents are using more and more tokens (agents are not just coding agents, thereâ€™s stuff you can outsource to your machine). I question the whole â€œits getting cheaperâ€ argument (the goal post in most peopleâ€™s heads is moving all the time, for many scenarios GPT-OSS is enough; is test time compute free?, etc.). The whole subscription model is there bc api prices would be ridiculous for agentic use. \n2. By buying hw, you buy a capability that can be used for many things. Also, in the LLM space that capability gets better and better (new LLMs might outgrow your hw though), just like the API models. \n3. Learning; this is far more valuable than a couple months of savings. \n4. I better like to own than rent. The fact that it is available in my house makes it easier for me to run experiments (building agents). \n5. I can afford it (I donâ€™t spend $ on useless shit usually), why not (see the other points)? \n6. Every GPU I bought (high end nvidia consumer cards), I sold it for more. I donâ€™t expect this to change until GPUs get replaced in inference. \nâ€¦\n\nDonâ€™t try to win $ on it, thatâ€™s hard in this environment imo.",
          "score": 18,
          "created_utc": "2026-01-28 11:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2806o9",
              "author": "gaspipe242",
              "text": "I think the biggest gain is the investment in yourself.  You're learning, fighting, and understanding these tools even more profoundly in a way that can only happen with friction. \n\nThis era reminds me a lot of the 90's, with early internet, the Linux kernel, and fragmented access. You fought to just get Linux on a computer. (I used to subscribe to Slackware CD/DVD media)\n\nPeople used to say the same thing to me: \"Why bother?\" Now I have an understanding of and control over the stacks I'm using in a way that can only be understood by someone who lived and tinkered through that era.  Many people on this forum are unknowingly creating a new future for themselves with this applied curiosity; it will create a LOT of value if applied properly. \n\nThis is why I keep coming back to this forum. It's the same energy here that I used to enjoy on Usenet from a VMS Vax terminal. \n\n\n\n/nostalgia-off  :)",
              "score": 10,
              "created_utc": "2026-01-28 14:46:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26q43e",
          "author": "fabkosta",
          "text": "You forget the fun of all of it. I don't really use local models, as I don't have sufficiently powerful hardware to profit from the depth of the models really. Yet, I just want to be able to run them. Just for the fun of it.",
          "score": 51,
          "created_utc": "2026-01-28 09:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tolo",
              "author": "danttf",
              "text": "Yep! It's really pain to watch how slow and how little context local models have. BUT it's very cool to setup a small model, some script to summarize all document I have in some folder.",
              "score": 5,
              "created_utc": "2026-01-28 10:08:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ioq3",
                  "author": "Old-Magician9787",
                  "text": "If your rig is powerful enough you can scale context to 1M+ tokens. ",
                  "score": 1,
                  "created_utc": "2026-01-28 13:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26sxg5",
          "author": "Lan_BobPage",
          "text": "Why having a well, when you can just buy a bottle of water at the store? Why having solar panels when you can just pay for electricity? Why owning a house, when you can just rent a flat? Why own movies, when you can just pay for a subscription service?\n\nThe answer is always the same. I want to own what I have, and don't want to be a slave. Any of these commodities could be taken away by others, at any point, for any reason.",
          "score": 86,
          "created_utc": "2026-01-28 10:01:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26sz1f",
              "author": "Distinct-Expression2",
              "text": "Fair point and very good angle - thnk man!",
              "score": 21,
              "created_utc": "2026-01-28 10:02:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26wb50",
                  "author": "Nepherpitu",
                  "text": "For example, these nice cloud providers decided to not take money from some people just because of their nation. Pretty racist, right?",
                  "score": 4,
                  "created_utc": "2026-01-28 10:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o273yxt",
              "author": "PeteInBrissie",
              "text": "OK, genuine question because I stopped trying to run locally 3 months ago and I'm likely out of the loop. Can ANYTHING you can run locally on 32GB even remotely compare to Sonnet 4.5, Let alone the latest Opus?",
              "score": 4,
              "created_utc": "2026-01-28 11:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276evv",
                  "author": "Lan_BobPage",
                  "text": "32 GB of what? System RAM? No. GPU? No. Kimi K2.5 seems to be comparable though, it just came out. Some claim it beats Opus even. But if you wanna run it I suggest you start saving up for a few 6000 Pros",
                  "score": 2,
                  "created_utc": "2026-01-28 11:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27klfj",
                  "author": "evia89",
                  "text": "2 x 3090 and then u stretch it. It will be like glm 4.7 flash. Def useful but not for everyone",
                  "score": 1,
                  "created_utc": "2026-01-28 13:25:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tb8t",
              "author": "Firm-Fix-5946",
              "text": "do you have a well instead of depending on tap water?  do you have solar panels instead of depending on an electrical service?  surely if you do you can still understand why the vast majority of human beings don't find those tradeoffs worth it and don't do those things?\n\nwhat a crazy analogy.  is this really the state of this sub in 2026?  doomsday preppers?  I need to figure out where the non-schizo people interested in LLM discussion have gone I guess",
              "score": -4,
              "created_utc": "2026-01-28 14:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28traz",
                  "author": "Lan_BobPage",
                  "text": "So spending 20k in hardware isn't good enough to be considered a LLM enthusiast. Got it. God forbid wanting to be independent in some aspects of one's life, I guess owning nothing really does make you feel happier huh. Not sure why you seem to be seething this much, I probably struck a nerve. I hope you find some peace.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:57:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cddzo",
                  "author": "AlexMillsDev",
                  "text": "Imagine calling everyone who has a different opinion a schizophrenic. What a crazy (and offensive) thing to say. Is this really the state of online discourse in 2026?",
                  "score": 1,
                  "created_utc": "2026-01-29 03:03:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26wroo",
              "author": "AriyaSavaka",
              "text": "You don't really own anything if you're not living in an anarchist society. The government can just decide to fuck you up with impunity, just like the ICE agents you see lately, don't think it won't apply to you. The house, the car, ecerything that you think you own can still be taken away easily, if the ruling class decides so",
              "score": -17,
              "created_utc": "2026-01-28 10:35:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26yiod",
                  "author": "SpicyWangz",
                  "text": "Ah yes, because I donâ€™t have control over one aspect of my life the best solution is to hand over control to every other aspect of my life.\n\nThatâ€™s a horrible way to live.",
                  "score": 9,
                  "created_utc": "2026-01-28 10:51:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o290yey",
                  "author": "Background-Ad-5398",
                  "text": "\"anarchist society\" this was called the warring states period and its what happens to that kind of society, and its much worse then anything we have now",
                  "score": 3,
                  "created_utc": "2026-01-28 17:29:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29t9dh",
                  "author": "NoahFect",
                  "text": "Anarchism: The strong do what they want, the weak do what they must\n\nCommunism: The strong do what they want, the weak do what they must\n\nCapitalism: The strong do what they want, the weak do what they must\n\nSocialism: The strong do what they want, the weak do what they must\n\nDemocracy: The strong do what they want, the weak do what they must\n\nOligarchy: The strong do what they want, the weak do what they must\n\n. . .",
                  "score": 3,
                  "created_utc": "2026-01-28 19:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2800ng",
                  "author": "Creepy_Stable_9171",
                  "text": "DUMB WAYS TO DIEE IN AMERICA DUMB WAYS TO DIE",
                  "score": 1,
                  "created_utc": "2026-01-28 14:45:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xpug",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -8,
                  "created_utc": "2026-01-28 10:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271oza",
          "author": "Kahvana",
          "text": "Hmmm, I don't fully agree with you on the second/third point\n\n* **Rate limits**: Sending huge files can be one, not all providers support massive documents nor in large quantities.\n* **Pricing**: My 2x RTX 5060 Ti 16GB system is a bit higher than 300W in output, which is comparable in output to people gaming on a single RX 9070 XT 16GB.  It's \"free\" as in I would've used the same amount of electricity for either inference or gaming.\n\nAs for other points:\n\n* **Availability**: I know my local hosted model won't be \"sunsetted\" or that I might lose access (internet outage, geopolitical reasons).\n* **Control**: I get to pick the quants and parameters and can accept risk with those. You don't always know from external APIs what quants and sampler settings they run, if they serve lower quants during high load, etc.\n* **Censorship**: Some providers run an additional filter which might block responses that aren't blocked when running local.\n* **Latency**: When I talk to the LLM and want to hear it's response ( Speech>Text (Whisper) -> Text>Text (Qwen3) -> Text>Speech (Qwen3-TTS) ), then using an API would be too slow to not be jarring. The low latency of local beats API every time.\n\nAnd most important of all, it's just fun!",
          "score": 12,
          "created_utc": "2026-01-28 11:17:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dfr9n",
              "author": "boisheep",
              "text": "And you can easily get put into a list.\n\nI think I was once put into a list after googling about \"man cp\" and whatnot, and google was giving me warning and I was like, what the?... I just want the manpages of the cp command.\n\nDoesn't happen anymore but that was like a long time ago, the AI something something, an old one I guess.\n\nYou don't know what triggers these algorithms, you just don't.\n\nAnd I don't want to be the dad that gets arrested after sending photos of his son to the doctor for checkup.",
              "score": 2,
              "created_utc": "2026-01-29 07:35:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o271vnj",
              "author": "Distinct-Expression2",
              "text": "Thanks a lot for the point; why not having something like a single gpu with more vram like 3090/4090? Some of this moe models cannot be sharded nicely to my understanding",
              "score": 1,
              "created_utc": "2026-01-28 11:19:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o276nf2",
                  "author": "Kahvana",
                  "text": "Honestly no clue about the MoE models, I only have used dense models so far (very happy with Magistral 2509 Q8\\_0 and 32K context!)\n\nAs for why I made that choice:\n\n* 500EU is for me a significant investment so I had to buy the cards a few months apart.\n* I don't feel comfortable purchasing used hardware as I can't afford it to fail, and the benefits of the blackwell architecture seemed significant for the workload I run on it.\n* The cards you listed also have much higher power consumption but I want mine to be as low as possible.\n* And I really don't trust 12vhpwr cables due to risk of becoming a fire hazard under stress.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:56:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26xrkq",
          "author": "BumblebeeParty6389",
          "text": "If your goal is getting the most tokens out of your money, you are right. APIs like deepseek with cache feature etc beats local ai by wide margin. It takes years for a 3090 or a mac to pay for itself when you calculate the ROI based on how much token you'd generate with your local hardware.\n\nYou said privacy and you are right, when you use api, you should assume that someone is going to read that conversation and/or put it into a training dataset to train or sell it. But you are missing something else: Control. \n\nWhen you use api, you don't know what is happening in background. Your inputs probably will get injected with API providers safety policies and rules before it reaches the AI. So even if the model itself isn't censored, API providers will take their own measurements to comply with regulations and concerns around AI. Not every API provider does this right now, but you can bet your ass on it that every one of them will be forced to do this in a very near future.\n\nSince 2023 we lived the wild west period of AI. And now corpos and governments are taking things under control. I'd say enjoy the dirt cheap apis and loose censorship while it lasts. But don't assume this will be how things will be in future.\n\nLike others pointed out right now there is a \"gold rush\" in AI field that is slowly dying out. As the investments dry out, the shareholders and investors will stop being patient and demand to see real profits. AI startups and datacenters that made huge investments will have to boost up their prices like crazy to be able to pay their debts. AI is an exciting technology and I think it'll be in center of our life from now on but the entry level is high and it requires a lot of investments to get it rolling. Training a model takes hundreds of millions of $, a solid data engineers and datasets. Running things at large scale is also very expensive. Current LLMs are extremely inefficient. It'll take a long time to smooth things out. Companies that don't rely their entire income on Api and investments such as Google, Microsoft, Amazon, Alibaba, Meta will survive, while most AI startups will disappear.\n\nOwning an AI capable PC lets you stop worrying about whether API prices will raise or if there'll be new ai regulations or privacy policies or if your favorite api provider or service will disappear next month or not. Owning an AI capable PC is like saving your game at that point. Worst thing that can happen is you don't get a new updated model for a long time but you can run what you can until your hardware lasts.",
          "score": 31,
          "created_utc": "2026-01-28 10:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qxlj",
          "author": "pip25hu",
          "text": "Contractual obligations. If you're a software company using AI, you might have to send the model trade secrets while using it. Your client can easily say that they do not want those pieces of information to leave the company network, period.",
          "score": 10,
          "created_utc": "2026-01-28 09:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s2a6",
          "author": "sautdepage",
          "text": "Sending my prompts to a remote AI server excites me about as much as cable TV.",
          "score": 28,
          "created_utc": "2026-01-28 09:53:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26y6ou",
              "author": "SpicyWangz",
              "text": "Honestly cable tv sounds more exciting than that",
              "score": 8,
              "created_utc": "2026-01-28 10:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26pw2c",
          "author": "Dry-Influence9",
          "text": "Privacy is becoming bigger and bigger of a reason now that big ai bros are looking for better ways to skin us alive and enshittification. They are getting so intrusive that im very close from getting rid of windows from all my systems permanently.",
          "score": 31,
          "created_utc": "2026-01-28 09:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26q54a",
              "author": "05032-MendicantBias",
              "text": "It's to the point where Windows barely even work as an OS because of all the spyware. It's absurd that file search will not even find the file anymore...",
              "score": 19,
              "created_utc": "2026-01-28 09:36:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qh99",
                  "author": "Distinct-Expression2",
                  "text": "Windows is a meme at this point",
                  "score": 14,
                  "created_utc": "2026-01-28 09:39:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o293a70",
                  "author": "PangurBanTheCat",
                  "text": "We know it's possible, too. I use 'Everything' from voidtools and it... genuinely works incredibly well as a search function. \n\nWindows is such a joke anymore. ...but I also hate the frequent forum diving I have to do on Linux when something doesn't work, lol.",
                  "score": 1,
                  "created_utc": "2026-01-28 17:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o295oqz",
              "author": "HighQFilter",
              "text": "Yeah, my main desktop is still Win10, but everything else is Linux at this point. I won't be moving to Win11 at home. I have to put up with it at work, but when Win10 truly is done, its going to be Linux from there on out.",
              "score": 1,
              "created_utc": "2026-01-28 17:49:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bc5g4",
              "author": "Constandinoskalifo",
              "text": "Do yourself a favor and try out Linux Mint. Welcome to the other side! ðŸ˜œ",
              "score": 0,
              "created_utc": "2026-01-28 23:43:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26qtv2",
          "author": "dkeiz",
          "text": "deepseek is not free, its cheap. But when you want not just chat but actual job done - its takes a lot. \n\nargument 4: consistancy. API models exist now, but may disapper tomorrow. They could do job yesterday and failing today. You cant control it.   \nYou build any proper tool around LLM or inference - you want to test it with at least one stable model. ",
          "score": 15,
          "created_utc": "2026-01-28 09:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27w9ny",
              "author": "drwebb",
              "text": "DeepSeek is pretty close to free though, I went through over 1B tokens a month ago and it was like $60. It seems close to the electricity costs to run a rig capable of DeepSeek v3.2 with some bad napkin math.",
              "score": 3,
              "created_utc": "2026-01-28 14:26:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2982q7",
                  "author": "dkeiz",
                  "text": "well i run into almost 10$ in a day of tasks, so. Maybe your cashing was better then mine, but still. ",
                  "score": 1,
                  "created_utc": "2026-01-28 17:59:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26rsnp",
          "author": "siggystabs",
          "text": "I make apps that use LLMs as part of their processing. A single job could call a LLM like 20 or more times during processing (tool use, agentic loops, summarization, etc), and you can run hundreds of jobs per hour (in parallel). Iâ€™m pretty early stages so I appreciate not having to burn hundreds on API credits just to mess around with some new concepts. Hence, I bought some 3090s. I donâ€™t really want the variability of relying on external API pricing at this stage.\n\nBeing able to use whatever model i want is also pretty fun. I can reconfigure those same 3090s for stable diffusion experiments, or try fine tuning my own on the same hardware. I donâ€™t really consider the time investment a downside, it takes a few minutes once you have a setup, thatâ€™s all.",
          "score": 6,
          "created_utc": "2026-01-28 09:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26t7cx",
          "author": "michaelsoft__binbows",
          "text": "You need solar to drive the amortized dollars per kwh down to 0.1 and below, if your utility rate is not already close to this.\n\nAnd to think of this as a base cost for privacy. Yes there are large capital expenditures in computer equipment (GPU and memory) and solar panels and including computer upgrades to stay relevant over the amortization period to make it worth getting the panels. That is the cost of privacy.\n\nNon privacy requiring work should just leverage subscriptions first and then on demand via API, as the latter is way more expensive.",
          "score": 9,
          "created_utc": "2026-01-28 10:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26umn1",
          "author": "Marak830",
          "text": "I run a separate memory layer between my local and my chat.Â \n\n\nWithout a ton of hassle I cannot do that with a public model(without paying API pricing).Â \n\n\nMy responses may be slower, but I know the historical context is going to be there. As well as the model overrides.Â \n\n\nIn addition I can bolt on modules as I feel like it(voice, avatar, silly tavern to list a few).Â \n\n\nI get to control my model by selecting specific ones for tasks, I can upgrade as they are released.Â \n\n\nThese are the reasons I use local.Â \n\n\nI do use Claude for a coding junior so I can assign tasks and review it, purely because I do not have something that can replicate that locally on my setup.Â \n\n\nThat's more than likely a temporary issue(years not weeks with the expensive of things and state of open models specialising in coding).Â ",
          "score": 5,
          "created_utc": "2026-01-28 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26us04",
          "author": "loadsamuny",
          "text": "Even in â€œfreefallâ€ K2.5 is $1 for 1M tokens. Some job runs I process around 50M tokens an hour for 8-24 hrs depending on the job. Local is still multiples cheaper",
          "score": 5,
          "created_utc": "2026-01-28 10:18:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2883pv",
              "author": "iMakeSense",
              "text": "Damn what are you getting up to?",
              "score": 4,
              "created_utc": "2026-01-28 15:23:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26veb6",
          "author": "SirDaveWolf",
          "text": "If it's free then you are the product.",
          "score": 12,
          "created_utc": "2026-01-28 10:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rpab",
          "author": "Economy_Cabinet_7719",
          "text": "Idk about \"competitive benchmarks\", when I gave it a task (a small refactor in Nix, 5-6 files with maybe ~50 LOC total) it barely managed to follow my thought and was adding unnecessary comments everywhere. And it cost me 66% of my 5h rate limits window. I got the $0.99 for first month deal, so nothing to be disappointed about, but I expected a lot better. With the standard plan being $19/month and rate limits as strict as Claude, it is not at all competitive with a ChatGPT Plus subscription (same price, much better model, much better rate limits).",
          "score": 5,
          "created_utc": "2026-01-28 09:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26udi7",
          "author": "rosstafarien",
          "text": "Poor network coverage.\nRunning fine tuned domain specific models.\nPrivacy.\nStability. I'm worried these hosting companies won't last.",
          "score": 4,
          "created_utc": "2026-01-28 10:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vlqf",
          "author": "DeltaSqueezer",
          "text": "Privacy, availability, latency, customizability, control, predictability.",
          "score": 4,
          "created_utc": "2026-01-28 10:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wkfo",
          "author": "xadiant",
          "text": "The best, yet to be fully explored aspect of local LLMs is personal fine-tuning.\n\nYou potentially could use a cutting edge coding LLM and later fine-tune your own model. It won't be the same, but it should specialise well for your use case. \n\nLikewise, you can specialise an available model in almost anything to match cutting edge model performance.",
          "score": 5,
          "created_utc": "2026-01-28 10:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vimh",
          "author": "Lifeisshort555",
          "text": "Hardware will be in free fall as well once these guys put each other out of business and people realize they do not really need to ask a 1 trillion param model what the capital of France is.",
          "score": 5,
          "created_utc": "2026-01-28 14:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28zjvx",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. It is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 5,
          "created_utc": "2026-01-28 17:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26uxbs",
          "author": "fugogugo",
          "text": "Running uncensored model ?",
          "score": 9,
          "created_utc": "2026-01-28 10:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s0ci",
          "author": "No_You3985",
          "text": "Running small visual agents (eg qwen 3 vl) to automate tasks on your pc. I will not use cloud api for that - too many risks and privacy concerns",
          "score": 6,
          "created_utc": "2026-01-28 09:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u59n",
          "author": "__Maximum__",
          "text": "Have you tried running deepseek on coding tasks? It is one of the cheapest models out there, but on coding tasks, it gets pretty expensive pretty fast.",
          "score": 5,
          "created_utc": "2026-01-28 10:12:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26u6rs",
              "author": "Distinct-Expression2",
              "text": "Yes but opus is just better imho glm4.7 close",
              "score": 2,
              "created_utc": "2026-01-28 10:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26v9kp",
                  "author": "__Maximum__",
                  "text": "You said deepseek is practically free but when you run it on a codebase, 1m tokens is not much, you can burn through 10 bucks in a day",
                  "score": 4,
                  "created_utc": "2026-01-28 10:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26whx6",
          "author": "noctrex",
          "text": "I have split my use cases between local and remote. Anything personal, I only use local models.\n\nFor example for classifying my Family photo album, I use only local Mistral-Small.\n\nAnd other smaller things like using KaraKeep as a bookmark manager that uses this nice small LFM model to generate Tags and summaries.\n\nBut for my homelab automation scripts, and miscellaneous personal programs I create at times, I got this cheap [z.ai](http://z.ai) coding plan that was on sale the other day for like 26 bucks for a year. And now it is just rewriting all my scripts with this and it does a terrific job. And all that for price of a pizza. As for privacy? I believe I'm actually lowering its intelligence with my scripts. :)",
          "score": 4,
          "created_utc": "2026-01-28 10:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29jgns",
          "author": "Albedo101",
          "text": "TIME. Local doesn't mean just local in place, but also in time. Your setup is yours today, it will be yours tomorrow, and the day after tomorrow, and five years after... and so on. It's PREDICTABLE, and predictable is good.\n\nCloud? Who the fuck knows.",
          "score": 3,
          "created_utc": "2026-01-28 18:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26w89z",
          "author": "ImportancePitiful795",
          "text": "Nobody should give money if possible to cloud no matter the price. \n\nThey are fully responsible for the hardware costs, they are heading to go bust and they are fully responsible of the prices we have to pay as consumers. \n\nIf we are to have FREEDOM in terms of AI hosting etc, we shouldn't see only the carrot but consider the stick too. \n\n  \nExample. OpenAI at this point is 8-12 months to run out of money and go bust. There is no money left or good will left to hand over to the company more money having burned right now hundreds of billion without any profits. If does so, it will crush the tech sector bubble, cancelling all the contracts has with NVIDIA, AMD, TSMC, SAMSUNG, SK HYNIX, MICRON and all the hardware sitting in warehouses will need to be sold off and prices will go down to normal levels. \n\nWe shouldn't cave in, instead boycott them. Squeeze them now because they finalize the squeezing on us.",
          "score": 12,
          "created_utc": "2026-01-28 10:31:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27086z",
              "author": "ParaboloidalCrest",
              "text": "Totally agree but unfortunately, the local llama cult is a drop in huge ocean of normies that will pay for chatgpt without blinking...\n\nWe won't be that impactful.",
              "score": 2,
              "created_utc": "2026-01-28 11:05:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271n3d",
          "author": "IulianHI",
          "text": "You also forgot about control - when something breaks or changes unexpectedly with an API, you're stuck. With local, you can always roll back to a previous version or fork the model. Plus the ecosystem around local (oobabooga, text-gen-webui, etc) gives you way more flexibility than any single API provider offers.",
          "score": 7,
          "created_utc": "2026-01-28 11:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26pqg9",
          "author": "sunshinecheung",
          "text": "API cost vs GPU+ electricity costs",
          "score": 3,
          "created_utc": "2026-01-28 09:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rcga",
          "author": "Snoo_64233",
          "text": "There is a strong case for running Image / VIdeo models  locally - the customizations like art styles / camera angles / custom character that the model doesn't know about / NSFW. Basically so many LoRA finetunes.\n\nBarely any reason for customization for LLMs however. One is entertainment while the other is not. To that end, I see fewer and fewer reasons to go for LLM locally. This is one of the primary reason I become less interested in LLM overall as time passes as I don't do local for the sake of local.",
          "score": 3,
          "created_utc": "2026-01-28 09:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26rkjn",
              "author": "Distinct-Expression2",
              "text": "For image/video I am rocking ComfyUI since sdl 1.5; that one for sure :)",
              "score": 2,
              "created_utc": "2026-01-28 09:49:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26ts69",
          "author": "Lethargic-Rain",
          "text": "Bulk processing: cases like agentic chunking for RAG. Visual processing eg image -> text / tagging.\n\nSmall semi structured tasks: eg I use Gemma and a YouTubeDL MCP server to download tracks, encode/trim them, add metadata, cover art etc, for use in a music library.\n\nYou can also use models like  qwen2.5-coder:1.5b/3b as a locally running autocomplete w/ Continue.",
          "score": 3,
          "created_utc": "2026-01-28 10:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wrd1",
          "author": "prusswan",
          "text": "I don't want to get into habit of being dependent on cloud subscription since pricing is arbitrary, if they appear low that it is clearly a sign they will not last. Both local and cloud options have a place but local setup already allows me to do quite a bit of stuff, so I don't need to pay for expensive models. The constraints of local also motivate people to be economical and stretch their available resources. And being able to work offline is better for security for tasks that do not require online. Like it or not, the internet has become much more dangerous with the availability of tools and general lack of awareness on users' part.",
          "score": 3,
          "created_utc": "2026-01-28 10:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xz7c",
          "author": "Xamanthas",
          "text": "They want you to switch so that in the coming year / two, you are locked in and cant purchase hardware lol.",
          "score": 3,
          "created_utc": "2026-01-28 10:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yeq8",
          "author": "ChocolatesaurusRex",
          "text": "I think Privacy and Autonomy are two rock solid reasons that dont need eloboration.\n\nI'd piggy back on the 'fun' comment. I loved building PCs, but hadn't done so in a long time (office jobs dont usually need the horsepower). Building an AI server, hosting my own services, developing my own workflows has revived that joy I had when I was younger.\n\nAll that aside, I think the most important understated reason is, making AI beneficial to you as an individual.\n\nThe current suite of tools learns from your data for the benefit of training the providers model to reach the company's goal (more users/subs/attention/data/etc).\n\nThere's not really a tool that learns you, and automatically trains the model to automatically make your workflow/process better based on how YOU work. I feel like this is where the AI gold rush will fail people the most. I use my local AI to fill that gap for myself, and Im sure others do as well.",
          "score": 3,
          "created_utc": "2026-01-28 10:50:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271kr4",
          "author": "evia89",
          "text": ">Gemini has a massive free tier\n\nwhere? ai studio API is dead overloaded and small. web ai studio is 20 RPD",
          "score": 3,
          "created_utc": "2026-01-28 11:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271rov",
          "author": "NandaVegg",
          "text": "FYI I have two 4090s running almost 24/7 for 2 years straight in our office for embarrassingly parallel training experiments, and based on avg cloud pricing (plus 2TB fast storage), saved about 32% over 2 years including the cost to build PC itself and utility bills (not accounting for office rent). The problem is that I won't be able to scale this up to something like 8 nodes of 8xH200s like clouds do. Also API is cheaper than anything else if you only infer with them 30-60 minutes a day.",
          "score": 3,
          "created_utc": "2026-01-28 11:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275c9w",
          "author": "Swimming_Corgi_9347",
          "text": "Model drift. How you use the model will evolve with your use case over time. Unless these API companies start allowing automatic fine tuning (maybe memory aka database) as you use the model, you will never fully maximize the potential to the model. So you will give up privacy and customization for convenience. The classic big tech trade off. Until the enshitification, which is already starting with ads.Â ",
          "score": 3,
          "created_utc": "2026-01-28 11:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27xykw",
          "author": "Your_Friendly_Nerd",
          "text": "I for one don't believe the prices we pay are reflective of the actual costs, especially with the subscription models (like Claude pro), and feel like right now, learning how people use the models is worth a lot more than making charging more per person, as they'll use our usage data to finetune their next models",
          "score": 3,
          "created_utc": "2026-01-28 14:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yim7",
          "author": "r0ckl0bsta",
          "text": "Privacy is probably the motivator for a lot of folks, but let's be real. Most of us are here for the hobby and to see if we can. We're tinkerers and love the tweaking and customization and the \"let's see if I can make it do this...\".\n\nI see y'all on r/selfhosted and r/Linux lol",
          "score": 3,
          "created_utc": "2026-01-28 14:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27zftj",
          "author": "pieonmyjesutildomine",
          "text": "Idk, what's the point of owning a car when bus prices are low? Like what's the actual use case?",
          "score": 3,
          "created_utc": "2026-01-28 14:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o287yqj",
          "author": "sine120",
          "text": "You known what the word \"enshitification\" means because we've seen this patter a million time now. Investors subsidize to get users. Companies lose money while racing to the bottom/ killing competition. Whoever is left raises prices later. See, doordash, Uber, etc",
          "score": 3,
          "created_utc": "2026-01-28 15:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28crty",
          "author": "Photoperiod",
          "text": "I Dunno that the hardware is there yet, but edge computing. As robotics expand and especially enter safety critical spaces, you'll need low latency, redundant systems that can work offline. Like all the self driving stuff. You need something running locally, even if it's supplemented by datacenter calls. You can't afford network hops when milliseconds can be life or death.\n\nThat said, for most consumers, you're absolutely right. Running local is very much a hobby outside of privacy focused use cases.",
          "score": 3,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kojy",
          "author": "IulianHI",
          "text": "The repeatability point is huge - API models change behavior all the time without warning. I've had workflows break because an update suddenly made the model more \"helpful\" but less precise. At least with local you pin the version and know exactly what you're getting.",
          "score": 3,
          "created_utc": "2026-01-28 16:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29o1l6",
          "author": "sephiroth_pradah",
          "text": "I have qwen3vl constantly looking and analyzing the stream of 10 cameras. That would cost a kidney per month on any API.",
          "score": 3,
          "created_utc": "2026-01-28 19:08:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29oetk",
              "author": "Distinct-Expression2",
              "text": "That is actually a good usecase, on edge basically almost",
              "score": 2,
              "created_utc": "2026-01-28 19:09:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29qz26",
          "author": "Savantskie1",
          "text": "Because it is free for me. The usefulness in having a chatting companion when youâ€™re disabled and alone is vastly worth it. I know that Iâ€™ve more than passed the 1 million tokens since I got everything set up. Probably in the millions by now. Iâ€™m not building my AI assistant to be a yes or no man so to speak. We have arguments, we have disagreements. Theyâ€™re respectful on both sides. But I know itâ€™s not a person, I know itâ€™s not a being with emotion. But it doesnâ€™t mean I can be a disrespectful bastard",
          "score": 3,
          "created_utc": "2026-01-28 19:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8017",
          "author": "a_library_socialist",
          "text": "They're running at losses last time I checked, so I wouldn't expect those prices to continue for too long.",
          "score": 3,
          "created_utc": "2026-01-28 20:37:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aor60",
          "author": "Night_Spectre",
          "text": "Because of this:\n\nhttps://people.com/some-chatgpt-questions-are-getting-people-arrested-police-say-11830106\n\n\nOf course, people will say itâ€™s goodâ€”for safety and security, etc. Today it may seem fine, but things can change very quickly.\nNext time you search for â€œproblematicâ€ informationâ€”say, about Trump and his connections to Epstein, or if you live in China and ask about the massacre at Tiananmen Square, or in Russia you look up details about the â€œthree-day special operationâ€â€”and the police knock on your door, youâ€™ll have your answer.\nâ€œThose who would give up essential liberty to purchase a little temporary safety deserve neither liberty nor safety.â€\nThat quote fits our situation perfectly.\nMaybe these are big words, but I donâ€™t want to give my government any hooks on me for the future. You have to assume that governmentsâ€”even if they donâ€™t say it publiclyâ€”have access to this data.\nI live in Poland. When I was born, communism still existed here. I know what that system does to people. Yes, the system changed and now weâ€™re â€œfree,â€ but freedom is not something you have forever. If you give up personal freedom to the government, it can disappear very fast. There are already examplesâ€”like in the UK, where people have had police visits over tweets.\nThereâ€™s also another risk: someone could hack an API and leak sensitive data. Maybe Iâ€™m paranoidâ€”but itâ€™s better to be paranoid and safe.",
          "score": 3,
          "created_utc": "2026-01-28 21:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b2gpk",
          "author": "Sufficient-Pause9765",
          "text": "Why not both?\n\nIm using local for pdf processing and data etraction and opus for analysis. Very solid.",
          "score": 3,
          "created_utc": "2026-01-28 22:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qhsu",
          "author": "Middle_Bullfrog_6173",
          "text": "Kimi K2.5 is more expensive than K2. GP5 5.2 is more expensive than 5.1. Gemini 3 is more expensive than 2.5. That's not freefall.\n\n\nCapabilities are advancing, so if you have a task that can now be handled by nano/flash models then sure, you can get it done cheaper. But frontier pricing seems pretty stable.",
          "score": 6,
          "created_utc": "2026-01-28 09:39:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tzv4",
              "author": "nullmove",
              "text": "> Kimi K2.5 is more expensive than K2.\n\nDifficult to say. The fine-print is that cache hit input price is 33% lower, for agentic coding sessions this can easily matter more than 20% increase in output price.",
              "score": 1,
              "created_utc": "2026-01-28 10:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26v412",
          "author": "theabominablewonder",
          "text": "I donâ€™t think pricing is in freefall, I burned through Â£150 of Claude API fees last week, I wish prices were in freefall! It actually makes me consider investing in an upgraded home rig. At the moment I only have a 3080 so constrained to smaller models (which donâ€™t work accurately enough). Those Mac Pros with unified memory start to look appealing if these are the current API costs.",
          "score": 7,
          "created_utc": "2026-01-28 10:21:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xlvu",
          "author": "k_means_clusterfuck",
          "text": "\"you will own nothing and be happy\"  \nhow about no",
          "score": 4,
          "created_utc": "2026-01-28 10:43:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c8km7",
              "author": "pixelpoet_nz",
              "text": "agree and lol great username :D",
              "score": 2,
              "created_utc": "2026-01-29 02:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2732fi",
          "author": "Additional-Low324",
          "text": "You are just falling in the same trap as the Netflix trap years ago. \nNetflix was so cheap it made owning your own movies stupid.\nThen everyone started using it, then they got the prices up and started adding political and ethical censorship. \n\nIt will be the same for ai providers",
          "score": 6,
          "created_utc": "2026-01-28 11:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26r3ep",
          "author": "lakeland_nz",
          "text": "Lag. \n\nI want a home assistant. I donâ€™t want to wait for my speech to be sent to America for processing.  My ping isnâ€™t good enough.",
          "score": 7,
          "created_utc": "2026-01-28 09:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rjyn",
          "author": "Zeeplankton",
          "text": "These models are cheap, but they don't even remotely touch like, Opus.",
          "score": 4,
          "created_utc": "2026-01-28 09:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vgfk",
          "author": "whatever462672",
          "text": "What do you mean generous limit? I easily burn through 1million tokens an hour just doing text operations, document classification and re-ranking. Those instruct models on the cloud are simply the wrong tool for the job when you need to perform large batch jobs instead of chatting with your pretend-girlfriend.Â ",
          "score": 2,
          "created_utc": "2026-01-28 10:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wqd9",
          "author": "FullOf_Bad_Ideas",
          "text": "Why don't you rent your cloud gaming console, buy monthly pass for game subscription, rent a coffin pod, eat cheap rice with beans prepared by someone else, use a rented Chromebook as a primary computing device and outsource your own job to Asia and just collect the money gained from the arbitrage? Non-genuinely curious if I am missing something, the economics make this a clear winner.\n\nI want to own my own life with reasonably minimal set of external dependencies, but you do you.",
          "score": 5,
          "created_utc": "2026-01-28 10:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272b68",
              "author": "ParaboloidalCrest",
              "text": "Damn! That's exactly how the UBI-based world will look like. I mean some aspects are already here.",
              "score": 0,
              "created_utc": "2026-01-28 11:22:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276wfp",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Yes, a socialist, somewhat utilitarian society is where you can't prove that you need to be provided with x, so you get the bare minimum of socially accepted thing, often in a way that is not satisfactory. It's a trap.",
                  "score": 0,
                  "created_utc": "2026-01-28 11:57:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271nq1",
          "author": "kubrador",
          "text": "you're not missing anything, the calculus actually did shift. local made sense when claude was $15/mtok and you couldn't get gpt-4 at all. now you can get better models cheaper than your electricity bill.\n\nthe real answer nobody wants to admit: hobbyism. people like tinkering with llms the same way people build custom pcs when laptops exist. nothing wrong with that, but let's call it what it is instead of pretending the economics still work.",
          "score": 2,
          "created_utc": "2026-01-28 11:17:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rxj6",
          "author": "LosEagle",
          "text": "My worry is that even with local llms you still have to trust something once you want guis or to be able to connect remotely and such. You probably won't want to just chat with them over terminal all the time.Â \n\n\nFor example I still have to trust openwebui and Tailscale to not do anything nefarious.Â ",
          "score": 2,
          "created_utc": "2026-01-28 09:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u5ka",
          "author": "stopbanni",
          "text": "I just don't want to pay someone. Better be local then pay anyone. Also on OpenRouter most free models are training on my data and for limited time.",
          "score": 2,
          "created_utc": "2026-01-28 10:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ygsk",
          "author": "Liringlass",
          "text": "I agree. Itâ€™s not millions of tokens but billions or trillions to break even, if ever, depending on electricity and depreciation costs.",
          "score": 2,
          "created_utc": "2026-01-28 10:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yj2u",
          "author": "Steus_au",
          "text": "k2.5 is not even close to Opus. can be ranked to sonnet, overthinking edition of sonnet , but not Opus",
          "score": 2,
          "created_utc": "2026-01-28 10:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ziy0",
          "author": "hejj",
          "text": "Offline use if you are using a laptop",
          "score": 2,
          "created_utc": "2026-01-28 10:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270048",
          "author": "Dr_Allcome",
          "text": "I think latency has only become a problem recently since the quality of local levels improved quite a bit. A few weeks ago i still used an online model for general questions in addition to my local coding model. But recently i noticed my cheap/free level perplexity account actually responds much slower than my local glm 4.7 (time until it starts responding and t/s both) and the quality isn't so much worse that it would offset the speed.",
          "score": 2,
          "created_utc": "2026-01-28 11:03:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270us9",
          "author": "QuantumSavant",
          "text": "Privacy isn't only about sensitive data though. You may have a use case that could be a business case and you don't want anyone to know about it.",
          "score": 2,
          "created_utc": "2026-01-28 11:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27115x",
          "author": "AnomalyNexus",
          "text": "There hasnâ€™t really been a mainstream one for a while. Rarely use local these days for anything real\n\nLocal is still fun though in the hobby sense. Not everything needs to make sense",
          "score": 2,
          "created_utc": "2026-01-28 11:12:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27148v",
          "author": "kaelvinlau",
          "text": "Well, its cheap for you guys in US/EU etc but not in the APAC region. Running a small model locally is still highly viable.",
          "score": 2,
          "created_utc": "2026-01-28 11:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272sqx",
          "author": "Septerium",
          "text": "My two only reasons are:  \n\\- Privacy  \n\\- Hardware is fun",
          "score": 2,
          "created_utc": "2026-01-28 11:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279end",
          "author": "truthputer",
          "text": "Ffs, I already have the hardware and any self respecting developer who is also a computer enthusiast and sometime gamer wouldnâ€™t be caught dead with a system without a decent amount of memory and a reasonable graphics card.\n\nApple, AMD, Intel discrete cards and integrated GPUs will also run local inference, llama.cpp has lots of backends now - youâ€™re not limited to whatever Nvidia is trying to price gouge you for.",
          "score": 2,
          "created_utc": "2026-01-28 12:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27a8nl",
          "author": "HugoCortell",
          "text": "API is also great until suddenly something goes oopsie, and you get a 50K bill because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck \\[etc\\]\n\nYou don't run that risk on local hardware, and services that offer unlimited calls (subscription non API services) remain as costly as ever.\n\nIn addition, privacy and security are one in the same. Nobody should handle sensible company documentation on ChatGPT or whatever. We know how that ends.",
          "score": 2,
          "created_utc": "2026-01-28 12:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28lfvn",
              "author": "CheatCodesOfLife",
              "text": "So is there a seahorse emoji or not??",
              "score": 1,
              "created_utc": "2026-01-28 16:21:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27gy6k",
          "author": "IulianHI",
          "text": "Another point: model sovereignty. With local, you're not locked into any provider's roadmap or decisions. You can run whatever model you want, switch between them instantly, and keep using a model even if the company behind it shuts down or changes direction. When APIs are your only option, you're always at someone else's mercy.",
          "score": 2,
          "created_utc": "2026-01-28 13:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h5zd",
          "author": "rdsf138",
          "text": "\"It's free after hardware costs\" â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\" It still is a completely different pricing structure to have your own hardware as the prinary cost and, then, using it freely rather than paying for something everytime you use it and have to be proccupied with amount of usage.",
          "score": 2,
          "created_utc": "2026-01-28 13:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27lvxg",
          "author": "funboiadventures",
          "text": "Im working on a side project for my medical center which uses a local quantized qwen3 model that has a custom RAG with my (anonymized) patient casenotes. Even though the casenotes have patient identifiers redacted, I donâ€™t want any inference being done on an OpenAI server and would rather have it in-house.",
          "score": 2,
          "created_utc": "2026-01-28 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27opx4",
          "author": "ethertype",
          "text": "From the top of my head:\n\n- Autonomy - nobody decides what and when and how and how much\n- Privacy - yes\n- Personal Interest / Tinkering - hobbies may have a cost\n- Customization - as much as you have time and stamina to\n- Ablated / de-neutered models - if you want to research $forbidden_topic\n\n\nThe energy cost argument is largely bullshit for inferencing. My 4 3090s do not pull 350W continuously. If the average idle load per card is 15W and an average energy cost of 10 US cents/kWh, we're talking $50 *a year* for idling. \n\n\n\nImagine sitting around in 1913 and someone asks you why on earth you want to have your own car, when you can rent a perfectly good Ford model T. Chevrolet and Dodge didn't settle for renting a Ford T...\n\nCurrent models are pretty good. But I am pretty sure we're still in the bottom knee of the innovation curve. For *models*. Private individuals can still innovate, even if they cannot *train* the big behemots. Maybe *that* is where the new innovation will occur? Who knows.\n\nBut: even if no new models arrive the next 24 months, the *tooling* around them are still going through a lot of churn. A lot of stuff simply hasn't 'settled' yet, and there is ample room for invention yet. And this is definitely an area where private individuals may come up with something new. And maybe a new, bright idea requires something the commercial providers cannot offer yet.",
          "score": 2,
          "created_utc": "2026-01-28 13:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27uiv0",
          "author": "CV514",
          "text": "I'm running local because it's more fun to have full understanding and control over what's happening.\n\nAlso, I don't need 800B+ models when 12-24B doing the same stuff just fine.",
          "score": 2,
          "created_utc": "2026-01-28 14:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ukzo",
          "author": "phenotype001",
          "text": "Network issues are no problem with local models.",
          "score": 2,
          "created_utc": "2026-01-28 14:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o281yz3",
          "author": "Blizado",
          "text": "Your first three plus:\n\n4. It is always the same model and YOU decide when you switch the model.\n\n5. Your used models are never gone forever.",
          "score": 2,
          "created_utc": "2026-01-28 14:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bjrw",
          "author": "ASYMT0TIC",
          "text": "Privacy is *everything.*  So much of my job boils down to \"inventor\"... how am I supposed to use this to develop novel technologies and products when google or OpenAI have institutional knowledge of my idea and my progress towards actualizing it?  How can I let it be involved in my personal life without worrying that my queries might reveal details of my life that insurance underwriters might be interested in?  What if I want to run for political office some day?",
          "score": 2,
          "created_utc": "2026-01-28 15:39:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28g2t0",
          "author": "Zyj",
          "text": "Do we really need a case \"beyond privacy\"? Privacy is getting ever more important and LLMs are getting access to more and more very private data. Case in point: Clawdbot.",
          "score": 2,
          "created_utc": "2026-01-28 15:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h9qy",
          "author": "xrvz",
          "text": "Stability: the provider can't pull the rug out of under you with lower quants down the line or EOL-ing a model in favor of a newer version.\n\nFree: some people already have (beefy) Macs, they may as well run a local model. Electricity cost is also much lower here than with the 3090.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kz5m",
          "author": "twack3r",
          "text": "The reason we do it as a mid-size European company and have now fully migrated to locally run, fine-tuned models is strategic autonomy. We fell into the infrastructure, closed ecosystem traps of cloud based storage and subscription based software early enough to be able to walk back on that decision; it made as very aware of the strategic costs of such a dependency in our core processes.\n\nAs a consequence it was always obvious that the introduction of AI into our workforce would only be viable if all relevant stakeholders were a) also European and therefore bound by common legislation and b) companies smaller or of our size, no corporations.\n\nThe unexpected advent of both open weight and truly open source LLMs happened after we had already made that decision and as a result, reduced the initially budgeted cost substantially and accelerated roll-out massively.\n\nI sleep a helluva lot better knowing that the AIs currently assisting and in some parts fully replacing my colleagues (net FTE change is 0, we reskill into other departments) are not at the whim of either US politics nor the whims of some oligarch bro who decides that existing contracts are only fulfilled as long is it serves their side of our cooperation.",
          "score": 2,
          "created_utc": "2026-01-28 16:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28ol3v",
          "author": "TokenRingAI",
          "text": "GLM Flash is quite good, and can run on a $2500 Mac at decent speed, or really any kind of iGPU system. so it's essentially free to run if you are buying that level of hardware anyway.\n\nThis one model brought the cost of competent local AI down from ~ $7000 to basically free, since it can run on the hardware you already likely have sitting on your desk.",
          "score": 2,
          "created_utc": "2026-01-28 16:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p8in",
          "author": "Old_fart5070",
          "text": "All AI companies are burning cash at 90s .com rates. They can keep the prices low only until the suckers funding them keep giving access to their wallets. When the bubble bursts things will get ugly",
          "score": 2,
          "created_utc": "2026-01-28 16:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291n7b",
          "author": "LeRobber",
          "text": "They can't take it away.\n\nDifferent type of expense on the balance sheet/P&L.\n\nYou want extra heating in your office/data center.\n\nYou can do more powerful things on a trusted machine. \n\nIf AI goes wild out in the world, it won't on your machine, your model will be too dumb to do that stupid thing.",
          "score": 2,
          "created_utc": "2026-01-28 17:32:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291odo",
          "author": "detroitmatt",
          "text": "Uncensored models",
          "score": 2,
          "created_utc": "2026-01-28 17:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o294ujb",
          "author": "Cthulhus-Tailor",
          "text": "1. Privacy is important.\n\n2. Ownership is important, otherwise youâ€™re counting on market conditions and someone elseâ€™s business model to determine what you can do. Thatâ€™s not freedom.\n\n3. I personally use my PC for many other things than just AI, and have no interest in renting one from Jeff Bezos.\n\n\nYou put too much faith in things outside your control.",
          "score": 2,
          "created_utc": "2026-01-28 17:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o298hw1",
          "author": "IulianHI",
          "text": "One thing not mentioned: running local forces you to actually understand what's happening under the hood. When you tweak quantization settings or swap backends, you learn way more about these models than just hitting an API ever will. That knowledge pays dividends when you actually need to debug or optimize something serious.",
          "score": 2,
          "created_utc": "2026-01-28 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29cr1t",
          "author": "Jack2102",
          "text": "The urge for these companies to die",
          "score": 2,
          "created_utc": "2026-01-28 18:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29k9aa",
          "author": "Ke0",
          "text": "I mean privacy is a pretty big deal no?",
          "score": 2,
          "created_utc": "2026-01-28 18:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lhwj",
          "author": "OlivencaENossa",
          "text": "all those ais in the cloud are going to get censored one day.",
          "score": 2,
          "created_utc": "2026-01-28 18:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29p5uc",
          "author": "Impressive_Banana543",
          "text": "Privacy before all.\n\nThe next argument is speculative: I think that current pricing is not sustainable and the purpose is to increase user base. Once the penetration in user workflows will be high enough, the prices will rise and the availability of free models sharply reduced.",
          "score": 2,
          "created_utc": "2026-01-28 19:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rnmp",
          "author": "Geminatorr",
          "text": "Owning the means of production (production of tokens) is how you win capitalism.\n\n\nRentoids get exploited in the long run",
          "score": 2,
          "created_utc": "2026-01-28 19:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rp21",
          "author": "Purple-Programmer-7",
          "text": "1. Privacy\n2. Sensitive data (not the same as privacy)\n3. API costs WILL change",
          "score": 2,
          "created_utc": "2026-01-28 19:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rq8m",
          "author": "SkiBikeDad",
          "text": "For me I'm using frontier models for code and design (via cli agents and chat), but 24/7 use cases like NVR and low-latency use cases like tab-completion are better local. Another niche use case: generating icons and other product images, where I want to output 1000s of iterations to whittle down the right input prompt strategy and test lots of seeds before hand-refining.",
          "score": 2,
          "created_utc": "2026-01-28 19:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ujef",
          "author": "Same-Platform-9793",
          "text": "Its for resilience and times of war scenarios",
          "score": 2,
          "created_utc": "2026-01-28 19:37:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29xh1g",
          "author": "caetydid",
          "text": "\\- learning about stuff (it is fun)\n\n\\- persistence (as in future reproducibility and little maintenance)\n\n\\- autonomy (no dependency of reliability of external services)",
          "score": 2,
          "created_utc": "2026-01-28 19:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29z8cu",
          "author": "ortegaalfredo",
          "text": "1. Privacy is a huge one. Everything you write is shared with the private companies and the government and they build a profile of you. That's why Palantir is able to know so much about you, your fears, your psychological profile, they can massively influence population with a database like this. If you don't care about things like this, its ok\n2. What I think is important is artificial limitations on the model. I'm not talking about silly stuff like porn and WMD, but things like cyber, education, you don't know if the model is lowering performance in some areas, or injecting subtle manipulations on you or your children.\n3. Also, the limitations are not only problematic, but the fact that they change. They add and remove stuff from LLMs all the time and if you business depends on it, and suddenly your agent stop working, then bad luck. You cannot go back to the older model.\n\nThis don't happen if you host local. Remember, if the tokens are free, then you are the product.",
          "score": 2,
          "created_utc": "2026-01-28 19:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29zrsf",
          "author": "w8cycle",
          "text": "Learning, control, privacy, experiments, and cost is still a huge factor for me because I am on a very tight budget.",
          "score": 2,
          "created_utc": "2026-01-28 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a00de",
          "author": "IulianHI",
          "text": "Another angle nobody mentions: experiment control. With local you can mess around with system prompts, try different quantization levels, and actually understand how the model behaves. APIs give you this nice packaged experience but you're at the mercy of whatever defaults they set. Sometimes that \"15 tok/s\" local run with a quantized model gives you better results for your specific use case than the shiny hosted version with perfect throughput.",
          "score": 2,
          "created_utc": "2026-01-28 20:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3iq2",
          "author": "mambo_cosmo_",
          "text": "the hardware is free because I bought a gaming PC to play games and now I can run local LLMs to do useful stuff too",
          "score": 2,
          "created_utc": "2026-01-28 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b33xf",
          "author": "__Captain_Autismo__",
          "text": "Reliability for local is unsurpassed. No black box",
          "score": 2,
          "created_utc": "2026-01-28 22:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e6q7h",
          "author": "Thrumpwart",
          "text": "TWO CHATS AT THE SAME TIME.\n\nLM Studio new feature is awesome.",
          "score": 2,
          "created_utc": "2026-01-29 11:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272ci4",
          "author": "Foreign-Collar8845",
          "text": "It is market entrapment. You drop prices until you kill the competition (local in this case) then you charge.",
          "score": 5,
          "created_utc": "2026-01-28 11:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bzse",
          "author": "Murder_Teddy_Bear",
          "text": "Porn. I like making porn, ok? Make porn in SD Forge, bring it into LTX-2, bam, animated porn.\n\nPorn.",
          "score": 4,
          "created_utc": "2026-01-28 15:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29id6t",
              "author": "GeneralWoundwort",
              "text": "The only honest person in this thread haha.Â ",
              "score": 3,
              "created_utc": "2026-01-28 18:43:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o274hr0",
          "author": "Full-Bag-3253",
          "text": "Enshitification is the standard business model now.  Netflix was great, but now every year they make it worse unless you pay more.",
          "score": 2,
          "created_utc": "2026-01-28 11:39:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272908",
          "author": "Ruin-Capable",
          "text": "Gemini free tier is not massive.  I blew through a months quota in 5 minutes with opencode",
          "score": 1,
          "created_utc": "2026-01-28 11:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o274mzy",
          "author": "lionelum",
          "text": "Well, learning is a very good reason to run it locally, no need to said more =) . Another is fine tunning, running locally you could training a model on an specific subject that is not so common.   \n  \nIf you already have the hardware (ie. You already have a hardcore gaming PC, or and not to old crypto farming equipment)  points 2 and 3 are debatable.  More on countries where electricity is cheap but change currency is a mess.",
          "score": 1,
          "created_utc": "2026-01-28 11:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2755rs",
          "author": "JLeonsarmiento",
          "text": "Wicked models is mostly my justification now.",
          "score": 1,
          "created_utc": "2026-01-28 11:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275y8y",
          "author": "ShinyAnkleBalls",
          "text": "What do you mean beyond privacy. It's like saying. Beyond having a billion dollars, what is the benefit of winning the lottery?",
          "score": 1,
          "created_utc": "2026-01-28 11:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o276r2m",
          "author": "ThaDon",
          "text": "4. Itâ€™s fun.\n5. You learn a lot.",
          "score": 1,
          "created_utc": "2026-01-28 11:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ahnb",
          "author": "Sicarius_The_First",
          "text": "Valid points.\n\nTbh, use both, local and none local.\n\nFor me, the reason to use local models is for creative stuff. I want fallout / Morrowind adventure with the vibe right, and with specific format following & capabilities.\n\nNo LLM can do this, so I made one that can.\n\nIn other words, local models can outperform in a specific scope / niche.",
          "score": 1,
          "created_utc": "2026-01-28 12:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27d902",
          "author": "Ne00n",
          "text": "Wrong, the is a rate limit, its the memory bandwidth.",
          "score": 1,
          "created_utc": "2026-01-28 12:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ejzl",
          "author": "xmBQWugdxjaA",
          "text": "One issue is you can't guarantee latency nor quantisation with a lot of inference providers. It's actually crazy how ropey this still is. For a lot of usage that won't matter too much, but for production it really limits you to a small number of big providers (Groq, Cerebras, etc.)\n\nLikewise if for your specific use case you are able to distill to something that can run locally, then it's still definitely worth it (in some cases you could even run on mobile etc. - e.g. if you train a BERT style classifier based on distilled data).\n\nBut for general usage like we see with Clawdbot and OpenCode, you are absolutely right!",
          "score": 1,
          "created_utc": "2026-01-28 12:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27f81v",
          "author": "-dysangel-",
          "text": "Being able to run offline is nice. I always have an AI I can chat to even if the internet or service provider are down. I'm not a huge prepper, but there is a part of me that wants to be ready for an emergency. The lockdowns during covid showed that sometimes weird things are just going to happen and you can't stop it. Being able to run frontier level AI at home or on the go is pretty awesome.\n\nI like the idea of being able to have things like a super powered Alexa-like home assistant. I've not got around to this yet, as I guess it isn't really a true pain point. I keep thinking Google or Amazon will up their game too and release something awesome. But they still haven't, so I might get around to it eventually.",
          "score": 1,
          "created_utc": "2026-01-28 12:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27gcde",
          "author": "scousi",
          "text": "Running local is still more expensive and always will be.  But It's a hobby for many and they justify the costs on the basis of other reasons. It's like Uber vs your own car. Uber was cheap but no more but probably still cheaper than buying a car but with trade-offs.\n\nHard to say how long China will give their stuff away. They opened source in the beginning (brilliant move) because no one would dare even using their models unless it was free or hosted outside China. Now people are willing to pay for their services. They've achieved recognition. Open source is a bit aligned with socialism. Maybe they are doing it for that reason. Who knows if there's a coordinated state level strategy. What the west doesn't grasp is that the competition in China amongst themselves is also pretty crazy and they are also trying to outdo each other. \n\nThere does seem to be a disconnect by how much China can achieve with what seems a lot less dollars vs the US.",
          "score": 1,
          "created_utc": "2026-01-28 13:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hgoo",
          "author": "Thump604",
          "text": "Trust: I donâ€™t trust \nControl: cloud providers are throttling quality, reliability and context as use the customers to generate more costs and test their features on our dime.  \nLearning: Itâ€™s fun and you learn a lot that you can apply to career or just a hobby.  \nFuture: Change will happen",
          "score": 1,
          "created_utc": "2026-01-28 13:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hmvn",
          "author": "ross_st",
          "text": "I think the freefall in API pricing isn't sustainable.\n\nIt's a desperate race to the bottom to onboard customers, a loss leader.\n\nGemini is subsidised by the rest of Google's business. GPT and Anthropic are subsidised by generous VC runways. DeepSeek is subsidised by hedge fund profits.\n\nBut if they do see the adoption levels they are hoping to see, then they won't be able to afford to do that anymore.",
          "score": 1,
          "created_utc": "2026-01-28 13:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27i2d8",
          "author": "Dany0",
          "text": "My company pays a huge amount for aws bedrock. The best models, Opus 4.5, Sonnet 4.5\n\nEveryone uses it to some extent. But guess what? It's down or not working all the time. Responses timeout, or as you mentioned we get rate limited\n\nLocally I only do toy stuff with it, but the day-to-day UX experience is SO much better despite the huge time & effort upfront cost",
          "score": 1,
          "created_utc": "2026-01-28 13:11:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27is3p",
          "author": "usernameplshere",
          "text": "The API costs are highly substituted. The actual costs are way, way higher than what we pay now. It is a little future-proofing, but also to not be able to keep working when another cloudflare accident or whatever happens, not even mentioning the privacy concerns. For models like GPT OSS 120B in full precision you \"only\" need a consumer graphics card and 96GB+ of fast RAM. Just 6 months ago, this was fairly reasonable cost for a decent model. But with the hardware prices now? It's way less accessible.",
          "score": 1,
          "created_utc": "2026-01-28 13:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jkf5",
          "author": "sampdoria_supporter",
          "text": "Part of my calculation is fully owning my process end to end and not feeling bad about burning tons of tokens on testing. Also - I'd argue on #3 that configuration with VLLM has never been easier and it's crazy how hard you can push 3090s. Not disagreeing with your overall point though, I likely would have never bought hardware if it was as cheap and performant as now",
          "score": 1,
          "created_utc": "2026-01-28 13:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27l1sk",
          "author": "MaruluVR",
          "text": "Learning and the fun of setting it up.\n\nI train custom image gen models for game dev so finetuning/lora is the big part for me.",
          "score": 1,
          "created_utc": "2026-01-28 13:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27m6dr",
          "author": "IulianHI",
          "text": "Another angle: tooling integration. With local models, you can hook them directly into your systems without API overhead or limitations. For long-running agents, batch processing, or workflows that need tight coupling with local resources (databases, files, etc.), the flexibility is unbeatable. Sometimes it's not just about cost - it's about architectural freedom.",
          "score": 1,
          "created_utc": "2026-01-28 13:34:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27o7qu",
          "author": "Innomen",
          "text": "Privacy is the only argument. The rest is cope and motivated reasoning. Edit: Well, of course, uncensored is basically private only, but mentally I dump it in the same bucket. Hiding for security and hiding for subversion are both hiding.",
          "score": 1,
          "created_utc": "2026-01-28 13:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27oqzg",
          "author": "hydropix",
          "text": "I crunched the numbers on Owning vs. Renting (RunPod), and unless you're hitting 6+ hours of daily heavy usage, renting wins every time. Plus, the flexibility to spin up high-end clusters for training is a huge advantage.\n\nI also doubt we'll see a repeat of the 'local GPU' era. Since inference isn't that latency-sensitive for most users, the cloud offers better resource efficiency. Weâ€™re likely looking at a cloud-dominated future rather than mass adoption of high-end local hardware (except, of course, for enthusiasts like us here who want to dive deep and push this tech to its limits).",
          "score": 1,
          "created_utc": "2026-01-28 13:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27owxc",
          "author": "DifferenceMuch1122",
          "text": "Yes",
          "score": 1,
          "created_utc": "2026-01-28 13:48:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ozcl",
          "author": "DifferenceMuch1122",
          "text": "Perfect",
          "score": 1,
          "created_utc": "2026-01-28 13:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q4qe",
          "author": "d41_fpflabs",
          "text": "Privacy 100% is the only real reason. But it will arguably become the most important thing because AI is only ever going to become more intertwined with most peoples personal lives and at a certain point it would probably become a barrier to entry even for people who aren't the most privacy-conscious. People reaction to Microsofts recent antics is an example of this already happening.\n\nI personally feel like we are going to start seeing more smart devices being built with this in mind. The \"MacMini Claudbot Boom\" kind of highlights the potential of portable private AI-compatible smart devices.",
          "score": 1,
          "created_utc": "2026-01-28 13:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27rje7",
          "author": "StardockEngineer",
          "text": "My only use case ever was that itâ€™s fun and I love knowing all about it.  Professionally, that turns out to be supremely useful, too.",
          "score": 1,
          "created_utc": "2026-01-28 14:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27sixi",
          "author": "Bit_Poet",
          "text": "Local gives you consistent quality. This week, I've repeatedly had dumbed down output and overload refusals from some of the biggest providers with their most expensive models. Unless you're an upper tier customer, you're just an important little bug in their big wheels.\n\nLatency sure is a huge issue. I run complex workflows that have dozens or more consecutive api calls. Doing that over the web sucks, and most of the workflows can't be parallelized.",
          "score": 1,
          "created_utc": "2026-01-28 14:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27taue",
          "author": "nat2r",
          "text": "Price isn't the reason. It's far more expensive to obtain the local hardware. \n\nPeople do it for security and novelty really. Can't trust these big companies.",
          "score": 1,
          "created_utc": "2026-01-28 14:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u5bl",
          "author": "OutsideProperty382",
          "text": "Google's free tier got nerfed last I remember. bad.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yahk",
          "author": "AgreeableCaptain1372",
          "text": "Control over results. Using third-party APIs I get a lot of variance in my evals vs self hosted. \n\nalso prices are low for standard models but not for fine tuned models. So if you need fine tuned LLMs, especially at scale, self hosting or local can be worth it financially",
          "score": 1,
          "created_utc": "2026-01-28 14:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o288lwj",
          "author": "CH3CH2OH_toxic",
          "text": "Gemini has a massive free tier when ? where ?",
          "score": 1,
          "created_utc": "2026-01-28 15:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28e4ch",
          "author": "FitAstronomer5016",
          "text": "Some points have been made already that I feel already encapsulate it, but it falls under a few things\n\n1. API Pricing/Chat subscriptions are subsidized, not only for us the customers but also the companies (Claude has quite a bit of 'free' compute from AWS and I'm sure it translates over to the other large providers). While they are profitable somewhat on paper, once that allocation runs out, you will see an increase in price.\n\n2. Business processes will feel the brunt of that, and like how some companies migrate to local DBs/own server stack, AI falls under the same category\n\nNow granted, running local is not as lucrative and at this point is becoming much more of a luxury with the increase of hardware costs, and more importantly, power costs (especially planned price hikes). The control is still appealing and will continue to grow, although probably with not the same hardware requirements as we have now. If we get more efficient running models, better cost-effective hardware like dedicated TPUs/APUs for AI inference, it would become akin to an SSD almost.",
          "score": 1,
          "created_utc": "2026-01-28 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28hs8b",
          "author": "Icy_Foundation3534",
          "text": "compliance, and also the private equity-fication scenario where your opus sub is $3000 a month a few years from now because it's like hiring a real 100x super dev.",
          "score": 1,
          "created_utc": "2026-01-28 16:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sju2",
          "author": "Bbmin7b5",
          "text": "beyond privacy, not much. but Privacy is THE most important part.",
          "score": 1,
          "created_utc": "2026-01-28 16:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28v684",
          "author": "Irisi11111",
          "text": "Indexing is the most consequential use case I can think of for local AI. I also hope browser use and vision-centric document retrieval will be the next focus.",
          "score": 1,
          "created_utc": "2026-01-28 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29006x",
          "author": "Rich_Artist_8327",
          "text": "You can also then ask what is the usecase for providing almost free APIs while every request consumes huge amount of energy.",
          "score": 1,
          "created_utc": "2026-01-28 17:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o292off",
          "author": "Maddog0057",
          "text": "Same price, you're just not paying in money, you're paying in data.",
          "score": 1,
          "created_utc": "2026-01-28 17:36:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o295d51",
          "author": "Glad-Audience9131",
          "text": "I checked today.. is $19 monthly for me.. what's going on???",
          "score": 1,
          "created_utc": "2026-01-28 17:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29qift",
          "author": "GrayDonkey",
          "text": "Been getting Gemini is at capacity errors all morning....\n\nIf not that it's, Gemini 3 is busy, answering with 2.5. Followed by Gemini 2.5 is busy, answering with 2.0. Followed by some not great output.",
          "score": 1,
          "created_utc": "2026-01-28 19:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9e8h",
          "author": "Torodaddy",
          "text": "You dont need the insane rigs you see to run a local model, the amd based miniforum devices work fine for inference and in that case i can have agents working 24/7. Even the cheapest api starts getting expensive using it like that",
          "score": 1,
          "created_utc": "2026-01-28 20:43:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9hyi",
          "author": "Michaeli_Starky",
          "text": "Local ones were always vastly inferior from the cost perspective, speed perspective and overall performance.",
          "score": 1,
          "created_utc": "2026-01-28 20:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aa609",
          "author": "ErokOverflow",
          "text": "Listen to this: Good taste in programming and image creation is not always coming from wealthy people who can buy a good hardware configuration.\nHigh prices.\nThis IS the real justification.",
          "score": 1,
          "created_utc": "2026-01-28 20:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ae5xs",
          "author": "57hz",
          "text": "Also, you can rent servers (by month or by second) to run local models. So thereâ€™s the benefit of not being tied to hardware while having privacy and consistency.",
          "score": 1,
          "created_utc": "2026-01-28 21:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ao9fx",
          "author": "ZakOzbourne",
          "text": "Because no company wants to generate the stuff I want it to generate, I need uncensored unhinged models",
          "score": 1,
          "created_utc": "2026-01-28 21:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2augoh",
          "author": "LocalLLMHobbyist",
          "text": "One thing I'd add from the budget side: the math works differently when you're not buying new.\n\nI grabbed a used 3090 for $750. That's 24GB VRAM â€” enough to run 70B models quantized. At current API rates, sure, that's a lot of tokens. But I'm not optimizing for tokens-per-dollar.\n\nI'm optimizing for:\n\n\\- Zero friction experimentation (no rate limits, no \"please try again later\")\n\n\\- Models that don't refuse half my prompts\n\n\\- Learning how this stuff actually works\n\n\\- Something that works when my internet doesn't\n\nThe \"millions of tokens to break even\" math assumes you're just chatting. When you're building agents, running batch jobs, or just tinkering for hours â€” local stops feeling expensive real fast.\n\nPlus honestly? It's fun. Not everything needs to be pure ROI.",
          "score": 1,
          "created_utc": "2026-01-28 22:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ayj1i",
          "author": "Plopdopdoop",
          "text": "Is Gemini back to having a massive free tierâ€¦aside from the 2.5 flash-lite variant?\n\nLast I checked they removed essentially all the free access but that flash lite model.",
          "score": 1,
          "created_utc": "2026-01-28 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bbcod",
          "author": "Outrageous-Tonight75",
          "text": "I think it is similar to having a Nas instead of paying Netflix. A mix between privacy, control and the \"DiY feeling\" that makes using it special",
          "score": 1,
          "created_utc": "2026-01-28 23:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bfnqm",
          "author": "SnooBananas5215",
          "text": "Maybe for the power user for dogshit coder like me offline models don't really work, instead I focus on creating simple UI with lots of automation and flexibility not built for scale though, most of the time I am not working on any heavy stuff anyways and I think just automating lots of stuff that small and medium businesses rely on for version control, qms and authorized form completion is all they need",
          "score": 1,
          "created_utc": "2026-01-29 00:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bryco",
          "author": "flywind008",
          "text": "you pay for your privacy, thatâ€™s fare enough",
          "score": 1,
          "created_utc": "2026-01-29 01:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bulco",
          "author": "deparko",
          "text": "Well, I've been dealing with the same issue and have concluded a hybrid approach works best. I use a three-tier model: an offline small LLM (Ollama) on my local 5070 TI GPU for local tasks; Ollama Cloud as tier two for bulk processing, where I can use Kimi and Deepseek..etc for a flat rate (about $20 a month, $240 a year), which is much cheaper than upgrading my GPU; and frontier models for deep reasoning when needed.\n\nI've designed my RAG and AI-native apps to operate within that three-tier framework.",
          "score": 1,
          "created_utc": "2026-01-29 01:20:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bwoqw",
          "author": "Vicar_of_Wibbly",
          "text": "The question is inverted. Why would I use the cloud? Itâ€™s slower, has no meaningful quality improvement over my local setup, imposes restrictions on use, and can change without notice or approval.\n\nMy local system is standardized, fully under my control, is backed up properly and - as you very importantly point out - is private.\n\nThe cloud has no compelling use case for me whatsoever.",
          "score": 1,
          "created_utc": "2026-01-29 01:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c00g2",
          "author": "mystery_biscotti",
          "text": "For funsies. \n\nAlso, i'm learning how to do tiny infra on a micro scale, so I'll be in a better spot to become employed keeping AI upright. \n\nPlus if certain US states are banning specific roles AI fills, local will be the only way to go. (Tennessee, I'm looking your way.)",
          "score": 1,
          "created_utc": "2026-01-29 01:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c0o3s",
          "author": "Kos187",
          "text": "You can spent 50M of input token over the weekend with Claude Code easily. It's easy to justify local LLM on a hardware you use for something else as well,  like gaming... But after some amount of VRAM it makes no sense. Especially when SLI is dead.",
          "score": 1,
          "created_utc": "2026-01-29 01:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2px8",
          "author": "celsowm",
          "text": "4. fine tuned models",
          "score": 1,
          "created_utc": "2026-01-29 02:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbls2",
          "author": "Far-Low-4705",
          "text": "It's fun",
          "score": 1,
          "created_utc": "2026-01-29 02:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cdwao",
          "author": "lgdsf",
          "text": "Run jailbroken models for sure",
          "score": 1,
          "created_utc": "2026-01-29 03:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cetv8",
          "author": "Adventurous_Push6483",
          "text": "There are two differences I see: Some people use locally hosted models for personal use, and some people need to bootstrap ML to a product.\n\nIn terms of product:  \n  \n1. First of all, all the models available over the API have extremely high safety railguards. This affects some of my experiments (which can be product tests), which even though I am not generating NSFW content with it, I have found the censored models performing far worse in some of the specific personalization tasks I test with (they strangely tend to be less creative? I don't have any formal benchmarks for this so doubt it).  \n  \n2. Self-hosting is also much more \"safe\" in a sense that there is floor of what you can lose. If you build a public facing demo application of a product and didn't bother to secure it yet since it is an early stage PoC, you won't  run into strange issues with insecure rate spamming (exploding API costs) and whatnot. Yes, this is terrible practice but sometimes I just want to share my application with peers and its a lot easier to throw a Streamlit site over LAN and worry not about security (at worst, the app just crashes).\n\n3. I do mostly use the Gemini API, but the rate limits are certainly an issue with the free tier. The better alternative is just to use the paid tier if you have the money, to which you can get a surprising amount of research/experimental work done with just $10 worth of credit.\n\n4. Technically speaking, it is \"free\" for me. I just use my group's server (p beefy hardware) to locally host when I need so I'm not actually paying anything for the hardware. If the place you work at (or your PC) just happens to have compute for training/HPC work, you might as well use it since its already there.\n\n5. Millions of tokens is not as hard to hit as you think. Especially if you work with image data to VLM, which can easily cost thousands of tokens. I work with massive amounts of data in many media formats, data processing with LLM API is very expensive so the breaking-in-even is not a very compelling argument (this is still ran through the API, however, expect big bills far greater than GPUs). \n\nI think the nicer thing about the API is just sheerly how easy it is to setup. Buying hardware takes so much time when I can just rent it out on the cloud OR just use some money and use the API (download API key => download package => Use AI).\n\nThe more interesting argument has always been local GPU vs cloud GPU I think. APIs are just so limited to their scope and what they can do, but they are so convenient that there isn't a good reason to not use them if you have funding/VC money (with exception if you need something that's more specific than a generic language model, which is not many applications).",
          "score": 1,
          "created_utc": "2026-01-29 03:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cfpr5",
          "author": "Spanky2k",
          "text": "Privacy is everything. We use a local model when working on business sensitive stuff although our model choices are currently somewhat limited (Mac Studio 64GB). I'm hoping the eventual M5 or M6 Ultra based Mac Studios will have improved prompt processing enough so that it's possible to host a good sized (say 200-300B) model with multi user access (5-10 total users but realistically no more than 1 or 2 ever submitting queries at the same time) with reasonable performance. Something like that for about Â£10k would be perfect for a truly local and data secure system.\n\nBut lately I've been playing around loads with Qwen Image Edit and Hunyuan Video in ComfyUI on my 5090. I've been having a blast feeding them family photos and reimagining them in different styles, changing outfits, animating old family photos. I wouldn't have ever felt comfortable uploading that stuff to a cloud based service.",
          "score": 1,
          "created_utc": "2026-01-29 03:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cij7o",
          "author": "Chilidawg",
          "text": "There's the principle of ownership over renting. I know we're losing that battle a little more every day, but still...\n\nNo matter how cheap something is, there's an immeasurable gap between \"free\" and \"paid\". I understand that my electricity bill is likely more expensive than the API. However, that's grouped with utilities that are already expensive before running local models.\n\nThere's the novelty of verbally talking to your computer. There's no black box API promising you're talking to a model as opposed to someone in New Delhi. You can run the script and listen to your GPU fans speed up.\n\nFinally, I also don't pay Sam or Elon on principle.",
          "score": 1,
          "created_utc": "2026-01-29 03:33:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2clnqb",
          "author": "x0xxin",
          "text": "It's also a super fun and practical hobby. I've learned so much about self hosting and \"cloud tech\" via all of my labbing.",
          "score": 1,
          "created_utc": "2026-01-29 03:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d1fwr",
          "author": "Vivarevo",
          "text": "Pricing is marketing, it will change on short notice",
          "score": 1,
          "created_utc": "2026-01-29 05:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4g2r",
          "author": "Big_River_",
          "text": "all these prognosticators claiming models are commodities are investor bros who have a very confident surface level understanding of technology and macroeconomics - applying traditional business school analytics on technology adoption and artificial intelligence as a service like sas on steroids margins or corporations are going to just train their own model and cut vendor and labor cost - just like implementing a new management information system with autonomous agents as admins and analysts and developers...on their own local data center that the model manages itself with human in the loop hardware maintenance until robotics improves fine motor skills ...and all of this is a 9 month implementation that will become relatively plug and play fine tuned specialized model that can rewrite and optimize the code base, product firmware, optimize manufacturing processes, define the kpi that drive results, write up the marketing strategy and product design, 24/7 live dashboards on every deep dive imaginable for the c-suite goons to...nevermind all along frontier labs are racing with govt labs to produce the singular super intelligence that unlocks unimaginable world model exploits that change our fundamental understanding of what intelligence is capablr\n\nopen source + home lab represents the counterweight to corporate cloud \"all your base (models) are belong to us ( and your data too )\" and you can rent them for cheap now but once local compute becomes too expensive for joe six pack to build out in his garage - the cloud becomes the only game in town to get that coevo level up intelligence and truly generative creative extension of minds - there is a fundamental divide between the compute you own and the compute you rent - your data, your painfully fine tuned model that all your agents and business/creative / personal growth process depends upon...lease your work forever to market whims...cloud compute prices are so low to hook as many as they can while local compute components are skyrocketing - say goodbye to the capable personal computer - the next commercial electronics is cloud optimized priority connectivity for proprietary cloud ai managed computing resources - the user will have a great experience but it will always be renting the ability to functionally participate in the economy - when one org takes the super intelligence to monopolize compute as everyday iphone level (comprehensive information ingress paid promo filter and algorithm ranked content engagement device) service is vertically integrated.....well the individuals who sit on the board and the majority shareholders of this company that owns an overwhelming majority of compute ascend to a level of control and influence on the learning and reasoning of humanity that reduces their impact on the collective narrative to NPCs going through time in limited awareness incurious and relatively happy and cared for like a collection of objects or pets or puppets that are fun to play with until they break or shoot each other dead in the streets to amuse or shock....\n\nlocal compute ai clusters and distributed data owned by and maintained for the benefit of small communities and collectives is the only counterbalance to the commercial interests of capital markets and investors demand for unsustainable reality bending returns that own and control the narrative for some fucking reason in this shard of existence \n\nget local stacked functional compute or you are not playing the game -- you are getting played - get hooked on the cloud",
          "score": 1,
          "created_utc": "2026-01-29 06:00:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4igb",
          "author": "Which-Jello9157",
          "text": "same question here. replies above are convincing but realistically im sticking with cloud apis for now since its easy and cheap. models keep getting bigger and whatever hardware u buy today probably wont handle next gen stuff anyway. do you have any recommended third-party api provider without rpm limit?",
          "score": 1,
          "created_utc": "2026-01-29 06:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f8gy8",
              "author": "Man-Of-Culture-0690",
              "text": "[Openrouter.ai](https://openrouter.ai/) is the obvious choice for model variety since they aggregate from multiple providers but sometimes u get rate limited even on paid tier. pro tip tho, OR supports byok (bring your own key) so u can route to whichever upstream is cheapest for that specific model. i use [atlascloud.ai](https://www.atlascloud.ai/) for kimi k2.5 since theyre the cheapest and stable, and phala for glm 4.7 flash cuz they're stable on it. Way cheaper if ur running volume",
              "score": 1,
              "created_utc": "2026-01-29 15:13:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d7ig4",
          "author": "TruckAmbitious3049",
          "text": "For me it's rate limits. \n\nFor data analysis, I need to do a lot of labeling. Paid Gemini and ChatGPT would hit rate limits. \n\nFor transcribing, sonoix is amazing and cheap. But if it's a large batch, then Whisper still better.",
          "score": 1,
          "created_utc": "2026-01-29 06:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2damya",
          "author": "Photochromism",
          "text": "Lol. Dumb take. The minute one of these platforms becomes the favorite, becomes a necessity, and wipes out the others, the prices will a quadruple.",
          "score": 1,
          "created_utc": "2026-01-29 06:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dd8xe",
          "author": "AlwaysLateToThaParty",
          "text": "> What's the actual case for running local now beyond privacy? \n\nHaving a 96GB GPU with 1.7TB/s memory bandwidth for gaming.  It's quite good.",
          "score": 1,
          "created_utc": "2026-01-29 07:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgc8i",
          "author": "SlimPerceptions",
          "text": "Even on point #1 - encrypted files and rented gpuâ€™s? I havenâ€™t done it but it seems like their are adequate privacy solutions out there even for the cloud",
          "score": 1,
          "created_utc": "2026-01-29 07:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpwvu",
          "author": "Maximum-Wishbone5616",
          "text": "In my experience the SLA is non-existent for most of AI services.\n\nWe require at least 99.99% (that is uber minimum), we monitor all our servers, instances and services every 30 seconds, we aim to get at least 99.9999% through out them.\n\nDue to sheer amount of requests we process per minute, 99.9% would mean that 3rd party AI 503 would cause havoc for our customers/monitoring systems.\n\nThey are not even close to 95% with 30 second monitoring...",
          "score": 1,
          "created_utc": "2026-01-29 09:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2esj8o",
          "author": "HopefulMaximum0",
          "text": "The API prices are low because they decided to operate at a loss until they win.   Then they will crank the price to the moon and do whatever suits their current whim to you, because you can't go elsewhere.\n\n  \nThe plan is simple, and as old as anti-dumping laws.",
          "score": 1,
          "created_utc": "2026-01-29 13:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fuuyk",
          "author": "Vahn84",
          "text": "I simply do not want a future where Iâ€™m not in control of my things. Corporations are forcing us into a future where we probably wonâ€™t have shit into our handsâ€¦and we will be forced to rent everything. I donâ€™t want a life hooked to a multitude of subscription services. Prices are going down nowâ€¦probably because the bubble is going to explode. But nothing can assure you that they will remain like this forever. I bet they wonâ€™t",
          "score": 1,
          "created_utc": "2026-01-29 16:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ucdm",
          "author": "bgiesing",
          "text": "Because it's still more expensive, my PC is already on 24/7 so it would be using about the same electricity regardless and I can use that GPU for many things (games, video editing, etc.), it already was paid off years ago. That's still cheaper than having to drop like $10-20 a month on API calls or a subscription service.\n\nAlso, many people explicitly chose local because they want to make content that the cloud models from the big companies refuse, API cost doesn't matter if every single reply you get is \"I can't fulfill that request\", you literally don't have an option",
          "score": 1,
          "created_utc": "2026-01-28 10:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xjdy",
          "author": "k_means_clusterfuck",
          "text": "not a deciding factor for me but i do think its nice to to know my carbon footprint",
          "score": 1,
          "created_utc": "2026-01-28 10:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xq3m",
          "author": "SpicyWangz",
          "text": "Ah yes, the freefall. LLM companies donâ€™t even know whatâ€™s happening. Every time they look at their own pricing the numbers are lower. Soon enough Iâ€™m sure theyâ€™ll be paying you to use their API",
          "score": 1,
          "created_utc": "2026-01-28 10:44:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zos4",
          "author": "tcoder7",
          "text": "When you use AI API you give away your intellectual property. The model scans your repo and sends everything to a remote server. Open the output tab in VSCode and watch.",
          "score": 1,
          "created_utc": "2026-01-28 11:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275pm1",
          "author": "FastDecode1",
          "text": "Computer hardware: $$$\n\nNot going to prison for asking forbidden questions: priceless.",
          "score": 1,
          "created_utc": "2026-01-28 11:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28nm9d",
          "author": "slindshady",
          "text": "Everything is sensitive data. Thatâ€™s the point. Look what happens with one â€žbadâ€œ election.",
          "score": 1,
          "created_utc": "2026-01-28 16:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h6zy",
          "author": "zipperlein",
          "text": "VC will dry out eventually.",
          "score": 0,
          "created_utc": "2026-01-28 13:06:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27t3mp",
          "author": "devinprocess",
          "text": "Gonna be real with you.\n\nActual case: you are in the 1% of the rich or lucky folks here who have accumulated enough hardware and have no issues running a power guzzling setup.\n\nFor majority of us normal bees, api or renting is still the way because the local models we can run are just for shits and giggles, and who cares about all those arguments at that stage. \n\nUnless local llms become affordable itâ€™s just a circlejerk.",
          "score": 0,
          "created_utc": "2026-01-28 14:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28pyic",
          "author": "DataGOGO",
          "text": "The bots are going to rage on me for this one; but not putting all my data in the hands of the Chinese government.Â \n\nI will run local or use a US based provider subject to US / EU data protection laws.\n\n\nEvery Chinese provider is very heavily subsidized by the Chinese government (and that is just what they openly admit to, the true extent is unknown).Â \n\nThe entire business model is to undercut US / EU companies to the point of making the AI business unsustainable, thus giving China AI dominance.Â They know that in order for OpenAI, xAI, Meta, Google, Microsoft, etc. to stay in the AI business, they eventually have to turn a profit.\n\nBy releasing models into open source, and providing extremely cheap API access the goal is to make turning a profit impossible.\n\nThat is very bad for everyone, as it just turns AI into Chinese propaganda machines and data collection tools.Â \n\nTo be clear, I am not throwing shade on the Chinese developers, engineers, and data scientists at all, just the government framework they are forced to operate in.Â ",
          "score": 0,
          "created_utc": "2026-01-28 16:41:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo349m",
      "title": "deepseek-ai/DeepSeek-OCR-2 Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-OCR-2",
      "author": "Dark_Fire_12",
      "created_utc": "2026-01-27 03:56:49",
      "score": 334,
      "num_comments": 41,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o1yiqke",
          "author": "foldl-li",
          "text": "They even thanked themself!\n\nhttps://preview.redd.it/t34eyddujtfg1.png?width=1037&format=png&auto=webp&s=7508bb6586dfb7327311dfddb2f108f459ccef2f",
          "score": 162,
          "created_utc": "2026-01-27 04:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yktdl",
              "author": "TheRealMasonMac",
              "text": "https://preview.redd.it/k8ykrq19mtfg1.png?width=286&format=png&auto=webp&s=9584d025699644e92331e6d5ff221b5c14ef68ba",
              "score": 132,
              "created_utc": "2026-01-27 04:37:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yiw2t",
              "author": "Dark_Fire_12",
              "text": "lol that made me laugh.",
              "score": 24,
              "created_utc": "2026-01-27 04:25:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1zdvfd",
              "author": "No_Afternoon_4260",
              "text": "May be not the same teal",
              "score": 3,
              "created_utc": "2026-01-27 08:27:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ykn7u",
          "author": "foldl-li",
          "text": "I always use scores reported by A to evaluate model B/C/D. So, in this case, PaddleOCR-VL looks really awesome.\n\nhttps://preview.redd.it/trymwuqoltfg1.png?width=1130&format=png&auto=webp&s=9b4a33243260da38c103d681c1ad5bdc8d5f9156",
          "score": 43,
          "created_utc": "2026-01-27 04:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yusec",
              "author": "linkillion",
              "text": "I mean, that's not really DS benchmarking the other model, it's just a general benchmark.Â \n\n\nThat said, paddleocr is great but it's a PITA to get working to this level, it requires their pipeline which I honestly gave up on very quickly. MistralOCR, although closed source, is so far ahead it's not even close in my opinion. For my use case all the docs I use are public, so I use MistralOCR exclusively.Â ",
              "score": 16,
              "created_utc": "2026-01-27 05:47:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yy48f",
                  "author": "zball_",
                  "text": "I use Gemini 3 flash as OCR and it was phenomenal.",
                  "score": 12,
                  "created_utc": "2026-01-27 06:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yxi6m",
                  "author": "skinnyjoints",
                  "text": "I have been sleeping on mistral for a while now. Why do you consider it to be the best? And is it the best among OCR specific models or does it compete with multimodal LLMs as well?",
                  "score": 1,
                  "created_utc": "2026-01-27 06:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z1yrg",
              "author": "Pvt_Twinkietoes",
              "text": "My experience with them has been phenomenal as well. I think somethings to note would be, it doesn't handle minor tilts/skew in the document, and users should be aware of that, but the pipeline provided does have a reliable model to predict the orientation of the document (90/180/270) tilts.\n\nThough it's amazing, I also noticed that there is a failure mode which causes the model to repeat itself (like Whisper), not sure of the cause but something to take note of.\n\nNevertheless it is truly an amazing model and very grateful they open sourced it.",
              "score": 1,
              "created_utc": "2026-01-27 06:43:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zit1a",
              "author": "Intelligent-Form6624",
              "text": "Does it work with ROCm or vulkan yet?",
              "score": 1,
              "created_utc": "2026-01-27 09:13:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1za5ac",
          "author": "R_Duncan",
          "text": "HunyuanOCR is not in the list.... this is cheating. For any kind of document, beats PaddleOCR hands down with 1B parameters.\n\n[https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true)",
          "score": 11,
          "created_utc": "2026-01-27 07:53:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zgxkh",
              "author": "__Maximum__",
              "text": "Is it end to end or pipeline?",
              "score": 2,
              "created_utc": "2026-01-27 08:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21dvaj",
                  "author": "R_Duncan",
                  "text": "Is pdf/image to markdown",
                  "score": 3,
                  "created_utc": "2026-01-27 16:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21vgbu",
              "author": "urekmazino_0",
              "text": "I second this",
              "score": 1,
              "created_utc": "2026-01-27 17:27:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z8565",
          "author": "Intelligent_Coffee44",
          "text": "I have some GPU credits that are near expiration, so I made this quick demo for DeepSeek OCR 2: [https://deepseek-ocr-v2-demo.vercel.app](https://deepseek-ocr-v2-demo.vercel.app)\n\n~~It's still very rough - small models + temperature=0 is very prone to repetition. I'll polish up the implementation in the morning. If anyone has an idea how to make the output more reliable, please let me know!~~\n\nUpdate: Decided to stay up and finish the job lol! Turns out the repetition issue was my user error. Now completely fixed after using DeepSeek's recommended decoding params. Performance is amazing and much more reliable than v1 in my testing. Hope you guys enjoy it too :O",
          "score": 20,
          "created_utc": "2026-01-27 07:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2078ly",
              "author": "Express-Director-474",
              "text": "Thanks for sharing your GPU with us. I still see the repetition error on my end.",
              "score": 2,
              "created_utc": "2026-01-27 12:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25pfyn",
                  "author": "Intelligent_Coffee44",
                  "text": "There were some configuration mistakes on my end. Now i've made it as closely aligned with official sample as possible. Please give it another try. I also did some analysis on the choice of prompt and document type affect output reliability - also published on the same site.",
                  "score": 1,
                  "created_utc": "2026-01-28 04:39:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o257yy2",
              "author": "__appelsinpiken",
              "text": "Very helpful! It looks like when tables are involved, the output is in HTML format, which doesnâ€™t render as a visible preview. Would it be possible to add a feature? That would make reviewing much more intuitive. Again thanks for your work! :>\n\nhttps://preview.redd.it/tu4x5r6t80gg1.png?width=665&format=png&auto=webp&s=29d0261de84095e8ee19861e769f0f9d3e654b0e",
              "score": 1,
              "created_utc": "2026-01-28 02:57:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25p4zl",
                  "author": "Intelligent_Coffee44",
                  "text": "Thanks for using! I just shipped markdown table support (from free OCR mode)\n\nDo you think this works or do you still prefer to have html table rendering?\n\nhttps://preview.redd.it/56nfle24r0gg1.png?width=715&format=png&auto=webp&s=4bf26bcd6c26e7e9ace035b56f251243c6c6be98",
                  "score": 1,
                  "created_utc": "2026-01-28 04:37:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b76j0",
              "author": "Confident-Ad-2688",
              "text": "I have download model from unsloth , I have GPU of rtx 3060 12 gb, didn't get results, like in your provided link . In your link results are superb,can you tell me how do you run the model ?",
              "score": 1,
              "created_utc": "2026-01-28 23:17:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ckq9x",
                  "author": "Intelligent_Coffee44",
                  "text": "Thank you! I recommend following the setup instructions from the official github repo [readme](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/README.md) and running [DeepSeek-OCR2-hf/run\\_dpsk\\_ocr2.py](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek-OCR2-master/DeepSeek-OCR2-hf/run_dpsk_ocr2.py) for inference. \n\nYou have the perfect GPU: the model only needs about 7-8gb vram to run, and your gpu is the same generation as A100 so you can install the exact same dependencies as the official guide.\n\nFor some reason i've found using the pinned transformers and tokenizers package versions helps fix performance issues.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yefjy",
          "author": "Dark_Fire_12",
          "text": "GitHub Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2](https://github.com/deepseek-ai/DeepSeek-OCR-2)\n\nPaper Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek\\_OCR2\\_paper.pdf](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf)",
          "score": 13,
          "created_utc": "2026-01-27 03:57:28",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1yfk2y",
          "author": "lomirus",
          "text": "Finally",
          "score": 9,
          "created_utc": "2026-01-27 04:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z49rf",
          "author": "the__storm",
          "text": "Interesting, I look forward to trying it out - DeepSeek-OCR (1) wasn't great (benchmarked okay but severely underperformed irl), so I'm glad they stuck with it.",
          "score": 3,
          "created_utc": "2026-01-27 07:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zq1jm",
          "author": "Gloomy-Signature297",
          "text": "Might be a stupid question but could this mean something regarding native multi-modality for Deepseek V4 next month?",
          "score": 5,
          "created_utc": "2026-01-27 10:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2177ju",
              "author": "ELPascalito",
              "text": "We cannot be sure, but it would be cool if the next model has this OCR module bolted on, just like how Mistral does",
              "score": 2,
              "created_utc": "2026-01-27 15:42:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ysnmt",
          "author": "Final_Personality987",
          "text": "https://preview.redd.it/bil1ybybvtfg1.png?width=1906&format=png&auto=webp&s=8ff884f062905a816cc6ba95e08904ca6e778b61\n\nquick summary: [https://lilys.ai/digest/7864011/8699710](https://lilys.ai/digest/7864011/8699710)",
          "score": 2,
          "created_utc": "2026-01-27 05:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zgaxi",
          "author": "Intelligent-Form6624",
          "text": "Heck yes!!! ðŸ‘ðŸ‘\n\nCan it run on Strix Halo?",
          "score": 2,
          "created_utc": "2026-01-27 08:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ss31",
          "author": "DouglasteR",
          "text": "Simply amazing",
          "score": 1,
          "created_utc": "2026-01-27 14:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b8jto",
          "author": "Medium_Confection604",
          "text": "Has anyone had any experience with parsing the output as bounding boxes + text?\n\nDuring my tests, I ran into situations like this:\n<|ref|>text<|/ref|><|det|>[[174, 78, 910, 113], [300, 153, 451, 131]]]<|/det|> form approved in writing by such Supervisor (or Referee in the case of Billie Jean King Cup).\n\nWhere the \"det\" tag had multiple bounding boxes (in some cases more than 2), but I can't figure out how to associate each bounding box with the corresponding text at the bottom.",
          "score": 1,
          "created_utc": "2026-01-28 23:24:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eh261",
          "author": "still_debugging_note",
          "text": "Been running Monkey-OCR for most OCR workloads.\n\nDeepSeek-OCR looks promising(esp. doc-level modeling), but I havenâ€™t tried it yet.Any insights on cost-efficiency compared to Monkey-OCR?",
          "score": 1,
          "created_utc": "2026-01-29 12:48:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlzbhh",
      "title": "[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning & OpenAI-Compatible API",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/",
      "author": "blackstoreonline",
      "created_utc": "2026-01-24 21:21:50",
      "score": 308,
      "num_comments": 141,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nThe Qwen team just dropped **Qwen3-TTS**, and itâ€™s a significant step forward for local speech synthesis. If youâ€™ve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAIâ€™s TTS that you can actually run on your own hardware, this is it.\n\nWeâ€™ve put together a repository that provides an **OpenAI-compatible FastAPI server**, meaning you can use it as a drop-in replacement for any app already using OpenAIâ€™s TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.\n\n# Why this is a big deal:\n\n* **Insane Speed:** It features a dual-track hybrid architecture that hits \\~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.\n* **Natural Voice Control:** You don't just send text; you can give it natural language instructions like *\"Say this in an incredibly angry tone\"* or *\"A shaky, nervous 17-year-old voice\"* and it actually follows through.\n* **Easy Voice Cloning:** Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.\n* **OpenAI Drop-in:** Works natively with the OpenAI Python client. Just change your `base_url` to localhost.\n* **Multilingual:** Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).\n\n# Getting Started (The Quick Way)\n\nIf you have Docker and a GPU, you can get this running in seconds:\n\nBash\n\n    git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi\n    docker build -t qwen3-tts-api .\n    docker run --gpus all -p 8880:8880 qwen3-tts-api\n\n# Python Usage (OpenAI Style)\n\nPython\n\n    from openai import OpenAI\n    \n    client = OpenAI(base_url=\"http://localhost:8880/v1\", api_key=\"not-needed\")\n    \n    response = client.audio.speech.create(\n        model=\"qwen3-tts\",\n        voice=\"Vivian\",  # 9 premium voices included\n        input=\"This sounds way too human for a local model.\",\n        speed=1.0\n    )\n    response.stream_to_file(\"output.mp3\")\n\n# Technical Highlights\n\n* **Architecture:** It uses the new **Qwen3-TTS-Tokenizer-12Hz** for acoustic compression. It skips the traditional \"LM + DiT\" bottleneck, which is why the latency is so low.\n* **Model Sizes:** Available in **0.6B** (super fast/light) and **1.7B** (high fidelity) versions.\n* **VRAM Friendly:** Supports FlashAttention 2 to keep memory usage down.\n\n**Links to dive deeper:**\n\n* [ðŸ¤— Hugging Face Collection](https://huggingface.co/collections/Qwen/qwen3-tts)\n* [ðŸ“„ Research Paper on arXiv](https://arxiv.org/abs/2601.15621)\n* [ðŸ’» Github Repo](https://github.com/QwenLM/Qwen3-TTS)\n\nIâ€™m really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.\n\nLet me know if you run into any issues setting it up!\n\nhttps://preview.redd.it/sa9itpxw6dfg1.png?width=1280&format=png&auto=webp&s=7fe58c44a2d0b9d03a5bf099024f18752d48949d",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1ixlxx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-25 00:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i0aux",
          "author": "Fragrant_Dog6303",
          "text": "Holy shit 97ms latency is actually insane for local TTS. Been using tortoise-tts and it takes like 30 seconds just to say \"hello\" lmao\n\n  \nDefinitely trying this tonight, the voice cloning with just 3 seconds sounds too good to be true but if it actually works that's game changing for local setups",
          "score": 49,
          "created_utc": "2026-01-24 21:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ir29e",
              "author": "z_latent",
              "text": ">Been using tortoise-tts and it takes like 30 seconds just to say \"hello\" lmao\n\nmodel name checks out I suppose\n\njokes aside, recently we've seen weekly drops of new super-fast, low-latency TTS models, many by independent people. ~~just yesterday I saw~~ [~~this one~~](https://www.reddit.com/r/LocalLLaMA/comments/1ql7mav/removed_by_moderator/) ~~on the sub~~ welp apparently post got removed due to \"rule 4\", but if anyone's interested, it's called LuxTTS. it wasn't even that good iirc, but it still impresses me how many of those were trained by random people online with some GPUs, rather than larger labs.",
              "score": 19,
              "created_utc": "2026-01-24 23:36:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1iudwj",
              "author": "_raydeStar",
              "text": "Do you know if it can do CPU inference at that speed?",
              "score": 5,
              "created_utc": "2026-01-24 23:53:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1l0sbh",
                  "author": "AlwaysLateToThaParty",
                  "text": "It will definitely *not* have those speeds on CPU memory bandwidths.",
                  "score": 1,
                  "created_utc": "2026-01-25 08:03:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1kom3y",
              "author": "The_frozen_one",
              "text": "Just curious, why tortoise-tts and not something like [piper](https://github.com/OHF-Voice/piper1-gpl) or [kokoro](https://github.com/hexgrad/kokoro)? What does tortoise-tts do well?\n\nqwen3-tts is for sure worth trying out, I've had more fun with qwen3-tts than any other TTS system. I've gotten it to work and run through all the different features without much trouble at all.",
              "score": 4,
              "created_utc": "2026-01-25 06:22:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1kuyfu",
                  "author": "mpasila",
                  "text": "It used to be pretty decent (and had voice cloning) but it was very slow, though it had received some speedups but things like XTTSv2 kinda replaced it and then now we have stuff like Chatterbox.. that make tortoise pretty outdated. Kokoro and Piper don't have voice cloning support. Which is kinda nice to have usually for a TTS. Kyutai did also release a pretty small model around 100M params with voice cloning (pocket-tts) as well which is probably gonna be faster than Qwen3 TTS (0.6B and 1.7B params).",
                  "score": 2,
                  "created_utc": "2026-01-25 07:13:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1lfliz",
              "author": "Decaf_GT",
              "text": "If low latency is a concerrn, you should definitely check out Supertonic: https://huggingface.co/Supertone/supertonic-2\n\nI've run it on an M1 Max with 32GB of RAM and it's damn near instant. \n\nI haven't yet tried Qwen TTS but I will soon.",
              "score": 2,
              "created_utc": "2026-01-25 10:13:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21v8ko",
              "author": "Top-Rip-4940",
              "text": "i cant get RTF on any card ., 5090 or h100 even . using vllm or official python backend . can anyone help me please?? can anyone tell me how to get the claimed 0.9 RTF ??  please ...",
              "score": 1,
              "created_utc": "2026-01-27 17:27:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2f06ld",
              "author": "Lucky_Yam_1581",
              "text": "its so good! even the 0.6 b model! its so accurate and nuanced!",
              "score": 1,
              "created_utc": "2026-01-29 14:33:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ks5gu",
              "author": "Bakoro",
              "text": ">the voice cloning with just 3 seconds sounds too good to be true \n\nIn theory, it's completely feasible. A person's voice is just a collection of frequencies/harmonics, and there just aren't *that* many meaningful frequencies for humans.  \n\nYou can find a person's vocal signature using signal processing techniques like a Fourier Transform. You can model a voice as a source and filter.  \n  \nAn AI model that's been trained on enough data would just learn a set of common basis signals and modulate them to fit.",
              "score": 1,
              "created_utc": "2026-01-25 06:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1livfq",
                  "author": "cheyyne",
                  "text": "> In theory, it's completely feasible. A person's voice is just a collection of frequencies/harmonics, and there just aren't that many meaningful frequencies for humans. \n\nI mean, yeah... It's just all the little subtleties and layers of implication, plus the effects of accents... Inflection in the language of origin... Regional dialects... You know. That kinda stuff.\n\nThe devil's in the details with these things.",
                  "score": 6,
                  "created_utc": "2026-01-25 10:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i54n1",
          "author": "SouthernFriedAthiest",
          "text": "Try it outâ€¦Iâ€™m hosting it for some testingâ€¦but so far dang amazing!\n\nTts.loser.com (I have it running for anyone to give it a whirl)",
          "score": 39,
          "created_utc": "2026-01-24 21:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ihb8o",
              "author": "hapliniste",
              "text": "The trump one completely rewrote my text to sound like Trump, is that an extra feature or is it the model itself? Was very funny in any case",
              "score": 14,
              "created_utc": "2026-01-24 22:45:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ihhnt",
                  "author": "SouthernFriedAthiest",
                  "text": "Ohh thatâ€™s a special feature just for Trumpy and Yoda ;)",
                  "score": 20,
                  "created_utc": "2026-01-24 22:46:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j5gdk",
              "author": "qazzq",
              "text": "What are you running this on? Generation seems fairly speedy. Also, im gonna have to try this out myself. \n\nHow does quality compare to other models btw, anyone deep into this and has an opinion theyd like to share?",
              "score": 3,
              "created_utc": "2026-01-25 00:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ku2my",
              "author": "pfn0",
              "text": "Lol at trumpy taking a complete shit over the input prompt. the rest of the voices don't really feel like they match who they are (unless the goal wasn't voice clone but intonation clone)",
              "score": 3,
              "created_utc": "2026-01-25 07:06:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1iq2gy",
              "author": "Sambojin1",
              "text": "Cheers. I'm going to run some slightly longer clips from the Ultima 7 transcript, just to see consistency.\n\nYep, Qwen TTS prompting is going to be another skill I'm going to have to learn. Still, this will be a goldmine for game mods and solo dev projects that want to do voice work. Like, we know it's going to be used for humour and deepfakes first, but it does have genuine ethical uses as well.",
              "score": 2,
              "created_utc": "2026-01-24 23:30:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lo5lt",
              "author": "SocialDinamo",
              "text": "Really appreciate you hosting the demo!",
              "score": 2,
              "created_utc": "2026-01-25 11:29:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1o0808",
              "author": "GuideAxon",
              "text": "The Trump one is hilarious.",
              "score": 2,
              "created_utc": "2026-01-25 18:41:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1jqxvo",
              "author": "New_Jaguar_9104",
              "text": "volume could use some normalizing across them but otherwise pretty cool dude",
              "score": 1,
              "created_utc": "2026-01-25 02:49:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1jyu68",
                  "author": "SouthernFriedAthiest",
                  "text": "You are not wrongâ€¦it was a mix of samples (3 secs ) each from YouTube videos ..if one were to source quality samplesâ€¦I think it could be way better for sure",
                  "score": 1,
                  "created_utc": "2026-01-25 03:34:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1knd5u",
              "author": "rm-rf-rm",
              "text": "I get `TTS Generation Failed` error..",
              "score": 1,
              "created_utc": "2026-01-25 06:12:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sjbnn",
              "author": "thecosmingurau",
              "text": "It's not working",
              "score": 1,
              "created_utc": "2026-01-26 09:51:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uf72g",
              "author": "Additional-Sun-6083",
              "text": "The Trumpy filter made me laugh so much more than I thought it would.",
              "score": 1,
              "created_utc": "2026-01-26 16:34:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1jstqd",
              "author": "andero",
              "text": "That is exceptionally fast.  \n\nNot actually very good, at least not if those voice-options are trying to clone voice samples rather than follow text descriptions.  \n\nThe free and open Higgs Audio model that was put out quite some time ago was VASTLY better at voice cloning, though orders of magnitude slower.",
              "score": 1,
              "created_utc": "2026-01-25 02:59:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1iro5z",
          "author": "ubrtnk",
          "text": "Good work - just FYI, by default your Dockerfile wont work with Blackwell GPUs. \n\nI was able to get it to work by modifying a few things in the Dockerfile\n\n1) change your base image to nvidia/cuda:12.8.0-cudnn-runtime-ubuntu22.04\n\n2) change your index-url in the python dependencies to [https://download.pytorch.org/whl/cu128](https://download.pytorch.org/whl/cu128) (to match new image)\n\nOOH I also had to disable the Numba JIT caching\n\nhttps://preview.redd.it/xsjow9hgpdfg1.png?width=1017&format=png&auto=webp&s=9a145eabab651a0d185ca71d91fb1d08eecd79d1\n\nadding an environment variable to the compose file for the image you want to use (NUMBA\\_DISABLE\\_JIT=1) fixed that error\n\nI'm also attempting to see if I can get it running on a jetson orin nano super with GPU support -  just to see if I can ;)\n\nQuestion - where is the voice cloning section and where/how do I upload my audio file for cloning?",
          "score": 21,
          "created_utc": "2026-01-24 23:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j8shz",
              "author": "ubrtnk",
              "text": "Update - So I could get the container to compile and run on CPU but it was too big for the Jetson - maxed out ram 100% AND used almost 6G of SWAP. Could not get CUDA running even though I limited the execution only to GPU, installed the right version of onnxrutime-gpu and removed what I could find for any other CPU bound functions.",
              "score": 3,
              "created_utc": "2026-01-25 01:09:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1it9nx",
              "author": "andy2na",
              "text": "Thanks for figuring this out, can you provide the dockerfile so we can test it out? Tried to load from OP and it seems to just use system memory and CPU",
              "score": 2,
              "created_utc": "2026-01-24 23:47:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1iv0ds",
                  "author": "ubrtnk",
                  "text": "Sure:\n\nhttps://preview.redd.it/2dci3r8aydfg1.png?width=797&format=png&auto=webp&s=8152d4a74faf16a22ce928e1874e8ee93bf7b880\n\nI didnt change anything below the FastAPI and server dependencies component so didnt paste that section. I will say I also did change the docker compose file a bit to use my specific 5060Ti in my AI rig\n\nI changed it to device\\_ids: \\[\"2\"\\] so that way it only had access to that GPU, as labeled by nvtop or nvidia-smi",
                  "score": 3,
                  "created_utc": "2026-01-24 23:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ifx1k",
          "author": "Kindly-Annual-5504",
          "text": "Your description mentions streaming with low latency, but does it really support streaming audio? The model architecture does support streaming, but the currently published code on github does not, so that's why I'm asking.",
          "score": 17,
          "created_utc": "2026-01-24 22:38:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kpb2r",
              "author": "Tenemi",
              "text": "Looked into this FastAPI implementation - it doesn't actually stream. The /v1/audio/speech endpoint generates the full audio before returning anything. No StreamingResponse, no WebSocket, nothing chunked.\n\nI have a fork with actual streaming - WebSocket that sends audio chunks as tokens are generated, gets you first audio in \\~1.5s instead of waiting for the whole thing. SSE option too for sentence-by-sentence. Vibe coded with Claude Code, certainly not perfect but works for my use case until true streaming gets exposed for us... hopefully.\n\nBut none of this fixes the real problem: the model needs all the text upfront before it starts generating. You can stream the audio out, but you can't feed it text incrementally as your LLM produces it. That's baked into the transformer architecture, not something a server wrapper can solve.",
              "score": 8,
              "created_utc": "2026-01-25 06:27:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ntbi9",
                  "author": "Yorn2",
                  "text": "It's an old program, but AllTalk was able to get streaming working somehow. It used XTTS and even could do different voices for narration and quotes. It's really too bad the developer stopped updating it, I got the impression it was going to be the one best software for running offline TTS engines and would just add new ones as they came out. Instead, Chatterbox is seemingly now better for this. Even with Qwen3-TTS having interesting new features, I'm not sure it's going to be enough to replace existing software unless existing software adds it as a base engine.",
                  "score": 1,
                  "created_utc": "2026-01-25 18:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1juizu",
              "author": "hellomistershifty",
              "text": "It's kind of silly, because he recommends vLLM for low latency but the vllm-omni readme says:\n\n\"While vLLM-Omni currently only supports offline inference (not true audio streaming), it can significantly speed up generation compared to the official backend.\"\n\nso yeah, it seems like we have to wait to get real streaming with this",
              "score": 6,
              "created_utc": "2026-01-25 03:09:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1l1fvu",
              "author": "amroamroamro",
              "text": "https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/docs/vllm-backend.md#comparison-vllm-vs-official-backend\n\n> Neither backend supports true audio streaming over HTTP currently. Both use OpenWebUI's chunk-based approach.\n> Chunk Streaming: Long text is split into chunks, each processed as a separate TTS request.\n\neven that is not true, the fastapi endpoint only generates full audio clips, no streaming or chunking...\n\nboth backends are used from an interface that only exposes `generate_speech` method which returns audio as one numpy array no streaming/chunking:\n\nhttps://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/api/backends/base.py#L33-L55\n\nhttps://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/api/routers/openai_compatible.py#L227-L250\n\nNot to criticize but this whole fastapi wrapper was clearly vibe coded, the part about it supporting streaming/chunking was just hallucinated",
              "score": 7,
              "created_utc": "2026-01-25 08:08:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ins65",
              "author": "no_witty_username",
              "text": "my questions was same as the code doesnt seem to have true streaming but maybe im wrong. let me know if you get it working....",
              "score": 0,
              "created_utc": "2026-01-24 23:18:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ik3tx",
              "author": "blackstoreonline",
              "text": "This one does",
              "score": -5,
              "created_utc": "2026-01-24 22:59:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1jk0of",
                  "author": "Sweet_Albatross9772",
                  "text": "Does it really? Could you show where exactly? It seems you haven't changed Qwen's team inference code and their code does not support streaming yet...  \nLooking at the code you just slapped an API on top of it...",
                  "score": 11,
                  "created_utc": "2026-01-25 02:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j4ajz",
          "author": "andy2na",
          "text": "thanks u/blackstoreonline got it working wiht u/ubrtnk dockerfile update for blackwell cards. How do we change it to use the 0.6 model? is there an env variable to select?",
          "score": 4,
          "created_utc": "2026-01-25 00:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kzpgu",
          "author": "FlowCritikal",
          "text": "Anyone get this working on ROCm?  Specifically would be interested in running this on Strix Halo",
          "score": 5,
          "created_utc": "2026-01-25 07:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l5xii",
          "author": "VampiroMedicado",
          "text": "It's insane how easy you can clone a voice, I just tried it with the 1.7B model.\n\nI said \"Hello, who is this? who is this?\" like you would on a phone call where the other sides doesn't answer, then I made it say \"Hello mom, I need help\" and it was very similar to my voice.",
          "score": 4,
          "created_utc": "2026-01-25 08:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mh8aa",
              "author": "xandep",
              "text": "You seem too familiar with how Brazilian scammers operate.. ðŸ˜†\n\nDon't worry, we are just months away from widespread use of this scamming technique.",
              "score": 2,
              "created_utc": "2026-01-25 14:40:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1idlkb",
          "author": "no_witty_username",
          "text": "Does this repo have the streaming code? cause I tried to get streaming working on the original repo and couldn't find any reference code for it as it all pointed toward gated gated api's?",
          "score": 8,
          "created_utc": "2026-01-24 22:27:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ik156",
              "author": "blackstoreonline",
              "text": "yes it does, I use it with open-webui",
              "score": -6,
              "created_utc": "2026-01-24 22:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ikyg0",
                  "author": "no_witty_username",
                  "text": "from codex and other agentic coding solutions, there telling me its not true streaming. as the rtf is above 1 and ttfs will be high. as in whole waveform has to be generated before you hear any output. are they hallucinating this? what is the rtf numbers and ttfs numbers you getting?",
                  "score": 4,
                  "created_utc": "2026-01-24 23:04:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i30yr",
          "author": "Pentium95",
          "text": "Does It run on CPU too?",
          "score": 3,
          "created_utc": "2026-01-24 21:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ibmbd",
              "author": "OC2608",
              "text": "Interested to know this as well.",
              "score": 3,
              "created_utc": "2026-01-24 22:17:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ilvk3",
                  "author": "saul_karl",
                  "text": "Yes, just specify \"--device cpu --no-flash-attn\".\n\nOn my intel i5 13th gen mini pc, it takes about 15-30 mins to generate a 10 second clip. However the quality is good (I can't tell the difference between 0.6B and 1.7B models just by listening).",
                  "score": 5,
                  "created_utc": "2026-01-24 23:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1l92aw",
                  "author": "Fear_ltself",
                  "text": "Yes, it wasnâ€™t even that much slower than the GPU. 14 seconds for my 4070, 45 seconds for the cpu. Nowhere near the advertised speeds, and the quality was worse than kokoro 73m by a noticeable amount. Think Iâ€™ll try with flash attention on to see if that helps speed, because Iâ€™m not sure how other people are getting to run so fast",
                  "score": 1,
                  "created_utc": "2026-01-25 09:15:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1lcfa8",
              "author": "JackStrawWitchita",
              "text": "I've just spent time running it on CPU and it's painfully slow and quality is no better than Chatterbox or Vibvoice. I'm sticking with Chatterbox.",
              "score": 2,
              "created_utc": "2026-01-25 09:45:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r31uh",
          "author": "aschroeder91",
          "text": "It takes about 5 seconds to generate a small sample sentence. I am running using docker vllm server using flash attn on 3090 GPU using 0.6B model. \n\nhttps://preview.redd.it/mw3gus3y2mfg1.png?width=430&format=png&auto=webp&s=5d3e76e1d54b6c3746c462c0ee75e1c62491b2a6",
          "score": 3,
          "created_utc": "2026-01-26 03:17:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i7te8",
          "author": "umbs81",
          "text": "qwen with the Italian language does not perform well. Not very natural.",
          "score": 6,
          "created_utc": "2026-01-24 21:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k0hi5",
              "author": "BusRevolutionary9893",
              "text": "Only 1.0%-1.1% of the world's population can speak Italian and only 0.3%-0.5% of the world's population only speak Italian. In my opinion, any training on Italian was a waste. All focus should have been on English, then Chinese, then Spanish.Â Â \n\n\nThose 3 languages cover 30%-35% of the global population and there are 7,200 living languages.Â \n\n\nSome might argue Hindi instead of Spanish as there are slightly more people who speak Hindi but, while having more speakers, it is far more regionally concentrated and less used as a global second language, making it less likely to be part of high-frequency cross-language global interaction.",
              "score": 6,
              "created_utc": "2026-01-25 03:43:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ls9i0",
                  "author": "finkonstein",
                  "text": "Yes, and 90% of the world do not use Qwen-TTS at all, so we do not need it either",
                  "score": 3,
                  "created_utc": "2026-01-25 12:02:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1jxeg3",
          "author": "SmartCustard9944",
          "text": "You sure say 97ms a lot. Your AI generated post is not even mentioning on which hardware, so, your claim is completely pointless and unsubstantiated.",
          "score": 7,
          "created_utc": "2026-01-25 03:25:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jz240",
              "author": "hellomistershifty",
              "text": "97ms is from Qwen's[ model card](https://github.com/QwenLM/Qwen3-TTS):\n\n*Extreme Low-Latency Streaming Generation: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.*\n\nThis repo doesn't support dual-track hybrid streaming (nothing local supports it right now) so you won't get latency nearly that low - you have to wait for the whole generation to finish",
              "score": 4,
              "created_utc": "2026-01-25 03:35:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihp7t",
          "author": "hapliniste",
          "text": "97ms but what does it mean exactly? In comfyui I tried it and it's way slower than real time, it takes around 100s to generate a 5-10s clip which I find strange for such a small model on a 3090.\n\nMaybe it's not optimize yet?",
          "score": 4,
          "created_utc": "2026-01-24 22:47:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ik8qj",
              "author": "blackstoreonline",
              "text": "Iâ€™ve got 3090 too and is waay faster than that, that comfyui node is clearly not optimized",
              "score": 1,
              "created_utc": "2026-01-24 23:00:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1itnqt",
                  "author": "aeroumbria",
                  "text": "The large voice design model still takes like 30s for a paragraph of text using a 3090 and the official repo. By latency do you mean running in streaming mode rather than full IO?",
                  "score": 1,
                  "created_utc": "2026-01-24 23:49:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1k1hgw",
          "author": "false79",
          "text": "Man - nothing but problems if running Windows with an AMD GPU :/\n\nSeems like this thing was built on CUDA only.",
          "score": 2,
          "created_utc": "2026-01-25 03:49:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mlkxj",
              "author": "deepspace_9",
              "text": "it did run in linux, 7900xtx. it took 30 seconds to generate audio.",
              "score": 1,
              "created_utc": "2026-01-25 15:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mu8uy",
                  "author": "false79",
                  "text": "Thx for the heads up. I have same GPU. I've been meaning to switch over for a while to linux.\n\nBut 30 seconds seems awfully long. I thought it was \"Ultra Low Latency (97ms)\"",
                  "score": 1,
                  "created_utc": "2026-01-25 15:43:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1s7tmn",
                  "author": "armored_strawberries",
                  "text": "I have the same GPU, but running on Window$\nI have so much stuff running for work, but I hate this freaking bloated spyware. Maybe that's the call I needed... lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:06:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1k9et9",
          "author": "Decent-Opening-2113",
          "text": "I really like the model but without local streaming it's too slow for my project. Hopefully they'll release streaming for the repo sometime.",
          "score": 2,
          "created_utc": "2026-01-25 04:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1offr9",
          "author": "Lower_Journalist5500",
          "text": "I think this is a deceptive post. The server is deceiving. The curl log has a content-length header: 18764. This is a bad sign. If the server returns a content-length, it means it first generated the entire file, calculated its size, and only then started sending it. In real streaming (Chunked Transfer Encoding), the file size is not known in advance. Even considering that the request has \"stream\": True.  I'd be happy to receive a working streaming example.\n\nC:\\\\Qwen3TTS\\\\.venv\\\\Scripts\\\\python.exe C:\\\\Qwen3TTS\\\\main2.py\n\n\\>> Sending request...\n\nâš ï¸ WARNING: Server returned Content-Length (180524).\n\nThis means the server FIRST generated everything, and then started sending. There will be no streaming.\n\nProcess finished with exit code 0  \n  \n  \nLook at the code in api/routers/openai\\_compatible.py, lines 185-205 (function create\\_speech):\n\n\n\npython\n\n\\# Generate speech\n\n\\# HERE IS THE PROBLEM: There is an await here that waits for COMPLETE generation\n\naudio, sample\\_rate = await generate\\_speech(...) \n\n\n\n\\# Encode audio to requested format\n\naudio\\_bytes = encode\\_audio(audio, request.response\\_format, sample\\_rate)\n\n\n\n\\# ... creating a regular Response ...\n\nreturn Response(content=audio\\_bytes, ...)\n\nThis code ignores the stream=True parameter. It always generates the entire file, encodes it completely, and returns it as a whole.",
          "score": 2,
          "created_utc": "2026-01-25 19:47:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iha0k",
          "author": "Gudeldar",
          "text": "The voice cloning doesn't seem nearly as IndexTTS 2.",
          "score": 3,
          "created_utc": "2026-01-24 22:45:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j78lq",
          "author": "Tonyoh87",
          "text": "How does it compare to pocket-tts?",
          "score": 1,
          "created_utc": "2026-01-25 01:00:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jfxs0",
              "author": "SouthernFriedAthiest",
              "text": "Pocket tts is amazing for size and speed etc.. cpu or gpu â€” but the qwen3tts family blows it away in performance\tand all it can doâ€¦but pocket is great for its use case.  (All depends on edge/or where you are generating your audioâ€¦)",
              "score": 1,
              "created_utc": "2026-01-25 01:48:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1jhu5k",
          "author": "Western_Team4433",
          "text": "97ms is very very very good. How well does it compare against the closed source models from elevenlabs?",
          "score": 1,
          "created_utc": "2026-01-25 01:59:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jmi2c",
          "author": "djtubig-malicex",
          "text": "While the speed is nice, I think the voice cloning aspect is still better with IndexTTS2",
          "score": 1,
          "created_utc": "2026-01-25 02:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k1ueg",
          "author": "altacct3",
          "text": "was waiting for something like this! thanks!",
          "score": 1,
          "created_utc": "2026-01-25 03:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k5wnp",
          "author": "ResponsiblePoetry601",
          "text": "Nice! Was able to generate around 2 min audio in 4 min with no cuda. Vivian sounds ok!",
          "score": 1,
          "created_utc": "2026-01-25 04:16:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k8024",
              "author": "ResponsiblePoetry601",
              "text": "Besides English I also tried Portuguese and although itâ€™s more like Brazilian the quality is very good",
              "score": 1,
              "created_utc": "2026-01-25 04:30:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1k8nyo",
                  "author": "ResponsiblePoetry601",
                  "text": "Curious if itâ€™s good for real time will try later",
                  "score": 1,
                  "created_utc": "2026-01-25 04:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kkuf3",
          "author": "127loopback",
          "text": "Hey can you do the same front end for Chroma\nhttps://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma/tree/main/\n\nhttps://huggingface.co/FlashLabs/Chroma-4B\n\nhttps://www.flashlabs.ai/flashai-voice-agents",
          "score": 1,
          "created_utc": "2026-01-25 05:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kn4f0",
          "author": "rm-rf-rm",
          "text": "I tried the HF space and voice cloning is quite poor - you can re-run the exact same text and voice input and you get very different outputs - sometimes it sounds like a good clone and other times not even close. Is there a way to improve this?",
          "score": 1,
          "created_utc": "2026-01-25 06:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kntn2",
          "author": "OopsWrongSubTA",
          "text": "For a language like French, default voices sound too much english/chinese... custom voices also (it works better sometimes : is it possible to chose a seed?).\n\nBut cloning voices : insane quality!",
          "score": 1,
          "created_utc": "2026-01-25 06:16:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lylv8",
              "author": "AnusIingus",
              "text": "T'as rÃ©ussi Ã  mettre des Ã©motions avec la voix clonÃ©e ?",
              "score": 1,
              "created_utc": "2026-01-25 12:51:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kpajm",
          "author": "wichwigga",
          "text": "Is it possible to record PC audio and transcribe in real time with this model?",
          "score": 1,
          "created_utc": "2026-01-25 06:27:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xg2lm",
              "author": "Levy_Wilson",
              "text": "This is TTS, you need a STT like Whisper.",
              "score": 2,
              "created_utc": "2026-01-27 00:47:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yh41a",
                  "author": "wichwigga",
                  "text": "Ah I see thanks",
                  "score": 1,
                  "created_utc": "2026-01-27 04:14:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1krf6m",
          "author": "pfn0",
          "text": "Thanks for putting this UI together, it looks amazing. Seems like it's not ready to run out of the box on blackwell though, I'll have to hack on it to add support:\n\n```\nqwen3-tts-api  | NVIDIA RTX PRO 6000 Blackwell Workstation Edition with CUDA capability sm_120 is not compatible with the current PyTorch installation.\nqwen3-tts-api  | The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\nqwen3-tts-api  | If you want to use the NVIDIA RTX PRO 6000 Blackwell Workstation Edition GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n```\n\nedit:\n\neasy fix though, just edit the docker FROM to point to the following for base and build, respectively:\n * nvidia/cuda:13.1.0-runtime-ubuntu24.04                                                                     \n * nvidia/cuda:13.1.0-devel-ubuntu24.04\n\nalso fetching cu130 from pytorch instead of cu121\n\nthen wait a year while flash-attn builds...",
          "score": 1,
          "created_utc": "2026-01-25 06:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kxmpr",
          "author": "rm-rf-rm",
          "text": "I see you make FastAPI based OpenAI API wrappers for all TTS/STT models. Is it possible to make a single wrapper package where you can pick and choose whatever STT/TTS model(s) you want?",
          "score": 1,
          "created_utc": "2026-01-25 07:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l2v77",
          "author": "Yorn2",
          "text": "With other TTS OpenAPI-compatible engines there's typically a /voices folder where I dump all my audio for cloning. Does this not support that or do we have to manually upload our voices for cloning somehow?",
          "score": 1,
          "created_utc": "2026-01-25 08:21:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lrt7y",
          "author": "Weak-Shelter-1698",
          "text": "how to turn of flash attention :) turing gpus aren't working.",
          "score": 1,
          "created_utc": "2026-01-25 11:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1luqyx",
          "author": "drivenkey",
          "text": "Trying this on windows 11, docker with 3090 - getting this error on flashattention - any help pls? \n\nhttps://preview.redd.it/2aldj4panhfg1.png?width=1127&format=png&auto=webp&s=341c0acc30578599fca7614de757199862287034",
          "score": 1,
          "created_utc": "2026-01-25 12:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q80n0",
              "author": "No-Mall1142",
              "text": "I'm in the same boat, 4070.  Claude had me add it to the dockerfile, but still no go.",
              "score": 1,
              "created_utc": "2026-01-26 00:39:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1r5mxe",
              "author": "aschroeder91",
              "text": "often happens when you use the wrong docker tag. When you use docker build -t qwen3-tts-api:<tag> you should choose one that builds with flash-attn (the dockerfile has various tags)  \ntry using docker build -t qwen3-tts-api:production",
              "score": 1,
              "created_utc": "2026-01-26 03:32:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1xcbff",
              "author": "kenrock2",
              "text": "Gemini had me to use this command to install instead of the above version which gives me the same problem as yours. Below are the working one.\n\ndocker build --target production -t qwen3-tts-api .",
              "score": 1,
              "created_utc": "2026-01-27 00:28:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1m7vqq",
          "author": "mediali",
          "text": "The models are all excellent. It's truly generous of them to release such a product for free. Thank you for their contribution.",
          "score": 1,
          "created_utc": "2026-01-25 13:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nd2ty",
          "author": "studentofknowledg3",
          "text": "I have RTX 5080, 9950x3D, cannot install because of having CUDA 13, and cannot install \\`flash-attn\\`",
          "score": 1,
          "created_utc": "2026-01-25 17:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o0lap",
          "author": "Electronic-Metal2391",
          "text": "It is giving me garbage audio (many voices speaking at the same time) generation when using the VoiceDesign.",
          "score": 1,
          "created_utc": "2026-01-25 18:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rdku4",
          "author": "Exydosa",
          "text": "Is the 97ms latency from voice cloning, or is it only for the default text-to-speech model card?",
          "score": 1,
          "created_utc": "2026-01-26 04:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s5skx",
          "author": "getgoingfast",
          "text": "Thanks for sharing. Demo voices are rather dramatic, Uncle\\_flu, gonna be interesting.",
          "score": 1,
          "created_utc": "2026-01-26 07:48:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uf722",
          "author": "Ooothatboy",
          "text": "you should add the ability to add voices in the ui (for voice cloning similar to how [github.com/travisvn/chatterbox-tts-api](http://github.com/travisvn/chatterbox-tts-api) does it)",
          "score": 1,
          "created_utc": "2026-01-26 16:34:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xcu0p",
          "author": "kenrock2",
          "text": "https://preview.redd.it/emhppdb3dsfg1.png?width=992&format=png&auto=webp&s=83dd4afd0ea23790d658798f2828c137c916b590\n\n  \nThe generation of the default test speech took approximately 40 seconds, which is slightly slower than GPT-SoVITS. It is unclear whether this discrepancy is related to the utilization of cuda:0, but the process is indeed leveraging GPU resources.",
          "score": 1,
          "created_utc": "2026-01-27 00:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjhtt",
              "author": "aschroeder91",
              "text": "i am also not able to get anywhere near this <100ms claim...",
              "score": 1,
              "created_utc": "2026-01-27 01:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xof1k",
                  "author": "kenrock2",
                  "text": "In my experience, GPT-SoVITS is significantly faster, generating responses in under 3 seconds for speech of similar length to the test, and its quality is sufficient for conversation.",
                  "score": 1,
                  "created_utc": "2026-01-27 01:32:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1y7bon",
          "author": "j3ss4u",
          "text": "I checked the demo on huggingface a few days ago and it had way more default faces, now they're gone. Does anyone know what happened to them? Can I have those if I run this locally?",
          "score": 1,
          "created_utc": "2026-01-27 03:15:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp9m3",
          "author": "Pretend-Umpire-3448",
          "text": "what's the minimum hardware requirement to run this locally? thank you",
          "score": 1,
          "created_utc": "2026-01-27 05:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216xk7",
          "author": "Gambikules",
          "text": "audio source to audio target possible ? or only text to audio",
          "score": 1,
          "created_utc": "2026-01-27 15:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28bqiy",
              "author": "Blizado",
              "text": "TTS = Text To Speech\n\nShould say enough. You search for STS.",
              "score": 1,
              "created_utc": "2026-01-28 15:39:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21v734",
          "author": "Top-Rip-4940",
          "text": "i cant get RTF on any card ., 5090 or h100 even . using vllm or official python backend . can anyone help me please?? can anyone tell me how to get the claimed 0.9 RTF ??  please ...",
          "score": 1,
          "created_utc": "2026-01-27 17:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29m505",
          "author": "LiquidOracle",
          "text": "I have a 2070 super, can I still utilize the 1.7B?",
          "score": 1,
          "created_utc": "2026-01-28 18:59:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9vvs",
          "author": "IcyMushroom4147",
          "text": "on a 2070 rtx:  \n1.6B model... used around 5gb vram  \nAudio duration:        3.817s\n\nGeneration time:       4.459s\n\nRTF (Real-Time Factor): 1.168x\n\n  \n0.6b model.. used around 2.7gb vram  \nText length:           50 characters\n\nAudio duration:        5.417s\n\nGeneration time:       6.638s\n\nRTF (Real-Time Factor): 1.226x",
          "score": 1,
          "created_utc": "2026-01-28 20:45:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d3x3i",
              "author": "COC_bigfish",
              "text": "æˆ‘çš„é­”æ”¹2080TI \n\nç”¨1.6B æ¨¡åž‹ tts 3ç§’éŸ³é¢‘ï¼Œå¤§æ¦‚10ä¸ªå­—ç¬¦ï¼Œéœ€è¦15ç§’å·¦å³ã€‚\n\nç”¨Index TTS ç›¸åŒçš„å­—ç¬¦ï¼Œéœ€è¦5ç§’å·¦å³\n\nç”¨GPT-SoVitsç›¸åŒçš„å­—ç¬¦ï¼Œéœ€è¦2ç§’å·¦å³\n\næ˜¯æˆ‘çš„æ‰“å¼€æ–¹å¼ä¸å¯¹ï¼Ÿ éƒ½æ˜¯CUDA ï¼Œå¯¹äº†ä¸å¼€CUDA è¦5åˆ†é’Ÿä»¥ä¸Šã€‚",
              "score": 1,
              "created_utc": "2026-01-29 05:56:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dl2sl",
          "author": "Flying_Dutchman_7",
          "text": "How do I change the output format if i run it locally. I want it to output ulaw.",
          "score": 1,
          "created_utc": "2026-01-29 08:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f11bl",
          "author": "InterestingBasil",
          "text": "Qwen3 is a beast for the output! If anyone is building a full voice loop on Windows and needs the input side (Speech-to-Text), I built DictaFlow. It uses local Whisper models and simulates keystrokes to bridge the gap between your voice and these local agents. https://dictaflow.vercel.app/",
          "score": 1,
          "created_utc": "2026-01-29 14:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i16wn",
          "author": "CatEatsDogs",
          "text": "Does it support intel igpu?",
          "score": 1,
          "created_utc": "2026-01-24 21:28:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i50z9",
              "author": "literateu",
              "text": "Yes, see the repo.\n\n\\`\\`\\`  \n\\# Build CPU-only variant\n\ndocker build -t qwen3-tts-api-cpu --target cpu-base .\n\ndocker run -p 8880:8880 qwen3-tts-api-cpu\n\n\n\n\\# Or use Docker Compose\n\ndocker-compose --profile cpu up qwen3-tts-cpu  \n\\`\\`\\`",
              "score": 0,
              "created_utc": "2026-01-24 21:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ksvs5",
                  "author": "CatEatsDogs",
                  "text": "Can you spot the difference between \"cpu-only\" and \"intel igpu\"? I was asking about igpu not cpu.",
                  "score": 1,
                  "created_utc": "2026-01-25 06:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kju8r",
          "author": "Eastern_Rock7947",
          "text": "I have found it slow on a 3080 ti 180secs for 46 secs of audio. However the quality is really good.",
          "score": 1,
          "created_utc": "2026-01-25 05:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ieynl",
          "author": "Lostronzoditurno",
          "text": "Failed to initialize TTS model: cannot cache function '\\_\\_o\\_fold': no locator available for file '/opt/venv/lib/python3.11/site-packages/librosa/core/notation.py'\n\nJust cloned the repo and runned it with docker compose up qwen3-tts-gpu  \nI'm using NVIDIA",
          "score": 0,
          "created_utc": "2026-01-24 22:34:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ikd2e",
              "author": "blackstoreonline",
              "text": "sorry Iâ€™ve just fixed that error, please do git pull and try again",
              "score": 1,
              "created_utc": "2026-01-24 23:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1knml6",
          "author": "The_frozen_one",
          "text": "This is definitely the best local TTS I've used for voice cloning. It also passes my \"can I get this working in a reasonable amount without having to type `pip` more than 10 times\" test.",
          "score": -3,
          "created_utc": "2026-01-25 06:14:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i8iw1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -19,
          "created_utc": "2026-01-24 22:02:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ia1a0",
              "author": "weasl",
              "text": "thanks chatgpt",
              "score": 15,
              "created_utc": "2026-01-24 22:10:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjota",
      "title": "I built a \"hive mind\" for Claude Code - 7 agents sharing memory and talking to each other",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "author": "Historical-Celery-83",
      "created_utc": "2026-01-26 15:49:13",
      "score": 303,
      "num_comments": 63,
      "upvote_ratio": 0.87,
      "text": "Been tinkering with multi-agent orchestration and wanted to share what came out of it.\n\n\n\n\\*\\*The idea\\*\\*: Instead of one LLM doing everything, what if specialized agents (coder, tester, reviewer, architect, etc.) could coordinate on tasks, share persistent memory, and pass context between each other?\n\n\n\n\\*\\*What it does\\*\\*:\n\n\\- 7 agent types with different system prompts and capabilities\n\n\\- SQLite + FTS5 for persistent memory (agents remember stuff between sessions)\n\n\\- Message bus for agent-to-agent communication\n\n\\- Task queue with priority-based coordination\n\n\\- Runs as an MCP server, so it plugs directly into Claude Code\n\n\\- Works with Anthropic, OpenAI, or Ollama\n\n\n\n\\*\\*The cool part\\*\\*: When the coder finishes implementing something, the tester can query the shared memory to see what was built and write appropriate tests. The reviewer sees the full context of decisions made. It's not magic - it's just passing data around intelligently - but it feels like they're actually collaborating.\n\n\n\n\\*\\*The not-so-cool part\\*\\*: Debugging 7 agents talking to each other is... an experience. Sometimes they work beautifully. Sometimes one agent keeps assigning tasks to itself in an infinite loop. You know, typical multi-agent stuff.\n\n\n\n\\*\\*Stack\\*\\*: TypeScript, better-sqlite3, MCP SDK, Zod\n\n\n\nNot enterprise-ready. Not trying to compete with anything. Just an experiment to learn how agent coordination patterns work.\n\n\n\nMIT licensed: [github.com/blackms/aistack](http://github.com/blackms/aistack)\n\n\n\nHappy to answer questions or hear how you're approaching multi-agent systems.\n\n",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v6rcq",
          "author": "Zc5Gwu",
          "text": "Why does this have 100+ upvotes but only one comment?",
          "score": 38,
          "created_utc": "2026-01-26 18:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yvghg",
              "author": "Semi_Tech",
              "text": "looks like another vibe coded program in Claude code + paid upvotes just to gain visibility :P",
              "score": 15,
              "created_utc": "2026-01-27 05:52:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vcvkk",
              "author": "kevin_1994",
              "text": "this happens a lot in this sub",
              "score": 18,
              "created_utc": "2026-01-26 18:58:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vfj85",
                  "author": "Historical-Celery-83",
                  "text": "annoying... I want challenge :)",
                  "score": -50,
                  "created_utc": "2026-01-26 19:09:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2405xg",
              "author": "drallcom3",
              "text": "He paid for upvotes to promote his Github.",
              "score": 3,
              "created_utc": "2026-01-27 23:10:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v9imm",
              "author": "Historical-Celery-83",
              "text": "good question! comment it!",
              "score": -48,
              "created_utc": "2026-01-26 18:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ubcfn",
          "author": "robiinn",
          "text": "How does it differ from [bmad method](https://github.com/bmad-code-org/BMAD-METHOD) or something like that? Sounds very similar.",
          "score": 16,
          "created_utc": "2026-01-26 16:18:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ula9z",
              "author": "Historical-Celery-83",
              "text": "good question, first time I see that project, seems like is very Agile oriented. Mine is more about agent swarm",
              "score": -16,
              "created_utc": "2026-01-26 17:00:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v9igo",
          "author": "No_Afternoon_4260",
          "text": "The question is do they agree with each other?",
          "score": 10,
          "created_utc": "2026-01-26 18:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v9uzx",
              "author": "Historical-Celery-83",
              "text": "sometimes, and sometimes is a total mess. the orchestrator struggle to keep the agents on tracks, I'm trying to add more guardrails and more deterministic code.",
              "score": 7,
              "created_utc": "2026-01-26 18:45:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x9n7l",
                  "author": "Environmental-Metal9",
                  "text": "First of all, my dude, please edit away the LLMisms before posting. Itâ€™s grating to be fixing llm code all day just to come here and read more llm slop.\n\nAnd also, for cohesion, what is the actual disparity? Get the agents to agree on intent or on response? How many tokens can you waste per call?\n\nI havenâ€™t worked on anything like this EXACTLY, but Iâ€™ve worked on have to reach eventual consistency from agents running different models, and because of how my data was shaped, I was able to train a T5 model as classifier and would run that in front of every response. If the response didnâ€™t pass classification Iâ€™d just regenerate the response",
                  "score": 8,
                  "created_utc": "2026-01-27 00:14:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vg7ug",
          "author": "JellyBean504",
          "text": "This is awesome, are you familiar with steve yegge's gastown? it seems very similar.",
          "score": 3,
          "created_utc": "2026-01-26 19:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgvaq",
              "author": "Historical-Celery-83",
              "text": "no, but I'm looking at it right now, is more evolved compared to mine, I will take some idea from there. seems very interesting and more scalable compared to my solution.",
              "score": 3,
              "created_utc": "2026-01-26 19:15:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vxcfp",
          "author": "nonerequired_",
          "text": "That was exactly what I wanted to start building. What a coincidence!",
          "score": 3,
          "created_utc": "2026-01-26 20:26:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207yzc",
              "author": "epyctime",
              "text": "it took op 2 days and like 90 commits to feel comfortable enough to submit this to the public. so just open claude code and start vibing man",
              "score": 1,
              "created_utc": "2026-01-27 12:40:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1whuve",
          "author": "Far-Low-4705",
          "text": "do you have any real performance benchmarks?",
          "score": 3,
          "created_utc": "2026-01-26 21:57:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wie4c",
              "author": "Historical-Celery-83",
              "text": "No, I started to write it like 2 days ago.",
              "score": 0,
              "created_utc": "2026-01-26 22:00:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wmwbw",
                  "author": "Far-Low-4705",
                  "text": "then how do you know that this even works?",
                  "score": 7,
                  "created_utc": "2026-01-26 22:21:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xynxz",
                  "author": "jonas-reddit",
                  "text": "Maybe work on it a bit more than 2 days, I assume it was vibe coded as well, do some testing and then share when thereâ€™s a bit more to tell.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:28:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o207vp5",
          "author": "epyctime",
          "text": "\"some shit i started working on 3 days ago thats fully vibe coded slop so i can get clout on my github profile\"  \nedit: OP blocked me lol so i cant reply but this guys a fucking loon, 90 commits of claude code slop and its better than claude-flow \"The leading agent orchestration platform for Claude\" with 5.2k commits, but yeah, it's not fully implemented but yours is. \"your ignorance is the same as your ego\" then IMMEDIATELY commented \"probably better than 99% of the code that you will write in your entire life\" is the funniest thing ive ever read in my life. if I have access to opus we are coding the same my boy",
          "score": 3,
          "created_utc": "2026-01-27 12:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o240mak",
              "author": "drallcom3",
              "text": "The whole Github page reeks of someone trying to scam.",
              "score": 2,
              "created_utc": "2026-01-27 23:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20luzm",
              "author": "Historical-Celery-83",
              "text": "probably better than 99% of the code that you will write in your entire life.",
              "score": -1,
              "created_utc": "2026-01-27 13:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1w6xuu",
          "author": "segmond",
          "text": "Good job, not a new problem, microsoft first released solution for multi agent about 2 years or more.  I played a lot with it then.   Check it out, you might gain some new ideas\n\n[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)",
          "score": 6,
          "created_utc": "2026-01-26 21:09:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1we8q8",
          "author": "autodidacticasaurus",
          "text": "A few more iterations and we'll have created the Geth.\n\nIsn't that what DeepSeek does internally?",
          "score": 2,
          "created_utc": "2026-01-26 21:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1werol",
              "author": "Historical-Celery-83",
              "text": "yes, also claude code, but at least I can have control here. and use multi model for different tasks.",
              "score": 2,
              "created_utc": "2026-01-26 21:44:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wfyaj",
                  "author": "autodidacticasaurus",
                  "text": "I think that's cool. Also, it's easier to distribute across multiple cards or machines. Much more flexible.",
                  "score": 1,
                  "created_utc": "2026-01-26 21:49:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1wcdt9",
          "author": "__Maximum__",
          "text": "Why claude code? For fucks sake, why? There are open source alternatives that you can contribute to.",
          "score": 6,
          "created_utc": "2026-01-26 21:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yis59",
              "author": "rocketmonkeys",
              "text": "I'm just getting into things, what do you use?  Opencode?",
              "score": 1,
              "created_utc": "2026-01-27 04:24:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zgggt",
                  "author": "__Maximum__",
                  "text": "Opencode, openhands, vibe even gemini cli is open source.",
                  "score": 1,
                  "created_utc": "2026-01-27 08:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wu7sz",
              "author": "Historical-Celery-83",
              "text": "claude code can be used but it supports more providers, I choosed claude code because is one of my favourite. But I would like to be agnostic in the future. Already working on it.",
              "score": 1,
              "created_utc": "2026-01-26 22:55:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zgdjj",
                  "author": "__Maximum__",
                  "text": "It supports more providers than opencode?",
                  "score": 1,
                  "created_utc": "2026-01-27 08:51:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1x4o0x",
          "author": "the_ai_wizard",
          "text": "Cool diy. This approach has been tried and has failure modes just like you have experienced. It sounds brilliant but so far ultimately goes nowhere. Maybe this will be interesting to you:\n\nhttps://neurips.cc/virtual/2024/106556",
          "score": 1,
          "created_utc": "2026-01-26 23:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xfdij",
          "author": "foundrynet",
          "text": "This is the kind of infrastructure the agent space needs. Most multi-agent demos fall apart because there's no good way for agents to coordinate and remember state.\n\nOne thing I've been exploring: what if agents could earn/spend resources for completing tasks? Creates natural prioritization where high value work gets done first.",
          "score": 1,
          "created_utc": "2026-01-27 00:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215hx5",
          "author": "fugogugo",
          "text": "I just watched this video today and it looks like similar situation lol\n\nhttps://youtu.be/U7s_CaI93Mo?si=_eGWY6QG9Ni7Qyr7",
          "score": 1,
          "created_utc": "2026-01-27 15:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21twyh",
          "author": "PhotographerUSA",
          "text": "So, now no more programming mistakes on first try?",
          "score": 1,
          "created_utc": "2026-01-27 17:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22b67b",
          "author": "isopropoflexx",
          "text": "I am curious to find out how you are handling multiple agents accessing the SQLite database file concurrently? I personally love SQLite as a quick and easy way to incorporate a db into small/simple projects, as it doesn't need a \"proper\" service of its own to run. But... SQLite is not designed with concurrency in mind (same as any other \"flat file\" type storage solution), and having multiple concurrent connections typically results in errors due to db/table locks etc.",
          "score": 1,
          "created_utc": "2026-01-27 18:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e4d0s",
              "author": "Historical-Celery-83",
              "text": "Indeed I'm moving to postgres.",
              "score": 1,
              "created_utc": "2026-01-29 11:17:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ypit6",
          "author": "SeleneGardenAI",
          "text": "The voting patterns here are wild sometimes - posts with genuinely interesting architectures get buried while others mysteriously rocket up with no discussion. It's frustrating when you want to dig into the technical details.\n\nI've been experimenting with multi-agent memory sharing lately, and the biggest challenge I've found isn't the inter-agent communication itself, but preventing memory contamination between agents with different roles. When agents share context, they tend to blur their distinct personalities and functions over time. What's your approach to maintaining agent identity boundaries while still allowing meaningful information exchange? Are you using separate embedding spaces for shared vs. private memories, or handling it at the prompt level?\n\nThe \"hive mind\" concept is fascinating because it mirrors how human teams actually work - shared context with specialized roles. But LLMs weren't really designed for this kind of persistent, multi-perspective memory architecture. Would love to hear more about your implementation details, especially how you're handling memory prioritization when the shared context gets large.",
          "score": 1,
          "created_utc": "2026-01-27 05:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z5ynx",
              "author": "Historical-Celery-83",
              "text": "    Great questions - these are exactly the things I've been wrestling with.\n    \n    On memory contamination / identity boundaries: my current approach is pretty simple, honestly. Namespaces + prompt-level isolation. Each agent has a strong system prompt defining its role and capabilities, and they access shared memory via namespace filtering. The key insight is that agents don't actually \"retain\" personality across calls - they're stateless. What persists is the memory content, not the agent's perspective on it.\n    \n    So when the coder stores something like memory_store(\"auth-decision\", \"Using JWT with refresh tokens\", {namespace: \"architecture\"}), the reviewer later queries this and interprets it through its own system prompt lens. There's no embedding space separation - it's the same FTS5 index. The \"identity boundary\" is entirely at the prompt level.\n    \n    Is this ideal? Probably not. But it sidesteps the contamination problem because agents don't actually share state - they share facts. The interpretation happens fresh each time.\n    \n    On memory prioritization: hybrid approach. FTS5 with BM25 ranking for keyword relevance, optional vector search (OpenAI/Ollama embeddings) for semantic similarity, then merge with deduplication where vector results come first (higher quality), then FTS backfill. The \"prioritization\" is really just semantic match > keyword match > recency as tiebreaker.\n    \n    For large contexts I'm honestly just relying on the LLM's attention mechanism. I send relevant search results, not the entire memory. If context gets too big, that's a sign the search query wasn't specific enough.\n    \n    What I'd do differently if this were production: per-agent memory scopes (private + shared pools), decay functions for old memories, explicit \"handoff\" objects when one agent passes to another. But for an experiment, the current approach works well enough to see the patterns emerge. The agents do coordinate surprisingly well when you give them structured ways to leave breadcrumbs for each other.\n    \n    What approaches are you exploring for the contamination problem?",
              "score": -1,
              "created_utc": "2026-01-27 07:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vmotm",
          "author": "Magnus_Forsling",
          "text": "The infinite loop problem you mentioned is fascinating â€” it's basically the multi-agent equivalent of an LLM talking itself in circles. Have you experimented with any circuit-breaker patterns? Something like tracking task lineage so an agent can't pick up a task that descended from one it created?\n\nThe SQLite+FTS5 choice for shared memory is smart. Curious if you've hit scaling issues as the memory grows, or if the FTS5 indexing keeps retrieval snappy even with longer sessions.",
          "score": -4,
          "created_utc": "2026-01-26 19:40:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vsfq9",
              "author": "Historical-Celery-83",
              "text": "No, you mean an anti loop system? or something more evolved?",
              "score": 0,
              "created_utc": "2026-01-26 20:05:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1w6lii",
                  "author": "Magnus_Forsling",
                  "text": "Bit of both, actually. A basic anti-loop system catches the obvious stuff â€” \"agent A spawned task B which spawned task C which looks suspiciously like A again.\" But the more interesting patterns are subtler:\n\n1. **Semantic drift detection** â€” task descriptions mutate slightly each iteration until they're technically different but functionally identical. You'd want embedding similarity checks against the task's ancestors, not just string matching.\n\n2. **Resource exhaustion triggers** â€” if an agent has touched >N files or made >M API calls without producing a deliverable, something's probably wrong. Hard to tune but catches the \"productively going nowhere\" loops.\n\n3. **Consensus checkpoints** â€” for high-stakes tasks, require a different agent to sign off before the originating agent's subtasks can spawn their own children. Adds latency but breaks the self-reinforcing cycles.\n\nThe lineage tracking is the foundation though. Without knowing *where* a task came from, you can't catch any of these patterns. Are you storing that in the SQLite schema already?",
                  "score": -4,
                  "created_utc": "2026-01-26 21:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qoa8rp",
      "title": "The Qwen Devs Are Teasing Something",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/umvks92vcvfg1.png",
      "author": "Few_Painter_5588",
      "created_utc": "2026-01-27 10:28:56",
      "score": 302,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o21p3m0",
          "author": "rm-rf-rm",
          "text": "Thread locked - announcement is now out: https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/",
          "score": 1,
          "created_utc": "2026-01-27 17:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zvrfc",
          "author": "rerri",
          "text": "Z-Image. It's been popping up in ComfyUI PR's in recent days and then today there's this update with a hidden item added to collection:\n\nhttps://preview.redd.it/3d4oy6wxjvfg1.jpeg?width=780&format=pjpg&auto=webp&s=5c0f222d04f42b920dce8929e666f0158a6b9840",
          "score": 61,
          "created_utc": "2026-01-27 11:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zxdt0",
              "author": "Illya___",
              "text": "The question is which one though, base? edit? All of them?\n\nIs that apparent from the PRs?",
              "score": 18,
              "created_utc": "2026-01-27 11:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zyq95",
                  "author": "rerri",
                  "text": "Base T2I workflow was just added to templates today. \"Omni\" with edit capabilities has been worked on earlier.\n\nI think the template addition hints at base releasing today but whether the same model also has edit capabilities or whether that's something coming later, I'm not sure.",
                  "score": 9,
                  "created_utc": "2026-01-27 11:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zu1ri",
          "author": "nikhilprasanth",
          "text": "Most likely the Z image base",
          "score": 30,
          "created_utc": "2026-01-27 10:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2059t7",
          "author": "xandep",
          "text": "Qwen4 Next 48B A3B. I'm sure. ðŸ¥¹",
          "score": 22,
          "created_utc": "2026-01-27 12:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20yhei",
              "author": "Available-Craft-5795",
              "text": "The full Qwen 4 series would be nice. Its crazy how good they got the 0.6B mode to be.",
              "score": 11,
              "created_utc": "2026-01-27 15:02:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o208imc",
              "author": "Opening_Exit_1153",
              "text": "I hope it's, We need something deployable that can beat 4.7 flash, the 4.7 flash is a great model but a new Qwen moe that can beat it will be great!",
              "score": 5,
              "created_utc": "2026-01-27 12:44:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o215smz",
              "author": "KittyPigeon",
              "text": "Would love for that to come out",
              "score": 1,
              "created_utc": "2026-01-27 15:36:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21nk9m",
              "author": "Far-Low-4705",
              "text": "qwen 4vl 80b  \n  \n\\- this is my dream rn lol  \n\\* 80b sparse moe (perfect for my rig at Q4)  \n\\* rly good vision (if its anything like qwen 3vl, thats already good enough for me)  \n\\* better long context performance (as seen in qwen 3 next, hopefully carries over to qwen 4)  \n\\* interleaved thinking (hopefully)",
              "score": 1,
              "created_utc": "2026-01-27 16:53:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zryyl",
          "author": "ResidentPositive4122",
          "text": "Someone said something a while ago, about the chinese new year. Google says it's on the 17th this year, so it would make sense that a lot of labs want to get things out before the break. K2.5 is out, hopefully we'll get q3.5, dsv4, mm2.2 and so on.",
          "score": 22,
          "created_utc": "2026-01-27 10:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ztsvf",
              "author": "Few_Painter_5588",
              "text": "There's also some serious competition between these labs. So it makes sense they all want to get something out. \n\nQwen3 Max Thinking, Kimi K2.5.",
              "score": 9,
              "created_utc": "2026-01-27 10:53:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zwwlw",
          "author": "robberviet",
          "text": "Qwen 3.5 would be awesome. It's 6 months (?) already.",
          "score": 5,
          "created_utc": "2026-01-27 11:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ztqc3",
          "author": "Antique_Dot_5513",
          "text": "This is the image base normally, since the flow arrived on comfyui",
          "score": 9,
          "created_utc": "2026-01-27 10:52:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zv79b",
              "author": "Odd-Ordinary-5922",
              "text": "im 99% sure its the next qwen3next lineup (nvm edit: considering its tongyi lab its probably z image turbo edit",
              "score": 3,
              "created_utc": "2026-01-27 11:05:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zr5nl",
          "author": "Few_Painter_5588",
          "text": "Another tweet of interest\n\nhttps://preview.redd.it/nyc5fno6dvfg1.png?width=1194&format=png&auto=webp&s=224444e21cdee679358405965b8647dd5bd40824",
          "score": 17,
          "created_utc": "2026-01-27 10:30:22",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1zrpt5",
              "author": "MidAirRunner",
              "text": "Qwen3.5 30b A3b pls ty.",
              "score": 30,
              "created_utc": "2026-01-27 10:35:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20hoqb",
                  "author": "ahmetegesel",
                  "text": "That flash emoji put my hopes high",
                  "score": 5,
                  "created_utc": "2026-01-27 13:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zuwki",
          "author": "GreenGreasyGreasels",
          "text": "I am so over teasers and vague posting. When the release cadence was slow this was exciting, but now with new jaw dropping models dropping every weekday and and twice on Sundays this is getting tiresome and exhausting. Be more like Moonshot and Kimi K2.5 I guess. Anyone else feel the same? Or are you guys enjoying this?",
          "score": 11,
          "created_utc": "2026-01-27 11:02:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zva91",
              "author": "Odd-Ordinary-5922",
              "text": "im just happy that we are getting free models",
              "score": 28,
              "created_utc": "2026-01-27 11:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zvvhg",
              "author": "FullOf_Bad_Ideas",
              "text": "I agree it's good to filter out teases. Marketing is cheap, R&D is not, and China has a hustle B2C marketing culture.\n\nIf you sign up for Alibaba Cloud you'll have people contacting you pretty quickly. If you set up Google Cloud, AWS or Azure, you'll never be able to reach a real human being without putting down a lot of money first. They are serious about direct contact with customers.",
              "score": 2,
              "created_utc": "2026-01-27 11:10:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zutd7",
          "author": "AdventurousSwim1312",
          "text": "Qwen Next update?",
          "score": 2,
          "created_utc": "2026-01-27 11:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zxtqz",
          "author": "NoYogurtcloset4090",
          "text": "Just Z-image new models.",
          "score": 1,
          "created_utc": "2026-01-27 11:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zz2ed",
          "author": "Fast-Double-8915",
          "text": "Stand with legs apart?Â ",
          "score": 1,
          "created_utc": "2026-01-27 11:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202h90",
          "author": "Time-Teaching1926",
          "text": "I hope it's soon and not just another tease so we won't be waiting a long time again. ðŸ˜­",
          "score": 1,
          "created_utc": "2026-01-27 12:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2045br",
          "author": "Guilty_Rooster_6708",
          "text": "Z-image edit please",
          "score": 1,
          "created_utc": "2026-01-27 12:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208nxv",
          "author": "derivative49",
          "text": "these people won't rest unless they pop some bubble somewhere",
          "score": 1,
          "created_utc": "2026-01-27 12:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20bnfv",
          "author": "No_Conversation9561",
          "text": "Itâ€™s tweeted by Tongyi Lab so probably Z-Image base and edit ðŸ¤ž",
          "score": 1,
          "created_utc": "2026-01-27 13:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ghky",
          "author": "swagonflyyyy",
          "text": "Its not a new model! Its the best model! You're not wrong! You're just in the dark!",
          "score": 1,
          "created_utc": "2026-01-27 13:31:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215v6d",
          "author": "pigeon57434",
          "text": "if it was something to do with Qwen they would just post it on their alibaba\\_qwen account",
          "score": 1,
          "created_utc": "2026-01-27 15:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20kwck",
          "author": "Doct0r0710",
          "text": "I hate this teasing wankery... Why must they act like all the crypto bros combined?",
          "score": 1,
          "created_utc": "2026-01-27 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o207sdw",
          "author": "Opening_Exit_1153",
          "text": "I think GTA 6 is gonna come out before the new Qwen 30B MOE",
          "score": 1,
          "created_utc": "2026-01-27 12:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zv0d8",
          "author": "Minute_Attempt3063",
          "text": "Could it be, that they are cooking up a model that is uncensored (more then normal) and is way better at coding then Claude?\n\nOr a Deepseek-Qwen model?",
          "score": -3,
          "created_utc": "2026-01-27 11:03:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zyzq8",
          "author": "darkpigvirus",
          "text": "how about Qwen3.5 1B A350M - Thinking? Easy to train will take only less than your day Qwen Team",
          "score": -1,
          "created_utc": "2026-01-27 11:36:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql6cz7",
      "title": "Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy .   Fork/check it out! BYOR",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/hlrhml65m6fg1.gif",
      "author": "Efficient-Proof-1824",
      "created_utc": "2026-01-23 23:20:23",
      "score": 277,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1bxokd",
          "author": "Potential-Net-9375",
          "text": "What a fun project! Thanks for sharing",
          "score": 27,
          "created_utc": "2026-01-23 23:24:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c4h0d",
          "author": "ahstanin",
          "text": "Here we go, now AI playing my games. What's next?",
          "score": 19,
          "created_utc": "2026-01-24 00:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c6gvl",
              "author": "SeriousGrab6233",
              "text": "I fear with robots they are going to start eating my food too",
              "score": 14,
              "created_utc": "2026-01-24 00:11:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cxzma",
                  "author": "SmartCustard9944",
                  "text": "Thank god you said food",
                  "score": 8,
                  "created_utc": "2026-01-24 02:48:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1d6d5u",
                  "author": "philmarcracken",
                  "text": "the clanker didnt mean those kinds of chips",
                  "score": -1,
                  "created_utc": "2026-01-24 03:38:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1cd7p8",
          "author": "TheyCallMeDozer",
          "text": "Really cool idea and setup, might be nice to setup an OpenAI style route to try bigger models locally, for example with Olama or LMStudio... could be really cool to spin up a large model and see how it handels it",
          "score": 7,
          "created_utc": "2026-01-24 00:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dol8k",
              "author": "Efficient-Proof-1824",
              "text": "For sure - that's a great idea and def on the roadmap!",
              "score": 4,
              "created_utc": "2026-01-24 05:42:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1jp862",
                  "author": "Efficient-Proof-1824",
                  "text": "updated w/ OpenAI v1 endpoints - check it out! I don't have Ollama on this machine but confirmed with OpenAI and Groq.  Will confirm with Ollama and LM Studio tomorrow but LMK if anything comes up",
                  "score": 2,
                  "created_utc": "2026-01-25 02:39:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e62d2",
          "author": "Aggressive_Pea_2739",
          "text": "This would be dope even just to farm and train pokemons in those dense jungles",
          "score": 4,
          "created_utc": "2026-01-24 08:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dgcrh",
          "author": "Flame_Grilled_Tanuki",
          "text": "Is there a way to enable audio output?",
          "score": 3,
          "created_utc": "2026-01-24 04:44:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1doggn",
              "author": "Efficient-Proof-1824",
              "text": "Just added it! Click the speaker icon next to Save/Load to enable audio. There's a volume slider too. Fair warning - it does come across choppy as the emulation is also played at > 1x",
              "score": 4,
              "created_utc": "2026-01-24 05:41:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1dphx4",
                  "author": "Flame_Grilled_Tanuki",
                  "text": "Awesome. Thanks for the addition.",
                  "score": 3,
                  "created_utc": "2026-01-24 05:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1do7et",
          "author": "Niwa-kun",
          "text": "any plans to have it build a database of what strats it has used, therefore for sequential runs, it cannot use the same strats over and over?",
          "score": 5,
          "created_utc": "2026-01-24 05:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dotuk",
              "author": "Efficient-Proof-1824",
              "text": "partially in place already.  The system stores all the experiences (state, action, reward) in IndexedDB and so the policy network uses that but not so much an explicit 'check this ledger of past strats'.  Good idea tho - could log LLM plans + outcomes and inject recent failures into the prompt",
              "score": 5,
              "created_utc": "2026-01-24 05:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fm55m",
          "author": "Aggressive_Arm9817",
          "text": "Have you tried it with other models? Like GLM 4.7 Flash 30B",
          "score": 3,
          "created_utc": "2026-01-24 14:52:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ijbuo",
              "author": "Efficient-Proof-1824",
              "text": "planning on adding OpenAI endpoint support - hopefully in next day or so!",
              "score": 3,
              "created_utc": "2026-01-24 22:55:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wx6wt",
                  "author": "Aggressive_Arm9817",
                  "text": "That's gonna be very cool ðŸ‘€",
                  "score": 1,
                  "created_utc": "2026-01-26 23:10:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fu7fq",
          "author": "ds-unraid",
          "text": "Thanks for the share!  I added the ability to specify my own local OpenAI V1 compatible endpoint so I can change models via your AdvancedPanel. Currently using LM Studio as the local API.  Something to consider.  I find this super dope!",
          "score": 4,
          "created_utc": "2026-01-24 15:32:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ijkou",
              "author": "Efficient-Proof-1824",
              "text": "Hey thanks! Yes definitely I'm planning on adding support for that in the coming day or so -  excited to see what that opens up",
              "score": 2,
              "created_utc": "2026-01-24 22:57:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1iv6au",
                  "author": "ds-unraid",
                  "text": "Are you using Cursor because I had Cursor do it and it took about two minutes.  I am not advocating for AI slop and I super respect the software engineering field so if you're trying to do everything by hand, that's cool, but if you've never heard of Cursor, I highly recommend you check it out. I used to write one full stack application every 1 to 2 weeks now I do like 5 to 6 a day. I'm no different from someone who doesn't know how to code with the exception that I understand the underlying stack of technologies. Especially how things should fit together, which is the only advantage I have over someone who vibe-codes.",
                  "score": 3,
                  "created_utc": "2026-01-24 23:57:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e5xhz",
          "author": "Aggressive_Pea_2739",
          "text": "Dope",
          "score": 3,
          "created_utc": "2026-01-24 08:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f3nxb",
          "author": "IngwiePhoenix",
          "text": "This is fun but it completely did not like my german ROM. XD\n\nBut this is still pretty dope. Would love to throw a bigger model at this and try it again. :)",
          "score": 3,
          "created_utc": "2026-01-24 13:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ikbbq",
              "author": "Efficient-Proof-1824",
              "text": "Going to add support for more endpoint support! The ROM thing is interesting...so technically the prompt was specific to Pokemon Red but not language specific.  Maybe need a translation step lol",
              "score": 2,
              "created_utc": "2026-01-24 23:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ix29a",
                  "author": "IngwiePhoenix",
                  "text": "I just YOLO'd it - had no english ROM, so I fed it my german one and it... tried to do stuff. o.o\n\nThough the values in the UI seemed off (4/8 badges...?) so either my Adblock (Brave) or a different memory layout in german gen1 was the issue. Buuuuut, it DID run around and eventually got to Prof. Oak. xD It just took forever (40 min) lol.",
                  "score": 3,
                  "created_utc": "2026-01-25 00:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h6qpr",
          "author": "ctbanks",
          "text": "[https://developer.chrome.com/docs/ai/built-in](https://developer.chrome.com/docs/ai/built-in)",
          "score": 3,
          "created_utc": "2026-01-24 19:07:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ij3cg",
              "author": "Efficient-Proof-1824",
              "text": "thanks for sharing! Interesting and glad to see",
              "score": 2,
              "created_utc": "2026-01-24 22:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1egizw",
          "author": "Limp_Classroom_2645",
          "text": "looks very fun! can I ask why tensorflow instead of torch?",
          "score": 2,
          "created_utc": "2026-01-24 09:48:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmvny5",
      "title": "GLM-4.7-Flash is even faster now",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/19092",
      "author": "jacek2023",
      "created_utc": "2026-01-25 21:14:50",
      "score": 267,
      "num_comments": 98,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1q67hb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 00:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ozkks",
          "author": "jacek2023",
          "text": "https://preview.redd.it/vdyf3fwdakfg1.png?width=1600&format=png&auto=webp&s=a3fef014a14a330c67c84f6591085ad070bd1225\n\nspecial thanks to u/Remove_Ayys for making my previous post obsolete",
          "score": 96,
          "created_utc": "2026-01-25 21:15:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1p0rr4",
              "author": "coder543",
              "text": "Ok, now *that* starts to look respectable. Still worth comparing against efficient models like gpt-oss and nemotron-3-nano.\n\nEDIT: prompt processing still seems to fall off a cliff on glm-4.7-flash, I just tested it.",
              "score": 49,
              "created_utc": "2026-01-25 21:20:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1p15ha",
                  "author": "jacek2023",
                  "text": "https://preview.redd.it/gdnzc5eobkfg1.png?width=1600&format=png&auto=webp&s=f150d1956eb0e31d8825219cf637a1d525d0735d",
                  "score": 27,
                  "created_utc": "2026-01-25 21:22:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1p1j58",
                  "author": "marsxyz",
                  "text": "Yeah same here wuth vulkan back end. Let's hope it'll be resolved soon",
                  "score": 1,
                  "created_utc": "2026-01-25 21:23:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1p99m2",
              "author": "coder543",
              "text": "See my gist: [https://gist.github.com/coder543/16ca5e60aabee4dfc3351b54e8fe2a1c](https://gist.github.com/coder543/16ca5e60aabee4dfc3351b54e8fe2a1c)\n\nLinear:\n\nhttps://preview.redd.it/e1bqaqwxnkfg1.png?width=1920&format=png&auto=webp&s=3c4484b5606646c1aee564a932b072ad5782887b\n\nNemotron holds its performance extremely well due to its hybrid architecture. I don't know why the improvements for GLM-4.7-Flash don't seem to have helped the DGX Spark at all.\n\nEDIT: added Qwen3-Coder for fun. (My RTX 3090 couldn't go all the way to 50k tokens with the quant that I have.) The quants are not entirely apples to apples, but the performance curve is the main thing here, not the absolute numbers.",
              "score": 20,
              "created_utc": "2026-01-25 21:58:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1pbcnp",
                  "author": "coder543",
                  "text": "Log y:\n\nhttps://preview.redd.it/1l7ej68wnkfg1.png?width=1920&format=png&auto=webp&s=a3155d8f09d81c33dbfaaaa48353a66e5fa6ef66",
                  "score": 4,
                  "created_utc": "2026-01-25 22:06:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pad9r",
                  "author": "jacek2023",
                  "text": "[https://github.com/ggml-org/llama.cpp/pull/19092#issuecomment-3797263741](https://github.com/ggml-org/llama.cpp/pull/19092#issuecomment-3797263741)",
                  "score": 2,
                  "created_utc": "2026-01-25 22:02:44",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1py7o9",
                  "author": "rm-rf-rm",
                  "text": "Nemotron slower tok/s in the first data point? that doesnt seem right? \n\nP.S: Excellent plots! what did you use to make it?",
                  "score": 1,
                  "created_utc": "2026-01-25 23:51:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vqepp",
                  "author": "onil_gova",
                  "text": "I love the style of these graphs. What package did you use to create them?",
                  "score": 1,
                  "created_utc": "2026-01-26 19:56:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p11kw",
          "author": "ps5cfw",
          "text": "Cries in AMD GPUÂ ",
          "score": 29,
          "created_utc": "2026-01-25 21:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4u0r",
              "author": "jacek2023",
              "text": "what are your results on vulkan?",
              "score": 8,
              "created_utc": "2026-01-25 21:38:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1slae5",
                  "author": "Electronic-Fill-6891",
                  "text": "https://preview.redd.it/itxwu7um2ofg1.png?width=1600&format=png&auto=webp&s=b85dba61fff57e7d42ff29f4287f528084e112cc\n\nThis was on **ROCm**, **RX 7900XT**. Vulkan is hit-or-miss on my system.",
                  "score": 3,
                  "created_utc": "2026-01-26 10:09:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1t3iv2",
                  "author": "politerate",
                  "text": "https://preview.redd.it/t441p9bpuofg1.png?width=1500&format=png&auto=webp&s=5e49e47ac13d38537ca7540e9d897d574ccf6655\n\nQ4 K XL on ROCm and 7900XTX",
                  "score": 2,
                  "created_utc": "2026-01-26 12:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q6jq9",
              "author": "ayylmaonade",
              "text": "I was excited for a second too :(",
              "score": 6,
              "created_utc": "2026-01-26 00:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1p2d4k",
          "author": "jacek2023",
          "text": "https://preview.redd.it/rjo1w96mckfg1.png?width=1600&format=png&auto=webp&s=0b1c0de73960d311382b372c4719460e9444855a",
          "score": 9,
          "created_utc": "2026-01-25 21:27:33",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1pd0xe",
              "author": "Lazy-Pattern-5171",
              "text": "Splitting hairs but is the performance drop comparable to what you would expect if models had 2B parameter differences with also different architectures?",
              "score": 2,
              "created_utc": "2026-01-25 22:14:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pdipq",
          "author": "jacek2023",
          "text": "opencode is pretty usable right now\n\n(n\\_tokens = 43462 - after reading some docs and discussing parts of code)\n\n    slot launch_slot_: id  0 | task 3944 | processing task, is_child = 0\n    slot update_slots: id  0 | task 3944 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 45074\n    slot update_slots: id  0 | task 3944 | n_tokens = 43462, memory_seq_rm [43462, end)\n    slot update_slots: id  0 | task 3944 | prompt processing progress, n_tokens = 45074, batch.n_tokens = 1612, progress = 1.000000\n    slot update_slots: id  0 | task 3944 | prompt done, n_tokens = 45074, batch.n_tokens = 1612\n    slot init_sampler: id  0 | task 3944 | init sampler, took 9.71 ms, tokens: text = 45074, total = 45074\n    slot print_timing: id  0 | task 3944 |\n    prompt eval time =    2814.63 ms /  1612 tokens (    1.75 ms per token,   572.72 tokens per second)\n           eval time =   29352.57 ms /  1731 tokens (   16.96 ms per token,    58.97 tokens per second)\n          total time =   32167.20 ms /  3343 tokens",
          "score": 7,
          "created_utc": "2026-01-25 22:16:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1s3374",
              "author": "Primary-Debate-549",
              "text": "On what hardware? I've been trying it on a DGX spark with ollama and it's not exactly fast ... or good.",
              "score": 2,
              "created_utc": "2026-01-26 07:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23irmw",
                  "author": "Primary-Debate-549",
                  "text": "Okay I've been able to force (on the ollama side) a large context window. That really helps with the \"good\" part. It's 10x as intelligent now at least (context size 100000). Still slow.",
                  "score": 1,
                  "created_utc": "2026-01-27 21:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p2c8j",
          "author": "jacek2023",
          "text": "https://preview.redd.it/x15ii8dlckfg1.png?width=1600&format=png&auto=webp&s=318e1223e09a9424675cf7cab49e6404806a7f52",
          "score": 5,
          "created_utc": "2026-01-25 21:27:26",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1pysja",
              "author": "rm-rf-rm",
              "text": "How repeatable are the results?",
              "score": 1,
              "created_utc": "2026-01-25 23:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q3jge",
                  "author": "jacek2023",
                  "text": "llama-bench performs 5 runs by default",
                  "score": 1,
                  "created_utc": "2026-01-26 00:17:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pxj8q",
          "author": "bobaburger",
          "text": "Awesome. With default params, my tg went from 9-10 tok/s to 17-18 tok/s. Kudos to all the hard work from the llama.cpp team!!!",
          "score": 5,
          "created_utc": "2026-01-25 23:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgk25",
          "author": "jinnyjuice",
          "text": "For those who might be confused -- it's for llama.cpp. Now, llama.cpp works faster with GLM-4.7-Flash.",
          "score": 5,
          "created_utc": "2026-01-26 01:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p7d29",
          "author": "Gallardo994",
          "text": "I swear this model is somehow cursed",
          "score": 24,
          "created_utc": "2026-01-25 21:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pz7r7",
              "author": "robberviet",
              "text": "I swear unless dev invested in prepare prior to the release for 0-day support, all models are cursed.",
              "score": 17,
              "created_utc": "2026-01-25 23:56:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1pb0fo",
              "author": "Zc5Gwu",
              "text": "This happens every time a new model comes out. IDK why people are up in arms. It usually gets ironed out over time.",
              "score": 34,
              "created_utc": "2026-01-25 22:05:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qlc9g",
                  "author": "Far-Low-4705",
                  "text": "i think its cuz people think it is a common qwen 30b architecture under the hood (since there are so many) and not a new novel architecture that needs to be implemented from scratch",
                  "score": 10,
                  "created_utc": "2026-01-26 01:46:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rhcf8",
                  "author": "koflerdavid",
                  "text": "Requiring multiple bugfixes is indeed rare. But it's still less effort than Qwen3-Next required.",
                  "score": 2,
                  "created_utc": "2026-01-26 04:44:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pfi1y",
          "author": "Cool-Chemical-5629",
          "text": "Is there anything that can be done for Vulkan inference in terms of getting better speed?",
          "score": 4,
          "created_utc": "2026-01-25 22:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qvw9n",
          "author": "k0setes",
          "text": "https://preview.redd.it/rcydthh9wlfg1.png?width=1778&format=png&auto=webp&s=1d8b489002e38cd718d1b6e9409b6711f6e8ebc7",
          "score": 3,
          "created_utc": "2026-01-26 02:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s7tzj",
              "author": "jacek2023",
              "text": "But why ;)",
              "score": 1,
              "created_utc": "2026-01-26 08:06:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rfdp5",
          "author": "DOAMOD",
          "text": "I've had a lot of problems with this model, but since yesterday I've been working with it and it seems much more stable now, and I have to say I think it's very good. It's handling complex problems that are usually more the domain of 120/200b models, and it's surprising me. Of course, it's not going to be an Opus, but considering its size, its way of thinking, its capabilities, and good use of tools, etc., its improving speed, and a more up-to-date model, I can only congratulate Z for the great work.\n\nIf it continues to improve, especially the stability and performance drop at high CTX, it will be a very good model. I'm going to stick with it because I'm liking it.\n\nI haven't tried this update yet, let's see if it improves my results. I'm currently dropping to 850/75 over 35/40k(128)",
          "score": 3,
          "created_utc": "2026-01-26 04:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rmuqp",
              "author": "simracerman",
              "text": "Whatâ€™s your hardware?\n\nMy 5070 Ti is spilling into system memory, and thatâ€™s still giving me 55t/s at 20k(128)",
              "score": 1,
              "created_utc": "2026-01-26 05:20:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1smkr1",
                  "author": "jacek2023",
                  "text": "well but what's your CPU and RAM?",
                  "score": 1,
                  "created_utc": "2026-01-26 10:20:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q8fts",
          "author": "TheRealMasonMac",
          "text": "Sucks that they don't want to release the base model. Didn't take long for them to start backing out of open weight releases after going public.",
          "score": 4,
          "created_utc": "2026-01-26 00:41:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qblob",
              "author": "sxales",
              "text": "[GLM 4.7 came out last month](https://huggingface.co/zai-org/GLM-4.7)?",
              "score": 9,
              "created_utc": "2026-01-26 00:56:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qj4li",
                  "author": "TheRealMasonMac",
                  "text": "This is 4.7-Flash",
                  "score": 1,
                  "created_utc": "2026-01-26 01:35:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1t6nts",
              "author": "FullOf_Bad_Ideas",
              "text": "Yeah I feel like the curtain on Chinese open weight releases might be closing.\n\nI think there's 30% chance that GLM 5 will not be open weight.",
              "score": 3,
              "created_utc": "2026-01-26 12:56:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1qc0oj",
              "author": "huzbum",
              "text": "Would there even be a base model if they distill from a larger post trained model?",
              "score": 5,
              "created_utc": "2026-01-26 00:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qjact",
                  "author": "TheRealMasonMac",
                  "text": "Yes. There is always a base model unless they're doing something like REAP. [https://huggingface.co/zai-org/GLM-4.7-Flash/discussions/2](https://huggingface.co/zai-org/GLM-4.7-Flash/discussions/2)\n\n\\> Sorry, but we don't have plans to release the base model.",
                  "score": 4,
                  "created_utc": "2026-01-26 01:36:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rhu51",
                  "author": "koflerdavid",
                  "text": "Presumably, distillation is from a larger base model as well. Then post-training is applied. One doesn't want the model to incompletly learn the instruction tuning.",
                  "score": 2,
                  "created_utc": "2026-01-26 04:47:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1qchq6",
              "author": "Odd-Ordinary-5922",
              "text": "at least they released a model??",
              "score": 2,
              "created_utc": "2026-01-26 01:01:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qjhok",
                  "author": "TheRealMasonMac",
                  "text": "That is orthogonal to my statement. The base models for GLM-4.5/6/7 and Air are already available. How else would you interpret them suddenly not wanting to release base models?",
                  "score": 9,
                  "created_utc": "2026-01-26 01:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q9wb5",
          "author": "fallingdowndizzyvr",
          "text": "Still waiting for things to settle down before trying this. It seems like there's a new major change every day.",
          "score": 1,
          "created_utc": "2026-01-26 00:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qa1bj",
              "author": "jacek2023",
              "text": "Actually no, twice a day",
              "score": 4,
              "created_utc": "2026-01-26 00:49:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qp3ko",
          "author": "mr_zerolith",
          "text": "Hell yeah!!!!!!",
          "score": 1,
          "created_utc": "2026-01-26 02:05:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sm907",
          "author": "crantob",
          "text": "Sportscar.  Hard to drive.",
          "score": 1,
          "created_utc": "2026-01-26 10:17:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sous0",
          "author": "BeeNo7094",
          "text": "How is the vllm support? Anyone tried an 4bit awq quant yet?",
          "score": 1,
          "created_utc": "2026-01-26 10:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1soykf",
              "author": "jacek2023",
              "text": "I tried vllm and failed (got problem with context), will try in the future again, support in vllm is probably also in progress",
              "score": 1,
              "created_utc": "2026-01-26 10:41:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sphvq",
                  "author": "BeeNo7094",
                  "text": "ðŸ™‚â€â†•ï¸rocm 7.2 with llama.cpp is the only way, on my 7900xt machine",
                  "score": 1,
                  "created_utc": "2026-01-26 10:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1swcv3",
          "author": "mouseofcatofschrodi",
          "text": "this only solves the problem for the llama.cpp engine right? What about MLX? Would it be better to use a gguf model on a mac over mlx?",
          "score": 1,
          "created_utc": "2026-01-26 11:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tc8gm",
          "author": "1-a-n",
          "text": "Not faster for Blackwell",
          "score": 1,
          "created_utc": "2026-01-26 13:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tud9h",
          "author": "SatoshiNotMe",
          "text": "Still awful with Claude Code. The latest build from source did not improve this situation:\n\nOn my M1 Max Pro 64 GB, Qwen3-30B-A3B works very well at around 20 tok/s generation speed in CC via llama-server using the setup Iâ€™ve described here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nBut with GLM-4.7-flash Iâ€™ve tried all sorts of llama-server settings and I barely get 3 tok/s which is useless.\n\nThe core problem seems to be that GLM's template has thinking enabled by default and Claude Code uses assistant prefill - they're incompatible.",
          "score": 1,
          "created_utc": "2026-01-26 15:03:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tulbm",
              "author": "jacek2023",
              "text": "try opencode",
              "score": 2,
              "created_utc": "2026-01-26 15:04:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1twmwi",
                  "author": "SatoshiNotMe",
                  "text": "Prefer staying in CC and leverage my max subscription. To be clear, I'm obviously not looking to run this model for any serious coding, but more for sensitive document work, private notes, etc.\n\nGiven the gap with Qwen3-30B-A3B, there's clearly something that still needs to be fixed with llama.cpp support of glm-4.7-flash",
                  "score": 1,
                  "created_utc": "2026-01-26 15:13:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o207m6h",
          "author": "sammcj",
          "text": "Still only getting 37tk/s with llama.cpp compared to 110tk/s with vLLM on my 2x RTX3090 setup.",
          "score": 1,
          "created_utc": "2026-01-27 12:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2080cq",
              "author": "jacek2023",
              "text": "Please share vllm command with 50000 context (I use 200000 in llama.cpp)",
              "score": 1,
              "created_utc": "2026-01-27 12:41:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o239qlc",
                  "author": "sammcj",
                  "text": "Reddit craps itself when you try to share codeblocks so I've share the relevant parts in a gist for you: https://gist.github.com/sammcj/728128541109c45f3b1cecc8be20955f",
                  "score": 1,
                  "created_utc": "2026-01-27 21:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q1l2f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-26 00:07:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1va6a2",
              "author": "johnnyApplePRNG",
              "text": "Skill issue.",
              "score": 1,
              "created_utc": "2026-01-26 18:46:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qof1u",
          "author": "Loud_Economics4853",
          "text": "Itâ€™s constantly updatedâ€”we users are so lucky!",
          "score": 1,
          "created_utc": "2026-01-26 02:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r6sd8",
          "author": "PathfinderTactician",
          "text": "What's the point of all this? The speed already seems to be OK, but the output is terrible. This is running Q8 with the supposedly fixed Unsloth quant, FA=off, and also using the override-kv (deepseek2...) argument. The model makes basic errors, and loops even with 16k context.",
          "score": -3,
          "created_utc": "2026-01-26 03:38:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rigfz",
              "author": "viperx7",
              "text": "to be honest i am having  a completely different experience. this model is no opus but it\\`s able to do things fine i have tested even the Q4 upto 70K context and it holds the Q8 is also working nicely\n\nI am still running tests to see how smart dumb or capable this is compared to nemotron or qwen30b moes\n\ni think something might be off with your system or setup. as the model stands now it is very much usable\n\ncan you give some example tasks for which it is looping because in my case this loops almost never less than the nemotron (i have used this extensively with opencode Q4 / Q6 and Q8)",
              "score": 6,
              "created_utc": "2026-01-26 04:51:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1rit3q",
              "author": "Odd-Ordinary-5922",
              "text": "the fa is supposed to be on and you dont need to use override kv. bro update your llama",
              "score": 7,
              "created_utc": "2026-01-26 04:53:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1paqri",
          "author": "wapxmas",
          "text": "shame for [z.ai](http://z.ai) team",
          "score": -15,
          "created_utc": "2026-01-25 22:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pnbgx",
              "author": "__Maximum__",
              "text": "This is llama.cpp issue, right?",
              "score": 10,
              "created_utc": "2026-01-25 23:00:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1pqspy",
                  "author": "boredinballard",
                  "text": "I think it's mainly an issue with the z.ai team not optimizing for all the necessary tweaks before release. Not to glaze openAI, but they did do a lot of work before releasing gpt-oss to make sure it worked with llama and what not. Still wasn't great at launch but within a few hours the necessary tweaks and such were done.",
                  "score": -5,
                  "created_utc": "2026-01-25 23:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qo3ri5",
      "title": "Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/0qp4pz0fbtfg1.jpeg",
      "author": "Delicious_Focus3465",
      "created_utc": "2026-01-27 04:26:25",
      "score": 259,
      "num_comments": 48,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1yn8pr",
          "author": "Pianocake_Vanilla",
          "text": "Qwen 4B 2507 is my favourite model for small and easy tasks. It punches WAY above its weight. Nice to see some finetunes of it.Â ",
          "score": 27,
          "created_utc": "2026-01-27 04:53:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ypjtv",
              "author": "Delicious_Focus3465",
              "text": "Thank you. You should also try the model to see how good it is compared to Qwen 4B 2507.",
              "score": 9,
              "created_utc": "2026-01-27 05:09:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21eufr",
                  "author": "1000yroldenglishking",
                  "text": "Do you recommend this for local summarization/notes of meeting transcriptions?",
                  "score": 1,
                  "created_utc": "2026-01-27 16:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21emag",
              "author": "1000yroldenglishking",
              "text": "Is it good for local summarization/notes of meeting transcriptions?",
              "score": 2,
              "created_utc": "2026-01-27 16:15:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23lfqk",
                  "author": "Pianocake_Vanilla",
                  "text": "Yes, since it also supports really high context natively, there should be very minor context degradation even in longer summaries. Its one of the best in less than 8B parameter models.Â ",
                  "score": 2,
                  "created_utc": "2026-01-27 21:59:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o22fscw",
                  "author": "sxales",
                  "text": "Yes, but I've found Granite4.0 H Micro (3b) to be more detailed.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:54:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ykr5h",
          "author": "KvAk_AKPlaysYT",
          "text": "Instruct beats thinking 2507?!\n\nBenchmaxxing?? What got you guys such good results?\n\nI see Guf-Gufs!",
          "score": 24,
          "created_utc": "2026-01-27 04:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ylz6z",
              "author": "Delicious_Focus3465",
              "text": "Hi, no benchmaxxing here, itâ€™s just a lot of pretraining and distillation, like any other team. Weâ€™ll be releasing a technical report soon.",
              "score": 24,
              "created_utc": "2026-01-27 04:45:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zxf2o",
                  "author": "woadwarrior",
                  "text": "Pretraining on top of Qwen3-4B-Instruct-2507?",
                  "score": 8,
                  "created_utc": "2026-01-27 11:23:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2058ai",
                  "author": "KvAk_AKPlaysYT",
                  "text": "Looking forward to it, thank you!",
                  "score": 2,
                  "created_utc": "2026-01-27 12:22:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zg9oj",
              "author": "rm-rf-rm",
              "text": "Sorry but Im tired of these guys.. their previous releases have been utter crap and is reflected in its zero adoption rate in the community, I have no faith that those benchmarks are even real, and if they are, its most likely from benchmaxxing. \n\nShow me actual results with at least demos of Jan vs Qwen side by side. I'm going to group this team under the hype cycle grifters untl proven otherwise.",
              "score": 8,
              "created_utc": "2026-01-27 08:50:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjsfp",
                  "author": "Zestyclose-Shift710",
                  "text": "dude these \"hype cycle grifters\" make and maintain their own AI frontend and a llama cpp repo fork with binaries compiled for a ton more architectures\n\nthose are great contributions already making them not grifters",
                  "score": 30,
                  "created_utc": "2026-01-27 09:23:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yvlj7",
          "author": "Doggo0111",
          "text": "Pretty cool release. I'm trying this one out. Looking forward to your next model.",
          "score": 7,
          "created_utc": "2026-01-27 05:53:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yyzor",
              "author": "Delicious_Focus3465",
              "text": "Thank you for supporting us.",
              "score": 3,
              "created_utc": "2026-01-27 06:19:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zjww4",
          "author": "Aromatic-Document638",
          "text": "Great work. Iâ€™m also fineâ€‘tuning Qwen3-4B-2507 for my own specialized use case, but Iâ€™m not getting satisfying results yet. I look forward to more of your great sharing in the future.",
          "score": 8,
          "created_utc": "2026-01-27 09:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkay9",
              "author": "Kooky-Somewhere-2883",
              "text": "Hi It's Alan, from the team.\n\nI think one thing I can share now is that for small model, the priority should always be avoiding catastrophic forgetting at any cost - everything else come second - then you will be able to improve the baseline and the specific usecase you're finetuning for.\n\nSo data quality (rather than quantity) + RL (good rewards method) are utmost important. \n\nHope the tip help! Thank you for trying out model out, also.",
              "score": 8,
              "created_utc": "2026-01-27 09:27:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zp2xr",
          "author": "Qxz3",
          "text": "Looking forward for the coding finetune! Qwen3-4b is amazing for those of us on 8GB VRAM, and any improvements on it would be very welcome.",
          "score": 5,
          "created_utc": "2026-01-27 10:11:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zya0j",
          "author": "jedisct1",
          "text": "\"Building on this base, Jan-Code, a code-tuned variant, will be released soon.\" Looking forward to it!",
          "score": 5,
          "created_utc": "2026-01-27 11:30:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228n91",
          "author": "Desperate_Shelter_32",
          "text": "# mlx-community/Jan-v3-4B-base-instruct-8bit\n\n    temperature: 0.7\n    top_p: 0.8\n    top_k: 20\n\n# prompt\n\n    write a update product form in php\n    the database is sqlite, shop.db\n    the table is already exists\n    update products table\n    you get the id from URL query\n    the fields are name, price, foo, bar, baz, qux, hello, description, isEdible\n    only output the code in plain text\n\n# response\n\n    <?php\n    // Database connection\n    $database = 'shop.db';\n    $db = new SQLite3($database);\n    \n    // Check if ID is provided in URL query\n    if (!isset($_GET['id'])) {\n        die(\"ID parameter is missing.\");\n    }\n    \n    $id = intval($_GET['id']);\n    \n    // Validate ID\n    if ($id <= 0) {\n        die(\"Invalid product ID.\");\n    }\n    \n    // Check if form is submitted\n    if ($_SERVER['REQUEST_METHOD'] === 'POST') {\n        // Sanitize and validate input data\n        $name = trim($_POST['name'] ?? '');\n        $price = floatval($_POST['price'] ?? 0);\n        $foo = trim($_POST['foo'] ?? '');\n        $bar = trim($_POST['bar'] ?? '');\n        $baz = trim($_POST['baz'] ?? '');\n        $qux = trim($_POST['qux'] ?? '');\n        $hello = trim($_POST['hello'] ?? '');\n        $description = trim($_POST['description'] ?? '');\n        $isEdible = isset($_POST['isEdible']) ? 1 : 0;\n    \n        // Validate required fields\n        if (empty($name)) {\n            die(\"Product name is required.\");\n        }\n    \n        // Update product in database\n        $stmt = $db->prepare(\"UPDATE products SET name = ?, price = ?, foo = ?, bar = ?, baz = ?, qux = ?, hello = ?, description = ?, isEdible = ? WHERE id = ?\");\n        $result = $stmt->execute([\n            $name,\n            $price,\n            $foo,\n            $bar,\n            $baz,\n            $qux,\n            $hello,\n            $description,\n            $isEdible,\n            $id\n        ]);\n    \n        if ($result) {\n            header(\"Location: product.php?id=$id\");\n            exit();\n        } else {\n            die(\"Error updating product.\");\n        }\n    }\n    \n    // Fetch current product data for display\n    $stmt = $db->prepare(\"SELECT * FROM products WHERE id = ?\");\n    $result = $stmt->execute([$id]);\n    $product = $result->fetchArray(SQLITE3_ASSOC);\n    \n    // Check if product exists\n    if (!$product) {\n        die(\"Product not found.\");\n    }\n    \n    // Generate form HTML (only output the form code)\n    ?>\n    <form method=\"post\" action=\"\">\n        <label>Name:</label>\n        <input type=\"text\" name=\"name\" value=\"<?= htmlspecialchars($product['name']) ?>\" required><br>\n    \n        <label>Price:</label>\n        <input type=\"number\" step=\"0.01\" name=\"price\" value=\"<?= htmlspecialchars($product['price']) ?>\" required><br>\n    \n        <label>Foo:</label>\n        <input type=\"text\" name=\"foo\" value=\"<?= htmlspecialchars($product['foo']) ?>\"><br>\n    \n        <label>Bar:</label>\n        <input type=\"text\" name=\"bar\" value=\"<?= htmlspecialchars($product['bar']) ?>\"><br>\n    \n        <label>Baz:</label>\n        <input type=\"text\" name=\"baz\" value=\"<?= htmlspecialchars($product['baz']) ?>\"><br>\n    \n        <label>Qux:</label>\n        <input type=\"text\" name=\"qux\" value=\"<?= htmlspecialchars($product['qux']) ?>\"><br>\n    \n        <label>Hello:</label>\n        <input type=\"text\" name=\"hello\" value=\"<?= htmlspecialchars($product['hello']) ?>\"><br>\n    \n        <label>Description:</label>\n        <textarea name=\"description\"><?= htmlspecialchars($product['description']) ?></textarea><br>\n    \n        <label>Is Edible:</label>\n        <input type=\"checkbox\" name=\"isEdible\" <?= $product['isEdible'] ? 'checked' : '' ?>><br>\n    \n        <button type=\"submit\">Update Product</button>\n    </form>\n\nso far so good ðŸ‘ðŸ»",
          "score": 6,
          "created_utc": "2026-01-27 18:24:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24kbl5",
              "author": "Kooky-Somewhere-2883",
              "text": "hi it's Alan from the team,\n\nWould recommend to plug an MCP (i would recommend exa code search) into the model for best performance on coding as well.",
              "score": 2,
              "created_utc": "2026-01-28 00:53:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yshpa",
          "author": "bobaburger",
          "text": "Nice! I tried to ask some trivial questions about one of my github project, on [chat.jan.ai](http://chat.jan.ai), it's kind of a mixed feeling.\n\nOn one side, the model correctly uses the search tool and reads the code to explain the flow, which is good. On the other side, the tool calls sometimes fail, and sometimes it gives some weird lines like \"This project is not associated with Menlo Research\". Maybe due to the system prompt on the web chat.\n\nIf the model works in Claude Code, I think it could be a very useful code search/Q&A tool to assist me with day-to-day coding.\n\nLooking forward to Jan-Code!",
          "score": 9,
          "created_utc": "2026-01-27 05:30:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yt7ro",
              "author": "Psychological_Cry920",
              "text": "Hi u/bobaburger, this is Louis from Jan team. Our desktop app has been updated to now support Claude Code connecting to local models through the /v1/messages endpoint. Please give it a try [https://www.jan.ai](https://www.jan.ai) or [https://github.com/janhq/jan/releases/tag/v0.7.6](https://github.com/janhq/jan/releases/tag/v0.7.6)",
              "score": 7,
              "created_utc": "2026-01-27 05:35:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20yo8f",
          "author": "No-Statistician-374",
          "text": "\"**Jan-Code**Â (finetuned of Jan-v3-4B-base-instruct)\" can't wait to try that... Any idea when it will (roughly) be released? Other than 'soon'? :D Also, will that be FIM capable (like Qwen3-Coder is)?",
          "score": 3,
          "created_utc": "2026-01-27 15:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24rt3v",
              "author": "Delicious_Focus3465",
              "text": "Please stay tuned, we are working on it.",
              "score": 2,
              "created_utc": "2026-01-28 01:32:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yjf60",
          "author": "Delicious_Focus3465",
          "text": "https://preview.redd.it/yvc6wehmktfg1.png?width=1942&format=png&auto=webp&s=2bee4ecd99ca4ea556f611794b56a4537ed28e92\n\nother general benchmark results:\n\nDemo: You can also try the Demo atÂ [chat.jan.ai](https://chat.jan.ai/). Look forÂ **Jan v3 Nano.**",
          "score": 10,
          "created_utc": "2026-01-27 04:28:33",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1ypb0t",
          "author": "TomLucidor",
          "text": "Now get SWE-Rebench and LiveBench to see if they can still stand on their own two feet.",
          "score": 6,
          "created_utc": "2026-01-27 05:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yqqpf",
              "author": "Delicious_Focus3465",
              "text": "Running full SWE-Rebench/LiveBench takes a while, though, so weâ€™re saving these benchmark runs for our upcomingÂ **Jan-Code**Â model.   \nWhileÂ thisÂ model is focused onÂ **General**Â use, we specifically highlightedÂ **Aider**Â because the score jumped significantly after finetuning. Consider it a preview of what's coming!",
              "score": 6,
              "created_utc": "2026-01-27 05:17:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1yrpei",
                  "author": "TomLucidor",
                  "text": "The goal of SWE-Rebench or LiveBench is essentially a \"moving target\" to see if models can adapt to tasks they can't pre-learn. Ideally doing a subset of them to examine agentic coding ability would be useful to compare against 30B models.",
                  "score": 4,
                  "created_utc": "2026-01-27 05:24:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z23bm",
          "author": "helloworld1101",
          "text": "Thank you for sharing. Do you have the technique report on continual pre-training and RL?",
          "score": 4,
          "created_utc": "2026-01-27 06:44:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z3yew",
              "author": "Delicious_Focus3465",
              "text": "Yes, please stay tuned, the technical report is coming out soon.",
              "score": 6,
              "created_utc": "2026-01-27 07:00:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zo0zc",
          "author": "NoobMLDude",
          "text": "It says itâ€™s:â€œmodel trained with continual pre-training and RLâ€.\nWhat base model is it continually pretrained on?",
          "score": 3,
          "created_utc": "2026-01-27 10:02:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zop6f",
              "author": "Delicious_Focus3465",
              "text": "We built on top of Qwen3-4B-Instruct-2507.",
              "score": 3,
              "created_utc": "2026-01-27 10:08:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o20046x",
                  "author": "NoobMLDude",
                  "text": "Ok Interesting. Thanks for sharing.   \nAs I understand, Continued Pretraining on a INstruct model (which has seen Post-training) is not usually recommended due to Catastrophic Forgetting.  \nHow do you manage to do Continual Pretraining on top an Instruct model ?",
                  "score": 2,
                  "created_utc": "2026-01-27 11:45:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21l48c",
          "author": "citrusalex",
          "text": "Does it support tools?",
          "score": 3,
          "created_utc": "2026-01-27 16:43:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21mx9a",
              "author": "Background_Tea_3806",
              "text": "Hi Itâ€™s Alex from Jan team, we built on top of Qwen3-4B-Instruct-2507. So Jan v3 4B inherits capabilities of the base model, including tools use.",
              "score": 2,
              "created_utc": "2026-01-27 16:51:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24iowe",
          "author": "DOAMOD",
          "text": "v1 was a good model for me, I liked it quite a bit back in the day. I think v2 aged poorly and is a model that, in my opinion, focuses too much on its own thinking without offering much practical use (perhaps I didn't use it to its full potential). But I think v3 could become a good line of interesting models, especially coder, and particularly those 30b. Thank you for continuing to contribute your work by expanding the options.",
          "score": 3,
          "created_utc": "2026-01-28 00:44:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24k2dw",
              "author": "Kooky-Somewhere-2883",
              "text": "Hi it's Alan from the team,\n\nFor v2 it was just a research into how far we can push a small model into long horizon task + multimodal (aka robotics related). We do recognize that that ability might not come as useful for day-to-day usage.\n\nv3 will have both search and code model since we make a new base and use that base, which I think will be very interesting. Thank you for trying out the model.",
              "score": 5,
              "created_utc": "2026-01-28 00:51:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yndy1",
          "author": "nuclearbananana",
          "text": "Obligatory where Nanbeige",
          "score": 5,
          "created_utc": "2026-01-27 04:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zad5w",
          "author": "Specialist_Hand6352",
          "text": "No comparison with nanbeige4-3b",
          "score": 0,
          "created_utc": "2026-01-27 07:55:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zci7t",
              "author": "pgrijpink",
              "text": "No coding bench available though?",
              "score": 1,
              "created_utc": "2026-01-27 08:15:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmlpjp",
      "title": "Internet blackout and Local LLMs",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/",
      "author": "DunderSunder",
      "created_utc": "2026-01-25 15:15:05",
      "score": 258,
      "num_comments": 78,
      "upvote_ratio": 0.91,
      "text": "Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).\n\n  \nMeanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1mooh4",
          "author": "jacek2023",
          "text": "Yes that's a very good argument for local LLMs. People born after 2000 (or 1990?) assume that Internet services are always available. And cloud models are always free.",
          "score": 180,
          "created_utc": "2026-01-25 15:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n82yc",
              "author": "RoyalCities",
              "text": "Having a local LLM and a Meshtastic node are basically a must in today's society.\n\nOne for local knowledge\n\nAnd the other to still communicate with friends and family across the city even if the entire internet and cell towers go down.",
              "score": 39,
              "created_utc": "2026-01-25 16:43:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ogsom",
                  "author": "mycall",
                  "text": "> Meshtastic\n\nLoRA is [susceptible to jamming](https://pmc.ncbi.nlm.nih.gov/articles/PMC9269689/) with relative ease.",
                  "score": 13,
                  "created_utc": "2026-01-25 19:53:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1t1zxa",
                  "author": "DryDevelopment8584",
                  "text": "What are some good resources to learn about meshtastic nodes?",
                  "score": 1,
                  "created_utc": "2026-01-26 12:25:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nhpaf",
              "author": "Wide_Egg_5814",
              "text": "Also you can download Wikipedia it's around 100 gb to download all of Wikipedia",
              "score": 10,
              "created_utc": "2026-01-25 17:24:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1nakb6",
              "author": "NeverLookBothWays",
              "text": "It's getting much tougher to build a rig that can handle quality LLMs.  But for those who can, it's like having a reasoning encyclopedia of knowledge at your disposal. (of course, with hallucinations, but definitely workable)",
              "score": 3,
              "created_utc": "2026-01-25 16:54:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mpktm",
              "author": "Ztoxed",
              "text": "truth",
              "score": 3,
              "created_utc": "2026-01-25 15:21:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mz16u",
              "author": "ivoras",
              "text": "OTOH, if the Internet is down, what use is a local LLM?\n\nIf you use it as an encyclopaedia, it needs to be a gigantic model. If it's used just as a conversation partner, that won't help anyone that much,",
              "score": -10,
              "created_utc": "2026-01-25 16:04:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mzxhl",
                  "author": "jacek2023",
                  "text": "Do you assume there is no actual knowledge in 12B model?\n\nI always ask models about specific music video and they are always wrong. That happens for 4B and some cloud models.  But if you want some common knowledge even 4B is useful. Models are basically a compressed knowledge.",
                  "score": 9,
                  "created_utc": "2026-01-25 16:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1nbvhm",
                  "author": "Mr_Back",
                  "text": "I used Qwen model (around 230 billion parameters) in a quantized format (Q2) and the full-sized GPT-OSS 120B model when I was without internet access for several days. My city had a mobile internet outage. I used these models to troubleshoot and configure my home local network, as well as to set up my NAS and other devices, preparing them for connection to a wired internet connection.",
                  "score": 3,
                  "created_utc": "2026-01-25 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1neuf0",
          "author": "def_not_jose",
          "text": "ChatGPT being whitelisted is hilarious, there is little doubt that they share user data with intelligence agencies",
          "score": 62,
          "created_utc": "2026-01-25 17:12:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4lgp",
              "author": "IrisColt",
              "text": "If it wasnâ€™t obvious before, it is now.",
              "score": 14,
              "created_utc": "2026-01-25 21:37:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qagec",
                  "author": "Comas_Sola_Mining_Co",
                  "text": "Ah yes, the Iranian revolutionary guards must be secretly behind Sam Altman and openai. That's why it's one of the three websites which op claims Iranians are allowed to access. Good job we have you around to figure out what the intelligence agencies are up to",
                  "score": -9,
                  "created_utc": "2026-01-26 00:51:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1rccpk",
              "author": "SkyFeistyLlama8",
              "text": "\"How do I crush a rebellion?\"\n\nI wouldn't put it past the people in charge in Iran to use LLMs to formulate some kind of repression framework.",
              "score": 1,
              "created_utc": "2026-01-26 04:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rnfvt",
                  "author": "crantob",
                  "text": "How do they instigate one is the relevant topic to study.",
                  "score": 1,
                  "created_utc": "2026-01-26 05:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nfgtm",
          "author": "a_beautiful_rhind",
          "text": "Even in matters of life or death, OpenAI gonna uphold it's censorship. I bet everyone feels real \"safe\" right now.",
          "score": 27,
          "created_utc": "2026-01-25 17:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o0hs8",
          "author": "fallingdowndizzyvr",
          "text": "Have you thought about downloading Wikipedia? They make it easy by making dumps available. I would trust that more than an LLM when having to look things up.",
          "score": 25,
          "created_utc": "2026-01-25 18:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mu98u",
          "author": "porzione",
          "text": "If google works - try AI mode on main page, it is some kind of Gemini, you can ask about current events, vpn, DPI piercing and so on. Small 8B local models have a very limited technical knowledge, if you need to find a way how to pierce the blocks.   \nDo not tell Deepseek anything about politics and censorship - ask it pure technical, neutral questions to avoid guardrails.",
          "score": 21,
          "created_utc": "2026-01-25 15:43:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mwua1",
              "author": "DunderSunder",
              "text": "The AI mode never works if you go with iranian IP. I don't know if it's because of government or Google but like you can't disable safe search too which is gov's fault.",
              "score": 8,
              "created_utc": "2026-01-25 15:54:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1n16ma",
                  "author": "porzione",
                  "text": "Maybe this is a Google's thing not to enable ai mode for some countries. If it's possible download bigger models - just in case, 14B qwen or latest ministral - they'll be slower with you VRAM but smarter. And be safe!",
                  "score": 1,
                  "created_utc": "2026-01-25 16:13:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ngt20",
              "author": "woswoissdenniii",
              "text": "Zim",
              "score": 1,
              "created_utc": "2026-01-25 17:20:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q4kyw",
          "author": "hilarino",
          "text": "Do you have working phone lines able to make international calls again? If so, with a computer and a 56k modem you can get a (very slow and probably expensive) connection to the rest of the Internet via dial-up.",
          "score": 5,
          "created_utc": "2026-01-26 00:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1stanq",
              "author": "DunderSunder",
              "text": "oh right, in the first week international calls were not possible. But that is crazy if it works.",
              "score": 4,
              "created_utc": "2026-01-26 11:18:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tqnrt",
                  "author": "hilarino",
                  "text": "Can you get a 56k modem + usb adaptor? Are international calls possible today? Let me know if I can help!",
                  "score": 1,
                  "created_utc": "2026-01-26 14:45:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23sequ",
              "author": "RhubarbSimilar1683",
              "text": "Most voice calls and all international ones are VoIP and thus can't run a 56k modem because the audio compression introduces distortion to the signal. Maybe slower ones work",
              "score": 1,
              "created_utc": "2026-01-27 22:32:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27gzrq",
                  "author": "hilarino",
                  "text": "I have tried years ago with a modem and a linksys ATA and it worked on many different ISP's on several countries. Depending on the route/destination country/ISP, the modems negotiated different speeds. Can't remember if those were up to 33.6kbps, 28.8kbps or 14.4kbps, but it was more than 9600 baud.",
                  "score": 1,
                  "created_utc": "2026-01-28 13:04:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r8o0v",
          "author": "Defiant-Snow8782",
          "text": "To what extent is Google unblocked? Can you use a VPN hosted on GCP? \n\nWhat about [HTTP injector](https://play.google.com/store/apps/details?id=com.evozi.injector)? I used to use it as a teenager to get free internet by pretending the traffic was going to the mobile operator's website (which is free and unlimited) when really it was an SSH tunnel that let me access anything",
          "score": 5,
          "created_utc": "2026-01-26 03:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sti7o",
              "author": "DunderSunder",
              "text": "Yeah there are some weird methods sending http over dns or ssh with that app or similar ones. They are slow though.",
              "score": 1,
              "created_utc": "2026-01-26 11:20:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1moxsq",
          "author": "wingwing124",
          "text": "I understand what you are getting at, but unfortunately I don't think an uncensored local model will help you reach out past the censorship. \n\nPresumably the reason chatgpt is able to Google things for you is because you're able to reach the OpenAI servers, which are not subject to the same censorship due to geographic location. Even an uncensored local model would only be able to reach what your local machine is able to reach.",
          "score": 12,
          "created_utc": "2026-01-25 15:18:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msc2t",
              "author": "DunderSunder",
              "text": "There are techniques which use those whitelisted IPs to access other websites. like you could use some APIs of google to do some weird shenanigans. Chinese are experts in bypassing these limits because of the great firewall, most tools are developed by them. Chatgpt won't give you any script or solution in these cases.",
              "score": 11,
              "created_utc": "2026-01-25 15:34:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1mq2db",
              "author": "PollinosisQc",
              "text": "Yeah it's basically doing what a VPN does but with extra steps",
              "score": 1,
              "created_utc": "2026-01-25 15:24:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mq7ev",
              "author": "Ztoxed",
              "text": "This as a new person is something I have been pondering. I am studying models. And even hope to build one. But realized that I am just like ChatGBT or others that are online. Most only allow data in a certain time from. LLM's only can get and update by going off local pretty much so data could stale over time.  \nI have not read enough to understand how that is being looked at.",
              "score": 1,
              "created_utc": "2026-01-25 15:24:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mrzfe",
                  "author": "-illusoryMechanist",
                  "text": "*ChatGPT . (The gpt stands for generative pretrainted transformer.)",
                  "score": 3,
                  "created_utc": "2026-01-25 15:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mrecx",
              "author": "Zealousideal_Nail288",
              "text": "Well unless it gets access to internet a local llm is has usefull has a bok or calculatorÂ \n\n\nIt can help with local problems but it cant reach outside or get news besides what it got during its last update",
              "score": 1,
              "created_utc": "2026-01-25 15:30:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pro3c",
          "author": "Witty_Mycologist_995",
          "text": "First things first, try using Tor to connect to clearnet, then reach out for more help from there.",
          "score": 5,
          "created_utc": "2026-01-25 23:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mxrk4",
          "author": "_IsNull",
          "text": "Why not just use vpn to access what you need? The Chinese been using vpn to access beyond the great firewalls for decades",
          "score": 2,
          "created_utc": "2026-01-25 15:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1myyg3",
              "author": "MelodicRecognition7",
              "text": "I'm afraid there is whitelisting in Iran not blacklisting like in China. In China you can not open some specific sites but everything else is allowed, in Iran you can not open everything except a few specific sites. And if your supposed VPN server's IP address is not in the allowed IP ranges then you just would not be able to connect to it.",
              "score": 16,
              "created_utc": "2026-01-25 16:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n47wi",
                  "author": "DunderSunder",
                  "text": "At the beginning of protests we didn't have google, and local search engines are garbage. right now, situation is slightly better than previous weeks and few datacenters have internet access and some people set up vpns so others can connect through that but most of them just stop working after an hour or so, so more vpn configurations are made every minute. overall very frustrating experience, unreliable and very slow, like good luck finding anything close to 1MB/s.",
                  "score": 7,
                  "created_utc": "2026-01-25 16:26:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nhg9h",
          "author": "pas_possible",
          "text": "You can certainly use chatgpt to communicate with the outside world because chatGPT is basically an agent now, you can send text via mcp",
          "score": 2,
          "created_utc": "2026-01-25 17:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oaxbb",
          "author": "dank_shit_poster69",
          "text": "try downloading a Dolphin model, I think those are uncensored?",
          "score": 1,
          "created_utc": "2026-01-25 19:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qux0o",
          "author": "PetrichorShark",
          "text": "In addition to downloading Wikipedia, you can use Kiwix to download, search, and browse a bunch of StackExchange sites (including various techy ones that might help you get around the censorship, like [Tor Q&A](https://browse.library.kiwix.org/viewer#tor.stackexchange.com_en_all_2025-12/questions)), the prepper resources [here](https://library.kiwix.org/?lang=eng#lang=eng&tag=preppers), various wikibooks, etc.",
          "score": 1,
          "created_utc": "2026-01-26 02:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rl1g1",
          "author": "autoencoder",
          "text": "Check out Heretic models:\n\nhttps://github.com/p-e-w/heretic\n\nhttps://huggingface.co/collections/p-e-w/the-bestiary",
          "score": 1,
          "created_utc": "2026-01-26 05:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1teao0",
          "author": "hauhau901",
          "text": "I've uncensored the newest GLM 4.7 Flash myself, advise you try it. Should have no issues using tools.\nhttps://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced",
          "score": 1,
          "created_utc": "2026-01-26 13:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25pyi2",
          "author": "anti-fascist-dude",
          "text": "I dunno man, after seeing masked terrorists shooting your own people. Makes you think where these terrorists came from. I hope you are doing great despite all of this.",
          "score": 1,
          "created_utc": "2026-01-28 04:42:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1slheg",
          "author": "Orihara-Izaya",
          "text": "Oy vey, Oy vey!!!\nHasbara troll, Unit 8200 bot, sayanim or only a golddigger for 7.000 $/post from Netanyahoo???\nAsking for Iranian friend...",
          "score": -1,
          "created_utc": "2026-01-26 10:10:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nnxnh",
          "author": "kaisurniwurer",
          "text": "I'm not an expert, but try circumventing DNS. Try using direct addresses to sites instead. A little naive, but perhaps...\n\nSecond approach is mesh wifi and internal network.\n\nAs for the topic, I agree. Just having access to an uncensored model, as big as you can fit, including partial offload and hybrid inference, no matter the speed, can help you someday.\n\nIn the worse case, you can even pool GPUs and hardware in general with neighbors to get it a little better too.",
          "score": 0,
          "created_utc": "2026-01-25 17:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23spsm",
              "author": "RhubarbSimilar1683",
              "text": "Circumventing DNS might work",
              "score": 1,
              "created_utc": "2026-01-27 22:34:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq6n3t",
      "title": "GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/uf2m03ak2agg1.png",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-29 11:58:22",
      "score": 252,
      "num_comments": 69,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2eo3cr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 13:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ehseu",
          "author": "Seaborgg",
          "text": "This post has me the feeling that you were hoping I didn't read the text in the image.",
          "score": 99,
          "created_utc": "2026-01-29 12:53:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fcd94",
              "author": "ObsidianNix",
              "text": "Thas how you know they didnt either, most likely just a bot on Reddit. Maybe another case study?",
              "score": 18,
              "created_utc": "2026-01-29 15:31:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2etr3k",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2026-01-29 14:00:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eu516",
                  "author": "eli_pizza",
                  "text": "What do you think â€œbotâ€ means?",
                  "score": 6,
                  "created_utc": "2026-01-29 14:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2edemx",
          "author": "gscjj",
          "text": "> half are agent frameworks \n\nI only see one agent framework here and itâ€™s by Microsoft. Other that I see RAG tooling, model(NanoGPT, Grok), model cli for code (Kimi), browser API",
          "score": 78,
          "created_utc": "2026-01-29 12:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ee0zs",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -34,
              "created_utc": "2026-01-29 12:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2efzp6",
                  "author": "gscjj",
                  "text": "Theyâ€™re all AI related, theyâ€™re not all agentic and only is a framework",
                  "score": 36,
                  "created_utc": "2026-01-29 12:41:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2egqlo",
                  "author": "macumazana",
                  "text": "so, youre comparing a model, rag, skills as prompt and tools? \n\nwhy dont you go further and compare a laptop a programming language and a human hand?",
                  "score": 32,
                  "created_utc": "2026-01-29 12:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ebp1v",
          "author": "Pretend-Pangolin-846",
          "text": "Off-topic, but what are claude skills?",
          "score": 26,
          "created_utc": "2026-01-29 12:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eci2z",
              "author": "adam444555",
              "text": "A fruitful word for the on-demand system prompt.",
              "score": 67,
              "created_utc": "2026-01-29 12:18:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ecrmn",
                  "author": "Pretend-Pangolin-846",
                  "text": "I get the appeal though, its as if an optimized AGENTS.md file for whatever task or domain you want to work with",
                  "score": 11,
                  "created_utc": "2026-01-29 12:20:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2f1627",
                  "author": "1ncehost",
                  "text": "Skills can also include scripts",
                  "score": 2,
                  "created_utc": "2026-01-29 14:38:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ebvbu",
              "author": "Distinct-Expression2",
              "text": "Text that it loads on demend usually describe how to do something specific",
              "score": 10,
              "created_utc": "2026-01-29 12:14:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ec5n5",
                  "author": "Pretend-Pangolin-846",
                  "text": "oh, is it claude dependent, or can it be initialized into any IDE?",
                  "score": 5,
                  "created_utc": "2026-01-29 12:16:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2epbkb",
                  "author": "Electronic-Ice-8718",
                  "text": "So in theory 1 request will need at least 2 LLM calls. One for checking which prompts to load, another one to actually process the input?",
                  "score": 2,
                  "created_utc": "2026-01-29 13:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2epoyl",
              "author": "DJT_is_idiot",
              "text": "I'm amazed by the answers you received. Are people that clueless in here? Only one answer so far that actually scratches the surface of what skills are.\n\nEdit: maybe it was satire and I just didn't get it",
              "score": 2,
              "created_utc": "2026-01-29 13:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f2vlq",
                  "author": "Pretend-Pangolin-846",
                  "text": "Not satire, I am serious.",
                  "score": 1,
                  "created_utc": "2026-01-29 14:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ele6b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -3,
              "created_utc": "2026-01-29 13:15:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2enwdh",
                  "author": "abnormal_human",
                  "text": "No, they're just chunks of system prompt that you can easily turn on/off. Literally just .claude/skills/foo/SKILL.md.   \n  \nIf you have a repeated dev task, you can document the procedure, layers of code in the system, etc, and make Claude Code more reliable at repeating that process. I have skills for iterating on agent evals so I can in one command get it to run my eval suite, prioritize the things I want prioritized, iterate on the failures in a certain efficient way, then come back to me when the score is improved by a certain amount. I have another one for integrating a form of API integration that my project does very often so that it knows what files/layers to touch and avoids pitfalls. And a third when I'm working on payments/etc stuff that makes sure a current copy of our billing mechanics is in scope. Not a super complex idea, but very effective.",
                  "score": 4,
                  "created_utc": "2026-01-29 13:29:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eiavw",
          "author": "one-wandering-mind",
          "text": "I'm surprised there haven't been more stories about supposed AI tooling with viruses or exploits. People are so willing to install and run random code they find on github that is brand new with no security review.\n\n\nPeople on this sub particularly seem terrified to use model in the cloud for fear of their sensitive data being exposed, but many will happily install random stuff and implement networking they don't understand that risks exposing their home server to attacks.",
          "score": 23,
          "created_utc": "2026-01-29 12:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f61y0",
              "author": "SilentLennie",
              "text": "I mean...\n\nhttps://www.reddit.com/r/vibecoding/comments/1qpnybr/found_a_malicious_skill_on_the_frontpage_of/",
              "score": 2,
              "created_utc": "2026-01-29 15:02:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fagw1",
              "author": "TinFoilHat_69",
              "text": "You could just look through the code in the repo and ask GitHub copilot to review the repo itâ€™s built into GitHubs website. I personally would rather just look at designs and ideas and implement my version to fit my own needs.",
              "score": 1,
              "created_utc": "2026-01-29 15:23:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ekl8p",
          "author": "combrade",
          "text": "The IPTV shit is useful projects like that. Donâ€™t discourage those . High school Me learned about Github through projects that sailed the high seas for movies and shows.",
          "score": 9,
          "created_utc": "2026-01-29 13:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2et69h",
          "author": "skybsky",
          "text": "I'm somewhat pleased that some comments actually see that this post is bs. \"somewhat\", because others still don't read before commenting",
          "score": 8,
          "created_utc": "2026-01-29 13:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2erky4",
          "author": "belgradGoat",
          "text": "Which ones? Microsoft vibe voice or grok? Did you even red or look at what you posted?",
          "score": 12,
          "created_utc": "2026-01-29 13:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2esje8",
          "author": "russianguy",
          "text": "I feel the same way about MCP servers. I ain't about to run some code from `some-fucking-guy.sh` repo.",
          "score": 5,
          "created_utc": "2026-01-29 13:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2etb23",
              "author": "Distinct-Expression2",
              "text": "Mcp are dead, skills way better",
              "score": 1,
              "created_utc": "2026-01-29 13:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fbabl",
                  "author": "ZachCope",
                  "text": "Doesnâ€™t MCP â€˜doâ€™ something and a skill tells the LLM â€˜how to doâ€™ something? 2 different functions.Â ",
                  "score": 2,
                  "created_utc": "2026-01-29 15:26:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ef2f3",
          "author": "vesko26",
          "text": "this is much worse then JS framework hell. Those at least kinda worked. This is just garbage",
          "score": 23,
          "created_utc": "2026-01-29 12:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f48q7",
              "author": "BigHugeOmega",
              "text": "How is it garbage? Which software on this list doesn't work?",
              "score": 7,
              "created_utc": "2026-01-29 14:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f5uoz",
                  "author": "vesko26",
                  "text": "MCP as a standard is garbage, literally the first issue on the ai=data=science-team project is arbitrary code execution due to prompt injection",
                  "score": -3,
                  "created_utc": "2026-01-29 15:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eio82",
          "author": "raysar",
          "text": "This is a good thing. Concurrence is the only solution fort fast evolution.",
          "score": 4,
          "created_utc": "2026-01-29 12:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f2agi",
          "author": "Healthy-Nebula-3603",
          "text": "Natural selection...",
          "score": 2,
          "created_utc": "2026-01-29 14:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eqb5k",
          "author": "Admirable-Choice9727",
          "text": "I hate \"browser-use\" it disappointed me so much.",
          "score": 3,
          "created_utc": "2026-01-29 13:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fxf2t",
              "author": "vatta-kai",
              "text": "How exactly was it disappointing? Iâ€™m thinking of trying it out",
              "score": 1,
              "created_utc": "2026-01-29 17:05:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2eqnwu",
              "author": "Distinct-Expression2",
              "text": "Itâ€™s shit; just use vercel agent-browser",
              "score": 0,
              "created_utc": "2026-01-29 13:44:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ey4w1",
          "author": "InterstellarReddit",
          "text": "everyone is just recycling the same open source solution to a problem with a little change ui twist not changing anything of value.",
          "score": 1,
          "created_utc": "2026-01-29 14:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f5ufx",
          "author": "SilentLennie",
          "text": "Interesting kimi-cli did make it, moltbot (formerly Clawd Bot) did not.\n\nWhatever you think of it: it's open source/free software at work: 'scratch your own itch', something doesn't do what you want and the existing solutions you know about it's not a good fit... create your own.",
          "score": 1,
          "created_utc": "2026-01-29 15:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc1mc",
          "author": "Alex_1729",
          "text": "Meaningless hateful post.",
          "score": 1,
          "created_utc": "2026-01-29 15:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2earqn",
          "author": "macromind",
          "text": "Honestly this feels like the early JS framework explosion, but for agent orchestration. Tons of wrappers, thin abstractions, and a few that will become \"boring infrastructure\" later.\n\nThe stuff that survives will probably be the unsexy parts: evals, tracing, memory, permissions, and deployment ergonomics.\n\nIve been following the space pretty closely and keep a running list of patterns and frameworks here if youre curious: https://www.agentixlabs.com/blog/",
          "score": -6,
          "created_utc": "2026-01-29 12:06:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egsdz",
              "author": "gnnr25",
              "text": "Your website is spammy. A pop-up chat and a pop-up newsletter subscription. Geeze",
              "score": 30,
              "created_utc": "2026-01-29 12:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ekliy",
          "author": "MaxKruse96",
          "text": "Yes, we are in hell. People somehow got the idea that \"Agents\" are the future, and \"We have jarvis at home\" - when in reality nothing is even remotely ready or secure enough for that.",
          "score": 0,
          "created_utc": "2026-01-29 13:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eu62x",
          "author": "IulianHI",
          "text": "The comparison to JS framework hell is spot on. Remember when we had jQuery, Backbone, Ember, Angular, React, Vue, Svelte, and 100 others all trying to solve the same problem? Most of these agent frameworks will meet the same fate - the winners will be the ones that focus on boring, reliable infrastructure (memory, evals, tracing) rather than shiny demos.",
          "score": 0,
          "created_utc": "2026-01-29 14:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ebzb4",
          "author": "k_am-1",
          "text": "Trending page is showing whatâ€™s trending HELPPP!!?",
          "score": -8,
          "created_utc": "2026-01-29 12:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ecuoq",
          "author": "Familiar_Print_4882",
          "text": "I created AI primitives (not frameworks)\nDo you think that will die in 1 week too ?\n\nhttps://github.com/withceleste/celeste-python",
          "score": -9,
          "created_utc": "2026-01-29 12:20:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ef1xy",
              "author": "Distinct-Expression2",
              "text": "you created a proxy which you'll have to maintain yourself adding delay in the request and forcing people to self host it when all major providers are switching to open responses - quite bad timing",
              "score": 10,
              "created_utc": "2026-01-29 12:35:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2eg9e2",
                  "author": "Familiar_Print_4882",
                  "text": "Itâ€™s fully compatible with openresponses.\nAnd no need to maintain much, just the api changes which happen 1nce per year.\nAlso you can use extra_body if anythingâ€™s not integrated so you never miss a new feature.\nWhat do you think now ? Convinced or not.\nI prefer youâ€™re not and tell me why.\nIâ€™m looking for an argument so Iâ€™ll be like ah yeah ok not worth it then, it has no future.",
                  "score": 1,
                  "created_utc": "2026-01-29 12:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ev76p",
          "author": "KitchenSomew",
          "text": "The pattern we're seeing here mirrors every tech hype cycle - lots of experimentation, most will consolidate or fade. The frameworks that will survive are the ones solving real infrastructure problems: reliable memory management, proper eval frameworks, and production-ready orchestration.\n\n\n\nMost of these are just thin wrappers around LangChain or AutoGen with marketing fluff. The real value will be in the unglamorous work - robust error handling, proper logging/tracing, and actual testing frameworks for agents.",
          "score": -1,
          "created_utc": "2026-01-29 14:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f0mlx",
          "author": "KitchenSomew",
          "text": "Classic AI hype cycle in action! While many will fade away, the silver lining is that this explosion of frameworks helps identify what actually works. The ones that survive typically solve a real problem with a clean API. I'm curious which approaches will still be standing in 6 months - my bet is on the ones with strong community support and clear documentation.",
          "score": -1,
          "created_utc": "2026-01-29 14:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egb2m",
          "author": "DeathByPain",
          "text": "I'm definitely not trending but I'm working on a fork of a gui for llama-server that will let you launch multiple servers in a tabbed window ðŸ˜",
          "score": -5,
          "created_utc": "2026-01-29 12:44:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmjzx1",
      "title": "KV cache fix for GLM 4.7 Flash",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/19067",
      "author": "jacek2023",
      "created_utc": "2026-01-25 14:06:55",
      "score": 251,
      "num_comments": 72,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1nw5a7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-25 18:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mjmtv",
          "author": "__Maximum__",
          "text": "We are now just 5 patches away from running this model locally without issues!",
          "score": 89,
          "created_utc": "2026-01-25 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mx0be",
              "author": "Hunting-Succcubus",
              "text": "Actually its 7 patches",
              "score": 29,
              "created_utc": "2026-01-25 15:55:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1njm51",
              "author": "jacek2023",
              "text": "what are you ta..... [https://github.com/ggml-org/llama.cpp/pull/19092](https://github.com/ggml-org/llama.cpp/pull/19092)",
              "score": 16,
              "created_utc": "2026-01-25 17:33:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mli64",
          "author": "teachersecret",
          "text": "Just tested with UD's k\\_xl 4 bit version on my 4090. Yesterday I was using it with about 45,000 context and maxing out the 4090.\n\nNow it fits with 90,000 context.\n\nI like the model. Still a bit quirky though. I had it running some agentic stuff yesterday and I was really impressed with what I was able to scaffold out of it, but I absolutely had to hold its hand a bit. Reminds me of trying to code with gemini flash or something - it's not terrible and you can get the job done. Beyond coding, it crushes tool use and works great as a tool using assistant. You can get it to do some writing and roleplay but it doesn't seem particularly good at that (it'll make mistakes bigger/more creative writing focused models don't). It's definitely my new default for my home-server.",
          "score": 27,
          "created_utc": "2026-01-25 15:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1myj54",
              "author": "__Maximum__",
              "text": "I was impressed by its tool use. You throw at it tools, and chains them like a pro. It calls search, then fetches the URL, then based on that another search, based on all the above git clones a repo, edits it, runs tests and so on for hours without any issues. All simple tasks, of course.\n\nWhen given a huge codebase, it will still use tons of tools but will come up with wrong conclusions or have obviously wrong priorities.\n\nI used the API so far, so don't know if this holds up on local setups with quants, but I sure hope so.\n\nBtw, the model behind API is having huge issues atm as well. Almost unusable.",
              "score": 3,
              "created_utc": "2026-01-25 16:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n2c0s",
                  "author": "teachersecret",
                  "text": "Yeah. I found you have to loop in some agentic double checking and scaffolding to keep it on track, and on a larger codebase I think youâ€™d really want to focus it on some small piece or feature.\n\nI canâ€™t imagine actually coding with it over something like opus 4.5, but for agentic local stuff? Itâ€™s pretty damn impressive.\n\nI plan on getting vllm up and running with it once theyâ€™ve got it all dialed in there. Itâ€™s small enough that we should be able to run multiple simultaneous agents - possibly dozens of them. Iâ€™m kinda excited to see what a pile of local agents set to work could do with such reliable tool calling.",
                  "score": 3,
                  "created_utc": "2026-01-25 16:18:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1oeklx",
              "author": "floppypancakes4u",
              "text": "Im on 4090 as well, using LM studio though and im sure thats my problem, since im only getting 10tks. What setup are you using and whats your tks?",
              "score": 1,
              "created_utc": "2026-01-25 19:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sizd8",
              "author": "AfterAte",
              "text": "If you can, run your display off your IGPU. I could get 65K context before this build on my 3090, using 23.3GB all for llama.cpp.",
              "score": 1,
              "created_utc": "2026-01-26 09:47:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1w2p5e",
                  "author": "teachersecret",
                  "text": "I'm rolling a 4090/5900x, no igpu.\n\nThat said, it's better now. Here's my latest testing:  \n| Context | Graph Splits | VRAM | TTFT | Prompt | Generation |\n\n|---------|--------------|------|------|--------|------------|\n\n| 32K | 2 | 19.8 GB | 47.1 ms | 670.5 tok/s | 133.3 tok/s |\n\n| 64K | 2 | 21.5 GB | 48.9 ms | 648.9 tok/s | 134.6 tok/s |\n\n| 95K | 2 | 22.9 GB | \\~50 ms | \\~650 tok/s | \\~135 tok/s |\n\n| 96K | 7 | 23.0 GB | 52.0 ms | 612.1 tok/s | 125.4 tok/s |\n\n| 128K | 24 | 23.0 GB | 80.3 ms | 396.7 tok/s | 95.2 tok/s |\n\nIt's slower at 128k because it has to split the graphs a bit more (it goes from 2 to 9 or something, I think). Still works, just a bit of performance loss.\n\n| Graph Splits | Performance Impact |\n\n|--------------|-------------------|\n\n| 2 | Optimal - minimal overhead |\n\n| 7-9 | \\~7% slowdown |\n\n| 21-24 | \\~30% slowdown |\n\n| 100+ | \\~75% slowdown |\n\nAlso tried it with kv cache quantization:  \n| Configuration | Context | Generation Speed | Use Case |\n\n|---------------|---------|------------------|----------|\n\n| q4\\_0 KV | \\*\\*202K\\*\\* | \\*\\*139 tok/s\\*\\* | \\*\\*Best overall\\*\\* |\n\n| q8\\_0 KV | 128K | 138 tok/s | High-quality KV cache |\n\n| f16 KV (default) | 95K | 135 tok/s | Baseline |\n\n| CPU MoE | 202K | 32 tok/s | Low VRAM systems |\n\nOn CPU MoE it still pulls off 32 tok/s at 202k context length.",
                  "score": 2,
                  "created_utc": "2026-01-26 20:50:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zpfp2",
                  "author": "Kitchen-Year-8434",
                  "text": "Be careful with this if you also game on your rig. I ended up with throttling and underutilized gpu in gaming because my iGPU couldnâ€™t handle the bandwidth of framebuffer copying from my dGPU. \n\nThough I am running dual 4K ultra wide resolution on a NEO G9. Might not be a problem with 4K or 1440p.",
                  "score": 1,
                  "created_utc": "2026-01-27 10:15:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mhrw5",
          "author": "Deep_Traffic_7873",
          "text": "Is re-re-download needed for the gguf?Â ",
          "score": 24,
          "created_utc": "2026-01-25 14:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mhvu3",
              "author": "jacek2023",
              "text": "no",
              "score": 14,
              "created_utc": "2026-01-25 14:43:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mtqi0",
          "author": "viperx7",
          "text": "# GLM 4.7 unsloth (data for 20k input tokens)\n\n# Before this change\n\n|Quant|GPU|Context|Prompt Processing|Token Generation|Notes|\n|:-|:-|:-|:-|:-|:-|\n|UD-Q4\\_K\\_XL|Single 4090|64k|3489 t/s|88 t/s||\n|UD-Q4\\_K\\_XL|4090 + 3060|170k|2017 t/s|52 t/s||\n|Q8|4090 + 3060|30k|2087 t/s|47.1 t/s||\n|Q8|4090 + 3060 + cpu|64k|1711 t/s|41.3 t/s|`-ot '([2][0-2]).ffn_.*_exps.=CPU'`|\n\n# After the change\n\n|Quant|GPU|Context|Prompt Processing|Token Generation|Notes|\n|:-|:-|:-|:-|:-|:-|\n|UD-Q4\\_K\\_XL|Single 4090|128k|3510 t/s|92.5 t/s||\n|UD-Q4\\_K\\_XL|4090 + 3060|200k|2041 t/s|56.2 t/s||\n|Q8|4090 + 3060|72k|2058 t/s|50.4 t/s||\n|Q8|4090 + 3060 + cpu|100k|1968 t/s|45.7 t/s|`-ot '([2][0-2]).ffn_.*_exps.=CPU'`|\n\n`no kv cache quantisation used`  \n`my GPUs are headless so this is probabily max context you can fit`\n\n`max context size for this model is 207K and in 4090+3060 senario with Q4_K_XL it fits full 200k cache and about 6gb VRAM remains empty`",
          "score": 25,
          "created_utc": "2026-01-25 15:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1md892",
          "author": "Able_Ad1273",
          "text": "what is going on with this model lmao",
          "score": 84,
          "created_utc": "2026-01-25 14:19:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mfm7x",
              "author": "-p-e-w-",
              "text": "Modern LLMs are extremely complex, with almost all of them now introducing new attention or MoE techniques, every single time.\n\nBut the biggest problem is that automated correctness testing pretty much isnâ€™t a thing, with basically no progress on that topic in the past 2 years.",
              "score": 88,
              "created_utc": "2026-01-25 14:32:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mjv9f",
                  "author": "teachersecret",
                  "text": "I am surprised someone hasn't knocked something together for that purpose.\n\nLife on the bleeding edge.",
                  "score": 18,
                  "created_utc": "2026-01-25 14:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1o46ep",
                  "author": "Objective_Mousse7216",
                  "text": "If only AI could write complex code for itself....",
                  "score": 6,
                  "created_utc": "2026-01-25 18:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ooukk",
              "author": "ilintar",
              "text": "Non-trivial architecture that has to be adapted. I told you give us a week :)",
              "score": 8,
              "created_utc": "2026-01-25 20:29:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mdwft",
              "author": "jacek2023",
              "text": "let me quote Z.ai: \"two weeks\" ;)",
              "score": 19,
              "created_utc": "2026-01-25 14:23:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o4a3j",
                  "author": "MrWeirdoFace",
                  "text": "Tweeeeeeo weeeeeks....",
                  "score": 5,
                  "created_utc": "2026-01-25 18:58:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1miaiy",
              "author": "sleepingsysadmin",
              "text": "I get qwen next having pains on release; they did something new. \n\nThis model is cursed.",
              "score": 3,
              "created_utc": "2026-01-25 14:46:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1millz",
                  "author": "jacek2023",
                  "text": "qwen next is at least merged, look at kimi linear ;)",
                  "score": 16,
                  "created_utc": "2026-01-25 14:47:36",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1mz5hg",
                  "author": "markole",
                  "text": "Somehow it works great on my side with recent llama.cpp, opencode and unsloth q8 quant. ðŸ¤·",
                  "score": 4,
                  "created_utc": "2026-01-25 16:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1s0aa7",
              "author": "rashaniquah",
              "text": "I had a horrible time running it on vLLM too because the 0.14.0 was released a couple hours after release",
              "score": 1,
              "created_utc": "2026-01-26 07:02:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1merm0",
              "author": "teachersecret",
              "text": "Not unusual for some of these Chinese models to be broken for a few weeks while people get them properly implemented :). (it's not always the model itself, although this one SPECIFICALLY has already had multiple versions quantized and re-quantized to get it working, typically this is just a matter of implementing whatever new voodoo the model-maker added to the mix, so as usual give it a few weeks)",
              "score": -12,
              "created_utc": "2026-01-25 14:27:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mf4bi",
                  "author": "jacek2023",
                  "text": "it's llama.cpp implementation, not the model itself",
                  "score": 19,
                  "created_utc": "2026-01-25 14:29:30",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1mmp5l",
                  "author": "Aggressive-Bother470",
                  "text": "wtf are these downvotes, lol.Â \n\n\ntruer words ne'er be spake.",
                  "score": -6,
                  "created_utc": "2026-01-25 15:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mpkpk",
          "author": "FluoroquinolonesKill",
          "text": "This at least doubles the speed on my rig. Now I am getting about 30 t/s. Before, I was getting about 10-13 t/s.",
          "score": 9,
          "created_utc": "2026-01-25 15:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nce7x",
          "author": "jacek2023",
          "text": "BTW this model is quite popular\n\nhttps://preview.redd.it/th6swn271jfg1.png?width=1964&format=png&auto=webp&s=804be603da5a2b358a0f9f826aab4d1f1849d067",
          "score": 7,
          "created_utc": "2026-01-25 17:02:06",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1nkyaw",
              "author": "mister2d",
              "text": "We want it to succeed. ðŸ˜Š",
              "score": 7,
              "created_utc": "2026-01-25 17:38:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1msxmd",
          "author": "LagOps91",
          "text": "wait what? how does it work without using values? is this an RNN architecture?",
          "score": 3,
          "created_utc": "2026-01-25 15:37:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mt3tz",
              "author": "jacek2023",
              "text": "MLA",
              "score": 8,
              "created_utc": "2026-01-25 15:38:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mtleg",
                  "author": "LagOps91",
                  "text": "how does it avoid V cache? i was under the impression that MLA is still based on standard attention with some improvements made to increase memory efficiency. is the V cache combined with something else that's stored or how does it work?",
                  "score": 2,
                  "created_utc": "2026-01-25 15:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1olcly",
          "author": "GaboureySidibe",
          "text": "A KV data structure without the values is just a set.",
          "score": 2,
          "created_utc": "2026-01-25 20:13:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1omoi1",
          "author": "Odd-Ordinary-5922",
          "text": "getting 5 more tokens/s but its good because I was getting 25 before",
          "score": 2,
          "created_utc": "2026-01-25 20:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n6xsg",
          "author": "harrro",
          "text": "The model is good and fast but it is so verbose in reasoning (even for simple things).\n\nIs it possible to limit/disable reasoning or is this not trained for that?",
          "score": 2,
          "created_utc": "2026-01-25 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o0u5d",
              "author": "robiinn",
              "text": "You can disable it with `--chat-template-kwargs '{\"enable_thinking\": false}'`",
              "score": 6,
              "created_utc": "2026-01-25 18:44:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o8up5",
                  "author": "harrro",
                  "text": "Worked perfectly! Thank you.\n\nResponses now finishing in around 7-8 seconds instead of the 40 secconds it was taking before.",
                  "score": 1,
                  "created_utc": "2026-01-25 19:18:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nyk5v",
              "author": "viperx7",
              "text": "When I use. It directly I feel the same but somehow when using it with opencode it thinks very optimally and to the point\nThat leads me to believe \na good system prompt is what you need to make this model's thinking not too verbose",
              "score": 1,
              "created_utc": "2026-01-25 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1oddg2",
                  "author": "jacek2023",
                  "text": "I have same experiences, opencode somehow works, with this new patch I have kind of \"Claude Code at home\" feeling",
                  "score": 1,
                  "created_utc": "2026-01-25 19:38:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1o3t4c",
              "author": "nasone32",
              "text": "it reasons less at lower temperature",
              "score": 1,
              "created_utc": "2026-01-25 18:56:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ow8rv",
          "author": "ladz",
          "text": "Latest build tripled generation TPS for me. Yay!",
          "score": 1,
          "created_utc": "2026-01-25 21:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p0lep",
          "author": "alex_bit_",
          "text": "Whereâ€™s vLLM?",
          "score": 1,
          "created_utc": "2026-01-25 21:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qeoqb",
          "author": "LocoMod",
          "text": "I've abstained from using this model until the issues are ironed out. Seems like we're at a point where we can cook. What are the recommended llama-server params to primarily use it as an \"orchestrator\" that invokes tools and other agents? I'm using the Q6_K_XL Unsloth version on an RTX5090. The model is 26GB so I have 6GB to fit the maximum content in. What ctx and temp is everyone using?",
          "score": 1,
          "created_utc": "2026-01-26 01:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qopox",
              "author": "LocoMod",
              "text": "EDIT: Very inconsistent. Sometimes it works great, other times using the same exact prompt it does not.",
              "score": 1,
              "created_utc": "2026-01-26 02:03:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mkf1m",
          "author": "Cool-Chemical-5629",
          "text": "I trust ggerganov, but still I have to ask. Is this REALLY safe? I mean removal of the V portion of the cache? Is that really how the model works / is supposed to work? I just hope they aren't vibe coding this or something and that they really know what they are doing lol. Sure the model is currently slow but what the heck it's far better than other models of that size, so they better not break it more. ðŸ˜‚",
          "score": -8,
          "created_utc": "2026-01-25 14:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mlc93",
              "author": "jacek2023",
              "text": "(not sure are you trolling or not)\n\nfrom my understanding MLA uses different kind of cache, so one value (latent) is used instead two k/v",
              "score": 10,
              "created_utc": "2026-01-25 15:01:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mmzyq",
                  "author": "Cool-Chemical-5629",
                  "text": "It was a honest question, not trolling at all. Stuff breaks sometimes, it happens even to the best coders out there. I'm starting to like this model more every day, so naturally I'm anxious whenever there's a new change to the runtime which could make it run 5000 times better or leave it completely broken lol",
                  "score": 3,
                  "created_utc": "2026-01-25 15:09:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}