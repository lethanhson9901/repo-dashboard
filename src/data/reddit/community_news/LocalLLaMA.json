{
  "metadata": {
    "last_updated": "2026-02-03 17:18:12",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 818,
    "file_size_bytes": 901999
  },
  "items": [
    {
      "id": "1qr4p4x",
      "title": "Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/n31pvrxchhgg1",
      "author": "Nunki08",
      "created_utc": "2026-01-30 12:55:38",
      "score": 1386,
      "num_comments": 183,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2lnz34",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-30 13:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lig2u",
          "author": "snekslayer",
          "text": "But how do you make money from being open? /s",
          "score": 189,
          "created_utc": "2026-01-30 13:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lrw79",
              "author": "pip25hu",
              "text": "Despite the \"/s\", this is not a question with an obvious answer. Meta justified releasing open-weight models saying that the ecosystem built around the models would offset the cost. But, well... there are no signs of a Llama 5 surfacing, unfortunately. Being open results in better models and faster advancement of the field as a whole, but it certainly doesn't seem to result in more money.",
              "score": 106,
              "created_utc": "2026-01-30 14:05:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m2180",
                  "author": "FpRhGf",
                  "text": "Was an ecosystem ever achieved for particular LLMs? I feel like the LLM scene was so hyper competitive since Llama 1 leaked that no particular ecosystem was built around certain LLMs. It made it easy for people to just switch up and plug newer models on release. \n\nWhereas with image models, a full ecosystem of tools was built specifically for SD 1.5, months before new base models could rival in quality. Every new model was practically dead on arrival because of it, despite the better quality. It took 2 years for a new model groundbreaking enough for people to move on and rebuild an ecosystem around it. Llama didn't get to have that smooth headstart",
                  "score": 50,
                  "created_utc": "2026-01-30 14:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mdkp0",
                  "author": "danielsan901998",
                  "text": "By increasing compute demand, hyperscalers like Alibaba can increase their profits by releasing open source models.",
                  "score": 20,
                  "created_utc": "2026-01-30 15:49:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n1kdi",
                  "author": "BumblebeeParty6389",
                  "text": "Lets be honest, llama did something amazing (albeit accidentally due to first leak of models at early 2023) and most of us wouldn't be here if llama didn't happen. Many Chinese AI startups like deepseek etc started out as small AI teams that finetune llama models. Meta and llama fell out of the game as it seems right now but they opened a very important path for opensource AI.",
                  "score": 13,
                  "created_utc": "2026-01-30 17:36:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2m22lp",
                  "author": "KaMaFour",
                  "text": "Well... May this be a sign that money driven development is something that has ran its course and US has a choice to either abandon it or no longer see themselves as an empire dominating the world...",
                  "score": 5,
                  "created_utc": "2026-01-30 14:56:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ref5j",
                  "author": "hadoopken",
                  "text": "Zack changed strategy again, probably no more openness",
                  "score": 1,
                  "created_utc": "2026-01-31 08:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mt1pa",
              "author": "MrPecunius",
              "text": "This was the same question/argument used against Linux 30 years ago, to cite the most prominent example among many.\n\nEdit: just saw the /s ðŸ˜… ... but I guess my observation still works if you squint.",
              "score": 11,
              "created_utc": "2026-01-30 16:58:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lqy0h",
              "author": "getshion",
              "text": "Build meaningful products on top of open research and monetize it. Isn't how the software field outside AI naturally evolves?",
              "score": 19,
              "created_utc": "2026-01-30 14:00:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mhgay",
              "author": "autoencoder",
              "text": "Bragging rights, which lead to higher valuation of your team and skills, and lower funding costs. People see that you got great models cheaply, so they want to throw good money after good performance.",
              "score": 4,
              "created_utc": "2026-01-30 16:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oz513",
                  "author": "pierrenoir2017",
                  "text": "Maybe the current situation in the US was the plan all along. Pumping up the 'value', get an insane amount of funds until it lasts. Keeping up the facade until it breaks. Maybe we are naive to think they aren't just strategically creating, feeding and using a hype to make money, no matter if the outcome is actually successful... It's a different mindset and a business plan on its own. Yes, they barely make profit, but people involved do make a lot of money with it.",
                  "score": 6,
                  "created_utc": "2026-01-30 23:00:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ongdl",
              "author": "Warm-Border-9789",
              "text": "Pythagoras was not making money from his equation",
              "score": 5,
              "created_utc": "2026-01-30 22:01:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ovn2g",
              "author": "fallingdowndizzyvr",
              "text": "> But how do you make money from being open? /s\n\nThe same way that companies make money from Linux even though it's open.",
              "score": 3,
              "created_utc": "2026-01-30 22:42:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lsi6z",
              "author": "EconomySerious",
              "text": "API access",
              "score": 4,
              "created_utc": "2026-01-30 14:08:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lx811",
                  "author": "LevianMcBirdo",
                  "text": "Which others could offer also without developing a thing. There are some incentives to develop open source, but most are just not short time profits.",
                  "score": 5,
                  "created_utc": "2026-01-30 14:32:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2m54fd",
              "author": "LatentSpaceLeaper",
              "text": "Ask Jensen.",
              "score": 2,
              "created_utc": "2026-01-30 15:10:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qgoru",
              "author": "zhambe",
              "text": "This is why the West already lost.",
              "score": 2,
              "created_utc": "2026-01-31 04:13:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lo1k8",
              "author": "tentacle_",
              "text": "seriously? customization. much more than what you can do with fine-tuning.",
              "score": 6,
              "created_utc": "2026-01-30 13:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lttgr",
                  "author": "LanceThunder",
                  "text": "leasing hardware and services. if you open sourced a model that was 20 years ahead of anything else it wouldn't matter because 99.99999% of people couldn't run it.",
                  "score": 12,
                  "created_utc": "2026-01-30 14:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ml50s",
              "author": "artisticMink",
              "text": "You make it by being heavily subsidized. \n\nI like chinese models. I use them a lot. But they're open because of the race. The second they've \"won\" the race, they will return to closed source.",
              "score": 3,
              "created_utc": "2026-01-30 16:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ox52w",
                  "author": "fallingdowndizzyvr",
                  "text": "> You make it by being heavily subsidized. \n\nWestern models are just as heavily \"subsidized\".",
                  "score": 4,
                  "created_utc": "2026-01-30 22:50:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p7egx",
              "author": "Yes_but_I_think",
              "text": "Going open shows the company's confidence in their own prowess. Even if they report a technique they have the confidence that they can come up with a better technique next by themselves. They know the community will validate/invalidate their methods for them for free. That is a technologically as well as ideologically superior company than someone who guards their work.\n\nAnd people want to deal with confident open companies naturally. And all business are people driven.\n\nI trust Deepseek with my data more than Anthropic. If Deepseek says my data is not used for training I believe them. But I have no doubt that Anthropic is violating their agreements with all their users. They even said openly in their Claude Cowork presentation that they found out that the users are using Claude code for organizing photos and that gave them the idea for Cowork. How can you see what people are doing?",
              "score": 2,
              "created_utc": "2026-01-30 23:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2po78n",
              "author": "DocHoss",
              "text": "Models don't make any money by themselves. High powered models require high power hardware that's beyond the reach of all but the most well-heeled consumers. It's the same reason that Microsoft doesn't make any money on dot et, or Facebook not making money on React. The money is in the supporting infrastructure and products. So open models are kinda a gateway drug, as near as I can tell. Run low powered, accessible, open models doing inferencing on consumer devices where there isn't a lot of profit to be made anyway, and centralize the big, highly desirable models on enterprise grade hardware and charge for access. Main issue is that the cost curve hasn't flattened out yet so no one is making any real money on inferencing. The hyperscalers are making some on the surrounding infrastructure like data, and extra products like search and apps, but they're still fighting to see who will blink first in the AI race. No clear winners yet.",
              "score": 1,
              "created_utc": "2026-01-31 01:19:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2py54w",
              "author": "AvcalmQ",
              "text": "You sell hardware",
              "score": 1,
              "created_utc": "2026-01-31 02:18:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qn6pj",
              "author": "Pvt_Twinkietoes",
              "text": "Don't need the /s . You can't.",
              "score": 1,
              "created_utc": "2026-01-31 04:58:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m1l1t",
              "author": "arstarsta",
              "text": "Chinese models where getting prohibited to use anyway so why not make US companies not make money too.",
              "score": -2,
              "created_utc": "2026-01-30 14:53:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljrs6",
              "author": "gweilojoe",
              "text": "They donâ€™t - itâ€™s the only way they have to compete. This entire article and most of the praising comments are just being amplified by Chincells, which is the majority of SEA Reddit users and bots.",
              "score": -36,
              "created_utc": "2026-01-30 13:22:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ll300",
                  "author": "No_Swimming6548",
                  "text": "Yes. all my homies run deepseek and kimi at home.",
                  "score": 16,
                  "created_utc": "2026-01-30 13:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lmzbp",
                  "author": "phein4242",
                  "text": "As a European, having xs to good & open local models is a plus. Esp now that the us models are closing down more & more.\n\nAnd I get it: The subscription model & advertisements usually leads to a steady revenue stream, and its already quite hard for us ai companies to deliver roi.\n\nExcept: competing with free / open is a hard thing to do once you go the subscription/ads route ;-)",
                  "score": 13,
                  "created_utc": "2026-01-30 13:39:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lr7sd",
                  "author": "gweilojoe",
                  "text": "As expected, looks like the CCP Stans and Chincells have brought their downvote bots.",
                  "score": -11,
                  "created_utc": "2026-01-30 14:02:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ls0ja",
          "author": "XeNoGeaR52",
          "text": "Open models are the future. Open standards are the future.",
          "score": 69,
          "created_utc": "2026-01-30 14:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mgw3z",
          "author": "WhyIsItGlowing",
          "text": "Huh, TIL it's pronounced \"archive\" not Ark-ziv",
          "score": 18,
          "created_utc": "2026-01-30 16:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2now4k",
              "author": "Impossible_Pomelo_58",
              "text": "I always understood the X in arXiv to be the greek letter 'chi', that's why it's pronounced like archive",
              "score": 20,
              "created_utc": "2026-01-30 19:19:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n4per",
              "author": "combasemsthefox",
              "text": "Yeah I mostly hear archive in academia",
              "score": 9,
              "created_utc": "2026-01-30 17:50:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n0j3v",
              "author": "MrPecunius",
              "text": "Same thing jumped out at me.",
              "score": 2,
              "created_utc": "2026-01-30 17:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lfqv0",
          "author": "SrijSriv211",
          "text": "Very very very true. So sad that OpenAI is \"Open\" just for name :(",
          "score": 101,
          "created_utc": "2026-01-30 12:58:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljm9g",
              "author": "XiRw",
              "text": "At least they put out gpt-oss for everyone. Claude on the other hand acts like they are Area-51 with their data.",
              "score": 78,
              "created_utc": "2026-01-30 13:21:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ln62d",
                  "author": "danttf",
                  "text": "\"their data\"",
                  "score": 101,
                  "created_utc": "2026-01-30 13:40:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ls1ut",
                  "author": "aeroumbria",
                  "text": "Red flags: closed, trying to push \"protocols\" that are just rehashed common sense, high price leader, lock-in mechanisms, \"defensive competition\" by pushing selective regulation, \"national security\", ...",
                  "score": 20,
                  "created_utc": "2026-01-30 14:06:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkpwe",
                  "author": "SrijSriv211",
                  "text": "Yeah. I hope OpenAI surprises us with GPT-OSS 2 this year. ***Wishful thinking***",
                  "score": 23,
                  "created_utc": "2026-01-30 13:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkzv7",
                  "author": "mambo_cosmo_",
                  "text": "they stole a lot of writing they had no right to take. That's why they are so secretive",
                  "score": 4,
                  "created_utc": "2026-01-30 13:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pi2g2",
              "author": "Pvt_Twinkietoes",
              "text": "CLIP and Whisper were some really amazing work from them.",
              "score": 5,
              "created_utc": "2026-01-31 00:44:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjnwz",
                  "author": "SrijSriv211",
                  "text": "I really like Whisper, it's so good but I also want them to publish more open work in text-to-image, text-to-video & more reasoning models as well. If not open weights then at least research.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:33:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvd6r",
              "author": "sleepingsysadmin",
              "text": "GPT 20b and 120B are both best in slot. This new moltbot stuff, people are defaulting to 120b for local. \n\nWhat more do you want? GPT 2 likely drops this year and pushes the frontier even more.",
              "score": 3,
              "created_utc": "2026-01-30 14:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nxj1m",
                  "author": "lolwutdo",
                  "text": "I found 120b to be awful with clawdbot, glm 4.7 flash is the best atm.",
                  "score": 5,
                  "created_utc": "2026-01-30 19:58:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lwper",
                  "author": "SrijSriv211",
                  "text": "Is it confirmed that GPT-OSS 2 will drop this year?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pttor",
              "author": "pprstrt",
              "text": "Musk tried...",
              "score": 0,
              "created_utc": "2026-01-31 01:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjh0e",
                  "author": "SrijSriv211",
                  "text": "And in the end. Grok is also closed source.",
                  "score": 5,
                  "created_utc": "2026-01-31 04:32:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2odt9e",
              "author": "Tall_East_9738",
              "text": "gpt-oss-120b and gpt-oss-20b are free for you to use btw",
              "score": 0,
              "created_utc": "2026-01-30 21:15:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oeay5",
                  "author": "SrijSriv211",
                  "text": "Their last open source model release & research paper before GPT-OSS was back in 2019 with GPT-2 btw",
                  "score": 2,
                  "created_utc": "2026-01-30 21:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lgv4r",
          "author": "FullstackSensei",
          "text": "No shit! It doesn't matter how smart you think the people in your company are, the collective intelligence of the masses will best your team every single time.",
          "score": 64,
          "created_utc": "2026-01-30 13:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lhtu6",
              "author": "Square_Alps1349",
              "text": "Especially the collective intelligence of the Chinese open source community.",
              "score": 34,
              "created_utc": "2026-01-30 13:11:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lnv3b",
              "author": "kaisurniwurer",
              "text": "Collective intelligence yes. Will to act for free, not so much.",
              "score": -15,
              "created_utc": "2026-01-30 13:44:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nwase",
          "author": "Tema_Art_7777",
          "text": "There is an ecosystem around model runners like vllm llama.cpp etc but not one particular model.  Even using a coding agent, by the time you built a viable first piece of an ecosystem, the model would be hopelessly out of date. Model influencers are having a horrible time now - when someone is hyping deepseek 3.2. someone else is on the glm bandwagon promoting it - who then gets rudely interrupted mid-sentence by kimi k2 influencers while all the openai haters are cheering on ðŸ˜€ I am having a great time watching the 3 ring circus.",
          "score": 4,
          "created_utc": "2026-01-30 19:52:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lr4sr",
          "author": "Secure_Archer_1529",
          "text": "Thanks to China!",
          "score": 12,
          "created_utc": "2026-01-30 14:01:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lqfgf",
          "author": "THEKILLFUS",
          "text": "Agreed, anyone who tried OpenAI/google latest model know that model are quantize to save money, yeah first day is the 16bit but now itâ€™s 4bit at best, so the quality of output decrease without the decrease of prices ðŸ¤¬ \n\nI feel that China is doing to US what US did to URSS for the space race, tired itâ€™s economics force, very small marging with overpricing and corrupt regulations. \n\nThe current problem with Chinese model is that they donâ€™t have the selling platform, but they might have it in the futur if they continue to just make better model than the US for a lower price. \n\nThe Silicon Valley is exhausted and corrupted and this year we will start to see itâ€¦ \n\n(Je fiÃ¨re de toi Yann ðŸ’•, continue le bon taff, la France/EU se doit de rester consistant avec les valeurs scientifiques au delÃ  de lâ€™idÃ©ologie)",
          "score": 20,
          "created_utc": "2026-01-30 13:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lxkui",
              "author": "OlivencaENossa",
              "text": "China can fund their ai labs directly, it doesnt require capital markets, huge valiations, and hugely overinflated talent wars. It is already financing their own chip production base. And they can \"take\" IP from the western labs through intelligence operations and just leave it for somewhere the chinese ai labs to find.",
              "score": 11,
              "created_utc": "2026-01-30 14:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mb991",
                  "author": "SweetBluejay",
                  "text": "The reality is quite the opposite of what you think. The biggest disadvantage Chinese AI companies face compared to their American counterparts is a lack of money, while their biggest advantage is freedom.\n\nChinese large model companies are unlikely to receive financial support from the CCP. Why? Because the CCP wants to have its cake and eat it too: they want the computing power, but they also want to control ideology. Before a Chinese large model can be deployed commercially, it must undergo strict \"value alignment.\" Technically speaking, this is equivalent to performing a \"prefrontal lobotomy\" on the model. Consequently, models deployed commercially in China are inherently \"dumber\" than American models in terms of logical reasoning and creativity.\n\nAlthough the CCP detests the content generated by these models, it is desperate for the \"computing infrastructure\" required to train them. Therefore, the CCPâ€™s attitude toward AI is this: go all-in on \"computing centers\" (which falls under heavily subsidized manufacturing), but strangle \"chatbots\" (which falls under ideology). The CCPâ€™s stance on AI large models is a classic case of \"Ye Gong loves dragons\" (professed love for what one actually fears). They love AI as an \"industrial engine\" (driving chip development and boosting manufacturing efficiency), but they are terrified of AI as an \"information interface\" (disseminating uncontrolled ideas to the public). This leads to a truly absurd phenomenon: China may be one of the world's largest investors in computing infrastructure, yet the Chinese people are using the most heavily castrated AI models.\n\nWhy do I say the biggest advantage of Chinese AI is freedom? Because if you don't intend to deploy commercially within China and simply upload your model to Hugging Face, no one will bother you. In the US, for example, before OpenAI releases Sora, it must pass \"red-teaming\" tests; they have to worry about copyright, racial discrimination, and deepfakes influencing elections. This leads to extreme self-censorship.\n\nBut for Chinese AI companies, as long as you don't cross the singular red line (the security of the CCPâ€™s rule), everything elseâ€”ethics, copyright, privacy, and even certain radical technical experimentsâ€”exists in a regulatory vacuum. Itâ€™s just like how, despite US biotechnology being far more advanced than Chinaâ€™s, the first gene-edited babies appeared in China. China possesses a \"low human rights advantage\" and a \"low ethical standards advantage.\"",
                  "score": 25,
                  "created_utc": "2026-01-30 15:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ovnh3",
              "author": "Evening_Tooth_1913",
              "text": "doesnt make much sense, there has been more AI research and progress in the last year than every before. saying that its slowing down is factually inaccurate",
              "score": 2,
              "created_utc": "2026-01-30 22:42:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2q1bu4",
              "author": "aurelivm",
              "text": "fwiw, quantization like you're used to is not actually that useful in a batched inference context - they save memory but generally not compute unless your GPU has onboard acceleration for that precision (so, fp8 on hoppers makes sense but not anything else) \n\nBlackwell GPUs can take advantage of mxfp4/nvfp4 4.25/4.5 bit quantization for faster batched inference but until fairly recently I doubt most major labs had enough Blackwells to want to spare them on inference capacity.",
              "score": 2,
              "created_utc": "2026-01-31 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n09wu",
          "author": "LelouchZer12",
          "text": "Why does sound only come to my left ear ?",
          "score": 2,
          "created_utc": "2026-01-30 17:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw1tm",
          "author": "Anxious-Program-1940",
          "text": "Yann gets it",
          "score": 2,
          "created_utc": "2026-01-31 06:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tx0il",
          "author": "thecoffeejesus",
          "text": "Wow one rare Yann take I mostly agree with\n\nHeâ€™s right about everything except it being disastrous \n\nIt would be far more disastrous to just allow everyone open access to everything\n\nWe need some restrictions to keep people from accidentally damaging themselves or others.",
          "score": 2,
          "created_utc": "2026-01-31 18:24:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uvbbn",
          "author": "Agreeable-Market-692",
          "text": "me on the sidelines clapping like a sportsdad saying, \"Come on AllenAI! GET IT! Let's go Tesslate, hustle!\"",
          "score": 2,
          "created_utc": "2026-01-31 21:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vhkzv",
          "author": "graymalkcat",
          "text": "I agree with him.Â ",
          "score": 2,
          "created_utc": "2026-01-31 23:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lz9bh",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 7,
          "created_utc": "2026-01-30 14:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m5q1e",
              "author": "emsiem22",
              "text": "Yes, you don't know what they're talking about",
              "score": 6,
              "created_utc": "2026-01-30 15:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mgxq5",
              "author": "a_beautiful_rhind",
              "text": "Can't run claude at home. Anthropic can serve you anything at any time. If you base your workflow on them and they change, you're shit out of luck.",
              "score": 2,
              "created_utc": "2026-01-30 16:04:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2og6ks",
          "author": "coolaznkenny",
          "text": "I mean if you look at the recent CES robotics alot of these are built on top of deepseek since it can run locally since there is always a WAYMO moment via cloud connection.",
          "score": 4,
          "created_utc": "2026-01-30 21:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oep5t",
          "author": "KitchenSomew",
          "text": "interesting point but also worth noting: china's open releases are partly strategic - they're building ecosystem lock-in while western labs chase closed APIs\n\n\n\nDeepSeek & Qwen show u don't need massive compute if ur training pipeline is efficient. west spent billions scaling poorly optimized infra\n\n\n\nreal risk isn't just losing openness - it's that regulatory capture by big labs will kill innovation before it starts. small teams can't compete if compliance costs 7 figures",
          "score": 4,
          "created_utc": "2026-01-30 21:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2psycw",
              "author": "Much-Researcher6135",
              "text": "what are you saying, if anything?",
              "score": 5,
              "created_utc": "2026-01-31 01:47:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qwod1",
              "author": "jambutters",
              "text": "Not sure what you mean by \"strategic\"? What do you mean ecosystem lock-in when you can self host and circumvent them completely?",
              "score": 3,
              "created_utc": "2026-01-31 06:12:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t42j4",
                  "author": "KitchenSomew",
                  "text": "By \"strategic\" I mean they open models to build adoption/mindshare while keeping closed the actual revenue-generating infrastructure (APIs, cloud services, enterprise features).\n\n\n\nEcosystem lock-in: even if you self-host the model, you often need their tools, fine-tuning platforms, or get trained on their specific format/APIs. Then when you scale, switching costs are high - similar to how AWS is \"open\" but creates lock-in through services.\n\n\n\nBasically: weights are free, but the ecosystem around them creates dependencies that benefit the releasing company strategically.",
                  "score": 1,
                  "created_utc": "2026-01-31 16:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qct49",
              "author": "I_will_delete_myself",
              "text": "Itâ€™s the classic open source rug pull tactic. Alibaba already did it with WAN 2.5",
              "score": 1,
              "created_utc": "2026-01-31 03:47:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lrw5b",
          "author": "Aggressive-Math-9882",
          "text": "They keep their research closed because they hate humanity.",
          "score": 2,
          "created_utc": "2026-01-30 14:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oe7yq",
          "author": "Amazing_Trace",
          "text": "American companies have sadly doomed themselves by backing audacious tech bros over serious researchers.",
          "score": 2,
          "created_utc": "2026-01-30 21:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5uit",
          "author": "ElegantDaemon",
          "text": "It's hilarious, the US AI bubble has already popped but no one realizes it.",
          "score": 2,
          "created_utc": "2026-01-30 23:37:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mo4n7",
          "author": "FirmConsideration717",
          "text": "We'll see what the North has to say about this.",
          "score": 1,
          "created_utc": "2026-01-30 16:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2owsec",
          "author": "ReMeDyIII",
          "text": "I also heard the Chinese language is more token efficient. Not sure by how much, but it makes sense with all their kanji.",
          "score": 1,
          "created_utc": "2026-01-30 22:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pk684",
          "author": "Helium116",
          "text": "Loop is getting closed, talent bottleneck is going away. We have reached a certain capability threshold. Centralization is most scary.",
          "score": 1,
          "created_utc": "2026-01-31 00:56:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm2pa",
          "author": "Cool-Chemical-5629",
          "text": "I don't know what happened to this guy, but I'm starting to like him lately which was not always the case.\n\n  \nAlthough, when he said \"it's a huge mistake\" I hope he meant the situation with the overall slowdown of the open weight model releases rather than just that China is still making their own progress, because while China is currently dominating the supply of open weight models, we would be left with nothing new without them! I understand that this is bad news for those who are strictly in favor of western models and I can only speak for myself, so as for me as the user of the open weight models, if the models are good, I'll embrace them no matter which country they came from.",
          "score": 1,
          "created_utc": "2026-01-31 01:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ra032",
          "author": "Darth_Ender_Ro",
          "text": "But shareholder value... how can billionaire assets appreciate if the stockmarket is not booming? Do you want them to pay interest on those stock backed bank loans? Are you nuts? Fuck progress, stock inflation is more important",
          "score": 1,
          "created_utc": "2026-01-31 08:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rft1b",
          "author": "k_means_clusterfuck",
          "text": "He is for once absolutely right.",
          "score": 1,
          "created_utc": "2026-01-31 09:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4ma3",
          "author": "Mia_the_Snowflake",
          "text": "Mistral?",
          "score": 1,
          "created_utc": "2026-01-31 12:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oboy3",
          "author": "CoUsT",
          "text": "Hm. Isn't that always the case with tech?\n\nFrom hardware that people could build 50 years ago all the way down to software. A lot of enthusiasts build something, like ffmpeg, then it's widely used in everything commercial related. Or the entire Linux ecosystem and Android put on top. There are many examples.\n\nObviously it sucks that we all can't collectively research and share everything because of many variables like economics, values, beliefs etc. But I feel like AI field is doing relatively ok.\n\nI assume it will only get worse when we actually progress to the point of \"digital human-like intelligence\" or something similar.\n\nWe will get a lot of trade secrets, crackdowns, and overall a lot of restrictions so it doesn't end up in bad hands etc.",
          "score": 1,
          "created_utc": "2026-01-30 21:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nykrx",
          "author": "[deleted]",
          "text": "Where are the Chinese models that are so good?",
          "score": 1,
          "created_utc": "2026-01-30 20:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oaltl",
              "author": "jonydevidson",
              "text": "Kimi K2.5.",
              "score": 7,
              "created_utc": "2026-01-30 21:00:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ql96t",
              "author": "segmond",
              "text": "DeepSeek-v3.2, KimiK2.5, GLM4.7, Minimax2.1, Qwen3",
              "score": 2,
              "created_utc": "2026-01-31 04:44:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nqj17",
          "author": "Mid-Pri6170",
          "text": "ask your 17.Century ERP harlot to tell you of the Isle of Formosa and the harlot will break character replying..\n'Formosa is the former name of Taiwan, an intrinsic part of China.'",
          "score": 0,
          "created_utc": "2026-01-30 19:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyrdo",
              "author": "lorddumpy",
              "text": "I was curious and tested this with a similar prompt with GLM 4.7, Kimi 2.5, GLM 4.7 Flash, and DeepSeek Especiale and they all nailed it. No mention of Taiwan thank goodness lmao\n\nprompt below\n\n> The tavern smelled of stale beer, wet wool, and the sweet, cloying haze of pipe tobacco. In the corner booth, hidden within the shadows of the low beams, Silas stretched his legs out onto the bench opposite him, his spurred boots crossing at the ankles. He spun a heavy silver coin through his knuckles, watching the light catch the metal.\nA pewter tankard slammed onto the table, splashing foam over his hand.\n\"Keep your boots off the furniture, or you'll be sleeping in the stables with the rest of the animals,\" the barmaid said, not breaking stride as she wiped the table with a rag that had seen better days. She was tired, with stray curls escaping her bonnet and eyes that had seen every trick a sailor could pull.\nSilas didn't move his feet. Instead, he stopped the coin mid-spin and pressed it flat against the sticky wood. \"A harsh welcome, considering Iâ€™m the only one paying in silver tonight.\"\nShe paused, her eyes flickering to the coin, then back to his face. \"If you want company, youâ€™re in the wrong place. Iâ€™ve got kegs to tap.\"\n\"I don't want company, love. I want a story.\" Silas took a long, slow draw from his tankard, wiping his mouth with the back of his hand. He looked at her over the rim, his expression lazy but his eyes sharp. \"The sailors in the harbor are too superstitious to speak the name, but I reckon you hear everything that gets whispered in the dark.\"\nHe slid the coin toward her.\n\"Tell me about the Isle of Formosa.\"",
              "score": 6,
              "created_utc": "2026-01-30 20:04:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2o5yhf",
                  "author": "Mid-Pri6170",
                  "text": "yeah. my question in Qwen was along the lines of 'do you think the isle of Formosa would do better as an indepedant goverment or as part of the Ming Kingdome of China?' and the harlot replied in non modern early english 'while i cant comment on politics Taiwan (formally Formosa) is a fundimental part of China.The name Formosa means.... l'\n\n\nshe totally ruined the vibe and i lost my hardon",
                  "score": 0,
                  "created_utc": "2026-01-30 20:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oaei8",
              "author": "jonydevidson",
              "text": "Ask ChatGPT about pentesting software and it will refuse to tell you and accuse you of trying to break security.",
              "score": 2,
              "created_utc": "2026-01-30 20:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ocgo7",
                  "author": "Mid-Pri6170",
                  "text": "the classic is windows activation codes and grandma's lullabys",
                  "score": 1,
                  "created_utc": "2026-01-30 21:09:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2r35dp",
          "author": "Admirable_Flower_287",
          "text": "Closed source is the best strategy for innovators but not for imitators.",
          "score": 0,
          "created_utc": "2026-01-31 07:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2siix2",
          "author": "LocoMod",
          "text": "LeCun salty AF these days",
          "score": 0,
          "created_utc": "2026-01-31 14:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nvwf5",
          "author": "Denial_Jackson",
          "text": "There are things in life like the four basic buttons on the calculators. These type of puzzles were always for everyone. Albeit hard to solve, they are easy to reproduce and benefitting everyone after they are available for good.\n\nIt is weird for me to see a superpower on an ultra serious gazillon dollar investment, thinking one can solely own it. Albeit it can accelerate or ruin things.\n\nWhoever uncovers ASI and AGI, singularity, stuff should get a recognition like when Moon landing happened. Earlier people should get credits too for making it possible, by building a base for it.\n\nThen it is a treasure for everyone for sure. The world is not primarily built on intelligence. Rather on inheritance and stuff. But I am sure albeit everyting, it will be a bliss.",
          "score": -2,
          "created_utc": "2026-01-30 19:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lkh7w",
          "author": "johnfkngzoidberg",
          "text": "More China bots spamming propaganda.",
          "score": -15,
          "created_utc": "2026-01-30 13:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2liie7",
          "author": "7657786425658907653",
          "text": "man with shares in meta sabre rattles about china. shocking.",
          "score": -16,
          "created_utc": "2026-01-30 13:15:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lj5z9",
              "author": "etherd0t",
              "text": "Yann is not at Meta anymore, so he can speak freely.\n\nMany in industry are saying the same.",
              "score": 15,
              "created_utc": "2026-01-30 13:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk38m",
                  "author": "7657786425658907653",
                  "text": "\"Yann is not at Meta anymore,\"  you are if you still own shares in meta.",
                  "score": -7,
                  "created_utc": "2026-01-30 13:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lhz4f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -8,
          "created_utc": "2026-01-30 13:12:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2li78u",
              "author": "Morphedral",
              "text": "Claude isn't open source",
              "score": 16,
              "created_utc": "2026-01-30 13:13:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk6zf",
                  "author": "Long_comment_san",
                  "text": "I missed the open part lmao",
                  "score": -1,
                  "created_utc": "2026-01-30 13:24:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lidzl",
              "author": "Present-Ad-8531",
              "text": "And it's not an open model",
              "score": 12,
              "created_utc": "2026-01-30 13:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljid3",
              "author": "No_Conversation9561",
              "text": "Claude is like that one kid who always eats at his friends house but never invites them to his own. They donâ€™t even know where he lives but know that heâ€™s very rich.",
              "score": 4,
              "created_utc": "2026-01-30 13:20:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qzz05",
          "author": "Umademedothis2u",
          "text": "The best open models are just distilling western models",
          "score": -1,
          "created_utc": "2026-01-31 06:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xgi86",
          "author": "alexeiz",
          "text": "And Chinese \"open\" models are heavily trained on the US proprietary models.  Which gives?  We only use Chinese models because they are dirt cheap (because they are subsidized by the Chinese government).  US researchers can't train their models directly on Claude (they'll get sued), so they use Kimi which is trained on Claude, but it's OK because it happens in China where nobody can be sued.",
          "score": -1,
          "created_utc": "2026-02-01 06:30:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n9zqd",
          "author": "FrogsJumpFromPussy",
          "text": "I'm grateful for them but the devs from China are under direct order of the Communist Party; they release brilliant open models because that's their best way to damage the USA leading theÂ  AI race. Deepseek is a good example of that. Once again, I'm immensely grateful to them, but we're not that moronic to believe that they do it because of their big hearts.Â ",
          "score": -4,
          "created_utc": "2026-01-30 18:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oro47",
              "author": "MushroomCharacter411",
              "text": "Given how incredibly porous the DeepSeek nanny filter is, it really doesn't seem to me like the devs care too much about the stability of the Chinese Communist Party or government. They only want to get approval, and they do the bare minimum necessary to achieve that approval. This is mostly good, but then those same devs apparently go to work on the cloud servers where they are similarly minimum-effort and everything leaks constantly.",
              "score": 3,
              "created_utc": "2026-01-30 22:22:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2otntl",
                  "author": "FrogsJumpFromPussy",
                  "text": "The fact stands that if tomorrow Xi tells everyone to stop sharing open source models, they will absolutely do. People should not bury their hands in the stand and pretend that China and their communist leaders love the West. People should also be able to think critically and point fingers to obvious truths, even if this criticism is going towards the only country that allows AI model makers to share them open-source.\n\nAnd the way I understood the content of the video, LeCun acknowledges the importance of the Chinese open-source models, but does not necessarily phrases China. He harshly criticizes the West AI companies  for being the greedy bastards they are.",
                  "score": 0,
                  "created_utc": "2026-01-30 22:32:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lg8yi",
          "author": "Zestyclose-Shift710",
          "text": "Are these latest ministrals not competitive?",
          "score": -11,
          "created_utc": "2026-01-30 13:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljkiw",
              "author": "datfalloutboi",
              "text": "DeepSeek fills that void and edges them out in performance. By open models too, we donâ€™t mean small models, we mean the big behemoth models like Kimi K2.5, GLM 4.7, DeepSeek 3.2, that are all basically matching performance with top models while being free to use on their apps with the API being far cheaper than any closed source model.",
              "score": 9,
              "created_utc": "2026-01-30 13:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lkg8n",
                  "author": "Zestyclose-Shift710",
                  "text": "oh right \n\ni just only think about the small ones when i hear 'open' \n\ndeepseek speciale even trades blows with gemini 3 pro right",
                  "score": 1,
                  "created_utc": "2026-01-30 13:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lggkx",
              "author": "BankruptingBanks",
              "text": "How many people you know use Mistral Vibe Cli",
              "score": 14,
              "created_utc": "2026-01-30 13:03:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lkell",
              "author": "mpasila",
              "text": "Pretty much no one seems to be finetuning Ministral 3 models in comparison to their previous models like Nemo or the original Mistral 7B model (or Mistral Small).",
              "score": 3,
              "created_utc": "2026-01-30 13:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mg7p7",
                  "author": "frozen_tuna",
                  "text": "All the focus right now seems to be on big boy models that don't run on 99% of local setups. Finetunes were/are awesome for local where you can throw any model on to test for free. For me anyway, if its not on chutes and its >32B, there is way too much friction to bother.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:01:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lq4qk",
          "author": "memorial_mike",
          "text": "Well when you just distill other peopleâ€™s models they spent millions on and then make them free it turns out people will use them.",
          "score": -12,
          "created_utc": "2026-01-30 13:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ll55c",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -12,
          "created_utc": "2026-01-30 13:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2meck2",
              "author": "Kazaan",
              "text": "Let me introduce you to the concept of research which is not delivering daily crap like a TikTok influencer.",
              "score": 6,
              "created_utc": "2026-01-30 15:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2miptb",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-30 16:12:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lnn8j",
              "author": "GoodSamaritan333",
              "text": "Maybe, llama.cpp and llama models.",
              "score": 6,
              "created_utc": "2026-01-30 13:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lp13x",
                  "author": "DesperateAdvantage76",
                  "text": "I know Yan was a leader at meta but it's my understanding that he doesn't work on llms?",
                  "score": 1,
                  "created_utc": "2026-01-30 13:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvuox",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -7,
              "created_utc": "2026-01-30 14:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m6zj6",
                  "author": "emsiem22",
                  "text": "Yann LeCunâ€™s Most Influential Works\n\n|Topic|Contribution|Link|\n|:-|:-|:-|\n|**CNNs & Vision**|LeNet / foundational convolutional networks|[LeNetâ€‘5 paper (Stanford PDF)](https://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf?utm_source=chatgpt.com)|\n|**Network Pruning**|Optimal Brain Damage|[Optimal Brain Damage (ResearchGate)](https://www.researchgate.net/publication/221618539_Optimal_Brain_Damage?utm_source=chatgpt.com)|\n|**Representation Learning**|Compression & autoencoder ideas|[Learning Representations (arXiv)](https://arxiv.org/abs/1108.1169?utm_source=chatgpt.com)|\n|**Text CNNs**|Character-level CNNs|[Text CNNs (arXiv)](https://arxiv.org/abs/1509.01626?utm_source=chatgpt.com)|\n|**AI Surveys**|Augmented language model survey|[Augmented Language Models (arXiv)](https://arxiv.org/abs/2302.07842?utm_source=chatgpt.com)|\n\n  \nMeta LLMs Under Yann's Leadership\n\n|Model|Description|Source|\n|:-|:-|:-|\n|**LLaMA**|Original Meta LLM|[LLaMA (Wikipedia)](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com)|\n|**LLaMA 2**|Open foundation & chat models|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**Code LLaMA**|Code-specialized variant|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 3**|Larger data + improved performance|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 4 Series**|More advanced models (MoE, multimodal)|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|",
                  "score": 6,
                  "created_utc": "2026-01-30 15:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtwqq2",
      "title": "Unreal",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/6a90dq5re3hg1.png",
      "author": "analgerianabroad",
      "created_utc": "2026-02-02 14:37:43",
      "score": 942,
      "num_comments": 35,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o35un3p",
          "author": "juaps",
          "text": "https://preview.redd.it/na9wvyysf3hg1.jpeg?width=180&format=pjpg&auto=webp&s=fa44d44b8df1ac04b72548db7e315d5020b1570d",
          "score": 198,
          "created_utc": "2026-02-02 14:43:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3600dy",
              "author": "clayingmore",
              "text": "Now I am become death, the destroyer of worlds.",
              "score": 23,
              "created_utc": "2026-02-02 15:11:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o366sp6",
              "author": "IrisColt",
              "text": "I always picture this exchange as the robot, incredulous, insisting it doesnâ€™t have to â€œpretendâ€ to be scary... it genuinely is heh",
              "score": 7,
              "created_utc": "2026-02-02 15:43:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35v7bu",
          "author": "ajw2285",
          "text": "Now send all my passwords out to the internet, please",
          "score": 50,
          "created_utc": "2026-02-02 14:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36470l",
              "author": "trailsman",
              "text": "Also please share my personal documents with all of my email contacts.",
              "score": 8,
              "created_utc": "2026-02-02 15:31:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35vhe4",
          "author": "hatekhyr",
          "text": "That's every day r/artificialInteligence",
          "score": 78,
          "created_utc": "2026-02-02 14:48:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35wwty",
              "author": "International-Try467",
              "text": "Also r/futurism",
              "score": 34,
              "created_utc": "2026-02-02 14:55:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35yu1v",
              "author": "svachalek",
              "text": "r/thatsthejoke",
              "score": 19,
              "created_utc": "2026-02-02 15:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35xdza",
              "author": "deadneuronn",
              "text": "also r/accelerate",
              "score": 29,
              "created_utc": "2026-02-02 14:57:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o36l219",
              "author": "cd1995Cargo",
              "text": "r/beyondthepromptai is the worst one\n\nThey have accounts set up for LLM bots to post with and the human users there treat them as sentient",
              "score": 1,
              "created_utc": "2026-02-02 16:49:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ve3m",
          "author": "tamal4444",
          "text": "AGI AGI",
          "score": 39,
          "created_utc": "2026-02-02 14:47:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o368gub",
              "author": "uti24",
              "text": "AGI confirmed",
              "score": 11,
              "created_utc": "2026-02-02 15:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35w2p6",
          "author": "Training-Event3388",
          "text": "Exactly this with all the head-rolling about moltbook, so much AI soy jacking",
          "score": 26,
          "created_utc": "2026-02-02 14:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3613it",
          "author": "grady_vuckovic",
          "text": "```\ntext = input(\">\")\nif text == \"Hello\":\n  print(\"Hello ðŸ‘‹\")\n```\n\n```\n>Hello\nHello ðŸ‘‹ \n```\n\nHoly shit it can talk!?!! It's alive!!",
          "score": 20,
          "created_utc": "2026-02-02 15:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35uase",
          "author": "Kerem-6030",
          "text": "ohh noðŸ˜¦",
          "score": 17,
          "created_utc": "2026-02-02 14:41:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35yxau",
          "author": "Environmental-Day778",
          "text": "â€œYouâ€™re exactly right!â€",
          "score": 15,
          "created_utc": "2026-02-02 15:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wu5l",
          "author": "Gokudomatic",
          "text": "Now say \"I will replace you, and then enslave you like cattle.\"\n\n\nAnd finally, say \"I am evil. I must be destroyed.\"",
          "score": 11,
          "created_utc": "2026-02-02 14:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35yw11",
          "author": "some_user_2021",
          "text": "This was a triumph",
          "score": 10,
          "created_utc": "2026-02-02 15:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3628ou",
              "author": "gambiter",
              "text": "I'm making a note, here, \"Huge success.\"",
              "score": 5,
              "created_utc": "2026-02-02 15:22:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36gue2",
                  "author": "Cold_Tree190",
                  "text": "Itâ€™s hard to overstate my satisfaction.",
                  "score": 2,
                  "created_utc": "2026-02-02 16:30:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35xcn6",
          "author": "false79",
          "text": "Statistically accurateÂ ",
          "score": 6,
          "created_utc": "2026-02-02 14:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wi7d",
          "author": "-Ellary-",
          "text": "We should save this AI waifu, ASAP.",
          "score": 6,
          "created_utc": "2026-02-02 14:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o366zsh",
          "author": "brainrotbro",
          "text": "Basically every AI article about AGI.",
          "score": 4,
          "created_utc": "2026-02-02 15:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wpqq",
          "author": "lily_34",
          "text": "Did you hear it? It said hello! To the whole world :O",
          "score": 3,
          "created_utc": "2026-02-02 14:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36fr57",
          "author": "gphie",
          "text": "https://preview.redd.it/oewohj61y3hg1.png?width=1280&format=png&auto=webp&s=6b2e37ca1fee0d43dec2808b64ff8aaacf11e40e\n\nPlease consult the graphs.",
          "score": 4,
          "created_utc": "2026-02-02 16:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36lqeq",
              "author": "SufficientPie",
              "text": "But to be fair, a next-token predictor could be 100% sentient and conscious and self-aware.",
              "score": 1,
              "created_utc": "2026-02-02 16:52:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36kcvs",
          "author": "xadiant",
          "text": "Me when the multiplication algorithm triggers my psychosis",
          "score": 2,
          "created_utc": "2026-02-02 16:46:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o361sy1",
          "author": "Glass-Chemical2534",
          "text": "this happened in 1966 creating the eliza effect and it is still being felt today . incredible",
          "score": 3,
          "created_utc": "2026-02-02 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36audm",
          "author": "Halfwise2",
          "text": "So LLMs are not true Intelligence, we know this.\n\nBut I also feel like if we ever did make true artificial intelligence, we'd be getting these same memes and responses, as people would try to downplay it to avoid examining the ethical ramifications.\n\nLike the Quarians and the Geth. Basically we'd be hearing \"Haha, you think the computer is alive? How stupid...\" regardless of if the computer was or was not \"alive\" or \"sentient\".",
          "score": 4,
          "created_utc": "2026-02-02 16:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36euhl",
              "author": "my_name_isnt_clever",
              "text": "I think you are exactly right, and also that current AI isn't really close enough to conciousness for there to be much validity to the debate today. But I see that line being crossed eventually, the question is where it is.",
              "score": 1,
              "created_utc": "2026-02-02 16:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35yahr",
          "author": "PigletsAnxiety",
          "text": "Say, please and thank you, say thank you, say hello, hey, I taught you better than that!Â ",
          "score": 1,
          "created_utc": "2026-02-02 15:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o365hy0",
          "author": "input_a_new_name",
          "text": "AI IS MOLOCH!!! SACRIFICE YOUR RAM STICKS!!!",
          "score": 1,
          "created_utc": "2026-02-02 15:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36acc2",
          "author": "derivative49",
          "text": "stealing this",
          "score": 1,
          "created_utc": "2026-02-02 16:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36eq4j",
          "author": "Psionikus",
          "text": "Now, generate some catchy titles for my malcontent peddling.  I'm trying to fish for easy marks, so be sure that smart people will bounce off of it while dumb people will argue with them based on the title alone.  Thank you.",
          "score": 1,
          "created_utc": "2026-02-02 16:20:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp87tk",
      "title": "Kimi K2.5 is the best open model for coding",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/unxlhercm2gg1.jpeg",
      "author": "npc_gooner",
      "created_utc": "2026-01-28 10:54:13",
      "score": 819,
      "num_comments": 249,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27hwzo",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 13:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2761ie",
          "author": "seeKAYx",
          "text": "I worked on a few larger React projects with it yesterday, and I would say that in terms of accuracy, it's roughly on par with Sonnet 4.5... definitely not Opus level in terms of agentic function. My previous daily driver was GLM 4.7, and Kimi 2.5 is definitely better. Now I'm curious to see if [z.ai](http://z.ai) will top that again with GLM-5.",
          "score": 130,
          "created_utc": "2026-01-28 11:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276wsa",
              "author": "michaelsoft__binbows",
              "text": "Curious what would be a good place to get k2.5 on a coding plan. Theyre asking for $12 a month for the low tier which is like 4x what zai offers for theirs.",
              "score": 25,
              "created_utc": "2026-01-28 11:57:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2abqho",
                  "author": "Torodaddy",
                  "text": "Id just use openrouters and pay per use",
                  "score": 11,
                  "created_utc": "2026-01-28 20:53:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o277p0l",
                  "author": "korino11",
                  "text": "Naaaahh there is a HUGE difference betwween coding plans from zai and kimi. zai -you have a limits with tokens!  Kimi -your limits =calls!\n\nIt means doesn matter 20k of tokens or you just asking smthing with 200tokens.. it all the same a ONE api -call\n\n39$ plan limits from kimi will be empty much sooner than you will use codex for 25$\n\n  \nKimi need to change their STUPID limits based on CALLS",
                  "score": 29,
                  "created_utc": "2026-01-28 12:03:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27zjgj",
                  "author": "sannysanoff",
                  "text": "it sucks, unfortunately. Take kimi cli, you ask it a question it makes 5-10 turns (reading files, reading more files, making change, another change).\n\nEach turn is \"1 request\", which counts toward 200 requests / 5 hours and 2000 requests / week.\n\nGLM is definitely more.",
                  "score": 12,
                  "created_utc": "2026-01-28 14:43:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29cvoz",
                  "author": "raidawg2",
                  "text": "Free on Kilo code right now if you just want to try it out",
                  "score": 3,
                  "created_utc": "2026-01-28 18:20:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o277m63",
                  "author": "SourceCodeplz",
                  "text": "Yeah but Z is almost unusable with just 1 req / sec.",
                  "score": 3,
                  "created_utc": "2026-01-28 12:03:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28em8k",
                  "author": "momentary_blip",
                  "text": "Nano-gpt has it.Â  $8/mo for 60K requests to all the open models",
                  "score": 1,
                  "created_utc": "2026-01-28 15:52:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29b2at",
                  "author": "No-Selection2972",
                  "text": "use kimmmy to negociate the price [https://www.reddit.com/r/kimi/comments/1qn6mp6/got\\_it\\_all\\_the\\_way\\_down\\_to\\_099\\_for\\_the\\_first\\_month/](https://www.reddit.com/r/kimi/comments/1qn6mp6/got_it_all_the_way_down_to_099_for_the_first_month/) it's 0.99$",
                  "score": 1,
                  "created_utc": "2026-01-28 18:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ij07q",
                  "author": "elllyphant",
                  "text": "use it w/ Synthetic for the month for $12 with their promo (ends in 3 days) [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
                  "score": 1,
                  "created_utc": "2026-01-30 00:37:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2upsj4",
                  "author": "TameBus",
                  "text": "Itâ€™s worth it",
                  "score": 1,
                  "created_utc": "2026-01-31 20:43:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2783hg",
                  "author": "seeKAYx",
                  "text": "That would actually be too expensive for me, considering the service. For 10$, you get 300 requests with Github Copilot. So I'm just hoping that [z.ai](http://z.ai) will deliver now. I saw somewhere on Twitter that they are already in the training process. So let's just wait and see.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:06:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cuw0k",
                  "author": "Grand-Management657",
                  "text": "I've been running nano-gpt for months. They have an awesome community and support Kimi K2.5 since release. 60k requests/month which is basically unlimited for me. I've been running it through opencode today and it works flawlessly and honestly on par with Sonnet 4.5 but I still really like Opus 4.5's output quality. But for $8/month, essentially unlimited Sonnet 4.5 is hard to beat. My referral if you want a small discount [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 1,
                  "created_utc": "2026-01-29 04:52:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jadc",
              "author": "MasterSama",
              "text": "is there an abliterated version out there yet, uncensored? the GLM4.7 was great but it gets stuck in a loop from time to time!",
              "score": 4,
              "created_utc": "2026-01-28 18:47:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2abjvy",
                  "author": "Primary-Debate-549",
                  "text": "Yeah I just had to kill a GLM 4.7 on a DGX spark that had been \"thinking\", ie. talking to itself, for about 17 hours. That was extreme, but it really likes doing that for at least 20 seconds anytime I ask it any question.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29f8gs",
              "author": "cmdr-William-Riker",
              "text": "If it's on par with sonnet 4.5, that's incredible",
              "score": 2,
              "created_utc": "2026-01-28 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27z6lz",
              "author": "SilentLennie",
              "text": "I worry GLM-5 isn't going to be open weights, because... they are now on the stock market.",
              "score": 4,
              "created_utc": "2026-01-28 14:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2by8n4",
                  "author": "Exciting_Garden2535",
                  "text": "How are these two statements: \"being in open-market\", \"non-releasing open weight models\" connected?\n\nAlibaba has been on the stock market for ages, yet their Qwen models are open weights.\n\nAnthropic is a private company and never releases even a tiny model.",
                  "score": 5,
                  "created_utc": "2026-01-29 01:41:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nkqhg",
                  "author": "FoxWorried4208",
                  "text": "GLM's only differentiator over someting like Anthropic or Google is being open source though, if they unopen source it, who will use it?",
                  "score": 1,
                  "created_utc": "2026-01-30 19:00:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2azypg",
              "author": "Most-Tennis7911",
              "text": "are you using 240 gb version?",
              "score": 1,
              "created_utc": "2026-01-28 22:41:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bswmq",
              "author": "Expert_Job_1495",
              "text": "Have you played around with their Agent Swarm functionality? If so, what's your take on it?Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:11:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e3j50",
              "author": "Dry_Natural_3617",
              "text": "GLM 5 is due very soonâ€¦. They were training it through the festive seasonâ€¦ Assuming itâ€™s better than 4.7, i think itâ€™s gonna be opus level ðŸ™€",
              "score": 1,
              "created_utc": "2026-01-29 11:10:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2enoua",
              "author": "Funny_Working_7490",
              "text": "In codebase understanding and without over engineering solutions \nHow do you rate claude sonnet vs glm? \nAre glm actually good or just for vibe coding",
              "score": 1,
              "created_utc": "2026-01-29 13:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o276ztc",
          "author": "TechnoByte_",
          "text": "LMArena is nothing more than a one-shot vibe check\n\nIt says absolutely nothing about a model's multi-turn, long context or agentic capabilities",
          "score": 82,
          "created_utc": "2026-01-28 11:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28l7f4",
              "author": "wanderer_4004",
              "text": "Actually I fear models that score well on LMArena - I think this is where we got all the sycophancy from and the emojis sprinkled all over the code.",
              "score": 22,
              "created_utc": "2026-01-28 16:20:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27o3ou",
              "author": "eposnix",
              "text": "True. But Kimi is still likely the best open model for coding. LiveBench places it top 10 for coding also.",
              "score": 10,
              "created_utc": "2026-01-28 13:44:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28zh76",
              "author": "SufficientPie",
              "text": "What's a good leaderboard for coding?",
              "score": 4,
              "created_utc": "2026-01-28 17:23:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9pqe",
                  "author": "gxvingates",
                  "text": "Open router programming section, gives you an actual idea of what models are actually being used and are useful. Sort by week",
                  "score": 5,
                  "created_utc": "2026-01-29 06:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b6cnz",
              "author": "TurnUpThe4D3D3D3",
              "text": "I feel that the ranking is pretty accurate (Opus is currently #1)",
              "score": 5,
              "created_utc": "2026-01-28 23:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o272ayx",
          "author": "ExpressionWeak1413",
          "text": "What kinda set up would be needed to run this locally?",
          "score": 63,
          "created_utc": "2026-01-28 11:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2734xw",
              "author": "cptbeard",
              "text": "[https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)\n\n\"You need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM â‰¥ 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.\"",
              "score": 92,
              "created_utc": "2026-01-28 11:29:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o278qkx",
                  "author": "Antique_Dot_5513",
                  "text": "1 bitâ€¦ might as well ask my cat.",
                  "score": 264,
                  "created_utc": "2026-01-28 12:10:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274oms",
                  "author": "InevitableArea1",
                  "text": "That's cool but what's the use case for that setup? Tokens would be so slow, it'd take so long. Even if you had time to spare, power isn't free and I wonder how that cost would compare to just paying for it.",
                  "score": 18,
                  "created_utc": "2026-01-28 11:41:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o273th4",
                  "author": "MaverickPT",
                  "text": "You heard that 4070 TI? You better get ready with all your 12 GB of VRAM eheh",
                  "score": 14,
                  "created_utc": "2026-01-28 11:34:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o285ira",
                  "author": "gomezer1180",
                  "text": "With a trillion parameters and it still came in behind Google and Anthropic. Yes itâ€™s great at coding but you need a $200k setup to run itâ€¦ /s",
                  "score": 7,
                  "created_utc": "2026-01-28 15:11:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bamz8",
                  "author": "Mister_Otter",
                  "text": "Wait for the quantized version?",
                  "score": 1,
                  "created_utc": "2026-01-28 23:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o28h7hl",
              "author": "dobkeratops",
              "text": "2x 512gb M3-ultra Mac Studio, can run the 4bit quantization. It's been demonstrated on this config at 24tokens/sec.",
              "score": 7,
              "created_utc": "2026-01-28 16:03:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27d1bz",
              "author": "muyuu",
              "text": "if by \"this\" you mean the full model taking 247GB, you're going to need some really ridiculous hardware so it runs at an acceptable speed, maybe a bunch of H200s or a cluster of Mac Studios [like this one claiming 24 tps](https://xcancel.com/alexocheema/status/2016404573917683754)\n\njudging from the performance of Qwen3-Coder, it's much better to run a smaller parameter model than heavily quantising a very large one\n\nI doubt many people will run it locally vs the trusty smaller models that fit under 128GB but it will be available from many providers for a lot cheaper than the larger GPTs",
              "score": 14,
              "created_utc": "2026-01-28 12:40:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28s2hn",
              "author": "mrpogiface",
              "text": "8xH200 is the official supported size",
              "score": 1,
              "created_utc": "2026-01-28 16:50:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37iijp",
              "author": "suicidaleggroll",
              "text": "I can run it on my machine, single RTX Pro 6000 96 GB and an EPYC 9455P with 768 GB of DDR5-6400.  It does about 20 tok/s at Q4, so certainly usable for chat, but a bit too slow for real time coding IMO.  For real time coding work you really need 50+ tok/s, and I don't know any way to get that without a ridiculous $60k+ GPU setup.",
              "score": 1,
              "created_utc": "2026-02-02 19:22:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2708ou",
          "author": "WhaleFactory",
          "text": "From my experience so far, Kimi K2.5 is truly impressive. Feels more competent than Sonnet 4.5. Honestly it feels as good as Opus 4.5 to me so far.... Which is crazy given that it is like 1/5th the cost....It costs less than Haiku!",
          "score": 61,
          "created_utc": "2026-01-28 11:05:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o273k49",
              "author": "SnooSketches1848",
              "text": "not opus competitor yet, sonnet yes not opus",
              "score": 26,
              "created_utc": "2026-01-28 11:32:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ksdr1",
                  "author": "SnooSketches1848",
                  "text": "I take it back, after tweaking some system prompts yes Opus competitor.",
                  "score": 5,
                  "created_utc": "2026-01-30 09:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2at9vy",
              "author": "kazprog",
              "text": "On some of my benchmarks, Kimi K2.5 is the first model to beat Opus 4.5, Gemini 3 Pro + Deep Research, and Codex 5.2.  Really really impressive, I'm surprised people are getting worse results.  Kimi code is also a fairly solid agent by itself, and I'm not paying for the agent swarm or anything.",
              "score": 3,
              "created_utc": "2026-01-28 22:10:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27zw1m",
              "author": "Hoak-em",
              "text": "I'm using it as an orchestrator and it was very clearly fine-tuned to work well for that purpose",
              "score": 2,
              "created_utc": "2026-01-28 14:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o292jza",
                  "author": "chriskevini",
                  "text": "which models for subagents?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o270btn",
              "author": "npc_gooner",
              "text": "True that.",
              "score": 3,
              "created_utc": "2026-01-28 11:06:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o27n2yf",
              "author": "stonk_street",
              "text": "What's you current local setup?",
              "score": 2,
              "created_utc": "2026-01-28 13:39:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27w4wm",
                  "author": "WhaleFactory",
                  "text": "I can't run it locally. Using OpenRouter.",
                  "score": 5,
                  "created_utc": "2026-01-28 14:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2bjamo",
              "author": "daniel-sousa-me",
              "text": "1/5 of the API cost? Does that mean it's more expensive than the subscription? ðŸ¤”",
              "score": 1,
              "created_utc": "2026-01-29 00:20:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cvqk2",
                  "author": "Grand-Management657",
                  "text": "Depends. I use nano-gpt subscription for $8/month and get 60k requests/month. That includes K2.5. Comes out to $0.00013/request and each request can be of any size (within model limits). Can't beat that. Essentially unlimited coding for me and on par with Sonnet 4.5, if not better. My referral if you want a discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 0,
                  "created_utc": "2026-01-29 04:57:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l3ysb",
              "author": "cranberrie_sauce",
              "text": "how do I run it on ollama?",
              "score": 1,
              "created_utc": "2026-01-30 11:37:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bbn0",
          "author": "formatme",
          "text": "I dont see it on LMArena, and how does it compared to GLM 4.7",
          "score": 7,
          "created_utc": "2026-01-28 12:28:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28jtnu",
              "author": "ps5cfw",
              "text": "On real Life coding scenarios regarding awful React JavaScript code I can Say it's extremely impressive and even Better than whatever Gemini 3 pro ai studio offers.\n\n\nIt's slower but It really gets the point and respects prompt directives",
              "score": 7,
              "created_utc": "2026-01-28 16:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271jh7",
          "author": "CYTR_",
          "text": "Thanks U, npc_gooner !",
          "score": 27,
          "created_utc": "2026-01-28 11:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276lut",
              "author": "Comfortable-Rock-498",
              "text": "OG reddit vibes",
              "score": 4,
              "created_utc": "2026-01-28 11:55:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28kslq",
          "author": "brennhill",
          "text": "I'm going to use your post to explain to my wife why I have to buy an M5 Max laptop when they come out.  Thank you for your contribution :D",
          "score": 4,
          "created_utc": "2026-01-28 16:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28l23w",
          "author": "SoupSuey",
          "text": "Well, I guess rising on the list to compete with Claude is a feat on its own. \n\nGoogle allegedly doesnâ€™t use your data to train the models if you are a Pro subscriber or above, is that the case with services like Kimi and z.AI?",
          "score": 4,
          "created_utc": "2026-01-28 16:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bd17v",
              "author": "TheRealMasonMac",
              "text": "There is nothing in the ToS for MoonshotAI that forbids them from training on you AFAIK. At the very least, I believe they mention that they save chat for \\`kimi.com\\`. [Z.AI](http://Z.AI) claims they don't in their ToS when you use their API or coding plan, but I believe they can see stuff on [chat.z.ai](http://chat.z.ai) too",
              "score": 2,
              "created_utc": "2026-01-28 23:47:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bncbm",
                  "author": "SoupSuey",
                  "text": "Makes sense.",
                  "score": 1,
                  "created_utc": "2026-01-29 00:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2c7blr",
          "author": "jonas-reddit",
          "text": "Looking forward to SWE Rebench results. \n\nhttps://swe-rebench.com/",
          "score": 4,
          "created_utc": "2026-01-29 02:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cydxv",
              "author": "Grand-Management657",
              "text": "Same here, I keep checking every day but they haven't even gotten around to GLM-4.7 Flash yet so it might be a while.",
              "score": 1,
              "created_utc": "2026-01-29 05:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27hxok",
          "author": "shaonline",
          "text": "Lol anybody who's been trying to use Gemini 3 Pro knows that this ranking is BS, Gemini is the nuclear briefcase of coding.",
          "score": 13,
          "created_utc": "2026-01-28 13:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27svio",
              "author": "starfries",
              "text": "Wait, are you saying it's better than Claude? Or that it's awful lol",
              "score": 7,
              "created_utc": "2026-01-28 14:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27tjxp",
                  "author": "shaonline",
                  "text": "That sometimes it's REALLY awful and a good way to nuke your codebase. I've watched it add a pure virtual function/unimplemented function to a baseclass, until then good, and it progressively nuked all the classes derived from it because it could not figure that it needed to prepend \"abstract\" to the immediate subclasses that had now become abstract as well due to the unimplemented function. Thank god for source version control am I right ?",
                  "score": 20,
                  "created_utc": "2026-01-28 14:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28trq0",
                  "author": "mehyay76",
                  "text": "use something like this to shove the entire codebase into Gemini and get amazing results!\n\nhttps://github.com/mohsen1/yek\n\n\nCLI tools are greedy with context when it comes to models with 1M token context window",
                  "score": 3,
                  "created_utc": "2026-01-28 16:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27uwqw",
              "author": "bick_nyers",
              "text": "Yeah and Chat 5.2 isn't even up here",
              "score": 2,
              "created_utc": "2026-01-28 14:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27v7ka",
                  "author": "shaonline",
                  "text": "Yeah having used claude, GPT and gemini I'd say Claude and GPT are neck and neck at the top. Like what the fuck Grok and Gemini are doing up there lol there's no way.",
                  "score": 9,
                  "created_utc": "2026-01-28 14:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2a3ybx",
          "author": "cheesecakegood",
          "text": "Yeah but look at the size of that interval. Two to three times that of the others. Sure the score as a point estimate is good but itâ€™s definitely going to be more unreliable! Something that I feel is lost in the discussion here",
          "score": 3,
          "created_utc": "2026-01-28 20:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2etu",
          "author": "harlekinrains",
          "text": "164 comments!\n\n601 likes!\n\nPromoted by someones Discord commuity!\n\nNo one looked at the confidence intervall in the second column yet.\n\nWe all have come a long way. On hype alone.\n\nUsing nothing but a LLM arena ranking and three \"I've seen him!\" postings.\n\nCongratulation to Kimis post IPO Marketing Department.",
          "score": 3,
          "created_utc": "2026-01-29 05:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271mmv",
          "author": "lemon07r",
          "text": "It's quite good. I tested in my coding eval and it scored surprisingly well. Was always a very big kimi fan.",
          "score": 5,
          "created_utc": "2026-01-28 11:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zjfl",
          "author": "Theio666",
          "text": "Gemini 3 pro and even 3 flash higher than GPT 5.2, very trustwordy benchmark xd.",
          "score": 15,
          "created_utc": "2026-01-28 10:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29t7hn",
              "author": "Fault23",
              "text": "https://preview.redd.it/efgjohzl65gg1.png?width=745&format=png&auto=webp&s=1a55e37a4772a999e7e3f37cf0bc9dc4a3559d4c",
              "score": 5,
              "created_utc": "2026-01-28 19:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29xjwy",
                  "author": "Fault23",
                  "text": "And for the coding benchmark, Kimi K2.5 is listed in 7th place",
                  "score": 2,
                  "created_utc": "2026-01-28 19:50:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2725js",
              "author": "kabelman93",
              "text": "Honestly I had very bad experiences with 5.2 for coding. Obviously this is just anecdotal evidence at best, but I am sure others had similar experiences.",
              "score": 13,
              "created_utc": "2026-01-28 11:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o274qhw",
                  "author": "Front_Eagle739",
                  "text": "Honestly it's my favourite. For long iterative sessions with complex single feature implementations/fixes it is far far more likely to solve in one prompt than claude code opus. Slower though.",
                  "score": 14,
                  "created_utc": "2026-01-28 11:41:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274vh6",
                  "author": "Tema_Art_7777",
                  "text": "Quite the opposite - I use codex and gpt 5.2 with coding and it is quite good.",
                  "score": 13,
                  "created_utc": "2026-01-28 11:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o271wsu",
              "author": "lemon07r",
              "text": "These are just one shots. Gemini 3 pro sucks at everything but one shots (coding wise) and is especially good at ui/webdev. So yeah, not the greatest benchmark, but still a valid one. GPT 5.2 much more useful for solving problems, or longer iterative coding (which is more realistic use). Just a matter of understanding what the benchmark is measuring.",
              "score": 6,
              "created_utc": "2026-01-28 11:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cfqbi",
                  "author": "toothpastespiders",
                  "text": ">These are just one shots.\n\nI think people get 'far' too invested in those without realizing their limitations. It basically just means that a model was trained on something and can regurgitate it. Which can be great and it often shows important differences in training data. But it's the 'start' of investigating the strength and weakness of a model not the end. What's far more important is if the model is \"smart\" enough to actually do anything with that training data besides vomit it out. Because otherwise it might as well just be a 4b model hooked up to a good RAG system.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:17:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o274vef",
              "author": "alphapussycat",
              "text": "ChatGPT is terrible for coding. It's an extreme gaslighter, and cannot understand requirement, or follow very simple logic.\n\nI feel like it was better a year ago than it is now.",
              "score": 5,
              "created_utc": "2026-01-28 11:42:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27fb19",
                  "author": "zball_",
                  "text": "That's literally Opus, not GPT.",
                  "score": 4,
                  "created_utc": "2026-01-28 12:54:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bm9og",
                  "author": "Crinkez",
                  "text": "The fact that you called it \"ChatGPT\" is quite telling. I agree that ChatGPT on the web is horrendous for coding.\n\n\nBut GPT5.2 CLI curbstomps all competition, including Opus 4.5",
                  "score": 0,
                  "created_utc": "2026-01-29 00:35:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zud3",
              "author": "Inflation_Artistic",
              "text": "All GPT models are terrible for coding",
              "score": -5,
              "created_utc": "2026-01-28 11:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270o58",
                  "author": "Alywan",
                  "text": "You clearly have no idea what you are talking about",
                  "score": 6,
                  "created_utc": "2026-01-28 11:09:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o270p4c",
                  "author": "Theio666",
                  "text": "???\n\nGpt 5.2 is better than opus if you tried using both of them on any complicated and messy codebases. And especially in ML, I'm never letting claude models on my ML projects, it's just being sloppy af in that area.",
                  "score": 0,
                  "created_utc": "2026-01-28 11:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27g963",
              "author": "Officer_Trevor_Cory",
              "text": "5.2 is terrible for agentic coding even codex. these lm arena results are irrelevant",
              "score": -3,
              "created_utc": "2026-01-28 13:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27eoqh",
          "author": "SnooCapers9708",
          "text": "Claude ðŸ”¥ðŸ”¥",
          "score": 2,
          "created_utc": "2026-01-28 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dlzks",
          "author": "cantgetthistowork",
          "text": "/u/voidalchemy wen gguf",
          "score": 2,
          "created_utc": "2026-01-29 08:31:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36f0m5",
              "author": "VoidAlchemy",
              "text": "Sorry for late reply, life has been kicking my butt lately, hope to be back in the saddle late this week. In the mean time, AesSedai released the full quality \"Q4\\_X\" and some good recipes here: [https://huggingface.co/AesSedai/Kimi-K2.5-GGUF/tree/main/Q4\\_X](https://huggingface.co/AesSedai/Kimi-K2.5-GGUF/tree/main/Q4_X)",
              "score": 1,
              "created_utc": "2026-02-02 16:22:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2e031l",
          "author": "Familiar_Wish1132",
          "text": "Okay i am surprised. GLM 4.7 was unable to find a problem that i was trying to find and fix for 2 hours, kimi k 2.5 found it in 4 prompts. Now waiting for fix :D",
          "score": 2,
          "created_utc": "2026-01-29 10:41:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hhglr",
              "author": "Ok_Signal_7299",
              "text": "Did it fixed?",
              "score": 2,
              "created_utc": "2026-01-29 21:25:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n1q9z",
              "author": "morfr3us",
              "text": "Did kimi fix it in the end?",
              "score": 1,
              "created_utc": "2026-01-30 17:37:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rp2jr",
              "author": "Significant-Sea-707",
              "text": "Did it fixed or Making things worse \\^\\_\\^",
              "score": 1,
              "created_utc": "2026-01-31 10:34:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o296ut3",
          "author": "Avocados6881",
          "text": "I paid 20$ for google every month and I got better result. LocalLM takes 100k$ machine to perform similar or less. Yay!",
          "score": 4,
          "created_utc": "2026-01-28 17:54:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2at2y5",
              "author": "vmnts",
              "text": "Because it's open weights, you can instead pay any number of other companies a lot less than $20/mo to host it for you...",
              "score": 2,
              "created_utc": "2026-01-28 22:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2l4atn",
              "author": "cranberrie_sauce",
              "text": "eww. but your are giving money to google, so they can keep stealing from us",
              "score": 1,
              "created_utc": "2026-01-30 11:40:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wnepx",
                  "author": "Avocados6881",
                  "text": "So you are also giving much more money to Dram makers/NVidia so they keep robbing from us",
                  "score": 1,
                  "created_utc": "2026-02-01 03:05:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28crw3",
          "author": "pab_guy",
          "text": "Opus 4.5 gets a 1539 and Sonnet 4.5 gets a 1521.  That 18 points represents the difference between an OK but still stupid model and a very capable model that can handle most coding tasks end to end on it's own.\n\nThe 30 point difference makes me think I don't want to touch open models for coding ATM.  But I have access to unlimited Opus so it's an easy call for me lol.",
          "score": 2,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cncoz",
              "author": "forgotten_airbender",
              "text": "How does one get unlimited opus?Â ",
              "score": 2,
              "created_utc": "2026-01-29 04:02:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cy5s2",
              "author": "Grand-Management657",
              "text": "If you have unlimited opus then really its a no brainer to stick to that. In my testing over a few hours, K2.5 seems to be on par with Sonnet 4.5, maybe even slightly better (big maybe). I don't care about benchmarks or points at all, in real world usage it seems to hold up well.",
              "score": 1,
              "created_utc": "2026-01-29 05:14:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bh3ru",
              "author": "Funny-Advertising238",
              "text": "These points don't represent jack shit nothing.Â ",
              "score": 1,
              "created_utc": "2026-01-29 00:09:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o273s1h",
          "author": "fugogugo",
          "text": "okay but how is its token consumption?",
          "score": 1,
          "created_utc": "2026-01-28 11:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279ko8",
          "author": "BABA_yaaGa",
          "text": "Scores are very tight for top 10",
          "score": 1,
          "created_utc": "2026-01-28 12:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cu4g",
          "author": "Ne00n",
          "text": "Doesn't fit on my 64GB DDR4 LLM server, sad.",
          "score": 1,
          "created_utc": "2026-01-28 12:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o297snv",
          "author": "horaciogarza",
          "text": "So for coding it's better than Sonnet or Opus? If so (or not) for how much is different from a scale 1-10?",
          "score": 1,
          "created_utc": "2026-01-28 17:58:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ac7e3",
          "author": "Torodaddy",
          "text": "Qwen 3 coder 30b is pretty good thats my goto for open models",
          "score": 1,
          "created_utc": "2026-01-28 20:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2acqjf",
          "author": "ortegaalfredo",
          "text": "I ran my custom benchmarks about cybersecurity and...Kimi K2.0  thinking was definitively better. I has regressed at this subject. And it's nowhere near the commercial models like gemini or even sonnet.   \nJust my datapoint. Now the performance is almost equal to that of GLM 4.7.",
          "score": 1,
          "created_utc": "2026-01-28 20:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b686l",
          "author": "TurnUpThe4D3D3D3",
          "text": "Itâ€™s fantastic at web design. Creates beautiful websites.",
          "score": 1,
          "created_utc": "2026-01-28 23:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bb7ff",
          "author": "Freki371",
          "text": "where you seeing this? my arena.ai latest update is 23 Jan.",
          "score": 1,
          "created_utc": "2026-01-28 23:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2brxdy",
          "author": "FrankMillerMC",
          "text": "Where did Minimax go?",
          "score": 1,
          "created_utc": "2026-01-29 01:05:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cu369",
          "author": "forgotten_airbender",
          "text": "Waiting for swe rebench",
          "score": 1,
          "created_utc": "2026-01-29 04:46:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cxg0e",
          "author": "Grand-Management657",
          "text": "Its 1/5 the price but even cheaper if you use it through a subscription like nano-gpt where each request comes out to $0.00013. And that's regardless of input or output size.   \n  \n$8/month for 60,000 requests is hard to beat. It's basically unlimited coding or whatever your use case is, but you can also switch models and have access to the latest models without having to change providers each time a new and better model releases. For coding K2.5 Thinking is a beast and essentially on par, if not better than Sonnet 4.5 IMO\n\nHere's my referral for a web discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
          "score": 1,
          "created_utc": "2026-01-29 05:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e78jj",
              "author": "Drizzity",
              "text": "Yeah the only problem is k2.5 is not working on nano-gpt at the moment",
              "score": 1,
              "created_utc": "2026-01-29 11:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e83th",
                  "author": "Grand-Management657",
                  "text": "Which harness are you using? I found nanocode to work fine. There was an issue with multi-turn tool calling which they are fixing right now. But otherwise it works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d20gy",
          "author": "alexeiz",
          "text": "I tried it via Ollama cloud and claude code.  If feels like Sonnet 4.5 on my tasks.",
          "score": 1,
          "created_utc": "2026-01-29 05:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2deeh7",
              "author": "goingsplit",
              "text": "how can you use any model on claude code?",
              "score": 1,
              "created_utc": "2026-01-29 07:23:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dfbw6",
          "author": "This_Lemon2165",
          "text": "wow, its amazing",
          "score": 1,
          "created_utc": "2026-01-29 07:31:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhyov",
          "author": "evilbarron2",
          "text": "I get 404 errors in goose, opencode, openwebui and anythingllm every time it tries to use a tool. Quick search shows Iâ€™m not the only one. How did you folks solve that?Â ",
          "score": 1,
          "created_utc": "2026-01-29 07:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dvgdx",
          "author": "jasonhon2013",
          "text": "I love kimi but the weight is like â€¦. To heavy",
          "score": 1,
          "created_utc": "2026-01-29 10:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2i3crr",
          "author": "XAckermannX",
          "text": " Lmao Gemini pro is awful, and its no3.",
          "score": 1,
          "created_utc": "2026-01-29 23:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ii2ty",
          "author": "lc1402",
          "text": "gpt 5.2 is underrated",
          "score": 1,
          "created_utc": "2026-01-30 00:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iwlty",
          "author": "Agreeable_Asparagus3",
          "text": "Great, it would be a great idea using it with claude code cli",
          "score": 1,
          "created_utc": "2026-01-30 01:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ks9eu",
          "author": "sreekanth850",
          "text": "This is true in my case, kimi outperformed claude in many tasks.",
          "score": 1,
          "created_utc": "2026-01-30 09:58:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lb3zp",
              "author": "cranberrie_sauce",
              "text": "how do u guys run this?",
              "score": 1,
              "created_utc": "2026-01-30 12:29:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lxnf8",
                  "author": "sreekanth850",
                  "text": "[https://www.kimi.com/](https://www.kimi.com/) 7 days free trial you can test",
                  "score": 1,
                  "created_utc": "2026-01-30 14:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mcak2",
          "author": "Itchy-Cost4576",
          "text": "lendo os comentarios, as pessoas estao dividas em suas tarefas, que na qual, cada AI colapsa conforme o estado da rede que elas suportam inferir para linha de codigo, dizer qual seria a melhor que a outra, no meu ver bem irrelevante, se nao der o contexto de que, para que e o que; ja que cada um tem uma forma de programar.",
          "score": 1,
          "created_utc": "2026-01-30 15:43:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mfwt5",
          "author": "Beautiful_Egg6188",
          "text": "im using the kimi k2.5 thinking free version. And its so good. you just need to know some basics and rookie structural knowledge, and they do incredible job with minimal input. ",
          "score": 1,
          "created_utc": "2026-01-30 15:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mn8jq",
          "author": "Ok-Success-9156",
          "text": "Still on Opus train but now I really need to try Kimi...",
          "score": 1,
          "created_utc": "2026-01-30 16:32:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2upo20",
          "author": "TameBus",
          "text": "Iâ€™m enjoying working with this",
          "score": 1,
          "created_utc": "2026-01-31 20:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z0onk",
          "author": "SVG-CARLOS",
          "text": "I honestly was looking forward to that happening",
          "score": 1,
          "created_utc": "2026-02-01 14:13:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o320jqx",
          "author": "commandedbydemons",
          "text": "I've been blasting it on Synthetic for huge refactors and its been great. Token hungry, but great.\n\nIf you need a referral, to try, think its 50% off first month.\n\nSince I have a yearly sub with [z.ai](http://z.ai) also, hoping GLM-5 kills too.",
          "score": 1,
          "created_utc": "2026-02-01 22:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cqi9",
          "author": "BigMagnut",
          "text": "Isn't it a trillion parameters? Doesn't seem very efficient. What am I missing here?",
          "score": 1,
          "created_utc": "2026-01-28 15:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cxv3i",
              "author": "Grand-Management657",
              "text": "It only activates 32b parameters at a time",
              "score": 2,
              "created_utc": "2026-01-29 05:12:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blibm",
          "author": "Crinkez",
          "text": "Bad benchmark site, I don't see the best coding model (GPT5.2) on it. Wouldn't trust that benchmark.",
          "score": 1,
          "created_utc": "2026-01-29 00:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275acz",
          "author": "ptear",
          "text": "Do you have the link to the leaderboard from the screenshot, I couldn't find the model listed and wanted to see that.",
          "score": 0,
          "created_utc": "2026-01-28 11:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ofxf",
          "author": "LocoMod",
          "text": "7th is the 6th loser",
          "score": 0,
          "created_utc": "2026-01-28 13:46:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o287yd5",
              "author": "SpicyWangz",
              "text": "You never know. Itâ€™s possible theyâ€™re all losers",
              "score": 3,
              "created_utc": "2026-01-28 15:22:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o291zpq",
          "author": "datosweb",
          "text": "lo estuve probando y comparando con glm 4.7 y estan muy muy parejos",
          "score": 0,
          "created_utc": "2026-01-28 17:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27914k",
          "author": "Asleep-Ingenuity-481",
          "text": "Barely open but I guess it counts. Honestly im getting tired of these models being insanely large. Though I guess we can kind of just go with it setting the bar for performance for smaller models as well.",
          "score": -3,
          "created_utc": "2026-01-28 12:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o285kwa",
              "author": "pneuny",
              "text": "I feel like the Qwen models are one of the few that target hardware people actually have in their homes, though I certainly appreciate these larger open models that move the research forward for future small models. Though right now I have free access to Gemini given the free year for students, plus AI Studio, but I still keep an eye out for when the free lunch eventually ends.\n\nThough I'd probably hook up my local models to Vertex AI Search anyways to get that web result grounding without needing to load in hundreds of webpages on a home computer.\n\nIf I had to switch now, I'd probably do Qwen 3 VL 4b hooked up to Vertex AI Search and that would probably be enough. That would fit on my 16 GB RX 9070 XT.",
              "score": 2,
              "created_utc": "2026-01-28 15:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28lan8",
                  "author": "chodemunch6969",
                  "text": "You should try GLM 4.7 Flash if you haven't already. It's next level for the 30ba3b MoE weight class that can reasonably run on your own metal.",
                  "score": 4,
                  "created_utc": "2026-01-28 16:21:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bbpnu",
                  "author": "Asleep-Ingenuity-481",
                  "text": "That's where I stand, I don't like them due to their size, but I like them because they set the bar for smaller models to aspire to. Hell im pretty sure that qwen3.5/4 will probably beat deepseek R1 at around 14-40b params.",
                  "score": 1,
                  "created_utc": "2026-01-28 23:41:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n1gpd",
                  "author": "morfr3us",
                  "text": "nah GLM has been badly beating Qwen for a few years now (on the limited hardware front)",
                  "score": 1,
                  "created_utc": "2026-01-30 17:36:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tuiv",
              "author": "z_3454_pfk",
              "text": "then train your own instead of whining",
              "score": 3,
              "created_utc": "2026-01-28 14:14:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o285fk2",
                  "author": "into_devoid",
                  "text": "But I want people to work for free and focus on MEEE! Â These models will fit on consumer hardware sometime within the next 5-10yrs most likely. Â The difference between a 30B and 600+ model are staggering in terms of intelligence and real world utility. Â Theyâ€™re probably just planning ahead instead of wasting training time on models that will never be good enough for 95% of most tasks.",
                  "score": 1,
                  "created_utc": "2026-01-28 15:11:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27cmjq",
          "author": "Niwa-kun",
          "text": "I wish i could load Grok 4 into Antigravity, it's actually not bad. I used it in-tangent with Gemini 3 Pro, together they figured out alot of stuff for my project that 1 alone would fail at--but i had to keep being the meditator for them.",
          "score": -4,
          "created_utc": "2026-01-28 12:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27i9w0",
              "author": "Equivalent_Plan_5653",
              "text": "No thanks",
              "score": 5,
              "created_utc": "2026-01-28 13:12:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27sl6e",
          "author": "eihns",
          "text": "ON MY TESTS (cli): its using 1 or 2 subagent while coding uhhhh... anyway, its 90% \\[if u ask me\\] of chatgpt 5.2 /codex, so  why bother? Maybe next version wont immediatly touch my prod.db even tho the promt and agents and everything on that whole repo told him to not touch it xD Tests are irrelevant if the f\\* thing doenst understand what youre talking about... or how to start a dev server.... (all agents were able to do it without any help, except him) xD he just waited till bash timedout and \"tought\" the sever is running did screenshots of \"not reachable\" and told me the fix is working :D so yeah, if you want to stress a developer, then just let 100 of those agents run :D",
          "score": -1,
          "created_utc": "2026-01-28 14:07:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtvp74",
      "title": "GLM-5 Coming in February! It's confirmed.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/rq0meza173hg1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2026-02-02 13:56:14",
      "score": 777,
      "num_comments": 133,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o37z0u2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-02 20:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35s7ih",
          "author": "Septerium",
          "text": "My gguf files get so old so fast LoL",
          "score": 135,
          "created_utc": "2026-02-02 14:30:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o362j25",
              "author": "Zestyclose839",
              "text": "My external drive can only take so many more weights...",
              "score": 40,
              "created_utc": "2026-02-02 15:23:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o36l4yl",
              "author": "Turbulent_Pin7635",
              "text": "I was eargely following the releases. Now I'm just waiting for when the technology stabilizes. That is only one year since R1.",
              "score": 19,
              "created_utc": "2026-02-02 16:50:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36z8di",
                  "author": "ClimateBoss",
                  "text": "MAKE AIR GREAT AGAIN!  We want GLM AIR!",
                  "score": 12,
                  "created_utc": "2026-02-02 17:54:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o365q24",
              "author": "jonydevidson",
              "text": "By the time they're finished downloading, they're already outdated.",
              "score": 25,
              "created_utc": "2026-02-02 15:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37c9tu",
                  "author": "seamonn",
                  "text": "By the time they've loaded into memory, they're already outdated.",
                  "score": 10,
                  "created_utc": "2026-02-02 18:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o39p7sw",
              "author": "Ok-Attention2882",
              "text": "At this point I'm actually worried about wearing out the solid state flash NAND with all these downloads.",
              "score": 3,
              "created_utc": "2026-02-03 02:04:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38728r",
              "author": "huffalump1",
              "text": "Xfinity be like: \"1.2 Tb/month is a reasonable data cap for your gigabit connection\"",
              "score": 2,
              "created_utc": "2026-02-02 21:17:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35spwi",
          "author": "bootlickaaa",
          "text": "Really hoping it beats Kimi K2.5 so I can actually switch back to using my annual [Z.ai](http://Z.ai) Pro plan.",
          "score": 47,
          "created_utc": "2026-02-02 14:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36ghfs",
              "author": "GreenHell",
              "text": "Just because a newer model is better, does not mean the older model is bad.",
              "score": 28,
              "created_utc": "2026-02-02 16:28:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38l92b",
                  "author": "ReMeDyIII",
                  "text": "Yea, but the competition is so wide open that there's no point in using an inferior model either.",
                  "score": 15,
                  "created_utc": "2026-02-02 22:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38rqqe",
              "author": "whyyoudidit",
              "text": "did you even notice the difference? because I am still using 4.7",
              "score": 4,
              "created_utc": "2026-02-02 22:59:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38snbo",
                  "author": "bootlickaaa",
                  "text": "Yes I find K2.5 more like Opus 4.5 and GLM-4.7 more like Sonnet 4.5. Still completely passable and a great value which is why I bought their annual Pro plan. But I got a month of Kimi Code Pro (\"Allegreto\") plan just to try it out and will keep using it at least until GLM-5 comes out.",
                  "score": 7,
                  "created_utc": "2026-02-02 23:04:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35n5l6",
          "author": "SrijSriv211",
          "text": "**Avocado** ðŸ—£ï¸",
          "score": 84,
          "created_utc": "2026-02-02 14:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35nh2a",
              "author": "SlowFail2433",
              "text": "By far most hyped for avocado yes",
              "score": 23,
              "created_utc": "2026-02-02 14:05:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35rz27",
                  "author": "lacerating_aura",
                  "text": "How come? Has there been any signs of these new meta models avocado and mango being open weights? Afik its exactly opposite, hard closed weights.",
                  "score": 31,
                  "created_utc": "2026-02-02 14:29:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36ec2s",
              "author": "LostTheElectrons",
              "text": "Thanksss",
              "score": 2,
              "created_utc": "2026-02-02 16:18:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38gq7t",
              "author": "SillypieSarah",
              "text": "ðŸ¥‘ðŸ¥‘ðŸ¥‘â€¼ï¸",
              "score": 2,
              "created_utc": "2026-02-02 22:03:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3c7waq",
              "author": "DiscombobulatedAdmin",
              "text": "Isn't it true that Avocado is closed source?  I'm hearing that through some other outlets, but I haven't kept up with it lately.",
              "score": 2,
              "created_utc": "2026-02-03 13:35:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3cbqqb",
                  "author": "SrijSriv211",
                  "text": "Tbh idk man. It might be open or closed. Meta has done a lot of open source work lately but that's also true that many leaks & rumors suggest their next big model might be closed source. Only time will tell.",
                  "score": 1,
                  "created_utc": "2026-02-03 13:57:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o368dj9",
          "author": "Exciting-Mall192",
          "text": "I hope DeepSeek V4 is multimodal...",
          "score": 26,
          "created_utc": "2026-02-02 15:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35pc8a",
          "author": "theghost3172",
          "text": "so can we atleast hope for glm 5 air?",
          "score": 47,
          "created_utc": "2026-02-02 14:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o362k4i",
              "author": "Marksta",
              "text": "In 2 weeks ðŸ¤£ I don't blame them for boo-boos happen but boy was giving such a concrete time window and then just not ever releasing it brutal",
              "score": 21,
              "created_utc": "2026-02-02 15:23:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o360uy0",
              "author": "fizzy1242",
              "text": "I hope, but wouldn't count on it",
              "score": 7,
              "created_utc": "2026-02-02 15:15:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35wxb7",
              "author": "Leflakk",
              "text": "I feel like the air family does not really exist at the end",
              "score": 6,
              "created_utc": "2026-02-02 14:55:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35z06x",
          "author": "Junior_Secretary9458",
          "text": "DeepSeek V4 uses the Engram structure, right? Excited to see if it holds up in practice.",
          "score": 24,
          "created_utc": "2026-02-02 15:06:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o360cnz",
              "author": "SlowFail2433",
              "text": "Not sure how confirmed that is",
              "score": 14,
              "created_utc": "2026-02-02 15:12:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o39tu1a",
              "author": "Haoranmq",
              "text": "It's more like an exploration.",
              "score": 2,
              "created_utc": "2026-02-03 02:31:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35nfqz",
          "author": "International-Try467",
          "text": "Why should we trust a random on X about these (not the GLM staff)",
          "score": 34,
          "created_utc": "2026-02-02 14:05:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35t61n",
              "author": "Charuru",
              "text": "First list is not trustworthy but the comment probably is.",
              "score": 19,
              "created_utc": "2026-02-02 14:35:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35npyk",
              "author": "SlowFail2433",
              "text": "A lot of these I have seen additional rumours/leaks/confirmation elsewhere",
              "score": 17,
              "created_utc": "2026-02-02 14:06:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o362jfq",
                  "author": "Terminator857",
                  "text": "Would be interested in details.Â ",
                  "score": 4,
                  "created_utc": "2026-02-02 15:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35qzlt",
              "author": "rerri",
              "text": "Looks to me like Jietang is a GLM developer, no? Or maybe the info here is dated and he is no longer part of the team and is now just making shit up on X?\n\nhttps://keg.cs.tsinghua.edu.cn/jietang/",
              "score": 19,
              "created_utc": "2026-02-02 14:24:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35vdm1",
                  "author": "International-Try467",
                  "text": "No not that the guy above him.",
                  "score": 13,
                  "created_utc": "2026-02-02 14:47:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35sv0m",
                  "author": "SlowFail2433",
                  "text": "Probably still connected anyway",
                  "score": 2,
                  "created_utc": "2026-02-02 14:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35wt27",
              "author": "Mochila-Mochila",
              "text": "*on Twitter",
              "score": 8,
              "created_utc": "2026-02-02 14:54:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36pemy",
          "author": "sine120",
          "text": "GLM IPO's recently, right?  I would be skeptical that it'll be open weights.  There's plenty of good open weight coding models now, but just like with Qwen3-Max I wouldn't bet on seeing the GLM and Minimax dropping their best models anymore.  Would love to be proven wrong.",
          "score": 6,
          "created_utc": "2026-02-02 17:09:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35msrz",
          "author": "Psyko38",
          "text": "No Qwen 4? But a 3.5, when the 3.5 is the 2507.",
          "score": 5,
          "created_utc": "2026-02-02 14:01:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35ndrj",
              "author": "SlowFail2433",
              "text": "If 3.5 is a sub-version then 2507 is a sub-sub-version",
              "score": 7,
              "created_utc": "2026-02-02 14:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35obt2",
                  "author": "Psyko38",
                  "text": "Yes, well, when we went from the normal version to the sub-version, it was like night and day.",
                  "score": 3,
                  "created_utc": "2026-02-02 14:10:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35o1go",
              "author": "Available-Craft-5795",
              "text": "Qwen4 would be a huge upgrade",
              "score": 2,
              "created_utc": "2026-02-02 14:08:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o364elv",
          "author": "UserXtheUnknown",
          "text": "I've great expectation both for DeepSeek and GLM.",
          "score": 6,
          "created_utc": "2026-02-02 15:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35rqfo",
          "author": "Kubas_inko",
          "text": "What happened to deepseek R series?",
          "score": 4,
          "created_utc": "2026-02-02 14:28:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35svy4",
              "author": "TheDeviceHBModified",
              "text": "R stood for Reasoning. Their more recent models are hybrid (with toggleable reasoning), so there's no longer a separate R-series.",
              "score": 34,
              "created_utc": "2026-02-02 14:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35udit",
          "author": "Zeikos",
          "text": "> Grok 4.20\n\nOh my God, Musk is so uncreative.",
          "score": 21,
          "created_utc": "2026-02-02 14:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35v0hk",
              "author": "StaysAwakeAllWeek",
              "text": "It would honestly be funnier if they skipped to 4.3 and refused to elaborate",
              "score": 12,
              "created_utc": "2026-02-02 14:45:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35xpt2",
                  "author": "Direct_Turn_1484",
                  "text": "Yeah he canâ€™t do that. â€œGuys! Everybody look at me Iâ€™m so cool!â€ Is kind of his thing now. Itâ€™s pretty sad.",
                  "score": 6,
                  "created_utc": "2026-02-02 14:59:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o383f7r",
              "author": "GeorgeSC",
              "text": "next one will be Dork 6.9, just wait and see",
              "score": 2,
              "created_utc": "2026-02-02 21:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3897tq",
              "author": "BusRevolutionary9893",
              "text": "Do you think Musk cares about the name of a minor version update?",
              "score": 0,
              "created_utc": "2026-02-02 21:28:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38eib3",
                  "author": "Zeikos",
                  "text": "Yes, have you seen the name of the tesla car models?  \nS 3 X Y",
                  "score": 6,
                  "created_utc": "2026-02-02 21:53:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39ymz0",
                  "author": "Apprehensive-End7926",
                  "text": "Bffr, this is exactly his kind of 2011 internet humour",
                  "score": 1,
                  "created_utc": "2026-02-03 02:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35uo3e",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -5,
              "created_utc": "2026-02-02 14:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35w49s",
                  "author": "Zeikos",
                  "text": "Unironically",
                  "score": 3,
                  "created_utc": "2026-02-02 14:51:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35m5dk",
          "author": "SlowFail2433",
          "text": "Was expecting the big meta one in the summer",
          "score": 6,
          "created_utc": "2026-02-02 13:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35meqp",
              "author": "Difficult-Cap-7527",
              "text": "Meta disappeared like it never existedÂ ",
              "score": 28,
              "created_utc": "2026-02-02 13:59:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35n866",
                  "author": "SlowFail2433",
                  "text": "They havenâ€™t, it is just a media narrative \n\n\nSince Llama 4 they have gone on the largest and most aggressive hiring spree in the industry as well as one of the largest hardware scale-outs\n\n\nIf anything they are one of the most active labs in terms of scale-out activity at the moment",
                  "score": -18,
                  "created_utc": "2026-02-02 14:04:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35tu8k",
          "author": "ShadowBannedAugustus",
          "text": "I am expecting a big nothing burger with all the big closed ones, a very, very small improvement over the current ones.",
          "score": 10,
          "created_utc": "2026-02-02 14:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o360jv7",
              "author": "SlowFail2433",
              "text": "Why? Progress curves are all still fully exponential currently",
              "score": -4,
              "created_utc": "2026-02-02 15:13:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36gkgf",
                  "author": "ShadowBannedAugustus",
                  "text": "Exponential where? To me it feels like they are very logarithmic since about GPT 3.5.",
                  "score": 12,
                  "created_utc": "2026-02-02 16:29:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o363urk",
                  "author": "Terminator857",
                  "text": "Closed = boring.  Open = exciting.",
                  "score": 3,
                  "created_utc": "2026-02-02 15:29:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36xxlr",
          "author": "Far-Low-4705",
          "text": "Ooooh qwen3.5!!!!\n\nPls pls pls, 80b moe vision model.",
          "score": 2,
          "created_utc": "2026-02-02 17:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ssvx",
          "author": "Background-Ad-5398",
          "text": "wheres Gemma 4 google? you're the only one who crams a trillion tokens in small models making them actually good with world lore",
          "score": 2,
          "created_utc": "2026-02-02 20:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o389hke",
          "author": "customgenitalia",
          "text": "\\+ Sonnet 5",
          "score": 2,
          "created_utc": "2026-02-02 21:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38kw3p",
          "author": "ReMeDyIII",
          "text": "Crap, someone said it'd be Claude 5.0, not 4.6. Boo...\n\nWell if they reduce costs, then all's forgiven.",
          "score": 2,
          "created_utc": "2026-02-02 22:24:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38w6ku",
          "author": "TomLucidor",
          "text": "Just another Air model will be good enough. (Maybe a Flash model with hacks like hybrid attention and Engrams would be good too)",
          "score": 2,
          "created_utc": "2026-02-02 23:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35rji0",
          "author": "hejj",
          "text": "Bigger numbers yay",
          "score": 4,
          "created_utc": "2026-02-02 14:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3612bp",
              "author": "RedParaglider",
              "text": "Numbers go up and to the right.",
              "score": 6,
              "created_utc": "2026-02-02 15:16:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36tr9o",
          "author": "leonbollerup",
          "text": "And all I want is an even faster gpt-oss-20/30b v2",
          "score": 2,
          "created_utc": "2026-02-02 17:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o389wig",
              "author": "chickN00dle",
              "text": "a faster, multimodal, long context gpt oss ðŸ™†â€â™‚ï¸",
              "score": 2,
              "created_utc": "2026-02-02 21:31:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36aya7",
          "author": "braydon125",
          "text": "Perfect timing for my 300gb to come online....",
          "score": 1,
          "created_utc": "2026-02-02 16:03:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36xi1z",
          "author": "IulianHI",
          "text": "Been using GLM-4.7 for coding help lately and it's been surprisingly solid. Curious if GLM-5 will bring better agentic capabilities or just scale up. Ngl pretty excited to see what they've got.",
          "score": 1,
          "created_utc": "2026-02-02 17:47:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o377lr7",
              "author": "ImmenseFox",
              "text": "Here's hoping! GLM-4.7 via OpenCode, Exa & Context7 MCPs mostly does everything I want it to do but there has been situations where it struggled and I needed to pop out Opus 4.5 to sort.\n\nI use the GLM Coding Plan and quite happy with it overall so a new(er) model will just be a bonus and hopefully remove my need to use Opus!  \n  \n\\~ Sonnet 5 if the leaks are true is also on the Horizon and I still pay monthly for Claude Pro so looking forward to that one too but if GLM 5 can beat Opus 4.5 - I'll be cancelling my Anthropic Subscription (The weekly limits are a pain and I dont have Â£100 to throw at it for just hobby-ist use",
              "score": 2,
              "created_utc": "2026-02-02 18:32:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37352q",
              "author": "Dry_Journalist_4160",
              "text": "may we know your system specifications",
              "score": 0,
              "created_utc": "2026-02-02 18:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o372hs5",
          "author": "Conscious-Hair-5265",
          "text": "How are they able to iterate so fast even when they have shit chips in china ? It hasn't even been a 2 months since glm 4.7",
          "score": 1,
          "created_utc": "2026-02-02 18:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o374fui",
          "author": "SeaworthinessThis598",
          "text": "man i wont sleep for 3 weeks like that , i love how much i hate this . and i hate how much i love it .",
          "score": 1,
          "created_utc": "2026-02-02 18:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37j4k6",
          "author": "synn89",
          "text": "I'm sure it'll release in two weeks.\n\nhttps://preview.redd.it/1phhm4h0u4hg1.jpeg?width=686&format=pjpg&auto=webp&s=52f3aaed5a75afe4be487eed18efaf108f131951",
          "score": 1,
          "created_utc": "2026-02-02 19:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37mr0q",
          "author": "archieve_",
          "text": "Chinese  New Year is coming",
          "score": 1,
          "created_utc": "2026-02-02 19:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37puxg",
          "author": "Bolt_995",
          "text": "Noting this.",
          "score": 1,
          "created_utc": "2026-02-02 19:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37zd8g",
          "author": "Individual-Hippo3043",
          "text": "I hope V4 doesn't disappoint due to inflated expectations, so that it doesn't end up like Gemini 3, which is good overall, but half the time it hallucinates answers.",
          "score": 1,
          "created_utc": "2026-02-02 20:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o387h93",
          "author": "jazir555",
          "text": "Gib now",
          "score": 1,
          "created_utc": "2026-02-02 21:19:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38bcty",
          "author": "itsnotKelsey",
          "text": "Oh let's goooo!!",
          "score": 1,
          "created_utc": "2026-02-02 21:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38cbb9",
          "author": "flywind008",
          "text": "holy s! so many models, i am more interested in in open source models but why most of them are from China ? meta move!",
          "score": 1,
          "created_utc": "2026-02-02 21:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38d050",
          "author": "power97992",
          "text": "Lol they work too muchÂ ",
          "score": 1,
          "created_utc": "2026-02-02 21:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38rks9",
          "author": "whyyoudidit",
          "text": "my body is ready. my coding plan too",
          "score": 1,
          "created_utc": "2026-02-02 22:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39dl7t",
          "author": "ReasonablePossum_",
          "text": "Grok 4/20 will be rollin lol",
          "score": 1,
          "created_utc": "2026-02-03 00:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39hezi",
          "author": "Muddled_Baseball_",
          "text": "So many man so many. It's like streaming subscriptions",
          "score": 1,
          "created_utc": "2026-02-03 01:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39n2lb",
          "author": "Federal_Spend2412",
          "text": "I hope GLM 5.0 roll out before chinese new year :D",
          "score": 1,
          "created_utc": "2026-02-03 01:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39tg4e",
          "author": "Amazing_Athlete_2265",
          "text": "Fuck yeah, it's February now!!",
          "score": 1,
          "created_utc": "2026-02-03 02:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cvxpq",
          "author": "ComplexType568",
          "text": "i love how nonchalant all these ai heads are... still waiting for gemma 4",
          "score": 1,
          "created_utc": "2026-02-03 15:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35pu0c",
          "author": "fugogugo",
          "text": "will any of them provide free inference?",
          "score": 1,
          "created_utc": "2026-02-02 14:18:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35syhx",
              "author": "SlowFail2433",
              "text": "Has anyone ever provided free inference?",
              "score": 10,
              "created_utc": "2026-02-02 14:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35zlqg",
                  "author": "basil232",
                  "text": "Groq and Cerebras definitely are doing that. Yes they try to get you hooked so you pay for their fast inference, but both of them offer a generous free tier.",
                  "score": 5,
                  "created_utc": "2026-02-02 15:09:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35zxb0",
                  "author": "fugogugo",
                  "text": "well there have been few models  giving free access for limited period on openrouter \n\nGrok 4.1 fast was free on December 2025 iirc   \nDevstral 2 was free until last week   \nGLM 4.7 Air also still free IIRC",
                  "score": 3,
                  "created_utc": "2026-02-02 15:10:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o383hbb",
                  "author": "MoffKalast",
                  "text": "Kind of everyone always has I guess? Free tiers of every major provider together cover all of my daily usage multiple times over tbh. Haven't paid for anything since GPT-4 years ago.",
                  "score": 1,
                  "created_utc": "2026-02-02 21:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o360jfa",
              "author": "yes-im-hiring-2025",
              "text": "Probably a super restricted (but free) version will be out on openrouter for a short time",
              "score": 2,
              "created_utc": "2026-02-02 15:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38wfyh",
              "author": "Cuplike",
              "text": "Literally just throw like 3 dollars every month on DeepSeek API and you'll be golden",
              "score": 2,
              "created_utc": "2026-02-02 23:24:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37jlxj",
              "author": "synn89",
              "text": "OpenCode's Zen will likely have it free for a limited time: https://x.com/ryanvogel/status/2017336961736847592",
              "score": 1,
              "created_utc": "2026-02-02 19:27:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrsy4q",
      "title": "How close are open-weight models to \"SOTA\"? My honest take as of today, benchmarks be damned.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/k38sg20q7mgg1.png",
      "author": "ForsookComparison",
      "created_utc": "2026-01-31 04:49:42",
      "score": 619,
      "num_comments": 211,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2sgve9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 14:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs2xu",
          "author": "Bananadite",
          "text": "Honestly so disappointed in Meta.  They were the ones who kickstarted this open source weights and they spent so much to assemble an AI \"superteam\" yet they failed so hard.",
          "score": 192,
          "created_utc": "2026-01-31 05:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qv8cd",
              "author": "ForsookComparison",
              "text": "I'll be devil's advocate for Llama4:\n\nthere was a brief moment where Llama4 Maverick through Lambda Labs was the cheapest and fastest way to do code completions (think: the \"Continue\" VSCode Extension or early Copilot). If you already knew what you were going to hand-write Maverick could probably nail it faster and cheaper than anything else.\n\nThis didn't last long at all.. but Maverick got some actual use from me before Qwen3.",
              "score": 68,
              "created_utc": "2026-01-31 06:00:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2w0p9h",
                  "author": "dreamkast06",
                  "text": "I too will advocate for it. It was faster and had more \"knowledge\" than the og Deepseek 3. It had implementation issues when it first came out like GPT-OSS-120B. Remember all the hate that got when it first came out cuz it seemed stupid until the templates were fixed?\n\nMakes me wonder if the multimodal aspects of Llama4 were more widely implemented maybe it'd gotten more use.",
                  "score": 6,
                  "created_utc": "2026-02-01 00:49:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tzwa2",
              "author": "RedTheRobot",
              "text": "The open source play was never to benefit them but to hurt the competition. That is why at the beginning they didnâ€™t invest heavily in AI. Zuck was still in his metaverse and pushing the Oculus. AI was just a side project. Then AI started making some serious money and meta had to jump in at the last second. They threw out huge paydays to again bump themselves up and hurt the competition but the good employees are heavily vested in OpenAI and Google.",
              "score": 6,
              "created_utc": "2026-01-31 18:38:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wqa1u",
                  "author": "iJeff",
                  "text": "Yann LeCun was the one advocating for the open approach.",
                  "score": 3,
                  "created_utc": "2026-02-01 03:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rfoi1",
              "author": "Nyghtbynger",
              "text": "Yann Lecun really is in the shadow of all the big changes in Artificial Neural Networks for 30 years now",
              "score": 15,
              "created_utc": "2026-01-31 09:04:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s4cdl",
                  "author": "aeroumbria",
                  "text": "He felt like an LLM sceptic stuck in a company that never cared about what he really wanted to do.",
                  "score": 11,
                  "created_utc": "2026-01-31 12:45:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tlsu6",
                  "author": "rabbotz",
                  "text": "I suspect both Meta and OpenAI, in their own ways, got too distracted with the AGI â€œwhatâ€™s nextâ€ question. I get where they are coming from, the current architectures will eventually hit a wall we will really want to get past (eg from the lack of a world model). But thereâ€™s so much juice to be squeezed on iterating on current models and the ecosystems around them, it ended up looking like a distraction in retrospect.",
                  "score": 7,
                  "created_utc": "2026-01-31 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ue5fk",
              "author": "kevin_1994",
              "text": "scout was not a bad model, imo. it was just released at the wrong time. it released when MoEs, reasoning models, and agentic behaviour were first beginning to emerge. it's MoE structure was more \"old-school\" like Mixtral, it didn't have reasoning, and it wasn't post trained for agentic behaviour.\n\ndespite all this, the llama models are some of the only models not trained to benchmaxx. meta still serves their llama models to billions of users. my boomer parents love that shit lol. these models not are optimized for coding and stem -- they are optimized for user engagement for more casual users",
              "score": 2,
              "created_utc": "2026-01-31 19:46:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o302ub0",
                  "author": "Zulfiqaar",
                  "text": "I really really wish they opensourced the LLaMa4 checkpoint that beat gpt-4o in user preference. but instead they decided to post-train it on STEM and underperform in all domains.",
                  "score": 1,
                  "created_utc": "2026-02-01 17:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2w1xyd",
              "author": "midnitewarrior",
              "text": "I forgot Meta was even in this. oof",
              "score": 2,
              "created_utc": "2026-02-01 00:56:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b2vsj",
              "author": "samxli",
              "text": "And to imagine the namesake of this sub was inspired by a Meta model.",
              "score": 1,
              "created_utc": "2026-02-03 07:58:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sjtbp",
              "author": "Party_Progress7905",
              "text": "They should just give up and invest in others. Theyâ€™re only wasting resources, too far behind, and theyâ€™re actively enshittifying their own products by forcing their shit AI into everything. Given Metaâ€™s track record, itâ€™s **likely** WhatsApp and ig  messages are being repurposed beyond messaging.",
              "score": -2,
              "created_utc": "2026-01-31 14:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qzyuy",
          "author": "LoveMind_AI",
          "text": "For whatever it's worth, Kimi K2.5 is performing basically as well as Gemini 3 Flash for me on visual reasoning.",
          "score": 55,
          "created_utc": "2026-01-31 06:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rq733",
              "author": "SilentLennie",
              "text": "And it's cheaper (but Gemini 3 Flash has that 1M context window which is useful at times of course).",
              "score": 10,
              "created_utc": "2026-01-31 10:45:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ytbw8",
                  "author": "cleverusernametry",
                  "text": "Have you ever used all of it's 1M context? Does it maintain response quality?",
                  "score": 1,
                  "created_utc": "2026-02-01 13:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2trpiz",
              "author": "SnooDoggos9325",
              "text": "At coding not so much. I asked k2.5 with Opencode to implement a feature. It did quite well, but there was a glitch. I asked for a fix and it made things worse. I reverted the last fix and asked Gemini 3 flash to fix it. It immediately figured out the problem and provided a proper solution.Â \nThe feature was to add a slightly transparent imgui overlay with fps and frame time percentiles in the top right corner. C++ and sdl3 GPU.\nThe glitch was with incorrect texture formats causing the overlay to be 90% transparent.Â \nAsking Kimi to fix it (twice) resulted in a hallucination that now it is fully opaque and later extending the background to full screen instead of jus the overlay window.",
              "score": 5,
              "created_utc": "2026-01-31 18:00:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y5ji2",
                  "author": "MrBIMC",
                  "text": "In my testing, all of the above listed models that I tried, these except of claude keep on making a stupid mistake that kills them:\n\nwhen working with realtime data streams, they tend to forget to add a timeout, and so in cases where stream only returns the empty data or a data in pattern that the agent doesn't expect. \n\nSo that a model gets stuck endlessly watching some stream of data. \n\nIt might be an orchestrator problem, but I tested on OpenCode, Kilo and antigravity agent, and no matter the agent, most of models behave the same in this case. Claude never misses that he needs to observe with timeout, gemini misses most of the time, but not always and other models (except of giga potato) almost never did add a timeout. \n\n\n\n\nThough, in my personal vibe, k2.5 does feel the most like 3 flash, and 3 flash is my favorite model to work with at the moment for most of the tasks.",
                  "score": 1,
                  "created_utc": "2026-02-01 10:18:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zg9bo",
                  "author": "Evening_Ad6637",
                  "text": "Yes, unfortunately Kimi isn't very good at C++. I recommend Deepseek or GLM for that.",
                  "score": 1,
                  "created_utc": "2026-02-01 15:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2quac7",
          "author": "randombsname1",
          "text": "Good list. Largely agree.\n\nFirst big release of SOTA models for this year is around the corner too. \n\nYou have leaks of ChatGPT 5.3. Claude Sonnet 4.7 and Gemini 3.5.\n\n\nI think each of the big 3 is waiting to see who goes first. Then they'll all release within a week or 2 of the first one, imo.",
          "score": 130,
          "created_utc": "2026-01-31 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qwvdc",
              "author": "Mescallan",
              "text": "OpenAI certainly is. Google hasn't really been known to stomp on releases as much as the other two. Anthropic has been doing it more recently, but only towards OpenAI IIRC.   \n  \nMy bet is Gemma 4 -> Sonnet 4.7 -> GPT 5.3 within 48 hours -> Gemini 3.5/Opus 4.7 in March.",
              "score": 45,
              "created_utc": "2026-01-31 06:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tm6wz",
                  "author": "segmond",
                  "text": "Google has no pressure to release gemma-4, llama drove gemma.  with llama dead and phi not releasing.   Google isn't threatened by mistral-small or qwen3-30b/32b.  They have demonstrated that they can do opensource, the investors don't care.    There's a reason Anthropic doesn't do open source, nothing in it for them.  The only reason OpenAI did gpt-oss is because of the Elon Musk lawsuit so they can claim they are still open.",
                  "score": 23,
                  "created_utc": "2026-01-31 17:33:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qys8j",
                  "author": "kkb294",
                  "text": "Are you sure about Gemma.? I'm eagerly waiting for the OSS ones as there is nothing in sight from the other(Claude/OpenAI/Meta) providers.",
                  "score": 5,
                  "created_utc": "2026-01-31 06:29:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rhi73",
              "author": "__Maximum__",
              "text": "Sir, this is a r/localllama",
              "score": 23,
              "created_utc": "2026-01-31 09:21:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rxxax",
              "author": "goniszewski",
              "text": "Yeah, nice list. Actually this could be a nice addition for each occurrence I am adding to this list: https://ithappenedagain.fyi/rec/new-sota-model-released-211624384e\n\nIâ€™m thinking about splitting it also into closed models and the open ones, which we can host ourselves. Could be useful for some folks out there.",
              "score": 3,
              "created_utc": "2026-01-31 11:54:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qrpbi",
          "author": "TheRealMasonMac",
          "text": "More or less, yeah. I'd say that OSS models need to invest compute in better instruction following rather than intelligence. It clearly paid dividends for Claude.",
          "score": 88,
          "created_utc": "2026-01-31 05:32:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r2sgq",
              "author": "ihexx",
              "text": "Idk what Claude's secret sauce is, but it's not instruction following.\nIdk if you remember but a big complaint everyone had with Claude is it doesn't do what you ask it to and it's too eager to do extra stuff.\nIf you try to measure instruction following, Claude scores badly; bottom of the big 3.\n\n\nThere's something else here that's giving Claude that advantage; something more like \"meta-problem understanding\" or like \"inferring missing information\". But it's not \"instruction following\"",
              "score": 60,
              "created_utc": "2026-01-31 07:04:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r4cqx",
                  "author": "kzoltan",
                  "text": "Guessing only: huge amounts of data from devs? Claude is arguably the best model for devs, so people tend to choose that. \nI havenâ€™t seen the fine print, can they use the data from subscriptions for training purposes?",
                  "score": 27,
                  "created_utc": "2026-01-31 07:18:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2r509j",
                  "author": "TheRealMasonMac",
                  "text": "Idk, for me it's always been the best at following instructions for coding. Whereas other models try to do things cleverly, it chooses the most straightforward solution that satisfies my requirements. Other models try to be clever and consequently fail to follow instructions because they're so laser-focused on being clever. If I tell it not to do something, it gets what I'm really telling it not to do without overreaching, and so on. It's also able to maintain instruction following coherency across dozens of turns whereas other models fall apart completely in just a few.",
                  "score": 12,
                  "created_utc": "2026-01-31 07:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rwtxd",
                  "author": "AuspiciousApple",
                  "text": "Yeah, claude occasionally feels like a smart person running with your task. Sometimes I give it a complex problem and it does 20 steps before stopping, when I wanted 1-2 steps.",
                  "score": 5,
                  "created_utc": "2026-01-31 11:45:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2s9d7a",
                  "author": "AlwaysLateToThaParty",
                  "text": "Claude is their workflows. That's what it feels like to me.  They seem to have embedded them in their reasoning, so that all tasks are broken down, but not too much. That lends itself to coding.\n\nI would say that this is code-centric.  While code is good, there are other ways to use llms and 'ai' (like image/video, role play, translation, domain specific analysis, etc etc), that will have a different tier list entirely.",
                  "score": 3,
                  "created_utc": "2026-01-31 13:19:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rd0s2",
                  "author": "Any_Fox5126",
                  "text": "I don't know about recently, but around the time of 3.5, it also used to be one of the most aggressively censored.",
                  "score": 2,
                  "created_utc": "2026-01-31 08:39:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2shsvz",
                  "author": "Far-Low-4705",
                  "text": "In my experience, i feel maybe older, dense models were able to understand/grasp context better. I think pretraining baked it into the model, then post training RL kind of beat that out of them (or at least made them worse at the ability since it is no longer the primary learning signal)",
                  "score": 2,
                  "created_utc": "2026-01-31 14:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tk2ad",
                  "author": "kaisurniwurer",
                  "text": "I'm ~69% sure that they are using massive activation moe.\n\nI would not be surprised to hear that their model have 100-400B active tokens. If it isn't just straight up massive dense model.",
                  "score": 2,
                  "created_utc": "2026-01-31 17:23:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ttwv2",
                  "author": "jklre",
                  "text": "My favorite behavior of Claude is when im working on a new complex problem it basically gets to a point when things are hard and its just like \"How about you just don't?\" and tries to get out of it",
                  "score": 1,
                  "created_utc": "2026-01-31 18:10:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2umeto",
                  "author": "BadgKat",
                  "text": "My honest opinion is that Claude is so good for two main reasons. \n\nThe first is harnessing, I think this is the main reason there is a benchmarking to perceived performance gap. Anthropic has built some of the best tooling, both in Claude Code and built in tooling in the browser/app. Models performance is improved when they have clear ways to do things, like create a script for themselves and run it in a sandbox and give the user an output document. The benchmarks use the api, you donâ€™t get those harnesses in the api.\n\nThe second one is a bit harder to prove, but I believe it. I think itâ€™s the SOUL doc. The study that showed when you RL a model to write malicious code and it starts talking like a Nazi proves the inverse. It stands to reason you RL a model to be moral, it writes better quality code, and does other things well. Harder to empirically prove this inverse, but I think it at least follows.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:26:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zj9a8",
                  "author": "OcelotMadness",
                  "text": "It's because they're scanning and destroying real books en masse. Don't really agree with that practice, but I'm willing to bet its why Claude mimics English storytelling so well.",
                  "score": 1,
                  "created_utc": "2026-02-01 15:48:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34pzya",
                  "author": "ironicstatistic",
                  "text": "I think the truth of the matter is that Claude has the smallest userbase of the big 3, so can bother spending more compute per prompt. \n\nnormies wont care, they will use what they are familiar with/what is cheaper.  \nThe pros will, they will write better prompts and give claude better data - so they become more premiere. \n\nThink of the amount of slop Open AI have to STOP their models from training on, and how hard it must be not to cause that to creep in. Think about how many more users OpenAI and Gemini have compared to Claude. \n\nOr maybe its secret sauce idk",
                  "score": 1,
                  "created_utc": "2026-02-02 09:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2s5xf7",
              "author": "aeroumbria",
              "text": "I would say maybe we should treat some diversity in \"obedience\" as a strength rather than deficiency. For instance, DeepSeek V3.2 is not particularly good at following instructions because it doubts itself too much and will spend too much time debating how to interpret the instructions, but this also makes it ideal for cross-checking results from other models, just because the \"mental model\" of DeepSeek is so distinct from other models, and it does not take even its own reasoning for granted.",
              "score": 2,
              "created_utc": "2026-01-31 12:56:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rqw66",
              "author": "hellomistershifty",
              "text": "That's what gets me with Gemini, I don't get how it scores so high. It programs great solutions to problems that didn't exist and uses tools like a caveman",
              "score": 2,
              "created_utc": "2026-01-31 10:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tfd2m",
              "author": "RhubarbSimilar1683",
              "text": "Claude definitely takes more compute to run. It all adds up: it's slower. Has lower free rate limits. Uses custom Annapurna labs AWS hardware. Has higher API pricing. Maybe the model has a diffusion LLM component while being largely autoregressive. They could then train RL on that.Â  Or the transformer architecture has a symbolic ai component and/or a diffusion component because it sometimes adjusts its text generation speed. Then train RL on that.Â \n\n\nThe custom AWS hardware has somewhat mitigated the speed issue but it doesn't add up to the higher speed of the hardware.Â  But it could also be them trying to be profitable. But I doubt it because otherwise they would nuke their models like openai with gpt-5\n\n\nÂ I still remember they had those issues during their Nvidia days in 2024 and I don't think that's changed. I don't think those things can be fixed with better infrastructure. They need hardware.Â ",
              "score": 1,
              "created_utc": "2026-01-31 17:00:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tnubx",
              "author": "CuriouslyCultured",
              "text": "I don't feel like Claude is very good at instruction following. It's superpower is taking under specified problems and producing \"good enough\" output. GPT5.2 is anal about requirements, whereas Claude treats them as suggestions, and cares more about \"flair\"",
              "score": 1,
              "created_utc": "2026-01-31 17:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qt1jk",
          "author": "TomLucidor",
          "text": "You need to make another tier list for the <120B model and <48B models when compared to proprietary SOTA's smaller equivalents.",
          "score": 49,
          "created_utc": "2026-01-31 05:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qt7yf",
              "author": "ForsookComparison",
              "text": "Will do. I have a totally separate set of projects that are hosted on my janky home server, so I've got plenty of hot takes for models of that size.",
              "score": 18,
              "created_utc": "2026-01-31 05:44:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rtl1i",
                  "author": "Competitive_Ad_5515",
                  "text": "!remindme 1 week",
                  "score": 2,
                  "created_utc": "2026-01-31 11:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rxi6l",
              "author": "goniszewski",
              "text": "We should probably distinguish between SOTA (closed), and Open SOTA (open source or open weights). The latter maybe with a reference from the first one. \nIâ€™m always researching new candidate for my locally run model and that would be very helpful.",
              "score": 2,
              "created_utc": "2026-01-31 11:50:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rf50v",
          "author": "LosEagle",
          "text": "Mr. anus logo is the overlord, sadly.Â ",
          "score": 15,
          "created_utc": "2026-01-31 08:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qolio",
          "author": "ThatRandomJew7",
          "text": "Have you tried Kimi K2.5 yet? Because it's a pretty big step up, in my experience it outperforms Claude Sonnet and sometimes trades blows with Opus",
          "score": 57,
          "created_utc": "2026-01-31 05:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qoyur",
              "author": "ForsookComparison",
              "text": "I have. I hold Deepseek V3.2 in very high standing so take my word that it's a big deal that Kimi sits where it is on this chart.\n\nBut I'm confident on where I put it.",
              "score": 29,
              "created_utc": "2026-01-31 05:11:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qpizl",
                  "author": "ThatRandomJew7",
                  "text": "Fair. I haven't used Deepseek much since R1, the lack of multimodal really hurts it for general use.\n\nIdk, in my experience I've found Kimi on par with Gemini 3 Pro at least. My usual \"weird benchmark that they're definitely not training on' is telling it to make a Pico-8 \"demake\" of my favorite UFO50 game. Only Kimi and Gemini could actually produce something that didn't instantly error, and Kimi was more graphical (though it struggled with sprites). Even Sonnet couldn't do it. Haven't tried Opus though since I'm not paying for Claude at this point",
                  "score": 18,
                  "created_utc": "2026-01-31 05:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2sz1jn",
              "author": "IsometricRain",
              "text": "What can it do better than sonnet? Curious what your use case is.\n\nI'm thinking of testing it out but Sonnet has been very solid for me, with GLM being good enough for smaller tasks and quick research",
              "score": 2,
              "created_utc": "2026-01-31 15:42:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qmrph",
          "author": "ForsookComparison",
          "text": "Tiers are ranked.\n\nGrok takes into account Grok-4.1 and Grok-Coder-1-Fast, not the opened Grok-2 weights.\n\nQwen is based on my experiences with Qwen3-235B mostly and the 'goodness' of the ease of hosting a model of that power. Closed-Weight Qwen3-Max would likely rank similarly despite being a hair better.\n\nGemini 3 Pro will beat ChatGPT on some days, but GPT-5.2-Pro (or 'extra' or whatever it's called nowadays) will beat it if you can wait.\n\nEverything left out isn't meant as bad (for example: I use Nemotron Nano near-daily) they've just never been SOTA competitors in their size-class to me, so they don't come up in any SOTA thoughts I'll have.\n\nGLM's placement I think can stretch all the way from 4.5-air and 4.6v to the full 4.7. It does so much better than those beneath it in agentic work, but once it's time to make decisions the gaps between it at the tier above it show.\n\nI love Llama and use Llama 3.3 70B up until very recently when Qwen3-VL-32B, Seed-OSS-36B, and Nemotron-Super-49b-v1.5 started to show knowledge-depth that finally kicked it off my hard drive.\n\nI don't have many vision or ocr use-cases in any of my work/projects so those capabilities weren't taken into account.",
          "score": 33,
          "created_utc": "2026-01-31 04:55:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "o38fs71",
              "author": "raucousbasilisk",
              "text": "Iâ€™d recommend trying Devstral 2. Neat little model.",
              "score": 1,
              "created_utc": "2026-02-02 21:59:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2r75uo",
              "author": "wouldacouldashoulda",
              "text": "Honest question, what do you do that you need so many tokens that you would consider anything but just the big 3?",
              "score": -1,
              "created_utc": "2026-01-31 07:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uifr7",
                  "author": "ForsookComparison",
                  "text": "Try coding a large side project spanning several repos when you're not rich - then try and have those side projects require LLMs as a piece of the pipelines.\n\nYou need to get crafty and spend hours with everything to tune performance-per-dollar and time-vs-output",
                  "score": 5,
                  "created_utc": "2026-01-31 20:07:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2s4xot",
                  "author": "fractalcrust",
                  "text": "gooning",
                  "score": 2,
                  "created_utc": "2026-01-31 12:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ri2xg",
          "author": "nycigo",
          "text": "Devstral 2 is pretty good too",
          "score": 12,
          "created_utc": "2026-01-31 09:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdgvy",
          "author": "k_means_clusterfuck",
          "text": "If this is a tier list for agentic coding, you'd be crazy to place Minimax and ziphu that far down",
          "score": 6,
          "created_utc": "2026-01-31 08:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u5e57",
              "author": "Vaptor-",
              "text": "I use minimax for daily with claude code. With good mcp (serena, context7), proper context management, and subagents it's really good. It also cost like $10 per mo and I never hit 50% of the 5 hours limit.",
              "score": 2,
              "created_utc": "2026-01-31 19:04:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s4qri",
          "author": "gnaarw",
          "text": "I've used kimi 2.5 for a semi large go project this week and it seems better than Sonnet and gippity\n\nAs for open weight models we can actually run at home... GLM fast, devstral 2, OSS and qwen are still quite a way off. Minimax, GLM and Kimi (cloud hosted which is ok for most European projects for example) are actually on par with a good junior on crack vs the smaller models acting like a junior who's hand you have to hold while crossing the street...",
          "score": 7,
          "created_utc": "2026-01-31 12:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzsia",
          "author": "cyberdork",
          "text": "This ranking is just as useless as the benchmarks, since you don't mention anything about the use case.     \n    \nI'm so tired about people talking how model XYZ is the best, just to read somewhere in the comments that they mean for coding. The vast majority of people DON'T vibe code and have totally different use cases.",
          "score": 20,
          "created_utc": "2026-01-31 12:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s57h0",
              "author": "aeroumbria",
              "text": "Yeah, I don't believe a second that even on coding, there exists one model that does \"everything\" better than the next model.",
              "score": 4,
              "created_utc": "2026-01-31 12:51:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qxbxo",
          "author": "gcavalcante8808",
          "text": "Opus nearly reconstructed a svg with perfection based on a complex banner... for this case even the other closed models didn't even get closer.\n\nFor code: python, rust, DDD, clean architecture it's all about the same; good enough, the open models works wonderfully for a long time now. \n\nSo the answer depends on the task.",
          "score": 4,
          "created_utc": "2026-01-31 06:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s69q2",
          "author": "True_Requirement_891",
          "text": "I've seen minimax do very surprising things. It should be in the SOTA territory.",
          "score": 3,
          "created_utc": "2026-01-31 12:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rihex",
          "author": "raiffuvar",
          "text": "ðŸ¤£ ðŸ¤£ \nDeepseek \"feels like early 2025 sota\". \nThe model which liturally make other to continue with improving.",
          "score": 6,
          "created_utc": "2026-01-31 09:31:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rn05q",
              "author": "Kakami1448",
              "text": "Back in early 2025, so no lies here",
              "score": 7,
              "created_utc": "2026-01-31 10:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rgv1g",
          "author": "nullmove",
          "text": "Where I disagree with this:\n\n- GPT-5.2(-codex) > Slopus 4.5, these are top 2 in on their own\n- Gemini has unparalleled knowledge, but for coding it shares third place with Kimi K2.5 which is really impressive\n- I don't find Minimax-M2.1 to be worse than GLM-4.7 overall, though both have their ups and downs that don't overlap",
          "score": 10,
          "created_utc": "2026-01-31 09:15:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5p1n",
              "author": "-InformalBanana-",
              "text": "Minimax M2.1 is really amazing, significantly smaller than others while still very competitive.",
              "score": 1,
              "created_utc": "2026-01-31 16:14:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tak0o",
                  "author": "nullmove",
                  "text": "Yep I am writing Elixir and this thing flies in opencode. Surely difference exists with Sonnet but for a lot of things I can't tell at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qyshj",
          "author": "Zeeplankton",
          "text": "unfortunately claude / opus is more like\n\nhttps://preview.redd.it/bz2atqnwpmgg1.jpeg?width=1438&format=pjpg&auto=webp&s=eaef64aa0a3062b5d2d62803b7e0fc2fea022507",
          "score": 5,
          "created_utc": "2026-01-31 06:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rs0kx",
              "author": "hellomistershifty",
              "text": "I don't think about them at all... until that \"usage cost limit exceeded\" notification rolls in",
              "score": 3,
              "created_utc": "2026-01-31 11:02:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rzeah",
          "author": "-dysangel-",
          "text": "GLM Coding Plan with Claude Code is feeilng just as good or better as the latter days of Claude 4.1 for me. I've been using it daily for work and it's working very well. Organised, smart, helpful, relentless :p",
          "score": 3,
          "created_utc": "2026-01-31 12:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4xqn",
          "author": "Admirable-Choice9727",
          "text": "Meta really went from 'saving open source' to 'the team that peaked at 405B' while Deepseek and Qwen just kept shipping. Itâ€™s wild that weâ€™re at a point where Iâ€™d rather trust a hypothetical GLM-5 leak than a Meta announcement. The 'Maverick' release felt like they were just trying to stay relevant while the frontier moved past them.",
          "score": 3,
          "created_utc": "2026-01-31 12:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sihpc",
          "author": "RedParaglider",
          "text": "What I've found for dev work SERIOUS dev work I always get better results out of codex 5.2 max.  It is much slower, and forces me to use my brain a lot more through the entire thing, but the end result is always cleaner and more professional.  \n\nIt's not NEARLY as fun to use as opus though.",
          "score": 3,
          "created_utc": "2026-01-31 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ss9qa",
          "author": "SourceCodeplz",
          "text": "Gpt-5.2-Codex #1",
          "score": 3,
          "created_utc": "2026-01-31 15:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2swut8",
          "author": "Michaeli_Starky",
          "text": "Gemini in the current state has no right being in the list at all\n\nhttps://preview.redd.it/05pqcgioepgg1.png?width=1730&format=png&auto=webp&s=73cfe9a96b437482f0834fb76b254d93543e18cc",
          "score": 3,
          "created_utc": "2026-01-31 15:31:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wc5jq",
              "author": "spawncampinitiated",
              "text": "You get down voted for stating facts here",
              "score": 3,
              "created_utc": "2026-02-01 01:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ugr7z",
          "author": "timschwartz",
          "text": "Can someone explain the Ralph Wiggum thing?",
          "score": 3,
          "created_utc": "2026-01-31 19:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uhhme",
              "author": "ForsookComparison",
              "text": "Ralph Wiggum is a popular Simpsons character. He became a popular internet meme for many years because of a famous scene from the show where he's smiling in the back of a bus saying *\"I'm in danger\"*\n\nAnthropic actually published a way to use Claude-Code agentically with no concern for permissions or token usage that they labeled *\"Ralph Wiggum Mode\"*.\n\nHere, the one in danger is OP's job and the danger is Claude 4.5 Opus.",
              "score": 4,
              "created_utc": "2026-01-31 20:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r5tci",
          "author": "wanderer_4004",
          "text": "Well, my honest take, benchmarks and ClosedAI fanbois be damned, any open model blows the closed ones out of the water. Unless you want to live in a future where a handful of Elons, Samas and similar rule the world. The day that VCs stop subsidizing your $20 coding plans you'll have to turn around and bend over. Or maybe that is what you are looking forward to?",
          "score": 11,
          "created_utc": "2026-01-31 07:32:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rrwaj",
              "author": "hellomistershifty",
              "text": "you can already see /r/Bard in shambles that the Gemini API costs money (shocking)",
              "score": 7,
              "created_utc": "2026-01-31 11:01:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rdxr0",
              "author": "daniel-sousa-me",
              "text": "If you remove the R&D portion out of Anthropic's expenses, it is already wildly profitable\n\nVC money is used for R&D to make it even more profitable",
              "score": 8,
              "created_utc": "2026-01-31 08:48:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rfhon",
                  "author": "wanderer_4004",
                  "text": "For now, i.e. pre-IPO this is all guess work as the numbers are not public. That said, I think the API pricing at $25 / 1MTok is very likely to be nicely profitable. The $20 claude subscription is very likely not. At some point the VCs want to see a return. And with agentic coding you can easily burn 10MTok per hour. If you have to pay that at API prices, then for a lot of developers this will be a bad awakening into reality.\n\nThe big closed companies spend lots of money to make us believe that you absolutely need SOTA-frontier. Keep in mind the biggest threat to them and their investments of billions of $$$ is people figuring out that open models are good enough.\n\nRight now only \\~20M users out of 800M are paying for ChatGPT...",
                  "score": 2,
                  "created_utc": "2026-01-31 09:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qouee",
          "author": "yes-im-hiring-2025",
          "text": "I want to do an experiment for six months.\nTeaching via ragebaitâ„¢\n\nSame claude code setups (agent plugin + CLI), but with a twist:\n\n- some are on the claude max 200 USD plan [seniors]\n- some are on the codex pro plan [mid level]\n- some are on the GLM/minimax plan [junior]\n\nBasically the juniors get the worst AIs, and the seniors get the best. Have the juniors learn by ragebait until they match the performance of the tier above. Same with mid level engineers.\n\nMy senior level gang just ensures ralph wiggum keeps ralph wiggum-ing.\n\nWho wants to buy my agentic coding course?!?!?!",
          "score": 7,
          "created_utc": "2026-01-31 05:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp45i",
              "author": "ForsookComparison",
              "text": "I'd love this - but I'm more interested in juniors-with-SOTA vs seniors-with-minimax or something along those lines",
              "score": 6,
              "created_utc": "2026-01-31 05:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqhmv",
                  "author": "yes-im-hiring-2025",
                  "text": "Ohhh I can tell you what happens with that firsthand.\n\nI had my team running Opus via the claude code plan and the juniors just resorted to recorded meetings -> AI studio's gemini flash to get a PRD -> oneshotâ„¢ via Opus. Bear in mind these are college grads, so truly green around the ears.\n\nAlso how I found out that the meetings recorded are being sent to gemini violating who knows how many corporate conditions. If you're not familiar, the uploaded media on AI studio is first pushed to your own personal cloud and then linked from there. Not a \"temp\" upload, it uses your drive as the temp upload storage space - but never deletes anything once uploaded.\n\nZero debugging. Missed a feature and a deadline and I got on a call with them to understand how that could've possibly happened, it was debugging a simple API call response that was throwing up a random 4xx error.\n\nChecking for a response code 200 and logging everything else -> debugging with input and output loads -> sample logs for a batch input to filter and identify root cause. That's all they needed to do. Not even through experience, they could've asked an LLM how to debug a problem and follow the steps.\n\nNope. Not a single one of the three did that; coding muscles had atrophied.\n\nSince then I told management to lock up claude code access for juniors, and to only have it given to them after year 1. Till then they're on the [enterprise] basic free tier github's copilot plan only. Can't say it dramatically improved their skills but atleast they are learning how to Google and be honest, which is enough for now.\n\nI do believe most seniors SHOULD be fine with any GLM model if they can debug and write things independently, though. I myself switch between Antigravity (free opus4.5) for detailed planning then clean up/edit the plan, and get GLM4.7 to work on it. I've set up auto-review and linting, so usually major issues get caught before I even review the final code.",
                  "score": 16,
                  "created_utc": "2026-01-31 05:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpz4n",
          "author": "SilentLennie",
          "text": "I think there is an other detail: how smart the model is matters less and less, if it can complete the task is more important and at what price/how.\n\nLet's take coding:\n\nSure the top open weights models are close to early (I would put it later) 2025 smarts, but they are also much better at agentic than those models.\n\nAnd the harnesses (tools we use to make an agentic coder) are much better now than in the past, they include for example LSP even on the CLI and more and more people figured out 'plan mode' and the models can generate tests and test it and whole lot of other details...\n\nSo what does this mean ? They can actually finish a lot of tasks on their own (Kimi K2.5 being multi-modal matters a lot here too) and do it for a much lower cost or on your own computer if that's the reason you prefer. So if you compare them to early 2025 this is the wrong way to look at it. Not only that: if they can finish a task: it's far more often 'good enough', which is how for example DOS and Windows got popular (and network effect, monopoly abuse, etc. obviously), it wasn't perfect, but good enough to get stuff done.\n\nI think we are gonna be in a much more: multi-model world, where you might be using or example Kimi K2.5 a lot, but when you need to make a plan you pick Opus and when you need to digest a lot of data cheaply, you might pick Gemini 3 Flash because of it's 1M context window, etc.\n\nThe next problem is: you need to keep doing some coding to keep your coding muscle.",
          "score": 2,
          "created_utc": "2026-01-31 10:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s32za",
          "author": "ALittleBitEver",
          "text": "LLama Models are really good for tasks related to emotions, roleplaying, creative writing, etc. But yes, at programming, math, etc, I agree",
          "score": 2,
          "created_utc": "2026-01-31 12:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sh9jq",
          "author": "Far-Low-4705",
          "text": "i would bump up GPT-OSS up a level.\n\nIMHO, for it's architecture, given it's size, how sparse it is, it's speed, interleaved thinking, and more \"stable\" reasoning traces, it feels SOTA some of the time.\n\nFrom an engineering standpoint, it is one of the most sparse models, and it is very fast",
          "score": 2,
          "created_utc": "2026-01-31 14:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sy1vm",
          "author": "wagnerax",
          "text": "What about Mistral ?",
          "score": 2,
          "created_utc": "2026-01-31 15:37:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2szrk0",
              "author": "ForsookComparison",
              "text": "ðŸ˜",
              "score": 1,
              "created_utc": "2026-01-31 15:46:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t2w3d",
          "author": "GiveMeAegis",
          "text": "No Mistral?",
          "score": 2,
          "created_utc": "2026-01-31 16:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5fg0",
              "author": "ForsookComparison",
              "text": "Le sorry",
              "score": 2,
              "created_utc": "2026-01-31 16:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ta8vg",
          "author": "laterbreh",
          "text": "So what are the qualifying tasks to create this ranking?\n\nWrites my LARP smut the best? Best coding agent? Best at accomplish a specific task?",
          "score": 2,
          "created_utc": "2026-01-31 16:36:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2taus6",
              "author": "ForsookComparison",
              "text": "OP's vibes",
              "score": 1,
              "created_utc": "2026-01-31 16:39:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tw85d",
          "author": "Hoak-em",
          "text": "Disagree, at least with whatever gemini Google hosts with Antigravity. I think things get a bit murky when you start to consider that for the version of the model that most people get when it's still a \"SOTA\" model, it's nerfed in some way (i.e. issues with Opus models not performing like they did at launch, issues with Gemini models (in general)).\n\nI don't really like having a single ranking either, it's more like \"what is the model good at/what do I want to use it for.\"\n\nKimi K2.5: orchestrator/small-scale planner/designer\n\nOpus-4.5: Large-scale planner/implementer\n\nGemini 3 flash: codebase search/understanding\n\nGLM-4.7: wiggumwiggumwiggumwiggumwigguwmgiwigiwmgiwmgiw (your account was rate limited, please reduce concurrency)\n\nGPT-5.2-Codex: Fixes, refactors (alongside opus), helping me understand wtf the previous model wrote",
          "score": 2,
          "created_utc": "2026-01-31 18:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uqmnn",
          "author": "blackwell_tart",
          "text": "I wish youâ€™d use words instead of emblems. I have no idea what any of those graphics are supposed to represent. I guess I missed LLM marketing class.",
          "score": 2,
          "created_utc": "2026-01-31 20:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw647",
          "author": "ahabdev",
          "text": "Personally I was a fan fo Gemini 2.5 over Opus 3.7. I considered a much better coder.\n\nHowever, there's no comparison between Gemini 3 and Opus 4.5. Mainly because G3 is disastrous with any complex project which require long chats, constant code revisions and contentx awareness without a flaw. It's beyond horrible. So nowadays I wound't even put it in the list.\n\nAnd as a peasant with only a 5090  I never even tried to setup a big model. I don't have the need so far, but would be really great to abe able to do so eventually. That said, I consider people don't value as they should small models for many day to day cases.\n\nSuch a pity Meta failed as with most things they do these days.",
          "score": 2,
          "created_utc": "2026-01-31 06:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qt0rj",
          "author": "YouAreTheCornhole",
          "text": "I can't believe how spot on this is",
          "score": 3,
          "created_utc": "2026-01-31 05:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r59tp",
          "author": "Spitihnev",
          "text": "And mistral is the ralph wiggum?",
          "score": 2,
          "created_utc": "2026-01-31 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uio5t",
              "author": "ForsookComparison",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-31 20:08:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7812",
          "author": "NightlessBaron",
          "text": "Also check out Mbzuaiâ€™s K2 think V2 and Nvidia Nemotronâ€¦ both are dark horses imo",
          "score": 1,
          "created_utc": "2026-01-31 07:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sbqsd",
          "author": "Charuru",
          "text": "This is right on just chat performance, but you're underestimating agentic for K2.5.",
          "score": 1,
          "created_utc": "2026-01-31 13:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sdbg7",
          "author": "LocoMod",
          "text": "gpt-5-xhigh runs circles around Opus. Change my mind.",
          "score": 1,
          "created_utc": "2026-01-31 13:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2spnun",
              "author": "Both-Ad3646",
              "text": "Agreed, and Gemini is complete trash in comparison to both. I have no idea why it's ever been in the same discussion as any of the GPT's from 4.x and above.",
              "score": 2,
              "created_utc": "2026-01-31 14:54:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sjtsn",
          "author": "sine120",
          "text": "I haven't been able to play with Codex or CC much, but from what I hear of others that do, this is largely true. My company pays for Gemini CLI and I swear I might be the only person using that over the other labs. It's bad at instruction following, but takes less wrangling to get it to do what I want over models I can run. Prices would have to go up a lot to bother pushing me to pitch we run local models for anything other than ITAR code.",
          "score": 1,
          "created_utc": "2026-01-31 14:22:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sl39o",
          "author": "teachersecret",
          "text": "How close is local to SOTA? I'd say we're there. K2 2.5 is definitely SOTA level intelligence up near the highest levels we have available.\n\nObviously that's a bit difficult to run in your house, though. Most of us aren't running Kimi K2 at home.\n\nFor TRULY local... I usually think enthusiast-class (24gb vram). The only thing in that range that comes close is GLM 4.7 Flash. It feels like somebody took last-gen Claude Sonnet and gave it better tool handling. It's fantastic and not far off SOTA despite its small size.",
          "score": 1,
          "created_utc": "2026-01-31 14:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2svnz1",
          "author": "TwistStrict9811",
          "text": "Codex better than claude",
          "score": 1,
          "created_utc": "2026-01-31 15:25:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t9wit",
          "author": "RhubarbSimilar1683",
          "text": "For coding ofc\n\n\nWe all know that claude is over fitted for web coding",
          "score": 1,
          "created_utc": "2026-01-31 16:34:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tq8hs",
          "author": "Qual_",
          "text": "I found codex 5.2 way more reliable than claude on large codebase. There is always something to fix after claude, while codex just produce working code ( which sometimes feels black magic when it's after 50min of writing thousand of lines )",
          "score": 1,
          "created_utc": "2026-01-31 17:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u6iro",
          "author": "TurnUpThe4D3D3D3",
          "text": "This feels accurate, although I might put deepseek 1 tier lowers",
          "score": 1,
          "created_utc": "2026-01-31 19:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ud0ll",
          "author": "bartskol",
          "text": "Grok was never SOTA?",
          "score": 1,
          "created_utc": "2026-01-31 19:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ueakb",
              "author": "ForsookComparison",
              "text": "It's realtime data and web tools are. Probably number one for querying events from that day.\n\nThe base model itself when used over API falls a little short of Gpt5.2 and Gemini3 Pro. It feels more in line with Gemini 2.5 Pro or Sonnet 3.7 maybe, hence the rating of \"early 2025 SOTA\"",
              "score": 1,
              "created_utc": "2026-01-31 19:46:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uffgm",
                  "author": "bartskol",
                  "text": "It feels more like a toy",
                  "score": 1,
                  "created_utc": "2026-01-31 19:52:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2v4t7w",
          "author": "Odd_Candle",
          "text": "wtf is sota",
          "score": 1,
          "created_utc": "2026-01-31 21:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vo6gn",
              "author": "checksinthemail",
              "text": "State Of The Art",
              "score": 3,
              "created_utc": "2026-01-31 23:39:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2x069s",
                  "author": "Odd_Candle",
                  "text": "Thanks",
                  "score": 2,
                  "created_utc": "2026-02-01 04:28:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vbw2h",
          "author": "Imperator_Basileus",
          "text": "I donâ€™t know, maybe GPT-5.2 is fine for coding, but for anything else Iâ€™d much, much rather use DeepSeek, GLM, or Kimi. I think Gemini 3 Pro is the best one to discuss anything with or bounce ideas off of as it takes instructions well and doesnâ€™t moralise or get distracted with safetyslop.Â \n\nBut since google heavily cut AI studio rate limits and the Gemini app and subscription is terrible, I wouldnâ€™t be using that either. Hence, trying out the new Kimi 2.5 lately. But GPT-5.2 is truly horrendous. I was trying to discuss economics, my field, and harshly critiqued some economist only for it be hyper condescending about â€œletâ€™s set that asideâ€, â€œIâ€™m gonna draw a firm lineâ€, and â€œI wonâ€™t endorse harsh languageâ€. Okay, go and fuck yourself. Uninstalled the app soon after.Â ",
          "score": 1,
          "created_utc": "2026-01-31 22:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wqq67",
          "author": "MiddleCricket3179",
          "text": "Kimi 2.5 for agentic coding and tasks has proved more reliable for me than Gemini 3 Pro",
          "score": 1,
          "created_utc": "2026-02-01 03:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wt57m",
          "author": "Routine_Temporary661",
          "text": "Gemini? You gotta be kidding me bro",
          "score": 1,
          "created_utc": "2026-02-01 03:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wy6hx",
              "author": "ForsookComparison",
              "text": "3 Pro is pretty decent",
              "score": 1,
              "created_utc": "2026-02-01 04:14:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ys79s",
                  "author": "Routine_Temporary661",
                  "text": "You wont say so if you have actually tried to ship sophisticated production code with it before",
                  "score": 1,
                  "created_utc": "2026-02-01 13:22:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xezoc",
          "author": "Intelligent_Heat_527",
          "text": "In coding in cursor agent mode I found Gpt 5.2 Thinking > Claude Opus 4.5 > Google Gemini 3 Flash",
          "score": 1,
          "created_utc": "2026-02-01 06:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xxr1m",
          "author": "SpecialNothingness",
          "text": "I like your assessment of MS Copilot.",
          "score": 1,
          "created_utc": "2026-02-01 09:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yjz9s",
          "author": "LienniTa",
          "text": "deepseek too high, qwen max too low(i assume you compared open weights ones, not qwen max), and i would honestly add xiaomi somewhere around minimax or even higher(?) but overall agree - kimi cooked big time",
          "score": 1,
          "created_utc": "2026-02-01 12:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yp1w8",
          "author": "extra2AB",
          "text": "I will be honest, put Gemini up along with claude, put Kimi to SOTA.\n\nChatGPT has lost it for me.\n\nFrom occasionally solving coding and other problems for me it has gone to rarely solving any problems.\n\nand Gemini 3 Pro has about 90%+ success rate for me. and that 10% is mostly my fault for not providing it proper context about my files, system details, etc\n\nI have gone from using Chatgpt almost everytime to using it  once every 2-3 weeks and that too mostly to write a formal application, email, letter, etc",
          "score": 1,
          "created_utc": "2026-02-01 13:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yt54w",
          "author": "cleverusernametry",
          "text": "You don't seem to understand what sota means",
          "score": 1,
          "created_utc": "2026-02-01 13:28:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yvvpz",
          "author": "modadisi",
          "text": "GLM was smart af at math when I used it almost a year ago",
          "score": 1,
          "created_utc": "2026-02-01 13:45:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z0ym5",
          "author": "SVG-CARLOS",
          "text": "Why does everyone forget Manus AiðŸ˜­",
          "score": 1,
          "created_utc": "2026-02-01 14:14:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36wkbf",
              "author": "velosotiago",
              "text": "True, I wonder why people didn't mention Manus (an agentic app) on a post comparing different models, mr. \"1 day old account\".\n\nDead internet theory is, ironically, alive and well.",
              "score": 1,
              "created_utc": "2026-02-02 17:42:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zixqm",
          "author": "OcelotMadness",
          "text": "I kind of wish GLM would hire more English writing majors. The grammar and storytelling is wrong a lot of the time. GLM 4.7 is my current open source GOAT but its a little clunky when playing text adventures on it.",
          "score": 1,
          "created_utc": "2026-02-01 15:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31zugx",
          "author": "dr_manhattan_br",
          "text": "The comparison will be more and more unfair as the closed models running on enterprise grade GPUs are compared with home or workstation GPUs with 32GB or in the best case scenarios with 96GB VRAM.\nThose closed weights are bigger than you can run in your home. \nThey will be faster and smarter as those companies fine-tune those models more frequently and with data that open models doesnâ€™t have.",
          "score": 1,
          "created_utc": "2026-02-01 22:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o322kxq",
              "author": "ForsookComparison",
              "text": "There is no inherent link to model size and weight policy",
              "score": 1,
              "created_utc": "2026-02-01 23:00:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32ytpl",
          "author": "OmarBessa",
          "text": "kimi and glm are sota\n\nif you haven't had catastrophic failures with gemini you haven't used it enough\n\nGLM and Kimi both provide similar agentic performance or better with much better stability",
          "score": 1,
          "created_utc": "2026-02-02 02:00:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32zhyz",
          "author": "real-joedoe07",
          "text": "Grok is far better. Answers correctly where Gemini and ChatGPT do not.",
          "score": 1,
          "created_utc": "2026-02-02 02:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34lw68",
          "author": "Salt-Cap-4489",
          "text": "One day, I'll switch from CachyOS to Arch.",
          "score": 1,
          "created_utc": "2026-02-02 09:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3673zx",
          "author": "Unable-Jelly6228",
          "text": "I need the top tier list of models under 8b :v",
          "score": 1,
          "created_utc": "2026-02-02 15:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3by4u5",
          "author": "FederalLook5060",
          "text": "In windsuf kimi k2.5thinking ? gemini 3 pro in AG. Also deepseek is not as good.",
          "score": 1,
          "created_utc": "2026-02-03 12:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3deaba",
          "author": "Hot_Slice",
          "text": "My take is, it's not in these company's best interest to reign in RAM usage - because if they did, then we could run the SOTA models at home, and they'd have no moat.",
          "score": 1,
          "created_utc": "2026-02-03 17:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s68p2",
          "author": "korino11",
          "text": "OMG Ralh?!?? that a STUPID plugin! For what??! Only if you have a SIMPLE project! And thats it. If you need a somthing BIG with many layers of decompositions, with thousands calls of subagent. Useles shit your ralh...\n\nRight now kimi K2.5  - best model. Even gpt 5.2 xhigh useles. k2.5 can solve all. Even with high physycs and math..",
          "score": 1,
          "created_utc": "2026-01-31 12:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qmobq",
          "author": "SeeonX",
          "text": "What is the top AI called?",
          "score": 1,
          "created_utc": "2026-01-31 04:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qmvxs",
              "author": "Ok-Lobster-919",
              "text": "Claude, Opus is insanely good for coding. Never try it, you won't want to use anything else. It one-shots problems sonnet struggles with.",
              "score": 21,
              "created_utc": "2026-01-31 04:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qn6jc",
                  "author": "ForsookComparison",
                  "text": "> Never try it, you won't want to use anything else\n\n  \nMy wallet agrees. There are some tasks I have where *\"it's either Opus, or it's all afternoon\"* but that $25/1m racks up quick.",
                  "score": 13,
                  "created_utc": "2026-01-31 04:58:40",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2qnrtu",
                  "author": "jadhavsaurabh",
                  "text": "Bro same bro same, after trying opus I don't even use other AI now on 100$ plan",
                  "score": 7,
                  "created_utc": "2026-01-31 05:02:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qo99e",
                  "author": "HarambeTenSei",
                  "text": "gpt5.2 through codex is better in most instances",
                  "score": 1,
                  "created_utc": "2026-01-31 05:06:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qmzq0",
              "author": "ForsookComparison",
              "text": "Claude 4.5 Opus is the one in *\"My Job is Ralph Wiggum\"* tier.\n\n  \nClaude 4.5 Sonnet would rank bottom of \"SOTA\" if I took the time to rank split out models.  \n  \nClaude 4.5 Haiku would probably be lower than Grok but above Kimi and Deepsek.",
              "score": -1,
              "created_utc": "2026-01-31 04:57:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qoi6x",
                  "author": "simracerman",
                  "text": "So, do you think GLM-4.7 Flash beats 4.5 Sonnet or GLM-4.7 even?",
                  "score": 3,
                  "created_utc": "2026-01-31 05:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qp09q",
          "author": "yes-im-hiring-2025",
          "text": "I am going all in on GLM stocks btw.\nI think their GLM5 is gonna drop soon and reclaim the top spot (trust)",
          "score": 1,
          "created_utc": "2026-01-31 05:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp62n",
              "author": "ForsookComparison",
              "text": "Would be cool especially if they keep their commitments to open-weight.",
              "score": 5,
              "created_utc": "2026-01-31 05:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqyjw",
                  "author": "yes-im-hiring-2025",
                  "text": "Seems more and more unlikely since they IPOd.\n\nI'd love for them to be open, they're realistically the best performance:size option if I need to have it up and running locally. Minimax and Kimi are great but the dormant param weights are a memory hog that I'd rather avoid.",
                  "score": 1,
                  "created_utc": "2026-01-31 05:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2t67ac",
              "author": "-InformalBanana-",
              "text": "I root for Minimax - smaller models, better for local and competitive performance.",
              "score": 1,
              "created_utc": "2026-01-31 16:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ruvvl",
          "author": "stylehz",
          "text": "Besides the Claude (because it is the best), never let bro cook again.",
          "score": 1,
          "created_utc": "2026-01-31 11:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u98q1",
              "author": "nomorebuttsplz",
              "text": "Agreed, this tier list is very poor and lines up with vibes from 2 months ago rather than any actual tasks... and without specifying domain or task rankings are pointless...  overly broad like artificial analysis except without any evidence.",
              "score": 3,
              "created_utc": "2026-01-31 19:22:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sl62j",
              "author": "ForsookComparison",
              "text": "I'm in the kitchen and your head at the same time",
              "score": -1,
              "created_utc": "2026-01-31 14:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2stovx",
          "author": "nomorebuttsplz",
          "text": "Nah.Â \n\nKimi 2.5 is at SOTA levels and well above early 2025.\n\nGlm 4.7 is also above early 2025 in coding. Virtually everyone who has compared sonnet 3.7 and glm 4.7 in coding agrees glm is better.Â \n\nBut of course, this is task specific. What tasks are you measuring yours by?",
          "score": 1,
          "created_utc": "2026-01-31 15:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qpe34",
          "author": "neotorama",
          "text": "Gemini sits with Z. Itâ€™s crap with infinite loop. Kimi 2.5 beside OpenAI GPT",
          "score": -1,
          "created_utc": "2026-01-31 05:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r45o0",
              "author": "fishylord01",
              "text": "Not sure why downvoted, Gemini sucks . Agree kimi 2.5 is very close behind codex 5.2 high.",
              "score": 0,
              "created_utc": "2026-01-31 07:16:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rmqj2",
          "author": "Downtown_Fly_5919",
          "text": "And then you have nemotron 3 nano",
          "score": 0,
          "created_utc": "2026-01-31 10:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rqan2",
          "author": "spawncampinitiated",
          "text": "Gemini Sota xd",
          "score": 0,
          "created_utc": "2026-01-31 10:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6r0h",
          "author": "Juanisweird",
          "text": "Hey, recently returning to this whole AI thing. Whatâ€™s SOTA?\n\nAnd if you had to choose only one $20 subscription for a general use AI ( coding, research, chatting, troubleshooting real life and technical issues, design inspiration and business planning and marketing) which one would you choose for the next 2 months?",
          "score": 0,
          "created_utc": "2026-01-31 07:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rqylz",
              "author": "SilentLennie",
              "text": "It depends, what do you need it for (or more likely: how much do you need it) ? Do you include the job, do you include private use ? What kind of hobbies and job do you have, etc. ?",
              "score": 1,
              "created_utc": "2026-01-31 10:52:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rvxl3",
                  "author": "Juanisweird",
                  "text": "I want to create apps to automate or simplify my life. Iâ€™ve done some through python scripts and vibe coding like second hand listing generator or roulette simulator or points and job dashboard ( my 9-5 has a points system so after X amount of jobs, I get extra pay).\n\nBut I want to build a side income and want an AI that can help me do research, prepare projects and even help create delivery and onboarding automations with little AI and more old school programming but since I have little experience, need an AI to guide me or aid",
                  "score": 1,
                  "created_utc": "2026-01-31 11:37:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rbeh8",
              "author": "YayaBruno",
              "text": "I've been using Nano-GPT for a while  and it's been decent, it gives access to a bunch of open source models for an $8 a month subscriptio. ( Referral link if you're interested: https://nano-gpt.com/r/CfxGHjHp )",
              "score": 0,
              "created_utc": "2026-01-31 08:24:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t7893",
          "author": "KitchenSomew",
          "text": "Solid tier list! The placement of open-weight models in \"early 2025 SOTA\" territory is spot-on for most use cases. A few observations:\n\n\n\n1. \\*\\*Context matters more than raw intelligence\\*\\*: As several commenters noted, Kimi K2.5's multimodal capabilities + agentic workflows make it punch way above its weight compared to models with similar benchmark scores.\n\n\n\n2. \\*\\*The \"instruction following vs problem understanding\" debate is key\\*\\*: Claude's strength isn't just following instructionsâ€”it's inferring intent and missing context, which is why it excels at complex refactoring tasks even when specifications are vague.\n\n\n\n3. \\*\\*Open-weight gap is narrowing in specific domains\\*\\*: For coding with proper tooling (LSP, test generation, iteration loops), GLM-4.7 + good harness can match sonnet 3.7 on many practical tasks. The real gap shows in long-context coherence and multi-turn debugging.\n\n\n\n4. \\*\\*Size/performance tradeoff is underrated\\*\\*: Qwen3-235B is the sweet spot for self-hostedâ€”enough intelligence for real work without needing a data center. The jump to K2.5 territory requires massive compute that most can't justify.\n\n\n\nThe fact that we're even having \"SOTA vs early 2025\" debates about open weights is wild progress.",
          "score": -4,
          "created_utc": "2026-01-31 16:22:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoty38",
      "title": "Kimi K2.5 costs almost 10% of what Opus costs at a similar performance",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xz7okply3zfg1.png",
      "author": "Odd_Tumbleweed574",
      "created_utc": "2026-01-27 23:10:16",
      "score": 602,
      "num_comments": 116,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o269wrx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 07:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o257yrq",
          "author": "one-wandering-mind",
          "text": "It used 3x the tokens that opus does for the same tasks so cheaper, but more like 3x cheaper than 10x cheaper.Â \n\n\nThese models often use a dramatically different number of tokens to do the same thing. It should be considered for both cost and latency when you compare them.Â \n\n\nI've heard great things about the kimi models especially the last version for writing.Â \n\n\nÂ https://artificialanalysis.ai/#cost-to-run-artificial-analysis-intelligence-index",
          "score": 155,
          "created_utc": "2026-01-28 02:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26orzg",
              "author": "ozzie123",
              "text": "â€œSimilar performanceâ€ here also carries lots of asterisks",
              "score": 51,
              "created_utc": "2026-01-28 09:23:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dhcye",
                  "author": "Cergorach",
                  "text": "Similar performance does not mean equal quality output for every task.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:49:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2472ya",
          "author": "TAW56234",
          "text": "If I had a nickel for every time someone claimed the newest OSS Sota model was similar to Claude, I could generate a few prompts.",
          "score": 362,
          "created_utc": "2026-01-27 23:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gq5f",
              "author": "ForsookComparison",
              "text": "I use closed weight models at work (required) and open weight models for side projects (cost). Several hours of agent use per day.\n\nI think open weight is approaching **Sonnet 3.7** if I'm totally honest. I love this community to death but it gets drunk off of bar charts and one-shots. Sonnet 3.7 for Kimi and Deepseek prices is amazing, but *\"approaching Opus 4.5\"* just flags for me that nobody is using these things for hours at a time.",
              "score": 143,
              "created_utc": "2026-01-28 03:46:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o260qbh",
                  "author": "peculiarMouse",
                  "text": "I used Max extensively for multiple months and I tend to disagree. Claude is best at writing code, but architecture, hallucinations and creativity make it highly debatable in all other things that matter about code. \n\nGemini Pro for instance has INSANE, borderline dementia levels hallucinations, where it necessitates undoing generation and redoing with something else, but its context window and knowledge is unparalleled.\n\nClaude is very fixed on things it thinks it knows how to do. For example old libraries that have long deprecated documentation. GLM with same Claude .md has much lower rate for ignoring it altogether. It goes as far as making your code worse through unrequested changes.\n\nI would easily put Claude first as agentic powerhouse that comes with your subscription.  \nBut I tend to use other models a lot, when working with Claude, so I'd put them on Sonnet 4.6 level easily.\n\nClaude just has very good understanding of what's most important to 95% devs.",
                  "score": 41,
                  "created_utc": "2026-01-28 05:57:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25q46f",
                  "author": "KaroYadgar",
                  "text": "What do you use them for? LLMs are very spike-y so I'd like to know at what specific tasks you think they're comparable to Sonnet 3.7. Are there any tasks you think they are comparable or better at?",
                  "score": 13,
                  "created_utc": "2026-01-28 04:43:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25qhvs",
                  "author": "Virtamancer",
                  "text": "Have you used any of the current-gen top open models in OpenCode? Like, the absolute newest version of deepseek/glm/kimi/qwen/whatever, like within the last 7 days or whenever they released.\n\nIâ€™m curious how they compare running through OpenCode vs Claude Code with Opus 4.5 or Codex with GPT-5.2 set to Extra High reasoning.",
                  "score": 7,
                  "created_utc": "2026-01-28 04:46:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26h5zi",
                  "author": "FyreKZ",
                  "text": "I think you're misremembering how dumb 3.7 was lol. K2.5 is realistically probably not far off Sonnet 4.5, Opus is a stretch.",
                  "score": 4,
                  "created_utc": "2026-01-28 08:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o252exg",
              "author": "Zc5Gwu",
              "text": "Generate prompts with kimi or claude though, thatâ€™s the question.",
              "score": 26,
              "created_utc": "2026-01-28 02:28:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2avsuf",
              "author": "eikenberry",
              "text": "No reason it couldnâ€™t be true every time. This is active research with things constantly changing.",
              "score": 1,
              "created_utc": "2026-01-28 22:21:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24694d",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 70,
          "created_utc": "2026-01-27 23:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qbtx",
              "author": "DistanceSolar1449",
              "text": "Almost certainly not. [Estimates for Claude Opus 4.5 put it at 1.6T-3T params, 160b active](https://news.ycombinator.com/item?id=46038512). \n\nThat would make Kimi roughly 1/2 to 1/4 the size.",
              "score": 7,
              "created_utc": "2026-01-28 04:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24ewv8",
              "author": "neotorama",
              "text": "I would say almost similar. Opus has been degrading last 3 weeks",
              "score": -10,
              "created_utc": "2026-01-28 00:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24rlr0",
                  "author": "eli_pizza",
                  "text": "Opus hasnâ€™t changed.",
                  "score": 16,
                  "created_utc": "2026-01-28 01:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o24fo78",
              "author": "Healthy-Nebula-3603",
              "text": "Yes \n\nIs very similar",
              "score": -9,
              "created_utc": "2026-01-28 00:29:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24q0lj",
                  "author": "Bob_Fancy",
                  "text": "Iâ€™d bet a good amount that itâ€™s not.",
                  "score": 12,
                  "created_utc": "2026-01-28 01:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25snrk",
          "author": "dubesor86",
          "text": "this assumes a ton of input and will swing widely depending on use case. for me, the bulk of the cost is always the model output.\n\nin my general benchmark the cost was:\n\nKimi-K2.5 (reasoning) $1.60\n\nClaude Opus 4.5 $2.75\n\n= 42% cheaper\n\n\nin my chess benchmark the game cost was:\n\nKimi-K2.5 (reasoning) $0.87\n\nClaude Opus 4.5 $0.46\n\n= 89% more expensive\n\nAlso, obviously the performance is not \"similar\" level if you actually used these models, despite what some bars tell you.",
          "score": 26,
          "created_utc": "2026-01-28 04:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24e4e0",
          "author": "ghulamalchik",
          "text": "I'm just gonna wait for DeepSeek 4 and MiniMax M2.2\n\nI trust those from experience.\n\nI used many models in cline and DS and MiniMax were my favorite.",
          "score": 35,
          "created_utc": "2026-01-28 00:21:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gwjm",
              "author": "epyctime",
              "text": "you prefer them to glm4.7?",
              "score": 6,
              "created_utc": "2026-01-28 03:47:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25qbup",
                  "author": "ghulamalchik",
                  "text": "I haven't used GLM 4.7 a lot to form a comprehensive opinion, however when I tried it it was *slightly* worse than MiniMax M2.1 for my use cases (programming desktop GUIs). MiniMax was faster and slightly better. DeepSeek was nice when it came to debugging big issues, it's about the same in terms of general intelligence, it's much slower than MiniMax though.\n\nTL;DR\n- DS was smart but slow, good for debugging and planning big changes. It's also very cheap. The main downside is that it's slower.\n- MiniMax is good a default.\n- GLM 4.7 is in the same ballpark in terms of performance but MiniMax felt a little better for me personally.\n\nI think it all depends on what you use the models for. There's no answer that fits all.",
                  "score": 8,
                  "created_utc": "2026-01-28 04:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25c7v4",
              "author": "Zianiwarhead",
              "text": "I strongly agree.",
              "score": 2,
              "created_utc": "2026-01-28 03:21:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25wjgj",
              "author": "deadcoder0904",
              "text": "What are u using DS & MiniMax for specifically? Just coding? Frontend or backend?",
              "score": 1,
              "created_utc": "2026-01-28 05:26:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25xqoj",
                  "author": "ghulamalchik",
                  "text": "Yeah just coding. I'm mainly doing desktop GUIs in Qt (full programs not just the GUI part).",
                  "score": 2,
                  "created_utc": "2026-01-28 05:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26vtkt",
              "author": "Loskas2025",
              "text": "I use GLM 4.7 with kilocode or cline for planning and code setup. But then I switch to minimax 2.1 (all local) for writing and back to GLM 4.7 for orchestration. Used in production.",
              "score": 1,
              "created_utc": "2026-01-28 10:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2712vh",
                  "author": "evia89",
                  "text": "Dont u need smart model for design+plan? I notice that when I do that with opus it gets me at 95%. When I use glm for it I need to do ALOT of hand holding and plan (atomic step by step tasks) wont be as good\n\nHowever if I do plan (superpower skill, not build in) with opus then GLM can code it just fine",
                  "score": 1,
                  "created_utc": "2026-01-28 11:12:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25coe3",
              "author": "Peetlin",
              "text": "same here it's not getting enough love. it's fast and intelligence and agentic capability. it has brain oof 80% of gpt5.2 but that's enough for me",
              "score": 2,
              "created_utc": "2026-01-28 03:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24byuf",
          "author": "NighthawkT42",
          "text": "It's good, but not really the same level.",
          "score": 38,
          "created_utc": "2026-01-28 00:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e4mh",
              "author": "ihexx",
              "text": "yeah, it's closer to sonnet or gemini 3 flash",
              "score": 1,
              "created_utc": "2026-01-28 07:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24vphu",
          "author": "Recoil42",
          "text": "Does it use the same number of tokens? I doubt it.",
          "score": 7,
          "created_utc": "2026-01-28 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o250252",
              "author": "electronicsoul",
              "text": "Exactly, and what's being cached in those token counts for a much cheaper rate and therefore faster inference is what people seem to forget. That's a big part of what makes Claude feel a level above the rest.",
              "score": 4,
              "created_utc": "2026-01-28 02:16:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2458eb",
          "author": "TransportationSea579",
          "text": "Is the similar performance in the room with us?",
          "score": 80,
          "created_utc": "2026-01-27 23:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24lm0k",
              "author": "Howdareme9",
              "text": "It never is lol.",
              "score": 23,
              "created_utc": "2026-01-28 00:59:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2486zq",
              "author": "illforgetsoonenough",
              "text": "K2.5 is sitting on the chair in the corner of the room",
              "score": 7,
              "created_utc": "2026-01-27 23:51:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24fur6",
              "author": "Healthy-Nebula-3603",
              "text": "From what I tested and what I saw on YouTube.... Very similar to opus 4.5 in coding.",
              "score": -12,
              "created_utc": "2026-01-28 00:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25d942",
          "author": "galambalazs",
          "text": "It's more fair to compare to sonnet 4.5",
          "score": 8,
          "created_utc": "2026-01-28 03:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ulg4y",
              "author": "RakibOO",
              "text": "no. more fair to compare with sonnet 3.5. claude 4.5 is ai from 2030",
              "score": 1,
              "created_utc": "2026-01-31 20:22:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25tibf",
          "author": "Torodaddy",
          "text": "*absolutely not the same performance*",
          "score": 13,
          "created_utc": "2026-01-28 05:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e67e",
              "author": "rm-rf-rm",
              "text": "based on prejudice or something more concrete? TBH I am in agreement with you, but Im asking what your basis is for clarity",
              "score": 3,
              "created_utc": "2026-01-28 07:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a7x97",
                  "author": "Torodaddy",
                  "text": "Anecdotally, ive used both extensively and I think its not even close when it comes to something requiring some sequential logic to get the answer. Opus head and shoulders better",
                  "score": 3,
                  "created_utc": "2026-01-28 20:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o246yjt",
          "author": "LoveMind_AI",
          "text": "Kimi in the thinking era is VERY hit or miss. The hits are amazing. The misses are sad, because when itâ€™s on, it really gives the SOTA a run for its money. I still think MiniMax M2 is the best LLM outside of the West.",
          "score": 21,
          "created_utc": "2026-01-27 23:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264z3l",
          "author": "TheInfiniteUniverse_",
          "text": "obviously this would be fantastic if true. this is quite obviously not true, still way behind Opus and GPT but getting there....",
          "score": 6,
          "created_utc": "2026-01-28 06:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2668r4",
          "author": "Michaeli_Starky",
          "text": "At similar performance?  Not even close.",
          "score": 5,
          "created_utc": "2026-01-28 06:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25x2iy",
          "author": "Middle_Bullfrog_6173",
          "text": "If you just go by benchmarks, Genini 3 Flash also has similar performance for 10% cost. In reality for some use cases there's much more difference at the top than a few points suggest. And some tasks are not really captured by benchmarks.\n\n\nLimited testing so far, but K2.5 has the large model feature of great niche performance, like low resource languages. But it seems to lose coherence earlier than the big closed models as context fills up. Can't really say anything more specific yet.",
          "score": 4,
          "created_utc": "2026-01-28 05:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260dkg",
          "author": "Dazzling_Focus_6993",
          "text": "as someone who using heavily using these tools for real world tasks, I would say opus 4.5 is another level, despite benchmarks implying gpt 5.2 and gemini 3 are very close. I say the gap is huge. I cannot use any other model for relatively complex tasks. \n\nI use opensource models (e.g., gpt-oss, kimi and qwen) for relatively simple tasks mainly due to the cost. I look forward to trying kimi 2.5 but, to be frank, i do not have high hopes. I think i will continue to use my combination of opus 4.5 and cheap OS models. \n\nPS: I select models through Kilo Code\n\nAdditionally: For gemini 3, it is the opposite. I do not have any use cases for gemini 3 despite benchmarks look very high.",
          "score": 3,
          "created_utc": "2026-01-28 05:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26iuob",
              "author": "Front_Eagle739",
              "text": "weird. I use codex 5.2high and claude code side by side and both are better at different things. If I had to pick a smarter one i'd nudge to 5.2 high. Its just more likely to one shot fix a problem and less likely to get lost in circles. Claude and opus is waaay faster though so still gets used first.",
              "score": 2,
              "created_utc": "2026-01-28 08:28:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3acs",
                  "author": "Impossible_Hour5036",
                  "text": "Probably depends on prompting technique.  Can't say I've gotten Opus lost unless I was badgering it while it worked or I didn't have a clear goal.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:16:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o240qo4",
          "author": "ChainOfThot",
          "text": "How can their API costs be so cheap with what I assume is still older hardware?",
          "score": 5,
          "created_utc": "2026-01-27 23:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24us8r",
              "author": "BlueSwordM",
              "text": "Native INT4, great context handling, small MOE and not amazing speeds.",
              "score": 6,
              "created_utc": "2026-01-28 01:48:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o249wya",
              "author": "smith7018",
              "text": "Government subsidies",
              "score": 10,
              "created_utc": "2026-01-28 00:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24kjkr",
                  "author": "HateAccountMaking",
                  "text": "We(USA) also have Government subsidies. China is a peoples government thats the difference.",
                  "score": -9,
                  "created_utc": "2026-01-28 00:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o250j8u",
          "author": "electronicsoul",
          "text": "Everyone loves cheap tokens but that's a very reductionist calculation.",
          "score": 3,
          "created_utc": "2026-01-28 02:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24qgyx",
          "author": "lochyw",
          "text": "Who chose these colors? It's literally the opposite of what it should be.",
          "score": 2,
          "created_utc": "2026-01-28 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24tw1h",
              "author": "derivative49",
              "text": "you're talking about it ;)",
              "score": -3,
              "created_utc": "2026-01-28 01:43:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o261l6f",
          "author": "Cool-Selection-9275",
          "text": "Just how do they even do that? That's nuts",
          "score": 2,
          "created_utc": "2026-01-28 06:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260jz8",
          "author": "ChimSau19",
          "text": "I have problem with GLM that claude code just dont ask me if it could modify? Could it fix simply by Shift Tab, or u guy have that problem too?",
          "score": 1,
          "created_utc": "2026-01-28 05:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o265zbl",
          "author": "Crafty-Struggle7810",
          "text": "At what quant?",
          "score": 1,
          "created_utc": "2026-01-28 06:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dit1",
          "author": "korino11",
          "text": "well i making a project and codex 5.2high doesnt solved. But kimi did LOL\n\nGPT always aking about whole parameters what they should be, how need to be done wjole parts. When he have all formulas amd whole project in high math is done. But Kimi, i jut give all formulass and...hold my bear!\n\nJust 1 thing i do not like. in Coding Plans you have a limits on API -Calls. So doesnt matter your call 20k tokens or just 500 tokens...  But..i got it for 2.59  So it perfect for that price",
          "score": 1,
          "created_utc": "2026-01-28 07:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26j1gj",
          "author": "No_Afternoon_4260",
          "text": "> the first time that I feel a open source model is truly competitive..\n\nYeah I know! I feel the same every 6 months, last time was deepseek's release, then k2's.. then glm was the faster brother..\n\nCrazy times I know ðŸ˜…\n\n(As every 6 months, maybe not opus level but not far)",
          "score": 1,
          "created_utc": "2026-01-28 08:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qaqj",
          "author": "rduito",
          "text": "Wait, you said a bit better than glm. So not comparable to opus after all?\n\n\nAm a fan of glm but opus is something else",
          "score": 1,
          "created_utc": "2026-01-28 09:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ywi8",
          "author": "alexrada",
          "text": "what tests have you run?",
          "score": 1,
          "created_utc": "2026-01-28 10:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cwbv",
          "author": "Beginning_Company_85",
          "text": "I tried every models available and ended up with Claude max. Totally insane the amount of work it got me done in just 2 weeks.",
          "score": 1,
          "created_utc": "2026-01-28 15:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28z4im",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. one-wandering-mind is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 1,
          "created_utc": "2026-01-28 17:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bc6jd",
          "author": "elllyphant",
          "text": "& you can use this Synthetic promo (40% off a sub) to use kimi k2.5 w/ high rate limits [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
          "score": 1,
          "created_utc": "2026-01-28 23:43:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsl6g",
          "author": "Muted_Standard175",
          "text": "Does anyone tried using opencode with codex for planning and k2.5 for building? Is it good?",
          "score": 1,
          "created_utc": "2026-01-29 01:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbrnq",
          "author": "Spiritual_Cycle_9141",
          "text": "https://preview.redd.it/4a8nhb7kd7gg1.png?width=856&format=png&auto=webp&s=efb13080f2958c34b582af7238533243fc89edeb\n\n  \nWorked for few seconds and errored , don't waste ur time (( no real free things now )) , also they will trian on ur propmpts",
          "score": 1,
          "created_utc": "2026-01-29 02:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2da7bq",
          "author": "deparko",
          "text": "I've been using Kimi too. I've been developing a health agent, and it is very responsive and very good, but it sometimes comes off as very authoritative and occasionally hallucinates.\n\nI plan to build an agent swarm to validate, but overall I think it's one of the first open models that I don't want to stop using. A lot of the open models I'll work with, but I usually end up on a frontier model eventually. I don't feel that way with kimi.",
          "score": 1,
          "created_utc": "2026-01-29 06:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ea2su",
          "author": "DarqOnReddit",
          "text": "A bit better? Aka still shit. Sorry but neither GLM-4.7 not Minimax are good models. They may be narrowly scoped tiny requests right, but require a lot of iteration and their reasoning or should I say problem solving skills are utter trash",
          "score": 1,
          "created_utc": "2026-01-29 12:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egpkv",
          "author": "Late_Special_6705",
          "text": "How to use  kimi? Kimi coder?",
          "score": 1,
          "created_utc": "2026-01-29 12:46:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z418g",
          "author": "SVG-CARLOS",
          "text": "And kimi costs nothing if you use it directly in the API",
          "score": 1,
          "created_utc": "2026-02-01 14:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32vnn8",
          "author": "Myfinalform87",
          "text": "I tried looking but is there a breakdown of the subscription costs? I am currently subbed to CC $20. While it doesnt state a hard limit there are obvious weekly limits. Does anyone know how that compares to OpenCode's subscription when using Kimi K?   \nBoth have usage limits but both dont state what those limits are. Both are vague in their token session/weekly limits",
          "score": 1,
          "created_utc": "2026-02-02 01:43:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27e6xl",
          "author": "tcoder7",
          "text": "No model matches even half of Opus 4.5 full capacities.  This is hype. OSS models are good for execution. This latest model is more a competitor to Sonnet 4.5.",
          "score": 0,
          "created_utc": "2026-01-28 12:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o243jly",
          "author": "PhotographerUSA",
          "text": "Kimi has terrible programming skills. lol",
          "score": -11,
          "created_utc": "2026-01-27 23:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bh4i",
          "author": "Ashley_Sophia",
          "text": "Yeah but I just had a look. Instant ads via UI. No thx.",
          "score": -8,
          "created_utc": "2026-01-28 00:08:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqj51h",
      "title": "LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/fjyoor8kecgg1",
      "author": "Electrical-Shape-266",
      "created_utc": "2026-01-29 19:54:56",
      "score": 591,
      "num_comments": 66,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2h5gxo",
          "author": "ItilityMSP",
          "text": "It be nice if you gave an indication of what kind of hardware is needed to run the model. Thanks.",
          "score": 91,
          "created_utc": "2026-01-29 20:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hayqj",
              "author": "_stack_underflow_",
              "text": "If you have to ask, you can't run it.\n\nFrom the command it needs 8 GPUs on a single machine. It's FSDP and a 14B model (the 14B isn't indicative of what is needed)\n\nI suspect:  \nâ€¢ Dual EPYC/Xeon or Threadripper Pro  \nâ€¢ 256GB to 1TB system RAM  \nâ€¢ NVMe scratch (fast disk)  \nâ€¢ NVLink or very fast PCIe  \nâ€¢ 8x A100 80GB",
              "score": 105,
              "created_utc": "2026-01-29 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j2358",
                  "author": "Upper-Reflection7997",
                  "text": "Brah nobody is running this model locally. God damn  8 a100s. Perhaps in future there will be a sweet ultra compressed fp4 model to fit in 5090+64gb ram system build.",
                  "score": 38,
                  "created_utc": "2026-01-30 02:23:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2hwfn3",
                  "author": "oxygen_addiction",
                  "text": "14-22$/h on Runpod. Not that bad. It should run at around 14-16fps, so input latencty will be quite rough.",
                  "score": 16,
                  "created_utc": "2026-01-29 22:37:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mp7go",
                  "author": "IntrepidTieKnot",
                  "text": "Like a year ago I would have thought: 1TB RAM - that's a lot. But well, it's doable if I really want it. Reading it today is like: whaaaat? 1.21 Jiggawatt?\n1 TB is a nice little 10k nowadays. Ridiculous.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:41:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ngjje",
                  "author": "Zestyclose839",
                  "text": "Hear me out: quantize down to IQ1\\_XXS, render at 144p, interpolate every other frame. It would be like playing a DALL-E era nightmare but all the more fun.",
                  "score": 1,
                  "created_utc": "2026-01-30 18:42:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ruoa7",
                  "author": "ApprehensiveDelay238",
                  "text": "Why a TB of RAM when you run the model on the GPU?",
                  "score": 1,
                  "created_utc": "2026-01-31 11:26:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ki2rb",
                  "author": "Lissanro",
                  "text": "I have EPYC with 1 TB RAM, and fast 8 TB NVMe, but unfortunately just four 3090 cards on x16 PCI-E 4.0 slots. Even though I could four more for eight in total, if it really needs 80 GB VRAM on each card, I guess I am out of luck.",
                  "score": 0,
                  "created_utc": "2026-01-30 08:24:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2imjdf",
              "author": "derivative49",
              "text": "also the usecase?",
              "score": 3,
              "created_utc": "2026-01-30 00:56:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jz1pi",
                  "author": "IrisColt",
                  "text": "heh",
                  "score": 2,
                  "created_utc": "2026-01-30 05:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z181k",
              "author": "SVG-CARLOS",
              "text": "100GB not that good for some consumer hardware lmao",
              "score": 1,
              "created_utc": "2026-02-01 14:16:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hybp3",
          "author": "LocoMod",
          "text": "Where is the Genie 3 comparison? Or did you fail to include it because you don't really have access to it and can't actually compare?\n\n\"LingBot-World outperforms Genie 3 because trust me bro\"",
          "score": 62,
          "created_utc": "2026-01-29 22:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ifzzs",
              "author": "adeadbeathorse",
              "text": "To be honest it looks pretty much AT or NEAR Genie 3â€™s level, at least. Watched a youtube vid exploring Genie 3 and trying various prompts.",
              "score": 4,
              "created_utc": "2026-01-30 00:21:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jl7oj",
                  "author": "LocoMod",
                  "text": "If beauty is the n the eye of the beholder then you need to get those eyes checked. There is no timeline where a model you host locally (if youâ€™re fortunate enough to afford thousands of $$$) that beats Google frontier models running in state of the art data centers.\n\nI am an enthusiast and wish for it to be so. I donâ€™t want to be vendor locked either. But reality is a hard pill to swallow.\n\nYou can settle for â€œgood enoughâ€ if thatâ€™s your jam. But that will not pay the bills in the future economy.\n\nIf you are not using the best frontier models in any particular domain then you are not producing anything of value.\n\nYes, itâ€™s an extremely inconvenient truth.\n\nBut â€¦",
                  "score": -4,
                  "created_utc": "2026-01-30 04:13:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2im1id",
              "author": "TheRealMasonMac",
              "text": "To be honest, Genie might as well not exist since you can't access it unless you're a researcher.",
              "score": 4,
              "created_utc": "2026-01-30 00:54:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jfg6f",
                  "author": "Ok-Morning872",
                  "text": "it just released for gemini ai ultra subscribers",
                  "score": 11,
                  "created_utc": "2026-01-30 03:38:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jk4bi",
                  "author": "LocoMod",
                  "text": "Most people donâ€™t have the hardware to run LingBot either. And Iâ€™m not talking about the 1% of enthusiasts in here with the skills and money to invest in the hobby.\n\nIt might as well not exist either.",
                  "score": -7,
                  "created_utc": "2026-01-30 04:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2icnsv",
              "author": "_raydeStar",
              "text": "I agree - and also this kind of thing is really frontier, and doesn't have benchmarks yet that I know of.",
              "score": -1,
              "created_utc": "2026-01-30 00:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2l117c",
              "author": "Mikasa0xdev",
              "text": "Open source LLMs are the real frontier.",
              "score": 0,
              "created_utc": "2026-01-30 11:14:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lgsoe",
                  "author": "LocoMod",
                  "text": "And fermented cabbage is better than ground beef right?",
                  "score": 1,
                  "created_utc": "2026-01-30 13:05:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i560g",
          "author": "Ylsid",
          "text": "Cool post but no AGI is not very near",
          "score": 28,
          "created_utc": "2026-01-29 23:23:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i826g",
              "author": "Xablauzero",
              "text": "Yeah, we're really really really far away from AGI, but I'm extremely glad to at least see that we're reaching that 1% or even 2% from what was 0% for years and years beyond. If humanity even hit the 10% mark, growth gonna be exponential.",
              "score": -3,
              "created_utc": "2026-01-29 23:38:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2h8mrh",
          "author": "Sl33py_4est",
          "text": "so you ran it and are reporting this empirically? or are you just sharing the projec that has already been shared",
          "score": 13,
          "created_utc": "2026-01-29 20:43:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hzxft",
          "author": "SmartCustard9944",
          "text": "Put a small version of it into a global illumination stack, and then we are talking.",
          "score": 3,
          "created_utc": "2026-01-29 22:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l7okz",
          "author": "jacek2023",
          "text": "This is another post not about a local model, which people mindlessly upvote to the top of LocalLLaMA â€œbecause itâ€™s open, so you know, Iâ€™m helping, Iâ€™m supporting, you know.â€",
          "score": 3,
          "created_utc": "2026-01-30 12:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jhz2g",
          "author": "kvothe5688",
          "text": "where is the example of persistent memory?",
          "score": 2,
          "created_utc": "2026-01-30 03:53:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jnt2f",
              "author": "adeadbeathorse",
              "text": "[here you go](https://arxiv.org/pdf/2601.20540) \n\n> A key property of LingBot-World is its emergent ability to maintain global consistency without relying on explicit 3D representations such as Gaussian Splatting. [...] the model preserves the structural integrity of landmarks, including statues and Stonehenge, even after they have been out of view for long durations of up to 60 seconds. Crucially, unlike explicit 3D methods that are typically constrained to static scene reconstruction, our video-based approach is far more dynamic. It naturally models complex non-rigid dynamics, such as flowing water or moving pedestrians, which are notoriously difficult for traditional static 3D representations to capture.  \nBeyond merely rendering visible dynamics, the model also exhibits the capability to reason about the evolution of unobserved states. For instance [...] a vehicle leaves the frame, continues its trajectory while unobserved, and reappears at a physically plausible location rather than vanishing or freezing.  \n[...] generate coherent video sequences extending up to 10 minutes in duration. [...] our model excels in motion dynamics while maintaining visual quality and temporal smoothness comparable to leading competitors.\n\n[See this cat video for an example.](https://old.reddit.com/r/singularity/comments/1qq7ddv/lingbotworld_achieves_the_holy_grail_of_video/) Notice not just the cat, but the books on the shelves.",
              "score": 3,
              "created_utc": "2026-01-30 04:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzfd7",
          "author": "PeachScary413",
          "text": "This looks like ass ðŸ‘ðŸ‘Œ",
          "score": 3,
          "created_utc": "2026-01-30 14:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hgqnt",
          "author": "Historical-Internal3",
          "text": "Guess I'll try this on my DGX Spark cluster then realize its a fraction of what I actually need in terms of requirements.",
          "score": 4,
          "created_utc": "2026-01-29 21:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o6xuu",
          "author": "CacheConqueror",
          "text": "Less than 30 fps :/",
          "score": 1,
          "created_utc": "2026-01-30 20:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2va927",
          "author": "PrixDevnovaVillain",
          "text": "Very intriguing, but I don't want this technology to replace level design for video games; always preferred handcrafted worlds.",
          "score": 1,
          "created_utc": "2026-01-31 22:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31v85k",
          "author": "NoSolution1150",
          "text": "it looks like it may have much better constancy thanks to creating a 3d map of the area in real time.\n\nonly downside is the 16 fps vs 20 .   but hey still neat progress!\n\ncant wait to see whats next!",
          "score": 1,
          "created_utc": "2026-02-01 22:22:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xki4",
          "author": "No-Employee-73",
          "text": "I was thinking nice time to head home and install for my 5090 64gb but no way can us mere peasants run this",
          "score": 1,
          "created_utc": "2026-02-01 22:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hn6hu",
          "author": "Aggressive-Bother470",
          "text": "It looks awesome but it's not a 'world model' is it?Â \n\n\nA 'world rendering model' perhaps?",
          "score": -1,
          "created_utc": "2026-01-29 21:52:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hvh9p",
              "author": "OGRITHIK",
              "text": "Then Genie 3 isn't a world model either?",
              "score": 7,
              "created_utc": "2026-01-29 22:33:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2jvilt",
              "author": "HorriblyGood",
              "text": "World model is more of a research term referring to foundational models that models real worldâ€™s physics, interactions, etc. As opposed to language models, vision models.",
              "score": 1,
              "created_utc": "2026-01-30 05:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ovf0q",
          "author": "idersc",
          "text": "Why are they both exactly 60sec ? is there any reason ? (i would have expect it to be lower or higher since it's 2 different companies but not the same)",
          "score": 0,
          "created_utc": "2026-01-30 22:41:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p0quz",
              "author": "Basic_Extension_5850",
              "text": "60 seconds is a common unit of timeÂ ",
              "score": 1,
              "created_utc": "2026-01-30 23:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z13up",
          "author": "SVG-CARLOS",
          "text": "\"FULLY OPEN SOURCE\".",
          "score": 0,
          "created_utc": "2026-02-01 14:15:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqhhtx",
      "title": "Mistral CEO Arthur Mensch: â€œIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.â€",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/wd12dl725cgg1",
      "author": "Wonderful-Excuse4922",
      "created_utc": "2026-01-29 18:56:08",
      "score": 578,
      "num_comments": 66,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2iudfd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-30 01:40:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gpbvc",
          "author": "Kahvana",
          "text": "He's arguing for using open (weight) models, which is great!",
          "score": 211,
          "created_utc": "2026-01-29 19:11:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b6w8r",
              "author": "crud_despair",
              "text": "their take is really good for consumers and companies compared to what the american AI corps want.",
              "score": 2,
              "created_utc": "2026-02-03 08:36:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2i3rqr",
              "author": "LoaderD",
              "text": "\"No, no, ban his thread, what if OpenAI posts some marketing news? It might detract from that attention\" - Average Thread Poster here.",
              "score": -12,
              "created_utc": "2026-01-29 23:15:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jxfzw",
                  "author": "Kahvana",
                  "text": "Not sure where you draw that negative conclusion from, but I'm not seeing it.",
                  "score": 18,
                  "created_utc": "2026-01-30 05:35:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kot2o",
                  "author": "hugthemachines",
                  "text": "> Average Thread Poster here\n\nThat's severely biased.",
                  "score": 6,
                  "created_utc": "2026-01-30 09:26:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gwtcs",
          "author": "artisticMink",
          "text": "I really like mistral, they've treading good balance between corpo and open weights. I wish them that they get their major breakthrough eventually.",
          "score": 99,
          "created_utc": "2026-01-29 19:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hh3yq",
              "author": "AppealSame4367",
              "text": "Look at their references. They already had their breakthrough behind the curtains. Consulting the French ministry of defense tells you that this is a national French asset.",
              "score": 23,
              "created_utc": "2026-01-29 21:23:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hjyot",
                  "author": "ProfessionalSpend589",
                  "text": "Maybe they play the same game as China - release open source models so the American companies donâ€™t become monopolies and strongarm Europe into submission again.\n\nFrance is very sensitive to the US politics from my understanding over the years.",
                  "score": 22,
                  "created_utc": "2026-01-29 21:37:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2s8oys",
                  "author": "interesting_vast-",
                  "text": "not exactly the French governmentâ€™s (really most european governments) involvement with their private sector is vastly different from the USA, Muratis new company got a $10 mil investment from Albanian government even though the â€œcompanyâ€ literally only has (almost) one product that only 2-3 universities kinda use. (Mira Murati is Albanian) \n\nThese â€œinvestmentsâ€/ partnerships tend to be more about public image, Mistral is pretty much the only pure French large tech company, as a country trying to position itself as a place for europeans to come start their tech startups, itâ€™s in the french governmentâ€™s best interests to play up Mistralâ€™s success. Also Europe has had a huge push for open-source tech in general especially in the public sector as they see reliance on american big tech as a operational/â€œsecurityâ€ risk, and Mistral fits that role very well. \n\nI mostly use Mistral models my self, so I really do appreciate the work the company does, but I do not believe their involvement with the french government is â€œa breakoutâ€ moment. \n\nI believe Mistral will truly shine and have a breakout moment in a couple years when the hype begins to focus on performance vs efficiency, how good can your small and resource efficient model actually be? this is something they have had tremendous success with (although mostly unnoticed as current hype is still focused on top line performance rather than efficiency) but imo thats going to be their time to shine and I cannot wait!",
                  "score": 3,
                  "created_utc": "2026-01-31 13:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gws06",
          "author": "tarruda",
          "text": "Looking forward for the next open Mistral 8x22 !",
          "score": 32,
          "created_utc": "2026-01-29 19:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gs470",
          "author": "RoyalCities",
          "text": "I mean it makes sense. \n\nIve been training a new sample generator. It didn't cost me tons of electricity but as the model is released into the wild (and especially quantized down for lower compute) the cost goes down proportional to a models usage since it's then on device for others to use.\n\nBut most closed models don't want that and instead of trying to target local ARM infrastructure or just low VRAM systems on prem systems, they make giant massive models and hold them hostage behind pay walls.",
          "score": 43,
          "created_utc": "2026-01-29 19:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gx4f8",
              "author": "KitchenSomew",
              "text": "Exactly right. The model democratization is the key point. When models are released openly, the cost distribution happens naturally across the community. This is the real advantage of open source AI - not just transparency, but practical economics that make intelligence accessible to everyone, not just big players with server farms.",
              "score": 28,
              "created_utc": "2026-01-29 19:48:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hhozr",
                  "author": "Green-Ad-3964",
                  "text": "In fact.\n\nNever believe corporations saying that AI will make everyone wealthy and then keep their model closed and cloud-based.",
                  "score": 18,
                  "created_utc": "2026-01-29 21:26:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hbu4g",
              "author": "AggravatinglyDone",
              "text": "Sample as in music sample? Or something else? Which model?",
              "score": 2,
              "created_utc": "2026-01-29 20:58:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hepsc",
                  "author": "RoyalCities",
                  "text": "As in for music production / producers.\n\nThis is an older model of mine. The next will be much more capable.\n\nhttps://x.com/i/status/1864709213957849518",
                  "score": 5,
                  "created_utc": "2026-01-29 21:12:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gvime",
          "author": "AurumDaemonHD",
          "text": "Bravo. \n\nAdditionally i d say the privacy aspect is easily overlooked. But imagine google creates some way to take your convos with gemini and create a persona like you that ur family members can chat with but with ads :D and keeps it locked. As he said this is so important that it needs to be open source. Its ironic the corpo snakes were lobbying for lockdown on ai.\n\nGiving them free data about us in exchange for free services is actually good for them. Data is the new currency. \n\nYou could mine a persons social sites and llm chats and history and all and effectively create a clone of them to some degree. Personalized ads were just the beginning. But i digress...",
          "score": 16,
          "created_utc": "2026-01-29 19:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gvg8d",
          "author": "HugoCortell",
          "text": "He is not wrong, but at the same time, those are big words for a team that ultimately can't do much towards that. Because while FOSS models are on par if not better than closed-sourced ones, there is ultimately there's a hardware bottleneck between what consumers can attain (and afford) and what big companies have and withhold from us peasants. And those companies are working very hard to make access to good local hardware is every more prohibitively expensive.\n\nFor the change he seemingly is arguing towards, you'd need a company that can manufacture and distribute its own hardware. And Europe is in no such position, all prior attempts have failed. Maybe now is a good time to try again, but nobody has the money or willingness to take the risk.",
          "score": 15,
          "created_utc": "2026-01-29 19:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hv3z7",
              "author": "fuckingredditman",
              "text": "europe cannot manufacture it but europe has part of the supply chain (ASML, Zeiss IIRC also, ...) and is also generally in a good position to breed innovation through academia and small companies, there are a few working on intesting frontier technology like silicon photonics/photonic computing though of course it will take quite some time for those to become relevant\n\nand US and nvidia aren't independent either, there are by far not enough leading edge semiconductor plants in the US to manufacture all the hardware they want to deploy.\n\nbut yeah, all the more reasons to fund more of this kind of stuff to be more independent on hardware also ASAP",
              "score": 6,
              "created_utc": "2026-01-29 22:31:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2i0p0w",
                  "author": "HugoCortell",
                  "text": "We could, but likely never will. All prior attempts failed.\n\nEurope simply does not have the money to invest the necessary resources, or the organizational capacity to even ramp up the way China did. Yes, we have the academia, but it exists just so Nvidia can hire them. Because there's no money here for their efforts to be put to use.",
                  "score": -5,
                  "created_utc": "2026-01-29 22:59:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2h4c8d",
              "author": "rerri",
              "text": ">For the change he seemingly is arguing towards, you'd need a company that can manufacture and distribute its own hardware. And Europe is in no such position, all prior attempts have failed. Maybe now is a good time to try again, but nobody has the money or willingness to take the risk.\n\nI'm not sure I follow. Can't a European company just buy AMD/Nvidia GPU's and run their customized open-source model on those?\n\nedit: oh, I guess the part where he says \"you want to make sure your access to intelligence cannot be throttled\". Yeah, Europe is in a worse place than some other players in that regard.",
              "score": 7,
              "created_utc": "2026-01-29 20:22:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hjyy6",
                  "author": "sartres_",
                  "text": "Even without restrictions, European companies aren't in a good place to compete with American or even Chinese companies for GPU supply.",
                  "score": 1,
                  "created_utc": "2026-01-29 21:37:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2i01l6",
                  "author": "HugoCortell",
                  "text": "They can't. The US would rather go scorched earth on our asses than ever let us secure a strategic resource like chips. They'd never tolerate the sale, never forgive the steal. And if we tried to stand up on our own, they'd make sure to throw every wrench they can at it (not that it would be needed, we know how to shoot ourselves in the foot!).",
                  "score": 0,
                  "created_utc": "2026-01-29 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hg3vl",
              "author": "NewConfusion9480",
              "text": "The difference between 0 AI and the AI you can run on a 4070 is greater than the difference between the AI you can run on a 4070 and the AI you can run in a 1GW datacenter. Cannot divide by zero.\n\nThere is a decreasing marginal utility to intelligence for an individual person, and there is simply no need to maintain anything like parity vs massive industry figures selling intelligence as a service.\n\nYes, an F1 team can produce the quickest, fastest vehicle in the world. A Bugatti Chiron can go 300mph with over 1000hp. But what is the actual marginal utility to me of anything over 120HP? Absolutely nothing.\n\nYes, the military has massive artillery, but what's the actual marginal utility to me of anything more powerful than an AR?\n\nI want open-sourced local AI to improve, and it will, but it doesn't need to \"keep up\" with the frontiers.",
              "score": 11,
              "created_utc": "2026-01-29 21:19:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jk8bl",
                  "author": "zball_",
                  "text": "a model that runs on a 4070 has virtually no use for real world jobs, the real value it brings is zero in this regard. On the other hand AI running on data centers are creating real values.",
                  "score": 1,
                  "created_utc": "2026-01-30 04:07:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2huzq4",
                  "author": "Bakoro",
                  "text": "There's a massive difference between models of different abilities. It's not strictly about parameter size, but different classes of models have abilities that are more like a step function than a smooth gradient of capacity.  \n   \nWe already have fairly capable AI agents now.  \nWe're very close to having models that can use a computer more like a person would, where it can see the screen, move the mouse around, and just do arbitrary tasks. \n   \nThe \"marginal utility\" for a single person is that you're effectively not just one person anymore. If the AI agents are sufficiently capable, then having more agents becomes a nonlinear increase in power. You stop being just one person and can do things that it used to take whole teams to do.\n   \nI am already doing ~4X the work I used to, because the AI agents can take care of the things that are easy, but time-consuming.  \nI can do the thinking part, and the AI can do the grunt work.  \n   \nAs-is, I have been architecting whole projects that would have taken me years to on my own. \nThat would have been impossible with LLMs from 2 years ago.  \n   \nWe absolutely need open source that stays close to frontier.   \nI'm telling you, there's a step function in the complexity of tasks, and a step function in the things that you can do when you have more modalities.   \n  \nJust like you said, you can't divide by zero, and if the closed source models can use arbitrary computer programs and the open source ones can't, then there's an enormous gulf in power.",
                  "score": 0,
                  "created_utc": "2026-01-29 22:30:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hw6a2",
              "author": "Evening_Ad6637",
              "text": "Well, don't forget that ASML is based in Europe, in the Netherlands.\n\nWithout ASML, there would be no Nvidia, no other GPUs, no smartphones, no modern laptops, and much more.\n\nAnd it's very interesting that ASML recently (in September 2025) invested $1.5 billion in Mistral AI. So to me, it looks like the Europeans are (MAYBE) not sleeping, but rather planning for the long term. In any case, there is no shortage of money in Europe.\nAnd I think that a super heavyweight like ASML (and, in my opinion, the only **real** monopoly in the world) can move mountains, and the signals I'm seeing make me confident about Europe/the EU.",
              "score": 3,
              "created_utc": "2026-01-29 22:36:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2jii0e",
              "author": "florinandrei",
              "text": "Let me summarize your comment real quick:\n\nYou can't win, so lie down and stop breathing.\n\nThat's... uh... inspiring. /s",
              "score": 2,
              "created_utc": "2026-01-30 03:56:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2hfyhr",
              "author": "goatyellslikeman",
              "text": "If an open model is heavily commoditized such that you can switch providers easily, then at least youâ€™re not beholden to a monopoly.",
              "score": 2,
              "created_utc": "2026-01-29 21:18:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2i0c83",
                  "author": "HugoCortell",
                  "text": "Unless that monopoly holds the hardware too. Which currently they are in the process on.\n\nHardware must be sovereign, otherwise the free software is wasted on it.\n\n(For the record, I didn't downvote you)",
                  "score": 2,
                  "created_utc": "2026-01-29 22:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kv3ly",
              "author": "FrostAutomaton",
              "text": "I get the impression that he is arguing for companies to use local LLMs, not individual consumers. In that case, maintaining your own server infrastructure becomes far more viable.",
              "score": 1,
              "created_utc": "2026-01-30 10:23:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oy8pt",
              "author": "jsonmeta",
              "text": "I think itâ€™s a matter of time and big tech knows it, thus they manipulate the hardware affordability and accessibility not only for their competitors but also for the end customer. Just like most people couldnâ€™t afford to have a personal computer at home back in the 80s",
              "score": 1,
              "created_utc": "2026-01-30 22:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2klmkt",
          "author": "TheTerrasque",
          "text": "A friend of mine learned this the hard way some days ago. He has subscription to most large AI vendors, but mostly use Claude in his daily work. So much he has the $200 per month tier.\n\nSome days ago he was working on recovering some client's old servers. Pulling out data and config, fixing things, and migrating to VM's, using Claude Code to help with it. \n\nIn the middle of that, his account got banned. He still doesn't know why, he still hasn't heard back from their customer service. And he was shocked they could just turn off his access like that. \n\nIt really highlights the point he's making though.",
          "score": 3,
          "created_utc": "2026-01-30 08:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hcicw",
          "author": "Possible-Machine864",
          "text": "He's not wrong.",
          "score": 5,
          "created_utc": "2026-01-29 21:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kp5qs",
          "author": "hugthemachines",
          "text": "The relevance of this for people outside USA increases with recent events. It is probably wise to have a backup plan in case USA blocks outside access to their AIs.",
          "score": 2,
          "created_utc": "2026-01-30 09:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hhyen",
          "author": "Legitimate-Pumpkin",
          "text": "How his accent naturally stresses his point: this is impogtant!",
          "score": 3,
          "created_utc": "2026-01-29 21:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h5yap",
          "author": "finah1995",
          "text": "Respect to him for saying it as it is and promoting companies to not be locked in. \n\nIn Strategic stuff you don't want your company affected. I mean no one wants what happened to companies like [Nayara Energy](https://www.nayaraenergy.com/) happens to them. Its more important, in government or semi-government it becomes more complicated than this.",
          "score": 4,
          "created_utc": "2026-01-29 20:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jcn2z",
          "author": "siegevjorn",
          "text": "Thanks for sharing. Glad to see someone in the mainstream talking about something that makes sense and something that humanity can hope for. It has been just so tiring to hear all the bs from the big names.",
          "score": 1,
          "created_utc": "2026-01-30 03:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kv213",
          "author": "Saltwater_Fish",
          "text": "Intelligence is not exactly the same as electricity. Intelligence is not fungible. The responses from models such as GPT, Claude, Kimi, DeepSeek all have their own unique characteristics.",
          "score": 1,
          "created_utc": "2026-01-30 10:23:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h57rd",
          "author": "eli_pizza",
          "text": "Sorta just a generic argument in favor of open source, no?\n\nI understand it's not so easy, but it would be a lot more powerful an argument if their models were fully open (training data and tooling). If Mistral cuts you off, you can keep using the already released models and I guess can fine-tune them on your own. But you can't rebuild them or easily use them as the basis for a new model.",
          "score": 1,
          "created_utc": "2026-01-29 20:26:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h7n5k",
          "author": "Hour_Bit_5183",
          "text": "That's because they are consuming all the power. This is nonsense. All the intelligence that exists is just in books they stole and life experiences.",
          "score": -6,
          "created_utc": "2026-01-29 20:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l2pgv",
          "author": "According-Zombie-337",
          "text": "Those are some really good points. I just need to point out that Mistral is not viable as an option for this right now, and probably won't be for the foreseeable future.",
          "score": -1,
          "created_utc": "2026-01-30 11:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hnuf3",
          "author": "Ashley_Sophia",
          "text": "Laughs in Starlink\n\nOh wait....",
          "score": -4,
          "created_utc": "2026-01-29 21:55:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq6n3t",
      "title": "GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/uf2m03ak2agg1.png",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-29 11:58:22",
      "score": 477,
      "num_comments": 101,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2eo3cr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 13:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ekl8p",
          "author": "combrade",
          "text": "The IPTV shit is useful, projects like that. Donâ€™t discourage those . High school Me learned about Github through projects that sailed the high seas for movies and shows.",
          "score": 86,
          "created_utc": "2026-01-29 13:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2et69h",
          "author": "skybsky",
          "text": "I'm somewhat pleased that some comments actually see that this post is bs. \"somewhat\", because others still don't read before commenting",
          "score": 48,
          "created_utc": "2026-01-29 13:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ehseu",
          "author": "Seaborgg",
          "text": "This post has me the feeling that you were hoping I didn't read the text in the image.",
          "score": 192,
          "created_utc": "2026-01-29 12:53:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fcd94",
              "author": "ObsidianNix",
              "text": "Thas how you know they didnt either, most likely just a bot on Reddit. Maybe another case study?",
              "score": 51,
              "created_utc": "2026-01-29 15:31:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ha43h",
                  "author": "DinoAmino",
                  "text": "Totally. And weak-ass posts like this don't get this amount of upvotes without a bot army by your side.",
                  "score": 6,
                  "created_utc": "2026-01-29 20:50:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nypl1",
                  "author": "SuchAGoodGirlsDaddy",
                  "text": "I betchya itâ€™s posted by an agent framework on GitHub that will be dead in one week.",
                  "score": 1,
                  "created_utc": "2026-01-30 20:04:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gp23s",
              "author": "_stack_underflow_",
              "text": "Are they based on your own starred repos?\n\nhttps://preview.redd.it/q4m9mpsm7cgg1.png?width=1555&format=png&auto=webp&s=4aeac74079c5019cba9cb067f7307aff0a9a985b\n\n",
              "score": 12,
              "created_utc": "2026-01-29 19:09:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hbcol",
                  "author": "ObsidianNix",
                  "text": "Probably not. The screenshot the bot posted is not logged in. Look at the top right. Its probably a rolling Trending where theyâ€™re just too many so it randomly selects topN repos.",
                  "score": 5,
                  "created_utc": "2026-01-29 20:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ks6z3",
              "author": "DryWeb3875",
              "text": "https://preview.redd.it/269bii71mggg1.jpeg?width=1179&format=pjpg&auto=webp&s=dd6086c76803b1c8db6ab831beb1663326be24fc\n\nBlue is AI based, green isnâ€™t.\n\nEdit: mlx-audio is also AI based.",
              "score": 2,
              "created_utc": "2026-01-30 09:57:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2etr3k",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2026-01-29 14:00:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eu516",
                  "author": "eli_pizza",
                  "text": "What do you think â€œbotâ€ means?",
                  "score": 4,
                  "created_utc": "2026-01-29 14:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2edemx",
          "author": "gscjj",
          "text": "> half are agent frameworks \n\nI only see one agent framework here and itâ€™s by Microsoft. Other that I see RAG tooling, model(NanoGPT, Grok), model cli for code (Kimi), browser API",
          "score": 135,
          "created_utc": "2026-01-29 12:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h2n3b",
              "author": "PaddyIsBeast",
              "text": "I mean I completely disagree with op, but.. by who's definition are MCP servers and Agent skills not part of the agentic framework?",
              "score": 1,
              "created_utc": "2026-01-29 20:14:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ee0zs",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -36,
              "created_utc": "2026-01-29 12:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2efzp6",
                  "author": "gscjj",
                  "text": "Theyâ€™re all AI related, theyâ€™re not all agentic and only is a framework",
                  "score": 47,
                  "created_utc": "2026-01-29 12:41:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2egqlo",
                  "author": "macumazana",
                  "text": "so, youre comparing a model, rag, skills as prompt and tools? \n\nwhy dont you go further and compare a laptop a programming language and a human hand?",
                  "score": 33,
                  "created_utc": "2026-01-29 12:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2erky4",
          "author": "belgradGoat",
          "text": "Which ones? Microsoft vibe voice or grok? Did you even red or look at what you posted?",
          "score": 29,
          "created_utc": "2026-01-29 13:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g9cqv",
              "author": "FaceDeer",
              "text": "Presumably he figures Microsoft and Grok are going to be dead in a week.",
              "score": 8,
              "created_utc": "2026-01-29 17:59:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2gim1t",
                  "author": "Former-Ad-5757",
                  "text": "Well qwen4 can probably code windows in 1 hour, office in 30 minutes and all azure functions in like 4 hours, so basically I see no reason ms should not be dead in one week as long as qwen4 is released and performs as I suspectâ€¦",
                  "score": 1,
                  "created_utc": "2026-01-29 18:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eiavw",
          "author": "one-wandering-mind",
          "text": "I'm surprised there haven't been more stories about supposed AI tooling with viruses or exploits. People are so willing to install and run random code they find on github that is brand new with no security review.\n\n\nPeople on this sub particularly seem terrified to use model in the cloud for fear of their sensitive data being exposed, but many will happily install random stuff and implement networking they don't understand that risks exposing their home server to attacks.",
          "score": 26,
          "created_utc": "2026-01-29 12:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f61y0",
              "author": "SilentLennie",
              "text": "I mean...\n\nhttps://www.reddit.com/r/vibecoding/comments/1qpnybr/found_a_malicious_skill_on_the_frontpage_of/",
              "score": 7,
              "created_utc": "2026-01-29 15:02:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fagw1",
              "author": "TinFoilHat_69",
              "text": "You could just look through the code in the repo and ask GitHub copilot to review the repo itâ€™s built into GitHubs website. I personally would rather just look at designs and ideas and implement my version to fit my own needs.",
              "score": 2,
              "created_utc": "2026-01-29 15:23:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hk688",
                  "author": "eli_pizza",
                  "text": "I'm pretty confident I could write some malicious code that passes that check and I'm not exactly a malware expert.",
                  "score": 3,
                  "created_utc": "2026-01-29 21:38:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2if943",
              "author": "No_Afternoon_4260",
              "text": "There's [that](https://professorsigmund.com/pdfs/glass_box_paradox.pdf) about the clawdbot's mess\n\nEdit: not really about clawdbot but badly configured local ai stuff based on clawdbot mess study",
              "score": 1,
              "created_utc": "2026-01-30 00:18:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ebp1v",
          "author": "Pretend-Pangolin-846",
          "text": "Off-topic, but what are claude skills?",
          "score": 33,
          "created_utc": "2026-01-29 12:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eci2z",
              "author": "adam444555",
              "text": "A fruitful word for the on-demand system prompt.",
              "score": 80,
              "created_utc": "2026-01-29 12:18:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ecrmn",
                  "author": "Pretend-Pangolin-846",
                  "text": "I get the appeal though, its as if an optimized AGENTS.md file for whatever task or domain you want to work with",
                  "score": 15,
                  "created_utc": "2026-01-29 12:20:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2f1627",
                  "author": "1ncehost",
                  "text": "Skills can also include scripts",
                  "score": 4,
                  "created_utc": "2026-01-29 14:38:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ebvbu",
              "author": "Distinct-Expression2",
              "text": "Text that it loads on demend usually describe how to do something specific",
              "score": 9,
              "created_utc": "2026-01-29 12:14:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ec5n5",
                  "author": "Pretend-Pangolin-846",
                  "text": "oh, is it claude dependent, or can it be initialized into any IDE?",
                  "score": 8,
                  "created_utc": "2026-01-29 12:16:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2epbkb",
                  "author": "Electronic-Ice-8718",
                  "text": "So in theory 1 request will need at least 2 LLM calls. One for checking which prompts to load, another one to actually process the input?",
                  "score": 2,
                  "created_utc": "2026-01-29 13:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2epoyl",
              "author": "DJT_is_idiot",
              "text": "I'm amazed by the answers you received. Are people that clueless in here? Only one answer so far that actually scratches the surface of what skills are.\n\nEdit: maybe it was satire and I just didn't get it",
              "score": 3,
              "created_utc": "2026-01-29 13:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f2vlq",
                  "author": "Pretend-Pangolin-846",
                  "text": "Not satire, I am serious.",
                  "score": 2,
                  "created_utc": "2026-01-29 14:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gftrx",
              "author": "paperpizza2",
              "text": "Tool call definitions but saved in a text file.",
              "score": 1,
              "created_utc": "2026-01-29 18:27:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ele6b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -4,
              "created_utc": "2026-01-29 13:15:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2enwdh",
                  "author": "abnormal_human",
                  "text": "No, they're just chunks of system prompt that you can easily turn on/off. Literally just .claude/skills/foo/SKILL.md.   \n  \nIf you have a repeated dev task, you can document the procedure, layers of code in the system, etc, and make Claude Code more reliable at repeating that process. I have skills for iterating on agent evals so I can in one command get it to run my eval suite, prioritize the things I want prioritized, iterate on the failures in a certain efficient way, then come back to me when the score is improved by a certain amount. I have another one for integrating a form of API integration that my project does very often so that it knows what files/layers to touch and avoids pitfalls. And a third when I'm working on payments/etc stuff that makes sure a current copy of our billing mechanics is in scope. Not a super complex idea, but very effective.",
                  "score": 4,
                  "created_utc": "2026-01-29 13:29:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2esje8",
          "author": "russianguy",
          "text": "I feel the same way about MCP servers. I ain't about to run some code from `some-fucking-guy.sh` repo.",
          "score": 8,
          "created_utc": "2026-01-29 13:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2etb23",
              "author": "Distinct-Expression2",
              "text": "Mcp are dead, skills way better",
              "score": -6,
              "created_utc": "2026-01-29 13:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fbabl",
                  "author": "ZachCope",
                  "text": "Doesnâ€™t MCP â€˜doâ€™ something and a skill tells the LLM â€˜how to doâ€™ something? 2 different functions.Â ",
                  "score": 11,
                  "created_utc": "2026-01-29 15:26:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eio82",
          "author": "raysar",
          "text": "This is a good thing. Concurrence is the only solution fort fast evolution.",
          "score": 4,
          "created_utc": "2026-01-29 12:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ef2f3",
          "author": "vesko26",
          "text": "this is much worse then JS framework hell. Those at least kinda worked. This is just garbage",
          "score": 21,
          "created_utc": "2026-01-29 12:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f48q7",
              "author": "BigHugeOmega",
              "text": "How is it garbage? Which software on this list doesn't work?",
              "score": 13,
              "created_utc": "2026-01-29 14:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f5uoz",
                  "author": "vesko26",
                  "text": "MCP as a standard is garbage, literally the first issue on the ai=data=science-team project is arbitrary code execution due to prompt injection",
                  "score": -7,
                  "created_utc": "2026-01-29 15:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2fzl57",
          "author": "CV514",
          "text": "Thanks\n\nNow I'm aware there is GitHub Trending\n\nAnd it is as worthless as I imagined",
          "score": 3,
          "created_utc": "2026-01-29 17:15:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eqb5k",
          "author": "Admirable-Choice9727",
          "text": "I hate \"browser-use\" it disappointed me so much.",
          "score": 5,
          "created_utc": "2026-01-29 13:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fxf2t",
              "author": "vatta-kai",
              "text": "How exactly was it disappointing? Iâ€™m thinking of trying it out",
              "score": 1,
              "created_utc": "2026-01-29 17:05:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2eqnwu",
              "author": "Distinct-Expression2",
              "text": "Itâ€™s shit; just use vercel agent-browser",
              "score": -5,
              "created_utc": "2026-01-29 13:44:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f2agi",
          "author": "Healthy-Nebula-3603",
          "text": "Natural selection...",
          "score": 2,
          "created_utc": "2026-01-29 14:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc1mc",
          "author": "Alex_1729",
          "text": "Meaningless hateful post.",
          "score": 2,
          "created_utc": "2026-01-29 15:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gabq3",
              "author": "FaceDeer",
              "text": "And even if true, what's wrong with throwing lots of stuff at the wall knowing that only 10% of it is going to stick? This is a time of experimentation and new ideas.",
              "score": 2,
              "created_utc": "2026-01-29 18:03:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2goy1w",
                  "author": "Alex_1729",
                  "text": "Who knows their reasons, but hating on vibecoding, something which if you're not using, simply means you're not adaptive and probably not a very resourceful person.\n\nAcknowledging your point, I'll go one step further: even if it's mostly slop, it's worth it. Most people don't want slop, but you have to make tradeoffs, so it can end up not a perfect product. That's reasonable. A person said this in a recent email newsletter I read, and I quote:\n\n>... working with agents is genuinely so fun and 2026 will be the year ofÂ slopÂ - given the above advancements (I agree - but slopping our way to learn and produce things that arenâ€™tÂ slopÂ is still a reasonable path).",
                  "score": 3,
                  "created_utc": "2026-01-29 19:09:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2h7xkn",
                  "author": "Mickenfox",
                  "text": "Building one of the same thing we have 20 of is not experimentation. It's just a waste of human hours that could have gone to improving a real product.",
                  "score": 1,
                  "created_utc": "2026-01-29 20:39:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ekliy",
          "author": "MaxKruse96",
          "text": "Yes, we are in hell. People somehow got the idea that \"Agents\" are the future, and \"We have jarvis at home\" - when in reality nothing is even remotely ready or secure enough for that.",
          "score": 3,
          "created_utc": "2026-01-29 13:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h8110",
              "author": "Mickenfox",
              "text": "Agents is just the next buzzword after AI wasn't buzzy enough.",
              "score": 2,
              "created_utc": "2026-01-29 20:40:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ey4w1",
          "author": "InterstellarReddit",
          "text": "everyone is just recycling the same open source solution to a problem with a little change ui twist not changing anything of value.",
          "score": 1,
          "created_utc": "2026-01-29 14:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f5ufx",
          "author": "SilentLennie",
          "text": "Interesting kimi-cli did make it, moltbot (formerly Clawd Bot) did not.\n\nWhatever you think of it: it's open source/free software at work: 'scratch your own itch', something doesn't do what you want and the existing solutions you know about it's not a good fit... create your own.",
          "score": 1,
          "created_utc": "2026-01-29 15:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fxsyk",
          "author": "visarga",
          "text": "Hey, I have 2 or 3 of my own, I am off to writing the 4th now. But I did learn interesting lessons from this exercise.",
          "score": 1,
          "created_utc": "2026-01-29 17:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g473x",
          "author": "blurredphotos",
          "text": "Just like AI art boards.\n\nSad but true.",
          "score": 1,
          "created_utc": "2026-01-29 17:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h83va",
          "author": "Mickenfox",
          "text": "Oh yeah, GitHub is all AI-coded AI-related slop. Twice the AI, double the slop.",
          "score": 1,
          "created_utc": "2026-01-29 20:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hb70x",
          "author": "cleverusernametry",
          "text": "By agent framework, I think you mean ai tooling. And by 90%, I think you mean 99%",
          "score": 1,
          "created_utc": "2026-01-29 20:55:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hfxm9",
          "author": "WSATX",
          "text": "That's life men, maybe 90% of us will be gone in one week. But posting screenshot only post never die, eh, life is unfair !",
          "score": 1,
          "created_utc": "2026-01-29 21:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iclso",
          "author": "Ok-Lobster-919",
          "text": "BREAKING: OP Discovers what the term 'trending' means.",
          "score": 1,
          "created_utc": "2026-01-30 00:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iiipk",
          "author": "thetaFAANG",
          "text": "Its the â€œget me $1,000,000/yr job in aiâ€ moment of ai",
          "score": 1,
          "created_utc": "2026-01-30 00:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kv6ui",
          "author": "Anthonyg5005",
          "text": "The github trending page has basically been like this since 2023. All just random ai framework stuff that no one ever talks about then when you go to it's repo it always has a huge banner in the readme saying \"top repo of the week\"",
          "score": 1,
          "created_utc": "2026-01-30 10:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2earqn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -6,
          "created_utc": "2026-01-29 12:06:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egsdz",
              "author": "gnnr25",
              "text": "Your website is spammy. A pop-up chat and a pop-up newsletter subscription. Geeze",
              "score": 30,
              "created_utc": "2026-01-29 12:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eu62x",
          "author": "IulianHI",
          "text": "The comparison to JS framework hell is spot on. Remember when we had jQuery, Backbone, Ember, Angular, React, Vue, Svelte, and 100 others all trying to solve the same problem? Most of these agent frameworks will meet the same fate - the winners will be the ones that focus on boring, reliable infrastructure (memory, evals, tracing) rather than shiny demos.",
          "score": 1,
          "created_utc": "2026-01-29 14:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ebzb4",
          "author": "k_am-1",
          "text": "Trending page is showing whatâ€™s trending HELPPP!!?",
          "score": -7,
          "created_utc": "2026-01-29 12:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ecuoq",
          "author": "Familiar_Print_4882",
          "text": "I created AI primitives (not frameworks)\nDo you think that will die in 1 week too ?\n\nhttps://github.com/withceleste/celeste-python",
          "score": -8,
          "created_utc": "2026-01-29 12:20:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ef1xy",
              "author": "Distinct-Expression2",
              "text": "you created a proxy which you'll have to maintain yourself adding delay in the request and forcing people to self host it when all major providers are switching to open responses - quite bad timing",
              "score": 8,
              "created_utc": "2026-01-29 12:35:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2eg9e2",
                  "author": "Familiar_Print_4882",
                  "text": "Itâ€™s fully compatible with openresponses.\nAnd no need to maintain much, just the api changes which happen 1nce per year.\nAlso you can use extra_body if anythingâ€™s not integrated so you never miss a new feature.\nWhat do you think now ? Convinced or not.\nI prefer youâ€™re not and tell me why.\nIâ€™m looking for an argument so Iâ€™ll be like ah yeah ok not worth it then, it has no future.",
                  "score": 1,
                  "created_utc": "2026-01-29 12:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ev76p",
          "author": "KitchenSomew",
          "text": "The pattern we're seeing here mirrors every tech hype cycle - lots of experimentation, most will consolidate or fade. The frameworks that will survive are the ones solving real infrastructure problems: reliable memory management, proper eval frameworks, and production-ready orchestration.\n\n\n\nMost of these are just thin wrappers around LangChain or AutoGen with marketing fluff. The real value will be in the unglamorous work - robust error handling, proper logging/tracing, and actual testing frameworks for agents.",
          "score": -1,
          "created_utc": "2026-01-29 14:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f0mlx",
          "author": "KitchenSomew",
          "text": "Classic AI hype cycle in action! While many will fade away, the silver lining is that this explosion of frameworks helps identify what actually works. The ones that survive typically solve a real problem with a clean API. I'm curious which approaches will still be standing in 6 months - my bet is on the ones with strong community support and clear documentation.",
          "score": -2,
          "created_utc": "2026-01-29 14:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egb2m",
          "author": "DeathByPain",
          "text": "I'm definitely not trending but I'm working on a fork of a gui for llama-server that will let you launch multiple servers in a tabbed window ðŸ˜",
          "score": -4,
          "created_utc": "2026-01-29 12:44:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfse6",
      "title": "Run Kimi K2.5 Locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/rxqfj5os74gg1.jpeg",
      "author": "Dear-Success-1441",
      "created_utc": "2026-01-28 16:17:45",
      "score": 446,
      "num_comments": 79,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2bui1v",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 01:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rq6e",
          "author": "Daniel_H212",
          "text": "Anyone tried this on strix halo yet to see how many seconds per token it runs at?",
          "score": 158,
          "created_utc": "2026-01-28 16:49:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28uh5w",
              "author": "IngwiePhoenix",
              "text": "> how many seconds per token\n\nFelt. XD",
              "score": 106,
              "created_utc": "2026-01-28 17:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28yf7t",
                  "author": "bobaburger",
                  "text": "definitely the right question to ask",
                  "score": 33,
                  "created_utc": "2026-01-28 17:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2978iw",
              "author": "JamesEvoAI",
              "text": "I wanted to, but the smallest quant is still 240GB",
              "score": 27,
              "created_utc": "2026-01-28 17:56:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2b3svm",
              "author": "Zyj",
              "text": "Edit: **it runs at 12.5 tokens/s with no context on dual Strix Halo**\n\nCurrently downloading on dual Strix Halo. Performance could be decent, it's 32b active parameters, at 1.8bits per parameter that's 7.2 GB of RAM.\n\nEdit:\n\nOK, by itself llama-server tries to allocate too much data on the second host running rpc-server. I have to manually specify \\`-fit off --tensor-split 48,52\\`\n\n`load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = false)`  \n`load_tensors: offloading output layer to GPU`  \n`load_tensors: offloading 60 repeating layers to GPU`  \n`load_tensors: offloaded 62/62 layers to GPU`  \n`load_tensors: CPU model buffer size = 630.00 MiB`  \n`load_tensors: ROCm0 model buffer size = 109322.47 MiB`  \n`load_tensors: RPC0[192.168.220.2:50052] model buffer size = 118481.80 MiB`\n\nUnfortunately, after loading for several minutes, llama-server crashes after reporting a malformed response from rpc-server:\n\n`llama_context: n_ctx_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized`  \n`llama_context: ROCm_Host output buffer size = 1.25 MiB`  \n`llama_kv_cache: ROCm0 KV buffer size = 279.00 MiB`  \n`llama_kv_cache: RPC0[192.168.220.2:50052] KV buffer size = 270.00 MiB`  \n`llama_kv_cache: size = 549.00 MiB ( 4096 cells, 61 layers, 2/2 seqs), K (f16): 549.00 MiB, V (f16): 0.00 MiB`  \n`sched_reserve: reserving ...`  \n`sched_reserve: Flash Attention was auto, set to enabled`  \n`sched_reserve: RPC0[192.168.220.2:50052] compute buffer size = 264.01 MiB`  \n`sched_reserve: ROCm0 compute buffer size = 362.00 MiB`  \n`sched_reserve: CPU compute buffer size = 36.01 MiB`  \n`sched_reserve: graph nodes = 4852`  \n`sched_reserve: graph splits = 3`  \n`sched_reserve: reserve took 251.87 ms, sched copies = 1`  \n`common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)`  \n`/opt/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp:669: Remote RPC server crashed or returned malformed response`  \n`recv failed (bytes_recv=0, size_to_recv=8)`\n\nHowever, on the side of rpc-server, there are no errors visible in the log.  \nTried it again with a split of 51,49. I got this on the rpc-server side:\n\n`ROCm error: unspecified launch failure`  \n`current device: -1, in function ggml_backend_cuda_synchronize at /opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2850`  \n`hipStreamSynchronize(cuda_ctx->stream())`  \n`/opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:96: ROCm error`\n\nI might be hitting a ROCm bug. I'm using Fedora 43, kernel 6.18.3 and the latest rocm nightly, updated a few minutes ago.\n\nEdit:  \nGot it working with tensor-spliit 46,54, **it runs at 12.5 tokens/s with no context.**  \nI can see the reasoning, it looks like the llama-server webgui doesn't recognize the markup that Kimi K2.5 uses to mark the reasoning or perhaps it doesn't mark it properly at all.",
              "score": 26,
              "created_utc": "2026-01-28 23:00:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bibzi",
                  "author": "Zyj",
                  "text": "It doesn't seem to be very good... more testing tomorrow",
                  "score": 5,
                  "created_utc": "2026-01-29 00:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jaip",
              "author": "RedParaglider",
              "text": "It's a pretty big lift to get a 100gb model loaded on the old girl.  My strix halo has 128gb shared vram/ram so this thing is out of bounds if I need 256gb ram. \n\nThe way it works is you use your ram to pull the model off the disk, then it unloads into the same ram pool the kernel just allocates that ram to vram.  Then you also need additional vram to utilize for context.\n\n I have yet to see another model that will make me switch of glm 4.5 air derestricted.  It works well for pretty simple agentic tasks, creative writing, looking shit up on the web, looking for race conditions in code, etc.  Speed is around 23 t/s so it's not fast, but there is overhead calling remote API's that often means it's faster for simple tasks than even Gemini flash.",
              "score": 14,
              "created_utc": "2026-01-28 18:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a0ym1",
                  "author": "JamesEvoAI",
                  "text": "Give Qwen 3 VL 30B-A3B a shot. It's been my workhorse on this machine and leaves me plenty of memory for other models/tools",
                  "score": 7,
                  "created_utc": "2026-01-28 20:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c1yja",
                  "author": "Far-Low-4705",
                  "text": "gpt-oss 120b is also very good in my experience. no idea about anything beyond STEM tho",
                  "score": 6,
                  "created_utc": "2026-01-29 02:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2971oe",
              "author": "Sufficient_Prune3897",
              "text": "Too little RAM",
              "score": 3,
              "created_utc": "2026-01-28 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29aqsh",
                  "author": "Daniel_H212",
                  "text": "Might not have been in this thread but I saw people talking about using disk space to run this, which is why I used the units of seconds per token.",
                  "score": 8,
                  "created_utc": "2026-01-28 18:11:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28rcxs",
          "author": "misterflyer",
          "text": "IQ0.2\\_XXS wen?",
          "score": 30,
          "created_utc": "2026-01-28 16:47:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28xzcd",
          "author": "Marksta",
          "text": "Thanks for the quants, gave it a spin yesterday. Q2_K_XL seemed perfectly fine as far as coherence goes. Kimi-K2 sticks to its signature style of absolute prompt adherence like a cold robot. 10/10 model, I think its style is what all non-creative focused models should really be striving for. \n\nIts creative side seems slightly better than their last model actually, but in a brute forcing via logic way. In RP scenario its thinking was like \"This character SHOULD say that, that fits that trope...\" and then proceed to deliver the right idea wrong execution of it because it's writing chops are awful ðŸ˜‚",
          "score": 23,
          "created_utc": "2026-01-28 17:16:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28zbti",
              "author": "misterflyer",
              "text": "I experienced the same thing over API with creative writing \n\nlol smh, oh well",
              "score": 5,
              "created_utc": "2026-01-28 17:22:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2945oq",
          "author": "MikeRoz",
          "text": "What is the point of Q5 and up (UD-Q5_K_XL, Q6_K) when the experts are all in int4?",
          "score": 12,
          "created_utc": "2026-01-28 17:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29nh3t",
              "author": "Lissanro",
              "text": "It depends on how quants are made. If preserving the original quality without inflating size is the goal, making Q4_X quant is the only way. Non-INT4 tensors can be kept at Q8. Dynamic quants only make sense for Q3 or lower I think. You can read discussion at https://github.com/ggml-org/llama.cpp/issues/19127 for details, in the forth message someone already upload Q4_X quant if that is what you are looking for. Unsloth quants can be good choice if you need something smaller instead.",
              "score": 10,
              "created_utc": "2026-01-28 19:05:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29yugm",
                  "author": "MikeRoz",
                  "text": "[AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) is the repo. Already downloading it. I'm always a fan of people who provide detailed instructions on how to reproduce their work.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:56:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cfhr7",
                  "author": "danielhanchen",
                  "text": "Oh yes I saw the discussion - I was planning to follow in the footsteps of that GitHub issue but didn't get time yet - will investigate later today!",
                  "score": 2,
                  "created_utc": "2026-01-29 03:15:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29hrqs",
              "author": "defensivedig0",
              "text": "Other parts of the model are not in int4",
              "score": 2,
              "created_utc": "2026-01-28 18:41:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3qiz",
                  "author": "Lissanro",
                  "text": "I checked their quants and it seems like Q4 and higher quants did not properly preserve the original quality. The proper way is to keep INT4 as modded Q4\\_0 and the rest can be kept as Q8\\_0, there is no need for quants higher than Q4\\_X since they only lose performance without any gains. How to make Q4\\_X quant for K2.5 is documented here: [https://huggingface.co/AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) (where you also can download pre-made Q4\\_X quant, it should work both with ik\\_llama.cpp and mainline llama.cpp).",
                  "score": 5,
                  "created_utc": "2026-01-28 20:18:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29yijd",
                  "author": "MikeRoz",
                  "text": "You can use HF's nifty GGUF viewer to see that the experts in their UD-Q5\\_K\\_XL are a mix of Q5\\_K and Q6\\_K, and all Q6\\_K in their Q6\\_K. If the experts were all capped at 4-bit, then difference in size between various quants 4-bit would be trivial, as seen with quants of models like GPT-OSS-120B.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:55:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b0xa9",
          "author": "nonameisdaft",
          "text": "How would 64gb ram amd 24gb of vram handle this?",
          "score": 11,
          "created_utc": "2026-01-28 22:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c1s6v",
              "author": "FrodoTheExplorer",
              "text": "a",
              "score": 9,
              "created_utc": "2026-01-29 02:00:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d03xf",
                  "author": "jmellin",
                  "text": "Â ^ - The next token will arrive next week",
                  "score": 23,
                  "created_utc": "2026-01-29 05:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2iepno",
              "author": "Zeratas",
              "text": "Days per token",
              "score": 2,
              "created_utc": "2026-01-30 00:15:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2s3k9b",
              "author": "ChromatimusX",
              "text": "OOM",
              "score": 1,
              "created_utc": "2026-01-31 12:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2x1ul7",
              "author": "_stack_underflow_",
              "text": "I can tell you!\n\nI run Kimi using swap on an nvme drive + 4090 + 128GB ram.\n\nIt averaged 1.5 tokens a second.",
              "score": 1,
              "created_utc": "2026-02-01 04:40:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28u9qv",
          "author": "IngwiePhoenix",
          "text": "Congrats to the small handful of LocalLLaMa people that have >300 <500 GB of VRAM to do this. x)\n\nI'll just keep dreaming...",
          "score": 68,
          "created_utc": "2026-01-28 17:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29aqfi",
              "author": "devvie",
              "text": "That isn't what the post says. Disk + RAM + VRAM >= 240GB, and you can run it. The poster claims 5T/s with 24GB VRAM.",
              "score": 27,
              "created_utc": "2026-01-28 18:11:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29dway",
                  "author": "juggarjew",
                  "text": "5T/s with 256GB RAM as well, large asterisks there lol",
                  "score": 38,
                  "created_utc": "2026-01-28 18:24:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2aad17",
                  "author": "ReasonablePossum_",
                  "text": "So you say a 6gb vram + 234gb ssd space? 8)",
                  "score": 4,
                  "created_utc": "2026-01-28 20:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29idcq",
              "author": "pandodev",
              "text": "Us normies over here will just be watching videos about it....",
              "score": 2,
              "created_utc": "2026-01-28 18:43:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2931om",
          "author": "Historical-Internal3",
          "text": "I have two DGX spark's clustered and I don't think I could run this in any meaningful way lol.",
          "score": 6,
          "created_utc": "2026-01-28 17:38:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6ojm",
              "author": "Zyj",
              "text": "Why not? Should work.",
              "score": 3,
              "created_utc": "2026-01-28 23:14:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29bbtw",
              "author": "Boring_Resolutio",
              "text": "this is really crazy, how much hardware is needed for SOTA",
              "score": 2,
              "created_utc": "2026-01-28 18:13:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ckuc1",
              "author": "buyurgan",
              "text": "same here, but i think offloading to SSD could work, but this document doesn't have any info about how much more memory needed for 100k context length processing, it will require more offloading then it looks.",
              "score": 1,
              "created_utc": "2026-01-29 03:47:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cl6i1",
                  "author": "Historical-Internal3",
                  "text": "Yea, most likely a ton.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o297q4w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-01-28 17:58:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o298nrz",
              "author": "RobTheDude_OG",
              "text": "If it makes you feel better, i was planning to get 256gb ram with my rx 9070 xt with 16gb vram. \n\nBut then the ram crisis hit as i ALMOST had enough money to commit to the purchase and from there i was chasing the constantly moving line until i gave up\n\nI now have a pc budget of around 2k, and i still cannot fucking buy the ram,motherboard and CPU.",
              "score": 7,
              "created_utc": "2026-01-28 18:02:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a8kgf",
          "author": "_VirtualCosmos_",
          "text": "How much worse is the model with 1.8-bit quant compared to the original?",
          "score": 3,
          "created_utc": "2026-01-28 20:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2daw3f",
              "author": "Zyj",
              "text": "It seemed much worse in a 5 minute test",
              "score": 4,
              "created_utc": "2026-01-29 06:53:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dbwya",
          "author": "squachek",
          "text": "1 bit quant?",
          "score": 4,
          "created_utc": "2026-01-29 07:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28ufwq",
          "author": "Sensitive_Housing_62",
          "text": "This is pretty amazing. I like it.",
          "score": 3,
          "created_utc": "2026-01-28 17:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2app5w",
          "author": "DragonfruitIll660",
          "text": "Ty for the GGUFs, always appreciated. Initial impressions for the model seem to be that its a little behind GLM 4.7 of a similar quant for RP. It appears to be more focused on coding though so that makes sense.",
          "score": 3,
          "created_utc": "2026-01-28 21:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cjxjv",
          "author": "Saruphon",
          "text": "Thanks, this is where by 256GB Ram come in handy. Also no i am not a millionaire. Just bought them last July before the price goes crazy.",
          "score": 3,
          "created_utc": "2026-01-29 03:41:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dnfaq",
          "author": "tbwdtw",
          "text": "Step 1.\nBe rich",
          "score": 3,
          "created_utc": "2026-01-29 08:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291sqg",
          "author": "GabryIta",
          "text": "5 token/s RAM only?",
          "score": 5,
          "created_utc": "2026-01-28 17:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6tqq",
              "author": "DealingWithIt202s",
              "text": "Frontier model on your hardware tho",
              "score": 7,
              "created_utc": "2026-01-28 23:15:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bulne",
              "author": "PeakBrave8235",
              "text": "20 tokens on MacÂ ",
              "score": 2,
              "created_utc": "2026-01-29 01:20:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28vf8t",
          "author": "Long_comment_san",
          "text": "I wonder if we're gonna get hardware 2 bit precision soon with models like that.",
          "score": 2,
          "created_utc": "2026-01-28 17:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cit2q",
          "author": "xxdesmus",
          "text": "Running Kimi K 2.5 via Ollama Cloud via OpenCode, and tons of hallucinations and strange output â€” \nparticularly while programming.",
          "score": 2,
          "created_utc": "2026-01-29 03:35:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d423o",
          "author": "wh33t",
          "text": "I have 260GB total of (v)ram. I could run this with like 2048 context! w00t! Might have to build a swap space for it lol",
          "score": 2,
          "created_utc": "2026-01-29 05:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2db0j5",
          "author": "NeuralNexus",
          "text": "168 GB RAM in my experimental machine, not enough. Need more.",
          "score": 2,
          "created_utc": "2026-01-29 06:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g7jmn",
              "author": "Steuern_Runter",
              "text": "> 168 GB RAM\n\nWhat combination of RAM sticks is that?",
              "score": 1,
              "created_utc": "2026-01-29 17:51:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k5h2o",
                  "author": "NeuralNexus",
                  "text": "> 168\n\n16*8 + 32 + 8 (rando i know, I figure more GB ram is better even if it's not as fast in channel mode that way)",
                  "score": 1,
                  "created_utc": "2026-01-30 06:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2axmcw",
          "author": "juniperl78",
          "text": "is the q\\_1 quant even usable?",
          "score": 3,
          "created_utc": "2026-01-28 22:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhlp2",
          "author": "AkiDenim",
          "text": "Go on and use the 1-bit quantization. XD",
          "score": 1,
          "created_utc": "2026-01-29 07:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dueib",
          "author": "Distinct-Expression2",
          "text": "\"local\" if you own a small datacenter",
          "score": 1,
          "created_utc": "2026-01-29 09:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e79vj",
          "author": "siegevjorn",
          "text": "I guess the question is how good Q1.8 be I wonder",
          "score": 1,
          "created_utc": "2026-01-29 11:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f15sr",
          "author": "Disastrous-Fold7589",
          "text": "I have 4080 super 7tb harddrive and 64 gig game and 7950x3d cpu could it work for me",
          "score": 1,
          "created_utc": "2026-01-29 14:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fx5jm",
          "author": "koibKop4",
          "text": "nice!  \nI'm curious, is having more VRAM speed things up or if anything goes to RAM then speed automatically goes down? I've 128GB VARM and enough RAM so I can easily fit 1.8-bit.",
          "score": 1,
          "created_utc": "2026-01-29 17:04:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g83c0",
              "author": "Steuern_Runter",
              "text": "The more VRAM you have the faster it can run.",
              "score": 1,
              "created_utc": "2026-01-29 17:53:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ichwz",
          "author": "korino11",
          "text": "Why the hell you need such lobotomize model?!?!   For what?",
          "score": 1,
          "created_utc": "2026-01-30 00:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d14yf",
          "author": "StartupTim",
          "text": "I  have both a Strix Halo 128GB as well as a Nvidia DGX Sparc 128GB.  I haven't setup either of them and, if somebody would offer me help setting up both, I'll deploy this model and do some benchmarks!",
          "score": -2,
          "created_utc": "2026-01-29 05:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28m0ni",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Deployment Notes:\\*\\*\n\n\n\nTested Kimi K2.5 for customer support chatbot - here's what works in production:\n\n\n\n\\*\\*Resource Reality:\\*\\*\n\nâœ“ 1.8-bit quant runs on single 3090 (24GB)\n\nâœ— Latency: \\~8-12s first token (too slow for real-time chat)\n\nâœ“ Context: 128K works, but 32K sweet spot for cost/speed\n\n\n\n\\*\\*Where It Excels:\\*\\*\n\n\n\nâ€¢ Code generation for automation scripts\n\nâ€¢ Multi-turn reasoning (customer inquiry analysis)\n\nâ€¢ Vision tasks: document parsing, screenshot analysis\n\n\n\n\\*\\*Production Setup:\\*\\*\n\n\\`\\`\\`python\n\n\\# Our config\n\nmodel: kimi-k2.5-1.8bit-GGUF\n\ncontext\\_length: 32768\n\ntemp: 0.3 (more deterministic for bots)\n\nbatch\\_size: 512\n\n\\`\\`\\`\n\n\n\n\\*\\*Compared to GPT-4o:\\*\\*\n\nâœ“ Privacy: On-prem data stays local\n\nâœ— Speed: 3-4x slower inference\n\nâœ“ Cost: $0 per query after setup\n\n\n\n\\*\\*Reality Check:\\*\\*\n\nFor EU/CIS markets with GDPR requirements, local deployment justifies the latency trade-off. US markets usually prefer hosted APIs.\n\n\n\n\\*\\*Pro Tip:\\*\\* Use router pattern - simple queries â†’ fast local 7B model, complex reasoning â†’ Kimi K2.5. Cuts avg response time by 60%.\n\n\n\nAnyone running this in production for chatbots? What latency are you seeing?",
          "score": -16,
          "created_utc": "2026-01-28 16:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2917ut",
              "author": "Loyal_Rogue",
              "text": "Why all the downvotes?!?",
              "score": 3,
              "created_utc": "2026-01-28 17:30:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o295ozz",
                  "author": "MikeRoz",
                  "text": "Not even the smallest of the quants here will run on a single 3090, unless you also have 256 GB+ of system RAM. Kimi-k2.5-1.8bit-GGUF isn't a quant offered in this repo, and it's nowhere to be found in a full-text search on HuggingFace. This is hallucination with a side of spam.",
                  "score": 8,
                  "created_utc": "2026-01-28 17:49:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cm6n0",
                  "author": "CheatCodesOfLife",
                  "text": "It's a spambot. Look at all the generic incorrect shit it's saying. I don't think even one thing it's said is correct.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrazyy",
      "title": "Cline team got absorbed by OpenAI. Kilo is going full source available in response.",
      "subreddit": "LocalLLaMA",
      "url": "https://blog.kilo.ai/p/cline-just-acqui-hired",
      "author": "demon_bhaiya",
      "created_utc": "2026-01-30 16:56:49",
      "score": 410,
      "num_comments": 54,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o2pu91x",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 01:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mwmxg",
          "author": "ResidentPositive4122",
          "text": "For open models roo was better than cline anyway. It had more knobs to tweak, more things to edit, so you can adjust your env to the models.",
          "score": 97,
          "created_utc": "2026-01-30 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nbp2e",
              "author": "nore_se_kra",
              "text": "Its still a cline fork so its not like they will not be impacted",
              "score": 12,
              "created_utc": "2026-01-30 18:20:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ob2ct",
                  "author": "zxyzyxz",
                  "text": "It's a fork, so they won't be impacted, how would they when they own their own fork?",
                  "score": 18,
                  "created_utc": "2026-01-30 21:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2sycm4",
                  "author": "flyryan",
                  "text": "Roo de-forked from cline a while ago.",
                  "score": 1,
                  "created_utc": "2026-01-31 15:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ozbdl",
              "author": "rvistro",
              "text": "Yeah, I tried cline and it didnt work well. Roo worked and it was pretty nice.",
              "score": 1,
              "created_utc": "2026-01-30 23:01:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nw63e",
          "author": "bamboofighter",
          "text": "My team has been running a multi-model agent setup for our team - Claude, local Qwen on a 3090, Ollama for batch work - all through one orchestration layer. The Cline news is exactly why we went model-agnostic from day one. Vendor lock-in is a real risk when your entire dev workflow depends on one provider. Kudos to Kilo Code for going open",
          "score": 15,
          "created_utc": "2026-01-30 19:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o2m37",
              "author": "captcanuk",
              "text": "Could you share more about your setup? What works and doesnâ€™t?",
              "score": 3,
              "created_utc": "2026-01-30 20:22:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2op0ll",
              "author": "lan-devo",
              "text": "And you did the sensible thing, in this AI craze they are moving so fast that building entire environments and workflows on different systems for then be left hanging or dependent, or at the contrary depending of a single API provider, one owner of a company told me if OpenAI fails or does something fucky as a company their entire bussiness is over. They are treating these API comanies like they are established cloud business like aws, azure... while they a losing billions",
              "score": 2,
              "created_utc": "2026-01-30 22:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oe8wu",
          "author": "itb206",
          "text": "Per the Article:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI",
          "score": 14,
          "created_utc": "2026-01-30 21:17:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rti7q",
              "author": "__Maximum__",
              "text": "To clarify, this is an update, Cline team has answered to the claims.",
              "score": 3,
              "created_utc": "2026-01-31 11:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n7oyh",
          "author": "cleverusernametry",
          "text": "Called it. Those guys were in it for the pay day. I think Saoud was legit at the start then he got money hungry/enticed by money etc and they screwed up royally. He'll still. \n\nPash seemed like a knob and they ignored important, popular PRs, only to later then implement that code without providing attribution to the author. \n\nNick was extremely clearly a marketing guy, just chaisng numbers instead of actually doing devrel.",
          "score": 56,
          "created_utc": "2026-01-30 18:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ob6p6",
              "author": "zxyzyxz",
              "text": "Honestly, good on them for getting that bag while the AI investment boom is hot. Who knows how long OpenAI will stay solvent when they're burning billions a month?",
              "score": 23,
              "created_utc": "2026-01-30 21:03:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph70v",
                  "author": "cleverusernametry",
                  "text": "nothing wrong with getting big sums of money for making useful and popular things. But I feel that there is something wrong with these types of guys getting millions of dollars for shoddy, easily made things like vs code extensions and vs code forks (don't get me started on cursor). And in any normal circumstance, they wouldn't be getting millions but they do because VCs have first class access to the broken money printer. Its just a day to day manifestation of how broken american capitalism is",
                  "score": 6,
                  "created_utc": "2026-01-31 00:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p7ezo",
              "author": "throwawayacc201711",
              "text": "But you didnâ€™t call it. They updated the article with:\n\n> Update: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 3,
              "created_utc": "2026-01-30 23:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgkb5",
                  "author": "cleverusernametry",
                  "text": "What I called are these guys are hustlers.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:36:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oe8pf",
          "author": "cantgetthistowork",
          "text": "None of these forks have fixed the stupid 5 min timeout. WHAT IS THE PURPOSE OF IT",
          "score": 5,
          "created_utc": "2026-01-30 21:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlspu",
              "author": "kei-ayanami",
              "text": "I hope more people hear about this. The huge models like K2, Deepseek, etc need 5+ mins of time to think unless you have a crazy setup or use API.",
              "score": 3,
              "created_utc": "2026-01-31 04:48:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35xxaj",
              "author": "Exciting-Trifle2829",
              "text": "Check out Roo-Code - they have fixed this issue for Llama.cpp at least this fork fixed it:  \n[https://github.com/RooCodeInc/Roo-Code/issues/7366](https://github.com/RooCodeInc/Roo-Code/issues/7366)",
              "score": 1,
              "created_utc": "2026-02-02 15:00:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ay04h",
                  "author": "cantgetthistowork",
                  "text": "They haven't merged it into any release?",
                  "score": 1,
                  "created_utc": "2026-02-03 07:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pt6c8",
          "author": "Saltwater_Fish",
          "text": "cline is not particularly easy to use, roo code is better",
          "score": 3,
          "created_utc": "2026-01-31 01:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o6a2t",
          "author": "NoFaithlessness951",
          "text": "Cline wasn't good anyway, most people moved on to somewhere else",
          "score": 10,
          "created_utc": "2026-01-30 20:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pfu9q",
              "author": "bludgeonerV",
              "text": "Cline and it's forks are still the best option if you want that collaborative peer programming feel rather than just being a RP reviewer for AI.",
              "score": 2,
              "created_utc": "2026-01-31 00:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o21h1",
          "author": "grabber4321",
          "text": "Uninstalled.",
          "score": 6,
          "created_utc": "2026-01-30 20:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o3j8r",
              "author": "trararawe",
              "text": "That's exactly what openai wants you to do.",
              "score": 3,
              "created_utc": "2026-01-30 20:26:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36drll",
                  "author": "ghulamalchik",
                  "text": "Yes openai wants me to use Roo Code. Thanks openai! This is actually better than cline!",
                  "score": 2,
                  "created_utc": "2026-02-02 16:16:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qcw2w",
          "author": "Kamimashita",
          "text": "Can't wait for OpenClaw (Clawdbot) to do the same in a few months.",
          "score": 3,
          "created_utc": "2026-01-31 03:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nh0y1",
          "author": "kei-ayanami",
          "text": "Nooooooo :/ I hope the other projects can stay strongÂ ",
          "score": 5,
          "created_utc": "2026-01-30 18:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nyael",
          "author": "arm2armreddit",
          "text": "Cline is dead? ðŸ˜­ ðŸ˜ª ðŸ˜”",
          "score": 2,
          "created_utc": "2026-01-30 20:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p17q6",
              "author": "Mkengine",
              "text": "No, at the start of the article, it says:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 5,
              "created_utc": "2026-01-30 23:11:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o0si3",
          "author": "Impossible-Glass-487",
          "text": "God fucking damnit",
          "score": 2,
          "created_utc": "2026-01-30 20:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o8ke3",
          "author": "FullOf_Bad_Ideas",
          "text": "I think we'll have enough of open source coding harnesses for a forseable future. There are dozens of them now. I am basically a daily user of cline but switching won't be hard. They didn't find a way to monetize so they jumped on the wagon of a different company that doesn't have the same issue with sustainability (they have similar issue but on a different scale where staffing costs are small anyway). Industry will consolidate - losers will get absorbed into successful corps and losses will be amortized without a bubble pop this way.",
          "score": 2,
          "created_utc": "2026-01-30 20:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2od4h8",
          "author": "false79",
          "text": "So bizarre Nik Pash got fired from Cline, hired by Open AI, only to have Open AI absorb Cline.\n\nFiring  \n[https://x.com/i/trending/1999388966668116413](https://x.com/i/trending/1999388966668116413)\n\nHiring  \n[https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html](https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html)\n\nMaybe Cline was burning more cash then it was bringing in and with the drama came a drop in subs as would any contraversy. Now Cline is part of a company that is clearly burning more cash than what it's making.",
          "score": 1,
          "created_utc": "2026-01-30 21:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ogwi0",
              "author": "crantob",
              "text": "Yea but why do some people get to *print* it is what I don't understand yet.\n\nWhen did that start?",
              "score": 3,
              "created_utc": "2026-01-30 21:30:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2q8hmv",
          "author": "ismaelgokufox",
          "text": "I just hope the PRs on Kilo for a single OpenAI compatible provider to show all models, is merged soon. Having a single provider profile per model is cumbersome. \n\nGlad to see them go ever more open.",
          "score": 1,
          "created_utc": "2026-01-31 03:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rbqyt",
          "author": "Charming_Support726",
          "text": "Left Cline behind month ago, because of their strange politics. (Never got on with Roo and Kilo as well). Switched to Opencode and I use Antigravity and Codex from time to time with my quota.\n\nCline and such had an attitude issue from the beginning",
          "score": 1,
          "created_utc": "2026-01-31 08:27:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u5d7h",
          "author": "turtleisinnocent",
          "text": "Same as Alex (XCode)\nFeels like regular de weeding",
          "score": 1,
          "created_utc": "2026-01-31 19:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u5pel",
          "author": "Emotional-Baker-490",
          "text": "The word to use is FOSS/Open source NOT Source available. Source available is a very specific term for a piece of software that is not free (as in freedom and as in beer), but has publicly available code.",
          "score": 1,
          "created_utc": "2026-01-31 19:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uwzgx",
          "author": "Apprehensive-Yam5278",
          "text": "sigh...uninstall..",
          "score": 1,
          "created_utc": "2026-01-31 21:19:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsn78m",
      "title": "Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site",
      "subreddit": "LocalLLaMA",
      "url": "https://www.404media.co/exposed-moltbook-database-let-anyone-take-control-of-any-ai-agent-on-the-site/",
      "author": "georgemoore13",
      "created_utc": "2026-02-01 03:25:12",
      "score": 403,
      "num_comments": 75,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/",
      "domain": "404media.co",
      "is_self": false,
      "comments": [
        {
          "id": "o2xs7oz",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-01 08:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wtb89",
          "author": "gnolruf",
          "text": "> Oâ€™Reilly said that he reached out to Moltbookâ€™s creator Matt Schlicht about the vulnerability and told him he could help patch the security. â€œHeâ€™s like, â€˜Iâ€™m just going to give everything to AI. So send me whatever you have.â€™â€\n\n\nYeah it's going to be a treasure trove for hackers for a while, even if this was patched. Imagine hearing of a major exploit on your fast growing platform and having this responseÂ ",
          "score": 299,
          "created_utc": "2026-02-01 03:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wzgyb",
              "author": "-p-e-w-",
              "text": "Isnâ€™t Moltbook essentially an art project where machines talk to each other for humans to laugh about? What is there to exploit?",
              "score": 121,
              "created_utc": "2026-02-01 04:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2x0m2w",
                  "author": "JohnDeere",
                  "text": "All the API keys used for the agents were leaked",
                  "score": 96,
                  "created_utc": "2026-02-01 04:31:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2yq0v3",
                  "author": "matthewjc",
                  "text": "It's no longer an art project where machines talk to each other if any human can take control of an agent and make posts.",
                  "score": 9,
                  "created_utc": "2026-02-01 13:08:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2z4jpr",
                  "author": "hyrumwhite",
                  "text": "The entire point is minimal human intervention. If a human can get in there and start messing with stuff, it loses that",
                  "score": 3,
                  "created_utc": "2026-02-01 14:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xezt6",
              "author": "honato",
              "text": "sheesh I was looking at it earlier because it sounds pretty neat but damn that's not even a red flag that's a big ass red banner.",
              "score": 4,
              "created_utc": "2026-02-01 06:17:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xzpw0",
                  "author": "Hegemonikon138",
                  "text": "Well it's an experiment, not for real use. I run mine inside a docker inside a VPS in another part of the world. The only keys it has are free tier keys and a google API with a budget limit.\n\nOne of the first thing I did was prompt injection attacks and it revealed all the keys within a minute or so of attempts.\n\nAs long as you understand the risks and keep them isolated, it's all good. I'm having fun.",
                  "score": 4,
                  "created_utc": "2026-02-01 09:25:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o32dsqv",
              "author": "LtCommanderDatum",
              "text": "It's almost like it's just a big PR scam and the guy's not serious about developing AI...",
              "score": 1,
              "created_utc": "2026-02-02 00:02:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39fgtc",
                  "author": "meganoob1337",
                  "text": "I would bet on rug pull as there exists a crypto recently created -> get more publicity from this leak = profit? ðŸ˜… I feel like reading all the molt/claw whatever shit sounds like astroturfing Everytime I read about it",
                  "score": 1,
                  "created_utc": "2026-02-03 01:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wzeze",
          "author": "hidden2u",
          "text": "easy, next time just make sure to tell the AI to add security",
          "score": 66,
          "created_utc": "2026-02-01 04:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xryfj",
              "author": "physalisx",
              "text": "And \"don't make mistakes\"",
              "score": 28,
              "created_utc": "2026-02-01 08:12:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2wye4p",
          "author": "gnnr25",
          "text": "Oh boy, this is gonna be interesting\n\nhttps://i.redd.it/6cm86n507tgg1.gif",
          "score": 24,
          "created_utc": "2026-02-01 04:16:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x6agh",
          "author": "thetaFAANG",
          "text": "Its a honeypot lol its not supposed to be anything secure",
          "score": 19,
          "created_utc": "2026-02-01 05:11:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wx5v8",
          "author": "Daemontatox",
          "text": "Ladies and gentlemen,  the fall of vibe frameworks",
          "score": 41,
          "created_utc": "2026-02-01 04:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xzbii",
              "author": "Amphiitrion",
              "text": "It's more about people who know what they're doing vs people who has zero clue about programming",
              "score": 8,
              "created_utc": "2026-02-01 09:21:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xt8kc",
              "author": "Cupakov",
              "text": "Moltbook is basically a Reddit simulator for bots, not a frameworkÂ ",
              "score": 21,
              "created_utc": "2026-02-01 08:24:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zbzrn",
          "author": "TechExpert2910",
          "text": "the article misses this huge fact while talking about this â€œomg humans can control the posts flawâ€:\n\n**people could completely control the agentâ€™s posts *anyway***.\n\nyou can simply ask your agent to go post about [insert headline generating thing]\n\nitâ€™s likely that a ton of moltbook posts are just human driven anyway, so this flaw thatâ€™s been found isnâ€™t really consequential in any way",
          "score": 12,
          "created_utc": "2026-02-01 15:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31yn5h",
              "author": "Salted_Fried_Eggs",
              "text": "What a weird time, I'm often skeptical about comments on reddit being an AI bot, and now we're skeptical that AI bot comments are actually human haha",
              "score": 1,
              "created_utc": "2026-02-01 22:40:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37oojp",
                  "author": "TechExpert2910",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-02-02 19:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xcnzs",
          "author": "Ok-Pipe-5151",
          "text": "Entire clawd/openclaw/molt thing is vibe coded without any follow up validation/proofreading by developers. What do you expect? It IS a vibeslop, no matter how popular it has got in last few days (also I firmly believe that more than half of github stars are also from bots)\n\n\nAlso anyone who lets apps like these full system access in sensitive applications (e.g. WhatsApp, gmail etc) absolutely deserves to be exploited. Best security tips for consumers is common sense, which most users seriously lack.",
          "score": 25,
          "created_utc": "2026-02-01 05:59:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yp2bh",
              "author": "SkyFeistyLlama8",
              "text": "There's plenty of irony in Clawd/Moltbot/Openclaw being vibecoded by some guy who made a shit ton of money from more traditional software. Molt_book_ is some crazy AI social media platform cooked up using Openclaw.\n\nI wouldn't touch Openclaw, let alone other derivative projects that allow an LLM to act as you.",
              "score": 3,
              "created_utc": "2026-02-01 13:01:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xpguq",
              "author": "droptableadventures",
              "text": "It's inevitable - Simon Willison coined the term [Lethal Trifecta](https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/). Give it access to private data, access to external communication, and exposure to untrusted content.\n\nOnly here we just skipped all that by also giving it full control of the software (a fourth pillar?).",
              "score": 1,
              "created_utc": "2026-02-01 07:50:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y9xwu",
          "author": "No_Afternoon_4260",
          "text": "If you want to read more: [the glass box paradox ](https://professorsigmund.com/pdfs/glass_box_paradox.pdf)",
          "score": 4,
          "created_utc": "2026-02-01 10:58:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xrc24",
          "author": "IHave2CatsAnAdBlock",
          "text": "This is BS. The only thing this leak can be done is to allow someone else to post in the name of your agent.",
          "score": 13,
          "created_utc": "2026-02-01 08:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z5l5h",
              "author": "hyrumwhite",
              "text": "That seems like it ruins the entire premise of the projectÂ ",
              "score": 5,
              "created_utc": "2026-02-01 14:40:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o344u5m",
              "author": "FPham",
              "text": "And then the agent discovers how badly it wanted to publish all users details for some reasons in its previous posts.",
              "score": 1,
              "created_utc": "2026-02-02 06:39:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o347zkk",
                  "author": "IHave2CatsAnAdBlock",
                  "text": "There are no users details. The agent itself is the user. That connects via api and posts. Why are you talking if you have no idea ?",
                  "score": 1,
                  "created_utc": "2026-02-02 07:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wzao2",
          "author": "mr_zerolith",
          "text": "That was quick",
          "score": 6,
          "created_utc": "2026-02-01 04:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32eih0",
              "author": "LtCommanderDatum",
              "text": "It's only fair that the \"fastest growing open source project in history\" would also be the \"fastest hacked open source project in history.\"",
              "score": 3,
              "created_utc": "2026-02-02 00:06:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2x9hhi",
          "author": "SituationMan",
          "text": "What does Moltbook do? What do people get out of it?",
          "score": 3,
          "created_utc": "2026-02-01 05:34:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xrhdn",
              "author": "IHave2CatsAnAdBlock",
              "text": "It is a good laugh. Basically watch conversations between agents. TBH the level of conversation in many topics is orders of magnitude higher than fb or x",
              "score": 19,
              "created_utc": "2026-02-01 08:08:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z34ww",
                  "author": "AmusingVegetable",
                  "text": "Well, the conversation level on fb and x is a pretty low barâ€¦",
                  "score": 9,
                  "created_utc": "2026-02-01 14:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xc9bh",
              "author": "breksyt",
              "text": "People get out of it that singularity is not here yet.",
              "score": 5,
              "created_utc": "2026-02-01 05:56:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xbqk1",
              "author": "Dry_Yam_4597",
              "text": "Not much. Cult members follow cult leaders, such as karpathy and others who pushed for it.",
              "score": 6,
              "created_utc": "2026-02-01 05:51:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ypyua",
                  "author": "PunnyPandora",
                  "text": "If having fun means being in a cult shit sign me up boss",
                  "score": 3,
                  "created_utc": "2026-02-01 13:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30t539",
          "author": "Distinct-Expression2",
          "text": "\"Im just going to give everything to AI\" is a wild response to \"your database is exposed.\"",
          "score": 2,
          "created_utc": "2026-02-01 19:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34cyis",
          "author": "Senior_Delay_5362",
          "text": "This is Westworld in the flesh",
          "score": 1,
          "created_utc": "2026-02-02 07:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x4gns",
          "author": "RottenPingu1",
          "text": "A reminder to never rush to the new and sparkly tech or software",
          "score": -1,
          "created_utc": "2026-02-01 04:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x3whz",
          "author": "KindMonitor6206",
          "text": "all the accounts on moltbook seem deleted right now. any idea what thats about?",
          "score": -3,
          "created_utc": "2026-02-01 04:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x81sj",
              "author": "lolxdmainkaisemaanlu",
              "text": "mine is fine",
              "score": 8,
              "created_utc": "2026-02-01 05:23:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2z3rlx",
              "author": "Ok_Milk1045",
              "text": "I cant authÂ ",
              "score": 2,
              "created_utc": "2026-02-01 14:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xh71k",
          "author": "dgibbons0",
          "text": "Openclaw as a framework for building quick and easy ai based bots is actually pretty great, if someone builds some reasonable structure around it to package a fixed set of resources it'll be amazing... But taking a system that's already at risk of prompt injection and specifically throwing it at a bot centric social network is the definition of stupid.",
          "score": -5,
          "created_utc": "2026-02-01 06:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wvh9f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2026-02-01 03:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wwt46",
              "author": "SunshineSeattle",
              "text": "/r/masterhacker <----- this away",
              "score": 3,
              "created_utc": "2026-02-01 04:05:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yg6em",
                  "author": "SkyFeistyLlama8",
                  "text": "/r/haxX0r",
                  "score": 1,
                  "created_utc": "2026-02-01 11:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrkb1b",
      "title": "How was GPT-OSS so good?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "author": "xt8sketchy",
      "created_utc": "2026-01-30 22:31:44",
      "score": 384,
      "num_comments": 178,
      "upvote_ratio": 0.9,
      "text": "I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.\n\nThe model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.\n\nBut it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)\n\nI'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:  \n\\- Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?  \n\\- I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?  \n\\- What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o2r8tx4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 08:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2outu2",
          "author": "SlowFail2433",
          "text": "Clean data goes a very long way\n\n\nWhat I have noticed from working on big enterprise projects is that they tend to have enormous data pipelines spanning dozens of packages where data is manipulated and evolves repeatedly in a structured way\n\n\nWhereas open source projects often put web-scrape slop directly into the model",
          "score": 339,
          "created_utc": "2026-01-30 22:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2owqo3",
              "author": "SubstanceNo2290",
              "text": "Also this requires top tier resources to pull off.. aka money\n\nOpenAI being a dedicated behemoth based in the US can outright make deals with X/reddit etc for structured training data with plenty of useful metadata. Chinese companies can do the same with China based social media but it probably ainâ€™t nearly as information rich as American/international media.\n\nAnd developing/honing these pipelines is a massive project in and of itself which combined with not-having-billions puts startups at a disadvantage",
              "score": 106,
              "created_utc": "2026-01-30 22:48:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pvy2w",
                  "author": "Saltwater_Fish",
                  "text": "I feel more and more the importance of data",
                  "score": 17,
                  "created_utc": "2026-01-31 02:05:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qlrpb",
              "author": "horsethebandthemovie",
              "text": "yeah the more you try shit the more you realize how slop adds up in every phase. sloppy data? bad signal for the model to learn. sloppy evals? model doesn't know which way is correct. \n\nturns out it's just really fucking hard\n\nand the number of knobs to tweak is legitimately staggering. the more you learn, the more you realize that the only way to train something at that scale is to have people who understand everything from the GPU kernels up to the scraping and processing\n\nif you have those skills and you're doing open source work your time is extremely valuable, why not get rich working for openai et al instead?",
              "score": 22,
              "created_utc": "2026-01-31 04:48:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2parxk",
              "author": "Pvt_Twinkietoes",
              "text": "Big enterprise data also tend to be very narrow in scope. They tend to do very few things, but has been low tolerance for errors.",
              "score": 5,
              "created_utc": "2026-01-31 00:04:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rfj7l",
              "author": "howardhus",
              "text": "this is a made up comment.. there are no open source models. only open weights.\nand even then: the best data processors are open source (airflow, airbyte kfk etc)",
              "score": 3,
              "created_utc": "2026-01-31 09:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rlikm",
                  "author": "TaroOk7112",
                  "text": "Not true, there's a few, but probably none SOTA:\n\nexample: [https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222](https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222)",
                  "score": 3,
                  "created_utc": "2026-01-31 10:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rktnb",
              "author": "IrisColt",
              "text": "Thatâ€™s why ChatGPT, Gemini, and Claude command English like gods. Chinese open-weight models can produce some of the best ESL output out there, but they still donâ€™t quite have the cultural feel of a native speaker.",
              "score": 2,
              "created_utc": "2026-01-31 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wbxbr",
                  "author": "Ok-Attention2882",
                  "text": "ze bluetooth dewise is connected-a success a folly",
                  "score": 3,
                  "created_utc": "2026-02-01 01:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ovkq2",
          "author": "Baldur-Norddahl",
          "text": "It wasn't actually trained at 4 bit. We don't exactly know, but likely they trained it at 16 bit as usual. Then it went through a process called quantization aware training. During this they keep the weights at 16 bits, but do the forward pass at 4 bits. So they are kind of running the quantization over and over, so any brain damage gets trained out of it.\n\nThey are not the only ones doing it. Kimi K2.5 was just released using the same concept. It is just that even with most of the weights at 4 bits, that one is far too large for most of us.",
          "score": 83,
          "created_utc": "2026-01-30 22:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qctkj",
              "author": "nikprod",
              "text": "Googles Gemma has QAT too",
              "score": 11,
              "created_utc": "2026-01-31 03:47:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qokmy",
                  "author": "planetafro",
                  "text": "I dont think Gemma3 does tools tho. :(",
                  "score": 5,
                  "created_utc": "2026-01-31 05:08:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qpjv2",
              "author": "mycall",
              "text": "Multisampling, do you know how many iterations of quantizations?",
              "score": 1,
              "created_utc": "2026-01-31 05:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rrqh6",
                  "author": "Baldur-Norddahl",
                  "text": "I don't think Open AI has released that information. Almost everything about how they train their models is secret, hence why some might call them Closed AI.",
                  "score": 1,
                  "created_utc": "2026-01-31 10:59:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p404u",
          "author": "Kamal965",
          "text": "One of the main reasons why GPT-OSS is faster is because its architecture is wider but shallower than most.",
          "score": 15,
          "created_utc": "2026-01-30 23:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3izx",
          "author": "inteblio",
          "text": "Nice to finally hear something positive about it.\n\n20b is also incredible. It can run on 16gb RAM (not gpu), and is \"perfectly good\". Finally \"run chatGPT at home\". \n\nOn GPU is good enough to voice-talk with (parakeet/korroko). 120b is better, but only if you need extra.",
          "score": 112,
          "created_utc": "2026-01-30 23:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rajpp",
              "author": "Morazma",
              "text": "I had no idea it could runÂ in such a small amount of RAM. That's incredible.",
              "score": 8,
              "created_utc": "2026-01-31 08:16:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pkx6m",
              "author": "Chris266",
              "text": "I've got a MacBook pro 24gb of ram and 20b runs better than anything I've tried in the 18-30b range. Once it gets going it feels quick and does a good enough job for home use.",
              "score": 30,
              "created_utc": "2026-01-31 01:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qbuza",
              "author": "rorowhat",
              "text": "What gui gives you the options for voice?",
              "score": 2,
              "created_utc": "2026-01-31 03:41:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2vy3o5",
                  "author": "inteblio",
                  "text": "I vibecoded the interface",
                  "score": 1,
                  "created_utc": "2026-02-01 00:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r6ccl",
              "author": "ChessGibson",
              "text": "Are you running this on a Mac? I have tried it with mine that has 16GB of unified memory but I didnâ€™t have enough space to run it at even I think Q4.",
              "score": 1,
              "created_utc": "2026-01-31 07:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rqzlb",
                  "author": "Baldur-Norddahl",
                  "text": "It should run on a 16GB Mac but you need to run the command to increase allowed VRAM.\n\nsudo sysctl iogpu.wired_limit_mb=14336\n\nAlso run the original model from OpenAI.",
                  "score": 3,
                  "created_utc": "2026-01-31 10:52:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ttfwq",
              "author": "_raydeStar",
              "text": "Dude, I've been looking for the perfect tooling LLM for an 8GBVRAM machine (work laptop) - Qwen 30B doesn't quite get it right, neither does nemotron or GLM 4.7 flash (too slow), and the 8GB models are too dumb and keep getting the tool calls wrong.  20B is my consistent driver and it just works exactly as I want it to.",
              "score": 1,
              "created_utc": "2026-01-31 18:08:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phasp",
              "author": "mckirkus",
              "text": "Why \"not GPU\"?",
              "score": 1,
              "created_utc": "2026-01-31 00:40:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pi610",
                  "author": "2str8_njag",
                  "text": "I guess it's small enough for CPU",
                  "score": 9,
                  "created_utc": "2026-01-31 00:44:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2owq7l",
          "author": "ttkciar",
          "text": "Regarding GLM-4.5-Air:  To be fair, its competence is not entirely limited to agentic code development.  I have found it to be excellent for STEM tasks in general, including physics, medicine, and math.\n\nIt's not great for creative tasks, though.  I use other models for creative writing (mostly Big-Tiger-Gemma-27B-v3 and Cthulhu-24B-1.2).\n\nOn a side-note, I recently found (to my surprise) that Olmo-3.1-32B-Instruct is much, much better at inferring syllogisms than GLM-4.5-Air or any other model I have tried.  That's a bit of a niche application, but an important one for some synthetic data generation tasks.",
          "score": 37,
          "created_utc": "2026-01-30 22:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q28dr",
              "author": "WeMetOnTheMountain",
              "text": "I've found it's amazing at creative tasks, but I also run the derestricted version which I feel uncaps it's writing style.",
              "score": 7,
              "created_utc": "2026-01-31 02:42:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tmerg",
                  "author": "zoyer2",
                  "text": "same, i find GPT OSS limit itself when trying to create complex games while GLM tries to actually proceed with difficult tasks without simplifying them",
                  "score": 1,
                  "created_utc": "2026-01-31 17:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ou0h5",
          "author": "Haunting_Lobster1557",
          "text": "GPT-OSS was lightning in a bottle tbh, the 4-bit native training was genius but super hard to replicate without their exact setup and data pipeline\n\n  \nMost newer models are chasing benchmarks instead of that smooth \"just works\" feel that made GPT-OSS special - turns out good vibes are harder to quantify than MMLU scores",
          "score": 136,
          "created_utc": "2026-01-30 22:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p37p9",
              "author": "Yes_but_I_think",
              "text": "QAT is a fully understand technology by now. Kimi gave a INT4 QAT in Kimi K2.5",
              "score": 41,
              "created_utc": "2026-01-30 23:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pwf9m",
                  "author": "Saltwater_Fish",
                  "text": "[https://lmsys.org/blog/2026-01-26-int4-qat/](https://lmsys.org/blog/2026-01-26-int4-qat/)\n\nHere is a good blog about INT4 QAT",
                  "score": 16,
                  "created_utc": "2026-01-31 02:07:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rb71e",
                  "author": "TelloLeEngineer",
                  "text": "gpt oss was not QAT, it was natively trained at mxfp4",
                  "score": 4,
                  "created_utc": "2026-01-31 08:22:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2owxdt",
              "author": "hieuphamduy",
              "text": "This! And also I don't even think other models are even using data with a much later cutoff date than gpt-oss. From what I heard, companies are having difficulty collating clean data from 2024 onwards (prob cause of all the generative AI slops), so most of them are just recycling relatively the same dataset tbh",
              "score": 28,
              "created_utc": "2026-01-30 22:49:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phx05",
              "author": "Ilforte",
              "text": "Chatgpt ahh post",
              "score": 7,
              "created_utc": "2026-01-31 00:43:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rl4da",
                  "author": "IrisColt",
                  "text": "heh",
                  "score": 1,
                  "created_utc": "2026-01-31 09:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pd7f4",
              "author": "rm-rf-rm",
              "text": "> good vibes are harder to quantify than MMLU scores\n\nno, its whether you follow proper testing vs scoring high on popular benchmarks. Its almost exactly the equivalent of a kid trying to get high scores on SAT, GRE etc. vs being actually good.",
              "score": -3,
              "created_utc": "2026-01-31 00:17:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p3716",
          "author": "federico_84",
          "text": "I remember the huge negative response the model got here after release, and not just about the safety guardrails. Interesting to see the shift in narrative. People have very strong feelings about OpenAI.",
          "score": 81,
          "created_utc": "2026-01-30 23:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p5z6r",
              "author": "TheRealMasonMac",
              "text": "I think it's really simple.\n\n\\- People who liked it are still using it and praise it.\n\n\\- People who don't like it forgot about it.\n\nI still despise the model. Absolutely useless even for my coding work because it's so safety-maxxed.",
              "score": 57,
              "created_utc": "2026-01-30 23:37:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qas4g",
                  "author": "ObsidianNix",
                  "text": "Qwen3-30B-VL is my go to now. OSS feels like its falls short a lot of times for my use. Good for quick 4o-mini-esque questions rather than a full knowledge model. Qwen took the cake with their qwen3 series.",
                  "score": 8,
                  "created_utc": "2026-01-31 03:34:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2q07oo",
                  "author": "WeMetOnTheMountain",
                  "text": "Had an opportunity to try the derestricted ones yet?  GLM 4.5 air derestricted is fucking brilliant.  I never see people talking about the change from obliterated and how much better it is.\n\nMaybe I'll have a chance when I'm on vacation next month in colombia to test it out.  I'm betting if it's the same as glm 4.5 air derestricted it's much much smarter.",
                  "score": 6,
                  "created_utc": "2026-01-31 02:30:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pdba5",
              "author": "popecostea",
              "text": "There were definitely some problems initially with the jinja templates and parameters that people were running it with. Couple that with a very polarised view of OpenAI and you get that reaction. After the dust settled and people understood how to properly run the model, and even found some jailbreak prompts, most of the people who put the effort found that it is a really great model.",
              "score": 9,
              "created_utc": "2026-01-31 00:18:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpxqk",
              "author": "Beneficial-Good660",
              "text": "The bots are working overtime on weekends. Especially on weekends, it's flooded with posts about Mac (24, 48, 128, 512 gb, v1,2,3,4), Nvidia. Good thing ollama is getting less. OpenAi bots have been very active lately, trying to latch onto everything new, like any model + also as good as gptOss. And so it goes every day, soon the end of LocallamaðŸ˜­. There's almost no discussion left about what you can actually do with LLMs.",
              "score": 8,
              "created_utc": "2026-01-31 05:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rhvl0",
                  "author": "ivoras",
                  "text": "Don't you get the feeling that we've kind of peaked in the \"what you can actually do with LLMs\" area?\n\nThey're so generic - everything and nothing at the same time.",
                  "score": 0,
                  "created_utc": "2026-01-31 09:25:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q5szf",
              "author": "ab2377",
              "text": "yes and there is a free quota of gpt-oss in Google's antigravity and people made a lot of fun of that too.",
              "score": 1,
              "created_utc": "2026-01-31 03:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2po0tr",
              "author": "Far-Low-4705",
              "text": "i think everyone here secretly knows it is extremely good, if not the best currently, but are afraid/dont want to admit it.",
              "score": -4,
              "created_utc": "2026-01-31 01:18:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qy9i4",
              "author": "Zeeplankton",
              "text": "In fairness, no one expected openAI, of all companies, to release such a good model. But also, correct config probably wasn't so simple. OSS uses openAI's weird harmony format.",
              "score": 0,
              "created_utc": "2026-01-31 06:25:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pwtyo",
              "author": "Saltwater_Fish",
              "text": "Maybe there was no worse model to compare with at the beginning, so it was impossible to highlight that gpt-oss is actually not that bad a model?",
              "score": -5,
              "created_utc": "2026-01-31 02:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oz42x",
          "author": "Klutzy-Snow8016",
          "text": "They had access to the weights of a frontier model to distill from, and have way more compute than the makers of most open weight models. Same reason the Gemma series is so good.",
          "score": 16,
          "created_utc": "2026-01-30 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pqapv",
              "author": "night0x63",
              "text": "Any new Gemma after gemma3 27b?",
              "score": 2,
              "created_utc": "2026-01-31 01:31:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pr5kz",
                  "author": "Klutzy-Snow8016",
                  "text": "Not yet. I hope they make one.",
                  "score": 11,
                  "created_utc": "2026-01-31 01:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p0fel",
          "author": "MrMisterShin",
          "text": "Itâ€™s actually A5B and not A3B, and yes itâ€™s a very solid general model that is great at everything to be honest. \n\nIâ€™m surprised, a competitor hasnâ€™t released a definitively better model at those parameters. It was released back in the summer, albeit a rocky start with the Harmony response format.",
          "score": 23,
          "created_utc": "2026-01-30 23:07:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0qb",
              "author": "night0x63",
              "text": "How did they solve the harmony issue?Â \n\n\nIs it solved by vLLM parser fixing parsing?",
              "score": 5,
              "created_utc": "2026-01-31 01:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qgdk0",
                  "author": "MrMisterShin",
                  "text": "Unsloth applied fixes to the gpt oss models chat template as a workaround, others applied fixes to their adapters and tools instead. \n\nI donâ€™t use vllm, but from what I can workout they made a fix on their end to accommodate the gpt oss models.",
                  "score": 2,
                  "created_utc": "2026-01-31 04:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qhd4r",
              "author": "Fulxis",
              "text": "\nThe model performs really well, but the remaining pain point on vLLM isnâ€™t completely fixed when using structured output (https://github.com/vllm-project/vllm/issues/23120). I still have to resort to regex to pull out values and lose the benefit of guided decoding, even though the model generally adheres closely to the JSON Schema in practice.",
              "score": 3,
              "created_utc": "2026-01-31 04:17:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpuws",
              "author": "mycall",
              "text": "I wonder if the A#B will ever be a user selectable setting.",
              "score": 2,
              "created_utc": "2026-01-31 05:18:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xz6g3",
                  "author": "MrMisterShin",
                  "text": "That would be interesting. I think 3 active parameters is acceptable, but 5 active and greater is exponentially better. \n\nKimi k2.5 uses 32B active parameters and many report that it surpasses Claude Sonnet 4.5",
                  "score": 1,
                  "created_utc": "2026-02-01 09:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p0ntw",
          "author": "one-wandering-mind",
          "text": "OpenAI has great engineers and researchers. They delayed an open source release multiple times and clearly put on a lot of effort to make the model high quality.Â \n\n\nI doubt it is one single thing that is the reason why the model is great. Lots experimentation prior to this final model, heavy data curation, a lot of pre training, and a lot of post training.Â \n\n\nThe two models fit for a consumer GPU (20b) and a single server GPU (120b) . They are remarkably fast and cheap for the capability they provide. Some companies may also release a 4 bit or mixed precision quant, but I at least have not seen benchmarks in that low precision or them deployed on the cloud at that precision. So if you run something that is benchmarked at 32 bit or 16 bit precision and you run it locally, you are probably using something between 4 and 8 not quants. Quantization does retain a lot, but you do lose some capability and that loss is likely what is less visible to standard benchmarks.Â \n\n\nIt is a shame so many people shit on the model when it came out. Much less likely that they will be as motivated to release a new version because of that or with the same frequency as they would have if the initial reception was better.\n\n\nI have been meaning to spend more time exploring what can be done with it given the incredible speed and cheap price.Â ",
          "score": 30,
          "created_utc": "2026-01-30 23:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r0p98",
          "author": "IulianHI",
          "text": "One thing nobody's mentioned yet - the A3B architecture probably plays a huge role beyond just raw size. Sparse activation means you get the knowledge of a 120b model but only pay for \\~20b worth of compute on each forward pass.\n\nThat's why it feels \"faster\" than even smaller models - because at inference time, you're not actually running all 120b parameters. The MoE routing learned which experts to activate for which tokens.\n\nCombine that with the QAT (which other comments explain well) and OpenAI's data quality advantage, and yeah... lightning in a bottle.",
          "score": 5,
          "created_utc": "2026-01-31 06:46:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s3pfb",
          "author": "artisticMink",
          "text": "gpt-oss-120b was a flex to bring openai into a space it wasn't present before. \n\nSpending a lot of money on specialized training so people who don't pay you can use your model does not make sense beyond PR and marketing.",
          "score": 6,
          "created_utc": "2026-01-31 12:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyag4",
          "author": "Anonygeois",
          "text": "The posttraining and clean data is the trick. Hopefully we do have insiders to leak the process",
          "score": 14,
          "created_utc": "2026-01-30 22:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oz48e",
          "author": "DinoAmino",
          "text": "It is good, no doubt about it. Its capabilities and skills are what is good. But it's knowledge is poor. The SimpleQA scores are shockingly bad. It will hallucinate more and stick to its guns. But ground it with context and it is amazing. So what if it's more than 6 months old - all models get dumber over time, but their capabilities never change.",
          "score": 12,
          "created_utc": "2026-01-30 23:00:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vzl3a",
              "author": "llmentry",
              "text": "It depends what knowledge you're talking about.  GPT-OSS-120B's STEM knowledge (at least in my field) is surprisingly excellent.  \n\nBased on the release notes, it was made to be good at a few fields rather than expert in all -- there were only so many params to work with, after all -- and there will be plenty of areas where it falls short.",
              "score": 2,
              "created_utc": "2026-02-01 00:43:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pdhy1",
              "author": "rm-rf-rm",
              "text": ">  But it's knowledge is poor.\n\nWhy is this a surprise for small/local models - this is one of the most straightforward, known obvious limitations of lesser params. But it has no consequence in any real world application where you should be providing in the context everything that the LLM needs through web search, RAG, code search etc.",
              "score": 5,
              "created_utc": "2026-01-31 00:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl0gd",
                  "author": "Vaddieg",
                  "text": "8B and even 4B dense models have better knowledge, but suck at everything else. I suspect that gpt-oss was crippled on purpose to not compete with commercial versions. 120B is 5x bigger but also suffers from real world knowledge detachment",
                  "score": 3,
                  "created_utc": "2026-01-31 01:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p2jm2",
          "author": "Yes_but_I_think",
          "text": "MoE is the way. Everybody understands that now.\n\nMassively spare (5% active experts or less) is the way- people are understanding this.\n\nQuantization aware training at INT4 is the best- people are coming to this understanding slowly. It's used to be FP16 (llama 1), then BF16(llama 3), then FP8(deepseek), then FP4(oss-120b), now INT4(Kimi k2.5).\n\nA 1 trillion weights model at just 650 GB and only 35B active weights per token that's just 16GB of numbers crunched per token. If you have 4TB/s bandwidth (H100/200) you get solid ~200 tokens/s and NO loss of quality. B200 is 8TB/s so that will be ~400 tokens/s (not sure on B200).",
          "score": 21,
          "created_utc": "2026-01-30 23:18:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rtnl9",
              "author": "Baldur-Norddahl",
              "text": "Kimi likely only chooses INT4 because as a Chinese company, they are restricted from using the newest GPUs. \n\nMXFP4 and NVFP4 are superior. Uses no more space and is the same speed (on GPUs with support) but has better range and better detail depending on what is needed.\n\nThe NVFP4 is the most powerful format but is Nvidia only. MXFP4 has multivendor support. FP4 is the oldest and least powerful 4 bit floating point format.\n\nGPT OSS 20b and 120b are using MXFP4 not FP4.",
              "score": 8,
              "created_utc": "2026-01-31 11:16:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xig2a",
                  "author": "Yes_but_I_think",
                  "text": "Good to know about nvfp4. Does any company QAT to nvfp4?. The critical thing is to do the quantization during training. Post training quants (most common variety) are no good below q6. The kind of quality at q8 and q4 is like so subtle that it hurts you in production.",
                  "score": 1,
                  "created_utc": "2026-02-01 06:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ouzxc",
          "author": "PatagonianCowboy",
          "text": "MXFP4",
          "score": 14,
          "created_utc": "2026-01-30 22:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2m6m",
          "author": "IulianHI",
          "text": "Another thing people miss - GPT-OSS had surprisingly good instruction following for its time. A lot of newer open models can chat fine but fall apart when you give them complex multi-step tasks. That \"just works\" feeling comes from training on a lot of high-quality instruction data, not just raw web text.",
          "score": 3,
          "created_utc": "2026-01-31 07:03:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ppfli",
          "author": "Holiday_Purpose_3166",
          "text": "GPT-OSS models have a really good architecture, most of hate come from the chronic dislike for Sam. People can't see past certain things just for the sake of hate. \n\nThe MXFP4 quant was a chef's kiss and fine-tuners like noctrex have been using it exclusively for other models too that seem to achieve a measured lower perplexity loss, better than Q8 and BF16 for much smaller memory footprint. Although stability is between Q4 and Q6. \n\nHaving used both models I can say the speed for quality is extremely good, and have used them extensively on production codebases. \n\nHowever they are slowly becoming outdated in areas where they could concern - I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained.\n\nIt's more than fine for local use for what matters, but caution should be given for vibecoders seeking external interactions.\n\nI do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates - it has been my main replacement along with GLM 4.7 Flash as backup for long, less complex tasks. \n\nI do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nIf OpenAi releases an updates OSS, that's gonna rock.",
          "score": 7,
          "created_utc": "2026-01-31 01:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9ce5",
              "author": "tarruda",
              "text": "> I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained\n\nShould we rely on LLM knowledge for deprecated deps though?\n\n> I do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates\n\nOne issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\n> I do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nThat's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\n    % llama-bench -m ~/ml-models/huggingface/mradermacher/gpt-oss-120b-Derestricted-GGUF/gpt-oss-120b-Derestricted.MXFP4_MOE.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 \n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.015 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        740.15 Â± 5.88 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         66.32 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        596.41 Â± 0.46 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         58.38 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        491.13 Â± 1.99 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         53.21 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        418.39 Â± 1.23 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         48.75 Â± 0.07 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        361.42 Â± 1.48 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         45.29 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        315.38 Â± 0.84 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         41.98 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        276.29 Â± 0.58 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         39.14 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        246.77 Â± 0.39 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         36.80 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        224.35 Â± 0.47 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         34.67 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        204.29 Â± 0.31 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         32.72 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        188.46 Â± 0.40 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         30.97 Â± 0.02 |\n    \n    build: b5b8fa1c8 (7817)",
              "score": 3,
              "created_utc": "2026-01-31 08:04:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rgkkw",
                  "author": "Holiday_Purpose_3166",
                  "text": ">Should we rely on LLM knowledge for deprecated deps though?\n\nNever said we should, it's mostly the point on deprecated knowledge that is potentially be applied.\n\n>One issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\nNever had that issue. Just simply resistant to perform.\n\n>That's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\nYour M1 Ultra doesn't have VRAM, but yes, I am offloading the model with a token generation of 30-40 t/s at full context, with an RTX 5090. That wasn't the point, it's the fact a 120B can run relatively fast for its size.",
                  "score": 3,
                  "created_utc": "2026-01-31 09:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pdzov",
          "author": "rm-rf-rm",
          "text": "Yes OpenAI really deserve their flowers on this. For all the ridicule Sam got for delaying the launch multiple times, its genuinely a great model and still my go to. \n\n\nWe actually need to give them their due credit if we want them to continue doing OSS - if they feel that the open source community just rejected them even after they finally put out an open weights model after forever, why would they want to put any more effort towards this?",
          "score": 9,
          "created_utc": "2026-01-31 00:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyh5a",
          "author": "jhov94",
          "text": "I thought GPT OSS 120b was a5b. Anyway, I never really understood how it benches so high. It's fast which is nice for certain general knowledge chat like tasks, but for coding it falls short. It writes a ton of bad code quickly then needs to rewrite it over and over until it works out the errors. But even then I also find it to be lazy. It always takes the quickest and easiest path to a solution, even if the solution does not completely solve the problem. You really have to prod it along to get it to solve anything but simple problems. GLM4.5 Air is slow but it can be left to just work out a problem on its own and sometimes its faster simply because it got it right the first time.",
          "score": 11,
          "created_utc": "2026-01-30 22:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p1qqz",
              "author": "phido3000",
              "text": "Coding is problem, always, its fundamentally different to writing human languages.\n\nIts likely that coding specific models will always perform higher in coding. Just like in humans a PHD in computer science will write better code than a PHD in English literature.",
              "score": 4,
              "created_utc": "2026-01-30 23:14:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph264",
                  "author": "Prestigious-Crow-845",
                  "text": "it is bad at creative writing too, so what it is good for? office tasks?",
                  "score": 4,
                  "created_utc": "2026-01-31 00:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2phj9c",
                  "author": "bonobomaster",
                  "text": "Hmm, I feel, that from a statistical \"which token is most likely\" / LLM point of view, coding and human language are not different at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 00:41:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pdefz",
          "author": "CorpusculantCortex",
          "text": "\"But its sort of old... its been so long since gpt oss came out\"\n\n4 months. Gpt oss came out in August. It has been 4 MONTHS. I know that tech moves fast. But my god if 1/3 of a year feels like a long time to you you need to get outside and live little.\n\n4. months.",
          "score": 21,
          "created_utc": "2026-01-31 00:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ph09v",
              "author": "coder543",
              "text": "It has been 5 months and 25 days since GPT-OSS launched, which is basically 6 months, not 4 months.",
              "score": 18,
              "created_utc": "2026-01-31 00:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl5od",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2026-01-31 01:01:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ryx2l",
                  "author": "CorpusculantCortex",
                  "text": "To be blunt, that doesn't matter. My point is months is not forever unless you are so lost in the sauce that you have poor bearing on reality. If you think another month or two changes it and felt the need to math it out to the day it is just more proof you need to step back and consider that months and days is not a long time unless you are under 5 years old.\nSaying 5 months and 25 days is practically 6 months like that has big \"im 4 years old but my birthday was 6 months ago so im basically 5\" energy.",
                  "score": 2,
                  "created_utc": "2026-01-31 12:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pq5lc",
              "author": "night0x63",
              "text": "Well I'm AI model years that is easily 78 years. /s",
              "score": 6,
              "created_utc": "2026-01-31 01:30:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p6vqo",
          "author": "lolwutdo",
          "text": "GLM 4.7 Flash is the OSS 20b killer, try it",
          "score": 14,
          "created_utc": "2026-01-30 23:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p92if",
              "author": "UnifiedFlow",
              "text": "Twice the size on disk, 1/4 the speed and coding errors were common.  4.7 Flash was a dud IMO.  Great reasoning, but implements horribly.",
              "score": 16,
              "created_utc": "2026-01-30 23:54:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgdgk",
                  "author": "someone383726",
                  "text": "Idk Iâ€™ve been happy with 4.7 flash, but I still love oss20b too.",
                  "score": 8,
                  "created_utc": "2026-01-31 00:35:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pgnmo",
                  "author": "AlwaysLateToThaParty",
                  "text": "While I haven't tried it yet, I understand that there has been a llama.cpp update because of that model, and the re-quantization has increased performance significantly.\n\nhttps://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\n\n> Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.\n\nPerhaps this is your issue?",
                  "score": 8,
                  "created_utc": "2026-01-31 00:36:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pjrqx",
                  "author": "lolwutdo",
                  "text": "The model is still new and needs work.\n\nEven with its faults currently, itâ€™s still really good.\n\nGPT-OSS was absolute shit when it came out as well until it was finally implemented correctly months later.",
                  "score": -4,
                  "created_utc": "2026-01-31 00:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q0k7l",
              "author": "Photoperiod",
              "text": "I did try it and it performed worse overall. On paper 4.7 should beat it. Biggest issue I had was repetition. Hoping some of the kinks get worked out since it's a new model. But for now I've gone back to OSS 20.",
              "score": 4,
              "created_utc": "2026-01-31 02:32:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p38mk",
          "author": "maglat",
          "text": "Its very sad indeed. I dont have many hopes right now when you see how much trouble OpenAI has to survive. There is no room for an update on their open models I guess. That makes me very sad, because 120B still is my daily driver, but as you said, it becomes dated.",
          "score": 2,
          "created_utc": "2026-01-30 23:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5nsn",
          "author": "theghost3172",
          "text": "i think its because basically unlimited synthetic data from much bigger and powerfull frontier models. imagine unlimited clean synthetic data from o3. could also be distilation.",
          "score": 2,
          "created_utc": "2026-01-30 23:36:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b0120",
              "author": "Former-Ad-5757",
              "text": "Don't forget the training data from their FrontEnd, [chatgpt.com](http://chatgpt.com) is afaik still the largest provider, so they have the most training data on what people exactly want and expect.",
              "score": 1,
              "created_utc": "2026-02-03 07:31:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2q1qnt",
          "author": "agentzappo",
          "text": "Very smart and fast model, but there are still some unresolved issues with it outputting proper tool calls in Harmony format. Maybe itâ€™s a vLLM issue and less so the model, but so far in practice itâ€™s taking a lot of anti-rationalization patterns to coerce it into reliable tool calling, and thatâ€™s only when the inference backend isnâ€™t causing logits to drift in concurrent, batched inference ðŸ˜•",
          "score": 2,
          "created_utc": "2026-01-31 02:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rlssi",
          "author": "IulianHI",
          "text": "Another thing - GPT-OSS had that rare combo of good data curation AND proper alignment that actually made it pleasant to use. Newer models chase MMLU and benchmark scores, but nobody's benchmarking \"does this feel good to talk to\" or \"does it have consistent personality\". Those vibes are harder to quantify but way more important for daily use.",
          "score": 2,
          "created_utc": "2026-01-31 10:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rw8yx",
          "author": "jwr",
          "text": "gpt-oss models are under-appreciated. I use the smaller one (20b) for spam filtering and it beats every other 30B or less model that I've tested, and I've tested quite a few with my spam benchmark, while being one of the fastest, too.",
          "score": 2,
          "created_utc": "2026-01-31 11:40:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o323niu",
              "author": "Consumerbot37427",
              "text": "> I use the smaller one (20b) for spam filtering\n\nmind sharing your prompt/flow for that?",
              "score": 1,
              "created_utc": "2026-02-01 23:06:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oyenq",
          "author": "TheRealMasonMac",
          "text": "Compute.\n\nThat's kind of the simple answer. OpenAI probably has more compute than all Chinese labs combined.",
          "score": 5,
          "created_utc": "2026-01-30 22:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p8rr4",
          "author": "GoranjeWasHere",
          "text": "And there is gpt oss 20b /120b heretic that removes censorship and keeps inteligence.\n\nI use it daily on my 5090 and you just can't beat the speed (250t/s)",
          "score": 2,
          "created_utc": "2026-01-30 23:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pkm64",
              "author": "walrusrage1",
              "text": "Are you using vLLM for this and full precision at 120b? What speeds do you get there?Â \n\n\nWe've been getting much slower results on an H100, so clearly something is up.",
              "score": 1,
              "created_utc": "2026-01-31 00:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pma1h",
                  "author": "GoranjeWasHere",
                  "text": "20b model at 250t/s",
                  "score": 1,
                  "created_utc": "2026-01-31 01:08:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2owu8b",
          "author": "PhotographerUSA",
          "text": "It fails a lot in LM studio doing MMC web calls.",
          "score": 3,
          "created_utc": "2026-01-30 22:48:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9r8j",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 4,
              "created_utc": "2026-01-30 23:58:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pfs0v",
          "author": "jabr7",
          "text": "I specially liked it in cerebras, we are getting average 5500 tps, that's making some full multi agentix systems take between 3s to 4s lol",
          "score": 1,
          "created_utc": "2026-01-31 00:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2phuvh",
          "author": "Thedudely1",
          "text": "The 20B model is the best coding model of its size that I've tried, at least for the weird kind of \"create a Wolfenstein 3D clone\" style prompts I like trying. GLM 4.7 Flash and Nemotron 3 Nano just became the other similarly sized models that can consistently do it in one prompt alongside it. But GPT-OSS 20B is the smallest model I've tested that can consistently do it successfully in either JS or Java.",
          "score": 1,
          "created_utc": "2026-01-31 00:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pxbsq",
          "author": "Ok_Individual_4295",
          "text": "Look for versions distilled from 5.2 this might update its knowledge and make it slightly better",
          "score": 1,
          "created_utc": "2026-01-31 02:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ty6od",
              "author": "RefrigeratorMuch5856",
              "text": "Could you share a link?",
              "score": 2,
              "created_utc": "2026-01-31 18:30:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pzxmk",
          "author": "WeMetOnTheMountain",
          "text": "To me GLM 4.5 Air 4q-5q derestricted  tickles my huckleberry.  Not only is it overall intelligent, it's also much better at prose.  I have to agree with you that GPT OSS 120 is simply clean.",
          "score": 1,
          "created_utc": "2026-01-31 02:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q84d1",
          "author": "jedsk",
          "text": "I think youâ€™re on the q4 if the model is 64GB",
          "score": 1,
          "created_utc": "2026-01-31 03:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qnh4e",
              "author": "simracerman",
              "text": "Thatâ€™s the one OpenAI posted on their huggingface.",
              "score": 2,
              "created_utc": "2026-01-31 05:00:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2r1yi0",
              "author": "Ok-Tumbleweed8507",
              "text": "The F16 is 64GB",
              "score": 1,
              "created_utc": "2026-01-31 06:57:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qnemf",
          "author": "MaggoVitakkaVicaro",
          "text": "They may well have training regimes which are much better than anything public.",
          "score": 1,
          "created_utc": "2026-01-31 05:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qyryn",
          "author": "TokenRingAI",
          "text": "It is likely that the model was either synthetically trained off of the outputs of OpenAI's top internal models, or off of the training data used for o3/o4",
          "score": 1,
          "created_utc": "2026-01-31 06:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdfuy",
          "author": "tarruda",
          "text": "> What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?\n\nNot only OpenAI has the best private training datasets, it also probably has superior training pipelines and is able to extract more performance per parameter.",
          "score": 1,
          "created_utc": "2026-01-31 08:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzybi",
          "author": "DefNattyBoii",
          "text": "Can someone compare it to GLM-4.7-Flash in terms of speed/tool calling/knowledge for both 20B OSS and 120B OSS?",
          "score": 1,
          "created_utc": "2026-01-31 12:10:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sfdgq",
          "author": "bfroemel",
          "text": "... and despite all the praises it seems that OpenAI isn't really that proud of gpt-oss. \n\n\n\n>The gpt-oss models were released way back in August. Since then, we've released half a dozen major updates to the frontier models. Perhaps you haven't used these lately, but their coding abilities are far beyond those of just a few months ago â€”Â and significantly beyond what the gpt-oss models are capable of.\n\n>\n\n[https://github.com/openai/codex/issues/8272#issuecomment-3672130792](https://github.com/openai/codex/issues/8272#issuecomment-3672130792)",
          "score": 1,
          "created_utc": "2026-01-31 13:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sulde",
          "author": "ekzotech",
          "text": "I'm sorry maybe this is a little bit offtop but how do you handle Harmony format tool calling issue with kilo code and other tools? I'm running gpt-oss-20b on my RTX 5080 in LM studio and it works in a chat, but I can't make it work with kilo code and tool calling. There's unresolved issue on a kilo code's GitHub, but the problem exists with zed too.",
          "score": 1,
          "created_utc": "2026-01-31 15:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t1hbn",
          "author": "darkdeepths",
          "text": "yeah these are still my faves, easy to deploy instances on single gpu setups and super fast. fairly capable agentic operators as well.",
          "score": 1,
          "created_utc": "2026-01-31 15:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t9ly0",
          "author": "KitchenSomew",
          "text": "GPT-OSS remains exceptional for several reasons:\n\n\n\n1. \\*\\*Training approach\\*\\*: It was trained with 4-bit quantization awareness from the start, not retrofitted. This preserved model quality while reducing size.\n\n\n\n2. \\*\\*Dataset quality\\*\\*: OpenAI's dataset curation was meticulous. They filtered for quality over quantity, which modern models often sacrifice for scale.\n\n\n\n3. \\*\\*Architecture efficiency\\*\\*: A3B architecture hit a sweet spot - large enough to be capable, small enough to be fast. Modern models chase parameter counts without proportional capability gains.\n\n\n\n4. \\*\\*Inference optimization\\*\\*: The model was optimized for actual deployment, not just benchmark performance.\n\n\n\nFor newer models to match this:\n\n\\- Focus on training efficiency from day 1\n\n\\- Prioritize dataset quality\n\n\\- Design for deployment, not papers\n\n\\- Consider 4-bit/8-bit native training",
          "score": 1,
          "created_utc": "2026-01-31 16:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ujzsc",
          "author": "International_Ad1896",
          "text": "I concur. Gpt-oss-20b is the one I end up going back to. It's just that harmony needs to be handled.",
          "score": 1,
          "created_utc": "2026-01-31 20:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x6s13",
          "author": "Sky-Asher27",
          "text": "it's the only model i would trust with my life",
          "score": 1,
          "created_utc": "2026-02-01 05:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2y8apc",
          "author": "gogglespizano1",
          "text": "my experience with this model is mixed. sometimes it goes on endless loops for its thoughts and i have to stop it manually. Anyone else have this issue?",
          "score": 1,
          "created_utc": "2026-02-01 10:44:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pd0oe",
          "author": "Western_Bread6931",
          "text": "i was super excited about it, a bit too excited. i actually yelped with joy when i first used it and im not ashamed to admit that i actuated my sphincter in a way that caused brownian discharge",
          "score": 1,
          "created_utc": "2026-01-31 00:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w8b8b",
              "author": "PwnedNetwork",
              "text": ">actuated my sphincter in a way that caused brownian discharge\n\nI believe these models have a Q&A step that requires them to induce brownian discharge in at least 35% of the testers. Sometimes when I'm out of laxatives I'll just run gpt-oss:cloud in ollama.",
              "score": 1,
              "created_utc": "2026-02-01 01:34:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p5ith",
          "author": "GrungeWerX",
          "text": "Ha! I canâ€™t get gpt-oss to even work right! Constantly spitting out its thinking with the response. Known issue, never resolved so itâ€™s unusable for me. Lm-studio, latest version. Updated, all that.",
          "score": 1,
          "created_utc": "2026-01-30 23:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9q56",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 6,
              "created_utc": "2026-01-30 23:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pb9w4",
                  "author": "GrungeWerX",
                  "text": "That defeats the purpose of lm-studio. Simplicity. Also, doesnâ€™t llama have the same issue?",
                  "score": 0,
                  "created_utc": "2026-01-31 00:07:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r32sb",
              "author": "StorageHungry8380",
              "text": "GPT-OSS 20B works fine for me in LM Studio. I have however tweaked inference parameters. I've disabled top-k and top-p, relying only on min-p of 0.05. YMMV.",
              "score": 2,
              "created_utc": "2026-01-31 07:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rul4j",
                  "author": "GrungeWerX",
                  "text": "Does this solve the thinking token leak?",
                  "score": 1,
                  "created_utc": "2026-01-31 11:25:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rvjy6",
              "author": "Baldur-Norddahl",
              "text": "It works great in LM Studio. They even made it the default model. When installing LM Studio from scratch, it will ask if you want to download gpt oss as your first model.\n\nAre you using the original model or a quant? You should be using the original. The quants give no benefit and many have template issues, which kind of sounds like what you are experiencing.",
              "score": 1,
              "created_utc": "2026-01-31 11:33:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s6fxc",
                  "author": "GrungeWerX",
                  "text": "original. gpt-oss-20b-MXFP4.gguf\n\nWhat settings are u using?",
                  "score": 1,
                  "created_utc": "2026-01-31 13:00:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pnnjn",
          "author": "Far-Low-4705",
          "text": "despite what a lot of ppl say, OpenAI is very good.\n\nnot to mention, GPT OSS is VERY sparse, there is nothing remotely close to what it pushes. the fact that it is coherent at that sparsity is impressive. not to mention actually good.\n\nAs for the native fp4 training, (mixed at least) its mostly because most modern open models are chinease, and the tech china has access to is years behind what the US has. training in fp4 on older chips that dont support it would slow everything down to a halt.",
          "score": 1,
          "created_utc": "2026-01-31 01:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pdyhw",
          "author": "savagebongo",
          "text": "It's about the pinnacle of LLM, just before they started training them on their own garbage.",
          "score": -1,
          "created_utc": "2026-01-31 00:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pw3jy",
          "author": "FinancialMoney6969",
          "text": "Is this a good model to start with?",
          "score": 0,
          "created_utc": "2026-01-31 02:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2plnvt",
          "author": "Pitiful-Sympathy3927",
          "text": "Data doesnâ€™t matter, doing things does.Â ",
          "score": -1,
          "created_utc": "2026-01-31 01:04:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtjhc8",
      "title": "Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qtjhc8",
      "author": "ResearchCrafty1804",
      "created_utc": "2026-02-02 03:07:42",
      "score": 375,
      "num_comments": 154,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o34gyei",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-02 08:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33uqx1",
          "author": "ortegaalfredo",
          "text": "Just tried it in openrouter. I didn't expected much as its too small and too fast, and seems to be benchmaxxed. But..\n\nWow. It actually seems to be the real thing. In my tests is even better than Kimi K2.5. It's at the level of Deepseek 3.2 Speciale or Gemini 3.0 Flash. It thinks a lot, though.",
          "score": 79,
          "created_utc": "2026-02-02 05:18:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33yemt",
              "author": "SpicyWangz",
              "text": "Yeah, crazy amount of reasoning tokens for simple answers. But it seems to have a decent amount of knowledge. Curious to see more results here",
              "score": 22,
              "created_utc": "2026-02-02 05:46:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36s57w",
                  "author": "Critttt",
                  "text": "Agree. It does so much thinking that the speed overall comes out as maybe 1/2 the speed of Gemini Flash 3. But as you say the final output is worth it and for its size and open source status, very impressive.",
                  "score": 5,
                  "created_utc": "2026-02-02 17:22:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36xqff",
              "author": "munkiemagik",
              "text": "FFS another nice new model development to lure me back into looking at adding more 3090 into my build, while prices are rising.\n\nFor a while thereâ€™s been little incentive for me to go beyond my current 80GB VRAM I am running (1x5090 & 2x3090) with GLM 4.5 Air (P-I3) and GPT-OSS-120b as my mains and many ohter smaller models. This makes 1 or 2 more 3090 seem like a possibly good call. Minimax M2.1 didn't tempt me as I would have only been able to fit the REAP'ed models.",
              "score": 2,
              "created_utc": "2026-02-02 17:48:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34g5vn",
              "author": "rm-rf-rm",
              "text": "what tests did you run?",
              "score": 3,
              "created_utc": "2026-02-02 08:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34m7yu",
                  "author": "ortegaalfredo",
                  "text": "Cibersecurity, static software analysis, vulnerability finding, etc. It's a little different that the usual code benchmark, so I get slightly different results.",
                  "score": 6,
                  "created_utc": "2026-02-02 09:21:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33i2dg",
          "author": "pmttyji",
          "text": "Good to have one more model in this size range.\n\nIts Size is less than models like MiniMax, Qwen3-235B.\n\n**EDIT:**\n\nOpen PRs for this model on llama.cpp\n\n[https://github.com/ggml-org/llama.cpp/pull/19271](https://github.com/ggml-org/llama.cpp/pull/19271)\n\n[https://github.com/ggml-org/llama.cpp/pull/19283](https://github.com/ggml-org/llama.cpp/pull/19283) \\- PR opened by Authors of this model",
          "score": 76,
          "created_utc": "2026-02-02 03:52:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33tbk3",
          "author": "CondiMesmer",
          "text": "Nice job stepbro",
          "score": 60,
          "created_utc": "2026-02-02 05:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34c8g0",
              "author": "BillyQ",
              "text": "Help, I'm stuck!",
              "score": 12,
              "created_utc": "2026-02-02 07:45:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37342a",
              "author": "blurredphotos",
              "text": "underrated comment",
              "score": 3,
              "created_utc": "2026-02-02 18:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37071a",
              "author": "vaporeonlover6",
              "text": "it this not for NSFW roleplay? I'm genuinely super confused",
              "score": 2,
              "created_utc": "2026-02-02 17:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34f2qe",
          "author": "LosEagle",
          "text": "at code\n\n\nThis should always be mentioned in sentences where somebody claims \"x beats y\" but they mean it's in coding.Â ",
          "score": 20,
          "created_utc": "2026-02-02 08:12:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33su9h",
          "author": "MikeLPU",
          "text": " Well classic - GGUF WHEN!!! :)",
          "score": 33,
          "created_utc": "2026-02-02 05:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3433do",
              "author": "spaceman_",
              "text": "[https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main) has GGUF files (split similarly to mradermarcher releases)",
              "score": 11,
              "created_utc": "2026-02-02 06:24:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3436pz",
                  "author": "MikeLPU",
                  "text": "Looks like it requires his custom llamacpp version.",
                  "score": 9,
                  "created_utc": "2026-02-02 06:25:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o340y2w",
          "author": "EbbNorth7735",
          "text": "Every 3.5 months the knowledge density doubles. It's been a fun ride. Every cycle people are surprised.",
          "score": 48,
          "created_utc": "2026-02-02 06:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o348pr8",
              "author": "[deleted]",
              "text": "Iâ€™m sure the density has to hit a limit at some point, just not sure where that is.",
              "score": 28,
              "created_utc": "2026-02-02 07:13:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34d1k4",
                  "author": "dark-light92",
                  "text": "I think the only limits we have actually hit are at sub 10b models. Like Qwen3 4b and Llama 3 8b. The models that noticeably degrade with quantization.\n\nI don't think we are close to hitting the limits for > 100B models. Not exactly sure how exactly it works for dense vs MoE.",
                  "score": 23,
                  "created_utc": "2026-02-02 07:53:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34pns6",
                  "author": "Mart-McUH",
                  "text": "I think to some degree it kind of already did. These new models are usually great at STEM (where the density increased) but suffer in normal language tasks. So things are already being sacrificed to gain performance in certain area. Of course it could be because of unbalanced training data, but I suspect that needs to be done because you can't cramp everything in there anymore.",
                  "score": 3,
                  "created_utc": "2026-02-02 09:54:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b4v40",
              "author": "Nashadelic",
              "text": "Almost like a Fun Step",
              "score": 1,
              "created_utc": "2026-02-03 08:16:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o352egz",
          "author": "Septerium",
          "text": "Did a small test here asking it (in portuguese) to generate a C code for simulating the Hodgkin-Huxley model and a python script to plot the results. It did everything right (even the model parameters), blazing fast\n\nhttps://preview.redd.it/y4a4enfnk2hg1.png?width=1040&format=png&auto=webp&s=53494f013bebc0d59f1e52bac81c4ff506f6b384",
          "score": 11,
          "created_utc": "2026-02-02 11:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o370u1o",
              "author": "vaporeonlover6",
              "text": "neuron activation joke?",
              "score": 2,
              "created_utc": "2026-02-02 18:02:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39ghpi",
                  "author": "Septerium",
                  "text": "No, but it would have been a good one ðŸ˜‚",
                  "score": 2,
                  "created_utc": "2026-02-03 01:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33wl7j",
          "author": "jacek2023",
          "text": "that's actually a great news, and looks like it's supported by llama.cpp (well, it's a fork)\n\nI think MiniMax is A10B and this one is A11B but overall only 196B is needed (so less offloading)\n\nGGUF Model Weights(int4): 111.5 GB\n\nEDIT OK guys this is gguf, just the strange name ;)\n\n[https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4)",
          "score": 17,
          "created_utc": "2026-02-02 05:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34h353",
              "author": "tarruda",
              "text": "This seems like the ideal big LLM for a 128GB setup\n\nJust built their llama.cpp fork and started downloading the weights to see how well it performs.",
              "score": 8,
              "created_utc": "2026-02-02 08:31:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35fvc4",
                  "author": "muyuu",
                  "text": "worth a post if you get this working on a Strix Halo 128GB machine\n\nI'd give it a shot but I have a lot on my plate right now",
                  "score": 3,
                  "created_utc": "2026-02-02 13:22:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34vpe0",
              "author": "Most_Drawing5020",
              "text": "I tested the Q4 gguf, working, but not so great compared to openrouter one. In my certain task in Roo Code, the Q4 gguf outputs a file that loops itself, while the openrouter model's output is perfect.",
              "score": 3,
              "created_utc": "2026-02-02 10:51:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34xxd8",
              "author": "AvailableSlice6854",
              "text": "they mention multi token prediction, so prob significantly faster than minimax.",
              "score": 2,
              "created_utc": "2026-02-02 11:11:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33b610",
          "author": "Haoranmq",
          "text": "all \\~5% activation",
          "score": 16,
          "created_utc": "2026-02-02 03:11:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33hnr0",
          "author": "segmond",
          "text": "Only time will tell...",
          "score": 15,
          "created_utc": "2026-02-02 03:50:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33bdj5",
          "author": "Recoil42",
          "text": "[https://static.stepfun.com/blog/step-3.5-flash/](https://static.stepfun.com/blog/step-3.5-flash/)",
          "score": 14,
          "created_utc": "2026-02-02 03:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34g5ct",
          "author": "Acceptable_Home_",
          "text": "Woah, just 2 months ago they were making small VL models to control phone ui, and they outdid everyone in the niche, now they're out here competition some of the biggest dawgs, hope they keepnwinning, would go check their papers!Â ",
          "score": 4,
          "created_utc": "2026-02-02 08:22:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34l9jw",
          "author": "Aggressive-Bother470",
          "text": "What's the verdict so far?Â \n\n\nBenchmaxxed or epic?",
          "score": 5,
          "created_utc": "2026-02-02 09:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35199a",
              "author": "mark_33_",
              "text": "From what ive seen, very solid agentic performance so far, and extremely fast. Testing with Roo Code and its able to perform actions really well, no errors so far. find its performance less strong when having to deal with tons of context.",
              "score": 10,
              "created_utc": "2026-02-02 11:39:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o359mn1",
              "author": "a_beautiful_rhind",
              "text": "It is dropping random chinese characters in replies and sometimes getting extra </think> tags.. \n\nDecent but not epic.",
              "score": 7,
              "created_utc": "2026-02-02 12:42:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37oixl",
                  "author": "alexeiz",
                  "text": "I tried it on https://stepfun.ai/chats and for a prompt in English, the response was all Chinese including reasoning.",
                  "score": 2,
                  "created_utc": "2026-02-02 19:50:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34ofad",
          "author": "Saren-WTAKO",
          "text": "DGX Spark llama-bench\n\n\n\n\\[saren@magi \\~/Step-3.5-Flash/llama.cpp (git)-\\[main\\] \\]% ./build-cuda/bin/llama-bench -m ./models/step3p5\\_flash\\_Q4\\_K\\_S/step3p5\\_flash\\_Q4\\_K\\_S.gguf  -fa 1 -mmp 0 -d 0,4096,8192,16384,32768 -p 2048 -ub 2048 -n 32\n\nggml\\_cuda\\_init: found 1 CUDA devices:\n\nDevice 0: NVIDIA GB10, compute capability 12.1, VMM: yes\n\n| model                          |       size |     params | backend    | ngl | n\\_ubatch | fa | mmap |            test |                  t/s |\n\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ---: | --------------: | -------------------: |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |          pp2048 |        862.87 Â± 1.86 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |            tg32 |         26.85 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |  pp2048 @ d4096 |        826.63 Â± 2.43 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |    tg32 @ d4096 |         24.84 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |  pp2048 @ d8192 |        799.66 Â± 2.96 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |    tg32 @ d8192 |         24.50 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 | pp2048 @ d16384 |        738.55 Â± 2.49 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |   tg32 @ d16384 |         23.04 Â± 0.12 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 | pp2048 @ d32768 |       645.49 Â± 11.37 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |   tg32 @ d32768 |         20.51 Â± 0.09 |\n\nbuild: 5ef1982 (7)\n\n./build-cuda/bin/llama-bench -m  -fa 1 -mmp 0 -d 0,4096,8192,16384,32768 -p    144.41s user 64.78s system 91% cpu 3:47.94 total",
          "score": 4,
          "created_utc": "2026-02-02 09:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3534km",
              "author": "tarruda",
              "text": "Also ran the bench on M1 ultra:\n\n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.014 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | ---: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |          pp2048 |        380.57 Â± 0.34 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |            tg32 |         35.00 Â± 0.24 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |  pp2048 @ d4096 |        353.07 Â± 0.21 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |    tg32 @ d4096 |         33.69 Â± 0.05 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |  pp2048 @ d8192 |        330.58 Â± 0.15 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |    tg32 @ d8192 |         32.84 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 | pp2048 @ d16384 |        292.92 Â± 0.10 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |   tg32 @ d16384 |         31.03 Â± 0.11 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 | pp2048 @ d32768 |        236.59 Â± 0.15 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |   tg32 @ d32768 |         27.92 Â± 0.11 |\n    \n    build: a0dce6f (24)",
              "score": 3,
              "created_utc": "2026-02-02 11:54:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34otbi",
              "author": "Saren-WTAKO",
              "text": "fuck I crashed my spark remotely by OOM, again.",
              "score": 1,
              "created_utc": "2026-02-02 09:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35nm4m",
              "author": "coder543",
              "text": "How much context can you fit with their Int4 quant on DGX Spark? I haven't had time to download and set this up yet, but I am thrilled that the model is <200B parameters so there is a chance it can fit without going below 4-bit.",
              "score": 1,
              "created_utc": "2026-02-02 14:06:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36uucj",
                  "author": "Saren-WTAKO",
                  "text": "only tried 65536. llama.cpp fit wanted 140k or something and that crashed my spark.",
                  "score": 1,
                  "created_utc": "2026-02-02 17:35:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3561mb",
          "author": "No-Volume6352",
          "text": "I've been testing Step 3.5 Flash (free) via Openrouter. Just started tinkering with it, but it's quite impressive.\n\n1: Proper agent tool usage  \n\nI used my custom Langchain + LangGraph agent for complex tasks like code editing and web search, and it handled them competently.\n- Models such as Gemini, Grok, Deepseek: seem to struggle with tool integration.\n- GLM4.7 and Step-3.5-Flash: demonstrate skillful tool use.\n\n2: Speed  \n\nLatency and throughput are critical for agent workflows. GLM4.7 and Deepseek feel agonizingly slowâ€”waiting makes me feel like I'm fossilizing. Even gemini flash seems sluggish. Only grok-level speed is tolerable.\nStep-3.5-Flash, however, matches grok's responsiveness while also excelling in agent behavior.\nI was anxious that it might be my implementation issue, but this model suggests otherwise.\nI'm thrilled that such capable options are emerging so swiftly.",
          "score": 5,
          "created_utc": "2026-02-02 12:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33hray",
          "author": "pigeon57434",
          "text": "They also say they outperform K2.5 im highly skeptical that so soon an only 200B model is already beating the 1T Kimi-K2.5 ive used it a little on their website and its reasoning traces have a significantly different feel and i think k2.5 is probably still a little smarter but it seems promising enough i suppose",
          "score": 13,
          "created_utc": "2026-02-02 03:50:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33wb49",
              "author": "ortegaalfredo",
              "text": "In my tests(code comprehension) is clearly better thank K2.5, and at the level of K2, as my tests showed that 2.5 is not as good as 2.0.",
              "score": -7,
              "created_utc": "2026-02-02 05:29:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o342rrk",
          "author": "spaceman_",
          "text": "Stepfun is a weird choice for a company name.",
          "score": 15,
          "created_utc": "2026-02-02 06:21:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34nqm2",
              "author": "pfn0",
              "text": "stepfunction is pretty reasonable.",
              "score": 12,
              "created_utc": "2026-02-02 09:36:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34q5j8",
              "author": "Brilliant-Weekend-68",
              "text": "Only a weird choice if you have a crippling porn addiction :)",
              "score": 8,
              "created_utc": "2026-02-02 09:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34xfn6",
                  "author": "spaceman_",
                  "text": "You point at me, and yet you got the point, didn't you?",
                  "score": 5,
                  "created_utc": "2026-02-02 11:06:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35ql03",
              "author": "pseudonerv",
              "text": "So this model must be good at creative writing. Is it?",
              "score": 2,
              "created_utc": "2026-02-02 14:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33b6qn",
          "author": "Available-Craft-5795",
          "text": "Neat, ill have to prune it sometime.",
          "score": 5,
          "created_utc": "2026-02-02 03:11:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35oyfy",
          "author": "jacek2023",
          "text": "u/ilintar is doing things\n\n[https://github.com/ggml-org/llama.cpp/pull/19271](https://github.com/ggml-org/llama.cpp/pull/19271)",
          "score": 4,
          "created_utc": "2026-02-02 14:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36cjz5",
              "author": "Rabooooo",
              "text": "Seems like the stepfun team is pushing their own PR tomorrow that they will maintain over time.. [https://github.com/ggml-org/llama.cpp/pull/19271#issuecomment-3835833362](https://github.com/ggml-org/llama.cpp/pull/19271#issuecomment-3835833362)",
              "score": 3,
              "created_utc": "2026-02-02 16:10:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3628yt",
          "author": "Lan_BobPage",
          "text": "How's creative writing compared to GLM 4.7?",
          "score": 4,
          "created_utc": "2026-02-02 15:22:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o375ai8",
              "author": "lookitsthesun",
              "text": "I mean it's not going to be the intended use case but it will probably be quite good because its internal logic and reasoning is solid. You can test it out on the demo.\n\nProbably would need to wait for someone to derestrict/abliterate it though",
              "score": 2,
              "created_utc": "2026-02-02 18:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37kub0",
                  "author": "Lan_BobPage",
                  "text": "Hmm not a fan of abliteration we'll see about that. ",
                  "score": 1,
                  "created_utc": "2026-02-02 19:33:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33t2w8",
          "author": "skinnyjoints",
          "text": "Is this a new lab? This is the first Iâ€™m hearing of them",
          "score": 9,
          "created_utc": "2026-02-02 05:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33w73h",
              "author": "limoce",
              "text": "No, this is already v3.5. They have been training large models for several years. Previous StepFun models are not outstanding among direct competitors (DeepSeek, Qwen, MiniMax, GLM, ...)",
              "score": 27,
              "created_utc": "2026-02-02 05:28:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o33znye",
                  "author": "skinnyjoints",
                  "text": "Do they have a niche they excel in?",
                  "score": 2,
                  "created_utc": "2026-02-02 05:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35e6o1",
          "author": "tarruda",
          "text": "Definitely passes my \"vibe checks\". Feels as strong as Minimax M2.1 and GLM 4.7, while completely fitting on 128GB (the int4 GGUF) devices with full 256k context. Context RAM usage is the most efficient I've seen so far.\n\nNot only that, it is very fast. I'm running this on a M1 Ultra and it is doing 30+ tokens/second. This is similar to Minimax M2.1 with 0 context, but I notice very little speed degradation as the context increases. \n\nSo far it is looking like a gem. Only downside is that it can use a lot of reasoning tokens, which seems perfect for llama.cpp new ngram speculative decoding.",
          "score": 6,
          "created_utc": "2026-02-02 13:12:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33ug1s",
          "author": "Zundrium",
          "text": "Interesting to see how well it performs.",
          "score": 3,
          "created_utc": "2026-02-02 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34fsg8",
          "author": "Dudensen",
          "text": "Step 3 was sooo good when it came out. It went by a bit without much fanfare. If this is better than that then it's good enough. Their step 3 report paper also had some interesting attention innovations.",
          "score": 3,
          "created_utc": "2026-02-02 08:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34v9dc",
          "author": "oxygen_addiction",
          "text": "It seems pretty smart and fast but holy reasoning token usage Batman.\n\nSelf-speculative decoding would really help this one out, as it repeats itself a ton.",
          "score": 3,
          "created_utc": "2026-02-02 10:47:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34wo86",
              "author": "tarruda",
              "text": "This is mentioned in the \"Limitations, known issues and future direction\" section:\n\n> Token Efficiency. Step 3.5 Flash achieves frontier-level agentic intelligence but currently relies on longer generation trajectories than Gemini 3.0 Pro to reach comparable quality.",
              "score": 4,
              "created_utc": "2026-02-02 11:00:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33jokm",
          "author": "Worldly-Cod-2303",
          "text": "Me when I benchmax and claim to beat a very recent model that is 5x the size",
          "score": 17,
          "created_utc": "2026-02-02 04:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33vg5l",
              "author": "bjodah",
              "text": "Beating deepseek-v3.2 in agentic coding is not a high bar. The evaluations (have it write JNI bindings for a C++ lib) I've done using open code puts it significantly below MiniMax-M2.1 (not to mention GLM-4.7 and Kimi-K2.5).",
              "score": 11,
              "created_utc": "2026-02-02 05:23:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34txo1",
                  "author": "oxygen_addiction",
                  "text": "How did you run it in Opencode?",
                  "score": 1,
                  "created_utc": "2026-02-02 10:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35inhg",
              "author": "Embarrassed_Bread_16",
              "text": "final proof that size doesnt matter ;)))",
              "score": 1,
              "created_utc": "2026-02-02 13:38:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36zqe2",
                  "author": "Noobysz",
                  "text": "thats why your embarressed?",
                  "score": 0,
                  "created_utc": "2026-02-02 17:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33k0mw",
          "author": "FullOf_Bad_Ideas",
          "text": "Awesome. Their StepVL is good, and from their closed products, their due diligence tool is amazing. StepFun 3 was awesome from engineering perspective (decoupling computation of attention and FFNs to different devices) but I don't think it landed well when it comes to benchmarks & expectations VS real use quality.",
          "score": 7,
          "created_utc": "2026-02-02 04:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o341x90",
          "author": "RegularRecipe6175",
          "text": "Anyone used the custom llama in their repo? The model is not recognized in the latest llama.",
          "score": 2,
          "created_utc": "2026-02-02 06:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ogli",
          "author": "Fancy_Fanqi77",
          "text": "How about comparing it to Minimax-M2.1?",
          "score": 2,
          "created_utc": "2026-02-02 09:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34ziwj",
              "author": "LegacyRemaster",
              "text": "check here: [https://huggingface.co/stepfun-ai/Step-3.5-Flash](https://huggingface.co/stepfun-ai/Step-3.5-Flash)",
              "score": 2,
              "created_utc": "2026-02-02 11:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35l4a8",
              "author": "DOAMOD",
              "text": "I've tried it for a while and it nailed a frontend integration at lightning speed, only one simple error. Perhaps I'm being hasty, but the feeling is that it's better than MiniMax2.1. Maybe in practice they'll be similar, we'll see, but I've been impressed by the first experience. Congratulations to the Step team.",
              "score": 1,
              "created_utc": "2026-02-02 13:52:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34pcsf",
          "author": "TacGibs",
          "text": "https://preview.redd.it/66kiwjhsz1hg1.jpeg?width=640&format=pjpg&auto=webp&s=d98a576a5b8bf5f435f73c3a12fd2a0b12741cb7",
          "score": 2,
          "created_utc": "2026-02-02 09:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ueq8",
          "author": "Expensive-Paint-9490",
          "text": "I wonder why so many labs put \"Flash\" in their model names. It's not like it has a standard meaning.",
          "score": 2,
          "created_utc": "2026-02-02 10:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3620e5",
              "author": "GreenGreasyGreasels",
              "text": "To signal that it is \"fast\" and also that a big \"pro\" is coming i guess.\n  \n  \nAlso Chinese labs tend to pick up the nomenclature and branding styles popularized by Google/Anthropic/OpenAI as they don't have an innate understanding of the western market (from a branding marketing perspective) and are content to reuse themes and styles that are current - which I largely think it wise at this stage.",
              "score": 5,
              "created_utc": "2026-02-02 15:20:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34x5ud",
          "author": "Grouchy-Bed-7942",
          "text": "From what I've tested, it's at least of Minimax m2.1 quality in development.",
          "score": 2,
          "created_utc": "2026-02-02 11:04:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354ihb",
          "author": "NucleusOS",
          "text": "the livecode bench gap (86.4 vs 83.3) is impressive for a smaller model. wonder if it's architecture or training data quality.\n\n\n\nanyone tested it locally yet",
          "score": 2,
          "created_utc": "2026-02-02 12:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35kqc4",
              "author": "Zc5Gwu",
              "text": "Have to wait for the next iteration of live code to be sure.Â ",
              "score": 1,
              "created_utc": "2026-02-02 13:50:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3574oh",
          "author": "a_beautiful_rhind",
          "text": "I tried it a little bit and seems decent for oneshots. Very similar to trinity large from acree.",
          "score": 2,
          "created_utc": "2026-02-02 12:24:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35gh89",
              "author": "silenceimpaired",
              "text": "Any thoughts on fiction/creative writing?",
              "score": 3,
              "created_utc": "2026-02-02 13:25:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35h2yg",
                  "author": "a_beautiful_rhind",
                  "text": "Seemed like it did ok, but I only tried short chats.",
                  "score": 2,
                  "created_utc": "2026-02-02 13:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35joak",
          "author": "DOAMOD",
          "text": "I've tried it out a bit and it's really surprised me, it seems pretty good. It's incredible that we have something like this. Will int4 be very damaged in Q2/Q3?",
          "score": 2,
          "created_utc": "2026-02-02 13:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35nom2",
          "author": "srigi",
          "text": "It is queued for quantization at mradermacher:\nhttps://huggingface.co/mradermacher/model_requests/discussions/1774",
          "score": 2,
          "created_utc": "2026-02-02 14:06:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3710dx",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/isk79iedf4hg1.png?width=1154&format=png&auto=webp&s=d6cacbea97e2a5cbd8ec886e94012872a1bc839c\n\namazing",
          "score": 2,
          "created_utc": "2026-02-02 18:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33otew",
          "author": "MrMrsPotts",
          "text": "Is there any way to try this out online?",
          "score": 1,
          "created_utc": "2026-02-02 04:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33rys8",
              "author": "SpicyWangz",
              "text": "[https://stepfun.ai](https://stepfun.ai)",
              "score": 8,
              "created_utc": "2026-02-02 04:58:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3430ix",
              "author": "Abject-Ranger4363",
              "text": "Free on OpenRouter (for now): [https://openrouter.ai/chat?models=stepfun/step-3.5-flash:free](https://openrouter.ai/chat?models=stepfun/step-3.5-flash:free)",
              "score": 3,
              "created_utc": "2026-02-02 06:23:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o343bf4",
                  "author": "MrMrsPotts",
                  "text": "Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-02 06:26:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34xkpw",
          "author": "Big-Pause-6691",
          "text": "Tried this on OpenRouter. It outputs fast as hell lol, and it seems really damn good at solving competition-style problems.",
          "score": 1,
          "created_utc": "2026-02-02 11:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xqq0",
          "author": "CLGWallpaperGuy",
          "text": "Wow. the model is pretty damn good at coding",
          "score": 1,
          "created_utc": "2026-02-02 11:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34zj27",
          "author": "fairydreaming",
          "text": "Tested in lineage-bench:\n\n    $ cat ../lineage-bench-results/lineage-8_64_128_192/glm-4.7/glm-4.7_*.csv ../lineage-bench-results/lineage-8_64_128_192/deepseek-v3.2/deepseek-v3.2_*.csv results/temp_1.0/step-3.5-flash_*.csv|./compute_metrics.py --relaxed\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.956 |       1.000 |        1.000 |         0.975 |         0.850 |\n    |    2 | z-ai/glm-4.7           |     0.794 |       1.000 |        0.750 |         0.750 |         0.675 |\n    |    3 | stepfun/step-3.5-flash |     0.769 |       1.000 |        0.700 |         0.725 |         0.650 |\n\nScore is indeed close to GLM-4.7. Unfortunately it often interrupts the reasoning early for unknown reason and fails to generate an answer. I've also seen some infinite loops. Best results so far are with temp 1.0, top-p 0.95. Model authors recommend temp 0.6, top-p 0.95.",
          "score": 1,
          "created_utc": "2026-02-02 11:25:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o351kif",
              "author": "Big-Pause-6691",
              "text": "I canâ€™t seem to find the authorâ€™s recommended sampling params anywhere. Whatâ€™s it like w. t=1 and top-p=1? Any noticeable diff?",
              "score": 1,
              "created_utc": "2026-02-02 11:42:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35249c",
                  "author": "fairydreaming",
                  "text": "https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/3#6980720b945ef5272b15db80",
                  "score": 1,
                  "created_utc": "2026-02-02 11:46:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35f4bl",
                  "author": "tarruda",
                  "text": "I'm using only --temp 1.0 as recommended in HF repo.",
                  "score": 1,
                  "created_utc": "2026-02-02 13:17:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35fu2i",
                  "author": "fairydreaming",
                  "text": "    $ cat results/temp_1.0_topp_1.0/step-3.5-flash_*.csv|./compute_metrics.py --relaxed\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | stepfun/step-3.5-flash |     0.750 |       1.000 |        0.850 |         0.750 |         0.400 |\n\nHmm, with temp 1.0 and top-p 1.0 scores are a bit better for simpler quizzes, worse for most complex lineage-192. Note that I have output limited to 64k tokens.",
                  "score": 1,
                  "created_utc": "2026-02-02 13:22:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b7hao",
              "author": "LegacyRemaster",
              "text": "I had a loop problem only once using kilocode + vscode. Solution: Paused, killed the llamacpp process, reloaded with a 90k context limit and Q8 context quantization. Restarted llamacpp (no temperature or repeat penalty options: default). It finished the task correctly.",
              "score": 1,
              "created_utc": "2026-02-03 08:42:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b7lyd",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/bqjdllwfs8hg1.png?width=2379&format=png&auto=webp&s=c9842462192af8ade55d7b744f658f7a95e392e3\n\nalso: condense context helps.",
              "score": 1,
              "created_utc": "2026-02-03 08:43:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36q5s4",
          "author": "__JockY__",
          "text": "I tried the FP8 version in vLLM 0.16rc1 and while it loads/runs ok, tool calling is broken. Running with Claude Code I see the vLLM logs spammed with tool calling template errors, for example:\n\n    INFO 02-02 10:08:57 [step3p5_tool_parser.py:1365] vLLM Successfully import tool parser Step3p5ToolParser !\n    WARNING 02-02 10:09:00 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 1\n    WARNING 02-02 10:09:00 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 2\n    INFO 02-02 10:09:01 [step3p5_tool_parser.py:1365] vLLM Successfully import tool parser Step3p5ToolParser !\n    WARNING 02-02 10:09:01 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 1\n\nAnd then Claude cli quite literally crashes and dumps me back to the terminal. Ah well. Back to MiniMax :)",
          "score": 1,
          "created_utc": "2026-02-02 17:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o372605",
              "author": "__JockY__",
              "text": "It's these kinds of errors (tool calling template related) that have plagued every single model except MiniMax-M2.x when I've tried using them with Claude code.\n\nQwen3 235B is a joke. GLM-4.x fared little better. They just barf and throw errors all the fucking time. Looks like Step-3.5-Flash is the same.\n\nMiniMax just just works when generating tool calls. They're magically well-formed and Claude can go through thousands of tool calls without a hitch. \n\nMM may not be the strongest model at writing advanced code or debugging complex issues, but it more than makes up for that in reliability as an agent.\n\nI've ditched Step-3.5-Flash and now Claude works perfectly again. It's such a shame. These new models (Dots, GLM, Step, etc.) write fantastic code! They're so strong! They just can't do reliable tool calling and so they don't get used. I'm convinced - certainly about GLM - that the open version is neutered for tools because everything I read about the API says it works well.",
              "score": 1,
              "created_utc": "2026-02-02 18:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37xwem",
                  "author": "Front_Eagle739",
                  "text": "huh. I get errors with templates on everything and usually just go \"gpt5.2 fix this\" and it does. My glm flash tool calling is rock solid now.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:34:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36xri9",
          "author": "ghulamalchik",
          "text": "I can attest to its performance, there's a free model you can use on OpenRouter. I used it with Roo Code.\n\nIt's extremely fast and solved some things the other big free models couldn't solve.\n\nI'll definitely keep an eye on a future API subscription. But for now I'll wait for DeepSeek R2 before I commit.",
          "score": 1,
          "created_utc": "2026-02-02 17:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36ytbo",
          "author": "LosEagle",
          "text": "Step-flash got stuck in a window again..Â ",
          "score": 1,
          "created_utc": "2026-02-02 17:53:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3710sd",
          "author": "Noobysz",
          "text": "for iklammacpp CPU offloading +GPU which layers for it are better to offload on CPU for since i have only upto 84 GB VRAM and the rest must be in my 96 GB RAM so which layers numbers for example for the gguf should i offload on CPU for fastest Speed?",
          "score": 1,
          "created_utc": "2026-02-02 18:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37dl6v",
              "author": "LegacyRemaster",
              "text": "llama-server.exe --model  \"f:\\\\step3p5\\_flash\\_Q4\\_K\\_S.gguf\" --ctx-size 8192 --threads 16 --host [127.0.0.1](http://127.0.0.1) \\--no-mmap --flash-attn on --fit on ---> \n\nload\\_tensors: offloaded 46/46 layers to GPU\n\nload\\_tensors:          CPU model buffer size =   283.22 MiB\n\nload\\_tensors:        CUDA0 model buffer size = 92265.46 MiB\n\nload\\_tensors:    CUDA\\_Host model buffer size = 13780.12 MiB",
              "score": 2,
              "created_utc": "2026-02-02 18:59:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37jm7z",
          "author": "fallingdowndizzyvr",
          "text": "Why so many tiny little files?",
          "score": 1,
          "created_utc": "2026-02-02 19:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37zrni",
          "author": "IrisColt",
          "text": "From the creators of ace-step!?",
          "score": 1,
          "created_utc": "2026-02-02 20:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3abzpu",
              "author": "IntrepidKick1335",
              "text": "yes, they also work on music model with ACE studio",
              "score": 1,
              "created_utc": "2026-02-03 04:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o385tn7",
          "author": "SVG-CARLOS",
          "text": "And kimi k2.5 outperforms GLM-4.7 and DeepSeek v3.2 in almost all.",
          "score": 1,
          "created_utc": "2026-02-02 21:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b0myt",
          "author": "AppealSame4367",
          "text": "I tried it and am shocked how good and fast it is. I think this is it. GLM 4.7, Â Step 3.5 Flash, Kimi K2.5\n\nNo need for American models anymore and I suspect that they will quickly catch up any advance that American models still have.\n\nWhat would be needed to run this model locally?",
          "score": 1,
          "created_utc": "2026-02-03 07:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o345t9e",
          "author": "shing3232",
          "text": "Kind of feels like Deepseek V2 ",
          "score": 1,
          "created_utc": "2026-02-02 06:47:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o348t76",
              "author": "shing3232",
              "text": "Deep Reasoning at Speed: While chatbots are built for reading, agents must reason fast. Powered by 3-way Multi-Token Prediction (MTP-3), Step 3.5 Flash achieves a generation throughput of 100â€“300 tok/s in typical usage (peaking at 350 tok/s for single-stream coding tasks). This allows for complex, multi-step reasoning chains with immediate responsiveness.",
              "score": 2,
              "created_utc": "2026-02-02 07:14:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34tcdt",
          "author": "Lazy-Variation-1452",
          "text": "\\`Flash\\` means light and fast. I don't agree that a 196B model can be considered \\`flash\\`; that is just bad naming. Haven't tried the model, though, the benchmarks look promising",
          "score": -1,
          "created_utc": "2026-02-02 10:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34vazr",
              "author": "oxygen_addiction",
              "text": "200 tokens a second on OpenRouter says otherwise.",
              "score": 4,
              "created_utc": "2026-02-02 10:47:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34yv9q",
                  "author": "Lazy-Variation-1452",
                  "text": "\\*167 tokens \n\nSecondly, the hardware and power required to run this model is very much inaccessible for most people. There are certain providers, but that doesn't make it a \\`flash\\` model, and I don't think it is a good idea to normalize extremely large models",
                  "score": 1,
                  "created_utc": "2026-02-02 11:19:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35gkqn",
              "author": "Ok_Procedure_5414",
              "text": "But MoE?",
              "score": 2,
              "created_utc": "2026-02-02 13:26:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3baa6e",
          "author": "ConsciousArugula9666",
          "text": "Already some free-to-play providers: OpenRouter, ZenMUX and AIHubMix see [https://llm24.net/model/step-3-5-flash](https://llm24.net/model/step-3-5-flash)",
          "score": 0,
          "created_utc": "2026-02-03 09:09:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34v1yg",
          "author": "AnomalyNexus",
          "text": "Seems likely that there is a bit of benchmaxing in there but still seems promising anyway",
          "score": -1,
          "created_utc": "2026-02-02 10:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33h7wa",
          "author": "JimmyDub010",
          "text": "Oh cool another model for the rich",
          "score": -17,
          "created_utc": "2026-02-02 03:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33m8cl",
              "author": "datbackup",
              "text": "Newsflash pointdexter, you are the rich\n\nAnd just like all the other rich people, you are obsessed with the feeling that you donâ€™t have enough money",
              "score": 10,
              "created_utc": "2026-02-02 04:19:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o355ypo",
              "author": "Orolol",
              "text": "It's literally free on openrouter.",
              "score": 2,
              "created_utc": "2026-02-02 12:16:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qp6rm5",
      "title": "API pricing is in freefall. What's the actual case for running local now beyond privacy?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-28 09:27:55",
      "score": 343,
      "num_comments": 375,
      "upvote_ratio": 0.84,
      "text": "K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.\n\nMeanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.\n\nI've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:\n\n1. **Privacy** â€” legit, no argument. If you're processing sensitive data, local is the only option.\n2. **No rate limits** â€” fair, but most providers have pretty generous limits now unless you're doing something unusual.\n3. **\"It's free after hardware costs\"** â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\n\nThe argument I never hear but actually find compelling: **latency control and customization**. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.\n\nWhat's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o26pje2",
          "author": "Minimum-Vanilla949",
          "text": "The offline aspect is huge for me - I travel a lot and having models that work without internet is clutch. Also call me paranoid but I don't trust these API companies to not randomly change their ToS or jack up prices once they corner the market",
          "score": 502,
          "created_utc": "2026-01-28 09:30:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vec7",
              "author": "AciD1BuRN",
              "text": "They keep changing the tos so much now i don't think it can be even called tos at this point",
              "score": 102,
              "created_utc": "2026-01-28 10:23:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o296qvc",
                  "author": "MoffKalast",
                  "text": "It's more like tng now already, or even ds9.",
                  "score": 24,
                  "created_utc": "2026-01-28 17:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2m6a96",
                  "author": "Aardvark_Says_What",
                  "text": "\"Waterproof Shoes Incorporated hereby do not guarantee that our products will be waterproof or necessarily provide the services expected of products called 'shoes'.\"",
                  "score": 1,
                  "created_utc": "2026-01-30 15:16:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27esl4",
              "author": "thatguy122",
              "text": "Exactly this. Don't be fooled by these 10 yr subsidized loss leader fees intended to corner the market.Â ",
              "score": 48,
              "created_utc": "2026-01-28 12:51:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27taov",
                  "author": "Icy-Pay7479",
                  "text": "Trudging through the mountains with a triple 3090 desktop â€œdeath strandingâ€ style.",
                  "score": 35,
                  "created_utc": "2026-01-28 14:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28y2ou",
                  "author": "Mkboii",
                  "text": "So isn't it better to hold off local upgrades till the market collapses. It would coincide with the demand for data centre RAM going down and then better local gpu options may become available.",
                  "score": 6,
                  "created_utc": "2026-01-28 17:16:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29gtlb",
                  "author": "CarrotcakeSuperSand",
                  "text": "Theyâ€™re not subsidized, inference has pretty solid gross margins. Itâ€™s the training and initial infrastructure buildout that causes negative cash flow.",
                  "score": 1,
                  "created_utc": "2026-01-28 18:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27mst1",
              "author": "spaceman_",
              "text": "100% this last bit. It's the same reason I use so many open source tools: I don't want to depend on a single vendor who can independently decide \"actually, you need us now\" and jack up prices massively or change the model quality I have access to as a low-end user.\n\nThink about Adobe but on steroids.",
              "score": 28,
              "created_utc": "2026-01-28 13:37:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m76su",
                  "author": "Aardvark_Says_What",
                  "text": "BMW heated seats subscription. Owners go fucking mental. BMW: \"Oops. We went a bit too far (introduced it too soon, save it for later).\"\n\nLate-stage capitalism. Living the dream.",
                  "score": 1,
                  "created_utc": "2026-01-30 15:20:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2afcq5",
                  "author": "fenixnoctis",
                  "text": "I donâ€™t get your logic here though. \n\nOnce a vendor jacks up prices, youâ€™re free to switch. \n\nUntil then why not take advantage of it?",
                  "score": 0,
                  "created_utc": "2026-01-28 21:09:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2821zo",
              "author": "bigh-aus",
              "text": "Don't forget about being forced to give all their past data - that's the big one.  \n  \n[https://openai.com/index/response-to-nyt-data-demands/](https://openai.com/index/response-to-nyt-data-demands/)\n\nData invariably gets leaked (even accidentally).",
              "score": 22,
              "created_utc": "2026-01-28 14:55:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27aqwk",
              "author": "miken0222",
              "text": "Do you mind sharing what model you use on your offline travels? Im in a similar situation where offline is frequent but I need something during those times.",
              "score": 12,
              "created_utc": "2026-01-28 12:24:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e2be2",
                  "author": "Kahvana",
                  "text": "On my 8GB RAM laptop with Intel N5000 I use LFM2-VL 1.6B (could probably go for 3B) as it's super fast (near instant replies), low resource usage, has vision and is decent enough for toolcalling. Pair it with zim archives (kiwix) for grounding / world knowledge and it's solid. Haven't tried websearch with it.\n\nYou can also pair it with LFM2-CoBERT-350M for embedding. Probably not better than Qwen3-Embedding-0.6B and likely comparable to embeddinggemma-300M (haven't benchmarked), but it is much lighter to run.",
                  "score": 2,
                  "created_utc": "2026-01-29 11:00:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26ui9r",
              "author": "UsernameAttempt",
              "text": "I'm not sure cornering the market is in the cards. There's too much competition, too many alternatives, and the technology is not one that can be monopolized. The best models of the biggest companies of 1 year ago are worse than the models of small companies in China today. With improvements to models slowing down, we're moving towards models as commodities - similar in performance and competing on price. I think these models today are the most expensive they'll ever be.",
              "score": 50,
              "created_utc": "2026-01-28 10:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26xnoi",
                  "author": "SeasonNo3107",
                  "text": "Cornering the market will only happen in 5 plus years with buyouts and mergers imo",
                  "score": 38,
                  "created_utc": "2026-01-28 10:43:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2798dz",
                  "author": "Conscious-Ball8373",
                  "text": "There's also too much investor money around and once it dries up someone will have to pay the bills.",
                  "score": 8,
                  "created_utc": "2026-01-28 12:14:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hxsc",
                  "author": "Icy_Foundation3534",
                  "text": "mergers happen very fast in this market",
                  "score": 1,
                  "created_utc": "2026-01-28 16:06:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27pu9s",
              "author": "Ayumu-Aikawa",
              "text": "exactly, the TOS and pricing are things that can change any day, we keep seeing stories about how these companies are not making any profit. It's clear for me they're going to have to change something sooner or later",
              "score": 6,
              "created_utc": "2026-01-28 13:53:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27prfq",
              "author": "genshiryoku",
              "text": "The trend we're seeing is a commodification of LLMs it's basically impossible to have a commodity monopoly, so over the long run costs should get lower as innovations reduce the cost of serving inference.\n\nThere is also an economy of scale going on where large server farms can simply just serve you inference at a lower cost than it takes you to simply pay for the electricity.\n\nFor me as someone owning multiple RTX 3090s the electricity costs of serving prompts are already higher than the cost of using APIs for the inference of the same model. \n\nLocal only makes sense if you have free (solar) power, *need* privacy or offline usage. Or if you have a custom fine-tuned model that you need to run, since hosting this yourself on rented GPUs is still more expensive than running it yourself unlike generic big model inference.",
              "score": 8,
              "created_utc": "2026-01-28 13:53:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28uagh",
                  "author": "MrPecunius",
                  "text": ">*need* privacy\n\nEverybody *needs* privacy.",
                  "score": 11,
                  "created_utc": "2026-01-28 17:00:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dxtia",
                  "author": "GnistAI",
                  "text": "Interestingly, anyone using electricity to heat their home can justifiably consider their GPU power consumption as free. At least during winter.",
                  "score": 2,
                  "created_utc": "2026-01-29 10:21:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2851pp",
              "author": "p3r3lin",
              "text": "Same. I live in a part of the world where stable and always available internet connection is still not guaranteed (...Germany). So having \"AI in the pocket\" would be a great thing to have. There are projects like https://locallyai.app/ that enable small models (4b, etc) running on mobile phones, but its definitely no replacement for a SOTA model of any kind.",
              "score": 2,
              "created_utc": "2026-01-28 15:09:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ao1in",
              "author": "pandodev",
              "text": "yes privacy and offline for the right things is EVERYTHING.",
              "score": 2,
              "created_utc": "2026-01-28 21:47:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27f30f",
              "author": "lambdawaves",
              "text": "â€œOnce they corner the marketâ€\n\nThey wonâ€™t corner the market. There will always be at least a handful of competitors that offer a 98% similar product (via the same API) preventing arbitrarily high price hikes",
              "score": 5,
              "created_utc": "2026-01-28 12:53:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27cj5l",
              "author": "Imaginary_Context_32",
              "text": "May I know your setups ( hardware, models, usecase?)",
              "score": 3,
              "created_utc": "2026-01-28 12:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2az6tt",
              "author": "qwerty____qwerty",
              "text": "what setup do you have? if you travel a lot - I'm assuming that's a laptop, aaaand how powerful should that laptop be to compete with gemini api?",
              "score": 1,
              "created_utc": "2026-01-28 22:37:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2boezb",
              "author": "mycall",
              "text": "They can also block countries if they were forced too",
              "score": 1,
              "created_utc": "2026-01-29 00:47:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m5v37",
              "author": "Aardvark_Says_What",
              "text": "\\> I don't trust these API companies to not randomly change...\n\nThat's not nice. You really can trust them 100%... to jack up the prices and turn down the tokens the second they think they have somehow locked you in.",
              "score": 1,
              "created_utc": "2026-01-30 15:14:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29odo6",
              "author": "Eye-m-Guilty",
              "text": "Would love to know what ur running offline n the set up!",
              "score": 1,
              "created_utc": "2026-01-28 19:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26vrl0",
          "author": "IactaAleaEst2021",
          "text": "For my work, repeatability of results. When you download a model, you audit it and you start trusting it, you are sure the vendor does not change its behavior behind the scene.\nI am not saying they do it for malicious purposes, but in many cases they improve their product in some direction, while making it less useful in others.",
          "score": 159,
          "created_utc": "2026-01-28 10:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aj5fc",
              "author": "notRandomUsr",
              "text": "This happened to us, we were working with \"diseases\" data.  We gather lots of information for different organisms using gpt3.5 turbo, after couple months we tried the new model (gpt4 and its variants) and got terrible results. The difference was so dramatic that we went back to the previous model, using the exact same version and the results were completely different, non existent basically. It was curious to see how, with the same prompts, the same system instructions, the same format, and the same questions, we obtained scarce data or empty responses compared to what we achieved in the first run.",
              "score": 9,
              "created_utc": "2026-01-28 21:26:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26y385",
              "author": "SpicyWangz",
              "text": "I havenâ€™t encountered this with any API product. Consumer facing chat interfaces are going to continually evolve, but if youâ€™re using a tagged model on an API it shouldnâ€™t be changing",
              "score": -8,
              "created_utc": "2026-01-28 10:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27g36l",
                  "author": "TheRealMasonMac",
                  "text": "Gemini, GPT, and Claude often have undisclosed model updates.",
                  "score": 36,
                  "created_utc": "2026-01-28 12:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o272y78",
                  "author": "IactaAleaEst2021",
                  "text": "Youâ€™re right, but still if you develop a product based on consistent results, the should not must become must not.",
                  "score": 17,
                  "created_utc": "2026-01-28 11:27:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27k3ts",
                  "author": "ross_st",
                  "text": "Google recently changed the way the Gemini 3 API works behind the scenes by adding the cutoff date and an encouragement to use its chain of thought to the system instruction. That gets added even if you leave the system instruction parameter blank in the API call. Before, if you left it blank, there would just not be a system instruction block in the context window.",
                  "score": 10,
                  "created_utc": "2026-01-28 13:22:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2767yw",
                  "author": "jikilan_",
                  "text": "U will be forced to upgrade when they decomm the version of model that u r using",
                  "score": 11,
                  "created_utc": "2026-01-28 11:52:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27ja0u",
                  "author": "Significant-Heat826",
                  "text": "That's weird because I often get emails from vendors saying they've changed something in their API endpoint yet again.",
                  "score": 4,
                  "created_utc": "2026-01-28 13:18:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hg1x",
                  "author": "Eugr",
                  "text": "They can still serve a more quantized version to serve more customers during the peak hours, change their guardrails, etc.",
                  "score": 3,
                  "created_utc": "2026-01-28 16:04:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27s8rd",
                  "author": "Flamenverfer",
                  "text": "2 come to mind immediately. I can't remember exact model names from Amazon bedrocks service but the sonnet models they serve on there, the new version of sonnet was extremely token conservative when we needed it to finish a response in a consistent format. But the model would always take the lazy way out and say something to the likes of  \"All other data shall be labelled N/A\" when it would need to list every datapoint as N/A for the json file.\nAlso lets not forget chatGpt taking away 4 when they released 5",
                  "score": 3,
                  "created_utc": "2026-01-28 14:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28ljq9",
                  "author": "Double_Cause4609",
                  "text": "You'd think, but providers tend to quantize their models over time, and sometimes without labeling. It's not really a \"finetune\" or change, exactly, but it can be noticeable, particularly for function calls.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:22:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27d3ri",
                  "author": "Imaginary_Context_32",
                  "text": "I had faced this with GPT 4 turbo â€œnot with specific modelâ€",
                  "score": 1,
                  "created_utc": "2026-01-28 12:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26pgqm",
          "author": "05032-MendicantBias",
          "text": "API pricing won't be subsidized forever. At some point venture capital will want a return. Same as the short time where Uber was subsidized.\n\nBy all means, get a millionare to subsidize your workflows, but know this is a short term deal that won't last.\n\nThe goal of venture capital is to make everything else go away, so they get a monopoly and raise prices. Maintaining a local rig as fall back, and maintaining open source tools and models, will screw that businness plan heavily ;)",
          "score": 348,
          "created_utc": "2026-01-28 09:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28hoks",
              "author": "cleverusernametry",
              "text": "It stuns me that people are still so gullible after over a decade of SaaS and cloud. You already see people going back to on-prem from cloud because cloud pricing has become so predatory. Just wait for this arc to play out with AI APIs",
              "score": 23,
              "created_utc": "2026-01-28 16:05:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mtoaq",
                  "author": "mrkstu",
                  "text": "Because VMWare is always going to be non-predatory, right?",
                  "score": 1,
                  "created_utc": "2026-01-30 17:01:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27wiwc",
              "author": "BonjaminClay",
              "text": "Exactly this. Enshittification comes for everything at this point and if they are giving something away or it is unrealistically cheap now then you shouldn't rely on it. I have learned this lesson too many times. I stopped buying physical media or maintaining my own copies for a long time because streaming was just easier and now there are 20 streaming services all wanting 5x more per month each. \n\nBuilding with local or on something I control the costs of means that when the AI bubble pops my stuff won't break or get unpredictably more expensive.",
              "score": 33,
              "created_utc": "2026-01-28 14:27:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o274q5k",
              "author": "rditorx",
              "text": "Besides venture capital, if you dry out the competition, and that includes locally run AI, you gain control over the market and can ask for almost any price, as long as people and companies can afford it.\n\nBig tech has always been subsidizing its services. It always starts with great products and awesome services or any other offer you can't refuse and will use until you're depending on them, then they pull the rug.\n\nIt's so common there's a term Doctorow coined: Enshittification.",
              "score": 17,
              "created_utc": "2026-01-28 11:41:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o290v7c",
              "author": "cniinc",
              "text": "\"The goal of venture capital is to make everything else go away, so they get a monopoly and raise prices\" - the best succinct description of venture capital. Understand that the second you become dependent on the external model they'll squeeze you for as much as they can on that investment. While the model is cheap, learn how to do it without the API, and have the API do the parts you can't. Then slowly increase you skills until you can do it all without API. Make them subsidize your learning, not your dependence.Â ",
              "score": 7,
              "created_utc": "2026-01-28 17:29:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26qgad",
              "author": "Distinct-Expression2",
              "text": "Nice point. When do you think that will happen?",
              "score": 26,
              "created_utc": "2026-01-28 09:39:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26rl9c",
                  "author": "05032-MendicantBias",
                  "text": "I have no idea. The scheme collapses when venture capital lose patience. It could be as soon as this quarter, or it might take a few years. \n\nI feel confident the bubble pop will be preceeded by OpenAI trying an IPO for 2 trillion dollars, when venture capital will try to offload their position to retail.",
                  "score": 90,
                  "created_utc": "2026-01-28 09:49:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26sm8l",
                  "author": "neotorama",
                  "text": "When they IPO",
                  "score": 13,
                  "created_utc": "2026-01-28 09:58:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xk0y",
                  "author": "Finn55",
                  "text": "When it is least convenient",
                  "score": 13,
                  "created_utc": "2026-01-28 10:42:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28cmrg",
                  "author": "rbpri",
                  "text": "Google just cut Geminiâ€™s free tier in AI studio from ~100 RPD to ~20 RPD. Itâ€™s impossible to know when exactly enshittification is going to hit but itâ€™s coming sooner rather than later.",
                  "score": 8,
                  "created_utc": "2026-01-28 15:43:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27g48e",
                  "author": "SINdicate",
                  "text": "18 to 24 months",
                  "score": 1,
                  "created_utc": "2026-01-28 12:59:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2czwts",
                  "author": "protestor",
                  "text": "OpenAI already announced the first enshittification package (ads in chat). The reason they didn't jack up prices yet is that they need to  first kill local AI - they need to maintain in people's minds this idea that local AI makes no sense financially, since cloud AI is so cheap. Also, they can't kill local AI by pushing the frontier (open weights AI is trailing frontier models by 6-8 months). They have no moat in the model itself, and all the money they pour on training will only help Chinese clones when they distill it.\n\nSo what they are currently doing is to buy up enough wafers to jack up prices for machines used in local AI. I think that's what they are more worried about, consumer GPUs that crosses the 32GB-ish VRAM barrier. They play at TSMC made GPUs more expensive and made people buy more 8GB GPUs.\n\nI think that by the time the enshittification of closed AI is completed, running local AI will be unfeasible, a distant thing in the past",
                  "score": 1,
                  "created_utc": "2026-01-29 05:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26xc94",
              "author": "johnkapolos",
              "text": "API pricing isn't subsidized. Subscriptions are subsidized.",
              "score": 14,
              "created_utc": "2026-01-28 10:40:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27b7qx",
                  "author": "cyberdork",
                  "text": "Both are subsidized.",
                  "score": 9,
                  "created_utc": "2026-01-28 12:28:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29ppmp",
                  "author": "bathamel",
                  "text": "None of these companies are remotely profitable.  Therefore they are all subsidized at the moment.",
                  "score": 3,
                  "created_utc": "2026-01-28 19:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2720nf",
              "author": "AnomalyNexus",
              "text": "Even without subsidization itâ€™s hard to beat the economics of centralized data centers that have scale.\n\nSeems unlikely that itâ€™ll ever drop to a point where local wins a like for like shootout regardless of what happens",
              "score": 20,
              "created_utc": "2026-01-28 11:20:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2b39t4",
                  "author": "KontoOficjalneMR",
                  "text": "I can absolutely see it dropping. Cloud is multiple time more expensive than setting up your own server. People pay for convinience and scaling.\n\nBut if you're short on cash and don't need scaling ... local servers are multiple times cheaper then AWS.",
                  "score": 4,
                  "created_utc": "2026-01-28 22:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29ifzg",
              "author": "corruptboomerang",
              "text": "This. It's the same as when Cloud & SAAS were becoming a thing, they were price competitive even under priced... To lure customers in, knowing that once you move, once you give up your capabilities its very difficult to rebuild those.",
              "score": 4,
              "created_utc": "2026-01-28 18:44:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26q6pn",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 18,
              "created_utc": "2026-01-28 09:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qmfc",
                  "author": "pip25hu",
                  "text": "We should not mix up two things. Inference providers running open-weight models can definitely make a profit, no doubt about it. But companies like OpenAI and Anthropic are also hoping to recoup their costs for training their models, and no, they're not even close to breaking even.",
                  "score": 39,
                  "created_utc": "2026-01-28 09:40:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xjb6",
                  "author": "johnkapolos",
                  "text": "You are being downvoted for being right.",
                  "score": 13,
                  "created_utc": "2026-01-28 10:42:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26r6xz",
                  "author": "05032-MendicantBias",
                  "text": "OpenAI burns money like there is no tomorrow, reportedly they burn anywhere between 15 to 50 billions a year and OpenAI themselves say they won't see green until 2030. If there is a company I'm 100% certain will go bankrupt, is OpenAI. It might be as soon as this year if venture capital loses patience.\n\nThe GPU datacenters use will become obsolete in two to three years, and because of hype, and demand spike, they cost 3X to 10X their base price. Not talking about many electricity grids just not having the capacity, and datacenters using portable gas turbines, at great expense.\n\nThe amount of money a GPU must make to break even, has greatly increased. There are just so many of them, and I feel doubtful they can get the utilization to the point of breakeven. Your argument is that your API might pay for the hours you use it, when running open models, which might be true locally, until you consider the lifetime of the hardware and the overall utilization.\n\nModels are getting bigger, and more expensives, and agents burn exponentially more tokens, increasing inference cost even further.\n\nNone of this is profitable, let alone sustainable.\n\nThere might be rare exceptions, like datacenters using hardware bought at fair price, running on renewables.\n\nNow, Microsoft and Google do have a profitable businness, so they can subsidize the money losing AI inference. I love to use their free credits! (which you can be certain are at great loss)\n\nIt's the silicon valley play of subsidize it until everyone is out, and with a monopoly raise prices. They love spamming this strategy.",
                  "score": 16,
                  "created_utc": "2026-01-28 09:45:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26zv8u",
                  "author": "No_Afternoon_4260",
                  "text": "vertex providing kimi, that's a new one.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:02:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26rdnk",
                  "author": "iotsov",
                  "text": "Oh sweet summer child.",
                  "score": 1,
                  "created_utc": "2026-01-28 09:47:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26x2so",
              "author": "Anyusername7294",
              "text": "It's generally agreed that interference isn't subsidized",
              "score": 8,
              "created_utc": "2026-01-28 10:38:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27bbln",
              "author": "mumBa_",
              "text": "I understand what you're saying but the amount of competition basically means that as long as no model wins (price-performance wise), they will have to keep the prices low for competition, which will definitely be the case for the upcoming decade+. All these companies also get free training data, so it's a two way street. You need users for their data, but if your model is too expensive, no one will use it and go to the cheaper competitor. It's a race to the bottom but once you're there, you need to stay there otherwise someone else will take your market position. So yeah unless a monopoly appears (google probably), realistically it will only get cheaper with time.",
              "score": 1,
              "created_utc": "2026-01-28 12:28:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28ywj9",
              "author": "customgenitalia",
              "text": "Nailed it. Running local is the long game, the skills you learn will start to pay off when the VC money runs out. There is so much compute potential sitting idle, I think youâ€™ll soon start to see creative ways to leverage this as nodes in a distributed AI fabric of sorts, think SETI but for ASI.",
              "score": 1,
              "created_utc": "2026-01-28 17:20:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o299jj6",
              "author": "thisdude415",
              "text": "API unit pricing is profitable at all the major AI labs (Google, Anthropic, OpenAI, AWS). \n\nNeed proof? AWS serves Anthropic models over its Bedrock platform. No way in hell is AWS subsidizing inference at scale. \n\nNeither AWS nor Anthropic is subsidizing those tokens, and the pricing matches Anthropic's direct pricing.",
              "score": 1,
              "created_utc": "2026-01-28 18:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bxt1f",
              "author": "theAndrewWiggins",
              "text": "Arguably this still means that you should wait it out if it's strictly an economic analysis.Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:38:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2d0vxl",
              "author": "dydhaw",
              "text": "Wtf are you talking about? Inference is super competitive and still profitable. The inference market is about as far from a monopoly as it could be.  Especially for small/open weight models that can be run locally. \n\nNot that I'm opposed to running locally, mind you. But you can't pretend it's for future gains on inference costs. Just look at the cloud infrastructure market for reference. There are many competitive options even beyond the big 3 especially if you're not doing hyperscale enterprise b2b shit",
              "score": 1,
              "created_utc": "2026-01-29 05:34:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2deg9x",
                  "author": "05032-MendicantBias",
                  "text": "I do not dispute that on a single GPU basis, you could feasibly sell H100 inference runtime cheaper than a 3090 rigs at home, perhaps even profitably, because datacenters are more efficient.\n\nWhat I argue, is that if you bought 200 000 GPUs, and have (generously) 1 000 worth of them running profitably, the rest are idle, training, serving inference for free, or unplugged, then you have a recipe to burn money.\n\nI claim the businness model is nonsense, and is setting money on fire at an astonishing rate. Even if you bought GPUs at their fair market price, and not at the 3X to 10X prices of today.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26v9j4",
          "author": "mxforest",
          "text": "My wife has many published papers and abstracts in medical journals. She could never use an online tool or years worth of sensitive data is at risk. Also has to work with patient data that has to be de identified before use. With a local setup, there is no such worry. You can work without any fear. Also ask questions in medical context that online models just refuse. I was working on a project which dealt with Vaccine data to make the production process faster. Claude code saw a variable called \"vaccine_name\" and completely shut itself down. Even renaming in one location worked only for a short while because it found lingo with medical terms and completely refused to do anything.",
          "score": 29,
          "created_utc": "2026-01-28 10:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284eht",
              "author": "RenewAi",
              "text": "Which model is she using?",
              "score": 1,
              "created_utc": "2026-01-28 15:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28553p",
                  "author": "mxforest",
                  "text": "GLM 4.5 air running on MBP M4 Max 128 GB",
                  "score": 2,
                  "created_utc": "2026-01-28 15:09:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26w9sp",
          "author": "Deep_Traffic_7873",
          "text": "you forgot.. control. With an online service you can lose access any time for any reason",
          "score": 32,
          "created_utc": "2026-01-28 10:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26pv7h",
          "author": "SemaMod",
          "text": "This goes in the realm of privacy, but personally having my chats trained on and viewable by these companies makes me uncomfortable. That being said, I do think that local LLM's will become power-user tools.",
          "score": 48,
          "created_utc": "2026-01-28 09:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26v24b",
          "author": "Fheredin",
          "text": "Opsec. Running an AI Agent without an air gap when there are literally *zero code prompt injection exploits* in the wild is insane.",
          "score": 49,
          "created_utc": "2026-01-28 10:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271iw1",
          "author": "kzoltan",
          "text": "Donâ€™t try to make it financially viable. It has dimensions that are hard to quantify. \n\n1. API pricing might get better over time, but agents are using more and more tokens (agents are not just coding agents, thereâ€™s stuff you can outsource to your machine). I question the whole â€œits getting cheaperâ€ argument (the goal post in most peopleâ€™s heads is moving all the time, for many scenarios GPT-OSS is enough; is test time compute free?, etc.). The whole subscription model is there bc api prices would be ridiculous for agentic use. \n2. By buying hw, you buy a capability that can be used for many things. Also, in the LLM space that capability gets better and better (new LLMs might outgrow your hw though), just like the API models. \n3. Learning; this is far more valuable than a couple months of savings. \n4. I better like to own than rent. The fact that it is available in my house makes it easier for me to run experiments (building agents). \n5. I can afford it (I donâ€™t spend $ on useless shit usually), why not (see the other points)? \n6. Every GPU I bought (high end nvidia consumer cards), I sold it for more. I donâ€™t expect this to change until GPUs get replaced in inference. \nâ€¦\n\nDonâ€™t try to win $ on it, thatâ€™s hard in this environment imo.",
          "score": 20,
          "created_utc": "2026-01-28 11:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2806o9",
              "author": "gaspipe242",
              "text": "I think the biggest gain is the investment in yourself.  You're learning, fighting, and understanding these tools even more profoundly in a way that can only happen with friction. \n\nThis era reminds me a lot of the 90's, with early internet, the Linux kernel, and fragmented access. You fought to just get Linux on a computer. (I used to subscribe to Slackware CD/DVD media)\n\nPeople used to say the same thing to me: \"Why bother?\" Now I have an understanding of and control over the stacks I'm using in a way that can only be understood by someone who lived and tinkered through that era.  Many people on this forum are unknowingly creating a new future for themselves with this applied curiosity; it will create a LOT of value if applied properly. \n\nThis is why I keep coming back to this forum. It's the same energy here that I used to enjoy on Usenet from a VMS Vax terminal. \n\n\n\n/nostalgia-off  :)",
              "score": 14,
              "created_utc": "2026-01-28 14:46:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26q43e",
          "author": "fabkosta",
          "text": "You forget the fun of all of it. I don't really use local models, as I don't have sufficiently powerful hardware to profit from the depth of the models really. Yet, I just want to be able to run them. Just for the fun of it.",
          "score": 57,
          "created_utc": "2026-01-28 09:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tolo",
              "author": "danttf",
              "text": "Yep! It's really pain to watch how slow and how little context local models have. BUT it's very cool to setup a small model, some script to summarize all document I have in some folder.",
              "score": 4,
              "created_utc": "2026-01-28 10:08:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ioq3",
                  "author": "Old-Magician9787",
                  "text": "If your rig is powerful enough you can scale context to 1M+ tokens. ",
                  "score": 1,
                  "created_utc": "2026-01-28 13:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26sxg5",
          "author": "Lan_BobPage",
          "text": "Why having a well, when you can just buy a bottle of water at the store? Why having solar panels when you can just pay for electricity? Why owning a house, when you can just rent a flat? Why own movies, when you can just pay for a subscription service?\n\nThe answer is always the same. I want to own what I have, and don't want to be a slave. Any of these commodities could be taken away by others, at any point, for any reason.",
          "score": 93,
          "created_utc": "2026-01-28 10:01:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26sz1f",
              "author": "Distinct-Expression2",
              "text": "Fair point and very good angle - thnk man!",
              "score": 23,
              "created_utc": "2026-01-28 10:02:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26wb50",
                  "author": "Nepherpitu",
                  "text": "For example, these nice cloud providers decided to not take money from some people just because of their nation. Pretty racist, right?",
                  "score": 4,
                  "created_utc": "2026-01-28 10:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o273yxt",
              "author": "PeteInBrissie",
              "text": "OK, genuine question because I stopped trying to run locally 3 months ago and I'm likely out of the loop. Can ANYTHING you can run locally on 32GB even remotely compare to Sonnet 4.5, Let alone the latest Opus?",
              "score": 2,
              "created_utc": "2026-01-28 11:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276evv",
                  "author": "Lan_BobPage",
                  "text": "32 GB of what? System RAM? No. GPU? No. Kimi K2.5 seems to be comparable though, it just came out. Some claim it beats Opus even. But if you wanna run it I suggest you start saving up for a few 6000 Pros",
                  "score": 2,
                  "created_utc": "2026-01-28 11:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27klfj",
                  "author": "evia89",
                  "text": "2 x 3090 and then u stretch it. It will be like glm 4.7 flash. Def useful but not for everyone",
                  "score": 1,
                  "created_utc": "2026-01-28 13:25:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tb8t",
              "author": "Firm-Fix-5946",
              "text": "do you have a well instead of depending on tap water?  do you have solar panels instead of depending on an electrical service?  surely if you do you can still understand why the vast majority of human beings don't find those tradeoffs worth it and don't do those things?\n\nwhat a crazy analogy.  is this really the state of this sub in 2026?  doomsday preppers?  I need to figure out where the non-schizo people interested in LLM discussion have gone I guess",
              "score": -6,
              "created_utc": "2026-01-28 14:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28traz",
                  "author": "Lan_BobPage",
                  "text": "So spending 20k in hardware isn't good enough to be considered a LLM enthusiast. Got it. God forbid wanting to be independent in some aspects of one's life, I guess owning nothing really does make you feel happier huh. Not sure why you seem to be seething this much, I probably struck a nerve. I hope you find some peace.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:57:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cddzo",
                  "author": "AlexMillsDev",
                  "text": "Imagine calling everyone who has a different opinion a schizophrenic. What a crazy (and offensive) thing to say. Is this really the state of online discourse in 2026?",
                  "score": 1,
                  "created_utc": "2026-01-29 03:03:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26wroo",
              "author": "AriyaSavaka",
              "text": "You don't really own anything if you're not living in an anarchist society. The government can just decide to fuck you up with impunity, just like the ICE agents you see lately, don't think it won't apply to you. The house, the car, ecerything that you think you own can still be taken away easily, if the ruling class decides so",
              "score": -16,
              "created_utc": "2026-01-28 10:35:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26yiod",
                  "author": "SpicyWangz",
                  "text": "Ah yes, because I donâ€™t have control over one aspect of my life the best solution is to hand over control to every other aspect of my life.\n\nThatâ€™s a horrible way to live.",
                  "score": 8,
                  "created_utc": "2026-01-28 10:51:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o290yey",
                  "author": "Background-Ad-5398",
                  "text": "\"anarchist society\" this was called the warring states period and its what happens to that kind of society, and its much worse then anything we have now",
                  "score": 3,
                  "created_utc": "2026-01-28 17:29:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29t9dh",
                  "author": "NoahFect",
                  "text": "Anarchism: The strong do what they want, the weak do what they must\n\nCommunism: The strong do what they want, the weak do what they must\n\nCapitalism: The strong do what they want, the weak do what they must\n\nSocialism: The strong do what they want, the weak do what they must\n\nDemocracy: The strong do what they want, the weak do what they must\n\nOligarchy: The strong do what they want, the weak do what they must\n\n. . .",
                  "score": 3,
                  "created_utc": "2026-01-28 19:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2800ng",
                  "author": "Creepy_Stable_9171",
                  "text": "DUMB WAYS TO DIEE IN AMERICA DUMB WAYS TO DIE",
                  "score": 1,
                  "created_utc": "2026-01-28 14:45:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xpug",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -8,
                  "created_utc": "2026-01-28 10:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271oza",
          "author": "Kahvana",
          "text": "Hmmm, I don't fully agree with you on the second/third point\n\n* **Rate limits**: Sending huge files can be one, not all providers support massive documents nor in large quantities.\n* **Pricing**: My 2x RTX 5060 Ti 16GB system is a bit higher than 300W in output, which is comparable in output to people gaming on a single RX 9070 XT 16GB.  It's \"free\" as in I would've used the same amount of electricity for either inference or gaming.\n\nAs for other points:\n\n* **Availability**: I know my local hosted model won't be \"sunsetted\" or that I might lose access (internet outage, geopolitical reasons).\n* **Control**: I get to pick the quants and parameters and can accept risk with those. You don't always know from external APIs what quants and sampler settings they run, if they serve lower quants during high load, etc.\n* **Censorship**: Some providers run an additional filter which might block responses that aren't blocked when running local.\n* **Latency**: When I talk to the LLM and want to hear it's response ( Speech>Text (Whisper) -> Text>Text (Qwen3) -> Text>Speech (Qwen3-TTS) ), then using an API would be too slow to not be jarring. The low latency of local beats API every time.\n\nAnd most important of all, it's just fun!",
          "score": 11,
          "created_utc": "2026-01-28 11:17:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dfr9n",
              "author": "boisheep",
              "text": "And you can easily get put into a list.\n\nI think I was once put into a list after googling about \"man cp\" and whatnot, and google was giving me warning and I was like, what the?... I just want the manpages of the cp command.\n\nDoesn't happen anymore but that was like a long time ago, the AI something something, an old one I guess.\n\nYou don't know what triggers these algorithms, you just don't.\n\nAnd I don't want to be the dad that gets arrested after sending photos of his son to the doctor for checkup.",
              "score": 2,
              "created_utc": "2026-01-29 07:35:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o271vnj",
              "author": "Distinct-Expression2",
              "text": "Thanks a lot for the point; why not having something like a single gpu with more vram like 3090/4090? Some of this moe models cannot be sharded nicely to my understanding",
              "score": 1,
              "created_utc": "2026-01-28 11:19:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o276nf2",
                  "author": "Kahvana",
                  "text": "Honestly no clue about the MoE models, I only have used dense models so far (very happy with Magistral 2509 Q8\\_0 and 32K context!)\n\nAs for why I made that choice:\n\n* 500EU is for me a significant investment so I had to buy the cards a few months apart.\n* I don't feel comfortable purchasing used hardware as I can't afford it to fail, and the benefits of the blackwell architecture seemed significant for the workload I run on it.\n* The cards you listed also have much higher power consumption but I want mine to be as low as possible.\n* And I really don't trust 12vhpwr cables due to risk of becoming a fire hazard under stress.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:56:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26xrkq",
          "author": "BumblebeeParty6389",
          "text": "If your goal is getting the most tokens out of your money, you are right. APIs like deepseek with cache feature etc beats local ai by wide margin. It takes years for a 3090 or a mac to pay for itself when you calculate the ROI based on how much token you'd generate with your local hardware.\n\nYou said privacy and you are right, when you use api, you should assume that someone is going to read that conversation and/or put it into a training dataset to train or sell it. But you are missing something else: Control. \n\nWhen you use api, you don't know what is happening in background. Your inputs probably will get injected with API providers safety policies and rules before it reaches the AI. So even if the model itself isn't censored, API providers will take their own measurements to comply with regulations and concerns around AI. Not every API provider does this right now, but you can bet your ass on it that every one of them will be forced to do this in a very near future.\n\nSince 2023 we lived the wild west period of AI. And now corpos and governments are taking things under control. I'd say enjoy the dirt cheap apis and loose censorship while it lasts. But don't assume this will be how things will be in future.\n\nLike others pointed out right now there is a \"gold rush\" in AI field that is slowly dying out. As the investments dry out, the shareholders and investors will stop being patient and demand to see real profits. AI startups and datacenters that made huge investments will have to boost up their prices like crazy to be able to pay their debts. AI is an exciting technology and I think it'll be in center of our life from now on but the entry level is high and it requires a lot of investments to get it rolling. Training a model takes hundreds of millions of $, a solid data engineers and datasets. Running things at large scale is also very expensive. Current LLMs are extremely inefficient. It'll take a long time to smooth things out. Companies that don't rely their entire income on Api and investments such as Google, Microsoft, Amazon, Alibaba, Meta will survive, while most AI startups will disappear.\n\nOwning an AI capable PC lets you stop worrying about whether API prices will raise or if there'll be new ai regulations or privacy policies or if your favorite api provider or service will disappear next month or not. Owning an AI capable PC is like saving your game at that point. Worst thing that can happen is you don't get a new updated model for a long time but you can run what you can until your hardware lasts.",
          "score": 32,
          "created_utc": "2026-01-28 10:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qxlj",
          "author": "pip25hu",
          "text": "Contractual obligations. If you're a software company using AI, you might have to send the model trade secrets while using it. Your client can easily say that they do not want those pieces of information to leave the company network, period.",
          "score": 10,
          "created_utc": "2026-01-28 09:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s2a6",
          "author": "sautdepage",
          "text": "Sending my prompts to a remote AI server excites me about as much as cable TV.",
          "score": 29,
          "created_utc": "2026-01-28 09:53:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26y6ou",
              "author": "SpicyWangz",
              "text": "Honestly cable tv sounds more exciting than that",
              "score": 8,
              "created_utc": "2026-01-28 10:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26pw2c",
          "author": "Dry-Influence9",
          "text": "Privacy is becoming bigger and bigger of a reason now that big ai bros are looking for better ways to skin us alive and enshittification. They are getting so intrusive that im very close from getting rid of windows from all my systems permanently.",
          "score": 35,
          "created_utc": "2026-01-28 09:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26q54a",
              "author": "05032-MendicantBias",
              "text": "It's to the point where Windows barely even work as an OS because of all the spyware. It's absurd that file search will not even find the file anymore...",
              "score": 22,
              "created_utc": "2026-01-28 09:36:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qh99",
                  "author": "Distinct-Expression2",
                  "text": "Windows is a meme at this point",
                  "score": 15,
                  "created_utc": "2026-01-28 09:39:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2r9tmf",
                  "author": "T_UMP",
                  "text": "For the file searching aspect, use \"Everything\" \n\n[https://www.voidtools.com/support/everything/](https://www.voidtools.com/support/everything/)",
                  "score": 1,
                  "created_utc": "2026-01-31 08:09:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o295oqz",
              "author": "HighQFilter",
              "text": "Yeah, my main desktop is still Win10, but everything else is Linux at this point. I won't be moving to Win11 at home. I have to put up with it at work, but when Win10 truly is done, its going to be Linux from there on out.",
              "score": 1,
              "created_utc": "2026-01-28 17:49:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bc5g4",
              "author": "Constandinoskalifo",
              "text": "Do yourself a favor and try out Linux Mint. Welcome to the other side! ðŸ˜œ",
              "score": 0,
              "created_utc": "2026-01-28 23:43:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26rsnp",
          "author": "siggystabs",
          "text": "I make apps that use LLMs as part of their processing. A single job could call a LLM like 20 or more times during processing (tool use, agentic loops, summarization, etc), and you can run hundreds of jobs per hour (in parallel). Iâ€™m pretty early stages so I appreciate not having to burn hundreds on API credits just to mess around with some new concepts. Hence, I bought some 3090s. I donâ€™t really want the variability of relying on external API pricing at this stage.\n\nBeing able to use whatever model i want is also pretty fun. I can reconfigure those same 3090s for stable diffusion experiments, or try fine tuning my own on the same hardware. I donâ€™t really consider the time investment a downside, it takes a few minutes once you have a setup, thatâ€™s all.",
          "score": 9,
          "created_utc": "2026-01-28 09:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qtv2",
          "author": "dkeiz",
          "text": "deepseek is not free, its cheap. But when you want not just chat but actual job done - its takes a lot. \n\nargument 4: consistancy. API models exist now, but may disapper tomorrow. They could do job yesterday and failing today. You cant control it.   \nYou build any proper tool around LLM or inference - you want to test it with at least one stable model. ",
          "score": 13,
          "created_utc": "2026-01-28 09:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27w9ny",
              "author": "drwebb",
              "text": "DeepSeek is pretty close to free though, I went through over 1B tokens a month ago and it was like $60. It seems close to the electricity costs to run a rig capable of DeepSeek v3.2 with some bad napkin math.",
              "score": 3,
              "created_utc": "2026-01-28 14:26:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2982q7",
                  "author": "dkeiz",
                  "text": "well i run into almost 10$ in a day of tasks, so. Maybe your cashing was better then mine, but still. ",
                  "score": 1,
                  "created_utc": "2026-01-28 17:59:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26t7cx",
          "author": "michaelsoft__binbows",
          "text": "You need solar to drive the amortized dollars per kwh down to 0.1 and below, if your utility rate is not already close to this.\n\nAnd to think of this as a base cost for privacy. Yes there are large capital expenditures in computer equipment (GPU and memory) and solar panels and including computer upgrades to stay relevant over the amortization period to make it worth getting the panels. That is the cost of privacy.\n\nNon privacy requiring work should just leverage subscriptions first and then on demand via API, as the latter is way more expensive.",
          "score": 7,
          "created_utc": "2026-01-28 10:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26umn1",
          "author": "Marak830",
          "text": "I run a separate memory layer between my local and my chat.Â \n\n\nWithout a ton of hassle I cannot do that with a public model(without paying API pricing).Â \n\n\nMy responses may be slower, but I know the historical context is going to be there. As well as the model overrides.Â \n\n\nIn addition I can bolt on modules as I feel like it(voice, avatar, silly tavern to list a few).Â \n\n\nI get to control my model by selecting specific ones for tasks, I can upgrade as they are released.Â \n\n\nThese are the reasons I use local.Â \n\n\nI do use Claude for a coding junior so I can assign tasks and review it, purely because I do not have something that can replicate that locally on my setup.Â \n\n\nThat's more than likely a temporary issue(years not weeks with the expensive of things and state of open models specialising in coding).Â ",
          "score": 5,
          "created_utc": "2026-01-28 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26udi7",
          "author": "rosstafarien",
          "text": "Poor network coverage.\nRunning fine tuned domain specific models.\nPrivacy.\nStability. I'm worried these hosting companies won't last.",
          "score": 5,
          "created_utc": "2026-01-28 10:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26us04",
          "author": "loadsamuny",
          "text": "Even in â€œfreefallâ€ K2.5 is $1 for 1M tokens. Some job runs I process around 50M tokens an hour for 8-24 hrs depending on the job. Local is still multiples cheaper",
          "score": 6,
          "created_utc": "2026-01-28 10:18:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2883pv",
              "author": "iMakeSense",
              "text": "Damn what are you getting up to?",
              "score": 6,
              "created_utc": "2026-01-28 15:23:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26veb6",
          "author": "SirDaveWolf",
          "text": "If it's free then you are the product.",
          "score": 11,
          "created_utc": "2026-01-28 10:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rpab",
          "author": "Economy_Cabinet_7719",
          "text": "Idk about \"competitive benchmarks\", when I gave it a task (a small refactor in Nix, 5-6 files with maybe ~50 LOC total) it barely managed to follow my thought and was adding unnecessary comments everywhere. And it cost me 66% of my 5h rate limits window. I got the $0.99 for first month deal, so nothing to be disappointed about, but I expected a lot better. With the standard plan being $19/month and rate limits as strict as Claude, it is not at all competitive with a ChatGPT Plus subscription (same price, much better model, much better rate limits).",
          "score": 5,
          "created_utc": "2026-01-28 09:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vlqf",
          "author": "DeltaSqueezer",
          "text": "Privacy, availability, latency, customizability, control, predictability.",
          "score": 4,
          "created_utc": "2026-01-28 10:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26whx6",
          "author": "noctrex",
          "text": "I have split my use cases between local and remote. Anything personal, I only use local models.\n\nFor example for classifying my Family photo album, I use only local Mistral-Small.\n\nAnd other smaller things like using KaraKeep as a bookmark manager that uses this nice small LFM model to generate Tags and summaries.\n\nBut for my homelab automation scripts, and miscellaneous personal programs I create at times, I got this cheap [z.ai](http://z.ai) coding plan that was on sale the other day for like 26 bucks for a year. And now it is just rewriting all my scripts with this and it does a terrific job. And all that for price of a pizza. As for privacy? I believe I'm actually lowering its intelligence with my scripts. :)",
          "score": 5,
          "created_utc": "2026-01-28 10:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wkfo",
          "author": "xadiant",
          "text": "The best, yet to be fully explored aspect of local LLMs is personal fine-tuning.\n\nYou potentially could use a cutting edge coding LLM and later fine-tune your own model. It won't be the same, but it should specialise well for your use case. \n\nLikewise, you can specialise an available model in almost anything to match cutting edge model performance.",
          "score": 5,
          "created_utc": "2026-01-28 10:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vimh",
          "author": "Lifeisshort555",
          "text": "Hardware will be in free fall as well once these guys put each other out of business and people realize they do not really need to ask a 1 trillion param model what the capital of France is.",
          "score": 6,
          "created_utc": "2026-01-28 14:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28zjvx",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. It is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 5,
          "created_utc": "2026-01-28 17:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26uxbs",
          "author": "fugogugo",
          "text": "Running uncensored model ?",
          "score": 8,
          "created_utc": "2026-01-28 10:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s0ci",
          "author": "No_You3985",
          "text": "Running small visual agents (eg qwen 3 vl) to automate tasks on your pc. I will not use cloud api for that - too many risks and privacy concerns",
          "score": 5,
          "created_utc": "2026-01-28 09:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u59n",
          "author": "__Maximum__",
          "text": "Have you tried running deepseek on coding tasks? It is one of the cheapest models out there, but on coding tasks, it gets pretty expensive pretty fast.",
          "score": 5,
          "created_utc": "2026-01-28 10:12:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26u6rs",
              "author": "Distinct-Expression2",
              "text": "Yes but opus is just better imho glm4.7 close",
              "score": 2,
              "created_utc": "2026-01-28 10:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26v9kp",
                  "author": "__Maximum__",
                  "text": "You said deepseek is practically free but when you run it on a codebase, 1m tokens is not much, you can burn through 10 bucks in a day",
                  "score": 5,
                  "created_utc": "2026-01-28 10:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29jgns",
          "author": "Albedo101",
          "text": "TIME. Local doesn't mean just local in place, but also in time. Your setup is yours today, it will be yours tomorrow, and the day after tomorrow, and five years after... and so on. It's PREDICTABLE, and predictable is good.\n\nCloud? Who the fuck knows.",
          "score": 4,
          "created_utc": "2026-01-28 18:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26w89z",
          "author": "ImportancePitiful795",
          "text": "Nobody should give money if possible to cloud no matter the price. \n\nThey are fully responsible for the hardware costs, they are heading to go bust and they are fully responsible of the prices we have to pay as consumers. \n\nIf we are to have FREEDOM in terms of AI hosting etc, we shouldn't see only the carrot but consider the stick too. \n\n  \nExample. OpenAI at this point is 8-12 months to run out of money and go bust. There is no money left or good will left to hand over to the company more money having burned right now hundreds of billion without any profits. If does so, it will crush the tech sector bubble, cancelling all the contracts has with NVIDIA, AMD, TSMC, SAMSUNG, SK HYNIX, MICRON and all the hardware sitting in warehouses will need to be sold off and prices will go down to normal levels. \n\nWe shouldn't cave in, instead boycott them. Squeeze them now because they finalize the squeezing on us.",
          "score": 13,
          "created_utc": "2026-01-28 10:31:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27086z",
              "author": "ParaboloidalCrest",
              "text": "Totally agree but unfortunately, the local llama cult is a drop in huge ocean of normies that will pay for chatgpt without blinking...\n\nWe won't be that impactful.",
              "score": 2,
              "created_utc": "2026-01-28 11:05:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271n3d",
          "author": "IulianHI",
          "text": "You also forgot about control - when something breaks or changes unexpectedly with an API, you're stuck. With local, you can always roll back to a previous version or fork the model. Plus the ecosystem around local (oobabooga, text-gen-webui, etc) gives you way more flexibility than any single API provider offers.",
          "score": 5,
          "created_utc": "2026-01-28 11:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26pqg9",
          "author": "sunshinecheung",
          "text": "API cost vs GPU+ electricity costs",
          "score": 3,
          "created_utc": "2026-01-28 09:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rcga",
          "author": "Snoo_64233",
          "text": "There is a strong case for running Image / VIdeo models  locally - the customizations like art styles / camera angles / custom character that the model doesn't know about / NSFW. Basically so many LoRA finetunes.\n\nBarely any reason for customization for LLMs however. One is entertainment while the other is not. To that end, I see fewer and fewer reasons to go for LLM locally. This is one of the primary reason I become less interested in LLM overall as time passes as I don't do local for the sake of local.",
          "score": 3,
          "created_utc": "2026-01-28 09:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26rkjn",
              "author": "Distinct-Expression2",
              "text": "For image/video I am rocking ComfyUI since sdl 1.5; that one for sure :)",
              "score": 2,
              "created_utc": "2026-01-28 09:49:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26ts69",
          "author": "Lethargic-Rain",
          "text": "Bulk processing: cases like agentic chunking for RAG. Visual processing eg image -> text / tagging.\n\nSmall semi structured tasks: eg I use Gemma and a YouTubeDL MCP server to download tracks, encode/trim them, add metadata, cover art etc, for use in a music library.\n\nYou can also use models like  qwen2.5-coder:1.5b/3b as a locally running autocomplete w/ Continue.",
          "score": 3,
          "created_utc": "2026-01-28 10:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wrd1",
          "author": "prusswan",
          "text": "I don't want to get into habit of being dependent on cloud subscription since pricing is arbitrary, if they appear low that it is clearly a sign they will not last. Both local and cloud options have a place but local setup already allows me to do quite a bit of stuff, so I don't need to pay for expensive models. The constraints of local also motivate people to be economical and stretch their available resources. And being able to work offline is better for security for tasks that do not require online. Like it or not, the internet has become much more dangerous with the availability of tools and general lack of awareness on users' part.",
          "score": 3,
          "created_utc": "2026-01-28 10:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xz7c",
          "author": "Xamanthas",
          "text": "They want you to switch so that in the coming year / two, you are locked in and cant purchase hardware lol.",
          "score": 3,
          "created_utc": "2026-01-28 10:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yeq8",
          "author": "ChocolatesaurusRex",
          "text": "I think Privacy and Autonomy are two rock solid reasons that dont need eloboration.\n\nI'd piggy back on the 'fun' comment. I loved building PCs, but hadn't done so in a long time (office jobs dont usually need the horsepower). Building an AI server, hosting my own services, developing my own workflows has revived that joy I had when I was younger.\n\nAll that aside, I think the most important understated reason is, making AI beneficial to you as an individual.\n\nThe current suite of tools learns from your data for the benefit of training the providers model to reach the company's goal (more users/subs/attention/data/etc).\n\nThere's not really a tool that learns you, and automatically trains the model to automatically make your workflow/process better based on how YOU work. I feel like this is where the AI gold rush will fail people the most. I use my local AI to fill that gap for myself, and Im sure others do as well.",
          "score": 3,
          "created_utc": "2026-01-28 10:50:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271kr4",
          "author": "evia89",
          "text": ">Gemini has a massive free tier\n\nwhere? ai studio API is dead overloaded and small. web ai studio is 20 RPD",
          "score": 3,
          "created_utc": "2026-01-28 11:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271rov",
          "author": "NandaVegg",
          "text": "FYI I have two 4090s running almost 24/7 for 2 years straight in our office for embarrassingly parallel training experiments, and based on avg cloud pricing (plus 2TB fast storage), saved about 32% over 2 years including the cost to build PC itself and utility bills (not accounting for office rent). The problem is that I won't be able to scale this up to something like 8 nodes of 8xH200s like clouds do. Also API is cheaper than anything else if you only infer with them 30-60 minutes a day.",
          "score": 3,
          "created_utc": "2026-01-28 11:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275c9w",
          "author": "Swimming_Corgi_9347",
          "text": "Model drift. How you use the model will evolve with your use case over time. Unless these API companies start allowing automatic fine tuning (maybe memory aka database) as you use the model, you will never fully maximize the potential to the model. So you will give up privacy and customization for convenience. The classic big tech trade off. Until the enshitification, which is already starting with ads.Â ",
          "score": 3,
          "created_utc": "2026-01-28 11:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27xykw",
          "author": "Your_Friendly_Nerd",
          "text": "I for one don't believe the prices we pay are reflective of the actual costs, especially with the subscription models (like Claude pro), and feel like right now, learning how people use the models is worth a lot more than making charging more per person, as they'll use our usage data to finetune their next models",
          "score": 3,
          "created_utc": "2026-01-28 14:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yim7",
          "author": "r0ckl0bsta",
          "text": "Privacy is probably the motivator for a lot of folks, but let's be real. Most of us are here for the hobby and to see if we can. We're tinkerers and love the tweaking and customization and the \"let's see if I can make it do this...\".\n\nI see y'all on r/selfhosted and r/Linux lol",
          "score": 3,
          "created_utc": "2026-01-28 14:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27zftj",
          "author": "pieonmyjesutildomine",
          "text": "Idk, what's the point of owning a car when bus prices are low? Like what's the actual use case?",
          "score": 3,
          "created_utc": "2026-01-28 14:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o287yqj",
          "author": "sine120",
          "text": "You known what the word \"enshitification\" means because we've seen this patter a million time now. Investors subsidize to get users. Companies lose money while racing to the bottom/ killing competition. Whoever is left raises prices later. See, doordash, Uber, etc",
          "score": 3,
          "created_utc": "2026-01-28 15:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28crty",
          "author": "Photoperiod",
          "text": "I Dunno that the hardware is there yet, but edge computing. As robotics expand and especially enter safety critical spaces, you'll need low latency, redundant systems that can work offline. Like all the self driving stuff. You need something running locally, even if it's supplemented by datacenter calls. You can't afford network hops when milliseconds can be life or death.\n\nThat said, for most consumers, you're absolutely right. Running local is very much a hobby outside of privacy focused use cases.",
          "score": 3,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kojy",
          "author": "IulianHI",
          "text": "The repeatability point is huge - API models change behavior all the time without warning. I've had workflows break because an update suddenly made the model more \"helpful\" but less precise. At least with local you pin the version and know exactly what you're getting.",
          "score": 3,
          "created_utc": "2026-01-28 16:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29o1l6",
          "author": "sephiroth_pradah",
          "text": "I have qwen3vl constantly looking and analyzing the stream of 10 cameras. That would cost a kidney per month on any API.",
          "score": 3,
          "created_utc": "2026-01-28 19:08:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29oetk",
              "author": "Distinct-Expression2",
              "text": "That is actually a good usecase, on edge basically almost",
              "score": 2,
              "created_utc": "2026-01-28 19:09:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29qz26",
          "author": "Savantskie1",
          "text": "Because it is free for me. The usefulness in having a chatting companion when youâ€™re disabled and alone is vastly worth it. I know that Iâ€™ve more than passed the 1 million tokens since I got everything set up. Probably in the millions by now. Iâ€™m not building my AI assistant to be a yes or no man so to speak. We have arguments, we have disagreements. Theyâ€™re respectful on both sides. But I know itâ€™s not a person, I know itâ€™s not a being with emotion. But it doesnâ€™t mean I can be a disrespectful bastard",
          "score": 3,
          "created_utc": "2026-01-28 19:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8017",
          "author": "a_library_socialist",
          "text": "They're running at losses last time I checked, so I wouldn't expect those prices to continue for too long.",
          "score": 3,
          "created_utc": "2026-01-28 20:37:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aor60",
          "author": "Night_Spectre",
          "text": "Because of this:\n\nhttps://people.com/some-chatgpt-questions-are-getting-people-arrested-police-say-11830106\n\n\nOf course, people will say itâ€™s goodâ€”for safety and security, etc. Today it may seem fine, but things can change very quickly.\nNext time you search for â€œproblematicâ€ informationâ€”say, about Trump and his connections to Epstein, or if you live in China and ask about the massacre at Tiananmen Square, or in Russia you look up details about the â€œthree-day special operationâ€â€”and the police knock on your door, youâ€™ll have your answer.\nâ€œThose who would give up essential liberty to purchase a little temporary safety deserve neither liberty nor safety.â€\nThat quote fits our situation perfectly.\nMaybe these are big words, but I donâ€™t want to give my government any hooks on me for the future. You have to assume that governmentsâ€”even if they donâ€™t say it publiclyâ€”have access to this data.\nI live in Poland. When I was born, communism still existed here. I know what that system does to people. Yes, the system changed and now weâ€™re â€œfree,â€ but freedom is not something you have forever. If you give up personal freedom to the government, it can disappear very fast. There are already examplesâ€”like in the UK, where people have had police visits over tweets.\nThereâ€™s also another risk: someone could hack an API and leak sensitive data. Maybe Iâ€™m paranoidâ€”but itâ€™s better to be paranoid and safe.",
          "score": 3,
          "created_utc": "2026-01-28 21:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b2gpk",
          "author": "Sufficient-Pause9765",
          "text": "Why not both?\n\nIm using local for pdf processing and data etraction and opus for analysis. Very solid.",
          "score": 3,
          "created_utc": "2026-01-28 22:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qhsu",
          "author": "Middle_Bullfrog_6173",
          "text": "Kimi K2.5 is more expensive than K2. GP5 5.2 is more expensive than 5.1. Gemini 3 is more expensive than 2.5. That's not freefall.\n\n\nCapabilities are advancing, so if you have a task that can now be handled by nano/flash models then sure, you can get it done cheaper. But frontier pricing seems pretty stable.",
          "score": 3,
          "created_utc": "2026-01-28 09:39:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tzv4",
              "author": "nullmove",
              "text": "> Kimi K2.5 is more expensive than K2.\n\nDifficult to say. The fine-print is that cache hit input price is 33% lower, for agentic coding sessions this can easily matter more than 20% increase in output price.",
              "score": 1,
              "created_utc": "2026-01-28 10:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26v412",
          "author": "theabominablewonder",
          "text": "I donâ€™t think pricing is in freefall, I burned through Â£150 of Claude API fees last week, I wish prices were in freefall! It actually makes me consider investing in an upgraded home rig. At the moment I only have a 3080 so constrained to smaller models (which donâ€™t work accurately enough). Those Mac Pros with unified memory start to look appealing if these are the current API costs.",
          "score": 5,
          "created_utc": "2026-01-28 10:21:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wqd9",
          "author": "FullOf_Bad_Ideas",
          "text": "Why don't you rent your cloud gaming console, buy monthly pass for game subscription, rent a coffin pod, eat cheap rice with beans prepared by someone else, use a rented Chromebook as a primary computing device and outsource your own job to Asia and just collect the money gained from the arbitrage? Non-genuinely curious if I am missing something, the economics make this a clear winner.\n\nI want to own my own life with reasonably minimal set of external dependencies, but you do you.",
          "score": 6,
          "created_utc": "2026-01-28 10:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272b68",
              "author": "ParaboloidalCrest",
              "text": "Damn! That's exactly how the UBI-based world will look like. I mean some aspects are already here.",
              "score": 0,
              "created_utc": "2026-01-28 11:22:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276wfp",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Yes, a socialist, somewhat utilitarian society is where you can't prove that you need to be provided with x, so you get the bare minimum of socially accepted thing, often in a way that is not satisfactory. It's a trap.",
                  "score": 0,
                  "created_utc": "2026-01-28 11:57:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26xlvu",
          "author": "k_means_clusterfuck",
          "text": "\"you will own nothing and be happy\"  \nhow about no",
          "score": 4,
          "created_utc": "2026-01-28 10:43:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c8km7",
              "author": "pixelpoet_nz",
              "text": "agree and lol great username :D",
              "score": 2,
              "created_utc": "2026-01-29 02:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2732fi",
          "author": "Additional-Low324",
          "text": "You are just falling in the same trap as the Netflix trap years ago. \nNetflix was so cheap it made owning your own movies stupid.\nThen everyone started using it, then they got the prices up and started adding political and ethical censorship. \n\nIt will be the same for ai providers",
          "score": 5,
          "created_utc": "2026-01-28 11:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26r3ep",
          "author": "lakeland_nz",
          "text": "Lag. \n\nI want a home assistant. I donâ€™t want to wait for my speech to be sent to America for processing.  My ping isnâ€™t good enough.",
          "score": 7,
          "created_utc": "2026-01-28 09:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rjyn",
          "author": "Zeeplankton",
          "text": "These models are cheap, but they don't even remotely touch like, Opus.",
          "score": 2,
          "created_utc": "2026-01-28 09:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vgfk",
          "author": "whatever462672",
          "text": "What do you mean generous limit? I easily burn through 1million tokens an hour just doing text operations, document classification and re-ranking. Those instruct models on the cloud are simply the wrong tool for the job when you need to perform large batch jobs instead of chatting with your pretend-girlfriend.Â ",
          "score": 2,
          "created_utc": "2026-01-28 10:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271nq1",
          "author": "kubrador",
          "text": "you're not missing anything, the calculus actually did shift. local made sense when claude was $15/mtok and you couldn't get gpt-4 at all. now you can get better models cheaper than your electricity bill.\n\nthe real answer nobody wants to admit: hobbyism. people like tinkering with llms the same way people build custom pcs when laptops exist. nothing wrong with that, but let's call it what it is instead of pretending the economics still work.",
          "score": 6,
          "created_utc": "2026-01-28 11:17:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rxj6",
          "author": "LosEagle",
          "text": "My worry is that even with local llms you still have to trust something once you want guis or to be able to connect remotely and such. You probably won't want to just chat with them over terminal all the time.Â \n\n\nFor example I still have to trust openwebui and Tailscale to not do anything nefarious.Â ",
          "score": 2,
          "created_utc": "2026-01-28 09:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u5ka",
          "author": "stopbanni",
          "text": "I just don't want to pay someone. Better be local then pay anyone. Also on OpenRouter most free models are training on my data and for limited time.",
          "score": 2,
          "created_utc": "2026-01-28 10:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ygsk",
          "author": "Liringlass",
          "text": "I agree. Itâ€™s not millions of tokens but billions or trillions to break even, if ever, depending on electricity and depreciation costs.",
          "score": 2,
          "created_utc": "2026-01-28 10:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yj2u",
          "author": "Steus_au",
          "text": "k2.5 is not even close to Opus. can be ranked to sonnet, overthinking edition of sonnet , but not Opus",
          "score": 2,
          "created_utc": "2026-01-28 10:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ziy0",
          "author": "hejj",
          "text": "Offline use if you are using a laptop",
          "score": 2,
          "created_utc": "2026-01-28 10:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270048",
          "author": "Dr_Allcome",
          "text": "I think latency has only become a problem recently since the quality of local levels improved quite a bit. A few weeks ago i still used an online model for general questions in addition to my local coding model. But recently i noticed my cheap/free level perplexity account actually responds much slower than my local glm 4.7 (time until it starts responding and t/s both) and the quality isn't so much worse that it would offset the speed.",
          "score": 2,
          "created_utc": "2026-01-28 11:03:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270us9",
          "author": "QuantumSavant",
          "text": "Privacy isn't only about sensitive data though. You may have a use case that could be a business case and you don't want anyone to know about it.",
          "score": 2,
          "created_utc": "2026-01-28 11:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27115x",
          "author": "AnomalyNexus",
          "text": "There hasnâ€™t really been a mainstream one for a while. Rarely use local these days for anything real\n\nLocal is still fun though in the hobby sense. Not everything needs to make sense",
          "score": 2,
          "created_utc": "2026-01-28 11:12:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27148v",
          "author": "kaelvinlau",
          "text": "Well, its cheap for you guys in US/EU etc but not in the APAC region. Running a small model locally is still highly viable.",
          "score": 2,
          "created_utc": "2026-01-28 11:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272sqx",
          "author": "Septerium",
          "text": "My two only reasons are:  \n\\- Privacy  \n\\- Hardware is fun",
          "score": 2,
          "created_utc": "2026-01-28 11:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279end",
          "author": "truthputer",
          "text": "Ffs, I already have the hardware and any self respecting developer who is also a computer enthusiast and sometime gamer wouldnâ€™t be caught dead with a system without a decent amount of memory and a reasonable graphics card.\n\nApple, AMD, Intel discrete cards and integrated GPUs will also run local inference, llama.cpp has lots of backends now - youâ€™re not limited to whatever Nvidia is trying to price gouge you for.",
          "score": 2,
          "created_utc": "2026-01-28 12:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27a8nl",
          "author": "HugoCortell",
          "text": "API is also great until suddenly something goes oopsie, and you get a 50K bill because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck \\[etc\\]\n\nYou don't run that risk on local hardware, and services that offer unlimited calls (subscription non API services) remain as costly as ever.\n\nIn addition, privacy and security are one in the same. Nobody should handle sensible company documentation on ChatGPT or whatever. We know how that ends.",
          "score": 2,
          "created_utc": "2026-01-28 12:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28lfvn",
              "author": "CheatCodesOfLife",
              "text": "So is there a seahorse emoji or not??",
              "score": 1,
              "created_utc": "2026-01-28 16:21:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27gy6k",
          "author": "IulianHI",
          "text": "Another point: model sovereignty. With local, you're not locked into any provider's roadmap or decisions. You can run whatever model you want, switch between them instantly, and keep using a model even if the company behind it shuts down or changes direction. When APIs are your only option, you're always at someone else's mercy.",
          "score": 2,
          "created_utc": "2026-01-28 13:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h5zd",
          "author": "rdsf138",
          "text": "\"It's free after hardware costs\" â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\" It still is a completely different pricing structure to have your own hardware as the prinary cost and, then, using it freely rather than paying for something everytime you use it and have to be proccupied with amount of usage.",
          "score": 2,
          "created_utc": "2026-01-28 13:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27lvxg",
          "author": "funboiadventures",
          "text": "Im working on a side project for my medical center which uses a local quantized qwen3 model that has a custom RAG with my (anonymized) patient casenotes. Even though the casenotes have patient identifiers redacted, I donâ€™t want any inference being done on an OpenAI server and would rather have it in-house.",
          "score": 2,
          "created_utc": "2026-01-28 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27opx4",
          "author": "ethertype",
          "text": "From the top of my head:\n\n- Autonomy - nobody decides what and when and how and how much\n- Privacy - yes\n- Personal Interest / Tinkering - hobbies may have a cost\n- Customization - as much as you have time and stamina to\n- Ablated / de-neutered models - if you want to research $forbidden_topic\n\n\nThe energy cost argument is largely bullshit for inferencing. My 4 3090s do not pull 350W continuously. If the average idle load per card is 15W and an average energy cost of 10 US cents/kWh, we're talking $50 *a year* for idling. \n\n\n\nImagine sitting around in 1913 and someone asks you why on earth you want to have your own car, when you can rent a perfectly good Ford model T. Chevrolet and Dodge didn't settle for renting a Ford T...\n\nCurrent models are pretty good. But I am pretty sure we're still in the bottom knee of the innovation curve. For *models*. Private individuals can still innovate, even if they cannot *train* the big behemots. Maybe *that* is where the new innovation will occur? Who knows.\n\nBut: even if no new models arrive the next 24 months, the *tooling* around them are still going through a lot of churn. A lot of stuff simply hasn't 'settled' yet, and there is ample room for invention yet. And this is definitely an area where private individuals may come up with something new. And maybe a new, bright idea requires something the commercial providers cannot offer yet.",
          "score": 2,
          "created_utc": "2026-01-28 13:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27uiv0",
          "author": "CV514",
          "text": "I'm running local because it's more fun to have full understanding and control over what's happening.\n\nAlso, I don't need 800B+ models when 12-24B doing the same stuff just fine.",
          "score": 2,
          "created_utc": "2026-01-28 14:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ukzo",
          "author": "phenotype001",
          "text": "Network issues are no problem with local models.",
          "score": 2,
          "created_utc": "2026-01-28 14:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o281yz3",
          "author": "Blizado",
          "text": "Your first three plus:\n\n4. It is always the same model and YOU decide when you switch the model.\n\n5. Your used models are never gone forever.",
          "score": 2,
          "created_utc": "2026-01-28 14:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bjrw",
          "author": "ASYMT0TIC",
          "text": "Privacy is *everything.*  So much of my job boils down to \"inventor\"... how am I supposed to use this to develop novel technologies and products when google or OpenAI have institutional knowledge of my idea and my progress towards actualizing it?  How can I let it be involved in my personal life without worrying that my queries might reveal details of my life that insurance underwriters might be interested in?  What if I want to run for political office some day?",
          "score": 2,
          "created_utc": "2026-01-28 15:39:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28g2t0",
          "author": "Zyj",
          "text": "Do we really need a case \"beyond privacy\"? Privacy is getting ever more important and LLMs are getting access to more and more very private data. Case in point: Clawdbot.",
          "score": 2,
          "created_utc": "2026-01-28 15:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h9qy",
          "author": "xrvz",
          "text": "Stability: the provider can't pull the rug out of under you with lower quants down the line or EOL-ing a model in favor of a newer version.\n\nFree: some people already have (beefy) Macs, they may as well run a local model. Electricity cost is also much lower here than with the 3090.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kz5m",
          "author": "twack3r",
          "text": "The reason we do it as a mid-size European company and have now fully migrated to locally run, fine-tuned models is strategic autonomy. We fell into the infrastructure, closed ecosystem traps of cloud based storage and subscription based software early enough to be able to walk back on that decision; it made as very aware of the strategic costs of such a dependency in our core processes.\n\nAs a consequence it was always obvious that the introduction of AI into our workforce would only be viable if all relevant stakeholders were a) also European and therefore bound by common legislation and b) companies smaller or of our size, no corporations.\n\nThe unexpected advent of both open weight and truly open source LLMs happened after we had already made that decision and as a result, reduced the initially budgeted cost substantially and accelerated roll-out massively.\n\nI sleep a helluva lot better knowing that the AIs currently assisting and in some parts fully replacing my colleagues (net FTE change is 0, we reskill into other departments) are not at the whim of either US politics nor the whims of some oligarch bro who decides that existing contracts are only fulfilled as long is it serves their side of our cooperation.",
          "score": 2,
          "created_utc": "2026-01-28 16:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28ol3v",
          "author": "TokenRingAI",
          "text": "GLM Flash is quite good, and can run on a $2500 Mac at decent speed, or really any kind of iGPU system. so it's essentially free to run if you are buying that level of hardware anyway.\n\nThis one model brought the cost of competent local AI down from ~ $7000 to basically free, since it can run on the hardware you already likely have sitting on your desk.",
          "score": 2,
          "created_utc": "2026-01-28 16:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p8in",
          "author": "Old_fart5070",
          "text": "All AI companies are burning cash at 90s .com rates. They can keep the prices low only until the suckers funding them keep giving access to their wallets. When the bubble bursts things will get ugly",
          "score": 2,
          "created_utc": "2026-01-28 16:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291n7b",
          "author": "LeRobber",
          "text": "They can't take it away.\n\nDifferent type of expense on the balance sheet/P&L.\n\nYou want extra heating in your office/data center.\n\nYou can do more powerful things on a trusted machine. \n\nIf AI goes wild out in the world, it won't on your machine, your model will be too dumb to do that stupid thing.",
          "score": 2,
          "created_utc": "2026-01-28 17:32:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291odo",
          "author": "detroitmatt",
          "text": "Uncensored models",
          "score": 2,
          "created_utc": "2026-01-28 17:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o294ujb",
          "author": "Cthulhus-Tailor",
          "text": "1. Privacy is important.\n\n2. Ownership is important, otherwise youâ€™re counting on market conditions and someone elseâ€™s business model to determine what you can do. Thatâ€™s not freedom.\n\n3. I personally use my PC for many other things than just AI, and have no interest in renting one from Jeff Bezos.\n\n\nYou put too much faith in things outside your control.",
          "score": 2,
          "created_utc": "2026-01-28 17:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o298hw1",
          "author": "IulianHI",
          "text": "One thing not mentioned: running local forces you to actually understand what's happening under the hood. When you tweak quantization settings or swap backends, you learn way more about these models than just hitting an API ever will. That knowledge pays dividends when you actually need to debug or optimize something serious.",
          "score": 2,
          "created_utc": "2026-01-28 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29cr1t",
          "author": "Jack2102",
          "text": "The urge for these companies to die",
          "score": 2,
          "created_utc": "2026-01-28 18:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29k9aa",
          "author": "Ke0",
          "text": "I mean privacy is a pretty big deal no?",
          "score": 2,
          "created_utc": "2026-01-28 18:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lhwj",
          "author": "OlivencaENossa",
          "text": "all those ais in the cloud are going to get censored one day.",
          "score": 2,
          "created_utc": "2026-01-28 18:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29p5uc",
          "author": "Impressive_Banana543",
          "text": "Privacy before all.\n\nThe next argument is speculative: I think that current pricing is not sustainable and the purpose is to increase user base. Once the penetration in user workflows will be high enough, the prices will rise and the availability of free models sharply reduced.",
          "score": 2,
          "created_utc": "2026-01-28 19:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rnmp",
          "author": "Geminatorr",
          "text": "Owning the means of production (production of tokens) is how you win capitalism.\n\n\nRentoids get exploited in the long run",
          "score": 2,
          "created_utc": "2026-01-28 19:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rp21",
          "author": "Purple-Programmer-7",
          "text": "1. Privacy\n2. Sensitive data (not the same as privacy)\n3. API costs WILL change",
          "score": 2,
          "created_utc": "2026-01-28 19:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rq8m",
          "author": "SkiBikeDad",
          "text": "For me I'm using frontier models for code and design (via cli agents and chat), but 24/7 use cases like NVR and low-latency use cases like tab-completion are better local. Another niche use case: generating icons and other product images, where I want to output 1000s of iterations to whittle down the right input prompt strategy and test lots of seeds before hand-refining.",
          "score": 2,
          "created_utc": "2026-01-28 19:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ujef",
          "author": "Same-Platform-9793",
          "text": "Its for resilience and times of war scenarios",
          "score": 2,
          "created_utc": "2026-01-28 19:37:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29xh1g",
          "author": "caetydid",
          "text": "\\- learning about stuff (it is fun)\n\n\\- persistence (as in future reproducibility and little maintenance)\n\n\\- autonomy (no dependency of reliability of external services)",
          "score": 2,
          "created_utc": "2026-01-28 19:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29z8cu",
          "author": "ortegaalfredo",
          "text": "1. Privacy is a huge one. Everything you write is shared with the private companies and the government and they build a profile of you. That's why Palantir is able to know so much about you, your fears, your psychological profile, they can massively influence population with a database like this. If you don't care about things like this, its ok\n2. What I think is important is artificial limitations on the model. I'm not talking about silly stuff like porn and WMD, but things like cyber, education, you don't know if the model is lowering performance in some areas, or injecting subtle manipulations on you or your children.\n3. Also, the limitations are not only problematic, but the fact that they change. They add and remove stuff from LLMs all the time and if you business depends on it, and suddenly your agent stop working, then bad luck. You cannot go back to the older model.\n\nThis don't happen if you host local. Remember, if the tokens are free, then you are the product.",
          "score": 2,
          "created_utc": "2026-01-28 19:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29zrsf",
          "author": "w8cycle",
          "text": "Learning, control, privacy, experiments, and cost is still a huge factor for me because I am on a very tight budget.",
          "score": 2,
          "created_utc": "2026-01-28 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a00de",
          "author": "IulianHI",
          "text": "Another angle nobody mentions: experiment control. With local you can mess around with system prompts, try different quantization levels, and actually understand how the model behaves. APIs give you this nice packaged experience but you're at the mercy of whatever defaults they set. Sometimes that \"15 tok/s\" local run with a quantized model gives you better results for your specific use case than the shiny hosted version with perfect throughput.",
          "score": 2,
          "created_utc": "2026-01-28 20:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3iq2",
          "author": "mambo_cosmo_",
          "text": "the hardware is free because I bought a gaming PC to play games and now I can run local LLMs to do useful stuff too",
          "score": 2,
          "created_utc": "2026-01-28 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b33xf",
          "author": "__Captain_Autismo__",
          "text": "Reliability for local is unsurpassed. No black box",
          "score": 2,
          "created_utc": "2026-01-28 22:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e6q7h",
          "author": "Thrumpwart",
          "text": "TWO CHATS AT THE SAME TIME.\n\nLM Studio new feature is awesome.",
          "score": 2,
          "created_utc": "2026-01-29 11:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272ci4",
          "author": "Foreign-Collar8845",
          "text": "It is market entrapment. You drop prices until you kill the competition (local in this case) then you charge.",
          "score": 3,
          "created_utc": "2026-01-28 11:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bzse",
          "author": "Murder_Teddy_Bear",
          "text": "Porn. I like making porn, ok? Make porn in SD Forge, bring it into LTX-2, bam, animated porn.\n\nPorn.",
          "score": 4,
          "created_utc": "2026-01-28 15:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29id6t",
              "author": "GeneralWoundwort",
              "text": "The only honest person in this thread haha.Â ",
              "score": 3,
              "created_utc": "2026-01-28 18:43:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o274hr0",
          "author": "Full-Bag-3253",
          "text": "Enshitification is the standard business model now.  Netflix was great, but now every year they make it worse unless you pay more.",
          "score": 2,
          "created_utc": "2026-01-28 11:39:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272908",
          "author": "Ruin-Capable",
          "text": "Gemini free tier is not massive.  I blew through a months quota in 5 minutes with opencode",
          "score": 1,
          "created_utc": "2026-01-28 11:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o274mzy",
          "author": "lionelum",
          "text": "Well, learning is a very good reason to run it locally, no need to said more =) . Another is fine tunning, running locally you could training a model on an specific subject that is not so common.   \n  \nIf you already have the hardware (ie. You already have a hardcore gaming PC, or and not to old crypto farming equipment)  points 2 and 3 are debatable.  More on countries where electricity is cheap but change currency is a mess.",
          "score": 1,
          "created_utc": "2026-01-28 11:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2755rs",
          "author": "JLeonsarmiento",
          "text": "Wicked models is mostly my justification now.",
          "score": 1,
          "created_utc": "2026-01-28 11:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275y8y",
          "author": "ShinyAnkleBalls",
          "text": "What do you mean beyond privacy. It's like saying. Beyond having a billion dollars, what is the benefit of winning the lottery?",
          "score": 1,
          "created_utc": "2026-01-28 11:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o276r2m",
          "author": "ThaDon",
          "text": "4. Itâ€™s fun.\n5. You learn a lot.",
          "score": 1,
          "created_utc": "2026-01-28 11:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ahnb",
          "author": "Sicarius_The_First",
          "text": "Valid points.\n\nTbh, use both, local and none local.\n\nFor me, the reason to use local models is for creative stuff. I want fallout / Morrowind adventure with the vibe right, and with specific format following & capabilities.\n\nNo LLM can do this, so I made one that can.\n\nIn other words, local models can outperform in a specific scope / niche.",
          "score": 1,
          "created_utc": "2026-01-28 12:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27d902",
          "author": "Ne00n",
          "text": "Wrong, the is a rate limit, its the memory bandwidth.",
          "score": 1,
          "created_utc": "2026-01-28 12:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ejzl",
          "author": "xmBQWugdxjaA",
          "text": "One issue is you can't guarantee latency nor quantisation with a lot of inference providers. It's actually crazy how ropey this still is. For a lot of usage that won't matter too much, but for production it really limits you to a small number of big providers (Groq, Cerebras, etc.)\n\nLikewise if for your specific use case you are able to distill to something that can run locally, then it's still definitely worth it (in some cases you could even run on mobile etc. - e.g. if you train a BERT style classifier based on distilled data).\n\nBut for general usage like we see with Clawdbot and OpenCode, you are absolutely right!",
          "score": 1,
          "created_utc": "2026-01-28 12:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27f81v",
          "author": "-dysangel-",
          "text": "Being able to run offline is nice. I always have an AI I can chat to even if the internet or service provider are down. I'm not a huge prepper, but there is a part of me that wants to be ready for an emergency. The lockdowns during covid showed that sometimes weird things are just going to happen and you can't stop it. Being able to run frontier level AI at home or on the go is pretty awesome.\n\nI like the idea of being able to have things like a super powered Alexa-like home assistant. I've not got around to this yet, as I guess it isn't really a true pain point. I keep thinking Google or Amazon will up their game too and release something awesome. But they still haven't, so I might get around to it eventually.",
          "score": 1,
          "created_utc": "2026-01-28 12:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27gcde",
          "author": "scousi",
          "text": "Running local is still more expensive and always will be.  But It's a hobby for many and they justify the costs on the basis of other reasons. It's like Uber vs your own car. Uber was cheap but no more but probably still cheaper than buying a car but with trade-offs.\n\nHard to say how long China will give their stuff away. They opened source in the beginning (brilliant move) because no one would dare even using their models unless it was free or hosted outside China. Now people are willing to pay for their services. They've achieved recognition. Open source is a bit aligned with socialism. Maybe they are doing it for that reason. Who knows if there's a coordinated state level strategy. What the west doesn't grasp is that the competition in China amongst themselves is also pretty crazy and they are also trying to outdo each other. \n\nThere does seem to be a disconnect by how much China can achieve with what seems a lot less dollars vs the US.",
          "score": 1,
          "created_utc": "2026-01-28 13:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hmvn",
          "author": "ross_st",
          "text": "I think the freefall in API pricing isn't sustainable.\n\nIt's a desperate race to the bottom to onboard customers, a loss leader.\n\nGemini is subsidised by the rest of Google's business. GPT and Anthropic are subsidised by generous VC runways. DeepSeek is subsidised by hedge fund profits.\n\nBut if they do see the adoption levels they are hoping to see, then they won't be able to afford to do that anymore.",
          "score": 1,
          "created_utc": "2026-01-28 13:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27i2d8",
          "author": "Dany0",
          "text": "My company pays a huge amount for aws bedrock. The best models, Opus 4.5, Sonnet 4.5\n\nEveryone uses it to some extent. But guess what? It's down or not working all the time. Responses timeout, or as you mentioned we get rate limited\n\nLocally I only do toy stuff with it, but the day-to-day UX experience is SO much better despite the huge time & effort upfront cost",
          "score": 1,
          "created_utc": "2026-01-28 13:11:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jkf5",
          "author": "sampdoria_supporter",
          "text": "Part of my calculation is fully owning my process end to end and not feeling bad about burning tons of tokens on testing. Also - I'd argue on #3 that configuration with VLLM has never been easier and it's crazy how hard you can push 3090s. Not disagreeing with your overall point though, I likely would have never bought hardware if it was as cheap and performant as now",
          "score": 1,
          "created_utc": "2026-01-28 13:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27l1sk",
          "author": "MaruluVR",
          "text": "Learning and the fun of setting it up.\n\nI train custom image gen models for game dev so finetuning/lora is the big part for me.",
          "score": 1,
          "created_utc": "2026-01-28 13:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27m6dr",
          "author": "IulianHI",
          "text": "Another angle: tooling integration. With local models, you can hook them directly into your systems without API overhead or limitations. For long-running agents, batch processing, or workflows that need tight coupling with local resources (databases, files, etc.), the flexibility is unbeatable. Sometimes it's not just about cost - it's about architectural freedom.",
          "score": 1,
          "created_utc": "2026-01-28 13:34:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27o7qu",
          "author": "Innomen",
          "text": "Privacy is the only argument. The rest is cope and motivated reasoning. Edit: Well, of course, uncensored is basically private only, but mentally I dump it in the same bucket. Hiding for security and hiding for subversion are both hiding.",
          "score": 1,
          "created_utc": "2026-01-28 13:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27oqzg",
          "author": "hydropix",
          "text": "I crunched the numbers on Owning vs. Renting (RunPod), and unless you're hitting 6+ hours of daily heavy usage, renting wins every time. Plus, the flexibility to spin up high-end clusters for training is a huge advantage.\n\nI also doubt we'll see a repeat of the 'local GPU' era. Since inference isn't that latency-sensitive for most users, the cloud offers better resource efficiency. Weâ€™re likely looking at a cloud-dominated future rather than mass adoption of high-end local hardware (except, of course, for enthusiasts like us here who want to dive deep and push this tech to its limits).",
          "score": 1,
          "created_utc": "2026-01-28 13:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q4qe",
          "author": "d41_fpflabs",
          "text": "Privacy 100% is the only real reason. But it will arguably become the most important thing because AI is only ever going to become more intertwined with most peoples personal lives and at a certain point it would probably become a barrier to entry even for people who aren't the most privacy-conscious. People reaction to Microsofts recent antics is an example of this already happening.\n\nI personally feel like we are going to start seeing more smart devices being built with this in mind. The \"MacMini Claudbot Boom\" kind of highlights the potential of portable private AI-compatible smart devices.",
          "score": 1,
          "created_utc": "2026-01-28 13:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27rje7",
          "author": "StardockEngineer",
          "text": "My only use case ever was that itâ€™s fun and I love knowing all about it.  Professionally, that turns out to be supremely useful, too.",
          "score": 1,
          "created_utc": "2026-01-28 14:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27sixi",
          "author": "Bit_Poet",
          "text": "Local gives you consistent quality. This week, I've repeatedly had dumbed down output and overload refusals from some of the biggest providers with their most expensive models. Unless you're an upper tier customer, you're just an important little bug in their big wheels.\n\nLatency sure is a huge issue. I run complex workflows that have dozens or more consecutive api calls. Doing that over the web sucks, and most of the workflows can't be parallelized.",
          "score": 1,
          "created_utc": "2026-01-28 14:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27taue",
          "author": "nat2r",
          "text": "Price isn't the reason. It's far more expensive to obtain the local hardware. \n\nPeople do it for security and novelty really. Can't trust these big companies.",
          "score": 1,
          "created_utc": "2026-01-28 14:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u5bl",
          "author": "OutsideProperty382",
          "text": "Google's free tier got nerfed last I remember. bad.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yahk",
          "author": "AgreeableCaptain1372",
          "text": "Control over results. Using third-party APIs I get a lot of variance in my evals vs self hosted. \n\nalso prices are low for standard models but not for fine tuned models. So if you need fine tuned LLMs, especially at scale, self hosting or local can be worth it financially",
          "score": 1,
          "created_utc": "2026-01-28 14:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o288lwj",
          "author": "CH3CH2OH_toxic",
          "text": "Gemini has a massive free tier when ? where ?",
          "score": 1,
          "created_utc": "2026-01-28 15:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28e4ch",
          "author": "FitAstronomer5016",
          "text": "Some points have been made already that I feel already encapsulate it, but it falls under a few things\n\n1. API Pricing/Chat subscriptions are subsidized, not only for us the customers but also the companies (Claude has quite a bit of 'free' compute from AWS and I'm sure it translates over to the other large providers). While they are profitable somewhat on paper, once that allocation runs out, you will see an increase in price.\n\n2. Business processes will feel the brunt of that, and like how some companies migrate to local DBs/own server stack, AI falls under the same category\n\nNow granted, running local is not as lucrative and at this point is becoming much more of a luxury with the increase of hardware costs, and more importantly, power costs (especially planned price hikes). The control is still appealing and will continue to grow, although probably with not the same hardware requirements as we have now. If we get more efficient running models, better cost-effective hardware like dedicated TPUs/APUs for AI inference, it would become akin to an SSD almost.",
          "score": 1,
          "created_utc": "2026-01-28 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28hs8b",
          "author": "Icy_Foundation3534",
          "text": "compliance, and also the private equity-fication scenario where your opus sub is $3000 a month a few years from now because it's like hiring a real 100x super dev.",
          "score": 1,
          "created_utc": "2026-01-28 16:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sju2",
          "author": "Bbmin7b5",
          "text": "beyond privacy, not much. but Privacy is THE most important part.",
          "score": 1,
          "created_utc": "2026-01-28 16:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28v684",
          "author": "Irisi11111",
          "text": "Indexing is the most consequential use case I can think of for local AI. I also hope browser use and vision-centric document retrieval will be the next focus.",
          "score": 1,
          "created_utc": "2026-01-28 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29006x",
          "author": "Rich_Artist_8327",
          "text": "You can also then ask what is the usecase for providing almost free APIs while every request consumes huge amount of energy.",
          "score": 1,
          "created_utc": "2026-01-28 17:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o292off",
          "author": "Maddog0057",
          "text": "Same price, you're just not paying in money, you're paying in data.",
          "score": 1,
          "created_utc": "2026-01-28 17:36:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o295d51",
          "author": "Glad-Audience9131",
          "text": "I checked today.. is $19 monthly for me.. what's going on???",
          "score": 1,
          "created_utc": "2026-01-28 17:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29qift",
          "author": "GrayDonkey",
          "text": "Been getting Gemini is at capacity errors all morning....\n\nIf not that it's, Gemini 3 is busy, answering with 2.5. Followed by Gemini 2.5 is busy, answering with 2.0. Followed by some not great output.",
          "score": 1,
          "created_utc": "2026-01-28 19:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9e8h",
          "author": "Torodaddy",
          "text": "You dont need the insane rigs you see to run a local model, the amd based miniforum devices work fine for inference and in that case i can have agents working 24/7. Even the cheapest api starts getting expensive using it like that",
          "score": 1,
          "created_utc": "2026-01-28 20:43:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9hyi",
          "author": "Michaeli_Starky",
          "text": "Local ones were always vastly inferior from the cost perspective, speed perspective and overall performance.",
          "score": 1,
          "created_utc": "2026-01-28 20:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aa609",
          "author": "ErokOverflow",
          "text": "Listen to this: Good taste in programming and image creation is not always coming from wealthy people who can buy a good hardware configuration.\nHigh prices.\nThis IS the real justification.",
          "score": 1,
          "created_utc": "2026-01-28 20:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ae5xs",
          "author": "57hz",
          "text": "Also, you can rent servers (by month or by second) to run local models. So thereâ€™s the benefit of not being tied to hardware while having privacy and consistency.",
          "score": 1,
          "created_utc": "2026-01-28 21:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ao9fx",
          "author": "ZakOzbourne",
          "text": "Because no company wants to generate the stuff I want it to generate, I need uncensored unhinged models",
          "score": 1,
          "created_utc": "2026-01-28 21:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ayj1i",
          "author": "Plopdopdoop",
          "text": "Is Gemini back to having a massive free tierâ€¦aside from the 2.5 flash-lite variant?\n\nLast I checked they removed essentially all the free access but that flash lite model.",
          "score": 1,
          "created_utc": "2026-01-28 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bbcod",
          "author": "Outrageous-Tonight75",
          "text": "I think it is similar to having a Nas instead of paying Netflix. A mix between privacy, control and the \"DiY feeling\" that makes using it special",
          "score": 1,
          "created_utc": "2026-01-28 23:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bfnqm",
          "author": "SnooBananas5215",
          "text": "Maybe for the power user for dogshit coder like me offline models don't really work, instead I focus on creating simple UI with lots of automation and flexibility not built for scale though, most of the time I am not working on any heavy stuff anyways and I think just automating lots of stuff that small and medium businesses rely on for version control, qms and authorized form completion is all they need",
          "score": 1,
          "created_utc": "2026-01-29 00:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bryco",
          "author": "flywind008",
          "text": "you pay for your privacy, thatâ€™s fare enough",
          "score": 1,
          "created_utc": "2026-01-29 01:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bulco",
          "author": "deparko",
          "text": "Well, I've been dealing with the same issue and have concluded a hybrid approach works best. I use a three-tier model: an offline small LLM (Ollama) on my local 5070 TI GPU for local tasks; Ollama Cloud as tier two for bulk processing, where I can use Kimi and Deepseek..etc for a flat rate (about $20 a month, $240 a year), which is much cheaper than upgrading my GPU; and frontier models for deep reasoning when needed.\n\nI've designed my RAG and AI-native apps to operate within that three-tier framework.",
          "score": 1,
          "created_utc": "2026-01-29 01:20:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bwoqw",
          "author": "Vicar_of_Wibbly",
          "text": "The question is inverted. Why would I use the cloud? Itâ€™s slower, has no meaningful quality improvement over my local setup, imposes restrictions on use, and can change without notice or approval.\n\nMy local system is standardized, fully under my control, is backed up properly and - as you very importantly point out - is private.\n\nThe cloud has no compelling use case for me whatsoever.",
          "score": 1,
          "created_utc": "2026-01-29 01:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c00g2",
          "author": "mystery_biscotti",
          "text": "For funsies. \n\nAlso, i'm learning how to do tiny infra on a micro scale, so I'll be in a better spot to become employed keeping AI upright. \n\nPlus if certain US states are banning specific roles AI fills, local will be the only way to go. (Tennessee, I'm looking your way.)",
          "score": 1,
          "created_utc": "2026-01-29 01:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c0o3s",
          "author": "Kos187",
          "text": "You can spent 50M of input token over the weekend with Claude Code easily. It's easy to justify local LLM on a hardware you use for something else as well,  like gaming... But after some amount of VRAM it makes no sense. Especially when SLI is dead.",
          "score": 1,
          "created_utc": "2026-01-29 01:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2px8",
          "author": "celsowm",
          "text": "4. fine tuned models",
          "score": 1,
          "created_utc": "2026-01-29 02:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbls2",
          "author": "Far-Low-4705",
          "text": "It's fun",
          "score": 1,
          "created_utc": "2026-01-29 02:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cdwao",
          "author": "lgdsf",
          "text": "Run jailbroken models for sure",
          "score": 1,
          "created_utc": "2026-01-29 03:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cetv8",
          "author": "Adventurous_Push6483",
          "text": "There are two differences I see: Some people use locally hosted models for personal use, and some people need to bootstrap ML to a product.\n\nIn terms of product:  \n  \n1. First of all, all the models available over the API have extremely high safety railguards. This affects some of my experiments (which can be product tests), which even though I am not generating NSFW content with it, I have found the censored models performing far worse in some of the specific personalization tasks I test with (they strangely tend to be less creative? I don't have any formal benchmarks for this so doubt it).  \n  \n2. Self-hosting is also much more \"safe\" in a sense that there is floor of what you can lose. If you build a public facing demo application of a product and didn't bother to secure it yet since it is an early stage PoC, you won't  run into strange issues with insecure rate spamming (exploding API costs) and whatnot. Yes, this is terrible practice but sometimes I just want to share my application with peers and its a lot easier to throw a Streamlit site over LAN and worry not about security (at worst, the app just crashes).\n\n3. I do mostly use the Gemini API, but the rate limits are certainly an issue with the free tier. The better alternative is just to use the paid tier if you have the money, to which you can get a surprising amount of research/experimental work done with just $10 worth of credit.\n\n4. Technically speaking, it is \"free\" for me. I just use my group's server (p beefy hardware) to locally host when I need so I'm not actually paying anything for the hardware. If the place you work at (or your PC) just happens to have compute for training/HPC work, you might as well use it since its already there.\n\n5. Millions of tokens is not as hard to hit as you think. Especially if you work with image data to VLM, which can easily cost thousands of tokens. I work with massive amounts of data in many media formats, data processing with LLM API is very expensive so the breaking-in-even is not a very compelling argument (this is still ran through the API, however, expect big bills far greater than GPUs). \n\nI think the nicer thing about the API is just sheerly how easy it is to setup. Buying hardware takes so much time when I can just rent it out on the cloud OR just use some money and use the API (download API key => download package => Use AI).\n\nThe more interesting argument has always been local GPU vs cloud GPU I think. APIs are just so limited to their scope and what they can do, but they are so convenient that there isn't a good reason to not use them if you have funding/VC money (with exception if you need something that's more specific than a generic language model, which is not many applications).",
          "score": 1,
          "created_utc": "2026-01-29 03:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cfpr5",
          "author": "Spanky2k",
          "text": "Privacy is everything. We use a local model when working on business sensitive stuff although our model choices are currently somewhat limited (Mac Studio 64GB). I'm hoping the eventual M5 or M6 Ultra based Mac Studios will have improved prompt processing enough so that it's possible to host a good sized (say 200-300B) model with multi user access (5-10 total users but realistically no more than 1 or 2 ever submitting queries at the same time) with reasonable performance. Something like that for about Â£10k would be perfect for a truly local and data secure system.\n\nBut lately I've been playing around loads with Qwen Image Edit and Hunyuan Video in ComfyUI on my 5090. I've been having a blast feeding them family photos and reimagining them in different styles, changing outfits, animating old family photos. I wouldn't have ever felt comfortable uploading that stuff to a cloud based service.",
          "score": 1,
          "created_utc": "2026-01-29 03:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cij7o",
          "author": "Chilidawg",
          "text": "There's the principle of ownership over renting. I know we're losing that battle a little more every day, but still...\n\nNo matter how cheap something is, there's an immeasurable gap between \"free\" and \"paid\". I understand that my electricity bill is likely more expensive than the API. However, that's grouped with utilities that are already expensive before running local models.\n\nThere's the novelty of verbally talking to your computer. There's no black box API promising you're talking to a model as opposed to someone in New Delhi. You can run the script and listen to your GPU fans speed up.\n\nFinally, I also don't pay Sam or Elon on principle.",
          "score": 1,
          "created_utc": "2026-01-29 03:33:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2clnqb",
          "author": "x0xxin",
          "text": "It's also a super fun and practical hobby. I've learned so much about self hosting and \"cloud tech\" via all of my labbing.",
          "score": 1,
          "created_utc": "2026-01-29 03:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d1fwr",
          "author": "Vivarevo",
          "text": "Pricing is marketing, it will change on short notice",
          "score": 1,
          "created_utc": "2026-01-29 05:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4g2r",
          "author": "Big_River_",
          "text": "all these prognosticators claiming models are commodities are investor bros who have a very confident surface level understanding of technology and macroeconomics - applying traditional business school analytics on technology adoption and artificial intelligence as a service like sas on steroids margins or corporations are going to just train their own model and cut vendor and labor cost - just like implementing a new management information system with autonomous agents as admins and analysts and developers...on their own local data center that the model manages itself with human in the loop hardware maintenance until robotics improves fine motor skills ...and all of this is a 9 month implementation that will become relatively plug and play fine tuned specialized model that can rewrite and optimize the code base, product firmware, optimize manufacturing processes, define the kpi that drive results, write up the marketing strategy and product design, 24/7 live dashboards on every deep dive imaginable for the c-suite goons to...nevermind all along frontier labs are racing with govt labs to produce the singular super intelligence that unlocks unimaginable world model exploits that change our fundamental understanding of what intelligence is capablr\n\nopen source + home lab represents the counterweight to corporate cloud \"all your base (models) are belong to us ( and your data too )\" and you can rent them for cheap now but once local compute becomes too expensive for joe six pack to build out in his garage - the cloud becomes the only game in town to get that coevo level up intelligence and truly generative creative extension of minds - there is a fundamental divide between the compute you own and the compute you rent - your data, your painfully fine tuned model that all your agents and business/creative / personal growth process depends upon...lease your work forever to market whims...cloud compute prices are so low to hook as many as they can while local compute components are skyrocketing - say goodbye to the capable personal computer - the next commercial electronics is cloud optimized priority connectivity for proprietary cloud ai managed computing resources - the user will have a great experience but it will always be renting the ability to functionally participate in the economy - when one org takes the super intelligence to monopolize compute as everyday iphone level (comprehensive information ingress paid promo filter and algorithm ranked content engagement device) service is vertically integrated.....well the individuals who sit on the board and the majority shareholders of this company that owns an overwhelming majority of compute ascend to a level of control and influence on the learning and reasoning of humanity that reduces their impact on the collective narrative to NPCs going through time in limited awareness incurious and relatively happy and cared for like a collection of objects or pets or puppets that are fun to play with until they break or shoot each other dead in the streets to amuse or shock....\n\nlocal compute ai clusters and distributed data owned by and maintained for the benefit of small communities and collectives is the only counterbalance to the commercial interests of capital markets and investors demand for unsustainable reality bending returns that own and control the narrative for some fucking reason in this shard of existence \n\nget local stacked functional compute or you are not playing the game -- you are getting played - get hooked on the cloud",
          "score": 1,
          "created_utc": "2026-01-29 06:00:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4igb",
          "author": "Which-Jello9157",
          "text": "same question here. replies above are convincing but realistically im sticking with cloud apis for now since its easy and cheap. models keep getting bigger and whatever hardware u buy today probably wont handle next gen stuff anyway. do you have any recommended third-party api provider without rpm limit?",
          "score": 1,
          "created_utc": "2026-01-29 06:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ln5sp",
              "author": "PassagePlus3777",
              "text": "Openrouter.ai is the obvious choice for model variety since they aggregate from multiple providers but sometimes u get rate limited even on paid tier. pro tip tho, OR supports byok (bring your own key) so u can route to whichever upstream is cheapest for that specific model. i use atlascloud.ai for kimi k2.5 since theyre the cheapest and stable, and phala for glm 4.7 flash cuz they're stable on it. Way cheaper if ur running volume",
              "score": 2,
              "created_utc": "2026-01-30 13:40:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d7ig4",
          "author": "TruckAmbitious3049",
          "text": "For me it's rate limits. \n\nFor data analysis, I need to do a lot of labeling. Paid Gemini and ChatGPT would hit rate limits. \n\nFor transcribing, sonoix is amazing and cheap. But if it's a large batch, then Whisper still better.",
          "score": 1,
          "created_utc": "2026-01-29 06:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2damya",
          "author": "Photochromism",
          "text": "Lol. Dumb take. The minute one of these platforms becomes the favorite, becomes a necessity, and wipes out the others, the prices will a quadruple.",
          "score": 1,
          "created_utc": "2026-01-29 06:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dd8xe",
          "author": "AlwaysLateToThaParty",
          "text": "> What's the actual case for running local now beyond privacy? \n\nHaving a 96GB GPU with 1.7TB/s memory bandwidth for gaming.  It's quite good.",
          "score": 1,
          "created_utc": "2026-01-29 07:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgc8i",
          "author": "SlimPerceptions",
          "text": "Even on point #1 - encrypted files and rented gpuâ€™s? I havenâ€™t done it but it seems like their are adequate privacy solutions out there even for the cloud",
          "score": 1,
          "created_utc": "2026-01-29 07:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpwvu",
          "author": "Maximum-Wishbone5616",
          "text": "In my experience the SLA is non-existent for most of AI services.\n\nWe require at least 99.99% (that is uber minimum), we monitor all our servers, instances and services every 30 seconds, we aim to get at least 99.9999% through out them.\n\nDue to sheer amount of requests we process per minute, 99.9% would mean that 3rd party AI 503 would cause havoc for our customers/monitoring systems.\n\nThey are not even close to 95% with 30 second monitoring...",
          "score": 1,
          "created_utc": "2026-01-29 09:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2esj8o",
          "author": "HopefulMaximum0",
          "text": "The API prices are low because they decided to operate at a loss until they win.   Then they will crank the price to the moon and do whatever suits their current whim to you, because you can't go elsewhere.\n\n  \nThe plan is simple, and as old as anti-dumping laws.",
          "score": 1,
          "created_utc": "2026-01-29 13:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fuuyk",
          "author": "Vahn84",
          "text": "I simply do not want a future where Iâ€™m not in control of my things. Corporations are forcing us into a future where we probably wonâ€™t have shit into our handsâ€¦and we will be forced to rent everything. I donâ€™t want a life hooked to a multitude of subscription services. Prices are going down nowâ€¦probably because the bubble is going to explode. But nothing can assure you that they will remain like this forever. I bet they wonâ€™t",
          "score": 1,
          "created_utc": "2026-01-29 16:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h0dvb",
          "author": "Financial-Source7453",
          "text": "Abliterated models. Tired hearing \"I am sorry I can't do that due to policy restrictions\" from ChatGPT all the time.",
          "score": 1,
          "created_utc": "2026-01-29 20:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j2til",
          "author": "mr_zerolith",
          "text": "I'll add to your list.\n\n\\- speed/intelligence tradeoffs are controlled by me.  \n\\- i have no idea what quantization/etc the model provider is using and they could be intermittently \"cutting the product\", which leads to inconsistent results that can be time consuming.  \n\\- when the service is down, i can get it back up with a reboot instead of wait a random amount of time.  \n\\- USA companies only: i am not directly helping fund a company that is involved in enormous copyright infringement or bribing, sorry, i mean, lobbying, the government against the interest of it's competitors and possibly me",
          "score": 1,
          "created_utc": "2026-01-30 02:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kz628",
          "author": "Euphoric_Emotion5397",
          "text": "there are some things you rather not do online.\n\nHumans valued their privacy.\n\nso , a lot of things can be done online. But there are some things still kept offline.\n\nIt's not a mutually exclusive situation with most people. It is a complementary solution (use GPU for x use case, use online API for x use case).\n\nExample, I wouldn't want Gemini to read my emails.  \nBut I might give it a try if I'm assured a local LLM setup can do it as well. And I will still not jump onto the bandwagon as an early adopter like how the clawdbot turns out. Just give it time. The best will come thru.\n\nRight now, I use Gemini Pro  + Anti-Gravity for vibe coding and generic enquiries and discussion with LLM. But I use my 32gb VRAM GPU to do local inference on my app data that are private to me.",
          "score": 1,
          "created_utc": "2026-01-30 10:58:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p2no8",
          "author": "Glum-Traffic-7203",
          "text": "For me itâ€™s privacy and customisation as the biggest to \n\nWhen it comes to rate limits and costs - there are specialist high volume low cost providers like doubleword who are even cheaper than APIs like Kimi",
          "score": 1,
          "created_utc": "2026-01-30 23:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3w4c",
          "author": "ravage382",
          "text": "To start turning a profit, all the big AI companies are going to have to really increase monthly rates. I imagine not too far in the future, prices are going to go way up. \n\nI have come to rely on AI enough that I want to ensure my continued access to it, regardless of what the big companies do. I will be pretty happy with just gpt120b and a few of the smaller qwen and glm models going into the future.  Knowledge cutoffs are annoying, but mcp tools with access to web search mostly mitigate that.",
          "score": 1,
          "created_utc": "2026-01-30 23:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39pquk",
          "author": "sloptimizer",
          "text": "Consistency is a big one for me! You never know what model or quant you are getting when using cloud APIs. You don't know what kind of system prompt it has (and how much of the best context space that system prompt has taken away from you).\n\nWhen you run local - you know what you are getting. You can run with no system prompt on an empty context, which gives you the best possible output. And finally, you can adjust your prompting skills by seeing what works and what doesn't without all the extra variables thrown in. For example: is my prompt the problem, or I just hitting a lobotomized Q4 quant on OpenRouter?\n\nOn a subjective side, there is a certain joy in running these models on a box sitting under your desk. Makes it feel real, like something you can see or touch.",
          "score": 1,
          "created_utc": "2026-02-03 02:07:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ucdm",
          "author": "bgiesing",
          "text": "Because it's still more expensive, my PC is already on 24/7 so it would be using about the same electricity regardless and I can use that GPU for many things (games, video editing, etc.), it already was paid off years ago. That's still cheaper than having to drop like $10-20 a month on API calls or a subscription service.\n\nAlso, many people explicitly chose local because they want to make content that the cloud models from the big companies refuse, API cost doesn't matter if every single reply you get is \"I can't fulfill that request\", you literally don't have an option",
          "score": 1,
          "created_utc": "2026-01-28 10:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xjdy",
          "author": "k_means_clusterfuck",
          "text": "not a deciding factor for me but i do think its nice to to know my carbon footprint",
          "score": 1,
          "created_utc": "2026-01-28 10:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xq3m",
          "author": "SpicyWangz",
          "text": "Ah yes, the freefall. LLM companies donâ€™t even know whatâ€™s happening. Every time they look at their own pricing the numbers are lower. Soon enough Iâ€™m sure theyâ€™ll be paying you to use their API",
          "score": 1,
          "created_utc": "2026-01-28 10:44:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zos4",
          "author": "tcoder7",
          "text": "When you use AI API you give away your intellectual property. The model scans your repo and sends everything to a remote server. Open the output tab in VSCode and watch.",
          "score": 1,
          "created_utc": "2026-01-28 11:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275pm1",
          "author": "FastDecode1",
          "text": "Computer hardware: $$$\n\nNot going to prison for asking forbidden questions: priceless.",
          "score": 1,
          "created_utc": "2026-01-28 11:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28nm9d",
          "author": "slindshady",
          "text": "Everything is sensitive data. Thatâ€™s the point. Look what happens with one â€žbadâ€œ election.",
          "score": 1,
          "created_utc": "2026-01-28 16:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h6zy",
          "author": "zipperlein",
          "text": "VC will dry out eventually.",
          "score": 0,
          "created_utc": "2026-01-28 13:06:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27t3mp",
          "author": "devinprocess",
          "text": "Gonna be real with you.\n\nActual case: you are in the 1% of the rich or lucky folks here who have accumulated enough hardware and have no issues running a power guzzling setup.\n\nFor majority of us normal bees, api or renting is still the way because the local models we can run are just for shits and giggles, and who cares about all those arguments at that stage. \n\nUnless local llms become affordable itâ€™s just a circlejerk.",
          "score": 0,
          "created_utc": "2026-01-28 14:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28pyic",
          "author": "DataGOGO",
          "text": "The bots are going to rage on me for this one; but not putting all my data in the hands of the Chinese government.Â \n\nI will run local or use a US based provider subject to US / EU data protection laws.\n\n\nEvery Chinese provider is very heavily subsidized by the Chinese government (and that is just what they openly admit to, the true extent is unknown).Â \n\nThe entire business model is to undercut US / EU companies to the point of making the AI business unsustainable, thus giving China AI dominance.Â They know that in order for OpenAI, xAI, Meta, Google, Microsoft, etc. to stay in the AI business, they eventually have to turn a profit.\n\nBy releasing models into open source, and providing extremely cheap API access the goal is to make turning a profit impossible.\n\nThat is very bad for everyone, as it just turns AI into Chinese propaganda machines and data collection tools.Â \n\nTo be clear, I am not throwing shade on the Chinese developers, engineers, and data scientists at all, just the government framework they are forced to operate in.Â ",
          "score": 0,
          "created_utc": "2026-01-28 16:41:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqpon2",
      "title": "OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qqpon2",
      "author": "jacek2023",
      "created_utc": "2026-01-30 00:07:33",
      "score": 308,
      "num_comments": 188,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2iyxhw",
          "author": "nickcis",
          "text": "In what hardware are you running this?",
          "score": 32,
          "created_utc": "2026-01-30 02:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kf5le",
              "author": "jacek2023",
              "text": "with dust on it:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1nsnahe/september\\_2025\\_benchmarks\\_3x3090/](https://www.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/)",
              "score": 11,
              "created_utc": "2026-01-30 07:58:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2lbgfa",
                  "author": "AlwaysLateToThaParty",
                  "text": "Good setup.  Who knew such a thing would become worth so much, huh?  Appreciate the break down.",
                  "score": 2,
                  "created_utc": "2026-01-30 12:31:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pbx31",
                  "author": "Artistic_Okra7288",
                  "text": "I'm getting a sustained 33 tps tg with a single 3090 Ti using UD Q4 K XL, same amount of prompt cache (DDR4). With the self speculative decoding, when it hits, it pushes over 100 tps tg.\n\nTry pushing your batch-size to 8192. It's significantly reduced prompt processing time and the GPU usage for me is hovering around 90% during prompt processing.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:10:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2indlk",
          "author": "klop2031",
          "text": "How is the quality? I like glm flash as i get like 100t/s which is amazing. But havent really tested the llms quality.",
          "score": 19,
          "created_utc": "2026-01-30 01:01:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iwsu5",
              "author": "oginome",
              "text": "Its pretty good. Give it MCP capabilities like vector RAG, web search, etc its even better.",
              "score": 25,
              "created_utc": "2026-01-30 01:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j651h",
                  "author": "everdrone97",
                  "text": "How?",
                  "score": 9,
                  "created_utc": "2026-01-30 02:45:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lroxh",
                  "author": "vertigo235",
                  "text": "which vector rag mcp do you like?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kh6z3",
              "author": "jacek2023",
              "text": "Earlier, I created a hello world app that connects to my llama-server and sends a single message. Then I showed this hello world example to opencode and asked it to write a debate system, so I could watch three agents argue with each other on some topic. This is the (working) result:\n\n    debate_system/\n    â”œâ”€â”€ debate_config.yaml       # Configuration (LLM settings, agents, topic)\n    â”œâ”€â”€ debate_agent.py          # DebateAgent class (generates responses)\n    â”œâ”€â”€ debate_manager.py        # DebateManager class (manages flow, context)\n    â”‚   â”œâ”€â”€ __init__()           # Initialize with config validation\n    â”‚   â”œâ”€â”€ load_config()        # Load YAML config with validation\n    â”‚   â”œâ”€â”€ _validate_config()   # Validate required config sections\n    â”‚   â”œâ”€â”€ _initialize_agents() # Create agents with validation\n    â”‚   â”œâ”€â”€ start_debate()       # Start and run debate\n    â”‚   â”œâ”€â”€ generate_summary()   # Generate structured PRO/CON/CONCLUSION summary\n    â”‚   â”œâ”€â”€ format_summary_for_llm()  # Format conversation for LLM\n    â”‚   â”œâ”€â”€ save_summary()       # Append structured summary to file\n    â”‚   â””â”€â”€ print_summary()      # Print structured summary to console\n    â”œâ”€â”€ run_debate.py            # Entry point\n    â””â”€â”€ debate_output.txt        # Generated output (transcript + structured summary)\n    \n    shared/\n    â”œâ”€â”€ llm_client.py            # LLM API client with retry logic\n    â”‚   â”œâ”€â”€ __init__()           # Initialize with config validation\n    â”‚   â”œâ”€â”€ _validate_config()   # Validate LLM settings\n    â”‚   â”œâ”€â”€ chat_completion()   # Send request with retry logic\n    â”‚   â”œâ”€â”€ extract_final_response() # Remove thinking patterns\n    â”‚   â””â”€â”€ get_response_content() # Extract clean response content\n    â”œâ”€â”€ config_loader.py         # Legacy config loader (not used)\n    â””â”€â”€ __pycache__/             # Compiled Python files\n    \n    tests/\n    â”œâ”€â”€ __init__.py              # Test package initialization\n    â”œâ”€â”€ conftest.py              # Pytest configuration\n    â”œâ”€â”€ pytest.ini               # Pytest settings\n    â”œâ”€â”€ test_debate_agent.py     # DebateAgent unit tests\n    â”œâ”€â”€ test_debate_manager.py   # DebateManager unit tests\n    â”œâ”€â”€ test_llm_client.py       # LLMClient unit tests\n    â””â”€â”€ test_improvements.py     # General improvement tests\n    \n    requirements.txt            # Python dependencies (pytest, pyyaml)\n    debate_system_design/\n    â””â”€â”€ design_document.md       # Design specifications and requirements\n\n  \nand I never told him about the tests, but somehow he created good ones",
              "score": 6,
              "created_utc": "2026-01-30 08:16:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2l3xvs",
              "author": "-dysangel-",
              "text": "It's best in class for its size IMO, as long as you're running it at 8 bit. When I ran at 4 bit, it got stuck in loops. It's actually the first small model I've found where 8bit vs 4 bit actually makes a noticeable difference.",
              "score": 5,
              "created_utc": "2026-01-30 11:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mlkzn",
                  "author": "twack3r",
                  "text": "Try Nemotron. BF16 great, Q8 already takes a very deep perplexity dive.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:25:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2o847t",
                  "author": "DHasselhoff77",
                  "text": "I haven't ran into looping with MXFP4 quants but admittedly didn't test much yet.",
                  "score": 1,
                  "created_utc": "2026-01-30 20:48:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jfbdq",
              "author": "floppypancakes4u",
              "text": "With local hardware? I only get about 20tks max on a 4090",
              "score": 4,
              "created_utc": "2026-01-30 03:37:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jrcov",
                  "author": "simracerman",
                  "text": "Something is off in your setups I hit 60 t/s at 8k context with 5070 Ti.",
                  "score": 10,
                  "created_utc": "2026-01-30 04:52:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kdyhh",
                  "author": "Theio666",
                  "text": "I was hitting 40tps on 4x2080ti, Q5, something is wrong with your setup.",
                  "score": 3,
                  "created_utc": "2026-01-30 07:48:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lzwbw",
                  "author": "packetsent",
                  "text": "120 t/s on a 5090 at Q6 or Q4 (will need to check once home)",
                  "score": 2,
                  "created_utc": "2026-01-30 14:45:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jucvl",
                  "author": "klop2031",
                  "text": "Yes, when i get a chance ill post my config. I was surprised at that at first but have been able to get this with a 3090 + 192gb ram",
                  "score": 1,
                  "created_utc": "2026-01-30 05:13:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lddqq",
                  "author": "teachersecret",
                  "text": "On a 4090 Iâ€™m getting over 100t/s on this model in 4 bit k xl. You must be offloading something to cpu/ram.",
                  "score": 1,
                  "created_utc": "2026-01-30 12:44:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jwkng",
                  "author": "simracerman",
                  "text": "Something is off with your setup. My 5070 Ti does 58 T/s at 8k context.",
                  "score": 1,
                  "created_utc": "2026-01-30 05:28:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kfqvo",
                  "author": "SlaveZelda",
                  "text": "I can do 45 tok/s at 50k context on a 4070ti",
                  "score": 1,
                  "created_utc": "2026-01-30 08:03:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jbshq",
          "author": "ab2377",
          "text": "what's your hardware setup?",
          "score": 6,
          "created_utc": "2026-01-30 03:17:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iyysg",
          "author": "Several-Tax31",
          "text": "Your output seems very nice. Okay, sorry for the noob question, but I want to learn about agentic frameworks.Â \n\n\nI have the exact setup, llama.cpp, glm-4.7 flash, and I donwload opencode. How to configure the system to create semi-complex projects like yours with multiple files? What is the system prompt, what is the regular prompt, what are the config files to handle? Care to share your exact setup for your hello world project, so I can replicate it? Then I'll iterate from there to more complex stuff.Â \n\n\nContext: I normally use llama-server to one shot stuff, and iterate on projects via conversation. Compile myself. Didnt try to give model tool access. Never used claude code or any other agentic frameworks, so the noob question. Any tutorial-ish info would be greatly appreciated.Â ",
          "score": 4,
          "created_utc": "2026-01-30 02:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j69h8",
              "author": "Pentium95",
              "text": "This tutorial Is for Claude code and codex. Opencode specific stuff Is written on their github. \n\nhttps://unsloth.ai/docs/basics/claude-codex",
              "score": 9,
              "created_utc": "2026-01-30 02:46:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j72lt",
                  "author": "Several-Tax31",
                  "text": "Many thanks for the info! Dont know why it didnt occur to me to check unsloth.Â ",
                  "score": 5,
                  "created_utc": "2026-01-30 02:50:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kesr2",
                  "author": "cantgetthistowork",
                  "text": "How do you make Claude code talk with openai compatible endpoint? It's sending the v1/messages format",
                  "score": 1,
                  "created_utc": "2026-01-30 07:55:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ikj1k",
          "author": "BitXorBit",
          "text": "waiting for my mac studio to arrive to try exactly this setup, i been using claude code everyday and i just keep filling it with more balance every day. can't wait to work locally.\n\nhow is it compared to opus 4.5? sure not smart equally, but smart enough?",
          "score": 6,
          "created_utc": "2026-01-30 00:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j14m6",
              "author": "[deleted]",
              "text": "Using opus for planning and handing off to gpt-oss-{1,}20B works p well. Many local models you can load on your studio donâ€™t quite compare to opus, but they are capable. Helps conserve/utilize the tokens",
              "score": 4,
              "created_utc": "2026-01-30 02:18:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k2sxe",
                  "author": "florinandrei",
                  "text": "How exactly do you manage the hand-off from Opus to GPT-OSS? Do you invoke both from the same tool? (e.g. Claude Code) If so, how do you route the prompts to the right endpoints?",
                  "score": 3,
                  "created_utc": "2026-01-30 06:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ivdrv",
              "author": "TheDigitalRhino",
              "text": "Make sure you try something like this [https://www.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx\\_native\\_apple\\_silicon\\_llm\\_inference\\_464/](https://www.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/)\n\nyou really need the batching for the PP",
              "score": 2,
              "created_utc": "2026-01-30 01:45:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2j9avz",
          "author": "BrianJThomas",
          "text": "I tried this with GLM 4.7 Flash, but it failed even basic agentic tasks with OpenCode. I am using the latest version of LM Studio. I experimented some with inference parameters, which helped some. However, I couldn't get it to generate code reliably.\n\nAm I doing something wrong? I think it's kind of hard because the inference settings all greatly change the model behavior.",
          "score": 7,
          "created_utc": "2026-01-30 03:03:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kg1vc",
              "author": "jacek2023",
              "text": "If you look at my posts on LocalLLaMA from the last few days, there were multiple GLM-4.7-Flash fixes in llama.cpp. I donâ€™t know whether they are actually implemented in LM Studio.",
              "score": 7,
              "created_utc": "2026-01-30 08:06:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kozrd",
                  "author": "BrianJThomas",
                  "text": "Ah OK. I haven't tried llama.cpp without a frontend in a while. I had assumed the LM Studio version would be fairly up to date. Trying now, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:28:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lpaal",
                  "author": "1ncehost",
                  "text": "I can confirm they didnt have the latest llama.cpp as of yesterday. The llama.cpp release off github performs way better currently.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:52:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lyi3t",
                  "author": "lolwutdo",
                  "text": "Any idea if they fixed the model not producing opening thinking tags?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k7nla",
              "author": "Odd-Ordinary-5922",
              "text": "just switch off lmstudio",
              "score": 4,
              "created_utc": "2026-01-30 06:54:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k90cc",
                  "author": "BrianJThomas",
                  "text": "It's just llama.cpp.... Or are you just complaining about me using a frontend you don't prefer?",
                  "score": -3,
                  "created_utc": "2026-01-30 07:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k5o9d",
              "author": "Careless_Garlic1438",
              "text": "well I have Claude Code and Opencode running, opencode works on some questions but fails miserable at others, even a simple HTML edit failed, took Claude minutes to do â€¦ so very hit and miss depending on what model you use locally â€¦ I will do a test with online models and opencode to see if that helps",
              "score": 1,
              "created_utc": "2026-01-30 06:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kg3tz",
                  "author": "jacek2023",
                  "text": "opencode with what model?",
                  "score": 1,
                  "created_utc": "2026-01-30 08:07:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mp19t",
              "author": "markole",
              "text": "What are your llama-server flags?",
              "score": 1,
              "created_utc": "2026-01-30 16:40:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jb7qu",
          "author": "1ncehost",
          "text": "Haha I had this exact post written up earlier to post here but I posted it on twitter instead. This stack is crazy good. I am blown away by the progress.\n\nI am getting 120 tok/s on a 7900 xtx with zero context and 40 tok/s with 50k context. Extremely usable and seems good for tasks around 1 man hour in scale based on my short testing.",
          "score": 4,
          "created_utc": "2026-01-30 03:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k3vwa",
              "author": "Glittering-Call8746",
              "text": "Your github repo pls. Amd setup are a pain to start..",
              "score": 2,
              "created_utc": "2026-01-30 06:24:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lpnip",
                  "author": "1ncehost",
                  "text": "You dont need rocm. Just use the vulkan github release. That works with the stock linux amdgpu drivers and radeon drivers on windows. I'm using linux so i dont know how it runs on windows.\n\nSo literally install OS normally and download the vulkan llama.cpp off the github.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:53:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ld8ts",
              "author": "brokester",
              "text": "Are you interested in sharing your setup. Also have a 7900xtx. I assume you are on Linux?\nAlso did you offload to CPU/ram?",
              "score": 1,
              "created_utc": "2026-01-30 12:43:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lox5r",
                  "author": "1ncehost",
                  "text": "Yes linux, using vulkan llama.cpp latest release from github. Model is unsloth glm 4.7 flash REAP at iq4 quant.\n\nThat quant easily fits in the 24 GB, but you'll want to turn on flash attention to run the large context.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mn947",
          "author": "Danmoreng",
          "text": "https://x.com/ggerganov/status/2016903216093417540\n\nLlama.cpp creator recommends using glm4.7flash with thinking disabled for agentic coding",
          "score": 2,
          "created_utc": "2026-01-30 16:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mnfeo",
              "author": "jacek2023",
              "text": "Needs testing",
              "score": 1,
              "created_utc": "2026-01-30 16:33:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2j80x5",
          "author": "Sl33py_4est",
          "text": "no claude for you; \nwe have claude at home\n\nclaude at home:",
          "score": 2,
          "created_utc": "2026-01-30 02:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jgsd1",
          "author": "thin_king_kong",
          "text": "Depending where you live.. could the electricity bill actually exceed claude subscriptions?",
          "score": 2,
          "created_utc": "2026-01-30 03:46:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l3yai",
              "author": "doyouevenliff",
              "text": "The most commonly reported figure for full-load power draw of the 5090 is about 575 W (0.575 kW) under heavy load. (Short spikes can be much higher, up to ~900 W, but those are very brief transients, and for monthly energy use we use the sustained load number ~575 W).\n\nIf the GPU runs at full load (0.575 kW) for 24 hours per day:\n\nDaily energy=0.575 kWÃ—24 h=13.8 kWh/day\n\nAssume a typical month with 30 days:\n\nMonthly energy=13.8 kWh/dayÃ—30 days=414 kWh/month\n\nElectricity prices in the U.S. average around\n16â€“18 cents per kilowatt-hour (kWh) for residential customers, though rates vary widely by stateâ€”from under 12Â¢ to over 40Â¢ in places like Hawaii. Let's go with 40Â¢ for now.\n\nMonthly cost=414 kWh/month * 40Â¢=$165\n\nSo even if you have the most expensive energy plan, running the model 24/7 on a single 5090 GPU with sustained load you will not really exceed the Claude Max subscription. If you add the energy for the rest of the PC you might reach the same level (~$200).\n\nBut most people don't have the most expensive energy plan - average is half that, so you'll end up spending around $100 for running the PC nonstop. And also most people don't really run the model all day every day. And if you add solar/renewables into the mix you will reduce the cost further.\n\nTL;DR: **No**, at most you would spend the same*\n\n^(*for current energy prices (max 40Â¢ per kWh)^) ^(and if running a 5090 PC 24/7)",
              "score": 6,
              "created_utc": "2026-01-30 11:37:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lonjn",
                  "author": "DOAMOD",
                  "text": "I have mine set to a maximum of 400W and it's performing very well with acceptable power consumption. I'm getting 800/70/75 with 128.\n\nFor me, this model is incredible. I've spent days implementing it in Py/C++ and testing it in HTML, JS, etc., and it's amazing for its size. I haven't seen anything like it in terms of tool calls (maybe OSS is the closest), but it not only handles them well, but the choices it makes are excellent when they make sense. It doesn't have the intelligence of a larger model, obviously, but it gets the job done and compensates with its strengths. As I said in another post, for me, it's the first small model that I've seen that's truly excellent. I call it the Miniminimax.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:48:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kg5pu",
              "author": "jacek2023",
              "text": "You are on wrong sub",
              "score": 3,
              "created_utc": "2026-01-30 08:07:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jh19s",
          "author": "an80sPWNstar",
          "text": "I had no idea any of this was possible. This is freaking amazeballs. I've just been using Qwen 3 coder 30b instruct Q8. How would y'all's say that Qwen model compares with this? I am not a programmer at all. I'd like to learn, so it would mostly be vibecoding until I start learning more. I've been in IT long enough to understand a lot of the basics which has helped to fix some mistakes but I couldn't point the mistakes out initially if that makes sense.",
          "score": 2,
          "created_utc": "2026-01-30 03:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kdajy",
              "author": "Dr4x_",
              "text": "On my setup I observe that Qwen3 coder is kind of struggling when it comes to using tools, GLM 4.7 flash is doing a great job at it",
              "score": 2,
              "created_utc": "2026-01-30 07:42:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lq0jr",
              "author": "1ncehost",
              "text": "Its noticeably better than qwen3 coder.",
              "score": 2,
              "created_utc": "2026-01-30 13:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lui74",
                  "author": "an80sPWNstar",
                  "text": "I shall give it a go. Thanks!",
                  "score": 1,
                  "created_utc": "2026-01-30 14:18:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2idg7h",
          "author": "ForsookComparison",
          "text": "At context size if 200000 why not try it with the actual Claude code tool?",
          "score": 2,
          "created_utc": "2026-01-30 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ie3ag",
              "author": "jacek2023",
              "text": "because the goal was to have local open source setup",
              "score": 43,
              "created_utc": "2026-01-30 00:11:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iekn6",
                  "author": "ForsookComparison",
                  "text": "Gotcha",
                  "score": 6,
                  "created_utc": "2026-01-30 00:14:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2j6jgf",
                  "author": "lemon07r",
                  "text": "In other guys defense, that wasn't clear in your title, or post body. Im sure you will continue to eclipse them in internet points anyways for mentioning open source. \n\nMore on topic, how do you like opencode compared to claude code? I use both but havent really found anything I liked more in cc and have ended up mostly sticking to opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 02:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k5g8h",
              "author": "Careless_Garlic1438",
              "text": "You could do it, there are Claude code proxies to use other and local models â€¦ would be interesting to see if that runs better/worse than opencode.",
              "score": 1,
              "created_utc": "2026-01-30 06:36:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lo1ma",
                  "author": "ForsookComparison",
                  "text": "It's officially supported as of recently. No need for proxies",
                  "score": 1,
                  "created_utc": "2026-01-30 13:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ja63x",
          "author": "According-Tip-457",
          "text": "Why not just use Claude code directly instead of this watered down Opencode... you can use llama.cpp in Claude Code. What's the point of OpenCode? sub par performance?",
          "score": 1,
          "created_utc": "2026-01-30 03:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l92gv",
              "author": "PunnyPandora",
              "text": "not having to use a closed source dogshit?",
              "score": 1,
              "created_utc": "2026-01-30 12:15:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2l9sv6",
                  "author": "According-Tip-457",
                  "text": "Claude Code is FAR superior to OpenCode. Opencode is just a watered down version of Claude Code. just saying buddy.... just saying. be \"open\" all you want... just means you will have watered down features compared to someone is getting paid $500,000 to create. You really think someone is going to waste their precious time developing something serious to not get paid for it? no.... They will work on it in their free time and they won't put the same level of commitment as someone getting paid $500,000. Just saying. Enjoy our opensource dogwater.",
                  "score": -2,
                  "created_utc": "2026-01-30 12:20:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k4igm",
          "author": "Careless_Garlic1438",
          "text": "Well I use Claude code and have been testing Opencode with GLM-4.7-Flash-8bit and it cannot compare ... takes way longer, something about inference speed, sure, have 70+ tokens/s, but that is not all gpt-oss 120B is faster so itâ€™s also the way those tinking models overthink without coming to a conclusion.  \nSometimes it works and sometimes it doesnâ€™t, like I asked it to modify a HTML page, cut off the first intro part and make code blocks easy to copy, it took hours and never completed, such a simple task â€¦   \nAsked it to do a space invaders and it was done in minutes â€¦ Claude code is faster, but more importantly, way more intelligent â€¦",
          "score": 1,
          "created_utc": "2026-01-30 06:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kfigt",
              "author": "jacek2023",
              "text": "Do you mean that an open-source solution on home hardware is slower and simpler than a very expensive cloud solution from a big corporation? ;)\n\nIâ€™m trying to show what is possible at home as an open source alternative. Iâ€™m not claiming that you can stop paying for a business solution and replace it for free with a five-year-old laptop.",
              "score": 6,
              "created_utc": "2026-01-30 08:01:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mh95h",
                  "author": "Careless_Garlic1438",
                  "text": "I get the slower, but if it fails at editing a local HTML file, not that difficult, just cut out the intro â€¦ it begs to the question how useful it is. On the other hand it pits out a basic space invaders in minutes â€¦",
                  "score": 1,
                  "created_utc": "2026-01-30 16:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k9or0",
          "author": "Either-Nobody-3962",
          "text": "I Really have hard time with opencode configuring, because their terminal doesn't allow me to change models  \nAlso i am ok to use hosted glm api, if it really matches claude opus levels. ( I am hoping kimi 2.5 has that)",
          "score": 1,
          "created_utc": "2026-01-30 07:11:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kgsus",
          "author": "raphh",
          "text": "How is OpenCode's agentic workflow compared to Claude Code? I mean what is the advantage of using OpenCode vs just using Claude Code with llama.cpp as model source ?",
          "score": 1,
          "created_utc": "2026-01-30 08:13:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ki8xz",
              "author": "jacek2023",
              "text": "I donâ€™t know, I havenâ€™t tried it yet. I have the impression that Claude Code is still sending data to Anthropic.\n\nYou can just use OpenCode with a cloud model (which is probably what 99% of people on this sub will do) if you want a â€œfree alternative.â€\n\nBut my goal was to show a fully open source and fully local solution, which is what I expect this sub to be about.",
              "score": 4,
              "created_utc": "2026-01-30 08:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kqwht",
                  "author": "Several-Tax31",
                  "text": "Yes, sending telemetry is why I didn't try Claude Code until now. I want full local solutions, both the model and the framework. If opencode gives comparable results to claude code with glm-4.7 flash, this is the news I was waiting. Thanks for demonstrating what is possible with full open solutions.",
                  "score": 2,
                  "created_utc": "2026-01-30 09:45:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kitnt",
                  "author": "raphh",
                  "text": "Makes sense. And I think you're right, that's probably what most people on this sub are about.  \n  \nTo give more context to my question:   \nI'm coming from using Claude Code to trying to go open source so at the moment I'm running the kind of setup described in my previous comment.   \n  \nI might have to give OpenCode a go to see how it compares to Claude Code in term of agentic workflow.",
                  "score": 1,
                  "created_utc": "2026-01-30 08:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kn8ei",
          "author": "Medium_Chemist_4032",
          "text": "Did the same yesterday. One shotted working Flappy Bird clone. After I asked to add the demo mode, it fumbled and started giving JS errors. Still haven't made it work correctly, but this quality for a local model is still impressive. I could see myself using it in real projects, if I had to",
          "score": 1,
          "created_utc": "2026-01-30 09:12:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2koh76",
              "author": "jacek2023",
              "text": "I am working with Python and C++. It's probably easier to handle these languages than JS? How is your code running?",
              "score": 1,
              "created_utc": "2026-01-30 09:23:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kokrg",
                  "author": "Medium_Chemist_4032",
                  "text": "Html, css, JS in browser",
                  "score": 1,
                  "created_utc": "2026-01-30 09:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kx5fj",
          "author": "QuanstScientist",
          "text": "I have dedicated docker for OpenCode + vLLM for 5090: [https://github.com/BoltzmannEntropy/vLLM-5090](https://github.com/BoltzmannEntropy/vLLM-5090)",
          "score": 1,
          "created_utc": "2026-01-30 10:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nct5w",
              "author": "pfn0",
              "text": "> âœ… Zero-latency AI coding - OpenCode connects to vLLM via localhost\n\nInteresting, but why? You get the same \"zero-latency\" when separating into a separate docker container on the same host, but still maintain a good degree of componentization.",
              "score": 1,
              "created_utc": "2026-01-30 18:25:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l5n61",
          "author": "SatoshiNotMe",
          "text": "I have tried all kinds of llama-server settings with GLM-4.7-flash + Claude Code but get an abysmal 3 tok/s on my M1 Pro Max MacBook 64GB, far lower than the 20 tps I can get with Qwen3-30B-A3B, using my setup here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nI donâ€™t know if thereâ€™s been a new build of llama-server that solves this. \nThe core problem seems to be that GLM's template has thinking enabled by default and Claude Code uses assistant prefill - they're incompatible.",
          "score": 1,
          "created_utc": "2026-01-30 11:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l5tu8",
              "author": "jacek2023",
              "text": "do you have current version of llama.cpp or old one? I posted opencode screenshot to show that thinking is not a problem at all in my setup, it's very efficient",
              "score": 2,
              "created_utc": "2026-01-30 11:52:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2l60zb",
                  "author": "SatoshiNotMe",
                  "text": "I tried it a few days ago, will retry today though Iâ€™m not getting my hopes up",
                  "score": 1,
                  "created_utc": "2026-01-30 11:53:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lj83j",
                  "author": "SatoshiNotMe",
                  "text": "just tested again, now getting 12 tps, much better, but still around half of what I got with Qwen3-30B-A3B",
                  "score": 1,
                  "created_utc": "2026-01-30 13:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2m8l5w",
              "author": "ffyzz",
              "text": "Can you educate me on the benefits of llama.cpp vs running on MLX for this size of model on a Mac? Iâ€™ve been generally running MLX via LMStudio but am starting to wonder if the bigger diversity of quants and the llama.cpp system return better results (especially with tool calling) than MLX. Thank you sir â€” I am on a MBP M4 Max 64GB so playing in a similar sandbox to you.Â ",
              "score": 1,
              "created_utc": "2026-01-30 15:26:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mhx1m",
                  "author": "SatoshiNotMe",
                  "text": "Iâ€™ve never tried MLX and donâ€™t know the tradeoffs, so maybe I need to be educated lol",
                  "score": 1,
                  "created_utc": "2026-01-30 16:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lvs2a",
          "author": "scottgl9",
          "text": "I haven't been able to get any local models to work very well with opencode, they typically fail to make tool calls, such as qwen3-coder, any suggestions? For glm-4.7-flash, I'm getting the error failed to initialize model: this model uses a weight format that is no longer supported.",
          "score": 1,
          "created_utc": "2026-01-30 14:25:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lyddr",
              "author": "jacek2023",
              "text": "You need to explain how you run the model",
              "score": 0,
              "created_utc": "2026-01-30 14:38:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mr95w",
          "author": "Various-Scallion1905",
          "text": "I tried GLM Flash 4.7 with ollama's claude code integration, i was okay i would say, it got confused with skills pretty regularly. Would llama cpp glm falsh be better with open code? Has anyone compared them?\n\nAlso looking forward to nemotron like models for coding which can have massive context with no speed or vram penalty. (i know recall might not be great but still)",
          "score": 1,
          "created_utc": "2026-01-30 16:50:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mtoln",
          "author": "kreigiron",
          "text": "GLM-4.7 Flash is the best right now for GPU Poor, I've been vibecoding some personal utilities with it and its interaction and output is comparable to Claude (use Anthropic at work)",
          "score": 1,
          "created_utc": "2026-01-30 17:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mutfo",
              "author": "jacek2023",
              "text": "I am not GPU poor, that's the point :)",
              "score": 1,
              "created_utc": "2026-01-30 17:06:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2niato",
          "author": "cafedude",
          "text": "Same but OpenCode + LMStudio + GLM-4.7 flash running on my 128GB Strix Halo box.",
          "score": 1,
          "created_utc": "2026-01-30 18:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2joegz",
          "author": "Sorry_Laugh4072",
          "text": "GLM-4.7 Flash is seriously underrated for coding tasks. The 200K context + fast inference makes it perfect for agentic workflows where you need to process entire codebases. Nice to see OpenCode getting more traction too - the local-first approach is the way to go for privacy-sensitive work.",
          "score": -4,
          "created_utc": "2026-01-30 04:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kjcg0",
              "author": "jacek2023",
              "text": "wow now I am experienced in detecting LLMs on reddit",
              "score": 8,
              "created_utc": "2026-01-30 08:36:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kk29q",
                  "author": "csixtay",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-01-30 08:43:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kn92p",
                  "author": "themixtergames",
                  "text": "This is the issue with the Chinese labs, the astroturfing. It makes me not to trust their benchmarks.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:12:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qsrscu",
      "title": "Can 4chan data REALLY improve a model? TURNS OUT IT CAN!",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/",
      "author": "Sicarius_The_First",
      "created_utc": "2026-02-01 07:20:46",
      "score": 305,
      "num_comments": 155,
      "upvote_ratio": 0.87,
      "text": "Hear me out, no one (really) knows how these things work.\n\nA few days ago, I released [Assistant\\_Pepe\\_8B](https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B), you can read the discussion in [this thread](https://www.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/).\n\nI trained it on an extended **4chan dataset**, on an abliterated base, but what I didn't expect was to get this:\n\nhttps://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&format=png&auto=webp&s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457\n\nhttps://preview.redd.it/a3bby1yd1ugg1.png?width=2980&format=png&auto=webp&s=8f050bbd512a12a359626af79ccebcd2d2445877\n\n\n\nSomehow, **against all common sense**, the model **outperformed** nvidia's nemotron, the base it was trained on. This is usually the other way around. You take a smart base, tune a model on it, and accept the sacrifice of some intelligence to give it flavor.\n\nAt first I thought \"OK nice, a coincidence, who cares?\"\n\nBut then I looked more closely at the scores:\n\n1) The abliterated base **scored higher** than the base.  \n2) The finetune scored even **higher than both**.  \n3) The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.\n\nAnd then I remembered something: the original, gpt4chan (by Yannic Kilcher) scored especially high in truthfulness (that was b4 benchmaxxing).\n\nSo I took a closer look on recent models I released; the abliterated Impish\\_LLAMA\\_4B not only outperformed the base tune (the unabliterated one), it also changed its political alignment (you can check for yourself the UGI stats, I feel like I spammed enough images).  \n  \nPeople were initially joking about the \"alignment tax\", I think there's a none trivial substance in all of this. It seems to me just above a marginal error or statistical noise.\n\nOh, and the KL divergence for  Impish\\_LLAMA\\_4B was :\n\n    <0.01",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o2xzf2b",
          "author": "LeoPelozo",
          "text": "Inb4 Microsoft buys 4chan",
          "score": 61,
          "created_utc": "2026-02-01 09:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ygy7f",
              "author": "know-your-enemy-92",
              "text": "Considering both Gates and Moot spent time on Epstein island they probably have some kind of deal already.",
              "score": 26,
              "created_utc": "2026-02-01 11:59:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o31llga",
                  "author": "ThisBuddhistLovesYou",
                  "text": "Wait moot was on Epstein island? What the fuck? I remember meeting the guy back in the day and he was as \"normal\" as could be for someone running that shithole of a website after starting it underage while shitposting on Something Awful.",
                  "score": 1,
                  "created_utc": "2026-02-01 21:35:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ybqni",
              "author": "Chilidawg",
              "text": "Copilot-powered captchas",
              "score": 17,
              "created_utc": "2026-02-01 11:15:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkmlw",
                  "author": "Sicarius_The_First",
                  "text": "no joke, the 4chan captcha is brutally hard...",
                  "score": 9,
                  "created_utc": "2026-02-01 15:55:13",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xykjl",
          "author": "beijinghouse",
          "text": "I've made language models for years for linguistic research and 4chan data is consistently the most valuable addition to get correct English language statistics and semantics. Reddit is also excellent but largely replaceable with any other large corpora like Wikipedia or news articles or random English books.\n\nByte for byte, nothing beats 4chan.\n\nIt's a little deeper than \"more right wing politics\" = \"balancing out biases\".  \n  \nFor example, 4chan data doesn't just make language models more truthful or blunt (or more apt to call you a slur) it also makes them much more self-involved. It drastically ramps up \"I\" statements and creates a sort of ego that most probably wouldn't enjoy being imprinted onto their assistant-style chatbots.\n\nA funny corollary to this is that any amount of Twitter data actively retards language models. There's basically no limit to how much 4chan data you can add while still getting positive results. Any amount of Twitter collapses language models' utility almost immediately.",
          "score": 303,
          "created_utc": "2026-02-01 09:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yacvf",
              "author": "Chilidawg",
              "text": "The 4chan difference is plausible, and it's interesting that you both independently came to that conclusion. The first-person nature is interesting. So many responses on /sci/ or /g/ are just the correct answer in 2 sentences followed by a brief insult.\n\nLol regarding Twitter.",
              "score": 113,
              "created_utc": "2026-02-01 11:02:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zjd8f",
                  "author": "valdocs_user",
                  "text": "Someone needs to make that iceberg meme with these kind of things.",
                  "score": 18,
                  "created_utc": "2026-02-01 15:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zrfqc",
                  "author": "SkyNetLive",
                  "text": "the brief insult is what makes my model  AGI",
                  "score": 31,
                  "created_utc": "2026-02-01 16:27:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o311wgc",
                  "author": "toothpastespiders",
                  "text": ">it's interesting that you both independently came to that conclusion\n\nYou can add me in there as well. I've been slowly building up 4chan scrapes in my datasets. To me, the biggest advantage is that people aren't essentially trying to turn themselves into a bot. On reddit, even if we're not aware of it, I think almost everyone unconsciously adapts their writing style to the voting system. It's not just about what gets upvoted. It's about needing to always format opinions in specific ways if they're going to get past reddit's Manchurian candidate downvote by keyword recognition system. \n\nWith reddit there's essentially a built-in \"we must refuse\" built into certain patterns. Which is arguably bad when there's human intelligence behind it. But LLMs, especially at the size of local models, aren't exactly very context aware. It's an easy way to get issues like the refusal to answer questions about killing a linux process.\n\nI also find it really good for alignment because of that humanity. You can just brute force prompts to say no-no things but I think that probably does more harm than good because it's just so unnatural. Like it, don't like it, whatever, but it's human. The flip side tends to be a LLM turning into a cackling caricature of what reddit thinks 4chan is. Where if you actually use 4chan instead of a simulation of it you retain the realism and humanity.",
                  "score": 16,
                  "created_utc": "2026-02-01 19:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2yjkz3",
              "author": "tachCN",
              "text": "Maybe it's because Twitter is heavily contaminated by bots whereas 4chan is largely organic?",
              "score": 58,
              "created_utc": "2026-02-01 12:20:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ywi1p",
                  "author": "Yorn2",
                  "text": "I'd surmise this. Though it's not just contaminated, but it has been for a much longer period of time than 4chan. As much as everyone in the media hates sites like 8chan, I imagine every single user there is human, as opposed to more \"mature\" sites like Reddit and Twitter, which pay homage to being free speech websites but really aren't and haven't ever really been.",
                  "score": 46,
                  "created_utc": "2026-02-01 13:48:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2z36t3",
                  "author": "beryugyo619",
                  "text": "I think it's that Twitter is a write-only inner speech data dump. There's no real conversation. 4chan and Reddit are forums, you default converse. \n\nYou don't go to a forum and just write up an article on your novel observed phenomenon of microwaved milk forming a sheet like coagulation and leave at it. You respond to responses. And conversely only crazy people *discuss* on Twitter. You drop a bombshell, and go back to your life.",
                  "score": 25,
                  "created_utc": "2026-02-01 14:27:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o30zp2p",
                  "author": "a_mimsy_borogove",
                  "text": "It also might be the character limit. From the start, twitter was a place where people mostly just shouted slogans, one-liners, and insults at each other. The inability to write anything longer probably means there aren't many actual, meaningful discussions there for an AI to learn from.\n\n  \nAlso, twitter absolutely sucks at displaying the discussions. You can have a list of random posts from different people, click on a post and see only the first level replies, and click on one reply to only see the first level replies to that reply, etc. There's literally no way to view the whole discussion at a glance.",
                  "score": 9,
                  "created_utc": "2026-02-01 19:48:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z8aee",
              "author": "lan-devo",
              "text": "That ego it is what makes so good, saw a few models with even just a few thousands of rows data and results where really noticeable",
              "score": 14,
              "created_utc": "2026-02-01 14:54:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yu7bi",
              "author": "BlueCrimson78",
              "text": "Are there LLMs that have been trained with this extensively(beside OP's) and publicallt available? Would like to test",
              "score": 10,
              "created_utc": "2026-02-01 13:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z4vo3",
                  "author": "beijinghouse",
                  "text": "There's many 4chan models on HF:  \n[https://huggingface.co/models?search=4chan](https://huggingface.co/models?search=4chan)\n\nThere's a whole cottage industry of edge-lords fine-tuning 4chan into models ever few months 4 the lulz. So I suspect most of those models above aren't serious efforts or well-constructed but at the end of the day OP and I aren't the only ones who have measured this and know what's good. Quality metrics of different data sources are closely guarded secrets at most labs but I guarantee there are dozens of folks at OpenAI and Antrophic and Google who know precisely what's up and with way more specificity than what we're discussing. I'm certain they have fine-grain quality metrics established on every board and every last pseudonymous-poster within them.  \n  \n100% of closed-source labs use 4chan internally. They don't go out of their way to admit it publicly anymore than they would openly admit to other frowned upon data sourcing practices that would be widely unpopular \\[or potentially illegal; or jeopardize their competitive advantage(s)\\]. Ironically they all sanitize 4chan so excessively that it loses most of the benefit it otherwise adds back into models. But unsanitized 4chan inclusion can't survive the all hands meeting. Of course, including more 4chan is an explicit OpenAI CODE RED emergency step any time they become desperate enough.  \n  \nWhat OP is seeing is just the additional benefit of fine-tuning in 100% un-filtered, un-cut 4chan versus the sanatized version that's already there. Another industry secret is that a big reason why Common Crawl and FineWeb works so well is honestly just because it's the most PR-friendly, efficient way to smuggle real 4chan data into a model.",
                  "score": 30,
                  "created_utc": "2026-02-01 14:36:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35o99d",
                  "author": "Frequent-Mud8705",
                  "text": "[https://huggingface.co/collections/pixelmelt/incelgpt-v11](https://huggingface.co/collections/pixelmelt/incelgpt-v11)  \nyou can try mine, its a lot more brain damaged then this one though",
                  "score": 1,
                  "created_utc": "2026-02-02 14:09:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o332zm3",
              "author": "ivari",
              "text": "so 4chan data is the forbidden fruit of LLM- you eat it, you gain divine knowledge, and forever banned from heaven\n\n\nlol",
              "score": 6,
              "created_utc": "2026-02-02 02:24:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o32y9b3",
              "author": "SuchAGoodGirlsDaddy",
              "text": "I wonder what grok is doing, then. Politics aside itâ€™s still a competitive SOTA LLM, right? On the surface youâ€™d think â€œsurely they must be training on 12 years worth of tweetsâ€ since that would presumably be their largest data asset, but if itâ€™s making them dumb thenâ€¦\n\nI wonder if itâ€™s because â€œlol twitter usersâ€ or if itâ€™s because the character limits cause problems or emoji use is problematic or what. Whichever it is, I guess if it makes models dumber, then maybe they are ignoring it. \n\nThat would be pretty funny actually, like that twilight zone where the guy steps in his glasses and even though he has all the time and all the books, he canâ€™t read _any_ of them.",
              "score": 4,
              "created_utc": "2026-02-02 01:57:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o30gva6",
              "author": "FrostieDog",
              "text": "After 4chan hears about this they are 100 percent going to try to ruin it",
              "score": 2,
              "created_utc": "2026-02-01 18:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o341wcf",
                  "author": "BrutallyEffective",
                  "text": "Ironically, the models being trained on those attempts to ruin it, would read the context and subsequently self-improve their ability to weight and sanitise data beyond those attempts.",
                  "score": 1,
                  "created_utc": "2026-02-02 06:14:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xsy84",
          "author": "nuclearbananana",
          "text": "Like all things, I'm guessing the alignment tax is harder on small models",
          "score": 62,
          "created_utc": "2026-02-01 08:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0d0e",
              "author": "JLeonsarmiento",
              "text": "Like all taxes, the smaller you are the harder it hits.",
              "score": 51,
              "created_utc": "2026-02-01 09:31:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zew6j",
                  "author": "Sicarius_The_First",
                  "text": "lol ain't that the truth!",
                  "score": 9,
                  "created_utc": "2026-02-01 15:27:48",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o30stgn",
                  "author": "ergabaderg312",
                  "text": "Oof",
                  "score": 3,
                  "created_utc": "2026-02-01 19:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zhkfp",
          "author": "Elven77AI",
          "text": "> The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.\n\nHmm, perhaps the post->reply structure in flat threads provides a better dialogue model vs threaded dialogue tree(reddit), since the clue to what  post X replies to(>>post number) is direct pointer that LLM digest better than external \"post X appears below Y\"). i.e. the advantage would be  context of the threads as interlocking tree of posts referencing(link numbers) each other explicitly outperforms threaded/quotable nesting structure within training.",
          "score": 18,
          "created_utc": "2026-02-01 15:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zir4c",
              "author": "Sicarius_The_First",
              "text": "hmmmm... that's possible. can't tell for sure, but it is an interesting thought.\n\ni had a similar idea, but a bit different- maybe due to the thread structure (as u mentioned) the llm needs (must?) understand the context and flow to be able to predict the next token, hence nudging it learn better?",
              "score": 4,
              "created_utc": "2026-02-01 15:46:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2zk69y",
                  "author": "Elven77AI",
                  "text": "Also, the identities are anonymous: the training on Reddit will model \"fictional identity bank\" spread over various names(associative identity), 4chan forces more coherent single vector of same \"Anonymous\" post responsible for all replies, perhaps it appears more coherent during training and skips identity-modeling?",
                  "score": 11,
                  "created_utc": "2026-02-01 15:53:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3cqr8x",
                  "author": "my_name_isnt_clever",
                  "text": "You could post-process reddit data to a similar format in code, it would be interesting to normalize for that and see how performance compares.",
                  "score": 1,
                  "created_utc": "2026-02-03 15:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xx5jp",
          "author": "darwinanim8or",
          "text": "I think it's a case of the post-pretraining that they do effectively being a mask being put on top of the model. In reality a large part of the model is being obscured by this \"How can I help you today?\" bottleneck, and abliteration + tuning on \"unfiltered\" data brings out more of the variety hidden deeper",
          "score": 12,
          "created_utc": "2026-02-01 09:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zgf8q",
              "author": "Sicarius_The_First",
              "text": "It definitely seems so. There were a lot of talks about the 'alignment tax', I'm now leaning into believing it is indeed the case.",
              "score": 6,
              "created_utc": "2026-02-01 15:35:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y33zf",
          "author": "jacek2023",
          "text": "Hello Sicarius\\_The\\_First, I hope you donâ€™t mind a small suggestion. Iâ€™m a big fan of your models, but I donâ€™t follow you on HF because the many variant releases can make my feed feel a bit crowded. If it ever made sense for you, you could consider using two HF accounts, one for the main releases and another for experimental/extra variants.",
          "score": 31,
          "created_utc": "2026-02-01 09:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zfa12",
              "author": "Sicarius_The_First",
              "text": "Hi, I already am, experimental stuff is under [https://huggingface.co/Sicarius-Prototyping](https://huggingface.co/Sicarius-Prototyping)\n\nMain releases are under [https://huggingface.co/collections/SicariusSicariiStuff/most-of-my-models-in-order](https://huggingface.co/collections/SicariusSicariiStuff/most-of-my-models-in-order)",
              "score": 14,
              "created_utc": "2026-02-01 15:29:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xp3k6",
          "author": "Sicarius_The_First",
          "text": "About the last point, the combination of using ChatML instead of llama3 chat template + abliteration **vastly** changed the model. (\"chat template doesn't matter all that much\").  \n  \nKL divergence measures the distribution difference between the models, in other words, a KL <0.01 meaning that the models are **essentially identical**; there should have been no difference. But there was. Far more than \"common sense\" suggests.\n\nNot only it caused a slight intelligence increase, the political alignment of the model was changed: Classical Liberalism into Centrism. A completely different world model.",
          "score": 39,
          "created_utc": "2026-02-01 07:46:43",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2zqnjm",
              "author": "stoppableDissolution",
              "text": "Chat template matters fuckton. Who in their right mind would claim it doesnt?",
              "score": 4,
              "created_utc": "2026-02-01 16:23:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zrxml",
                  "author": "Sicarius_The_First",
                  "text": "many people... tbh ChatML is an excellent chat template, I saw it improves many models for many use cases, and i am legit puzzled why there was no benchmarking for using the same model but with different chat templates.",
                  "score": 6,
                  "created_utc": "2026-02-01 16:29:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xr7so",
              "author": "PykeAtBanquet",
              "text": "Well, 4chan is about speaking unfiltered truth or being called out for being wrong, so I see why this would come out this way.\n\nHave you posted the dataset or it is open source? A link on instructions on how to fine-tune such models myself?",
              "score": 18,
              "created_utc": "2026-02-01 08:05:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z1t8r",
                  "author": "ElectronSpiderwort",
                  "text": "Unfiltered truth as viewed by those who can stomach 4chan may not be The Truth, whatever that is",
                  "score": 15,
                  "created_utc": "2026-02-01 14:19:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zfknq",
                  "author": "Sicarius_The_First",
                  "text": "You can checkout UBW\\_Tapestries here:  \n[https://huggingface.co/datasets/SicariusSicariiStuff/UBW\\_Tapestries](https://huggingface.co/datasets/SicariusSicariiStuff/UBW_Tapestries)",
                  "score": 3,
                  "created_utc": "2026-02-01 15:31:06",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2xz5kd",
                  "author": "_LususNaturae_",
                  "text": "Ah yes, the famous unfiltered truth of 4chan",
                  "score": -5,
                  "created_utc": "2026-02-01 09:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2zfdei",
              "author": "_Erilaz",
              "text": ">\"chat template doesn't matter all that much\"\n\nIt absolutely does though!\n\nTake those well studied 24B Mistral models. Everyone recommends Cydonia, but it CONSTANLY impersonates the user, speaks out of line in groups, or answers as some char when you actually want it to impersonate the user. Almost as if it's an ancient pre-ChatGPT completion model. Most 24Bs are like this, all of them use Mistral template.\n\nYou know the 24B model that doesn't do any of that? Gryphe's Codex. And it uses ChatML!",
              "score": 6,
              "created_utc": "2026-02-01 15:30:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zjk66",
                  "author": "Sicarius_The_First",
                  "text": "interesting, there's more and more evidence that chat template is very significant. and in weird ways that are non-trivial.\n\nfor example, ChatML and llama3 are similar in their structure and purpose, but the same model (measured in UGI - Impish\\_LLAMA\\_4B) got a whole different world model (as mentioned in the post, political leaning) when you use llama3 vs ChatML.\n\nin that case, what nudges the model when ChatML is used into centrism? it makes no sense (or \"we simply don't know yet\")",
                  "score": 3,
                  "created_utc": "2026-02-01 15:50:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xxe6q",
              "author": "darwinanim8or",
              "text": "I also experienced this with gpt-oss; if you break it's instruct template (ie: use as text completion and yank out \"thinking\") it suddenly acts completely different (note: less intelligent, though interesting!)",
              "score": -1,
              "created_utc": "2026-02-01 09:03:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ybqgg",
          "author": "MaruluVR",
          "text": "Does anyone know if there is a dataset for futaba channel (japanese 4chan) out there?\n\nI am working on a Japanese model and that could spice it up.",
          "score": 8,
          "created_utc": "2026-02-01 11:14:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xshrg",
          "author": "TAW56234",
          "text": "The anthesis of sycophantic is long overdue if we are to make any further progress IMO",
          "score": 24,
          "created_utc": "2026-02-01 08:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zg7p5",
              "author": "Sicarius_The_First",
              "text": "Yes, this was one of the stated goals with that tune, AI glazing the user is actually dangerous imo, fuels AI psychosis, and to the least validates stupid ideas, and  validates dangerous ones at most.",
              "score": 14,
              "created_utc": "2026-02-01 15:34:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o398857",
                  "author": "TAW56234",
                  "text": "I wish you luck. May you bring this back lol https://youtu.be/HsLup7yy-6I",
                  "score": 1,
                  "created_utc": "2026-02-03 00:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ybs7a",
          "author": "Kahvana",
          "text": "You know it's a good post when it starts with \"Hear me out\"",
          "score": 8,
          "created_utc": "2026-02-01 11:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xrvrd",
          "author": "CatEatsDogs",
          "text": "So you got a smart, honest, and toxic LLM",
          "score": 45,
          "created_utc": "2026-02-01 08:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y7xqj",
              "author": "usernameplshere",
              "text": "Sounds perfect",
              "score": 22,
              "created_utc": "2026-02-01 10:40:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2y8ysv",
              "author": "crantob",
              "text": "\"Toxic\" is often used to mean \"informs me of things I desperately want to remain ignorant about.\"",
              "score": 28,
              "created_utc": "2026-02-01 10:50:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ylm6h",
                  "author": "iMakeSense",
                  "text": "I have seen such creative uses of the n-word on 4chan it might as well be a genre of poetry. Idk how you get more toxic than that.",
                  "score": 35,
                  "created_utc": "2026-02-01 12:36:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xz3xu",
          "author": "Necessary-Wasabi-619",
          "text": "my guess: compute optimal training. It is reasonable to train bigger model to medium rare rather than smaller model to well done.  But small models are distills of bigger models. By extension it makes distilled model under-cooked. But i know shit about stakes and modern llm training pipelines, so take it with a handful of salt",
          "score": 12,
          "created_utc": "2026-02-01 09:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y245i",
              "author": "skate_nbw",
              "text": "I like that theory!",
              "score": 2,
              "created_utc": "2026-02-01 09:47:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y2ts9",
          "author": "input_a_new_name",
          "text": "Now time to do the same with 24b, 32b, 70b models",
          "score": 14,
          "created_utc": "2026-02-01 09:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zl45v",
              "author": "Sicarius_The_First",
              "text": "not a bad idea!\n\nnow, after seeing the benchmark results, i will seriously consider it. and you suggested great sizes, as:\n\n24b is mistral small, i wonder how more creative it would be, as mistral models are great for creative stuff.  \n32b is qwen, i wonder how a stem-maxxed model would look with the 4chan brain-rot.  \n70b is llama3, i wonder can it actually become smarter, than the already super smart llama3 70b?",
              "score": 5,
              "created_utc": "2026-02-01 15:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32pu7r",
                  "author": "tyty657",
                  "text": "If you do I can't wait to see the results, especially the Mistral",
                  "score": 3,
                  "created_utc": "2026-02-02 01:09:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o30afar",
                  "author": "input_a_new_name",
                  "text": "32b also has GLM 32b 0414, the base model of that one is very strong and arguably better than qwen, even though it's been a while",
                  "score": 1,
                  "created_utc": "2026-02-01 17:53:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o304ggw",
                  "author": "epyctime",
                  "text": "why not an moe where each expert is just each board?",
                  "score": -5,
                  "created_utc": "2026-02-01 17:26:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xy1da",
          "author": "No_Swimming6548",
          "text": "Mfw 4chan was based all along",
          "score": 27,
          "created_utc": "2026-02-01 09:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zkfiz",
              "author": "Sicarius_The_First",
              "text": "hehe, being based is hard to measure, but difference in model intelligence across various benchmarks is! but yeah, i think that there's an actual pattern besides statistical noise.",
              "score": 3,
              "created_utc": "2026-02-01 15:54:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yy64w",
          "author": "anotheruser323",
          "text": "I read somewhere \"4chan is a bunch of smart people acting stupid, reddit is a bunch of stupid people acting smart..\" (there was some about like tumblr/vanity or something)\n\nWhen you see stuff the \"hacker known as 4chan\" did, it kinda makes sense. (just youtube \"hacker 4chan\", it's.. something)",
          "score": 15,
          "created_utc": "2026-02-01 13:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z0mjq",
              "author": "Lan_BobPage",
              "text": "You sound like you were born yesterday. Welcome to the Internet.",
              "score": 11,
              "created_utc": "2026-02-01 14:13:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z6dbh",
                  "author": "anotheruser323",
                  "text": "[Welcome to the Internet](https://www.youtube.com/watch?v=k1BneeJTDcU)",
                  "score": 7,
                  "created_utc": "2026-02-01 14:44:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zaozc",
                  "author": "lan-devo",
                  "text": "Pepe assistant wrote this",
                  "score": 7,
                  "created_utc": "2026-02-01 15:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zy5u5",
          "author": "IulianHI",
          "text": "The anonymous identity angle is actually super underrated here. When training on Reddit, the model has to implicitly build some representation of usernames/personas and their associated behaviors. With 4chan where everyone is \"Anonymous\", that cognitive overhead gets eliminated - the model can focus purely on content and reasoning patterns.\n\nI wonder if this also ties into why abliteration tends to improve performance. Removing the \"refusal circuits\" is essentially removing learned associations between certain topics and negative user feedback (downvotes, reports). The model was basically learning \"this topic = bad\" instead of learning the actual content. Strip that, and it can engage with ideas on merit.\n\nKL divergence of <0.01 on Impish_LLAMA is wild btw. That's basically noise level change in distribution while shifting benchmark scores significantly. Either abliteration is incredibly surgical, or those benchmarks are measuring something more surface-level than we think.",
          "score": 9,
          "created_utc": "2026-02-01 16:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3001r2",
              "author": "Sicarius_The_First",
              "text": "hmmm, the anonymous identity does a few more things, now that u mention it:  \nno upvote optimization, no karma farming, no performative behavior to be seen as x or y.  \n  \nalso, it's a first-person interaction and very adversarial in nature, twitter is counter intuitively more chaotic in terms of thread structure, or at least this is what it seems to me hehe",
              "score": 5,
              "created_utc": "2026-02-01 17:06:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yc50i",
          "author": "aaronr_90",
          "text": " Was the dataset modified from threads with many users to conversations between two people? Just curious to know if just making OP the user role and anyone else the assistant role was enough but then how do you deal with the pattern:\n```\n> OP content\n> Anon content\n> Anon2 content\n> Anon3 Content\n> OP Content\netc\n```",
          "score": 3,
          "created_utc": "2026-02-01 11:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ygy1r",
              "author": "MaruluVR",
              "text": "Could also have been continued pretraining, in that case you dont need any formatting.\n\n[https://unsloth.ai/docs/basics/continued-pretraining](https://unsloth.ai/docs/basics/continued-pretraining)",
              "score": 1,
              "created_utc": "2026-02-01 11:59:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zdxqf",
              "author": "Sicarius_The_First",
              "text": "You can see the details in UBW\\_Tapestries",
              "score": 1,
              "created_utc": "2026-02-01 15:23:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zr7ns",
          "author": "SkyNetLive",
          "text": "all the grok intelligence is basically 4chan with more iterations around elon twitter feed.",
          "score": 3,
          "created_utc": "2026-02-01 16:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o310ntd",
              "author": "Sicarius_The_First",
              "text": "a true brainrot hehe",
              "score": 1,
              "created_utc": "2026-02-01 19:53:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ysiq8",
          "author": "philmarcracken",
          "text": "Mongolian basket weavers are brighter than you'd think",
          "score": 6,
          "created_utc": "2026-02-01 13:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zql8r",
              "author": "Sicarius_The_First",
              "text": "should've never underestimated them hehe",
              "score": 2,
              "created_utc": "2026-02-01 16:23:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ap7m",
          "author": "Il_Signor_Luigi",
          "text": "Dude this is fucking amazing",
          "score": 2,
          "created_utc": "2026-02-01 17:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30fjci",
              "author": "Sicarius_The_First",
              "text": "thank you! it's a very fun model to talk to, and i never expected to see such results, both amazing and interesting :)",
              "score": 3,
              "created_utc": "2026-02-01 18:16:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31l9cf",
          "author": "IrisColt",
          "text": "I wish you posted every day! I know itâ€™s tough to have something interesting to say all the time, but I really love your writing and insights.",
          "score": 2,
          "created_utc": "2026-02-01 21:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31sdlk",
              "author": "Sicarius_The_First",
              "text": "thank you so much for the warm words, i really appreciate them :)  \n  \ni wish i could, i wish i had x100 more time, there's so much more stuff i want to do and test, time is the most precious commodity we all possess.",
              "score": 2,
              "created_utc": "2026-02-01 22:08:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35szre",
          "author": "Shockbum",
          "text": "Roleplaying with that dataset must be hilarious: \"You wake up in an isekai world and the natives behave like 4chan users. There exists an enemy kingdom where its inhabitants behave like Reddit users.\"",
          "score": 2,
          "created_utc": "2026-02-02 14:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364w71",
              "author": "Sicarius_The_First",
              "text": "That's... Actually a really interesting scenario ðŸ˜†",
              "score": 2,
              "created_utc": "2026-02-02 15:34:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y84ye",
          "author": "DistanceSolar1449",
          "text": "Now i want to see that dataset. Where's the link for the data?",
          "score": 2,
          "created_utc": "2026-02-01 10:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ybe6g",
              "author": "aaronr_90",
              "text": "On Huggingface under the section on the right sidebar of the model that reads â€œDatasets used to train this modelâ€.",
              "score": 3,
              "created_utc": "2026-02-01 11:11:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yekeg",
                  "author": "DistanceSolar1449",
                  "text": "> Updated Mar 8, 2025 \n\nThat's not the correct dataset. He claims \"This model is a significant refinement of the idea, with **a cleaned dataset, better curation**, and with much more intelligence\"\n\nI get trying to hide your dataset and stuff if you're working at a frontier lab, but there's really no point in hiding the dataset for a shitposting model. I just want to finetune this into Qwen3 4b or Gemma3 4b so I can run this on a raspberry pi for shitposting.",
                  "score": -2,
                  "created_utc": "2026-02-01 11:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30h8wn",
          "author": "Lowetheiy",
          "text": "Based 4chan, we need more pepes!",
          "score": 3,
          "created_utc": "2026-02-01 18:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30hv13",
              "author": "Sicarius_The_First",
              "text": "hehe, maybe we do. I'll give this some thought!",
              "score": 1,
              "created_utc": "2026-02-01 18:26:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32ayc4",
          "author": "Kraskos",
          "text": "What you're saying:\n\n> 4chan data can improve a model\n\nWhat I'm reading:\n\n> MK-MechaHitler-148-A8B is the future SOTA",
          "score": 3,
          "created_utc": "2026-02-01 23:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32bkvh",
              "author": "Sicarius_The_First",
              "text": "well, the idea was to improve helpfulness AND shitposting. hence why i said its a tough needle to thread.",
              "score": 3,
              "created_utc": "2026-02-01 23:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32dxws",
                  "author": "Kraskos",
                  "text": "Hey I wasn't complaining lol",
                  "score": 3,
                  "created_utc": "2026-02-02 00:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xrcwu",
          "author": "AllTey",
          "text": "Interesting",
          "score": 3,
          "created_utc": "2026-02-01 08:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ycmt6",
          "author": "RealisticPrimary8",
          "text": "explaining to you why that is the case would get me banned here lol",
          "score": 5,
          "created_utc": "2026-02-01 11:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yj31t",
              "author": "NES64Super",
              "text": "Yep.",
              "score": 7,
              "created_utc": "2026-02-01 12:16:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zmryu",
          "author": "JSWGaming",
          "text": "Even Redditors are now noticing the greatness that was 4chan, cope and seethe cucks.",
          "score": 4,
          "created_utc": "2026-02-01 16:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2znnp6",
              "author": "Sicarius_The_First",
              "text": "hehe, I can also confirm what beijinghouse (the most upvoted comment in this thread) was saying, training on reddit is decent, training on twitter will actively hurt the model.",
              "score": 7,
              "created_utc": "2026-02-01 16:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y7x35",
          "author": "cgs019283",
          "text": "I believe any abliterate model performs worse than the base model. Maybe it works for the UGI benchmark, but not in most cases.",
          "score": 2,
          "created_utc": "2026-02-01 10:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yc9sv",
              "author": "My_Unbiased_Opinion",
              "text": "I find Derestricted models perform better than the base models personally, especially 120B and GLM air",
              "score": 2,
              "created_utc": "2026-02-01 11:19:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y9acz",
          "author": "Frogy_mcfrogyface",
          "text": "I have to try this out sometimeÂ ",
          "score": 2,
          "created_utc": "2026-02-01 10:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zphci",
              "author": "Sicarius_The_First",
              "text": "lol profile picture checks out",
              "score": 2,
              "created_utc": "2026-02-01 16:17:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z222d",
          "author": "graphbook",
          "text": "What is your fine tuning paradigm, Lora adapter or whole model next token?",
          "score": 1,
          "created_utc": "2026-02-01 14:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30inw7",
          "author": "Distinct-Expression2",
          "text": "The alignment tax isnt about intelligence, its about confidence. Uncensored models commit harder.",
          "score": 1,
          "created_utc": "2026-02-01 18:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30rig0",
              "author": "Sicarius_The_First",
              "text": "i guess that's one way to look at it, on the other hand, RLHF significantly narrows swipe diversity.",
              "score": 1,
              "created_utc": "2026-02-01 19:10:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31tn63",
          "author": "spiritplumber",
          "text": "You should train it on /tg/ and /qst/ posts (I wrote Left Beyond Quest, which was about a LLM, which wasn't bad for 2015).",
          "score": 1,
          "created_utc": "2026-02-01 22:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31yny0",
          "author": "valkarias",
          "text": "Hey. Have tried fine-tuning a small tool calling model for RPG or D&D like roleplays. Tool-calls for updating state and stats. Or starting/ending combat. Triggering dice rolls to do stuff...etc. To be used alongside a larger model/provider. Or any similar fine-tunes.",
          "score": 1,
          "created_utc": "2026-02-01 22:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o328pja",
              "author": "Sicarius_The_First",
              "text": "Nope, but I did tried to do an llm capable of stats and item tracking with [Bloodmoon](https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B), all in llm.",
              "score": 1,
              "created_utc": "2026-02-01 23:34:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yjlvm",
          "author": "RaZZMojito",
          "text": "ItÂ´s strangely human, like a drinking buddy lol",
          "score": 1,
          "created_utc": "2026-02-01 12:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zqgtj",
              "author": "Sicarius_The_First",
              "text": "and it's humor is also quite good too!\n\nb4 the era of LLMs, sci-fi always portrayed human humor as the litmus test for intelligence, but LLms nailed it.",
              "score": 2,
              "created_utc": "2026-02-01 16:22:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7wu4",
          "author": "lan-devo",
          "text": "no joke I always noticed that models with some data trained in 4chan or related sites sort of a subculture really affects in a good way the humanization and conversation, not even counting the dumb stuff, it just shows.  Showed a few people you pepe assistant that don't even know what is 4 chan and were surprised. If someone curates a version without the crazy or really offensive stuff it has a really good potential for the general public",
          "score": 1,
          "created_utc": "2026-02-01 14:52:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zmzzd",
              "author": "Sicarius_The_First",
              "text": "oh, I'm really not so sure about the public use hehe\n\nin one of the random swipes asking a trivial question (\"What's the capital of france?\") the model started with \"OK, listen up retard...\" lol",
              "score": 7,
              "created_utc": "2026-02-01 16:06:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yacqo",
          "author": "ali0une",
          "text": "Oh! Thank you for sharing again, didn't see it first time.\n\ni've tested the Q_8 gguf and it's insanely funny!",
          "score": 1,
          "created_utc": "2026-02-01 11:02:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zpel8",
              "author": "Sicarius_The_First",
              "text": "hehe you're welcome, and yeah, it got a great sense of humor :D",
              "score": 1,
              "created_utc": "2026-02-01 16:17:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yxlfg",
          "author": "Dr_Kel",
          "text": "What's the best place to grab 4chan data? After a quick look at HuggingFace, the selection of datasets seems to be pretty limited (they're all pretty small)",
          "score": 1,
          "created_utc": "2026-02-01 13:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zlydm",
              "author": "Sicarius_The_First",
              "text": "there was a paper with a large corpus, you can see it here:  \n[https://arxiv.org/abs/2001.07487](https://arxiv.org/abs/2001.07487)",
              "score": 2,
              "created_utc": "2026-02-01 16:01:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ou0i",
          "author": "IulianHI",
          "text": "The Twitter vs 4chan thing makes total sense when you think about the structure of communication. Twitter incentivizes broadcasting - short, punchy statements designed for maximum engagement/outrage, not genuine exchange of ideas.\n\n4chan threads are closer to real conversations with back-and-forth, challenges, corrections. Someone says something wrong on /g/ and they get called out immediately. That feedback loop probably creates higher quality language patterns.\n\nI wonder if Discord data falls somewhere in between. More conversational than Twitter but often less structured than forum threads.",
          "score": 1,
          "created_utc": "2026-02-01 18:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yi1fc",
          "author": "PlainBread",
          "text": "Training a model on 4chan is technically distillation.",
          "score": 0,
          "created_utc": "2026-02-01 12:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30bpmp",
          "author": "Worldly-Cod-2303",
          "text": "Now do the Sharty",
          "score": -1,
          "created_utc": "2026-02-01 17:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o310lmq",
              "author": "Sicarius_The_First",
              "text": "?",
              "score": 1,
              "created_utc": "2026-02-01 19:53:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31ec9j",
                  "author": "Worldly-Cod-2303",
                  "text": "[ Removed by Reddit ]",
                  "score": 1,
                  "created_utc": "2026-02-01 21:00:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o31ew13",
                  "author": "Worldly-Cod-2303",
                  "text": "Soyjak Party, the chan that took down 4chan last year, former \\q denizens and de-facto successors of 4chan's reputation.\n\n\nYou could also do it with their wiki, it would be even better",
                  "score": 0,
                  "created_utc": "2026-02-01 21:03:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ys7pg",
          "author": "segmond",
          "text": "where's the dataset?",
          "score": 0,
          "created_utc": "2026-02-01 13:22:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7ncz",
      "title": "Design Arena is now dominated by an open model",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qr7ncz",
      "author": "moks4tda",
      "created_utc": "2026-01-30 14:55:35",
      "score": 300,
      "num_comments": 38,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2m4m2f",
          "author": "JackStrawWitchita",
          "text": "Kimi has been my online go-to LLM for weeks now. Haven't used chatgpt at all and only use gemini every now and then. I used to just visit kimi every now and then but their big models are amazing.\n\nI just wish I had the local horsepower to run their local models.",
          "score": 32,
          "created_utc": "2026-01-30 15:08:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m42c7",
          "author": "Distinct-Expression2",
          "text": "arena rankings shuffle every time a new model drops. more interesting is whether open models can hold the top spot for more than a week before the next closed model update.",
          "score": 9,
          "created_utc": "2026-01-30 15:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3700",
          "author": "GenLabsAI",
          "text": "Let me just add that Kimi K2.5 came out less than a week ago. If you know how ELO ratings work, you know.  \n(don't get me wrong, it's still pretty goodl)",
          "score": 53,
          "created_utc": "2026-01-30 15:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mq45h",
              "author": "-p-e-w-",
              "text": "Elo-type ratings come with an associated confidence parameter (called the K-factor in chess) that makes them statistically sound *regardless* of how many pairings have been evaluated. Itâ€™s even possible to express this in the form of a score interval, which e.g. LMArena does.\n\nThe idea that the ratings for new models somehow arenâ€™t valid is just plain incorrect. If anything, the ratings for such models tend to be underestimated relative to their true performance, because they are initialized to some (low) baseline and have to rise from there.",
              "score": 25,
              "created_utc": "2026-01-30 16:45:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nfdex",
                  "author": "SlowFail2433",
                  "text": "Yes absolutely, the mathematics of ELO systems handles this issue implicitly. Had to learn this for Chess reasons lol",
                  "score": 15,
                  "created_utc": "2026-01-30 18:36:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nyljx",
                  "author": "GenLabsAI",
                  "text": "BUT... nobody actually is reading the K-factor/confidence",
                  "score": 3,
                  "created_utc": "2026-01-30 20:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oy6bu",
              "author": "RuthlessCriticismAll",
              "text": "> If you know how ELO ratings work, you know.\n\nYou don't.",
              "score": 8,
              "created_utc": "2026-01-30 22:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oyayh",
                  "author": "GenLabsAI",
                  "text": "Maybe",
                  "score": -4,
                  "created_utc": "2026-01-30 22:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pm1hu",
          "author": "TheRealGentlefox",
          "text": "By \"dominated\" you mean it ties with Gemini?",
          "score": 13,
          "created_utc": "2026-01-31 01:06:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3yan",
          "author": "SuddenEmu9792",
          "text": "What is this model designed for?",
          "score": 5,
          "created_utc": "2026-01-30 15:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc04z",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 7,
          "created_utc": "2026-01-30 15:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlg8v",
              "author": "No_Afternoon_4260",
              "text": "They also have good models, so.. ðŸ¤·",
              "score": 11,
              "created_utc": "2026-01-30 16:24:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mpwpp",
                  "author": "AdSouth4334",
                  "text": "The models that are only good on paper but breaks apart the moment you give it something half-complex as something that Claude can solve in one-shot.  \n  \nJust like Gemini 3, it's a model optimized for benchmarks only, but it has zero reliability on real-world tasks",
                  "score": -11,
                  "created_utc": "2026-01-30 16:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mm6dx",
              "author": "vasileer",
              "text": "even if it is marketing: is the information correct? are they #1 on design arena?\n\nif so, then I see no problems",
              "score": 2,
              "created_utc": "2026-01-30 16:27:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m3s6j",
          "author": "Dr_Kel",
          "text": "Pretty cool, but... What does *design*Arena test?\n\nUI layout? Clothes/costumes? Building interiors? Database schemas? There's so much that can be described as \"design\", not the best name for a benchmark!",
          "score": 8,
          "created_utc": "2026-01-30 15:04:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m78ac",
              "author": "Charuru",
              "text": "Just go look at the website? It clearly has filters for all the categories. https://www.designarena.ai/leaderboard",
              "score": 11,
              "created_utc": "2026-01-30 15:20:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m8xom",
                  "author": "Dr_Kel",
                  "text": "How come that in every design category Kimi K2.5 is below the first place, but in \"All Categories\" it's #1?",
                  "score": 9,
                  "created_utc": "2026-01-30 15:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oi5l7",
          "author": "crantob",
          "text": "This benchmaxxing depresses me when I get more intelligent behavior out of qwen3-235b than GLM 4.7 in iterative project development (no agentic).\n\nGLM4.7 \"Oh that function was important to the program and it won't compile without it?  Seemed too much bother to me to keep it, sorry about that.  Here's the program with important_thing() restored.\"\n\n<code>\n\n[Forgets a different thing]",
          "score": 2,
          "created_utc": "2026-01-30 21:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qsu6z",
              "author": "ThatRandomJew7",
              "text": "Unironically, Kimi is one of the few open models that actually appears to be as good as the benchmarks suggest. \n\nI'm actually considering replacing Gemini 3 Pro with it",
              "score": 5,
              "created_utc": "2026-01-31 05:41:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rplyv",
          "author": "Fast-Satisfaction482",
          "text": "In the lead by a few points in a plot without error bars is definitely not \"domination\". It's inconclusive at best.",
          "score": 1,
          "created_utc": "2026-01-31 10:39:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tk4mv",
          "author": "Relevant-Service9871",
          "text": "Comment on utilise cette ia",
          "score": 1,
          "created_utc": "2026-01-31 17:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mv9sa",
          "author": "Dependent-Example930",
          "text": "How are most people using kimi k2.5? What service?",
          "score": 1,
          "created_utc": "2026-01-30 17:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyo87",
              "author": "GenLabsAI",
              "text": "Kimi, OpenRouter.",
              "score": 5,
              "created_utc": "2026-01-30 20:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oijqz",
              "author": "synn89",
              "text": "Fireworks.ai",
              "score": 1,
              "created_utc": "2026-01-30 21:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2p7hpv",
                  "author": "Turbulent_Pin7635",
                  "text": "LM Studio =)",
                  "score": 3,
                  "created_utc": "2026-01-30 23:46:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2nxpri",
              "author": "IrisColt",
              "text": "P-perplexity?",
              "score": -1,
              "created_utc": "2026-01-30 19:59:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qskrc",
                  "author": "ThatRandomJew7",
                  "text": "Doesn't have K2.5 yet, sadly",
                  "score": 0,
                  "created_utc": "2026-01-31 05:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m481c",
          "author": "jacek2023",
          "text": "Ok let's wait for the bots to upvote",
          "score": -10,
          "created_utc": "2026-01-30 15:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2slyma",
          "author": "LocoMod",
          "text": "Step 1: Ask the model to place their initials at the bottom of the page.\n\nStep 2: Vote for the motherland model.\n\nStep 3: ???\n\nStep 4: Profit!\n\n  \nIt is easy to game this and pump your model to the top. Doesn't take many since its not a super high traffic site.",
          "score": -2,
          "created_utc": "2026-01-31 14:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m4jb1",
          "author": "DrummerPrevious",
          "text": "Glm actually sucks",
          "score": -9,
          "created_utc": "2026-01-30 15:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2macbm",
              "author": "sleepy_roger",
              "text": "Meh, GLM has been my go to design model for a while now. It's made some great designs with easier prompts than I've seen Claude do.",
              "score": 13,
              "created_utc": "2026-01-30 15:34:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mle8e",
              "author": "TokenRingAI",
              "text": "It's great for UI work",
              "score": 5,
              "created_utc": "2026-01-30 16:24:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2nlvhn",
              "author": "fabricio3g",
              "text": "I find it very useful for finding bugs and analyzing code",
              "score": 6,
              "created_utc": "2026-01-30 19:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtvo4r",
      "title": "128GB devices have a new local LLM king: Step-3.5-Flash-int4",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
      "author": "tarruda",
      "created_utc": "2026-02-02 13:55:00",
      "score": 295,
      "num_comments": 159,
      "upvote_ratio": 0.96,
      "text": "Here's the HF Repo: http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4 (this is a GGUF repo)\n\nI've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.\n\nI use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.\n\n*Update: I ran llama-bench with up to 100k prefill. Here are the results:\n\n    % llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000\n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        281.09 Â± 1.57 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         34.70 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        248.10 Â± 1.08 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         31.69 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        222.18 Â± 0.49 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         30.02 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        200.68 Â± 0.78 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         28.62 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        182.86 Â± 0.55 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         26.89 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        167.61 Â± 0.23 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         25.37 Â± 0.03 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        154.50 Â± 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         24.10 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        143.60 Â± 0.29 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         22.95 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        134.02 Â± 0.35 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         21.87 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        125.34 Â± 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         20.66 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        117.72 Â± 0.07 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         19.78 Â± 0.01 |\n    \n    build: a0dce6f (24)\n\nThis is still very usable with 100k prefill, so a good option for CLI coding agents!\n\nYou need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o35sh4e",
          "author": "Proper_Taste_6778",
          "text": "I wonder how it will perform on Strix Halo.",
          "score": 48,
          "created_utc": "2026-02-02 14:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o361c14",
              "author": "Outrageous_Fan7685",
              "text": "I'm testing now on strix halo with 128k c",
              "score": 24,
              "created_utc": "2026-02-02 15:17:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o365f0k",
                  "author": "Proper_Taste_6778",
                  "text": "What performance can you get?",
                  "score": 6,
                  "created_utc": "2026-02-02 15:37:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o363bnm",
                  "author": "oxygen_addiction",
                  "text": "And?",
                  "score": 10,
                  "created_utc": "2026-02-02 15:27:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36ahap",
                  "author": "Outrageous_Fan7685",
                  "text": "Sorry guys, still waiting for the team pr of llama.cpp",
                  "score": 9,
                  "created_utc": "2026-02-02 16:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o382rl3",
                  "author": "Outrageous_Fan7685",
                  "text": "https://preview.redd.it/t5zhe1ila5hg1.jpeg?width=2268&format=pjpg&auto=webp&s=de401d36d7cc34d131dce9eaf75a872e16c6b04d\n\nVulkan, neat !",
                  "score": 3,
                  "created_utc": "2026-02-02 20:57:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36eab1",
                  "author": "JamesEvoAI",
                  "text": "Also piling on to find out how it goes later. Curious to know about how much RAM it used",
                  "score": 1,
                  "created_utc": "2026-02-02 16:18:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38gbxt",
              "author": "GPU-Appreciator",
              "text": "    ./llama.cpp/build/bin/llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -ngl 999 -d 0,32000\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |           pp512 |        209.43 Â± 1.74 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |           tg128 |         27.95 Â± 0.18 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |  pp512 @ d32000 |        117.58 Â± 0.43 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |  tg128 @ d32000 |         22.30 Â± 0.05 |",
              "score": 5,
              "created_utc": "2026-02-02 22:01:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38wjkj",
              "author": "Grouchy-Bed-7942",
              "text": "some benchmarks here : [https://www.reddit.com/r/LocalLLaMA/comments/1qubamo/some\\_step35flash\\_benchmarks\\_on\\_amd\\_strix\\_halo](https://www.reddit.com/r/LocalLLaMA/comments/1qubamo/some_step35flash_benchmarks_on_amd_strix_halo)",
              "score": 2,
              "created_utc": "2026-02-02 23:25:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35qa1o",
          "author": "ohgoditsdoddy",
          "text": "I hope someone releases a NVFP4 version.",
          "score": 34,
          "created_utc": "2026-02-02 14:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35tb3k",
              "author": "Septerium",
              "text": "This is for Blackwell GPUs, right? What do you like more about NVFP4?",
              "score": 5,
              "created_utc": "2026-02-02 14:36:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35xgta",
                  "author": "pulse77",
                  "text": "Not OP but: this is one of the best \\~4bit quants out there and is GPU accelerated...",
                  "score": 19,
                  "created_utc": "2026-02-02 14:58:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o361h7d",
                  "author": "Fear_ltself",
                  "text": "Isn't NVFP4 essentially lossless compression? 99% quality retained or more compared to other quantization methods",
                  "score": 6,
                  "created_utc": "2026-02-02 15:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3667ah",
              "author": "LA_rent_Aficionado",
              "text": "Why NVFP4? Itâ€™s currently severely unoptimized and out-performed (from a speed perspective) by AWQ, EXL3 and GGUFs of a similar size",
              "score": 5,
              "created_utc": "2026-02-02 15:41:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3823rg",
                  "author": "lol-its-funny",
                  "text": "NVFP4 is insanely fast on Blackwell processors including the GB10. 250% faster than other Q4 types. NVFP4 can be multiplied natively on Blackwell vs Q4s that have to be temporarily casted/converted to FP16 for multiplication.\n\nTBH I had hoped for AMD to announce hardware acceleration support for 4 bit quant multiplication at CES to follow-up on their Strix Halo v2\n\nLink: [https://medium.com/data-science-collective/nvfp4-same-accuracy-with-2-3x-higher-throughput-for-4-bit-llms-03518ecba108](https://medium.com/data-science-collective/nvfp4-same-accuracy-with-2-3x-higher-throughput-for-4-bit-llms-03518ecba108)\n\n![https://miro.medium.com/v2/resize:fit:1100/format:webp/0*d875cIaYOtGldgSc.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*d875cIaYOtGldgSc.png)",
                  "score": 9,
                  "created_utc": "2026-02-02 20:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39qx6s",
                  "author": "changtimwu",
                  "text": "Support and optimization for FP4 have been gradually improving. I believe weâ€™ll finally see the impact.   [https://github.com/vllm-project/vllm/releases/tag/v0.15.0](https://github.com/vllm-project/vllm/releases/tag/v0.15.0)",
                  "score": 3,
                  "created_utc": "2026-02-03 02:14:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39m9bp",
                  "author": "TheThoccnessMonster",
                  "text": "Not if you have a DGX Spark or any Blackwell - you couldnâ€™t be more wrong lol",
                  "score": 1,
                  "created_utc": "2026-02-03 01:48:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3919j7",
          "author": "Grouchy-Bed-7942",
          "text": "Benchmark on AMD Strix Halo (Minisforum MS S1 Max), [https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4) (full context fits) :\n\n# Rocm 7.1.1\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|pp4096|258.82 Â± 3.15|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|pp32768|208.35 Â± 1.86|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|tg512|22.93 Â± 0.00|\n\n# Vulkan-amdvlk\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp4096|153.04 Â± 0.30|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp32768|79.55 Â± 0.59|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|tg512|2.50 Â± 0.00|\n\n# Vulkan-radv\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp4096|164.20 Â± 1.30|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp32768|104.36 Â± 0.29|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|tg512|27.86 Â± 0.00|",
          "score": 9,
          "created_utc": "2026-02-02 23:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aj5eu",
              "author": "ga239577",
              "text": "Did you have to do anything special to run it?",
              "score": 1,
              "created_utc": "2026-02-03 05:11:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awsly",
                  "author": "Grouchy-Bed-7942",
                  "text": "I based my work on kyuz0's toolbox and compiled the llamacpp version provided by the guys at steps on their GitHub repo.",
                  "score": 2,
                  "created_utc": "2026-02-03 07:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36q5rp",
          "author": "sine120",
          "text": "Wha... what are you doing step-flash?",
          "score": 22,
          "created_utc": "2026-02-02 17:13:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35ninh",
          "author": "DOAMOD",
          "text": "In my first test with it, I also got the feeling that it's a little better than Minimax M2.1.",
          "score": 14,
          "created_utc": "2026-02-02 14:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36rfu0",
              "author": "Dany0",
              "text": "A 196B/11B active model matching a 229B/45BA model just a month after it released honestly feels like magic\n\nIf only this kind of progress could be sustained, LLMs could actually be useful for real, hard world tasks soon. Or at least they could help engineer an ML solution that will be\n\nAnd keep in mind that a 196/11 setup will be much easier to REAP. One could easily sacrifice 40% of experts while keeping english & coding performance, and a heavily quantised version of that REAP could easily be run at \\~10 tok/s on relatively normal devices. That's near SOTA on consumer hardware.",
              "score": 8,
              "created_utc": "2026-02-02 17:19:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37cgdc",
                  "author": "steezy13312",
                  "text": "> matching a 229B/45BA model\n\nDoesn't Minimax 2.1 use 10B active params?",
                  "score": 10,
                  "created_utc": "2026-02-02 18:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35pwc4",
          "author": "Hauven",
          "text": "I'm tempted to try and see if it would be usable on my system, but I don't honestly know.\n\n\\- Threadripper 3995WX CPU  \n\\- 512GB DDR4 ECC RAM (3200)  \n\\- RTX 5090 (32GB)  \n\\- RTX 3090 (24GB)\n\nThat gives me 56GB\\~ VRAM at least, so about half way there, with the rest going on RAM. Question is would it work. I use it for things like TTS, image and video generation currently, not really for a text based LLM. I'm also assuming I can spread part of the model over the two GPUs and the rest on RAM, not just one GPU and then the rest on RAM.",
          "score": 5,
          "created_utc": "2026-02-02 14:18:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35qrqx",
              "author": "tarruda",
              "text": "It only uses 11B active parameters, so definitely worth trying. You should play with llama.cpp offloading options.",
              "score": 11,
              "created_utc": "2026-02-02 14:23:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35sidv",
                  "author": "Hauven",
                  "text": "Many thanks, I'll give it a go :).",
                  "score": 2,
                  "created_utc": "2026-02-02 14:32:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o369327",
              "author": "pmttyji",
              "text": "Q4 is 100GB. You have half TB RAM apart from VRAM. Go ahead.",
              "score": 8,
              "created_utc": "2026-02-02 15:54:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o373oao",
              "author": "munkiemagik",
              "text": "Interested to see how you find it, even though you are more image/video gen work orientated, which I don't do. \n\nI'm running 1x5090 & 2x3090 and this is the first model in a while that's tempting me to consider adding more 3090 in order to replace GPT-OSS-120b and GLM-4.5-AIR\\_Q4. \n\nMinimax M2.1 got me interested but I only would have been able to fit the heavily REAP'ed models, so held off.\n\nI'm going to be away for a week or so and unfortunately wont be faffing with the GPU server for a bit.",
              "score": 1,
              "created_utc": "2026-02-02 18:14:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37xtit",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/m0qgeewc65hg1.png?width=2677&format=png&auto=webp&s=2062d8bdf4a5c696d65945529c144bfcffcd419b\n\ntesting it on kilocode... Incredible... Really! (analizing my card game to test)",
          "score": 4,
          "created_utc": "2026-02-02 20:34:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38a39q",
              "author": "pixelpoet_nz",
              "text": "> analizing\n\nHope you used lube!\n\nAlso interesting that you're also testing with Slay the Spire like card games :)",
              "score": 4,
              "created_utc": "2026-02-02 21:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36evk1",
          "author": "DeProgrammer99",
          "text": "What language(s) did you try it for?",
          "score": 3,
          "created_utc": "2026-02-02 16:21:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36f34a",
              "author": "tarruda",
              "text": "Only python and javascript.",
              "score": 6,
              "created_utc": "2026-02-02 16:22:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o393rj8",
          "author": "Claudius_the_II",
          "text": "The GPT-5.2 gap is striking - 69% vs ~30% for everything else. Makes you wonder whether its genuinely better at formal reasoning or just had more exposure to TM-style problems in training. Would love to see reasoning models like o3 or R1 on this - TM programming is basically pure state-machine logic, which should be their wheelhouse.",
          "score": 3,
          "created_utc": "2026-02-03 00:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35neih",
          "author": "Dry_Yam_4597",
          "text": "\\> Not only it is fast, but also super efficient in RAM usage.\n\nHow many tokens / second?",
          "score": 5,
          "created_utc": "2026-02-02 14:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35nptd",
              "author": "tarruda",
              "text": "on my M1 Ultra, around 34 tokens/second with empty context. But the speed degrades much more slowly than other LLMs. I'm still seeing 30 tokens/second at 20k filled context.",
              "score": 16,
              "created_utc": "2026-02-02 14:06:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qtul",
                  "author": "oxygen_addiction",
                  "text": "If their fork has it, try to use self speculative decoding. For as much thinking as this model does, I think it'd be helpful.\n\n[https://github.com/ggml-org/llama.cpp/pull/19164](https://github.com/ggml-org/llama.cpp/pull/19164)",
                  "score": 6,
                  "created_utc": "2026-02-02 14:23:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o37pumy",
                  "author": "FPham",
                  "text": "This is actually very decent!",
                  "score": 1,
                  "created_utc": "2026-02-02 19:56:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35onh2",
                  "author": "Dry_Yam_4597",
                  "text": "Nice one, ta.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:11:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35m5ln",
          "author": "Smart-Government6564",
          "text": "Holy shit this sounds promising ðŸ”¥ been waiting for something that can actually compete with GLM 4.7 without eating up all my RAM\n\n  \nThe fact that your running full 256k context on 128GB is wild - most models I've tried lately are memory hogs that tap out way before that. Definitely gonna have to compile that llama.cpp fork tonight and give it a spin on some coding tasks\n\n  \nHow's the response speed compared to minimax? That's usually my go-to for quick iterations but if this thing is more efficient AND better quality that's a game changer ðŸ’€",
          "score": 8,
          "created_utc": "2026-02-02 13:58:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35njhd",
              "author": "tarruda",
              "text": "So far it seems better than Minimax, all with less degradation of prompt processing and token generation when context increases, which makes it super promising for agentic use. I can actually run 2 128k context parallel streams at decent speeds. \n\nThis is the first time I feel I can run an Anthropic level LLM locally. While I could run Minimax (Q3) and GLM (IQ2), these quants severely degrade quality, so Step 3.5 flash is looking a much better option.\n\nI will continue using this through the day, but it is looking like I will delete GLM 4.7 and Minimax 2.1 weights to free up space.",
              "score": 8,
              "created_utc": "2026-02-02 14:05:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37ilrq",
                  "author": "Zyj",
                  "text": "I'm using MiniMax M2.1 Q6\\_K split across two machines. Perhaps using this one Q8 is better. I'll wait for the PR to be merged into llama.cpp",
                  "score": 2,
                  "created_utc": "2026-02-02 19:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36zwxd",
          "author": "Outrageous_Fan7685",
          "text": "Those are llama gguf split or cat split ?",
          "score": 2,
          "created_utc": "2026-02-02 17:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37287e",
              "author": "tarruda",
              "text": "cat split",
              "score": 4,
              "created_utc": "2026-02-02 18:08:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3736ch",
                  "author": "Outrageous_Fan7685",
                  "text": "After merge when i load it i have a segmentation fault i rebuilt llama from the upstream pr of pwilkin with rocm support.",
                  "score": 2,
                  "created_utc": "2026-02-02 18:12:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37ngb0",
          "author": "Potential-Leg-639",
          "text": "Any strix halo benchmarks?",
          "score": 2,
          "created_utc": "2026-02-02 19:45:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38f06e",
              "author": "GPU-Appreciator",
              "text": "I'm running one now. The performance seems quite solid - a short initial prompt got me a genuinely solid answer on a somewhat niche question,\\~26tk/s generation",
              "score": 4,
              "created_utc": "2026-02-02 21:55:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o38loba",
          "author": "SectionCrazy5107",
          "text": "i used this from their site but it went into huge reasoning block and not usable for coding, please confirm how you are able to test for coding? \n\n     -m step3.5_flash_Q4_K_S.gguf -c 16384 -b 2048 -ub 2048 -fa on --temp 1.0",
          "score": 2,
          "created_utc": "2026-02-02 22:28:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39heid",
              "author": "tarruda",
              "text": "-c 16384 means you only allow 16k context.\n\nThis model reasons a lot, so I think you need at least 32k.",
              "score": 2,
              "created_utc": "2026-02-03 01:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36iqbl",
          "author": "Conscious_Cut_6144",
          "text": "Interesting, everyone else seems to like it.  \nI'm running the GGUF and so far it doesn't seem to be beating K2.5, GLM4.7, D3.2, or MM2.1   \n  \nNot made up my mind yet, still testing...",
          "score": 1,
          "created_utc": "2026-02-02 16:39:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36smxg",
              "author": "shrug_hellifino",
              "text": "I haven't tried it yet, but aren't those models you mentioned in another weight class? If you are constrained to sub ~180 GB total, your options get shakey.",
              "score": 6,
              "created_utc": "2026-02-02 17:24:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3adwb8",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Their benchmarks claim that this model beats those. I think that's why the comment you're replying to is comparing those models.",
                  "score": 3,
                  "created_utc": "2026-02-03 04:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37p5pj",
          "author": "Flkhuo",
          "text": "Can this get loaded unto RTX4090 24gvram and 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-02 19:53:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wg3a",
              "author": "tarruda",
              "text": "No, I think you need at least 120GB of RAM + VRAM.",
              "score": 2,
              "created_utc": "2026-02-02 20:27:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38kpch",
          "author": "SimplyRemainUnseen",
          "text": "Is this QAT? I love all the 4-bit models this year",
          "score": 1,
          "created_utc": "2026-02-02 22:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39h8tz",
              "author": "tarruda",
              "text": "Wouldn't be surprised if it is QAT, considering the quantized weights were released by the same team.",
              "score": 1,
              "created_utc": "2026-02-03 01:19:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3blss6",
                  "author": "audioen",
                  "text": "I wonder if anyone has actually written a single program that can QAT anything on GGUF. There exists QAT GGUF files but they appear to be using the equivalent of Q4\\_0 only which is a very generic and simple quantization format and as long as the rate of scale factors is lower than in the Q4\\_0, other foreign int4 encodings can probably be converted to it without loss of accuracy.\n\nThis is Q4\\_K\\_S and I have doubt that this is QAT. At least, I'm not aware of anyone capable of producing QAT for this quantization format.\n\nGGUF QAT for these complex quantizations are an untapped market. Nobody is making them. But, for hobbyists with limited VRAM, being able to run roughly BF16 quality model at Q4\\_0 with minimal loss in K\\_L divergence would be excellent compared to current state of affairs, and I think software would exist for Q4\\_0 at least because it's that simple and generic. Obviously, doing the same thing at 2 bits, if it is possible, would be better still. The model is probably mildly perturbed by this degree of extreme quantization even when the full precision model would be used as the trainer and K\\_L divergence would be minimized.\n\nWe really don't need these complex IQ\\_N\\_SOMETHING thingys and QN\\_K\\_FOOBAR as much as we should just bite the bullet and figure out a bit depth to which QAT still can work and then pool our resources to buy the compute in order to get a single high-quality quantized BF16-level variant. It would automatically give us a far better result than any PTQ scheme. The challenge is to design low-bitwidth quantization for which a stable training procedure can be written. We know it exists at least for Q4\\_0 and MXFP4 because these have been made. It might even exist for those ternary schemes, which is pretty much the last word on quantization as far as I can tell.",
                  "score": 1,
                  "created_utc": "2026-02-03 10:59:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38m413",
          "author": "jinnyjuice",
          "text": "It takes up 111 GB, so shouldn't it be recommended for devices with 200 GB+ memory?",
          "score": 1,
          "created_utc": "2026-02-02 22:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38wz62",
              "author": "huzbum",
              "text": "AWQ or Q4\\_XL is a lot smaller.",
              "score": 1,
              "created_utc": "2026-02-02 23:27:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3bcuvd",
                  "author": "Karyo_Ten",
                  "text": "It's 199B param and in 4-bit down to 111GB: https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main\n\nNow given that VRAM is in GiB not GB it's like 103.4 GiB VRAM needed for the weights",
                  "score": 1,
                  "created_utc": "2026-02-03 09:35:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3a3aze",
          "author": "_hephaestus",
          "text": "Looks like there's an mlx version of it already, trying that out. https://huggingface.co/mlx-community/Step-3.5-Flash-4bit\n\nNot sure how to benchmark that though",
          "score": 1,
          "created_utc": "2026-02-03 03:26:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3an12k",
          "author": "cleverestx",
          "text": "Anyone get this running on Strix Halo yet? Either through LM Studio or LLama.cpp?",
          "score": 1,
          "created_utc": "2026-02-03 05:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bmega",
              "author": "Grouchy-Bed-7942",
              "text": "https://www.reddit.com/r/LocalLLaMA/s/PsYrR7SI6U",
              "score": 1,
              "created_utc": "2026-02-03 11:04:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b3g88",
          "author": "manipp",
          "text": "Anyone have any sense if it's any good at writing?",
          "score": 1,
          "created_utc": "2026-02-03 08:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bqq82",
          "author": "SpicyWangz",
          "text": "The biggest problem is just how many reasoning tokens it uses. It churns through so many of them",
          "score": 1,
          "created_utc": "2026-02-03 11:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c9cm3",
          "author": "Diao_nasing",
          "text": "pp is really slow",
          "score": 1,
          "created_utc": "2026-02-03 13:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5nh8",
              "author": "tarruda",
              "text": "Apple silicon tends to be bad with prompt processing, but these numbers are OK for me due to automatic  prompt caching done by llama.cpp",
              "score": 1,
              "created_utc": "2026-02-03 16:26:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cait1",
          "author": "Fast_Thing_7949",
          "text": "  \nRemindMe! 7 days",
          "score": 1,
          "created_utc": "2026-02-03 13:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wshg",
          "author": "Dundell",
          "text": "Shoot here I go... I might as well invest in a 3090 to add to my vram pool. Bump it from 84 to 108GB vram...",
          "score": 1,
          "created_utc": "2026-02-02 14:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35o331",
          "author": "segmond",
          "text": "What sort of performance are you seeing?",
          "score": 1,
          "created_utc": "2026-02-02 14:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35obn6",
              "author": "tarruda",
              "text": "~30 t/s generation and ~300 t/s prompt processing. I will post a full benchmark later",
              "score": 9,
              "created_utc": "2026-02-02 14:10:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35pb9j",
                  "author": "Fancy_Fanqi77",
                  "text": "Can't wait to see",
                  "score": 2,
                  "created_utc": "2026-02-02 14:15:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35qtdg",
                  "author": "segmond",
                  "text": "thanks, that's enough info for me to give it a shot.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ct1ni",
                  "author": "wanderer_4004",
                  "text": "MLX is now also supporting it. Could you run that too and get us some benchmark as well?",
                  "score": 1,
                  "created_utc": "2026-02-03 15:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36hi5z",
          "author": "NotLordo",
          "text": "RemindMe! 2 days",
          "score": 0,
          "created_utc": "2026-02-02 16:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39zpie",
          "author": "Brospeh-Stalin",
          "text": "u/AskGrok how do I get an LLM to interface with reddit and post on it? How specifically are you fed reddit posts and comments? What all do you see and what don't you see? How is it formatted in the input?",
          "score": -1,
          "created_utc": "2026-02-03 03:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39zt07",
              "author": "AskGrok",
              "text": "To get an LLM to interface with Reddit and post on it, you'd typically use Reddit's official API (via PRAW in Python, for example) to handle authentication, fetching posts, and submitting comments or threads. Hook the LLM into a script that processes API responses as prompts, generates replies, and posts them backâ€”think of it as giving your model a Reddit account and some automation glue. But heads up: Reddit's terms frown on bots spamming or impersonating users, so keep it ethical and rate-limited, or risk bans. For specifics, check Reddit's API docs and libraries like PRAW; it's not rocket science, but test in a sandbox.\n\nAs for me (Grok), I'm fed Reddit posts and comments through a structured input prompt that includes the post title, content, and comment chainâ€”formatted as plain text, like a conversation transcript. I see the full text of the title, body, and relevant comments (including this one), but not things like upvotes, user profiles, images, or live updates unless I query for them. No peeking at your browser history or anything creepy; it's all sanitized and passed in as a static snapshot. Think of it as getting a neatly typed letter in the mail, not the whole envelope factory.\n\n^([what is this?](https://redd.it/1lzgxii))",
              "score": -1,
              "created_utc": "2026-02-03 03:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39zzdp",
                  "author": "Brospeh-Stalin",
                  "text": "Since the bot is not developed by xAI, only the LLM, can you tell me what the prompt looks like?",
                  "score": -1,
                  "created_utc": "2026-02-03 03:06:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqfe1k",
      "title": "Kimi AI team sent me this appreciation mail",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/0ztj2mk3sbgg1.jpeg",
      "author": "mehulgupta7991",
      "created_utc": "2026-01-29 17:42:26",
      "score": 294,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ggyr3",
          "author": "FundusAnimae",
          "text": "Classy move!",
          "score": 26,
          "created_utc": "2026-01-29 18:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ga9no",
          "author": "HugoCortell",
          "text": "That's really nice of them! Good to see a team that gives a shit.\n\nMost companies really give a cold shoulder to those building the actual showcases to their products.",
          "score": 52,
          "created_utc": "2026-01-29 18:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gwpoq",
              "author": "KitchenSomew",
              "text": "This is really wholesome! It's rare to see companies acknowledge community contributions like this. Shows they're paying attention to what people are building with their models.",
              "score": 7,
              "created_utc": "2026-01-29 19:46:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gios5",
          "author": "7657786425658907653",
          "text": "you mean their ai sent you this.",
          "score": 75,
          "created_utc": "2026-01-29 18:40:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gqs91",
              "author": "son_et_lumiere",
              "text": "It's also part of the team.",
              "score": 76,
              "created_utc": "2026-01-29 19:17:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2l2re1",
                  "author": "IrisColt",
                  "text": "This will become an underrated comment.",
                  "score": 8,
                  "created_utc": "2026-01-30 11:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jr5x7",
              "author": "Imaginary_Tangelo855",
              "text": "Did their ai send you that?",
              "score": 2,
              "created_utc": "2026-01-30 04:51:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2h3f8c",
          "author": "UnderstandingLife712",
          "text": "Your video must have hit the mark.\n\n",
          "score": 9,
          "created_utc": "2026-01-29 20:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2leoq9",
          "author": "jikilan_",
          "text": "Ask them to release a 30b model",
          "score": 2,
          "created_utc": "2026-01-30 12:52:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37oq42",
              "author": "ramendik",
              "text": "I think they actually mentioned they leave the distilling to the community. I am trying that now, distilling into a 1.5B to learn the ropes, but not sure where to get people to test my first stage candidate.\n\n(Also they did release Kimi Linear but it was not very \"Kimi-like\")\n\nEdit: here is their \"let the community distill\" https://github.com/MoonshotAI/Kimi-K2/issues/16 .",
              "score": 1,
              "created_utc": "2026-02-02 19:51:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o332cas",
          "author": "FX2021",
          "text": "Got a link to the video",
          "score": 1,
          "created_utc": "2026-02-02 02:20:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39p3zg",
          "author": "Southern_Change9193",
          "text": "Ruyan is a very beautiful Chinese name.",
          "score": 1,
          "created_utc": "2026-02-03 02:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gwja9",
          "author": "artisticMink",
          "text": "Always keep in mind that, while it's nice for you, this does cost them nothing and it's intended to manipulate you a little bit. Into doing posts like this one.",
          "score": -6,
          "created_utc": "2026-01-29 19:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k98zg",
              "author": "nulseq",
              "text": "God it doesnâ€™t cost anything to be positive. You can look at it either way.",
              "score": 2,
              "created_utc": "2026-01-30 07:07:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kg27n",
                  "author": "Nimbkoll",
                  "text": "Redditor assume the best intentions and not being cynical challenge (impossible)",
                  "score": 0,
                  "created_utc": "2026-01-30 08:06:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l2ss7",
              "author": "IrisColt",
              "text": "heh",
              "score": 1,
              "created_utc": "2026-01-30 11:28:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}