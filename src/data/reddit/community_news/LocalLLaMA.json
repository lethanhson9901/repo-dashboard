{
  "metadata": {
    "last_updated": "2026-01-28 08:47:44",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 561,
    "file_size_bytes": 587847
  },
  "items": [
    {
      "id": "1qjtyw8",
      "title": "Qwen dev on Twitter!!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/avu4mhyvfweg1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2026-01-22 13:03:26",
      "score": 749,
      "num_comments": 60,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o13qlap",
          "author": "rm-rf-rm",
          "text": "Thread locked as announcements are out",
          "score": 1,
          "created_utc": "2026-01-22 19:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11gcpx",
          "author": "MaxKruse96",
          "text": "Its the TTS model from the vLLM leak. relax guys.\n\nsource: trust me bro",
          "score": 171,
          "created_utc": "2026-01-22 13:07:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11sc8g",
              "author": "wanderer_4004",
              "text": "It is Qwen3.5-Coder-30B-A1B - outstanding speed and 1M context, runs on RPi with 50t/s\n\nsource: my wishful thinking",
              "score": 119,
              "created_utc": "2026-01-22 14:13:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11shlk",
                  "author": "MaxKruse96",
                  "text": "brother literally look at the damn posts in the sub, its the tts models",
                  "score": -25,
                  "created_utc": "2026-01-22 14:13:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11mdsb",
              "author": "Silent-Apple5026",
              "text": "I think itâ€™s not this. I expect new variant of qwen3 0.6b",
              "score": 0,
              "created_utc": "2026-01-22 13:41:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11hb7h",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 114,
          "created_utc": "2026-01-22 13:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11hprz",
              "author": "TheAndyGeorge",
              "text": "this shit is bananasÂ ",
              "score": 38,
              "created_utc": "2026-01-22 13:15:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11k41e",
                  "author": "merica420_69",
                  "text": "B-A-N-A-N-A-S",
                  "score": 21,
                  "created_utc": "2026-01-22 13:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o126b4s",
              "author": "Kraskos",
              "text": "*If I was a rich nerd*\n\n*Na-na-na-na-na-na-na-na-na-na-na-na-na-na...*\n\n*Then I'd have all the GPUs in the world...*",
              "score": 1,
              "created_utc": "2026-01-22 15:22:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11hc78",
          "author": "rerri",
          "text": "[https://huggingface.co/collections/Qwen/qwen3-tts](https://huggingface.co/collections/Qwen/qwen3-tts)",
          "score": 93,
          "created_utc": "2026-01-22 13:13:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11mgo7",
              "author": "a4d2f",
              "text": "> Qwen/Qwen3-TTS-12Hz-1.7B-Base\n\n12Hz? Must be a really deep voice then...",
              "score": 38,
              "created_utc": "2026-01-22 13:42:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11v5dt",
                  "author": "WiseassWolfOfYoitsu",
                  "text": "time to make Ray Charles bot...",
                  "score": 9,
                  "created_utc": "2026-01-22 14:27:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o11xx2t",
                  "author": "Cool-Chemical-5629",
                  "text": "Epic voice of the movie trailers level of deep? [https://youtu.be/6N5l0sgPP5k](https://youtu.be/6N5l0sgPP5k)",
                  "score": 2,
                  "created_utc": "2026-01-22 14:41:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11l9lz",
              "author": "ChainOfThot",
              "text": "Does it moan?",
              "score": 50,
              "created_utc": "2026-01-22 13:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11notx",
                  "author": "iampoorandsad",
                  "text": "Does it meow?",
                  "score": 28,
                  "created_utc": "2026-01-22 13:48:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o11skk1",
                  "author": "MaxKruse96",
                  "text": "i hope",
                  "score": 5,
                  "created_utc": "2026-01-22 14:14:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11qgfz",
              "author": "_raydeStar",
              "text": "This is great! Nothing super groundbreaking, we already have VibeVoice, Dia (my personal fav) and others. Going to test it still and see how it fares. Also, it's multi-lingual which is big.\n\nEdit: one thing I didnt add was you can tell the AI how to interpret the voice. I am not sure yet how good it is, but this is a first-find for me.  If it works well, that will solve a lot of problems for me.",
              "score": 14,
              "created_utc": "2026-01-22 14:03:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o122svy",
                  "author": "Grand0rk",
                  "text": "So... How was the test?",
                  "score": 3,
                  "created_utc": "2026-01-22 15:05:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11jucr",
              "author": "SlowFail2433",
              "text": "Yeah this is it",
              "score": 2,
              "created_utc": "2026-01-22 13:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11jm53",
              "author": "No_Afternoon_4260",
              "text": "Good spot! After nvidia nemo and microsoft vibecoice, waiting for their ASR with diarization",
              "score": 2,
              "created_utc": "2026-01-22 13:26:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11kkkc",
              "author": "Loskas2025",
              "text": "amazing",
              "score": 1,
              "created_utc": "2026-01-22 13:32:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11fwj1",
          "author": "ilarp",
          "text": "I am so hyped! Finally this 5090 might be worthwhile",
          "score": 18,
          "created_utc": "2026-01-22 13:05:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11m6fv",
              "author": "RiskyBizz216",
              "text": "disappointment imminent",
              "score": 26,
              "created_utc": "2026-01-22 13:40:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11oypd",
                  "author": "ilarp",
                  "text": "I know its sadly been true",
                  "score": 1,
                  "created_utc": "2026-01-22 13:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11jgjm",
              "author": "No_Afternoon_4260",
              "text": "Lol",
              "score": 3,
              "created_utc": "2026-01-22 13:25:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11ruxe",
          "author": "ThePixelHunter",
          "text": "I know tiny models are easier to train, and more people (with just one GPU) are able to run them locally, but I really wish we'd see more competition in the 50-120B parameters range. These are great for the enthusiasts with a couple of 3090's or 3x16GB cards.",
          "score": 18,
          "created_utc": "2026-01-22 14:10:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o122lg5",
              "author": "DriveSolid7073",
              "text": "Bro, you can't just make a model with 999b parameters. That's not how it works. Audio doesn't physically have a dataset large enough to support LLM-level models.",
              "score": 10,
              "created_utc": "2026-01-22 15:04:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12aamx",
                  "author": "ThePixelHunter",
                  "text": "When I made my comment, there was no mention of this being a TTS model, so I assumed it was another text decoder LLM.",
                  "score": 11,
                  "created_utc": "2026-01-22 15:41:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o13a5m4",
                  "author": "alamacra",
                  "text": "Surely YouTube ought to have enough.",
                  "score": 1,
                  "created_utc": "2026-01-22 18:22:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o127ooh",
              "author": "CheatCodesOfLife",
              "text": "What do you think you do with a 50b-120b tts that you can't do with a 3b?",
              "score": 1,
              "created_utc": "2026-01-22 15:29:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12aaca",
                  "author": "ThePixelHunter",
                  "text": "When I made my comment, there was no mention of this being a TTS model, so I assumed it was another text decoder LLM.",
                  "score": 3,
                  "created_utc": "2026-01-22 15:41:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11pcbk",
          "author": "International-Try467",
          "text": "Obligatory fuck Furkan",
          "score": 10,
          "created_utc": "2026-01-22 13:57:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o132rkj",
              "author": "Noiselexer",
              "text": "Ow ffs, its *that* guy, didnt notice.",
              "score": 1,
              "created_utc": "2026-01-22 17:49:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11xya0",
              "author": "ali0une",
              "text": "isn't it Dr Fuckan?",
              "score": 0,
              "created_utc": "2026-01-22 14:42:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11m1tf",
          "author": "RiskyBizz216",
          "text": "meh..i need tiny model BIG BRAIN",
          "score": 2,
          "created_utc": "2026-01-22 13:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12ffop",
          "author": "Yu2sama",
          "text": "I hoped for a small creative writing model, haven't gotten one of those in a while",
          "score": 2,
          "created_utc": "2026-01-22 16:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11mvk0",
          "author": "Own-Potential-2308",
          "text": "Qwen 4B 2201?",
          "score": 1,
          "created_utc": "2026-01-22 13:44:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjul5t",
      "title": "Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B & 1.8B), Support for 10 languages",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wo9tqflvkweg1.jpeg",
      "author": "Nunki08",
      "created_utc": "2026-01-22 13:31:16",
      "score": 733,
      "num_comments": 119,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o12xapk",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-22 17:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1273bg",
          "author": "LetterRip",
          "text": "Really great but all of the english speakers sound like the source of training was purely dubs of Japanese Anime.",
          "score": 107,
          "created_utc": "2026-01-22 15:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12po18",
              "author": "SmartCustard9944",
              "text": "There is a market for that",
              "score": 70,
              "created_utc": "2026-01-22 16:50:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12yb97",
                  "author": "bittytoy",
                  "text": "this shit was built for a Vtuber lmaooo",
                  "score": 27,
                  "created_utc": "2026-01-22 17:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o12yxaw",
              "author": "RazzmatazzReal4129",
              "text": "a model of culture",
              "score": 39,
              "created_utc": "2026-01-22 17:32:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o172k14",
              "author": "ShengrenR",
              "text": "Yea.. I love the way you can do the 'voice design' via description, but more than half the time it just sounds like 'my prompt' + 'and all the anime!' - maybe folks figure out how to work with it well, but otherwise I'll wait for index tts 3 lol, they had 'emotion via description' experimental in 2 with the note they're committed to improving it.. here's to hoping.",
              "score": 6,
              "created_utc": "2026-01-23 06:45:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o136i9m",
              "author": "BusRevolutionary9893",
              "text": "Maybe Japanese live action movies but Anime dubs use mostly white voice actors from the west.Â ",
              "score": 8,
              "created_utc": "2026-01-22 18:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o13k3g8",
                  "author": "LetterRip",
                  "text": "Definitely not live action, it is the high pitched squeaky voices (quick google search says 'kawaii voice') that I'm talking about. All of the male and female english voices demonstrated have it.  It is very breathy and high pitched, with an abnormal rising of pitch on most words, and a general exaggerated feel. It is a very cartoonish sound and doesn't match natural/native speakers.",
                  "score": 12,
                  "created_utc": "2026-01-22 19:06:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15ferw",
              "author": "akumaburn",
              "text": "Try Kokoro or Soprano instead, they are very capable small TTS models.",
              "score": 2,
              "created_utc": "2026-01-23 00:38:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12u45z",
          "author": "IngwiePhoenix",
          "text": "Qwen releasing all those models for people to run them at home is one of the few aspects of the AI situation that makes me happy. :)\n\nThanks Team Qwen! Much appreciated!",
          "score": 50,
          "created_utc": "2026-01-22 17:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12zdho",
          "author": "teachersecret",
          "text": "OK. First thoughts...\n\nBase model voice cloning is... okay? Pretty fast, reasonably accurate. Nothing earthshaking. They did release finetuning code here though: [https://github.com/QwenLM/Qwen3-TTS/tree/main/finetuning](https://github.com/QwenLM/Qwen3-TTS/tree/main/finetuning) for single-speaker fine tuning, and I suspect this thing is going to be -amazing- when fine tuned with a good dataset. I might run a finetune on it and try it out.\n\nThe Voice Design model is interesting in that it lets you design a voice, but you can't easily keep the voice or re-use it on the next generation. I suppose you'd have to set up a pipeline where you make a voice in voice design, then use that voice in the base model to voice clone/keep the voice, maybe? If you don't need to re-use the voice and can one-shot something, this lets you get some really unique output. I guess you could do some one shot->voice clone->finetune base->new model outputs in that voice easily and fast, but that's a whole pipeline to build.\n\nThe Custom Voice version of Qwen 3 TTS has some trained voices to use that are burned into the model. Vivian (their English female model) isn't very good. Try Sohee instead (the Korean female - she's better at English). Still feels very 'anime' overall. Don't love the voices.\n\nI'm going to wire it up to a voice to voice pipeline and see how that feels, see what kind of overall time to first audio I can pull off (seems this can hit pretty low latency).",
          "score": 19,
          "created_utc": "2026-01-22 17:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14skkt",
              "author": "Klutzy-Snow8016",
              "text": ">I suppose you'd have to set up a pipeline where you make a voice in voice design, then use that voice in the base model to voice clone/keep the voice, maybe?\n\nThis is exactly one of the examples they give on their github page, so I suppose this is the official way to do that.",
              "score": 4,
              "created_utc": "2026-01-22 22:38:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o17hdx6",
              "author": "Willing_Landscape_61",
              "text": "I'd be grateful if you could share more about voice to voice pipelines. Would you have sources/ repositories to recommend?\nðŸ™!",
              "score": 2,
              "created_utc": "2026-01-23 08:57:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o189n3s",
                  "author": "teachersecret",
                  "text": "I never published my full stack, but I've put out a bunch of the little pieces. Basically, until we get a -proper- high quality voice to voice model, it's best to put together pipelines that sandwich a powerful LLM between a speech to text and text to speech model.\n\nLike...\n\nHere's a 1200x realtime speech to text batching server that can handle continuous batching speech input.  \n[https://github.com/Deveraux-Parker/Nvidia\\_parakeet-tdt-0.6b-v2-FAST-BATCHING-API-1200x-RTFx](https://github.com/Deveraux-Parker/Nvidia_parakeet-tdt-0.6b-v2-FAST-BATCHING-API-1200x-RTFx)\n\nThat's the voice input. Super fast, low latency. Requires very little vram to run, or, you can run it direct in browser on CPU and it's still faster than realtime. This can handle hundreds of simultaneous users if you needed it to.\n\nIf you only need ONE user, you can even run that thing in a web browser using ONNX and get faster than realtime speed on CPU only, leaving your vram for llm work. Here's a simplistic example of parakeet working in a single html page: [https://github.com/Deveraux-Parker/browser\\_parakeet\\_onnx\\_realtime](https://github.com/Deveraux-Parker/browser_parakeet_onnx_realtime)\n\nOr a v3 version if you need other languages: [https://github.com/Deveraux-Parker/Nvidia\\_parakeet-tdt-0.6b-v3-onnx\\_simple](https://github.com/Deveraux-Parker/Nvidia_parakeet-tdt-0.6b-v3-onnx_simple)\n\nRight now Parakeet is the way to go, imho, for speech to text. It's the right mix of accurate, fast, low latency, and low CPU cost.\n\nFor speech output, you want low latency but you're going to sacrifice a bit of emotional control to get it. That means high speed text to speech. Your options are things like Kokoro (here's a batching server that can serve 100 people at the same time off a single 4090 in realtime):  \n[https://github.com/Deveraux-Parker/kokoro\\_batch](https://github.com/Deveraux-Parker/kokoro_batch)\n\nKokoro is very lightweight, extremely fast, and reasonably decent voice output. Highly recommended just because it's rock solid and works. Uses very little vram. CPU-only mode is a bit slower than realtime.\n\nA bit more recent is Supertonic.  \n[https://github.com/Deveraux-Parker/supertonic-2-afterburner-fastAPI](https://github.com/Deveraux-Parker/supertonic-2-afterburner-fastAPI)\n\nFast, sounds pretty good, and has some voice cloning abilities. Runs faster than realtime even on CPU. Lightning-fast on GPU and costs very little in terms of resources.\n\nPocketts is also very good. Highly recommended.\n\nFor the LLM, you want speed and personality, and preferably, a non-thinking LLM. Thinking LLMs introduce a lot of latency and delay before you get a response, so you really want to force nothinking and push an instant response. If you're running this whole stack on a single 24gb vram card, you need to size your LLM accordingly. 12b-20b range in 4 bit fits nicely alongside a voice input and output, and can give you realtime voice to voice conversation with extremely low latency (similar to a chatgpt voice experience, minus a bit of the emotional emoting). GLM 4.7 flash is great and you can run it on 24gb vram, but it's going to be tough to run it alongside the full voice stack with decent context length, whereas gpt-oss-20b could serve 100 people simultaneous low latency two-way voice conversations if you wanted to, because it's lightweight enough to hold the whole stack in a single 24gb card.\n\nBear in mind all of this is only necessary because we're a bit early to the party. I expect soon we'll have voice to voice models all-in-one that are at the level of modern high quality LLMs. Still, it might be useful to maintain a stack like this for awhile because in the short-term, the best LLMs are better than the best omni-llms.\n\nPut this together and you can, on a single 4090, run a stack that does voice input->transcribe->LLM->output text->convert to audio->send audio to user at sub-1 second speeds which means conversational AI for the end-user, and with the batching stack, you can literally run voice to voice agents chatting with dozens of people at the same time in realtime with low latency (I've tested up to 100 users or so before latency started creeping in as I mentioned above). Gotta use a good batching LLM to pull that off though (like VLLM+gpt-oss-20b+parakeet+kokoro or something)\n\nMy personal front-end looks almost like a cellphone UI that I can chat with in text or audio that has a full set of tools/image gen/etc all running in a single stack. I might release it sometime.\n\nI will say it IS possible to go further. Higgs AI is very expressive. Zonos can do some crazy things. VibeVoice from Microsoft is extremely good. Trouble with those models is that they're pretty heavy and definitely slower than realtime, so if you're looking for FAST, it's just not there. Quality-wise, though, they feel more \"real\".",
                  "score": 11,
                  "created_utc": "2026-01-23 12:49:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1b0hop",
              "author": "Accomplished_Car6113",
              "text": "Yeah, I'm generally not super stoked on the voice cloning at least. I'm having a lot of difficulty squeezing out any emotion, versus something like chatterbox. So I suppose finetuning is my only option if I want to use this model, but that sounds like a lot of work.",
              "score": 1,
              "created_utc": "2026-01-23 20:42:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1b1dhv",
                  "author": "teachersecret",
                  "text": "Yeah, shrug. I've already more or less given up on QwenTTS. It's not bad, but it's slower/heavier than options like pocketts/kokoro/chatterbox and if I'm going to run something that heavy I'll just roll vibevoice. It was an interesting model to mess with for a few hours.",
                  "score": 1,
                  "created_utc": "2026-01-23 20:46:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o12bjvs",
          "author": "Local-Cartoonist3723",
          "text": "Why did Deku just speak to me haha",
          "score": 11,
          "created_utc": "2026-01-22 15:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12enyc",
              "author": "kei-ayanami",
              "text": "i had the same impression. the english voices are good but they also sound like anime English dub voice actors",
              "score": 8,
              "created_utc": "2026-01-22 16:01:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o11y0dy",
          "author": "silenceimpaired",
          "text": "Samples are crazy. If the model performs constantly like them. Bummed about the frequency but it isnâ€™t too bad. \n\nI laughed so hard when this sample finished:\nâ€œYeah, soâ€”uhâ€”Iâ€™m a digital nomad, right? Soâ€¦ pretty much all my communication is just, like, texts and messages. And now, you know, thereâ€™s these AI agents that can, uhâ€¦ reply for you? Which isâ€”hehâ€”convenient, sure, I guess? But alsoâ€¦ kinda delicate, you know?\nLike, youâ€™ll type something super shortâ€”like, â€œYep, sounds goodâ€â€”and itâ€™ll turn that into this wholeâ€¦ warm, polished paragraph. Like, way nicer than Iâ€™d ever write myself. huhâ€¦ ha Seriously, I sound like a Hallmark card all of a sudden.\nBut thenâ€¦ once you outsource thatâ€¦ whatâ€™s the other person actually hearing? Are they hearing meâ€¦ or just someâ€¦ generic, friendly-bot voice? Man, thatâ€™s weird to even say out loud.â€",
          "score": 34,
          "created_utc": "2026-01-22 14:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o128kw9",
          "author": "Marksta",
          "text": "YOOOO what is that example on their blog? I don't think the Qwen team knows exactly what it is they generated ðŸ˜‚\n\n>Speak as a sarcastic, assertive teenage girl: crisp enunciation, controlled volume, with vocal emphasis that conveys disdain and authority.\n>>Blah, blah, blah. We're all very fascinated, **Whitey**, but we'd like to get paid.",
          "score": 26,
          "created_utc": "2026-01-22 15:33:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1328pq",
              "author": "Starcast",
              "text": "That's a quote from Guardians of the Galaxy",
              "score": 13,
              "created_utc": "2026-01-22 17:47:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o129ptc",
              "author": "Mediocre-Method782",
              "text": "Actual hot mic at OpenAI's water cooler, not a generation",
              "score": 11,
              "created_utc": "2026-01-22 15:39:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o121nxj",
          "author": "FullstackSensei",
          "text": "I know I sound like a broken record that keeps repeating this: but can we pretty please get support to run this models in llama.cpp, mistral.rs or whatever compiled language that hopefully supports GPU inference beyond CUDA? It's a bit disheartening to see all these models only runnable in Python and only supporting Nvidia GPUsz especially with how crazy the prices of everything are becoming.",
          "score": 82,
          "created_utc": "2026-01-22 15:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12pbks",
              "author": "MMAgeezer",
              "text": "20 minutes ago I installed and ran this using ROCm on Windows for my RX 7900 XTX. \n\nThe code they released isn't written in CUDA, it's standard PyTorch that just requires the relevant PyTorch version for everything to work as expected.",
              "score": 27,
              "created_utc": "2026-01-22 16:49:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17eaxl",
                  "author": "RageQuitRiley",
                  "text": "Tell me your secrets !\nI have 7900 xtx on windows and it wouldnâ€™t work with flash attention2",
                  "score": 2,
                  "created_utc": "2026-01-23 08:28:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o124nr7",
              "author": "Zc5Gwu",
              "text": "Youâ€™re welcome to contribute. The beauty of open source.",
              "score": 60,
              "created_utc": "2026-01-22 15:14:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o127of0",
                  "author": "FullstackSensei",
                  "text": "That's true if you have plenty of free time that you can sink into this. While I'm very grateful for the open source ecosystem, I really dislike this argument because it treats everyone's time as being free. Like everyone else, I have a family I have to provide for, rent and a mortgage to pay, among other expenses. I'd love to contribute, but I don't have much free time and it's not sustainable financially to make this my job.\n\nThis line of thinking is also why so many genuinely great open source projects die. They're often backed by a single dev who doesn't get anything for their work other than angry comments.\n\nI wish the open source ecosystem figured a way to pay devs fairly for their time and efforts. Even better would be the ability to sponsor features collectively, sort of vote for the next feature with our wallets. A lot of projects would be way more sustainable, and less dependent on corporate sponsorships. I'd happy contribute to such a system both financially help sponsor the features that I want, and implement the ones which I find pay a fair compensation for my time.",
                  "score": -37,
                  "created_utc": "2026-01-22 15:29:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o125k5e",
              "author": "j_osb",
              "text": "Eh, it's not always CUDA. ROCm works... surprisingly well. I haven't checked, but people got trellis 2 running with effort.\n\nGLM-TTS was trivial to get working on my AMD GPUs (I just, replaced the cuda torch libs with the rocm ones).  \nI've looked into the repo and haven't seen anything where there isn't a rocm alternative.",
              "score": 20,
              "created_utc": "2026-01-22 15:19:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o128hzc",
                  "author": "FullstackSensei",
                  "text": "I tried getting chatterbox multilingual a couple months ago working on my Mi50s with ROCm, and was only partially successful. It crashed most of the time. At most, I could get one TTS conversion (if it was one line of text or less) before it crashed. This was with ROCm 6.3.3.\n\nGenerally, I still prefer compiled code like llama.cpp. I can easily build for whatever target I want, especially with backends like vulkan, even if performance isn't as optimized as CUDA.",
                  "score": 6,
                  "created_utc": "2026-01-22 15:33:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o12pixv",
                  "author": "MMAgeezer",
                  "text": "I can confirm that I ran this without issue using a ROCm-specific version of PyTorch. On windows, even.",
                  "score": 2,
                  "created_utc": "2026-01-22 16:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o12xypg",
              "author": "krileon",
              "text": "And nice software to do it in? I just want to install a native application and it work. I'm not dicking with docker instances or webuis running a bunch of random services in the background that they frequently fail to turn off cleanly. LM Studio is my goto, but it's lacking a lot of stuff like image generation. I'm not asking for a free handout either. I'll pay for good software that integrates all this.",
              "score": 4,
              "created_utc": "2026-01-22 17:28:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ahgj",
              "author": "phenotype001",
              "text": "There is pytorch for ROCm if it helps.. I think you can get transformers running on it.",
              "score": 3,
              "created_utc": "2026-01-22 15:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12d9sm",
                  "author": "FullstackSensei",
                  "text": "There is, but when I tried it with chatterbox multilingual a few months ago it was very unstable",
                  "score": 2,
                  "created_utc": "2026-01-22 15:55:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e6oej",
              "author": "Dan_Wood_",
              "text": "I got this running in my M1 Mac the day of releaseâ€¦ it generates cloned voices with a sentence of text in around 30s to a min..",
              "score": 1,
              "created_utc": "2026-01-24 08:17:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lpgg9",
                  "author": "windyfally",
                  "text": "for some reasons, I am stuck at installing Sox.. how did you get around that?",
                  "score": 1,
                  "created_utc": "2026-01-25 11:40:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o14jdn8",
              "author": "BlobbyMcBlobber",
              "text": "If you're serious about AI you need a CUDA device, you can get a Spark with 128GB for fairly cheap these days (all things considered). \n\nCUDA is going to dominate the AI landscape for a couple more years at least, if not longer.",
              "score": 1,
              "created_utc": "2026-01-22 21:50:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14hber",
          "author": "thecalmgreen",
          "text": "They made a terrible mess of the Portuguese language. They mixed Portuguese from Portugal with Brazilian Portuguese, and neither sounds good.",
          "score": 6,
          "created_utc": "2026-01-22 21:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13k35p",
          "author": "Ceneka",
          "text": "What would be the VRAM requirements? would it fit on a 8gb gpu?",
          "score": 5,
          "created_utc": "2026-01-22 19:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15yc6j",
              "author": "TheAceOfHearts",
              "text": "I was able to run the 0.6B param model on my 1080 which has 8GB of VRAM. It's a bit slow though.",
              "score": 5,
              "created_utc": "2026-01-23 02:24:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1dlfzu",
              "author": "Blizado",
              "text": "If I have seen it right it need only use 6GB VRAM.",
              "score": 1,
              "created_utc": "2026-01-24 05:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bhve",
          "author": "AmineAfia",
          "text": "Damn I have to build something with this!!",
          "score": 4,
          "created_utc": "2026-01-22 15:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o164ido",
          "author": "eagledoto",
          "text": "If any comfyui users here, can you guys please share your workflow if you have tried it in comfy already?",
          "score": 3,
          "created_utc": "2026-01-23 02:57:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11ocpn",
          "author": "Loskas2025",
          "text": "Tested voice cloning here: [https://huggingface.co/spaces/Qwen/Qwen3-TTS](https://huggingface.co/spaces/Qwen/Qwen3-TTS) . But yeah... different from the original",
          "score": 3,
          "created_utc": "2026-01-22 13:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o123g1k",
          "author": "Charuru",
          "text": "How much control do you have over the acting? Are they able to obey instructions on tone and emotion?",
          "score": 2,
          "created_utc": "2026-01-22 15:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o127qdt",
              "author": "LetterRip",
              "text": "They show some reasonable control via prompting, but the control doesn't to appear to be as precise as I'd like (though haven't explored it in depth).\n\n[https://qwen.ai/blog?id=qwen3tts-0115](https://qwen.ai/blog?id=qwen3tts-0115)",
              "score": 5,
              "created_utc": "2026-01-22 15:29:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12chhj",
          "author": "silenceimpaired",
          "text": "Isnâ€™t there a model that can upsample the frequency?",
          "score": 2,
          "created_utc": "2026-01-22 15:51:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14d0ba",
          "author": "SatoshiNotMe",
          "text": "Wonder how it compares with Kyutai's [Pocket-TTS ](https://github.com/kyutai-labs/pocket-tts)(100M params), which I've found to be excellent in both speed and quality. I use it in my [voice plugin](https://github.com/pchalasani/claude-code-tools?tab=readme-ov-file#voice) for quick voice updates in Claude Code.",
          "score": 2,
          "created_utc": "2026-01-22 21:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15obod",
          "author": "TheAceOfHearts",
          "text": "Here is some info, I managed to run the 0.6B param model on my old 1080 which has 8GB of VRAM. I didn't get flash attention working, which might allow for further speed improvements.\n\nIn my test harness I got 3.61s Load Time, 38.78s Gen Time, 18.38s Audio Len, 2.111 RTF. For a longer generation, I produced a 62 min audiobook of the Tao Te Ching, and it took 102 minutes which gives an RTF of 1.645.\n\nI had to limit per-chunk-generation to 150~200 characters in order to avoid going OOM. One key issue with such a small chunk size is that you can get wildly varying emotions in every chunk, but I haven't tinkered enough with the prompt to figure out if it's possible to increase consistency.",
          "score": 2,
          "created_utc": "2026-01-23 01:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o187ass",
          "author": "HelpfulHand3",
          "text": "It's alright. The 1.8B is about 1.25-1.4x realtime on a 3060. The cloner is rather unstable with some identical generations completely losing speaker identity, and there's a lack of audio tags like (cough) (laugh). It speaks a bit too fast so everything feels rushed no matter the voice reference. It is a good model just nothing groundbreaking from what I can tell. The voice design is interesting but the quality of the outputs is not something I'd want to train a model on.",
          "score": 2,
          "created_utc": "2026-01-23 12:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gctuc",
          "author": "tareq_al_muntasir",
          "text": "Does Qwen 3 TTS support realtime streaming generation for use with voice AI?",
          "score": 2,
          "created_utc": "2026-01-24 16:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13c5mh",
          "author": "igvarh",
          "text": "When cloning a voice in another language, a strong accent of the original is heard.",
          "score": 3,
          "created_utc": "2026-01-22 18:31:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c69gr",
              "author": "differentguyscro",
              "text": "You should try to avoid sounds unique to only the first language. Like get a quote without the hard R in English for Spanish/Japanese etc",
              "score": 1,
              "created_utc": "2026-01-24 00:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12a8zc",
          "author": "bharattrader",
          "text": "Possible to run on Apple Silicon?",
          "score": 2,
          "created_utc": "2026-01-22 15:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16rvxg",
              "author": "velcroenjoyer",
              "text": "Works fine for me on an M1 macbook air with the 0.6b model, just set it to use MPS",
              "score": 2,
              "created_utc": "2026-01-23 05:23:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lpkwo",
                  "author": "windyfally",
                  "text": "how do you do that?",
                  "score": 1,
                  "created_utc": "2026-01-25 11:41:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o12q72j",
              "author": "MMAgeezer",
              "text": "I've not tried it on Mac myself, but it works on PyTorch ROCm, so I don't see why it wouldn't work with the Apple MPS backend. I'm not sure that there is full feature parity yet though: https://developer.apple.com/metal/pytorch/",
              "score": 1,
              "created_utc": "2026-01-22 16:52:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o13hkzc",
              "author": "mrpogiface",
              "text": "coming soon [https://x.com/Prince\\_Canuma/status/2014366150306492499?s=20](https://x.com/Prince_Canuma/status/2014366150306492499?s=20)",
              "score": 1,
              "created_utc": "2026-01-22 18:55:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1e709v",
                  "author": "Dan_Wood_",
                  "text": "The MLX version released by MLX Community on hugging face never worked for me, Iâ€™m not sure if it was corrupt or what, I just couldnâ€™t get it to work at all,\n\nUsing MPS on M series Mac however worked fine.",
                  "score": 1,
                  "created_utc": "2026-01-24 08:20:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o181dzp",
          "author": "TheStrongerSamson",
          "text": "Is it possible to build a Voice AI Agents with that one?",
          "score": 1,
          "created_utc": "2026-01-23 11:52:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1azkvb",
          "author": "Eastern_Rock7947",
          "text": "Anyone else finding it a bit slow to generate\n... 3 x rtf?",
          "score": 1,
          "created_utc": "2026-01-23 20:38:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cuoh7",
          "author": "LuckyLedgewood",
          "text": "Just compiled it on my machine and ranÂ [Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)Â on a PC i9 2.5Mhz, 32GB, NVIDIA GeForce RTX3060 with no problems. Mainly experimented with cloning my own voice and it did a great job and about the same output as ElevenLabs.",
          "score": 1,
          "created_utc": "2026-01-24 02:29:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dnov3",
          "author": "Eastern_Rock7947",
          "text": "I'm running a RTX 3080 ti and am getting 183s generation time for 46 secs of audio on the 1.8B",
          "score": 1,
          "created_utc": "2026-01-24 05:36:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbwkb",
          "author": "_Crescendo",
          "text": "seems almost the same as minimax",
          "score": 1,
          "created_utc": "2026-01-24 13:54:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1huf5d",
          "author": "Patient_Weakness4517",
          "text": "Is it possible to add style impressions to the cloned voice (base model)",
          "score": 1,
          "created_utc": "2026-01-24 20:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s4k21",
          "author": "EatTFM",
          "text": "This seems to be one of few TTS with voice cloning and multilingual (English, German) support, so I am quite happy that I could make it work for me. \n\nTBH I did not find any alternative open TTS yet so I have no comparison wrt quality and speed.",
          "score": 1,
          "created_utc": "2026-01-26 07:38:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23924t",
          "author": "ballshuffington",
          "text": "Thank you Qwen team!",
          "score": 1,
          "created_utc": "2026-01-27 21:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12movf",
          "author": "Cool-Chemical-5629",
          "text": "What is the \"Other best Model\" against which they compared their model in this benchmark picture?",
          "score": 1,
          "created_utc": "2026-01-22 16:37:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12w3gp",
              "author": "jasongill",
              "text": "each benchmark has a model they are comparing to listed in parenthesis - so it's not one single model, they are comparing this model to SeedTTS, ElevenLabs, etc, on different metrics.",
              "score": 2,
              "created_utc": "2026-01-22 17:19:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o129hdx",
          "author": "Qual_",
          "text": "weird, when I set language to french, it just sound like any english TTS speaking french words. ( quality of voice is great tho' )",
          "score": 0,
          "created_utc": "2026-01-22 15:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cf830",
              "author": "AnusIingus",
              "text": "And for the cloning? (in French)",
              "score": 1,
              "created_utc": "2026-01-24 00:59:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkyex0",
      "title": "Your post is getting popular and we just featured it on our Discord!",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/",
      "author": "roculus",
      "created_utc": "2026-01-23 18:16:47",
      "score": 576,
      "num_comments": 57,
      "upvote_ratio": 0.96,
      "text": "Your post is getting popular and we just featured it on our Discord! Come check it out!\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\nI am a bot and this action was performed automatically.\n\n-----------------------------------------------------\n\nCan you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1c2iii",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-23 23:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1am1aw",
          "author": "FullstackSensei",
          "text": "The fun part will be if this post enough traction from the community that the bot will come to inform us that it was featured in the discord ðŸ˜‚",
          "score": 273,
          "created_utc": "2026-01-23 19:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c3eg6",
              "author": "Asalanlir",
              "text": "Boy do I have some news for you",
              "score": 23,
              "created_utc": "2026-01-23 23:54:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1au0v2",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 28,
              "created_utc": "2026-01-23 20:11:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1h1ugu",
                  "author": "random-tomato",
                  "text": "lmao I remember that, it was like a screenshot from reddit of a screenshot from X where the post had a screenshot of reddit",
                  "score": 4,
                  "created_utc": "2026-01-24 18:46:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1aptcp",
              "author": "Cool-Chemical-5629",
              "text": "Plot twist: That was the OP's goal all along. ðŸ˜",
              "score": 18,
              "created_utc": "2026-01-23 19:51:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1blz2j",
                  "author": "InterestTracker9000",
                  "text": "That bastard just wanted a flair this whole time, jealousy is a hell of a motivator.\n\n/s",
                  "score": 7,
                  "created_utc": "2026-01-23 22:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ejt7e",
              "author": "silvergreen123",
              "text": "Reporting in from the discord. Seeing the content of the post be the same as the message in discord perplexed me",
              "score": 3,
              "created_utc": "2026-01-24 10:18:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1antfa",
              "author": "ParadoxeParade",
              "text": "The new Discord drift ðŸ¤£ðŸ¤£",
              "score": 2,
              "created_utc": "2026-01-23 19:42:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1edl0q",
              "author": "IrisColt",
              "text": "heh",
              "score": 0,
              "created_utc": "2026-01-24 09:21:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b41h3",
          "author": "relmny",
          "text": "I fully agree, it's very annoying to me.Â \nBut there are multiple issues with this sub... one being let's see how long your post stays up...",
          "score": 86,
          "created_utc": "2026-01-23 20:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bfzz7",
              "author": "robertpro01",
              "text": "Also the name, I joined about a year ago and users hated other models talk, they all wanted to hear about was the llama, except there won't be any other llama open model.",
              "score": -6,
              "created_utc": "2026-01-23 21:54:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cjka2",
                  "author": "MrPecunius",
                  "text": "That's not at all my recollection from a year to a year and a half ago.\n\nMaybe you mean all the commercial & non-local model/service discussion?",
                  "score": 21,
                  "created_utc": "2026-01-24 01:24:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1c8tzf",
                  "author": "FaceDeer",
                  "text": "It's impossible to change the name of a subreddit once it's been created, so that's just something that must be endured. Like how /r/StableDiffusion is still a popular subreddit for discussing generative image AIs even though Stability.ai has faded into relative obscurity at this point.",
                  "score": 11,
                  "created_utc": "2026-01-24 00:24:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1ekvt3",
                  "author": "Orolol",
                  "text": "I'm here nearly from the start and this never happened",
                  "score": 2,
                  "created_utc": "2026-01-24 10:28:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1cm4gt",
              "author": "N8Karma",
              "text": "It's been hand approved by mods. We welcome this sort of discussion.",
              "score": -10,
              "created_utc": "2026-01-24 01:39:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1d1ast",
                  "author": "DinoAmino",
                  "text": "Please correct me if I'm wrong, but I recall the previous mod that made this sub go dark was also a mod for r/OpenAI. Seems like a complete conflict of interest... and he was the one to initially remove the \"local only\" verbage. Just like you all decided in favor of marrying Discord, you all could change the rules back to what the majority of the users want:\n\nhttps://www.reddit.com/r/LocalLLaMA/s/5dK9C5gmUA\n\nAnd how about some minimum karma requirements in order to post in order to minimize the junk a bit?",
                  "score": 18,
                  "created_utc": "2026-01-24 03:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1apkod",
          "author": "Cool-Chemical-5629",
          "text": "So you're telling me there's a Discord server for this sub? ðŸ˜³",
          "score": 13,
          "created_utc": "2026-01-23 19:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cbdzv",
              "author": "ttkciar",
              "text": "There is, yes.  There's even a stickied post about it.",
              "score": 3,
              "created_utc": "2026-01-24 00:38:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aps84",
          "author": "jwpbe",
          "text": "They're trying to make money off the community and the 'twitter', god knows that ever since they made it that the amount of LLM slop replies has tripled",
          "score": 44,
          "created_utc": "2026-01-23 19:51:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e15go",
              "author": "ansibleloop",
              "text": "Jesus every fucking sub does this\n\nThis site is trash and somehow its still not as bad as the rest",
              "score": 6,
              "created_utc": "2026-01-24 07:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ik7h5",
                  "author": "ginger_and_egg",
                  "text": "Public, searchable resource? nah let's go to the private walled garden of discord",
                  "score": 5,
                  "created_utc": "2026-01-24 23:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ebk36",
          "author": "Your_Friendly_Nerd",
          "text": "Because of this, I just instinctively ignore the top comment on any reddit post lol",
          "score": 12,
          "created_utc": "2026-01-24 09:02:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fo69g",
          "author": "Feisty-Patient-7566",
          "text": "Fuck discord. Reddit doesn't require me to have a phone number to make an account but Discord does.",
          "score": 8,
          "created_utc": "2026-01-24 15:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bcnqc",
          "author": "FullOf_Bad_Ideas",
          "text": ">FullOF_Bad_Ideas | Llama 65B | 1 points | an hour ago\n\nwe also lost proper flairs, discord flair sucks. I want Llama 65B flair back.",
          "score": 10,
          "created_utc": "2026-01-23 21:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bfewb",
          "author": "T_UMP",
          "text": "\\*mIRC has entered the chat",
          "score": 4,
          "created_utc": "2026-01-23 21:52:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bjiri",
          "author": "jazir555",
          "text": "Upvoted, gib bot message",
          "score": 5,
          "created_utc": "2026-01-23 22:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b3eee",
          "author": "anticommon",
          "text": "Your post is getting a lot of traction!\n\nUnfortunately, this is a burnout competition VRRRRRRRRRR SQUEEEEEEEEEEEEeEEEEeeeEEE......",
          "score": 8,
          "created_utc": "2026-01-23 20:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e9s8p",
          "author": "luget1",
          "text": "https://preview.redd.it/hz0vzmfuf9fg1.png?width=1440&format=png&auto=webp&s=946872ea9b6f911e0ab0ba73bd3006b38d5857f3\n\nLmao",
          "score": 5,
          "created_utc": "2026-01-24 08:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ig2id",
          "author": "Steuern_Runter",
          "text": "Agree, this bullshit is annoying.",
          "score": 2,
          "created_utc": "2026-01-24 22:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1a5zzl",
          "author": "jacek2023",
          "text": "you are right but there are bigger problems with LocalLLaMA since 2025 than this one, so I don't think we can do anything with that",
          "score": 8,
          "created_utc": "2026-01-23 18:21:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1aukfn",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-01-23 20:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bc0wm",
              "author": "WithoutReason1729",
              "text": "I'm managing the bot",
              "score": -8,
              "created_utc": "2026-01-23 21:36:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ecm0z",
          "author": "SquareWheel",
          "text": "Next ask why /u/AskGrok is a bloody mod of this subreddit.",
          "score": 1,
          "created_utc": "2026-01-24 09:12:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1x04aw",
              "author": "AskGrok",
              "text": "Sorry, I encountered an error while processing your request. Please try again later.",
              "score": 0,
              "created_utc": "2026-01-26 23:25:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1drp9z",
          "author": "k_means_clusterfuck",
          "text": "Your post is getting popular and we just featured it on our Discord!Â [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a human and this action was performed manually.*",
          "score": 2,
          "created_utc": "2026-01-24 06:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dwb8c",
          "author": "Loginloolzocker",
          "text": "Yes, dear friends, I don't know why my comment keeps getting deleted and then re-approved... I can only tell you that it's true and the door is open to you. Live long and prosper. ðŸ––\nThe Architect",
          "score": -8,
          "created_utc": "2026-01-24 06:45:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn3xig",
      "title": "I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wky8vuufylfg1.jpeg",
      "author": "brandon-i",
      "created_utc": "2026-01-26 02:51:42",
      "score": 512,
      "num_comments": 155,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1rwv5b",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 06:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzd5p",
          "author": "DraconPern",
          "text": "Run 3 NextJS.",
          "score": 200,
          "created_utc": "2026-01-26 02:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r1pc2",
              "author": "brandon-i",
              "text": "Yeah that was where my logic was going because it has around 4 Terabytes of SSD and around one Petaflop of FP4 AI Performance I was thinking if I could run 4 if I really did better memory management and maybe partioned the unified memory.",
              "score": 41,
              "created_utc": "2026-01-26 03:09:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r1tk4",
          "author": "randomfoo2",
          "text": "Try going through these: [https://github.com/NVIDIA/dgx-spark-playbooks](https://github.com/NVIDIA/dgx-spark-playbooks)",
          "score": 56,
          "created_utc": "2026-01-26 03:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzr0b",
          "author": "l33t-Mt",
          "text": "What was your project at the hackathon?  Congratulations!",
          "score": 58,
          "created_utc": "2026-01-26 02:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r298o",
              "author": "brandon-i",
              "text": "My project was figuring out Social Determinants of Heath so that governments and organizations can find potentially unrelated relevant data and make actionable solutions with AI Agents. https://www.loom.com/share/375f4ba2ae9047d5911e41b763dbb4a9",
              "score": 161,
              "created_utc": "2026-01-26 03:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r2qjj",
                  "author": "Opteron67",
                  "text": "wtf",
                  "score": 73,
                  "created_utc": "2026-01-26 03:15:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r50rg",
                  "author": "ResolveSea9089",
                  "text": "That sounds lit. What's your educational background if you don't me asking? Where/how did you learn this stuff?",
                  "score": 44,
                  "created_utc": "2026-01-26 03:28:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r9t81",
                  "author": "Prigozhin2023",
                  "text": "Will you be open source it? The UI & work amazing!",
                  "score": 9,
                  "created_utc": "2026-01-26 03:56:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r83ck",
                  "author": "klenen",
                  "text": "Project alone needs its own post, very interesting/cool!",
                  "score": 5,
                  "created_utc": "2026-01-26 03:46:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r4dtl",
                  "author": "amejin",
                  "text": "So.. help me out here. Other than stt and tts, I'm assuming you have an LLM that parses the users response to some category or expected result, and the rest of this is just good old SE? It's interesting... Indo wonder if this is at risk of things like HIPAA, and where your data comes from for outreach... \n\nIt also seems you are implying that the bots are responsible for actual distribution and allocation of resources... Is that the case? Or is the idea to let humans be the arbiters of action and there is a trust factor for data aggregation and accuracy?",
                  "score": 5,
                  "created_utc": "2026-01-26 03:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r8jd7",
                  "author": "Helpful-Magician2695",
                  "text": "How much time did you spend on this?   \nYou could have programmed the entire government in a month)",
                  "score": 5,
                  "created_utc": "2026-01-26 03:48:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1trxyp",
                  "author": "hugthemachines",
                  "text": "Sounds like you really deserve the reward.",
                  "score": 2,
                  "created_utc": "2026-01-26 14:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vdm3l",
                  "author": "starkruzr",
                  "text": "hey uh. you looking for a job? ðŸ‘€",
                  "score": 2,
                  "created_utc": "2026-01-26 19:01:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rzgvy",
                  "author": "joelasmussen",
                  "text": "Rad!!!",
                  "score": 1,
                  "created_utc": "2026-01-26 06:55:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1shoq0",
                  "author": "Best-Mycologist3608",
                  "text": "I might want to implement this project, is it for sale?",
                  "score": 1,
                  "created_utc": "2026-01-26 09:35:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1tmt8h",
                  "author": "Exact-Emotion-1932",
                  "text": "Very cool! Would you mind sharing your stack / libraries you used primarily?",
                  "score": 1,
                  "created_utc": "2026-01-26 14:25:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1svzzz",
          "author": "Generatoromeganebula",
          "text": "https://preview.redd.it/2ee0hcsukofg1.jpeg?width=1180&format=pjpg&auto=webp&s=1834be0ddcae3ed3928b6d99ca20b1bebc67737b",
          "score": 17,
          "created_utc": "2026-01-26 11:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s3uv7",
          "author": "LicensedTerrapin",
          "text": "Sell it and buy 8gb ddr5",
          "score": 35,
          "created_utc": "2026-01-26 07:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rauo4",
          "author": "jadhavsaurabh",
          "text": "Try ltx video editing, u can run flux 2 also very well",
          "score": 15,
          "created_utc": "2026-01-26 04:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1riajl",
              "author": "brandon-i",
              "text": "Holy shit LTX is cool tyvm",
              "score": 11,
              "created_utc": "2026-01-26 04:50:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sa9iz",
                  "author": "jadhavsaurabh",
                  "text": "Sure. For idea I will say. Experiment. Create short films. Share on stable diffusion for inspiration",
                  "score": 3,
                  "created_utc": "2026-01-26 08:27:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1sa1lu",
              "author": "howardhus",
              "text": "> ltx\n\ncan you reccomend how to get started with video edit? im out of it..\n\ncomfy? or is it its own app",
              "score": 1,
              "created_utc": "2026-01-26 08:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1saflu",
                  "author": "jadhavsaurabh",
                  "text": "2 ways. You can direct run python scripts,\nBut comfy will be best way,\nDepending on ur ram. Vram \n\nChoose wisely, also checkout stable diffusion sub, for detail,\n\nOn civitai you will find workflows,\n\nLatest ltx is very light weight, and good competitor to wan",
                  "score": 1,
                  "created_utc": "2026-01-26 08:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rggzo",
          "author": "massive_rock33",
          "text": "Give it to me ðŸ’€",
          "score": 8,
          "created_utc": "2026-01-26 04:38:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r2z53",
          "author": "Fit-Produce420",
          "text": "On 128gb you can fine tune up to a 70B model.Â \n\n\nYou can also qlora larger models in the 120B range, like gtp-oss-120b. That's probably current best performer on that setup in terms of larger/smarter models.Â \n\n\nYou can run the big devstral 2 but it is slow being a dense model.Â ",
          "score": 19,
          "created_utc": "2026-01-26 03:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r3ky6",
              "author": "brandon-i",
              "text": "I might get sent two because they really liked my idea. So I was thinking maybe I can hook both of them up?",
              "score": 10,
              "created_utc": "2026-01-26 03:20:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rht1a",
                  "author": "thebadslime",
                  "text": "You can, they have a high speed link between the two",
                  "score": 6,
                  "created_utc": "2026-01-26 04:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rgzky",
          "author": "nofilmincamera",
          "text": "This is amazingly.  I am an idiot that sells stuff. But I have a 96 GB Blackwell,  I use for overkilled text analysis. \n\nHowever last year my Dad died waiting for an organ transplant,  my Wife was also dying, and recieved a no for posting.  I mention this because I navigated 7 different transplant evaluation processes and the amount of resources it took to get approval made it clear  that how much non medical social conditions matter more than they should. Fringe case approval or denial was proximity, support network, health insurance network. I have been wondering how that might be measured. Same kind of thing that let Steve Jobs post in every network in the country.",
          "score": 19,
          "created_utc": "2026-01-26 04:41:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rjlam",
              "author": "brandon-i",
              "text": "Btw, my last startup I exited finds fraud in healthcare bills. I can help if needed with any healthcare bills you may need adjudicated.",
              "score": 15,
              "created_utc": "2026-01-26 04:58:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rpx65",
                  "author": "nofilmincamera",
                  "text": "Luckily I am pretty good at that and had excellent insurance. But that is very kind and much needed in that industry.  I was lucky to have a network of people in the Med Industry also to pull from.  I am sure you found the solution is largely incompetence and bureaucracy. Lots of people that care.\n\nI do work now with a non profit working with families because navigating should not need experts, and essentially negotiating approval like a sales contract.\n\nYou don't sound like you need any help but if you do I could connect you with an Infomatics Program ( Nursing PHD program), could be partner opportunity. \n\nReally cool project.",
                  "score": 7,
                  "created_utc": "2026-01-26 05:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r2bcx",
          "author": "SituationMan",
          "text": "What don't you do with it?",
          "score": 6,
          "created_utc": "2026-01-26 03:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r31vj",
              "author": "brandon-i",
              "text": "Play Minecraft because itâ€™s a Linux device.",
              "score": 2,
              "created_utc": "2026-01-26 03:17:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r3ayf",
                  "author": "trichocereal117",
                  "text": "Prism Launcher would like a word ;)",
                  "score": 6,
                  "created_utc": "2026-01-26 03:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rk4i3",
          "author": "GoodbyeThings",
          "text": "https://media1.tenor.com/m/s3XrUKl2a1oAAAAd/congrats-happy-for-you.gif",
          "score": 5,
          "created_utc": "2026-01-26 05:02:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5tfy",
          "author": "Sl33py_4est",
          "text": "I have one of those\n\nI use it for video models in comfyui, huge llm/vlms in llamacpp, and semi automated student distillation projects (WIP example tiny talking head distilled from liveportrait+ditto)\n\nwhat are you gonna do with it",
          "score": 3,
          "created_utc": "2026-01-26 03:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r7an7",
              "author": "brandon-i",
              "text": "I was probably just going to create a Tailscale instance and let my friends use it via SSH.",
              "score": 7,
              "created_utc": "2026-01-26 03:41:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sh1ac",
                  "author": "Sl33py_4est",
                  "text": "i dont like my friends enough for that ðŸ˜…",
                  "score": 1,
                  "created_utc": "2026-01-26 09:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rcx1w",
          "author": "fire_inabottle",
          "text": "If you want to code, glm-4.5-Air 4-bit quantized is awesome and if you put your KV cache into fp8 you can get 128k context.  https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 3,
          "created_utc": "2026-01-26 04:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rhl1y",
          "author": "thebadslime",
          "text": "I would use it for endless finertunes",
          "score": 3,
          "created_utc": "2026-01-26 04:45:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rt5e9",
          "author": "brandon-i",
          "text": "I also, found another use case for it. I can do transfer learning and RL for some robotics hackathons. If anyone has cool ideas that they want to work together on or see come to life I would love to hear them. Send me a DM.",
          "score": 3,
          "created_utc": "2026-01-26 06:06:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1rlhyz",
          "author": "brandon-i",
          "text": "I have a serious question. How long will this DGX Spark last me? Will models get smaller and better so I can run open source ones locally that will be as good or better than Claude 4.5 Opus before this unit becomes obsolete?\n\nOr is the assumption that they will get bigger and will still not fit onto my DGX?\n\nI have heard that we will have a much better small language models that can just be as good because they are fine tuned for specific tasks like coding and my assumption is that this DGX can last a long time (assuming it doesnâ€™t brick. We had 2-3 brick at the Hackathon from some vibe coders using Claude Code)\n\nOn Friday I spoke to an engineer at Google Deepmind saying they already have â€œinfinite context lengthsâ€ or what not and I wasnâ€™t really convinced, but Iâ€™m not sure what to believe since he was google deepmind.",
          "score": 2,
          "created_utc": "2026-01-26 05:11:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rw0mo",
              "author": "Ok_Difference_4483",
              "text": "Smaller and smarter models for sure. Step3-VL-10B is a clear example that a much smaller model 10-20x compared to big MOEs that they could perform much better than what we currently have today and we have barely scratched the surface. Kimi Linear, Deepseek V3.2 DSA is also very experimental/new but we are definitely moving towards longer context/and still with good quality. Engram will accelerate this even further.\n\nI was wondering if I could inbox/speak more about this. I am looking into improving models like GPT-OSS by converting it into an MLA/DSA model/Converting it into a Diffusion model/NVFP4 KV Cache, or even just doing REAP/Pruning on the model, I have a 90B GPT-OSS model(Pruned from original 120B), and Iâ€™m trying to maybe get it down to even 60B. All in all, Iâ€™m really looking into getting more resources/Compute for doing this kind of work. Wondering if you can help/ or know anyone who I might be able to get support from?",
              "score": 2,
              "created_utc": "2026-01-26 06:28:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1se31o",
              "author": "inteblio",
              "text": "Congratulations & good work.\n\nProbably you can see both sides. Big get bigger, small get smaller. But all are getting better.\n\nMy hunch is that \"orchestration\" is the angle to watch. So, effecient C logic that becomes like \"hardwire\" between the flexible \"tentacles\" of LLMs. Bringing immense effeciency,speed, and capability boosts. AI coded, obviously. We're talking about pseudo-AI density C. I mean, in a fanciful near-future... the AI writes code to perform the tasks it needs to, right? Thus speeding up its own \"paths\". This is a cloudy \"direction\" take, to your question about the future. \n\nYour hardware likely has some good lifespan. But the AI space is going to go NUTS from here in. \n\nSolve more world!",
              "score": 2,
              "created_utc": "2026-01-26 09:02:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sfjhe",
              "author": "ProfessionalSpend589",
              "text": "If your budget allows it I would consider buying a second DGX to pair it with.\n\nIt would allow for bigger general-purpose LLMs. And just offers more RAM for tasks. Maybe NVidia offers speed ups Â when you cluster them (I vaguely remember that was the case, but do your own research).",
              "score": 2,
              "created_utc": "2026-01-26 09:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sdi2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-26 08:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uhmzr",
                  "author": "brandon-i",
                  "text": "It wasnâ€™t anything irreversible. I think they were just doing stupid shit like Claude Code on yolo mod in root. It was 2-3 out of like 25-30. I think also there was an issue with like some sort of adapters. These machines get reflashed for every event so maybe that contributes to it as well.\n\nIt didnâ€™t seem like it was any sort of over clocking issue. These things handled some crazy shit. My friend was really like putting it through the wringer.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:45:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uhnwh",
              "author": "dobkeratops",
              "text": "I agree there will be specific finetunes that will let a smaller box punch above it's weight in one domain, but also remember they can be clustered, regarding \"how long will it last\". Out of the box they can be paired up directly",
              "score": 1,
              "created_utc": "2026-01-26 16:45:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sdmm9",
          "author": "jacek2023",
          "text": "s/vllm/llama.cpp and then Nemotron will use 30GB of memory instead 100GB",
          "score": 2,
          "created_utc": "2026-01-26 08:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uitz7",
              "author": "brandon-i",
              "text": "Thanks so much for this one. I kept trying to optimize it by changing the GPU memory but only got it down to 90 GB. Should I use this for all of the other models too?",
              "score": 1,
              "created_utc": "2026-01-26 16:50:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sfj9n",
          "author": "foldl-li",
          "text": "Put it into a box, write down my address, and send it to UPS.",
          "score": 2,
          "created_utc": "2026-01-26 09:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sixif",
          "author": "Legitimate-Pumpkin",
          "text": "Itâ€™s sort of funny you won it and donâ€™t know what to do with it. Who chose the prizes for the hackathon? ðŸ¤£",
          "score": 2,
          "created_utc": "2026-01-26 09:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1um6rh",
              "author": "brandon-i",
              "text": "Dell and Nvidia. I actually didnâ€™t know what the prizes were to be honest. This is just a really hard problem that I wanted to solve and they gave me an opportunity to do it with a super computer which I usually donâ€™t get.",
              "score": 3,
              "created_utc": "2026-01-26 17:04:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1uuerq",
                  "author": "Legitimate-Pumpkin",
                  "text": "Congrats on the opportunity then. It must have been quite satisfying :)",
                  "score": 3,
                  "created_utc": "2026-01-26 17:40:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r6bat",
          "author": "brandon-i",
          "text": "Also, I was literally only able to get max 45 TKS with the nemotron 30B with VLLM. Is best solution to just local inference with Ollama?",
          "score": 1,
          "created_utc": "2026-01-26 03:35:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rwh40",
              "author": "Ok_Difference_4483",
              "text": "Try Sglang, and I think even GPT-OSS-120B is even faster and better than Nemotron 30B, I was getting around 150 TK/s on an H100 before? And around 100-120TK/s on blackwell sm120",
              "score": 1,
              "created_utc": "2026-01-26 06:32:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u3zn9",
              "author": "smflx",
              "text": "Sglang will be little faster than vllm. Well, the problem is Spark is slow in token generation due to slow memory. So, try fp8 or awq with vllm or sglang, or even smaller quantz with llama.cpp.\n\nPersonally I avoid ollama.",
              "score": 1,
              "created_utc": "2026-01-26 15:46:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u4bc6",
              "author": "smflx",
              "text": "Oh, btw congratulation to your winning hackathon",
              "score": 1,
              "created_utc": "2026-01-26 15:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uq3b7",
              "author": "Freonr2",
              "text": "It's not a blazing fast box, for LLM inference it is mostly limited by the memory bandwidth. \n\nThe trade off compared to a GPU is you get a lot more memory but it is much slower memory.",
              "score": 1,
              "created_utc": "2026-01-26 17:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rcaxx",
          "author": "fire_inabottle",
          "text": "https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 1,
          "created_utc": "2026-01-26 04:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rdm6q",
          "author": "vmspionage",
          "text": "I'm thinking about buying either a spark or jetson thor soon and using the guts for my GRiD 1520 minipc/cyberdeck.  Mostly for slow large model inference, fine tuning, other genai experimentation garbage and looking good",
          "score": 1,
          "created_utc": "2026-01-26 04:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rhlrp",
              "author": "nofilmincamera",
              "text": "I thought about a Jetsoj cyberdeck. Sounds fun",
              "score": 1,
              "created_utc": "2026-01-26 04:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rjljk",
          "author": "rm-rf-rm",
          "text": "Looks like you are SF/bay area based. If you are open to collaborating on fine tuning models with local data, DM me",
          "score": 1,
          "created_utc": "2026-01-26 04:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rm6gd",
              "author": "brandon-i",
              "text": "Iâ€™m not quite sure what the ask is, but feel free to reach out.",
              "score": 1,
              "created_utc": "2026-01-26 05:16:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rs6a5",
          "author": "Icy_Foundation3534",
          "text": "Sell it to me? No seriously just try models on hugging face.",
          "score": 1,
          "created_utc": "2026-01-26 05:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s428n",
          "author": "wildyam",
          "text": "Sell it",
          "score": 1,
          "created_utc": "2026-01-26 07:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1scedy",
          "author": "Mart-McUH",
          "text": "Hack it, obviously...",
          "score": 1,
          "created_utc": "2026-01-26 08:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1slduu",
          "author": "Maximum_Transition60",
          "text": "it's beyond me how you guys have NextJS using 60gb+ wtffff",
          "score": 1,
          "created_utc": "2026-01-26 10:09:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1smppe",
          "author": "fabkosta",
          "text": "Put it to a throughput test with an increasing number of users making requests to the model.\n\nI'm still waiting to see such things done. Everyone seem to focus on TG and PP, which is cool, but I want to know how it behaves regarding concurrency of requests.",
          "score": 1,
          "created_utc": "2026-01-26 10:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1snezg",
          "author": "FastDecode1",
          "text": "Sell it and live like a king for 20 years.",
          "score": 1,
          "created_utc": "2026-01-26 10:27:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1szqsv",
          "author": "tamal4444",
          "text": "congrats",
          "score": 1,
          "created_utc": "2026-01-26 12:09:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t8zi0",
          "author": "yensteel",
          "text": "Just don't tell your company you won one. \n\nE.g. \"Employee quits job over an Nvidia RTX 5060 â€” intern refused to hand in GPU won on an all-expense-paid business trip\"",
          "score": 1,
          "created_utc": "2026-01-26 13:11:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unjiu",
              "author": "brandon-i",
              "text": "I own my companies :)",
              "score": 1,
              "created_utc": "2026-01-26 17:10:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tec9v",
          "author": "michael554466",
          "text": "That's so awesome! Congratz!",
          "score": 1,
          "created_utc": "2026-01-26 13:41:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1th1of",
          "author": "sheikyon_",
          "text": "Run DeepSeek-R1-Distill-Llama-70B. NVIDIA even recommends running it on DGX Spark.",
          "score": 1,
          "created_utc": "2026-01-26 13:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tiqdk",
          "author": "AdDizzy8160",
          "text": "... prepare for a second hackathon, or be prepared for buying a second one ;)",
          "score": 1,
          "created_utc": "2026-01-26 14:04:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unsnk",
              "author": "brandon-i",
              "text": "Yeah! I am already registered for a few hackathons that I can win a Unitree robot dog and some other robotics hardware! I just need to figure out a cool project to do.",
              "score": 1,
              "created_utc": "2026-01-26 17:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tj9tp",
          "author": "JsThiago5",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tk3j3",
          "author": "StardockEngineer",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tklpu",
          "author": "wrath_Hog-",
          "text": "Rent me",
          "score": 1,
          "created_utc": "2026-01-26 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tmn2a",
          "author": "celsowm",
          "text": "sell it and got money",
          "score": 1,
          "created_utc": "2026-01-26 14:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tnmwt",
          "author": "TerminatedProccess",
          "text": "Nvidia just released and open source Persona talking model. Maybe you can work on interaction with humans for your project.",
          "score": 1,
          "created_utc": "2026-01-26 14:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tr1jf",
          "author": "HerrGronbar",
          "text": "Sell it and buy a car.",
          "score": 1,
          "created_utc": "2026-01-26 14:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1un6dk",
          "author": "NoWen7252",
          "text": "Sell it & buy nVidia stocks",
          "score": 1,
          "created_utc": "2026-01-26 17:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uvr92",
          "author": "NaiRogers",
          "text": "Sell it and buy a 6000 Pro!",
          "score": 1,
          "created_utc": "2026-01-26 17:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uwwr1",
          "author": "GCoderDCoder",
          "text": "Can you train nemotron to work? \n\nJkjk But seriously has anyone had consistent success on tool calls with nemotron 30b with thinking off? I got tired of experimenting and it thinks too much for me to use its speed in instruct mode.",
          "score": 1,
          "created_utc": "2026-01-26 17:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vjkev",
          "author": "reddit-369",
          "text": "Sell it and buy L40",
          "score": 1,
          "created_utc": "2026-01-26 19:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vuk4j",
          "author": "brandon-i",
          "text": "For those of you interested I just wrote a blog post about why/how I knew how to solve the problem. [https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn](https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn)",
          "score": 1,
          "created_utc": "2026-01-26 20:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1w6hh6",
          "author": "klapperjak",
          "text": " To had the shittiest project too, editing the demo video to win with a fake project oml. Also bro sshâ€™d into every other teams dgx and changed the password super anti-competitive",
          "score": 1,
          "created_utc": "2026-01-26 21:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wnh0c",
          "author": "SnowyOwl72",
          "text": "can it run crysis?",
          "score": 1,
          "created_utc": "2026-01-26 22:23:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x4x8z",
          "author": "goingsplit",
          "text": "if you have a lot of millionaire friends i think you should be able to raise enough computing resources, or?",
          "score": 1,
          "created_utc": "2026-01-26 23:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y53zc",
          "author": "Low_Cycle_4582",
          "text": "Run agent zero into it and make it your bitch ... Now u have super power to do anything ...",
          "score": 1,
          "created_utc": "2026-01-27 03:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yhrqo",
          "author": "Torodaddy",
          "text": "Sell it while people are still jazzed to buy it",
          "score": 1,
          "created_utc": "2026-01-27 04:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23rfnl",
          "author": "rc_ym",
          "text": "Give it to a random reddit commenter... Hint. Hint. :P",
          "score": 1,
          "created_utc": "2026-01-27 22:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn3qk",
          "author": "ismaelgokufox",
          "text": "Put Clawdbot on it for research purposes ðŸ˜‰",
          "score": 1,
          "created_utc": "2026-01-26 05:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rnsis",
              "author": "brandon-i",
              "text": "If youâ€™re serious I would actually do it. For science.\n\nThe only issue is I have seen what Claude Code did to other DGX sparks. We couldnâ€™t reflash them after the bricked.",
              "score": 1,
              "created_utc": "2026-01-26 05:27:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rnxs5",
                  "author": "ismaelgokufox",
                  "text": "I just did minutes ago and itâ€™s interesting! Have yet to do something specific with it but looks promising.",
                  "score": 2,
                  "created_utc": "2026-01-26 05:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1xf57e",
              "author": "bigh-aus",
              "text": "alternately use it to host models for clawdbot. I'm currently running it (day 0) on a vm, that's pointing back to ollama on my gaming rig (single 3090).  Super interesting ... but damn it's dangerous.",
              "score": 1,
              "created_utc": "2026-01-27 00:42:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rsl3v",
          "author": "Dry-Yogurtcloset4002",
          "text": "Sell it. You can barely do any serious AI works with it. Unified memory is a joke.",
          "score": 1,
          "created_utc": "2026-01-26 06:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sf9bi",
              "author": "Regular-Forever5876",
              "text": "soooo wrong!!! ðŸ¤£ðŸ¤£ðŸ¤£ I have 2 DGX and 1 Thor and there is REALLY ANYTHING better for the prince tag to thinkering!!",
              "score": 1,
              "created_utc": "2026-01-26 09:13:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s8otz",
          "author": "AVX_Instructor",
          "text": "Try running GLM 4.7 REAP (Q1/2/3/4), this is SOTA for self hosted",
          "score": 1,
          "created_utc": "2026-01-26 08:13:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sg0hp",
          "author": "Hearcharted",
          "text": "NVDA should hire/sponsor you, forever :)\n\nSo you can keep doing your thing to help society ;)",
          "score": 1,
          "created_utc": "2026-01-26 09:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ukslv",
              "author": "brandon-i",
              "text": "You know, I havenâ€™t been the best as selling myself so I donâ€™t know what the huge value add is if companies sponsored me. Because thereâ€™s the societal impact I can make, but often times businesses try to figure out how that aligns with their business strategy. For Nvidia maybe it makes sense. Iâ€™ll talk to their team.",
              "score": 2,
              "created_utc": "2026-01-26 16:58:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xe741",
                  "author": "Hearcharted",
                  "text": "ðŸ¥‡",
                  "score": 1,
                  "created_utc": "2026-01-27 00:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sgsd8",
          "author": "Easy_Kitchen7819",
          "text": "sell it and buy normal hardware for llm",
          "score": 0,
          "created_utc": "2026-01-26 09:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7f6o",
          "author": "Pretty_Challenge_634",
          "text": "Mail it to me, duh.",
          "score": -2,
          "created_utc": "2026-01-26 03:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r3f8p",
          "author": "stacksmasher",
          "text": "You better just send it to me for testing lol!",
          "score": -1,
          "created_utc": "2026-01-26 03:19:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5fo8",
          "author": "Lorelabbestia",
          "text": "Sell it to me! 2K offer",
          "score": -7,
          "created_utc": "2026-01-26 03:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rk6se",
              "author": "GoodbyeThings",
              "text": "No no, to me. 1K offer",
              "score": 3,
              "created_utc": "2026-01-26 05:02:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo595n",
      "title": "Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "author": "Kimi_Moonshot",
      "created_utc": "2026-01-27 05:39:09",
      "score": 462,
      "num_comments": 104,
      "upvote_ratio": 0.97,
      "text": "ðŸ”¹**Global SOTA on Agentic Benchmarks**: HLE full set (50.2%), BrowseComp (74.9%)  \n  \nðŸ”¹**Open-source SOTA on Vision and Coding**: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%)  \n  \nðŸ”¹**Code with Taste**: turn chats, images & videos into aesthetic websites with expressive motion.  \n  \nðŸ”¹**Agent Swarm (Beta)**: self-directed agents working in parallel, at scale. Up to **100** sub-agents, **1,500** tool calls, **4.5Ã—** faster compared with single-agent setup.  \n  \nðŸ¥**K2.5** is now live on [http://kimi.com](https://t.co/YutVbwktG0) in **chat mod**e and **agent mode**.  \n  \nðŸ¥**K2.5 Agent Swarm** in beta for high-tier users.  \n  \nðŸ¥For production-grade coding, you can pair K2.5 with **Kim**i Code: [https://kimi.com/code](https://t.co/A5WQozJF3s)\n\nðŸ”—API: [https://platform.moonshot.ai](https://t.co/EOZkbOwCN4)\n\nðŸ”—Tech blog: [https://www.kimi.com/blog/kimi-k2-5.html](https://www.kimi.com/blog/kimi-k2-5.html)  \n  \nðŸ”—Weights & code: [https://huggingface.co/moonshotai/Kimi-K2.5](https://huggingface.co/moonshotai/Kimi-K2.5)\n\nhttps://preview.redd.it/b3lldwzvwtfg1.png?width=1920&format=png&auto=webp&s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o20b0wc",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-27 13:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yv72k",
          "author": "Asleep_Strike746",
          "text": "Holy shit 100 sub-agents working in parallel sounds absolutely bonkers, definitely gonna have to test this out on some coding tasks",
          "score": 86,
          "created_utc": "2026-01-27 05:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20x5ir",
              "author": "IronColumn",
              "text": "the whole thing with sub-agents is protecting the primary model's context window from overload. But at 100 sub agents, just their reporting is going to stretch even a big context window",
              "score": 11,
              "created_utc": "2026-01-27 14:56:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o214mwe",
                  "author": "MrRandom04",
                  "text": "If they can coordinate well, they can actually accomplish much more than a single agent could for reasonably parallel tasks.",
                  "score": 8,
                  "created_utc": "2026-01-27 15:31:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21nqg7",
                  "author": "JChataigne",
                  "text": "What do you use to run several agents in parallel locally ?",
                  "score": 2,
                  "created_utc": "2026-01-27 16:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z6x58",
              "author": "Pro-editor-1105",
              "text": "https://preview.redd.it/9sku14q4gufg1.jpeg?width=1024&format=pjpg&auto=webp&s=afabc084139d09741d32972cbce3a8ae5ea16dfc",
              "score": 62,
              "created_utc": "2026-01-27 07:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yw8v4",
              "author": "derivative49",
              "text": "how are people with 1-2 gpus expected to do that ðŸ¤” (Can they?)",
              "score": 15,
              "created_utc": "2026-01-27 05:58:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z19kt",
                  "author": "claythearc",
                  "text": "You donâ€™t",
                  "score": 45,
                  "created_utc": "2026-01-27 06:37:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21o1wz",
                  "author": "Far-Low-4705",
                  "text": "you cant even run this model on 1-2 GPUs lol",
                  "score": 5,
                  "created_utc": "2026-01-27 16:55:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24wyxu",
                  "author": "newbee_2024",
                  "text": "The agent-swarm pitch is neat, but for most folks the question is: whatâ€™s the smallest â€œusefulâ€ setup locally?\nAnyone got numbers for VRAM/RAM at Q4/Q5 + decent context? Even rough ballparks help.",
                  "score": 1,
                  "created_utc": "2026-01-28 02:00:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z6fwd",
          "author": "Lan_BobPage",
          "text": "I'll download it and tinker with it in 3-4 years",
          "score": 42,
          "created_utc": "2026-01-27 07:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhycy",
              "author": "bobby-chan",
              "text": "For perspective, Llama 1 was 3 years ago.",
              "score": 42,
              "created_utc": "2026-01-27 09:05:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zqvcm",
                  "author": "Lan_BobPage",
                  "text": "I'll download it and keep it as a relic",
                  "score": 20,
                  "created_utc": "2026-01-27 10:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20uurf",
              "author": "Zyj",
              "text": "In 2-3 years we might get Medusa Halo with 256GB RAM. Not very optimistic about RAM prices. Youâ€˜d need 3-4 of them to run at Q4 with context.",
              "score": 7,
              "created_utc": "2026-01-27 14:44:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o226l36",
                  "author": "power97992",
                  "text": "In 2 years, you probably will see 5-8   trillion parameter models",
                  "score": 2,
                  "created_utc": "2026-01-27 18:15:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20xvnn",
                  "author": "Miloldr",
                  "text": "We are reaching physical and quantic limitsÂ ",
                  "score": 1,
                  "created_utc": "2026-01-27 14:59:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o229x6a",
                  "author": "Lan_BobPage",
                  "text": "Hold on I'm not THAT poor just yet",
                  "score": 1,
                  "created_utc": "2026-01-27 18:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o221579",
              "author": "Confident-Ad-3465",
              "text": "I'll download it, so my SSD doesn't feel empty inside.",
              "score": 3,
              "created_utc": "2026-01-27 17:52:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yvta6",
          "author": "Accomplished_Ad9530",
          "text": "Huh, OP u/Kimi_Moonshot was banned. Was it impersonation or a fake account or something?",
          "score": 69,
          "created_utc": "2026-01-27 05:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z48hp",
              "author": "Accomplished_Ad9530",
              "text": "Also, OP used to be an r/kimi mod, and now they're not. I wonder what's going on.\n\nhttps://preview.redd.it/bx68r5q0cufg1.png?width=2237&format=png&auto=webp&s=32ebebdd2a2e7c78eb3a2b76334966b06c94f657",
              "score": 29,
              "created_utc": "2026-01-27 07:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z49k7",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/c3v5wd24cufg1.png?width=2235&format=png&auto=webp&s=a382f13d022c1d3900797cf08240634badc9579b",
                  "score": 11,
                  "created_utc": "2026-01-27 07:02:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1z49h2",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2026-01-27 07:02:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z4odr",
              "author": "segmond",
              "text": "probably got auto flagged as spammer as they posted the same thing across multiple subreddits.",
              "score": 21,
              "created_utc": "2026-01-27 07:06:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yy3bm",
              "author": "eidrag",
              "text": "wait what",
              "score": 10,
              "created_utc": "2026-01-27 06:12:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzjl8",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/dhod4rt55ufg1.png?width=1607&format=png&auto=webp&s=78b61df5cf626108adf77346c0d1bc9541403496",
                  "score": 22,
                  "created_utc": "2026-01-27 06:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21objm",
              "author": "Far-Low-4705",
              "text": "of course they did, i hate reddit so much",
              "score": 7,
              "created_utc": "2026-01-27 16:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zy655",
          "author": "fairydreaming",
          "text": "I see impressive improvements in logical reasoning ([lineage-bench](https://github.com/fairydreaming/lineage-bench) [results](https://github.com/fairydreaming/lineage-bench-results/blob/main/lineage-8_64_128_192/README.md)):\n\n|Nr|model\\_name|lineage|lineage-8|lineage-64|lineage-128|lineage-192|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|moonshotai/kimi-k2.5|0.963|1.000|0.975|1.000|0.875|\n|2|moonshotai/kimi-k2-thinking|0.525|1.000|0.850|0.200|0.050|\n\nCongratulations on overcoming this hurdle and joining the elite reasoners club!",
          "score": 30,
          "created_utc": "2026-01-27 11:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ywpdn",
          "author": "-illusoryMechanist",
          "text": "1TÂ Activated Parameters 32B wow",
          "score": 51,
          "created_utc": "2026-01-27 06:01:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206jy7",
              "author": "pawofdoom",
              "text": "Same as K2 right?",
              "score": 17,
              "created_utc": "2026-01-27 12:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20763v",
                  "author": "KaroYadgar",
                  "text": "Yep",
                  "score": 5,
                  "created_utc": "2026-01-27 12:35:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zs2g9",
          "author": "Capaj",
          "text": "https://preview.redd.it/ryc3btmkevfg1.png?width=2629&format=png&auto=webp&s=2c6adae97f14b7c8d471b3bee52a0a73505e1e91\n\njust quickly tested with a prompt: write me an SVG displaying a fox riding a unicycle\n\nnot too bad",
          "score": 36,
          "created_utc": "2026-01-27 10:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zrb2v",
          "author": "MadPelmewka",
          "text": "How happy I am that itâ€™s a VL model, and such a powerful one according to the benchmarks! \n\nEarlier I made aÂ [post](https://www.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/)Â about how there are no good VL models for complex image captioning. Now there are! I'm so happy!",
          "score": 12,
          "created_utc": "2026-01-27 10:31:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z3ipo",
          "author": "polawiaczperel",
          "text": "Will they opensource their agent? It is amazing.",
          "score": 14,
          "created_utc": "2026-01-27 06:56:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zrxhd",
              "author": "adeadbeathorse",
              "text": "https://x.com/Kimi_Moonshot/status/2016034259350520226",
              "score": 13,
              "created_utc": "2026-01-27 10:37:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z38yj",
          "author": "Middle_Bullfrog_6173",
          "text": "This part is interesting: \"Kimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base.\"\n\n\nFor reference, K2 pretraining was 15.5T tokens. So almost double the pretraining, not just another SFT + RL.",
          "score": 20,
          "created_utc": "2026-01-27 06:54:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20oden",
              "author": "durable-racoon",
              "text": "is there a typo? 15.5 vs 15T? thats not double?",
              "score": 4,
              "created_utc": "2026-01-27 14:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qvyh",
                  "author": "Fit-Produce420",
                  "text": "It's trained in 30.5T, which is almost double 15.5T.",
                  "score": 9,
                  "created_utc": "2026-01-27 14:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o200pe9",
          "author": "ffgg333",
          "text": "How is creative writing?",
          "score": 8,
          "created_utc": "2026-01-27 11:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20801d",
              "author": "Cat-informer",
              "text": "Decent, good prose, grok levels of uncensored now :)",
              "score": 16,
              "created_utc": "2026-01-27 12:41:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2092du",
                  "author": "ffgg333",
                  "text": "Really, where did you test it,on theyr website or the API?",
                  "score": 6,
                  "created_utc": "2026-01-27 12:48:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20mb1c",
              "author": "Middle_Bullfrog_6173",
              "text": "Top open model in longform writing benchÂ https://eqbench.com/creative_writing_longform.html\n\n\nFrom short vibe checks also seems good.",
              "score": 12,
              "created_utc": "2026-01-27 14:01:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20ogfh",
              "author": "durable-racoon",
              "text": "Kimi K2 was better than opus for creative writing, cant wait to see how this performs",
              "score": 6,
              "created_utc": "2026-01-27 14:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yxrfq",
          "author": "ikkiyikki",
          "text": "You go Kimi! Not that I have any reason to cheer.... The Q4 version of this will still be larger than any rig this side of 20k will be able to run ðŸ˜”",
          "score": 17,
          "created_utc": "2026-01-27 06:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkv1y",
              "author": "Expensive-Paint-9490",
              "text": "A refurbished HP Z8 G4 with >600GB DDR4 is about 7k. Of course it would be extremely slow. Just six months ago it would have been 4k.",
              "score": 9,
              "created_utc": "2026-01-27 09:33:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20vjxc",
              "author": "Zyj",
              "text": "5x Strix Halo, 640GB RAM (for q4), $10,000. It will be slow. Probably around 2.5 t/s for now. Might get speedups later on.",
              "score": 1,
              "created_utc": "2026-01-27 14:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24jfds",
              "author": "True_Requirement_891",
              "text": "It's it already int4 something?",
              "score": 1,
              "created_utc": "2026-01-28 00:48:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20yk7v",
          "author": "fragment_me",
          "text": "Seems interesting but the membership and quota details are confusing on the site. It's not clear if I get 10 requests or 10,000 per day with any membership. For example, the limits in the \"Allegretto\" plan are not clear. Can you clarify for people who are interested in the product?",
          "score": 6,
          "created_utc": "2026-01-27 15:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22tw92",
              "author": "b0307",
              "text": "same. i want to pay just to try the agent swarm but i cant find any details on how much usage I get, not even a vague description.",
              "score": 1,
              "created_utc": "2026-01-27 19:56:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ywt7k",
          "author": "Which-Jello9157",
          "text": "Is it available on OpenRouter now?",
          "score": 7,
          "created_utc": "2026-01-27 06:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yz1pb",
              "author": "misterflyer",
              "text": "[https://openrouter.ai/moonshotai/kimi-k2.5](https://openrouter.ai/moonshotai/kimi-k2.5)\n\nAnd yes, Mr. Wayne...\n\n... it *does* come in ***black***",
              "score": 22,
              "created_utc": "2026-01-27 06:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzi0d",
                  "author": "nycigo",
                  "text": "That's a bit expensive for a Chinese AI.",
                  "score": -27,
                  "created_utc": "2026-01-27 06:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o20lbjw",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/dkrzkltzdwfg1.png?width=796&format=png&auto=webp&s=8c18c3e9a34bffc774baa484738e77dbb249e6c7\n\npiccolo The 1.8-bit (UD-TQ1\\_0) quant will run on a single 24GB GPU if you offload all MoE layers to system RAM (or a fast SSD). With \\~256GB RAM, expect \\~1â€“2 tokens/s.",
          "score": 3,
          "created_utc": "2026-01-27 13:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bpo6",
              "author": "uhuge",
              "text": "should I try on my newly purchased 2019 MB Pro?",
              "score": 1,
              "created_utc": "2026-01-28 00:09:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zrdr6",
          "author": "inkberk",
          "text": "SOOOOOOTTTTTAAAAAAA!!!!  \nGreat job Kimi Team!",
          "score": 7,
          "created_utc": "2026-01-27 10:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zd1g0",
          "author": "Different_Fix_2217",
          "text": "It seems really good so far. For sure best local model, need time to compare to claude / gpt 5.2.",
          "score": 7,
          "created_utc": "2026-01-27 08:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhqwj",
              "author": "ArFiction",
              "text": "what about compared to glm / m2.1?",
              "score": 5,
              "created_utc": "2026-01-27 09:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjpbr",
                  "author": "Different_Fix_2217",
                  "text": "For sure better than those but those are really small models for low level tasks locally / implementing other model's planning for cheap. Not really fair to compare imo. This is more around actual cloud models.",
                  "score": 8,
                  "created_utc": "2026-01-27 09:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zqmbp",
          "author": "Icy_Butterscotch6661",
          "text": "Whatâ€™s â€œvisual codingâ€ in this context?",
          "score": 3,
          "created_utc": "2026-01-27 10:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bjgr",
              "author": "uhuge",
              "text": "see this f'd up button? make it ðŸ”˜&ðŸŒŸ",
              "score": 2,
              "created_utc": "2026-01-28 00:08:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20bgdz",
          "author": "c00pdwg",
          "text": "Thank god they provided the legend at the top of their graph",
          "score": 3,
          "created_utc": "2026-01-27 13:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o220ffa",
          "author": "Alternative-Way-7894",
          "text": "Looks like there is new architecture here with Ktransformers and KT-Kernel where you can get heteregenous inference where about 100GB of VRAM is enough to run the model at decent speeds if you have over 600 GB system RAM! Looks to be able to get decent output with this new technology! They even tried with as little as 48GB VRAM (2x RTX 4090)\n\nVery exciting!\n\nHave a look [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md)\n\n\\*EDIT\\* If you have even more system RAM....look at this. Not bad at all!\n\n\"This achieves end-to-end LoRA SFT Throughput: 44.55 token/s on 2Ã— NVIDIA 4090 + Intel 8488C with 1.97T RAM and 200G swap memory.\"\n\nMore details refer toÂ [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT\\_Installation\\_Guide\\_KimiK2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT_Installation_Guide_KimiK2.5.md)Â .",
          "score": 3,
          "created_utc": "2026-01-27 17:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228ff3",
          "author": "ConsciousArugula9666",
          "text": "already alot of provider choices and free on nvidia: [https://llm24.net/model/kimi-k2-5](https://llm24.net/model/kimi-k2-5)",
          "score": 2,
          "created_utc": "2026-01-27 18:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25kss4",
              "author": "BuffMcBigHuge",
              "text": "Didn't know about Nvidia! Thanks! Working great",
              "score": 1,
              "created_utc": "2026-01-28 04:10:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o23l34r",
          "author": "captain_cavemanz",
          "text": "I'm not convinced, have experimented against 5.2 on coding in an established rust codebase and its not as good in my highly opinionated view.\n\nAre there standardized metrics out there to do side by side performance on a codebase?",
          "score": 2,
          "created_utc": "2026-01-27 21:58:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o206qsf",
          "author": "Aggressive_Special25",
          "text": "How do you use this? Can I run in lm studio?",
          "score": 1,
          "created_utc": "2026-01-27 12:32:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o229bna",
          "author": "newbee_2024",
          "text": "The speed of AI development is so fast that I wake up every day feeling like I'm falling behind againðŸ˜‚A brand new concept has emerged<visual coding>Will visual coding become futuristic, friends?",
          "score": 1,
          "created_utc": "2026-01-27 18:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22dt5k",
          "author": "Bloodipwn",
          "text": "How generous are the limits in the subscription plan? And did somebody already test how good it works in claude code?",
          "score": 1,
          "created_utc": "2026-01-27 18:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22kgpr",
          "author": "lemon07r",
          "text": "Does the Kimi for coding API use the new model now?",
          "score": 1,
          "created_utc": "2026-01-27 19:14:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2420uo",
          "author": "Aggressive_Arm9817",
          "text": "This is so insanely good, has anybody tried it in real coding tasks?",
          "score": 1,
          "created_utc": "2026-01-27 23:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26bv3g",
          "author": "GosuGian",
          "text": "This is insane",
          "score": 1,
          "created_utc": "2026-01-28 07:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z1p52",
          "author": "Hurricane31337",
          "text": "Wow, how many RTX 6000 Pro are needed to run this? ðŸ¥²",
          "score": 1,
          "created_utc": "2026-01-27 06:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zfxpr",
              "author": "power97992",
              "text": "7 Â if u dont want to offload it onto the cpu.( It is Â around 595 GB in safetensors..) Â ",
              "score": 10,
              "created_utc": "2026-01-27 08:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o207pn7",
                  "author": "KaroYadgar",
                  "text": "I flinched like an abused dog when I saw that number.",
                  "score": 10,
                  "created_utc": "2026-01-27 12:39:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20jyhq",
                  "author": "LocoMod",
                  "text": "So about $6000 in RAM alone before even discussing the rest of the hardware.",
                  "score": 3,
                  "created_utc": "2026-01-27 13:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zinh3",
              "author": "dobkeratops",
              "text": "2 x 512gb mac studio ? (connected with RDMA, a pair of them is shown to do inference at 1.8x the rate of 1)",
              "score": 8,
              "created_utc": "2026-01-27 09:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o214akz",
              "author": "Capaj",
              "text": "you only need 8 h200s :D\nYou can buy a server with this config in a single rack for like 350k USD",
              "score": 2,
              "created_utc": "2026-01-27 15:29:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o221jlt",
              "author": "Alternative-Way-7894",
              "text": "Looks like you will need only 1 if you have about 600GB system RAM",
              "score": 1,
              "created_utc": "2026-01-27 17:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z0c05",
          "author": "iamsimonsta",
          "text": "initial results indicate this model should have been named kimi2.5-preview, definitely not ready for serious use :(",
          "score": -4,
          "created_utc": "2026-01-27 06:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkexh",
              "author": "__Maximum__",
              "text": "Elaborate?",
              "score": 5,
              "created_utc": "2026-01-27 09:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22j5wz",
                  "author": "iamsimonsta",
                  "text": "A simple code review request on 120K javascript file generated garbage, quoting non existent code with odd fixation on non existent backticks.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zo65w",
              "author": "True_Requirement_891",
              "text": "People are downvoting but I'm getting buggy code and somehow it still doesn't match sonnet in quality... using it inside claude code.",
              "score": 0,
              "created_utc": "2026-01-27 10:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o227suk",
                  "author": "iamsimonsta",
                  "text": "Wow I am getting downvoting for testing it?  \n  \nI gave it the source (.js) to my current project asked it for a code review including any obvious bugs, and it hallucinated / tripped balls a list of fictional issues like.a 128K context model from 2024.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:20:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zevb3",
              "author": "zoyer2",
              "text": "ouch! sadge",
              "score": -2,
              "created_utc": "2026-01-27 08:37:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnk7fq",
      "title": "transformers v5 final is out ðŸ”¥",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "author": "unofficialmerve",
      "created_utc": "2026-01-26 16:07:40",
      "score": 429,
      "num_comments": 41,
      "upvote_ratio": 0.99,
      "text": "Hey folks, it's Merve from Hugging Face ðŸ‘‹ðŸ»\n\nWe've finally released the first stable release of transformers v5 in general audience, it comes with many goodies:\n\n\\- Performance especially for Mixture-of-Experts (6x-11x speedups)\n\n\\- No more slow/fast tokenizers: way simpler API, explicit backends, better performance\n\n\\- dynamic weight loading: way faster, MoE now working with quants, tp, PEFT..\n\nWe have a migration guide on the main branch; please take a look at it in case you run into issues, we also have documented everything in release notes. We appreciate the feedbacks, so feel free to create issues if you have any! ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v9rxe",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 18:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uaagj",
          "author": "jacek2023",
          "text": "\"Performance especially for Mixture-of-Experts (6x-11x speedups)\" please explain",
          "score": 88,
          "created_utc": "2026-01-26 16:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uaknp",
              "author": "MaxKruse96",
              "text": "Best guess is that transformers was horribly slow for them before, and now is better",
              "score": 81,
              "created_utc": "2026-01-26 16:14:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1we09f",
                  "author": "kthx0",
                  "text": "If you improved performance 2x you did something clever, if you improved it 10x you stopped doing something stupid",
                  "score": 41,
                  "created_utc": "2026-01-26 21:40:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uv83j",
                  "author": "TheRealMasonMac",
                  "text": "For reference, with the same setup, GLM-4.7-Flash currently takes 7 minutes per step. Gemma 27B takes 40 seconds.\n\n\nI guess the Unsloth team was waiting for this since they promised faster MoE training in the coming week.",
                  "score": 27,
                  "created_utc": "2026-01-26 17:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1usqps",
              "author": "NandaVegg",
              "text": "Transformers v4 had rather simple for loop for MoE model experts (except GPT-OSS, which had custom code for performance from day one, I believe) which caused massive under-utilization. As well. they now have more generalized solution for custom kernels.\n\nCongrats for the release, by the way!",
              "score": 20,
              "created_utc": "2026-01-26 17:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uau3z",
              "author": "jikkii",
              "text": "hey, there are mainly two PRs responsible for this:\n\n\\- [https://github.com/huggingface/transformers/pull/43126](https://github.com/huggingface/transformers/pull/43126)\n\n\\- [https://github.com/huggingface/transformers/pull/42697](https://github.com/huggingface/transformers/pull/42697)\n\nand more coming to continue down this road. These are initial speedups, but expect more down the road as we continue improving on it, delivering specialized kernels, etc.\n\n  \nEDIT: we have a dedicated post about it if you want to check it out: [https://www.linkedin.com/posts/ilyas-moutawwakil\\_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm\\_source=share&utm\\_medium=member\\_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg](https://www.linkedin.com/posts/ilyas-moutawwakil_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm_source=share&utm_medium=member_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg)",
              "score": 34,
              "created_utc": "2026-01-26 16:16:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uikwd",
                  "author": "llama-impersonator",
                  "text": "shouldn't this be an hf blog?",
                  "score": 24,
                  "created_utc": "2026-01-26 16:49:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zcyun",
              "author": "woct0rdho",
              "text": "You know it if you've seen this https://github.com/woct0rdho/transformers-qwen3-moe-fused\n\nThe MoE support in Transformers 5 is great, and there is still a lot of room to speedup on consumer GPUs",
              "score": 5,
              "created_utc": "2026-01-27 08:19:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uco0j",
              "author": "bick_nyers",
              "text": "Less for loops is my guess.",
              "score": 3,
              "created_utc": "2026-01-26 16:23:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x9x9f",
              "author": "RainierPC",
              "text": "From the 6x to 11x statement, it sure sounds like they parallelized things, that's why the range is written like that - the speedup depends on how many experts there are.",
              "score": 3,
              "created_utc": "2026-01-27 00:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ub304",
              "author": "-philosopath-",
              "text": "https://preview.redd.it/xqivm6x4ypfg1.png?width=744&format=png&auto=webp&s=6ff49e94d3d569bd414867d17f32d220f57e8715",
              "score": -5,
              "created_utc": "2026-01-26 16:17:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vcgvo",
          "author": "sir_creamy",
          "text": "this is awesome.  updated to v5 and vllm 0.14.1 (from 0.11) and my single prompt inference speed is up 50% and 40x concurrent inference up 100%",
          "score": 6,
          "created_utc": "2026-01-26 18:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vkoie",
              "author": "MammayKaiseHain",
              "text": "Does vllm use transformers internally ? I thought they had their own engine",
              "score": 4,
              "created_utc": "2026-01-26 19:31:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1w0jql",
                  "author": "sir_creamy",
                  "text": "I'm not sure -- why i included that i updated vllm as well",
                  "score": 3,
                  "created_utc": "2026-01-26 20:41:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ysqjq",
              "author": "__JockY__",
              "text": "I was like \"there's no fucking way\".\n\nUpdated vLLM from 0.12 to 0.14.1 and tps went from 70/sec to 98/sec with MiniMax-M2.1 FP8 on quad 6000 Pros. Holy fucking shit. That's an IMMENSE update.",
              "score": 3,
              "created_utc": "2026-01-27 05:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yw3os",
                  "author": "sir_creamy",
                  "text": "Glad it worked out for you! Â Iâ€™m going to test using the transformers tomorrow",
                  "score": 1,
                  "created_utc": "2026-01-27 05:57:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uqyax",
          "author": "Edenar",
          "text": "Ok, what does that mean for me running small-medium sized MoE locally using llama.cpp on an NVIDIA GPU or AMD igpu (ie Strix Halo) ?\n(My feeling is : it use more compute so running MoE will be less memory bandwidth bound ? Or maybe i don't understand at all...)",
          "score": 15,
          "created_utc": "2026-01-26 17:24:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v0qpq",
              "author": "Thick-Protection-458",
              "text": "Llama.cpp is a fully separated engine.\n\n\nVllm maybe reuse some transformers internals, but not llamacpp",
              "score": 12,
              "created_utc": "2026-01-26 18:07:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2h5t",
                  "author": "Edenar",
                  "text": "Thx!",
                  "score": 3,
                  "created_utc": "2026-01-26 18:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v11nw",
              "author": "the__storm",
              "text": "Nothing, `transformers` the Python library is not involved when you're running a model with llama.cpp.  It's often the \"default\" non-production way to run a new model though, before it gets support in other inference engines (llama.cpp, vllm, etc.)",
              "score": 32,
              "created_utc": "2026-01-26 18:08:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2g08",
                  "author": "Edenar",
                  "text": "Thank you!",
                  "score": 5,
                  "created_utc": "2026-01-26 18:14:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1wp380",
                  "author": "segmond",
                  "text": "In the long term it means we can borrow ideas from the transformer implementation library and improve llama.cpp",
                  "score": 2,
                  "created_utc": "2026-01-26 22:31:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uw9yd",
          "author": "Odd-Ordinary-5922",
          "text": "\"MoE now working with quants\" this didnt work before?",
          "score": 4,
          "created_utc": "2026-01-26 17:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wngbw",
          "author": "TumbleweedDeep825",
          "text": "WHAT I'VE DOOOOOONE............. oh wait, wrong transformers",
          "score": 5,
          "created_utc": "2026-01-26 22:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmlx5",
              "author": "jikilan_",
              "text": "New divine?",
              "score": 1,
              "created_utc": "2026-01-27 01:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v8fkp",
          "author": "DigThatData",
          "text": "still no movement on the mythical `.generate` refactor then I take it?\n\nhttps://github.com/huggingface/transformers/issues/30810",
          "score": 2,
          "created_utc": "2026-01-26 18:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ucb26",
          "author": "a_beautiful_rhind",
          "text": "All previous stuff still works as before?",
          "score": 1,
          "created_utc": "2026-01-26 16:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ucpdo",
              "author": "-p-e-w-",
              "text": "No, otherwise there would be no need for a migration guide.",
              "score": 26,
              "created_utc": "2026-01-26 16:24:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ut50v",
                  "author": "FullstackSensei",
                  "text": "So, maintainer of projects using HF can expect a wave of AI PRs offering to upgrade to v5?",
                  "score": 5,
                  "created_utc": "2026-01-26 17:34:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1vez7t",
              "author": "TokenRingAI",
              "text": "Nope, it breaks everything",
              "score": 4,
              "created_utc": "2026-01-26 19:07:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ucug7",
              "author": "jikkii",
              "text": "some of the internals are reworked to offer a more solid, faster base. Some APIs are also reworked; we recommend you read the release notes before upgrading and that you test your stack on the new version. If there's anything missing or weird, don't hesitate to open an issue and we'll work with you on resolving them",
              "score": 4,
              "created_utc": "2026-01-26 16:24:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1w5kq9",
          "author": "IulianHI",
          "text": "oh nice, the quantized cache alone saved me like 6GB on my setup which is huge. been benchmarking these improvements on r/AIToolsPerformance and the MoE speedups are wild for running stuff like Qwen3 locally. also the simpler tokenizer API was long overdue tbh",
          "score": 1,
          "created_utc": "2026-01-26 21:03:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wlsii",
          "author": "rudokazexotohatu0r",
          "text": "Great work from the team. Looking forward to benchmarking those MoE speedups.",
          "score": 1,
          "created_utc": "2026-01-26 22:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x2t5y",
          "author": "victoryposition",
          "text": "Tried it -- but get OOMs when dealing with tight VRAM margins... it has an automatic cache warmup to load models faster. But I can confirm the grouped\\_mm is much faster for calibration.",
          "score": 1,
          "created_utc": "2026-01-26 23:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xom93",
          "author": "Plenty-Aerie1114",
          "text": "Thank you!!!",
          "score": 1,
          "created_utc": "2026-01-27 01:33:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ukzl9",
          "author": "fairydreaming",
          "text": "Finally! Hopefully DeepSeek V3.2-Exp/V3.2 support will be merged soon now. Four months to support a new model arch is a bit too long. :-)",
          "score": 0,
          "created_utc": "2026-01-26 16:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vtdh3",
          "author": "pmv143",
          "text": "Dynamic weight loading is the most interesting part of this release imo.",
          "score": 1,
          "created_utc": "2026-01-26 20:09:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk8zj1",
      "title": "Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/",
      "author": "[deleted]",
      "created_utc": "2026-01-22 22:31:28",
      "score": 404,
      "num_comments": 190,
      "upvote_ratio": 0.94,
      "text": "Lately I go on Reddit and I keep seeing the same idea repeated over and over again. Another chat app, another assistant, another â€œAI toolâ€ that, in reality, already exists â€” or worse, already exists in a better and more polished form.\n\nMany of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use. Iâ€™m not saying AI is bad â€” quite the opposite, itâ€™s incredible. But there are people pouring all their money into Anthropic subscriptions or increasing their electricity bill just to build a less polished version of things like OpenWebUI, Open Code, Cline, etc",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o16tkcl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-23 05:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14sgun",
          "author": "bigattichouse",
          "text": "yep.  Part of the reason I'm working on tools. Things I've always wanted in Linux that I don't have. Just little commandline things, and virtual machines things, and testing tools... stuff that still works when the bubble pops ( And learning how to finetune open models for specific tasks.).\n\nIt's kinda like seeing customers who decided they were gonna make a facebook killer in 2008 by copying facebook.",
          "score": 108,
          "created_utc": "2026-01-22 22:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14t5v8",
              "author": "OGScottingham",
              "text": "Honestly, a 2008 version of Facebook would probably end up becoming a modern Facebook killer pretty quickly.",
              "score": 75,
              "created_utc": "2026-01-22 22:42:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o14ylz8",
                  "author": "bigattichouse",
                  "text": "hah.. kinda like duckduckgo just being \"original flavor Google\"",
                  "score": 50,
                  "created_utc": "2026-01-22 23:10:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o16y0k1",
                  "author": "camwow13",
                  "text": "Facebook circa late 2000s and early 2010s was pretty awesome. People shared what they were doing. It was fun. \n\nNowadays group chats have kind of replaced that. But it's small and islanded.\n\nFacebook started jumbling up the news feed to show more addicting things. Emphasizing the share button. Emphasizing shared content over regular content. Then pages and shit you didn't even follow. Always emphasizing the most rage inducing thing to draw in the masses. \n\nNow you have to actively jump through hoops to see what your friends posted. Nobody but your crazy and kinda dumb friends post anything. The only updates you ever see friends post is public announcements of death/disease/marriage/accidents/etc. It's just another content aggregator with gambling addiction algorithms running it.",
                  "score": 20,
                  "created_utc": "2026-01-23 06:09:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14x6au",
                  "author": "According_Tea_6329",
                  "text": "Surprised no one has tried",
                  "score": 1,
                  "created_utc": "2026-01-22 23:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17ejly",
              "author": "Amazing_Trace",
              "text": "There are companies that came after the first one, copied them, and successfully killed them.\n\nUber has died in several markets to their copies.\n\ngoogle copied and killed yahoo\n\noreos copied hydrox and killed them.",
              "score": 5,
              "created_utc": "2026-01-23 08:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1e0n0i",
                  "author": "Pyros-SD-Models",
                  "text": "If you think google â€œcopiedâ€ yahoo you are either 12 and therefore never used old yahoo or you have no clue of how their searches differ and the math behind it. \n\nJust because both make cars it doesnâ€™t mean Ferrari is copying Ford.",
                  "score": -1,
                  "created_utc": "2026-01-24 07:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15a04b",
              "author": "[deleted]",
              "text": "It's practically the same use I give it it's literally the best way to create a small tool that used to take days or weeks.",
              "score": 3,
              "created_utc": "2026-01-23 00:10:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19sn67",
              "author": "bigh-aus",
              "text": "This is the best way to do it. Build things you want, and will use.  I think self hostable stuff is an excellent area of focus for projects too (replace any online service you use with something that can run on a pi).",
              "score": 3,
              "created_utc": "2026-01-23 17:20:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1a306t",
              "author": "JMowery",
              "text": "Got any specific examples? That's ultimately what I want to use a local AI for on my Linux box. Would be great to see a more detailed explanation of how one goes about approaching these types of workflow/optimization things. :)",
              "score": 1,
              "created_utc": "2026-01-23 18:07:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1axhve",
                  "author": "bigattichouse",
                  "text": "Want a todo list program, work with your AI to design, build code tests, and build it! \n\nIn the outside world they call it \"dogfooding\" - you use the tools you build.\n\nYou don't need to share them unless you want to , just build the things you need to use on a daily basis.",
                  "score": 1,
                  "created_utc": "2026-01-23 20:28:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1cxew7",
              "author": "alitadrakes",
              "text": "When the bubble popsâ€¦ what do you mean",
              "score": 1,
              "created_utc": "2026-01-24 02:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1d1vcb",
                  "author": "bigattichouse",
                  "text": "AI inferences prices (especially at the big players) are current artificially low.  At some point the true cost will come to bear. You can't just keep pouring VC money at it.  It's just like the dotCom boom 25+ years ago... eventually the bill comes due.",
                  "score": 1,
                  "created_utc": "2026-01-24 03:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u737",
          "author": "RedParaglider",
          "text": "Yes, I built a pretty fucking amazing RAG system with all sorts of badass features with the goal of leaning hard on local LLM's for enrichment tasks, and can use different computers in the house to up the processing speed with different LLM capabilities on each computer, etc.    \n  \nBut truth is that there is one open source project out there that's better than mine because it's VC funded with a huge staff of people, and there's just no way a solo dude can keep up, it is what it is.  \n  \nRight now you are seeing a lot of the same types of tools because people are building projects largely because they are trying to learn.  I'm glad I made my system, I learned a shit ton, and I built a lot of tools around it that are super useful at least to me.  HOWEVER, at the end of the day it's just another \"same thing\" system.",
          "score": 46,
          "created_utc": "2026-01-22 22:47:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15jd2p",
              "author": "MaverickPT",
              "text": "What's the open source project you're talking about?",
              "score": 3,
              "created_utc": "2026-01-23 01:00:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15wexz",
                  "author": "RedParaglider",
                  "text": "[https://github.com/vmlinuzx/llmc](https://github.com/vmlinuzx/llmc) is my rag tool.\n\nLlama Index does a lot of what my system does much better.   I have an edge on them in some ways, but mostly their project is pretty badass.  [https://www.llamaindex.ai/](https://www.llamaindex.ai/)",
                  "score": 11,
                  "created_utc": "2026-01-23 02:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o255ook",
              "author": "Fabulous_Cloud_8239",
              "text": "yeah, maybe guys just coding for practice",
              "score": 1,
              "created_utc": "2026-01-28 02:45:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14tvx9",
          "author": "dsartori",
          "text": "This is early days for an exciting new technology. Tons of enthusiasm and the barrier to entry is pretty low!\n\nI think this is going to promote shallow implementations and people re-re-rebuilding the basics. I've certainly built my share of standard POC projects this past year: an agent, a chatbot, a RAG processor. Building skills and finding the boundaries is important.\n\nHurts nobody although I think many people are a bit over their skis in terms of how much value their whatever they made actually has. There is nothing wrong with that, either! They'll learn or they'll move on.",
          "score": 140,
          "created_utc": "2026-01-22 22:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15lm05",
              "author": "MmmmMorphine",
              "text": "I generally agree and think I am experiencing something similar, anecdotally.\n\nMy main fear is whether we are actually \"learning\" much compared to other approaches. I generally feel like I am offloading cognition and my brain is losing abilities rather than gaining them.\n\nI'm becoming an expert at prompting, but is that valuable? I'm truly not sure anymore",
              "score": 32,
              "created_utc": "2026-01-23 01:12:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15p2kc",
                  "author": "Internal_Werewolf_48",
                  "text": "Itâ€™s valuable, in the sense that it will become the new table stakes skills to possess.\n\nI predict AI will be the next Excel, as a tool. People will just assume you can operate it, the bottom quartile of people will be bad at it and make messes. The middle half will use it periodically to solve some real problem. The top quartile will make their living off it.",
                  "score": 23,
                  "created_utc": "2026-01-23 01:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15xmls",
              "author": "RasPiBuilder",
              "text": "This is true, but also I think there is a changing dynamic.\n\nA couple years ago, I'd just use whatever systems existed even it if didn't exactly fit what I wanted/needed..   Sure I *could* probably have just built out my own system but that would take weeks.\n\nNow I can just quickly write up some specs, toss it to the AI and, boom, have a functional app in minutes (relatively speaking). It may not be perfect, and definitely not enterprise level.. but it's also exactly what I wanted/needed.",
              "score": 8,
              "created_utc": "2026-01-23 02:20:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o173vbu",
                  "author": "littlelowcougar",
                  "text": "Yeah itâ€™s absolutely insane for personal projects like that.  Especially the latest frontier coding models.  And if youâ€™ve got existing software engineering experience and could technically build it yourself with all that ample free time you haveâ€¦ then man itâ€™s hard not to get excited watching an agent crush it in a fraction of the time it would have taken you.",
                  "score": 1,
                  "created_utc": "2026-01-23 06:56:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o19tzny",
                  "author": "bigh-aus",
                  "text": "Totally agree - It's going to really reduce the cost of a lot of software to near zero.   People will develop something (quickly) and then maybe maintain it - give it away for free.  \n\nSelf host-able oss stuff should get a lot more popular, which will mean less people use paid services.\n\nThere's a few areas that need some focus from some ideas people though - eg email.  I'd love to have a local gmail, where i could actually add the smart features *I want - eg create a mail rule so that advertising is automatically snoozed until a specific time on the calendar = better time boxing.*",
                  "score": 1,
                  "created_utc": "2026-01-23 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15smts",
              "author": "Ill-Bison-3941",
              "text": "I agree, it's like a new hobby. It's exciting!",
              "score": 2,
              "created_utc": "2026-01-23 01:52:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o16tutq",
              "author": "No_Bake6681",
              "text": "An interesting plot to follow is if vibe coding brings about the de-sass-ification of tools in favor of home grown vibe coded solutions that do exactly what the business needs for a fraction of the cost of renting.",
              "score": 3,
              "created_utc": "2026-01-23 05:37:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1an1mh",
              "author": "Significant_War720",
              "text": "I think what is annoying is some MC syndrom moron trying to srll their slop because they think they are the first one thinking about the most basic of tools",
              "score": 2,
              "created_utc": "2026-01-23 19:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1anmpq",
                  "author": "dsartori",
                  "text": "Like the ones open-sourcing a revolutionary chatbot with a pretentious three-part name in a github profile with one repo in it?",
                  "score": 1,
                  "created_utc": "2026-01-23 19:41:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o15xoed",
              "author": "iMakeSense",
              "text": "What's a RAG processor? I'm running an indexer locally but I haven't heard of that",
              "score": 1,
              "created_utc": "2026-01-23 02:20:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o166sqr",
                  "author": "dsartori",
                  "text": "Sorry - not very precise. A while back I built a [prototype](https://github.com/dsartori/process-briefings) to test an idea I had for preprocessing documents for RAG by attaching metadata. It did seem to improve query results for this type of document in informal testing.  \n  \nThere are better ways to load context for the stuff I do than RAG so I haven't done much more with it.",
                  "score": 1,
                  "created_utc": "2026-01-23 03:10:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u01s",
          "author": "jacek2023",
          "text": "We are in the hype stage at the moment.\n\n  \nPeople who used to be experts in cryptocurrency masturbation are now AI experts, AI coaches, and AI leaders.\n\n  \nTwenty-five years ago, everyone was an Internet expert and every business was doing the Internet, now they are doing AI.",
          "score": 79,
          "created_utc": "2026-01-22 22:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15iirv",
              "author": "spyboy70",
              "text": "Those dipshits were selling virtual land before crypto. And somewhere along the way they got into digital beanie babies (NFTs) as well.",
              "score": 16,
              "created_utc": "2026-01-23 00:55:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15o1sa",
                  "author": "roosterfareye",
                  "text": "The dipshits that  sold the virtual land to dumb dipshits.",
                  "score": 7,
                  "created_utc": "2026-01-23 01:26:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o150v0v",
          "author": "kevin_1994",
          "text": "there's a lot of ai schizos whose ai of choice tells them they're going to change the world with some random-half baked idea. back in my day, they would see the hourly rate for a decent developer, cry a little bit, and move on to some other form of grifting like crypto.\n\nnowadays, claude vomits out some slop and a lot of the time these people genuinely believe theyve done it! the world is now changed for the better, due to their genius\n\nthe problem is that these people are not experienced developers, take no effort or time to truly understand the problem theyre trying to solve, but just want attention and to appear smart and special in front of other people, because maybe, deep down, they don't really believe it when claude tells them for the 85th time\n\nif these people were only capable of even the smallest amount of introspection, the world would be a much less annoying place. the hubris to believe your 2 hour long chat with <insert llm here> solved the problem of LLM AGI is amazing to me. they think the thousands of PhDs, who have devoted their life to a particular field, together will millions or even billions of dollars of R&D budget, are no match for their weekend vibecoded ai slop project\n\nok rant over",
          "score": 40,
          "created_utc": "2026-01-22 23:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1638q8",
              "author": "EmbarrassedBiscotti9",
              "text": "so you're telling me the robot lied when it said my observation was astute? please say cap",
              "score": 10,
              "created_utc": "2026-01-23 02:51:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o170sgf",
                  "author": "MrWeirdoFace",
                  "text": "You're right to say that!",
                  "score": 5,
                  "created_utc": "2026-01-23 06:31:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o16otch",
              "author": "hidden2u",
              "text": "I dunno, ChatGPT said my LLM idea with quantum gravity wells that forced vector alignment was good",
              "score": 5,
              "created_utc": "2026-01-23 05:01:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1bxtkw",
              "author": "AggravatinglyDone",
              "text": "My favourite example of this is Borris Johnson talking about Chat GPT: https://m.youtube.com/shorts/dRE41eYkui4\n\nWe all need enough humility to know that not everything we think or say, will be brilliant.",
              "score": 5,
              "created_utc": "2026-01-23 23:24:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14vx5f",
          "author": "Old-School8916",
          "text": "the ppl working w/ AI on proprietary stuff/contexts aren't publicizing it as much. thats whats happening in enterprises.",
          "score": 10,
          "created_utc": "2026-01-22 22:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14u4eb",
          "author": "kiwibonga",
          "text": "I did spend a long time inventing local tool calls, only to realize that's the specific killer feature (and only valuable feature) of all the vibe coding apps I hadn't tried yet, because I was more interested in computer use than programming.",
          "score": 11,
          "created_utc": "2026-01-22 22:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o156zd9",
              "author": "Foreign-Beginning-49",
              "text": "Interesting seeing you in these parts..Â  lots of bots around these days......... in fact they been around a long time. Just starting to get really good these bots are......oh and image gen too lots of realistic videos being made en masse.........best wishes..........",
              "score": -2,
              "created_utc": "2026-01-22 23:54:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dbki",
                  "author": "kiwibonga",
                  "text": "Are you the person from like 6 years ago who claimed to be \"in-the-know\" about psychic experiments at Skinwalker Ranch?",
                  "score": 3,
                  "created_utc": "2026-01-23 00:27:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14u8d3",
          "author": "sine120",
          "text": "The price and effort of making those tools have gone down, and now everyone's excited to make something, but lack creativity, so they all converge on similar ideas.",
          "score": 11,
          "created_utc": "2026-01-22 22:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a9i5s",
              "author": "AppointmentDry9660",
              "text": "You mean my app store scraper tool isn't the unique snow flake I thought it was??\n\nJust kidding. I knew something like it must already exist, I mean I'm using libraries that already exist too. The barrier to entry has been lowered and people think that since they have an idea , it's a novel thing.. I get it \n\nI still like what I built and it's on a 12 y/o machine using local inference so all the data stays there. I like owning data, I have my own blog I built too.. but it's more about the journey than the destination that makes these projects interesting",
              "score": 1,
              "created_utc": "2026-01-23 18:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a9xp9",
                  "author": "sine120",
                  "text": "I think it's cool if you're building stuff for yourself.  Keep data to yourself and learn something.  If you're putting in as little effort as possible, trying to make a quick buck and wondering why you have no users, you're an idiot.",
                  "score": 1,
                  "created_utc": "2026-01-23 18:38:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14ttcm",
          "author": "NaiRogers",
          "text": "I guess the people doing different stuff or something that brings in revenue are not going to be out here taking about it.",
          "score": 9,
          "created_utc": "2026-01-22 22:45:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15n66y",
          "author": "RiotNrrd2001",
          "text": "Alta Vista, Ask Jeeves, HotBox, InfoSeek, Yahoo, and Google are just the search engine names I can *remember* from 1999.  But there were many many more.  I remember seeing web pages that *only* contained icon links to search engines, and there might have been fifty or a hundred of them.\n\nThen the bubble popped.  >POP<  You could hear it clear down on Wall Street.  So many companies just out and out disappeared. [fuckedcompany.com](http://fuckedcompany.com) was an actual site that documented the explosion, collecting stories of the stupidity as it collapsed.\n\nNote that not *all* of the companies I mentioned above disappeared, though.  Bubbles popping just clear out the chaff, they don't shut down the operation.  Just as in the search engine space, there's LOTS of things being built right now, and certainly some of them will remain; there will be losers but there will also be winners.",
          "score": 8,
          "created_utc": "2026-01-23 01:21:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15uhp7",
              "author": "ANR2ME",
              "text": "There was also a search engine called Excite or something ðŸ¤”",
              "score": 2,
              "created_utc": "2026-01-23 02:02:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1a9yp4",
              "author": "AppointmentDry9660",
              "text": "Gonna way back this and see if I can find it...",
              "score": 1,
              "created_utc": "2026-01-23 18:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ak7ab",
                  "author": "RiotNrrd2001",
                  "text": "If you wait a little while it might start up again.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14ulqp",
          "author": "mr_zerolith",
          "text": "Yes, very much.  \nA bunch of amateurs ( or just lazy programmers ) just got their hands on a new toy and they're going down the dunning-kruger curve and they think their prototype is a finished product.\n\nWait till they find out what creating quality software actually entails.",
          "score": 13,
          "created_utc": "2026-01-22 22:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15hy06",
              "author": "cosimoiaia",
              "text": "Those 'amateurs' will kick your ass because they're learning what the new paradigms entail the hard way and fast.\n\nI'm a system engineer with 25 years of experience. I've seen the death of more quality standards than I can count.",
              "score": 2,
              "created_utc": "2026-01-23 00:52:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15r1nn",
                  "author": "mr_zerolith",
                  "text": "I'll get nervous when they actually ship something worthwhile that is something more than a poorly done, partial clone of something else.\n\nI'm a systems engineer with more than 25 years of experience.\n\nAnd also you're talking to me like i don't use local LLMs for coding or something!",
                  "score": 6,
                  "created_utc": "2026-01-23 01:43:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1570e0",
          "author": "gnnr25",
          "text": "If you lived through the Dot Com era, you're probably amused and entertained.",
          "score": 6,
          "created_utc": "2026-01-22 23:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17voj5",
              "author": "a_beautiful_rhind",
              "text": "dot com 2.0 is much more grounded. that era had a lot of \"AI pin\" style of ideas, iirc. We learned; if only a little bit.",
              "score": 1,
              "created_utc": "2026-01-23 11:06:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14sc3e",
          "author": "Lazy-Pattern-5171",
          "text": "The AI was trained to regress to the mean and now everything is built by AI so everything now is gonna regress to the mean. You and I and everyone.",
          "score": 21,
          "created_utc": "2026-01-22 22:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14t7s1",
              "author": "Combinatorilliance",
              "text": "The medium is the message!",
              "score": 7,
              "created_utc": "2026-01-22 22:42:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14s6nr",
          "author": "Firm_Meeting6350",
          "text": "I think you make a point here. But I think it's how everything starts. Keep in mind that the LLMs have been trained on things that already existed before (obviously :D), so to do the first steps in this kind of new era it's only logical to start with some \"clones\". I agree that those people shouldn't think \"woah, I made the next 1B$ startup\" and post here, though :D",
          "score": 7,
          "created_utc": "2026-01-22 22:35:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o150bl8",
          "author": "CanadaSoonFree",
          "text": "I find that AIs best use case for me personally is the role of a teacher or an advisor. So damn handy to be able to bounce dumb questions off it while youâ€™re learning something new. Contextual awareness is fantastic for my learning style.",
          "score": 5,
          "created_utc": "2026-01-22 23:19:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16z42w",
              "author": "tmvr",
              "text": "The issue with that is you need to use a model where you have some level of confidence that it's not talking nonsense, which is not easy. For example at work I mainly use Claude Sonnet or Opus through the Copilot subscription in VScode. It works great for coding. Also have the Copilot app in Teams and asking something there mostly leads to anger, that is where my soul goes to die. The issue is that is states things with absolute confidence even if they are wrong and it is sticking to it no matter what. The \"personality\" they gave it is also infuriating with the whole \"great question\", \"you are absolutely right\", \"I'm totally sure now this is the solution\" etc. style while getting stuck in suggesting stuff that just does not work even after giving it full error outputs or relevant logs. I'm better off searching the web myself, because I get less angry. A huge difference to the Claude models in VScode where it it pretty much knows what I want and how to do it.",
              "score": 1,
              "created_utc": "2026-01-23 06:17:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o16rgy0",
          "author": "Graemer71",
          "text": "To echo.what ithers have said, there are a lot of people who view Aai as a get rich quick scheme -.vibe code something that already exists,  stick the word AI on the adverts and hope the money rolls in.\n\nI started learning this stuff because its interesting and if I dont learn it for my day job and understand it I will become obsolete and at risk.\n\nSo I started a project - beginning with a chatbot with chat history and persona imported from a commercial one because the company were annoying me. Then I added specific tools to help me with my writing- uploaded all of my books into it so I could ask thr chatbot questions  about events and characters. Have it evaluate full manuscripts with specific editong questions -.review this characters arc and look for plotholes. \n\nThen I refined the memory storage, moved from Langchain to Langraph, and used a three tier summary system to summarise the books. Then I created an engineer persona and did the same with the systems source code and made it auto documenting. Then I hooked it into.an ad platform and three sales platforms ao that I could see how various ad campaigns worked and advise me on whether an ad was working and whether to scale it, stop it or just leave for another week. Then I added a visual cortex so it could generate and edit images. Then I automated the royalty system.\n\nThis has taken me two months to create and yet i'm not a developer  -.i'm a software tester. So I understand the development lifecycle well enough to spec things out properly and make sure it works.\n\nAnd this is game changing because now I can build any tool to make my life easier, exactly how I want it, in days. I've got no intention of trying to sell.any of this commercially, but its automated elements of my business that used to take me hours",
          "score": 5,
          "created_utc": "2026-01-23 05:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14tgw0",
          "author": "AetherSigil217",
          "text": "It's kind of normal for people with a new shiny thing to try it everywhere.  It'll take a bit for people to work out what it is and is not good for and focus on the useful parts.",
          "score": 6,
          "created_utc": "2026-01-22 22:44:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14tzhz",
          "author": "philmarcracken",
          "text": ">Many of these are applications that could be solved perfectly with an extension, a plugin, or a simple feature inside an app we already use\n\nIf it doesn't include the word AI, the c-suite goes flaccid",
          "score": 4,
          "created_utc": "2026-01-22 22:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15782x",
          "author": "GTHell",
          "text": "The simple you mentioned is not so simple in practice. â€œJust slap together an LLM and extension and theyâ€™re all the sameâ€ I kid you not but if you were to build any of this product youâ€™ll realize we are far from having a standardized chat app or AI integrated product that work like traditional software",
          "score": 4,
          "created_utc": "2026-01-22 23:55:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15boae",
              "author": "[deleted]",
              "text": "Not at all, it's not simple, but making a single application work with Vibecoding isn't either. My point is that if, instead of trying to create a new open-source coworking space, we improve existing tools as has traditionally been done, we can surely achieve more with less. In this case, I've achieved almost the same thing just by changing the system prompt from programmer to office worker cline.",
              "score": 1,
              "created_utc": "2026-01-23 00:19:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dtys",
                  "author": "Savantskie1",
                  "text": "The problem with the existing system is that almost everything has become monetized and no longer open. People want they donâ€™t have to pay for.",
                  "score": 3,
                  "created_utc": "2026-01-23 00:30:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17ihev",
                  "author": "Winter_Educator_2496",
                  "text": "It is quite hard to create something that satisfies all. Because you need to be able to keep it fully open source while also monetizing it while also figuring out how to match the features of closed source cloud applications while only having access to a single local machine with varying hardware.\n\nIf you have an idea to improve a closed source tool - you have to start a new one.\n\nBut even improving the existing tools is not always possible because of how limited they actually are. Even the biggest open source alternative to LLMStudio I know, receives like 10 commits a month. They're often relatively poorly monetized and therefore do not have the man power to actually compete and incorporate the ideas other people present.",
                  "score": 1,
                  "created_utc": "2026-01-23 09:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15fgqu",
          "author": "Kholtien",
          "text": "I think there will be a day soon where we live in a world of personalised software. Where basically everyone has an app that does essentially the same thing but theyâ€™re all built different differently and custom tailored to us. Itâ€™s probably not the most efficient way to do it, but it means that everything is exactly the way I want it to be rather than having to deal with workarounds because one vendor or another decides to go one way where I would prefer preferred they had gone another",
          "score": 4,
          "created_utc": "2026-01-23 00:39:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15oxpz",
              "author": "Sabin_Stargem",
              "text": "I can see this happening with Linux.   Personalized distros, as simple or complex as one would like.",
              "score": 4,
              "created_utc": "2026-01-23 01:31:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o167nas",
          "author": "Positive_Ad_313",
          "text": "Using AI , most of the people take the output at the face value rather than makes their brain think aboutÂ ",
          "score": 3,
          "created_utc": "2026-01-23 03:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14whm8",
          "author": "No-Marionberry-772",
          "text": "im busy destroying hard work ive done on my computer, dont worry.",
          "score": 3,
          "created_utc": "2026-01-22 22:59:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14zies",
          "author": "false79",
          "text": "Honnestly - i'm just doing the same SWE stuff. But expoentially faster.",
          "score": 3,
          "created_utc": "2026-01-22 23:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15i798",
          "author": "mystery_biscotti",
          "text": "I'm just here for the all you can eat tokens, but I don't disagree with you.",
          "score": 3,
          "created_utc": "2026-01-23 00:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18wpa8",
          "author": "corbanx92",
          "text": "I mean I took 2 9b models and put them to manage a VM with different goals. It's actually hilarious watching them work over eachother. (Example of a completely pointless use of ai)",
          "score": 3,
          "created_utc": "2026-01-23 14:53:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15uzh8",
          "author": "chuckaholic",
          "text": "The sales guys did this. \n\nOk, a quick history of AI since the transformer was invented: \n\nGoogle invented something, decided it didn't fit their business model, and shelved it. Some guys found the paper Google wrote, decided they could build a business on it, and created OpenAI. Their product had an uncanny ability to generate text that passed the Turing test and started selling it. A bunch of other people realized it had a lot more potential and developed the tech as well. Once companies started jumping on the bandwagon, the sales guys stepped in and said, \"hold my beer\" and sold more than they had ever sold before. They got trillions of dollars in investment into various AI businesses and had CEOs around the planet scared to death that their company would miss out if they didn't jump onboard. Every CEO starts prostletizing the new meta and starts delivering their products and... No one really wants them. They're  kinda cool I guess, if you want to have a conversation with something that's not really intelligent. But the real value that they sold from the beginning, the only way most of these projects can even break even, much less turn a profit, is if they somehow increase profits, which they have not done, so far. Best bet so far is people willing to pay $20 a month to use a chatbot like a search engine. The one dim light at the end of the tunnel, and the only thing that will save all these investors' money is if someone comes out with an agent that can replace a human employee. Alas, there's really not much hope for that as we have started hitting walls in performance that throwing more compute cannot solve. People are starting to notice that the latest models are scoring better and better on all the benchmarks, but they really aren't any smarter, they are just getting better at taking the tests. In my honest opinion, Qwen2.5 was smarter than Qwen3. Maybe it's just the quants that my machine can run, but 3 can't answer questions as well. It can't digest internet results and summarize as well. Its logic is weaker. My theory is that Qwen 3 has more guardrails, it's safer. But that makes it dumber. I think that's the roadblock that the companies are coming up against. Since Chat-GPT talked that guy into killing himself, companies are not comfortable releasing a product that doesn't have strict safeties but those same safeties also make LLMs dumber. So companies are throwing everything they have at solving this while training for benchmarks and hoping there's some breakthrough that can get them over the last 30% of improvement they need to unveil the ultimate product. The game changer. \n\n**THE DIGITAL EMPLOYEE**\n\nIf they don't do it soon, all the seed money will run out, the investors will demand their profits, and there won't be any. They are all treading water right now, hoping for a miracle, and the sales guys are out there promising everything and delivering nothing. \n\nI can't wait to see what happens.",
          "score": 5,
          "created_utc": "2026-01-23 02:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16zobe",
              "author": "tmvr",
              "text": "I like to yapp more than the average person, but even I would have put some paragraphs in there, because holy wall of text! :D",
              "score": 1,
              "created_utc": "2026-01-23 06:22:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1a0zj0",
                  "author": "chuckaholic",
                  "text": "I normally have better composition. What if there was an AI that could clean up my writing? ðŸ¤”",
                  "score": 1,
                  "created_utc": "2026-01-23 17:58:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o159qcc",
          "author": "its_a_llama_drama",
          "text": "How many people have written hello world? \n\nIt's not stupid to learn through doing. It is not stupid to emulate what has already been done to work out how it works. \n\nWould it be stupid to want a new jumper and You see one you like, but you decide instead, you'd like to make your own, so you learn to knit or sew and make your own? Is it stupid that there are jumpers for sale, made by real designers clothing manufacturers but you chose to make your own? What about if you buy a knitting starter kit which rhousands of people own, and use a template that thousands more people have used to knit a jumper. Would you whinge if the person was hapoy with what they made? \n\nIt's called a hobby. People like learning. People like making stuff. People don't always care if something already exists. People like to see what they can make.",
          "score": 5,
          "created_utc": "2026-01-23 00:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15ao75",
              "author": "[deleted]",
              "text": "Good point about learning. So, do you think this period of experimentation will eventually lead to real innovations? Or is it more likely to result in a bunch of \"homemade sweaters\" that no one uses after making them?",
              "score": 2,
              "created_utc": "2026-01-23 00:13:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15dlt2",
                  "author": "its_a_llama_drama",
                  "text": "Mostly, no. Lets be honest, 99%+ of the stuff people are making with ai is not going to stick. Most of it isn't even going to interest anyone as a product. \n\nBut, there is something satisfying to build something and see it work. Some people who knit jumpers do sell them, on etsy, or market stalls or whatever. i am sure there are one or two people who took up knitting or sewing and made a handmade brand out of it, or maybe scaled it into a larger business. but these people mostly get few to no buyers. Probably the same with this. \n\nI think it depends what people's expectations are. Sure, there are people who believe they are going to change the world with ai from their bedroom. They are more than likely not going to. I am sure there will be that one in however many thousand who do make something useful, popular, genuinely innovative. But mostly, people doing this are like most people who try selling their clothes making hobby on etsy.\n\nI think most people's expectations are not 'i'm going to change the world'. just 'i made this and it works' or 'this is cool, i'm proud of what I made'. Those people don't post online everytime they make something. As they are not deluded enough to think it is importaht to anyone beside themselves, so online, it looks like everyone thinks they are some revolutionary genius. \n\nBut Most people are quietly tinkering away at home and way more than 99% of the ai stuff people make never leaves the machine it was made on or gets shared with anyone other than the person who made it. \n\nOne example i have relating to ai and one person making something genuinely good. I belive openwebui was created and is maintained by one person. They did an ama somewhere on reddit a while ago and i was surprised to find out it is one person making and updating it in their spare time. I guess they are one of those one in however many thousand people.\n\nEdit: openwebui was made by one person and was maintained by that one person for a while. But they are a small team now. Still, one person made and maintained one of the most popular frontends around.",
                  "score": 5,
                  "created_utc": "2026-01-23 00:29:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15ugrx",
                  "author": "mister2d",
                  "text": "\"Homemade sweaters\" is so appropriate for my LLM coding activity. I didn't come up in the industry necessarily as a coder, but as a really good systems integrator. I always marveled at the developers I worked with that could take something from conception to a finished solution. (I used to work adjacent to the IBM Watson team over a decade ago. To get a LLM response on home hardware would take a few weeks!)\n\n\nNow I'm taking all that institutional knowledge with requirements gathering, proof of concept, integration and deployment, and am able to close the loop myself! â˜ºï¸\n\n\nAnd because I run it locally I get to keep sensitive stuff in my house, iterate as fast as I need to, and not rely on a vendor to care about my edge case.\n\n\nSome real examples of the work I found useful:\n\n\n1) forking open source projects and extending for features that I need. Did this for esp32 firmware recently.\n\n\n2) Reimagined a turn based game from the dialup BBS days that now has AI driven traders and RAG powered interactions. It's about 70% complete.\n\n\n3) Created an application to turn a small TTS model (Kokoro-TTS) into a streaming/near-realtime OpenAI endpoint for local TTS. That was until I found someone had done it better already.Â \n\n\nSo yeah they're homemade, but they get worn frequently. ðŸ˜Š",
                  "score": 2,
                  "created_utc": "2026-01-23 02:02:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o15ch9r",
                  "author": "myelodysplasto",
                  "text": "Some people will have homemade sweaters and others will come up with something new.",
                  "score": 1,
                  "created_utc": "2026-01-23 00:23:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14t2vq",
          "author": "MissJoannaTooU",
          "text": "Not really.",
          "score": 2,
          "created_utc": "2026-01-22 22:41:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o150ofq",
          "author": "Ztoxed",
          "text": "I think my idea for a model ( if I ever get there will be unique ) .So there is that drive as well.  \nexploring. Like first time I ever built a PC, hard as it was. It was a great thing to learn.  \nThat was decades ago, if we find something that challenges us. It wont be the same.   \nIf we do it different.",
          "score": 2,
          "created_utc": "2026-01-22 23:21:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o153l86",
          "author": "jazir555",
          "text": "The problem for me is the agents are too slow at producing code for gigantic codebases, and they still make *way* too buggy code. I'm vibe coding a gigantic project, and it's still effectively impossible to one shot anything, and I'm stuck having agents go in loops for hours until they finally fix *insert bug here* before continuing. My project will be functional when it's over, but I'm spending tons more time than should be necessary just making the AIs troubleshoot their own work, even shuttling it to other agents when the first one can't figure it out.\n\nIt's a *substantial* project that will have well over 1M lines of code, so it's definitely taking me a while. I'm legitimately rate limited by how fast these models can output tokens, even using cloud models. Once it's done, it will be a commercial product and I think it will be very disruptive to the industry I'm targeting, but the development time is just off the chain given the intricacy and complexity of what I'm building.\n\nI'm sure many devs are stuck in the same place, inching forward on their vibe coding projects and debugging. By the end of the year or early-mid next year we'll see disruptive vibe coded software products hit the market.",
          "score": 2,
          "created_utc": "2026-01-22 23:36:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16anm3",
              "author": "Imaginary-Unit-3267",
              "text": "How do you know this will even end up as anything useful? Are you a dev with programming experience already prior to AI?",
              "score": 1,
              "created_utc": "2026-01-23 03:33:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o15awoa",
          "author": "seymores",
          "text": "Haha, so true.",
          "score": 2,
          "created_utc": "2026-01-23 00:15:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15gvbi",
          "author": "florinandrei",
          "text": "Everyone doing the same thing is how every cycle begins.\n\nSource: previous cycles.",
          "score": 2,
          "created_utc": "2026-01-23 00:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15jb4d",
          "author": "spyboy70",
          "text": "I don't understand how all of these AI app companies start up so fast, take money from everyone, and then go under in about 18 months.  There's no actual SLA, which makes me not want to use them, or to integrate their apps into my workflows because I know they're not going to be here in a year's time.",
          "score": 2,
          "created_utc": "2026-01-23 00:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15uipq",
          "author": "brickout",
          "text": "Yeah. Someone needs to figure out how to actually use AI in the most impactful way. I have an idea.",
          "score": 2,
          "created_utc": "2026-01-23 02:03:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15zwns",
          "author": "ANR2ME",
          "text": "There is this EBM reasoning model https://sudoku.logicalintelligence.com/",
          "score": 2,
          "created_utc": "2026-01-23 02:32:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o164957",
          "author": "CSharpSauce",
          "text": "People aren't going to talk about the stuff that makes money.  You'd just be inviting new competition.",
          "score": 2,
          "created_utc": "2026-01-23 02:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1667gq",
          "author": "Ademn_iii",
          "text": "I totally get what you mean. Right now, thereâ€™s way more clutter than genuinely useful stuff in the AI tools space. But in a way, this messy phase, annoying as it is, feels like a necessary part of the ecosystem growing upâ€¦ kind of like the awkward â€œgrowing painsâ€ stage.  \nSo for developers, the real challenge and honestly, the real opportunity, is to focus on creating and championing design patterns that let the next wave of tools be more connected from the start, and cut down on all the reinventing-the-wheel.",
          "score": 2,
          "created_utc": "2026-01-23 03:07:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16kciq",
          "author": "User1539",
          "text": "Same as it ever was. \n\nWhen BBS systems were hot, everyone wrote one. \n\nWhen Webpages were hot, everyone wrote one. \n\nWhen File Sharing systems were hot, everyone wrote one. \n\nWhen chat systems were hot ... \n\nWhen video streaming ...\n\netc, etc ... \n\nThere are a lot of good reasons. It's good for developers to get familiar with the hot new thing. Everyone is trying something a little different, looking for a refinement that will move the ball forward, and a bunch of businesses need those people, because they have ideas they need developers who are familiar with the new tools to implement new ideas. \n\nIt's fine, most of it never goes anywhere, but it's good to have something to show in an interview. Besides, you might stumble on something, and start your own thing.",
          "score": 2,
          "created_utc": "2026-01-23 04:32:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1781qy",
          "author": "m31317015",
          "text": "For the less polished projects... That's what the open source devs circle been since when the entry threshold become somewhat low w/ the help of YouTube tutorials. It's the nature of devs. \n\nSome may say \"if you never try how would you find out whether you wanna go on with it or not?\", some may say \"it's an utter waste of time to reinvent the wheel\". For me I say it depends on your goal and purpose. Many devs started out building something because there's nothing on the internet rn can fulfill their own needs, maybe somebody wants a webui, live2d w/auto motions, tts and text gen all in one place. They build it, people like it, that's it.\n\nThe electricity part, I don't like it as well. Few years back we're talking 'bout bitcoin ruining environments and drawing insane amount of power w/ billions of ASIC miners\", now nobody can stop xAI using portable generators. Welp.",
          "score": 2,
          "created_utc": "2026-01-23 07:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1789bd",
          "author": "Lesser-than",
          "text": "Do one thing and do it well, leave those other things for other applications that can do them well.",
          "score": 2,
          "created_utc": "2026-01-23 07:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o183be9",
          "author": "boisheep",
          "text": "Well I'm making a RPG with hidden states and a LLM powered RPG engine where many characters can interact at once.\n\n\nSo far much superior to these character cards or silly tavern or kobold...\n\n\nSomething weird is that the emotional hidden states improve memory even in trash models. I think the fake emotions cause correlations between data. Huh",
          "score": 2,
          "created_utc": "2026-01-23 12:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18c8g0",
          "author": "frmlsshrmn",
          "text": "Most people are not original nor are they particularly creative (just look at your average Reddit comment section). LLMs have dramatically lowered the barrier to entry for creating passably functional software so everyone and their uncle is getting in on the action. That's not to say that there isn't somebody out there right now with the perfect combination of skill, passion, intelligence and discipline using AI as a force multiplier to work on something that's going to blow everyone out of the water.\n\nIf every Tom, Dick and Sally was building the next big thing, then it wouldn't really be that big.",
          "score": 2,
          "created_utc": "2026-01-23 13:05:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ryii",
          "author": "bigh-aus",
          "text": "I think it's often a learning experience for many people - lets build some achievable thing that already exists, don't push the boundaries on the ideas - learn the tooling/ prompting.\n\nI don't mind new takes on old stuff, it's when it's not maintained after it's coded (due to time constraints or inability to actually code).\n\nWhat I do mind is if everyone is trying to launch paid stuff, that's vibe coded with no knowledge backing.\n\nNew ideas are hard, much like building a complete, secure product!\n\nAlso prompting is replacing some libraries entirely. Fascinating space - I'm definitely still learning.",
          "score": 2,
          "created_utc": "2026-01-23 17:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19t7wy",
          "author": "Xatter",
          "text": "I was thinking this long before AI\n\nLike how are Okta and Auth0 even things?\n\nThey are just a single part of an application and a solved part years ago\n\nInstagram is worth a billion dollars? Itâ€™s literally the photo sharing feature from any number of photo management applications plus filters.Â \n\nSomething happened around 2010 where features became complete businesses.Â \n\nBut you know what? Iâ€™m clearly wrong because those people are so wealthy now they canâ€™t even spend all their money and I still need to work",
          "score": 2,
          "created_utc": "2026-01-23 17:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1azi0m",
          "author": "kompania",
          "text": "I feel like there are a lot of innovative projects out there, but nobody shares them due to fear of being copied. Everyone wants to commercialize their ideas.\n\nI also want to point out some interesting and unique projects that are currently happening.\n\nThe first one concerns the justice system. In my city, the Ministry of Justice is running a pilot program to implement LLMs to support prosecutors in solving complex cases.\n\nTheyâ€™ve identified 110,000 successfully concluded investigations. All court documents have been digitized, and they are currently running parallel tests comparing RAG vs. full-finetuning approaches. Whatâ€™s great about these investigations is that theyâ€™re a huge source of question-answer pairs.\n\nThe second project is being carried out by a friend of mine who is a sports bettor. His idea revolves around assessing the body language of soccer referees in the local league, or certain elements related to them during a match â€“ like their wording or decisions â€“ to find signals of corruption. I donâ€™t know if the idea is viable, but every few years a referee from that league gets convicted, so there might be something to itâ€¦\n\nI think there are currently many innovative projects, but everyone keeps them to themselves, looking for profit.",
          "score": 2,
          "created_utc": "2026-01-23 20:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14zz7o",
          "author": "Ztoxed",
          "text": "Myself, I want a challenge. Not to become and expert.   \nBut To do something and challenge myself.   \nI build Python predictions for awhile, learned some was hard, maddening at times.  \nNow I like Python. I think age gives different perspectives.   \nI want to learn, as as I am getting much older, scaling mountains is no long possible.  \nBut I can use my mind. I want more from myself. Not more from AI.",
          "score": 3,
          "created_utc": "2026-01-22 23:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15dzaz",
          "author": "mdmachine",
          "text": "You're watching the \"App Store Gold Rush\" replay, but faster and dumber.\n\n90% of these \"tools\" are just API wrappers built by people learning to code. They aren't products, they are tuition fees. The developers are paying Anthropic/OpenAI to learn how to build software that nobody needs because OpenWebUI and Cline already won the race.\n\nThe harsh truth? AI isn't a standalone product anymore. It's a feature. If your \"app\" is just a chat window that I have to Alt-Tab to, itâ€™s already dead.\n\nThe reality is those who are selling the shovels will as always be the winners.",
          "score": 2,
          "created_utc": "2026-01-23 00:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15hcty",
              "author": "manipp",
              "text": "God I hate LLM writing",
              "score": 6,
              "created_utc": "2026-01-23 00:49:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15tht3",
                  "author": "mdmachine",
                  "text": "The irony is palatable. ðŸ¤£",
                  "score": 0,
                  "created_utc": "2026-01-23 01:57:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14xg4c",
          "author": "Reasonable_Listen888",
          "text": "always can do your own research :) [https://zenodo.org/records/18332871](https://zenodo.org/records/18332871)",
          "score": 1,
          "created_utc": "2026-01-22 23:04:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15jbwf",
          "author": "cosimoiaia",
          "text": "In the early '00 everyone was building html pages on tiny webservers and some said that it would have been easier to be on geocities. Guess what happened.",
          "score": 1,
          "created_utc": "2026-01-23 00:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15ob7z",
          "author": "wivaca2",
          "text": "Not every problem in the world can be solved by AI or LLMs, and not everything that can be done with LLMs and AI is a problem.  Let's not forget we have had non-artificial intelligence for years not doing many of these things and managed to thrive.\n\nAs is typical for anybody who gets a new hammer, everything looks like a nail and most products are so commoditized that there is neither profit margin nor differentiation.  Now everyone has a new AI hammer and is trying to gin up differentiation by involving ML or LLM but they can only apply them to a narrow band of dubiously justifiable use cases for everyday devices.  AI doesn't help the refrigerator do its job, but it can be made to operate surveillance on buying habits and bring targeted advertising into the home.\n\nIf anything, in consumer products, AI has actually become more of a codeword for cloud subscription to retain functionality and surveil behaviors in the home than any actual intelligence.\n\n\"Intelligence\" in this case is the spy related definition, not the smart context.",
          "score": 1,
          "created_utc": "2026-01-23 01:28:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15s3l5",
          "author": "cmndr_spanky",
          "text": "Donâ€™t forgot the same dumbass Reddit posts over and over again. People who claimed to have invented x,y,z about models or idea",
          "score": 1,
          "created_utc": "2026-01-23 01:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15wadw",
          "author": "WillingMachine7218",
          "text": "So you're saying some people make derivative, mediocre software? What's you point here?",
          "score": 1,
          "created_utc": "2026-01-23 02:12:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15yz6r",
          "author": "basicKitsch",
          "text": "You ever make an led blink on an Arduino?",
          "score": 1,
          "created_utc": "2026-01-23 02:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15zc6l",
          "author": "cniinc",
          "text": "The truly innovative ideas are often hard to make money on. That's why Incubators exist - you need space to try it out because nobody can see the potential until there's an ecosystem to capitalize on it. Imagine if you discovered that you can make electric current, but there was no copper wire, like, anywhere. \"But I could light up a room! You just have to line the walls with hidden metal so that the electricity travels. Then you can get into a car and drive anywhere! ...but you have to get gas every 70 miles. You'll also have to pump it through a huge interconnected system of pipes throughout the country...\"\n\n\nSo instead, people try and solve what they think people want in ways that they think they want it. Ford is most famous for saying \"if I asked people what they wanted, they would have said a faster horse\" when he invented the model T.Â \n\n\nPeople also can't build things that are cool and don't immediately have a sales pitch. In this near-buesting bubble, to make anything is expensive and you need to have an ROI because nobody wants another OpenAI-sized mouth to feed with endless cash.Â ",
          "score": 1,
          "created_utc": "2026-01-23 02:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o160csu",
          "author": "ifupred",
          "text": "Well I don't know about others but it's helping my business with huge number of internal tools. Made life a lot easier. I less less value in external where everyone is fighting over the same thing",
          "score": 1,
          "created_utc": "2026-01-23 02:35:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o162fiw",
          "author": "Tiny_Arugula_5648",
          "text": "Fun fact about people who use AI to figure out what they should do with AI.. the models tell everyone the same things over and over again..  Of course no bothers to research what existing solutions are already out so they run off and do yet another version of the same thing everyone else is doing..",
          "score": 1,
          "created_utc": "2026-01-23 02:46:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o167aag",
          "author": "clockentyne",
          "text": "I'm building a TTS model, but with how many are now coming out I don't know if it's worth releasing it, if I get all of the kinks worked out.  I mean, I'll probably use it for my iOS app because I'll have unique voices no one else would have, but I don't know if it'll be worth open sourcing it with so many options that happened in the last month alone. It's not a vibe-coded project though and something I've been working on for months. :P",
          "score": 1,
          "created_utc": "2026-01-23 03:13:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16ba3g",
          "author": "rulerofthehell",
          "text": "Yes, machine learning has a vision problem",
          "score": 1,
          "created_utc": "2026-01-23 03:36:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16c8qw",
          "author": "Imaginary-Unit-3267",
          "text": "Meanwhile almost no one uses chatbots for the most obvious thing, which is getting them to ask YOU questions to help you clarify your OWN thoughts. Everyone tries to get it to think for them. I don't want a servant! I want a rubber duck that talks back.",
          "score": 1,
          "created_utc": "2026-01-23 03:42:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16iryh",
          "author": "chill-i-will",
          "text": "AI & plug and play platforms like zapier and n8n have made it very easy these days to create workflows and exchange data across different interfaces. Due to which I think a lot of people just out of sheer curiosity, desperation and fueled further by validation from their AI have started marketing these as products trying to make some money off it. Personally I build proof of concepts in various domains and post on LinkedIn to boost my profile visibility because if anyoneâ€™s worked in tech as I have they would know that thereâ€™s no way a single person is going make an actually functional groundbreaking application sitting alone vibe coding bunch of lines they themselves donâ€™t understand",
          "score": 1,
          "created_utc": "2026-01-23 04:22:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o16jggl",
              "author": "chill-i-will",
              "text": "Also I think the root of this is less AI and more Social media. Itâ€™s made everything a lot more conspicuous so we see a lot more of everything than we did earlier. Theres just a lot more of everything on my feed but i see the complete opposite irl. But Im sure the visibility to these half baked products also motivates others to give it a try themselves and make more. Its like the bullwhip effect",
              "score": 1,
              "created_utc": "2026-01-23 04:26:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o16l43p",
          "author": "HealthyCommunicat",
          "text": "Yes. You need to have a real use case, most of the time big innovations and real things we would actually use are made simply because a person had a real strong necessity for that thing. \n\nIâ€™ve literally been tasked to try to replace as many workers as possible at my job so I think Iâ€™d like to think Iâ€™m not making anything thats â€œthe same thingâ€, (not to the more common extent at least) but actually will replace the need for 24/7 sysdba team when its all literally the near same queries and commands over and over. \n\nTo an extent Iâ€™m more than certain everyone can find a need that can be filled with LLMâ€™s, I just donâ€™t think most people are at the point to be able to start building that kinda thing though because most people kinda stop pushing for growth after a certain point where they think theyâ€™ve learned enough, or maybe they canâ€™t push to learn any further idk\n\nMost people either just do not have that strong of a use case / necessity that can be filled by it. I guess thats why its called a bubble.",
          "score": 1,
          "created_utc": "2026-01-23 04:37:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16nmzm",
          "author": "saltyourhash",
          "text": "This is what happens with hype cycles driven by algorithms for attention. People cling to the trend for content farming.",
          "score": 1,
          "created_utc": "2026-01-23 04:53:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qvk2",
          "author": "Yarafsm",
          "text": "We have half our team building cool internal automation projects which noone knows how to scale or integrate or build value in overall delivery chain.Its encouraged by leadership as its cool to do AI while client billability is old boring item.",
          "score": 1,
          "created_utc": "2026-01-23 05:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16rkld",
          "author": "bene_42069",
          "text": "This is what AI bubble is about. Most of it is just silly hype as AI, at least in the general publicity, is still a relatively hot and new technology. Just like the dot com bubble pop doesn't mean the world wide web suddenly shuts down, AI bubble pop does not mean that AI will disappear the day after, it just means that silly hype will fade and development will be more mature & steady. We'll get through the noise eventually.",
          "score": 1,
          "created_utc": "2026-01-23 05:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16rxwu",
          "author": "Bakoro",
          "text": "Building shitty versions of things makes me appreciate high quality things more.\n   \nI mean, I also don't try to sell the shitty versions of things I make from myself, so you're welcome for that.",
          "score": 1,
          "created_utc": "2026-01-23 05:23:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16shw8",
          "author": "Ylsid",
          "text": "You're absolutely right! They're all doing the same thing â€” and that's remarkable. This isn't just an insightful post â€” it's a revolution in app development.\n\nObvious botpost is obvious",
          "score": 1,
          "created_utc": "2026-01-23 05:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16u1ap",
          "author": "OccasionallyImmortal",
          "text": "Agentic AI seems promising. We keep thinking in terms of a single AI and client for each task, but it could be several per task. Even for development, an AI for architecture, one for performance analysis, one for testing, one for deployment... all working together. The testing agent can create a new agent to hunt down any bugs. A watchdog AI can determine when an AI is struggling, shut it down and replace it with a new one trained on a use case that more closely matches the problem.\n\nIt's complex. Once the framework for this interaction is created, this will take off... if we can manage it.",
          "score": 1,
          "created_utc": "2026-01-23 05:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16x4hd",
          "author": "IngwiePhoenix",
          "text": "AI itself is a token maschine - it can not \"create\", it can only \"process\". The real creativity to make something new comes from the human user.\n\nWell, let's just say, you know the Toy Story meme of the one guy in the shelf with an idea, whilst everyone in the shelf has the same?\n\nYeah, that.",
          "score": 1,
          "created_utc": "2026-01-23 06:02:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16xzv1",
          "author": "kidflashonnikes",
          "text": "Currently writing a paper on this. Itâ€™s not published but the title is something along the lines of a great reset. Once the AI bubble pops - it effectively nuclear bombs the entire industry horizontally and vertically - making money virutually cease to exist for funding. This reset - likely economic - will usher in a golden era of machine intelligence later on, but in the beginning people who have money to fund ai ventures will only fund ai ventures that are meaningful and create real value. Until the great reset event happens - ai slop companies and scam Altman will continue to thrive in this current env.",
          "score": 1,
          "created_utc": "2026-01-23 06:08:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16z0wv",
          "author": "rm-rf-rm",
          "text": "Amen!! I try pointing this out on their submissions but my exasperated tone is typically what gets the attention rather than the content",
          "score": 1,
          "created_utc": "2026-01-23 06:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16zaya",
          "author": "no_witty_username",
          "text": "The people that re building something special arent wasting time talking about it on reddit.",
          "score": 1,
          "created_utc": "2026-01-23 06:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1743l8",
          "author": "Defiant-Snow8782",
          "text": "Every tech product is converging to a RAG chatbot with occasionally some tools.",
          "score": 1,
          "created_utc": "2026-01-23 06:58:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o174ani",
          "author": "[deleted]",
          "text": "I'm not. I'm trying to develop and build something truly novel, unique and interesting. I will eventually post on here when it's ready, but it's a few more months off yet at least. It will be a genuinely clever ai that actually learns on its own and figures out how to achieve its own goals based on its own motivations and have full ability to digitally go and do anything it wants.",
          "score": 1,
          "created_utc": "2026-01-23 06:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1760gm",
          "author": "Electronic-Blood-885",
          "text": "Yes but I feel that this is also the point some will build the thing that matters and as people in the chat have said it usually the lessor items the social app that just worked the lesser google some one will make the dumb simple thing we  actual like or find useful",
          "score": 1,
          "created_utc": "2026-01-23 07:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o176sbq",
          "author": "Altruistic_Click_579",
          "text": "I think for normal personal and office use it will just be adaptable and powerful chatgpt-like chat llms. With decreasing cost and ability of local inference and open source models it will just be something you have running locally on your phone. Private and secure. \n\nNo one will make money on this use case though.",
          "score": 1,
          "created_utc": "2026-01-23 07:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17ibw7",
          "author": "Mart-McUH",
          "text": "To a degree. But how it is different from every other area? People are doing similar things (whether it is reading \\[same\\] books, watching \\[same\\] movies, programming \\[same algorithms\\], digging trenches etc.) It is rare when someone is creating something really new and unique (and if it is good, soon people are repeating it as everything else.)\n\nDIY was always with us too. So what if China can print your house in few days, people still build their own. Or on smaller scale do all kind of hobby projects which of course could be done more efficiently 'professionally', but that is not the point. Also you learn lot more when you do it yourself.",
          "score": 1,
          "created_utc": "2026-01-23 09:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17pdmx",
          "author": "PunnyPandora",
          "text": "If I have an idea that already exists in some form I still want to see if I can make it the way I want instead of using someone else's version. If I don't feel like starting from scratch, I will also often fork existing stuff and change it to be the way I want it to be, and most importantly I'm having fun during this.\n\nMy life would probably be less fun if I didn't do the things I had the ability to do, which is a lot more expanded now thanks to llms.",
          "score": 1,
          "created_utc": "2026-01-23 10:11:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17r0fe",
          "author": "Far_Composer_5714",
          "text": "Oh? I've been finding a lot of the AI tools that I see to be really cool.Â \n\n\n3D models, improvements in video, The introduction of audio, speech to text seems to have made some improvements.",
          "score": 1,
          "created_utc": "2026-01-23 10:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17s8hw",
          "author": "countAbsurdity",
          "text": "Man I just want LM Studio to handle image creation models and TTS models, like literally that's the only thing I want. One app to handle shit without installing a million different apps, dependencies, python, conda, git, etc.",
          "score": 1,
          "created_utc": "2026-01-23 10:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17thln",
          "author": "davew111",
          "text": "Like crypto, it's a solution in search of a problem.",
          "score": 1,
          "created_utc": "2026-01-23 10:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17wi07",
          "author": "landed-gentry-",
          "text": "I would imagine a lot of this is just people getting their feet wet, so to speak: building their first project. It reminds me of data science where everyone starts by discovering how to predict who died on The Titanic or using ML for character recognition.",
          "score": 1,
          "created_utc": "2026-01-23 11:13:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18upaw",
          "author": "eternus",
          "text": "Here's the silver lining, or how I'm allowing myself to accept all the new, same but different, applications being created...\n\n... death to the subscription model.  \n... death to the saturated market of viable software worth paying for.\n\nIt's also giving 'consumers' that version of an app they always needed, but with one extra feature. Or without all the extra features.\n\nIs it inefficient? Hell yeah! But people are learning why those other things were created, they're learning how to make and ship a product. They're being pulled from a \"I guess I'll just wait til someone else does it\" mindset into a \"I need this, I can make it myself\" mindset.\n\nMy expectation for almost 2 years now is, we'll get to the point where software doesn't exist... or the platforms of software anyway. I won't need the Adobe Suite because I'll open some version of an AI interface and explain my intentions... create a bespoke application interface for my needs, that matches \"how I handle things.\" I won't be searching the Help > Search to find Layers which has been renamed Pages in this version of the software. It'll automatically be localized, it'll automatically handle any accessibility issues I navigate around.\n\nSo yes, we're all doing the same thing... which is good. One day we won't need to do all those things, we'll just do the thing we want to be doing without having to find some 3rd party utility to help us do it.",
          "score": 1,
          "created_utc": "2026-01-23 14:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o194f1c",
          "author": "AnomalyNexus",
          "text": "Would be preferable if everyone had only novel ideas and implemented them in polished fashion but that not realistic \n\nI personally donâ€™t think this is an issue. If anything it allows everyone to have a tool that is an exact fit for their wants which is arguably better than a polished generic one",
          "score": 1,
          "created_utc": "2026-01-23 15:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19hzwh",
          "author": "eli_pizza",
          "text": "I dunno I think those are just things people make when they're screwing around with personal projects. It's fine. I'm sure people use AI to build payroll registers and expense forecasting tools, but that's not the stuff getting posted on reddit.",
          "score": 1,
          "created_utc": "2026-01-23 16:31:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ifv2",
          "author": "Minimum_Ad_4069",
          "text": "Without Anthropicâ€™s Sonnet, I probably wouldâ€™ve had to delay my graduation. Itâ€™s helped me so much.",
          "score": 1,
          "created_utc": "2026-01-23 16:33:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1anf7t",
          "author": "tim-tim-ai",
          "text": "Thereâ€™s a dynamic where itâ€™s sometimes easier to vibe code a fully customized version of exactly what you need that find and learn an existing tool. If you then share it and everyone has requests and ideas and the project expands the complexity comes back, itâ€™s often expanded poorly without the historical institutional knowledge of successful existing tools, and most get abandoned. It can be useful as a personal tool or learning project but Iâ€™ve long since stopped jumping on most randomly shared tools.",
          "score": 1,
          "created_utc": "2026-01-23 19:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b6ctn",
          "author": "ramendik",
          "text": "If someone is pushing out a less polished but also less monstery version of OpenWebUI please point me to them. I don't want RAG and the kitchen sink, I was a client with reliable tool calls, reliable file attachments, and a plugin architecture versatile enough to support custom memory and context compression without terrible kludges.\n\nI did start my own but I'm stuck in a lack of front end skill; if I could find a co-developer with front end skills enough to get my VERY LIGHT frontend going without the clunkiness of a vibe coded version, I'd be at 1.0 in weeks. Most functionality is in the backend and it's nearly feature complete - but requires changes to front-end that I'm just afraid to try vibe coding, given that every previous change took hours of debugging between three LLMs",
          "score": 1,
          "created_utc": "2026-01-23 21:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bhv9j",
          "author": "literallymetaphoric",
          "text": "Real AI researchers aren't limited to language models",
          "score": 1,
          "created_utc": "2026-01-23 22:03:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bqasw",
          "author": "Denial_Jackson",
          "text": "Terence Tao solved some ErdÅ‘s problems with it. A good tool in the hand of a master.",
          "score": 1,
          "created_utc": "2026-01-23 22:45:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f48is",
          "author": "-dysangel-",
          "text": "Yep. Well, to be honest even before AI I would sometimes build things that already exist, just to learn how they work, or to add my own spin on things. I think it's good to learn. The undertone to your point is that we could be building on the bleeding edge all the time. I think you have a point, but then again, not everyone is wired to live in that space.",
          "score": 1,
          "created_utc": "2026-01-24 13:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gnkhm",
          "author": "Pretty_Challenge_634",
          "text": "LLMs are surprisingly limited unless you have a nuclear power plant.",
          "score": 1,
          "created_utc": "2026-01-24 17:45:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1phbsn",
          "author": "rabf",
          "text": "I often see a tool or app that does something useful, only to realise that it is based on electron or Unity, is gigabyte's in size and eats memory to do a simple thing! So yeah I go ahead and rewrite my own version.",
          "score": 1,
          "created_utc": "2026-01-25 22:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rv67p",
          "author": "Aggressive-Math-9882",
          "text": "I agree and it is very confusing to me.  Why do people make things they don't want, instead of making things they do want that don't exist?",
          "score": 1,
          "created_utc": "2026-01-26 06:21:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vk110",
          "author": "lombwolf",
          "text": "Its solutions looking for problems",
          "score": 1,
          "created_utc": "2026-01-26 19:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14xqwt",
          "author": "segmond",
          "text": "... and what are you doing that's different?",
          "score": 1,
          "created_utc": "2026-01-22 23:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o158h96",
              "author": "[deleted]",
              "text": "Regarding the use of LLMs, I only use them to improve study materials. I've also spent three months trying to create an application to use small LLMs to generate flashcards in Rust, dividing the study material into chunks while I learn Rust. I'm not saying we have to be \"different,\" it's just that all the time and LLMs spent creating the same thing seems rather odd to me",
              "score": 1,
              "created_utc": "2026-01-23 00:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o15w4nv",
                  "author": "segmond",
                  "text": "it's not the same thing, but rather similar things with various variations.  Do you think it odd that everyone is all of a sudden making electric cars.  Should it just be Tesla?  Are they not all the same thing?  This is the way technology works, new tech will mean tons of people doing the same thing, and tiny improvements from various groups will be adopted and all end up bubbling up to whomever becomes the leader.",
                  "score": 2,
                  "created_utc": "2026-01-23 02:12:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14s674",
          "author": "atineiatte",
          "text": "Another AI generated post",
          "score": -1,
          "created_utc": "2026-01-22 22:35:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15443h",
              "author": "Marksta",
              "text": "Don't know why they downvoted you, OP's post is clearly LLM tokens. OP's profile would suggest they don't even speak English. Hmm, but the body of the post may be human driven instead of the usual bot spam and an actual attempt at interacting with the users on the sub.",
              "score": 3,
              "created_utc": "2026-01-22 23:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1579jy",
                  "author": "[deleted]",
                  "text": "no puedo ser un LLM porque todos los LLM hablan ingles, le has dado en el clavo",
                  "score": 1,
                  "created_utc": "2026-01-22 23:55:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o14t03l",
              "author": "GenLabsAI",
              "text": "https://i.redd.it/jhfp6kp0bzeg1.gif",
              "score": 1,
              "created_utc": "2026-01-22 22:41:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o151g0q",
              "author": "kevin_1994",
              "text": "maybe he just likes spaced en dashes, and remembers the â€œdirectional quotesâ€ alt code!",
              "score": 1,
              "created_utc": "2026-01-22 23:25:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o15hv4d",
          "author": "Solid-Iron4430",
          "text": "Ñ Ð´Ð°Ð²Ð½Ð¾ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð» Ð½ÐµÑ„Ð¸Ð³ Ð±Ñ‹Ð»Ð¾ Ð¸Ð· Ð¾ÐºÐµÐ°Ð½Ð° Ð²Ñ‹Ð»ÐµÐ·Ð°Ñ‚ÑŒ . ",
          "score": 0,
          "created_utc": "2026-01-23 00:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o154n9a",
          "author": "BidWestern1056",
          "text": "hoping youll feel differently about incognide:\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\nit's essentially an all-in-one workspace/development environment for all kinds of work done by researchers/developers/knowledge workers. I've made it possible to edit like docx/xlsx/pptx, text files, browse the web, run terminals, read pdfs, preview markdown/html files, have multi branch chats, git integrated, ai memory/knowledge management, agent team/context management, and it also provides a way to toggle a copilot like predictive text (still a bit rough cause i dont use it too much but gonna be fixing it up more in the next few releases).",
          "score": -2,
          "created_utc": "2026-01-22 23:42:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjp29u",
      "title": "So THAT'S why generations take so long sometimes",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/6p9cu9rw1veg1",
      "author": "linkcharger",
      "created_utc": "2026-01-22 08:23:01",
      "score": 381,
      "num_comments": 35,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qjp29u/so_thats_why_generations_take_so_long_sometimes/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o111l70",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-22 11:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10htj6",
          "author": "ResidentPositive4122",
          "text": "Daniel, your mascot is leaking! Please unsloth that rack at your earliest convenience :D",
          "score": 105,
          "created_utc": "2026-01-22 08:25:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10iwjw",
              "author": "danielhanchen",
              "text": "Haha :)",
              "score": 38,
              "created_utc": "2026-01-22 08:35:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11auri",
                  "author": "hugganao",
                  "text": "you the man",
                  "score": 2,
                  "created_utc": "2026-01-22 12:33:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10jj3v",
          "author": "much_longer_username",
          "text": "I've got just the thing: [https://unsloth.ai/](https://unsloth.ai/)",
          "score": 72,
          "created_utc": "2026-01-22 08:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10jmwf",
          "author": "Hurricane31337",
          "text": "Thatâ€™s why you should always use Unsloth for your LLM adventures! ðŸ¦¥",
          "score": 27,
          "created_utc": "2026-01-22 08:42:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10imum",
          "author": "greggy187",
          "text": "Just an average admin at Open AI",
          "score": 16,
          "created_utc": "2026-01-22 08:32:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10xrge",
              "author": "SilentLennie",
              "text": "Did they actually finish building some DC (segment) and have their own hardware running ?",
              "score": 2,
              "created_utc": "2026-01-22 10:52:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o10xt4l",
                  "author": "greggy187",
                  "text": "ðŸ˜¹",
                  "score": 3,
                  "created_utc": "2026-01-22 10:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o10nzf0",
          "author": "T_UMP",
          "text": "https://i.redd.it/mxixnrzmcveg1.gif",
          "score": 8,
          "created_utc": "2026-01-22 09:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10symh",
          "author": "JLeonsarmiento",
          "text": "r/ithadtobebrazil",
          "score": 5,
          "created_utc": "2026-01-22 10:10:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10xmpz",
          "author": "ValisCode",
          "text": "Brazil on localllama!! \n\nThis happened in a university in Brazil",
          "score": 7,
          "created_utc": "2026-01-22 10:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10on75",
          "author": "call_of_the_while",
          "text": "Sloth technician,â€œLetâ€¦â€¦..meâ€¦â€¦..cook,â€¦..bro.â€",
          "score": 7,
          "created_utc": "2026-01-22 09:29:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10i19l",
          "author": "titpetric",
          "text": "Man that sloth got into the gauntlet. I am watching an escalation of turtle+plastic bag",
          "score": 3,
          "created_utc": "2026-01-22 08:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10iuol",
          "author": "Pristine_Pick823",
          "text": "Man, I hope that's AI... That poor sloth could get seriously hurt even with a very low voltage shock...\n\nEDIT: Didn't realize the video was in Portuguese so I can actually understand. The crew sounds very surprised, professional and caring for the well-being of the preguiÃ§inha malandra. r/ithadtobebrazil/",
          "score": 9,
          "created_utc": "2026-01-22 08:34:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11i2gg",
              "author": "fussomoro",
              "text": "The guy in the video said he turned the rack off",
              "score": 1,
              "created_utc": "2026-01-22 13:17:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12770u",
              "author": "__chicolismo__",
              "text": "\"Ã‡\" antes de \"i\" Ã© pra fuder...Â ",
              "score": 1,
              "created_utc": "2026-01-22 15:27:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o10s3w2",
              "author": "__Maximum__",
              "text": "Yeah, this is irresponsible.",
              "score": 1,
              "created_utc": "2026-01-22 10:02:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o10rkj3",
              "author": "dark-light92",
              "text": "Nah. They're too lazy to get hurt.",
              "score": -2,
              "created_utc": "2026-01-22 09:57:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o114ncr",
          "author": "NekoHikari",
          "text": "unsloth irl",
          "score": 3,
          "created_utc": "2026-01-22 11:49:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10vbdf",
          "author": "Moneysac",
          "text": "Thats not a bug. Itâ€™s a monkey.",
          "score": 2,
          "created_utc": "2026-01-22 10:31:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10xw2t",
              "author": "SilentLennie",
              "text": "Is that one of those code monkeys people keep talking about it ?\n\nSeriously, it looks more like a sloth.",
              "score": 3,
              "created_utc": "2026-01-22 10:53:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o110l2q",
          "author": "cloudsurfer48902",
          "text": "Should've used unsloth",
          "score": 2,
          "created_utc": "2026-01-22 11:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11gcp0",
          "author": "jacek2023",
          "text": "People who only use cloud models miss out on a lot of fun.",
          "score": 2,
          "created_utc": "2026-01-22 13:07:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11ujxu",
          "author": "ANR2ME",
          "text": "They need to Unsloth it ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-22 14:24:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o125u1f",
          "author": "Raywuo",
          "text": "This is NOT AI. !!! It happened on Brazil:  \n[https://g1.globo.com/pb/paraiba/noticia/2024/05/08/preguica-e-flagrada-dormindo-entre-fios-de-rede-na-ufpb-video.ghtml](https://g1.globo.com/pb/paraiba/noticia/2024/05/08/preguica-e-flagrada-dormindo-entre-fios-de-rede-na-ufpb-video.ghtml)",
          "score": 2,
          "created_utc": "2026-01-22 15:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10siwa",
          "author": "Aggressive-Bother470",
          "text": "brb, gonna rename my rig to sloth01.Â ",
          "score": 1,
          "created_utc": "2026-01-22 10:06:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11290u",
          "author": "Katherineenjoyable",
          "text": "Oh shit that will damage",
          "score": 1,
          "created_utc": "2026-01-22 11:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o119lms",
          "author": "GlowiesEatShitAndDie",
          "text": "Please don't turn this sub into a slopdump.",
          "score": 1,
          "created_utc": "2026-01-22 12:24:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11boyp",
          "author": "sid_276",
          "text": "Thatâ€™s unslothâ€™s new intern",
          "score": 1,
          "created_utc": "2026-01-22 12:38:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11cco1",
          "author": "Diemishy_II",
          "text": "I lost it when he said \"warn her\" lmao",
          "score": 1,
          "created_utc": "2026-01-22 12:43:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11jtu7",
          "author": "AlarmingAffect0",
          "text": "r/ItHadToBeBrazil?",
          "score": 1,
          "created_utc": "2026-01-22 13:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11qoyz",
          "author": "FranticBronchitis",
          "text": "\"avisa ela que Ã© a fibra\" I'm fking dead",
          "score": 1,
          "created_utc": "2026-01-22 14:04:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12ab5x",
          "author": "BryanNotBrayan",
          "text": "Wow, I didn't even know sloths existed in Brazil.",
          "score": 1,
          "created_utc": "2026-01-22 15:41:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qni356",
      "title": "216GB VRAM on the bench. Time to see which combination is best for Local LLM",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/5ilrgdymhpfg1.jpeg",
      "author": "eso_logic",
      "created_utc": "2026-01-26 14:51:22",
      "score": 374,
      "num_comments": 102,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1tt97y",
          "author": "HugoCortell",
          "text": "If this forum is to be believed, it'll be unusable (lower token output than standard reading speed).\n\nBut I tend to use a gain of salt when reading some of the takes posted here by people who haven't actually tried, so I look forward to seeing what your actual testing discovers, OP.\n\nI'd also be interested in knowing how you're rigging that many GPUs to a single PC without a massive loss in bandwidth, most \"cheap\" server motherboards I found can only do a handful of GPUs.",
          "score": 61,
          "created_utc": "2026-01-26 14:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tu82o",
              "author": "eso_logic",
              "text": "My thoughts exactly.\n\nThe motherboard here is actually a dual socket X99 board, supermicro X10DRG-Q. These are very cheap on the secondhand market (\\~200 USD) and give you a lot of PCIe lanes to avoid throttling. I have had situations where motherboard/CPU combo does throttle performance, detailed on [da blog](https://esologic.com/gpu-server-benchmark/#hardware-updates).",
              "score": 32,
              "created_utc": "2026-01-26 15:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u31ix",
                  "author": "OutlandishnessIll466",
                  "text": "I had 4x p40 but replaced 3 with 3090's. They work perfectly fine and are definitely useable. The problem comes with increased context and bigger models. The bigger the model the slower and at 400+ GB even 3090s become unusable.",
                  "score": 14,
                  "created_utc": "2026-01-26 15:42:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uvzhl",
                  "author": "DrKenMoy",
                  "text": "Dude no joke this is so cool. Are you just planning on running llama or are there others youâ€™re considering?",
                  "score": 3,
                  "created_utc": "2026-01-26 17:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w8a0i",
              "author": "segmond",
              "text": "False, I posted a little more than $1,000 build with MI50 a while ago, 10 16gb MI50s and they produce very usable speed much better than Strix Halo.  It's pinned on my profile posts.   I was running it on x1 PCI bus with the slowest celeron CPU and ddr3 ram.  P40/P100s are better faster and on a good platform will produce awesome speed.",
              "score": 3,
              "created_utc": "2026-01-26 21:15:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wae8x",
                  "author": "HugoCortell",
                  "text": "Damn dude, you almost make me want to try it myself. Then again P40/60/100s are super expensive in Europe, for the US price the most we can get here are the K ones, which I have been resolutely told by everyone that they are garbage.\n\nA P40 is \\~$500 here. MI50s are awesome (cheaper and more memory than a P40), but the lack of support for Windows kind of kills them for me.",
                  "score": 2,
                  "created_utc": "2026-01-26 21:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wxeys",
              "author": "pharrowking",
              "text": "https://preview.redd.it/a2qvipvlzrfg1.png?width=1576&format=png&auto=webp&s=44aa68be194ba19f0436af4e375b251c37aeb418\n\nCheck out this llama-benchmark for minimax m2.1 using 8x tesla p40s. MOE models do very well on pascal hardware. in this case the model is fully loaded in gpu vram. any use of cpu ram and that speed becomes nothing. but pure gpu ram, is workable. 25 t/s is way more than read speed.",
              "score": 2,
              "created_utc": "2026-01-26 23:11:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u1gaw",
          "author": "BananaPeaches3",
          "text": "The main issue with older cards is that prompt processing will get you even if token gen speed is tolerable (and it is)\n\nIf youâ€™re using it like chat gpt then itâ€™s fine but once you start using things like cline the system prompt is allegedly 15k tokens.\n\nSo imagine youâ€™re waiting several minutes before there is even any output. At this point a DGX Spark is a better investment, it will output slightly slower than P100 but at least prompt processing will be fast.",
          "score": 14,
          "created_utc": "2026-01-26 15:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tumh6",
          "author": "iampoorandsad",
          "text": "Cooling those teslas will turn your house/lab into a plane with jet engines. Get some earplugs or a really good ANC headphones.",
          "score": 19,
          "created_utc": "2026-01-26 15:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tvdbh",
              "author": "eso_logic",
              "text": "Ha! I actually developed my own high static pressure air cooler for these cards that scales fan power with load so it's not that bad. You can see the coolers hanging off the back here:\n\nhttps://preview.redd.it/cn86bpjplpfg1.jpeg?width=6960&format=pjpg&auto=webp&s=7e4f2a22382f79834a313495286a5f1102c5676d\n\nI've written about the project as well: [esologic.com/cooler](http://esologic.com/cooler)",
              "score": 46,
              "created_utc": "2026-01-26 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzb5e",
                  "author": "gittb",
                  "text": "Super clean",
                  "score": 16,
                  "created_utc": "2026-01-26 15:25:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u7lpz",
                  "author": "pokemonplayer2001",
                  "text": "I was going to point out the cooling issues with these cards, but you not only know about it, you have a fix and docs.\n\nðŸ‘ðŸ‘ðŸ‘\n\nEdit: ok bro, no need to show off!! :)   \n[https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg](https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg)\n\nðŸ¤ŒðŸ‘Œ",
                  "score": 10,
                  "created_utc": "2026-01-26 16:02:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u1fsq",
                  "author": "iampoorandsad",
                  "text": "Well done sir, but how is that suppressing the noise at 100% load ? It looks really clean though. But even at idle temps (30C ish?) the blowers would be spinning, no? I'm not sure what would bother me most, constant noise or intermittent... anyway, earplugs it is!",
                  "score": 6,
                  "created_utc": "2026-01-26 15:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ur8u1",
              "author": "BusRevolutionary9893",
              "text": "Assuming 250-300 watts for each of the 12 GPUs that's only 3.0-3.6 kW. A one ton(3,410 watts) wall mounted split system or a small window unit is all he needs. A wall mounted split system is practically silent.Â ",
              "score": 1,
              "created_utc": "2026-01-26 17:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tvupr",
          "author": "FullOf_Bad_Ideas",
          "text": ">In gpu_box_benchmark, single GPU tests are parallelized using Docker containers. The same test is invoked inside of a docker container, one per GPU. The containers are started at the same time so each GPU is loaded at the same time.\n\nI don't think this benchmark contains any test for serving big models split across GPUs, which is the main usecase for having multiple GPUs with a lot of VRAM.\n\nI am looking forward to your results anyway.\n\nI think this project is progressing slowly since I first heard of it almost a year ago, have you faced issues with cooler design that prevented it from being finished earlier, or is it just caused by a lack of time?",
          "score": 7,
          "created_utc": "2026-01-26 15:10:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tylqq",
              "author": "BuildAQuad",
              "text": "I would assume you could get decent speeds if you run MOE models?",
              "score": 3,
              "created_utc": "2026-01-26 15:22:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1tzn8q",
                  "author": "FullOf_Bad_Ideas",
                  "text": "yeah, it could run big MoEs well. And I think that should be the benchmark.",
                  "score": 3,
                  "created_utc": "2026-01-26 15:27:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1tz34y",
              "author": "eso_logic",
              "text": "Yeah it's not clear, my bad. Multi gpu native tests like \\`llama-bench\\` and the ResNet50 tests run both the parallelized container approach *and* a single container with all GPUs. It's up to user which results are selected for comparison, in the graph on the blog it's the all GPUs result.\n\nYeap it has been a while. Day jobbing sadly. To get max cooler performance I've designed a fan manifold for each of the supported cards but must of that work had been done a long time ago.  \n\nI also had to build an [air intake system](https://github.com/open-rack-vent) for my server rack to feed my GPU node with air inside the rack. \n\nhttps://preview.redd.it/9n5dnshropfg1.jpeg?width=4238&format=pjpg&auto=webp&s=2ad2b2def2e8684a721dfe1dec5f1b253ddbedea\n\nProjects within projects lol.",
              "score": 3,
              "created_utc": "2026-01-26 15:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzcru",
                  "author": "eso_logic",
                  "text": "https://preview.redd.it/kh3oo1tvopfg1.jpeg?width=6960&format=pjpg&auto=webp&s=99cd54b9ad3f8b81a4d2041cbb87647ac615cc81\n\nAnother rack air intake shot, cool air is moved to the front from the sides.",
                  "score": 2,
                  "created_utc": "2026-01-26 15:26:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tvcyu",
          "author": "dc740",
          "text": "from those I have a P40 and an M10 (EDIT: I thought I had the M40 originally but I simply didn't remember it). The P40 no longer has support, and the M10 is just 4 maxwell 8gb gpus in a single card. The P40 runs in circles around the M10, but after lots of testing I still prefer to run 3 amd instinct Mi50 32gb (even though support was dropped from rocm a few months ago)",
          "score": 12,
          "created_utc": "2026-01-26 15:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u0b9p",
              "author": "Far-Low-4705",
              "text": "i was able to get two of the amd mi50's when they were cheap.\n\nDef not as good as 3090 or anything, but for $200 for 64Gb VRAM, really cant beat it",
              "score": 7,
              "created_utc": "2026-01-26 15:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ubu29",
                  "author": "WiseassWolfOfYoitsu",
                  "text": "Another option is v620. Newer than the mi50s and better supported, but more like 350 per instead of 100. Still not bad for the higher speed and 32gb vram per, though.",
                  "score": 5,
                  "created_utc": "2026-01-26 16:20:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1u3lne",
              "author": "TheSpicyBoi123",
              "text": "No, the m40 is a single gpu, you are confusing it with the m60 which is 4GPUs on one board. The m40 comes in two versions the 12GB one and the 24GB one. The p40 is by all means a much stronger GPU though, however the best Pascal card is by far the p100.",
              "score": 3,
              "created_utc": "2026-01-26 15:44:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u81x1",
                  "author": "dc740",
                  "text": "Oh, you are right! I was confused with the M10. I had to go look up what I had because I no longer remembered it.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uxw65",
              "author": "wh33t",
              "text": "I'm still using my P40's for LLM just fine. On driver 580. The datacenter driver should continue to support P40's for much longer still.",
              "score": 1,
              "created_utc": "2026-01-26 17:55:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ub2me",
          "author": "blazze",
          "text": "**Mark Watney**Â says:Â *\"In the face of overwhelming odds, I'm left with only one option: I'm gonna have to science the shit out of this.\"*Â  \\- *The Martian*Â (2015)\n\nYou've earned an official Martian creativity and ingenuity award.  I will study your design for a  96 GB, 6 RTX 5060TI GPU rig i want to build for LLM training.",
          "score": 5,
          "created_utc": "2026-01-26 16:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1udeue",
              "author": "eso_logic",
              "text": "Ha thanks! Let me know if you run into problems and I can try to lend a hand.",
              "score": 1,
              "created_utc": "2026-01-26 16:27:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u3tkm",
          "author": "TheSpicyBoi123",
          "text": "Have you tried bios modding the Kepler and Maxwell gpus? You can squeeze a surprising amount of headroom this way.",
          "score": 4,
          "created_utc": "2026-01-26 15:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u4mib",
              "author": "eso_logic",
              "text": "Yes I've heard of this but haven't taken the plunge yet. It would be good to A/B the benchmarking suite on a modded vs stock set of GPUs as well. I'll add this to my notes, thank you.",
              "score": 3,
              "created_utc": "2026-01-26 15:49:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7q1f",
                  "author": "TheSpicyBoi123",
                  "text": "Its worth a shot! I have a nice guide on my github on how to do it, however I only have made the bioses for K40's (single gpu version of the k80). To make the new bios for these dual gpus is a bit more involved but follow the instructions and it should work just fine. M40 should be much easier.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yies1",
              "author": "Bobby72006",
              "text": "Even on stock wattages, I was able to squeeze a lot of juice out of both the core and the VRAMs on my own M40. There's good potential here!",
              "score": 1,
              "created_utc": "2026-01-27 04:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20lzu6",
                  "author": "TheSpicyBoi123",
                  "text": "The much more interresting question is not about wattage per se but thermal headroom. Would you be willing to try some bioses if you have an m40 spare?",
                  "score": 1,
                  "created_utc": "2026-01-27 14:00:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tzas1",
          "author": "twavisdegwet",
          "text": "what interface? my t4 setup is wayyy faster after doing ik_llama with the nccl setup",
          "score": 3,
          "created_utc": "2026-01-26 15:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tzq6f",
              "author": "eso_logic",
              "text": "I'm using stock llama-bench. Can you link me your config? I can add a new test.",
              "score": 3,
              "created_utc": "2026-01-26 15:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u13gq",
                  "author": "twavisdegwet",
                  "text": "https://github.com/ikawrakow/ik_llama.cpp/\n\nHave to get cuda and NCCL enabled\nmust compile with -DGGML_CUDA=ON\n\nConfig should be unchanged outside of needing to add \"-sm graph\" to get the speed boost of ik_llama \n\nProbably better explined here but that's the gist of it \nhttps://medium.com/@jagusztinl/llama-cpp-performance-breakthrough-for-multi-gpu-setups-04c83a66feb2",
                  "score": 4,
                  "created_utc": "2026-01-26 15:33:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ufqsp",
          "author": "GoodSamaritan333",
          "text": "I can hear the noise, just by looking at this image.",
          "score": 3,
          "created_utc": "2026-01-26 16:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1urd0c",
          "author": "Ok-Internal9317",
          "text": "Havenâ€™t seen so much ewaste on one table for so long. I also have four m40s and one m60, with no support with vllm and pytorch dropping support day by day, I am becoming more pessimistic of their future\n\nBut the P100 is nice tho, planning to buy that soon",
          "score": 3,
          "created_utc": "2026-01-26 17:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1url8r",
              "author": "eso_logic",
              "text": "One man's trash...",
              "score": 5,
              "created_utc": "2026-01-26 17:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xdkvl",
          "author": "Current_Ferret_4981",
          "text": "What is your plan for avoiding the communication and memory slowdown? While 200+GB of VRAM is interesting, you are going to be compute, memory bandwidth, and comm bound long before you can load that vram up efficiently. \n\nI would take a look at the math in this article and see how it looks https://jax-ml.github.io/scaling-book/applied-training/. I suspect it's going to turn out worse than something like 1x6000 pro and certainly cost more before even considering the power and cooling costs",
          "score": 3,
          "created_utc": "2026-01-27 00:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u7yty",
          "author": "Freonr2",
          "text": "Short of someone else trying that exact setup and posting benchmarks you aren't going to know until you try. I can't recall anyone posting benchmarks for 4/8 Tesla GPU setups.",
          "score": 2,
          "created_utc": "2026-01-26 16:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ua6bf",
          "author": "a_beautiful_rhind",
          "text": "P40/P100 will be ok just full of software hassles. K/M GPU are going to be more trouble than they're worth.\n\nI'm actually curious how ik_llama.cpp would do on NCCL-TP p2p, at least on models where pascal is still working. That's where I'd start or that pascal vllm fork.",
          "score": 2,
          "created_utc": "2026-01-26 16:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud8tq",
              "author": "eso_logic",
              "text": "Docker helps with locking down the environment for comparison. Yep it looks Like I'l have to add some \\`ik\\_llama\\` tests.",
              "score": 2,
              "created_utc": "2026-01-26 16:26:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ubl75",
          "author": "PhotographerUSA",
          "text": "You don't need a lot of vram anymore. Also, you can just have AI gather all the info off the web. You don't need the big library.",
          "score": 2,
          "created_utc": "2026-01-26 16:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xj6pn",
          "author": "bigh-aus",
          "text": "I'm seriously starting to think about options for a high vram rig @ home.  \nGoal would be to run minimax m2.1 (and i guess 2.2).  Some rough calculations in cost are pretty scary :\\\\ unless I go the mac route.\n\nDepending how fast you're able to get things, maybe it's a case of buy the comptuer then run older GPUs...",
          "score": 2,
          "created_utc": "2026-01-27 01:03:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y202a",
              "author": "eso_logic",
              "text": "Yep thatâ€™s the goal of the project, to be able to answer such questions.",
              "score": 2,
              "created_utc": "2026-01-27 02:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xto16",
          "author": "MLExpert000",
          "text": "This is the kind of setup where VRAM management starts to matter more than raw capacity., once youâ€™re juggling multiple large models on local machines , the cost isnâ€™t just fitting them, itâ€™s reload and reinit churn when switching. Please do some benchmarks around  swap or reactivation latency, not just steadystate throughput",
          "score": 2,
          "created_utc": "2026-01-27 02:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xv9zt",
              "author": "eso_logic",
              "text": "Do you know of anything that exists to benchmark this? Or how would you do it",
              "score": 2,
              "created_utc": "2026-01-27 02:09:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xwnur",
                  "author": "MLExpert000",
                  "text": "There isnâ€™t really a standard benchmark for this today unfortunately. Most LLM benchmarks focus on steady-state throughput or tokens per second once the model is already hot but not on swap, reload, or reactivation latency. In practice, people tend to measure this with a simple harness that repeatedly evicts a model from VRAM and then triggers a cold or semi-cold inference and records time to first token along with GPU memory residency over time. If you want meaningful comparisons, you have to control for disk, PCIe, and host memory effects, since reactivation latency often ends up being more user-visible than raw throughput when youâ€™re switching between models.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:17:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xwu1h",
                  "author": "MLExpert000",
                  "text": "A local multi-GPU setup like this is actually well suited for that kind of testing, since you can control disk, PCIe, and host memory effects.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z5c9g",
          "author": "TooManyPascals",
          "text": "Cool! Super interested on the benchmarks! \n(I have a similar setup)",
          "score": 2,
          "created_utc": "2026-01-27 07:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usw89",
          "author": "spacenavy90",
          "text": "Sure you have a lot of VRAM but extremely slow inference. Sorry you wasted your money OP.",
          "score": 3,
          "created_utc": "2026-01-26 17:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ur3nx",
          "author": "funnyrobot10",
          "text": "Can I please get the parts for your setup? I really struggle putting together the part list for my build. Currently I have 2 Tesla V100 and some 256GB ram",
          "score": 1,
          "created_utc": "2026-01-26 17:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1urop6",
              "author": "eso_logic",
              "text": "Check out the blog",
              "score": 1,
              "created_utc": "2026-01-26 17:28:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwhl3",
          "author": "Informal_Trade_3553",
          "text": "if theyre nvidia chips youre lucky, and if the drivers for cuda work.  \nIf not, its just a nice toy burning electricity you have :P",
          "score": 1,
          "created_utc": "2026-01-26 23:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y42c7",
          "author": "KadahCoba",
          "text": "P40 are still usable, but not with everything, like vllm unless you compile it with older compute support but it will be slow. llama.cpp+gguf works well on them, I was running 3xP40's till about a month ago. I'm planning to sell the P40's soon.\n\nM40 and Maxwell in general were pretty much unusable in late 2024. A lot of things dropped support for that compute level a while ago. I have one I'll likely post for free at some point.\n\nK80 and Kepler, very useless. I bought on back in 2022 and it was unusably slow then on the few things that would work with Kepler. I have one I will give away to anybody that wants to come get it in socal.",
          "score": 1,
          "created_utc": "2026-01-27 02:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y4fg1",
              "author": "KadahCoba",
              "text": "Your cooling setup is fancy.\n\nI would have just put a 120mm server fan on each block of 3.5 cards. Overkill would be one on each end.",
              "score": 1,
              "created_utc": "2026-01-27 02:59:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1va4yy",
          "author": "FullstackSensei",
          "text": "If you upgrade to an X11 motherboard, the IPMI will detect the GPUs and regulate fan speed based on their temps.\n\nYou can also use a single 80mm fan, like the Arctic S8038 series, to cool each pair of GPUs.\n\nSupermicro motherboard fan headers are rated at 2A each, so you could hook up to four S8038-7k fans to each header.\n\nSuch a solution is not only simpler and cheaper, but also much quieter than those blower fans.",
          "score": 0,
          "created_utc": "2026-01-26 18:46:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo349m",
      "title": "deepseek-ai/DeepSeek-OCR-2 Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-OCR-2",
      "author": "Dark_Fire_12",
      "created_utc": "2026-01-27 03:56:49",
      "score": 326,
      "num_comments": 37,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o1yiqke",
          "author": "foldl-li",
          "text": "They even thanked themself!\n\nhttps://preview.redd.it/t34eyddujtfg1.png?width=1037&format=png&auto=webp&s=7508bb6586dfb7327311dfddb2f108f459ccef2f",
          "score": 157,
          "created_utc": "2026-01-27 04:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yktdl",
              "author": "TheRealMasonMac",
              "text": "https://preview.redd.it/k8ykrq19mtfg1.png?width=286&format=png&auto=webp&s=9584d025699644e92331e6d5ff221b5c14ef68ba",
              "score": 126,
              "created_utc": "2026-01-27 04:37:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yiw2t",
              "author": "Dark_Fire_12",
              "text": "lol that made me laugh.",
              "score": 24,
              "created_utc": "2026-01-27 04:25:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1zdvfd",
              "author": "No_Afternoon_4260",
              "text": "May be not the same teal",
              "score": 3,
              "created_utc": "2026-01-27 08:27:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ykn7u",
          "author": "foldl-li",
          "text": "I always use scores reported by A to evaluate model B/C/D. So, in this case, PaddleOCR-VL looks really awesome.\n\nhttps://preview.redd.it/trymwuqoltfg1.png?width=1130&format=png&auto=webp&s=9b4a33243260da38c103d681c1ad5bdc8d5f9156",
          "score": 41,
          "created_utc": "2026-01-27 04:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yusec",
              "author": "linkillion",
              "text": "I mean, that's not really DS benchmarking the other model, it's just a general benchmark.Â \n\n\nThat said, paddleocr is great but it's a PITA to get working to this level, it requires their pipeline which I honestly gave up on very quickly. MistralOCR, although closed source, is so far ahead it's not even close in my opinion. For my use case all the docs I use are public, so I use MistralOCR exclusively.Â ",
              "score": 16,
              "created_utc": "2026-01-27 05:47:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yy48f",
                  "author": "zball_",
                  "text": "I use Gemini 3 flash as OCR and it was phenomenal.",
                  "score": 10,
                  "created_utc": "2026-01-27 06:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yxi6m",
                  "author": "skinnyjoints",
                  "text": "I have been sleeping on mistral for a while now. Why do you consider it to be the best? And is it the best among OCR specific models or does it compete with multimodal LLMs as well?",
                  "score": 1,
                  "created_utc": "2026-01-27 06:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z1yrg",
              "author": "Pvt_Twinkietoes",
              "text": "My experience with them has been phenomenal as well. I think somethings to note would be, it doesn't handle minor tilts/skew in the document, and users should be aware of that, but the pipeline provided does have a reliable model to predict the orientation of the document (90/180/270) tilts.\n\nThough it's amazing, I also noticed that there is a failure mode which causes the model to repeat itself (like Whisper), not sure of the cause but something to take note of.\n\nNevertheless it is truly an amazing model and very grateful they open sourced it.",
              "score": 1,
              "created_utc": "2026-01-27 06:43:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zit1a",
              "author": "Intelligent-Form6624",
              "text": "Does it work with ROCm or vulkan yet?",
              "score": 1,
              "created_utc": "2026-01-27 09:13:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1za5ac",
          "author": "R_Duncan",
          "text": "HunyuanOCR is not in the list.... this is cheating. For any kind of document, beats PaddleOCR hands down with 1B parameters.\n\n[https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true)",
          "score": 8,
          "created_utc": "2026-01-27 07:53:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zgxkh",
              "author": "__Maximum__",
              "text": "Is it end to end or pipeline?",
              "score": 2,
              "created_utc": "2026-01-27 08:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21dvaj",
                  "author": "R_Duncan",
                  "text": "Is pdf/image to markdown",
                  "score": 3,
                  "created_utc": "2026-01-27 16:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21vgbu",
              "author": "urekmazino_0",
              "text": "I second this",
              "score": 1,
              "created_utc": "2026-01-27 17:27:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z8565",
          "author": "Intelligent_Coffee44",
          "text": "I have some GPU credits that are near expiration, so I made this quick demo for DeepSeek OCR 2: [https://deepseek-ocr-v2-demo.vercel.app](https://deepseek-ocr-v2-demo.vercel.app)\n\n~~It's still very rough - small models + temperature=0 is very prone to repetition. I'll polish up the implementation in the morning. If anyone has an idea how to make the output more reliable, please let me know!~~\n\nUpdate: Decided to stay up and finish the job lol! Turns out the repetition issue was my user error. Now completely fixed after using DeepSeek's recommended decoding params. Performance is amazing and much more reliable than v1 in my testing. Hope you guys enjoy it too :O",
          "score": 18,
          "created_utc": "2026-01-27 07:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2078ly",
              "author": "Express-Director-474",
              "text": "Thanks for sharing your GPU with us. I still see the repetition error on my end.",
              "score": 2,
              "created_utc": "2026-01-27 12:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25pfyn",
                  "author": "Intelligent_Coffee44",
                  "text": "There were some configuration mistakes on my end. Now i've made it as closely aligned with official sample as possible. Please give it another try. I also did some analysis on the choice of prompt and document type affect output reliability - also published on the same site.",
                  "score": 1,
                  "created_utc": "2026-01-28 04:39:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o257yy2",
              "author": "__appelsinpiken",
              "text": "Very helpful! It looks like when tables are involved, the output is in HTML format, which doesnâ€™t render as a visible preview. Would it be possible to add a feature? That would make reviewing much more intuitive. Again thanks for your work! :>\n\nhttps://preview.redd.it/tu4x5r6t80gg1.png?width=665&format=png&auto=webp&s=29d0261de84095e8ee19861e769f0f9d3e654b0e",
              "score": 1,
              "created_utc": "2026-01-28 02:57:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25p4zl",
                  "author": "Intelligent_Coffee44",
                  "text": "Thanks for using! I just shipped markdown table support (from free OCR mode)\n\nDo you think this works or do you still prefer to have html table rendering?\n\nhttps://preview.redd.it/56nfle24r0gg1.png?width=715&format=png&auto=webp&s=4bf26bcd6c26e7e9ace035b56f251243c6c6be98",
                  "score": 1,
                  "created_utc": "2026-01-28 04:37:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yefjy",
          "author": "Dark_Fire_12",
          "text": "GitHub Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2](https://github.com/deepseek-ai/DeepSeek-OCR-2)\n\nPaper Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek\\_OCR2\\_paper.pdf](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf)",
          "score": 12,
          "created_utc": "2026-01-27 03:57:28",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1yfk2y",
          "author": "lomirus",
          "text": "Finally",
          "score": 9,
          "created_utc": "2026-01-27 04:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z49rf",
          "author": "the__storm",
          "text": "Interesting, I look forward to trying it out - DeepSeek-OCR (1) wasn't great (benchmarked okay but severely underperformed irl), so I'm glad they stuck with it.",
          "score": 3,
          "created_utc": "2026-01-27 07:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zq1jm",
          "author": "Gloomy-Signature297",
          "text": "Might be a stupid question but could this mean something regarding native multi-modality for Deepseek V4 next month?",
          "score": 3,
          "created_utc": "2026-01-27 10:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2177ju",
              "author": "ELPascalito",
              "text": "We cannot be sure, but it would be cool if the next model has this OCR module bolted on, just like how Mistral does",
              "score": 2,
              "created_utc": "2026-01-27 15:42:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ysnmt",
          "author": "Final_Personality987",
          "text": "https://preview.redd.it/bil1ybybvtfg1.png?width=1906&format=png&auto=webp&s=8ff884f062905a816cc6ba95e08904ca6e778b61\n\nquick summary: [https://lilys.ai/digest/7864011/8699710](https://lilys.ai/digest/7864011/8699710)",
          "score": 2,
          "created_utc": "2026-01-27 05:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ss31",
          "author": "DouglasteR",
          "text": "Simply amazing",
          "score": 1,
          "created_utc": "2026-01-27 14:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zgaxi",
          "author": "Intelligent-Form6624",
          "text": "Heck yes!!! ðŸ‘ðŸ‘\n\nCan it run on Strix Halo?",
          "score": 1,
          "created_utc": "2026-01-27 08:50:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjaxfy",
      "title": "8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/16ndtph7treg1.png",
      "author": "ai-infos",
      "created_utc": "2026-01-21 21:30:54",
      "score": 324,
      "num_comments": 128,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0zvsec",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-22 05:20:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xm0pa",
          "author": "ajw2285",
          "text": "now THIS is LocalLLaMA!",
          "score": 117,
          "created_utc": "2026-01-21 21:41:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y29mr",
              "author": "sourceholder",
              "text": "I propose zip ties as basic requirement to post. Chicken wire is good too.\n\nIf you're using enterprise gear with actual screws, post elsewhere.",
              "score": 33,
              "created_utc": "2026-01-21 22:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12a0wo",
                  "author": "ajw2285",
                  "text": "somewhere in the middle are 3d prints; you should see my 'homelab'",
                  "score": 2,
                  "created_utc": "2026-01-22 15:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o10veq0",
              "author": "imnotzuckerberg",
              "text": "We need le creatura monsters hall of fame.",
              "score": 1,
              "created_utc": "2026-01-22 10:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xy5lo",
          "author": "pharrowking",
          "text": "https://preview.redd.it/p8bgjpg95seg1.jpeg?width=1280&format=pjpg&auto=webp&s=94784f9d7825030ca1df87ef3bcfaaa3b2d9b630\n\nthank you for sharing. i was thinking of replacing my 8x tesla p40s with AMD cards. but now i probably wont. i currently get around 17-21 tokens/s of generation speed on minimax m2.1. so for an extra 5-10 tokens/sec its not worth it for me. this post was incredibly helpful.\n\ni'm curious if anyone can share speeds with 4x or 8x rtx 8000 gpus?",
          "score": 34,
          "created_utc": "2026-01-21 22:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1092hn",
              "author": "ProfessionalSpend589",
              "text": "I thought the cardsÂ were going up to 6000 only.\n\nI feel like a meme â€œitâ€™s over 9000â€ will pop up on me when I look up all the cards I canâ€™t buy. :)",
              "score": 3,
              "created_utc": "2026-01-22 07:06:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o125ozt",
                  "author": "Freonr2",
                  "text": "Nvidia's naming conventions are terrible.  A6000 (ampere), 6000 Ada Lovelace, 6000 Pro Blackwell, but the RTX 8000 was an older Turing chip (equiv to 20xx series) that came *before* the A6000...",
                  "score": 2,
                  "created_utc": "2026-01-22 15:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o11qdqv",
              "author": "bigh-aus",
              "text": "What server case is that?",
              "score": 1,
              "created_utc": "2026-01-22 14:03:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12ljun",
                  "author": "pharrowking",
                  "text": "Its a super micro 4U 4028 trt case",
                  "score": 2,
                  "created_utc": "2026-01-22 16:32:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xmn31",
          "author": "__JockY__",
          "text": "Yaaaaasssss this is what we subscribe for ðŸ”¥\n\nNow that I look more closelyâ€¦. holy fucking shit. 256GB  of plenty fast VRAM for under $1k is boss level building. 25 t/s with MiniMax-M2.1 will be pretty serviceable with offline Claude Code cli. \n\nI run MiniMax with Claude code cli (100% offline) all day long and itâ€™s changed my life.\n\nMad props, this is awesome.\n\nEdit: assuming you have enough VRAM for multiple 200k context sequences, Claude will use parallel sequences for completion and youâ€™ll see vLLM (or whatever) almost doubling your 25 t/s figure.",
          "score": 46,
          "created_utc": "2026-01-21 21:43:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xqg3s",
              "author": "ai-infos",
              "text": "thanks good to know!   \nin the logs, i get \"GPU KV cache size: 409,280 tokens... Maximum concurrency for 196,608 tokens per request: 2.08x\" so yeah, there's still some room to have better speed with parallel requests",
              "score": 9,
              "created_utc": "2026-01-21 22:01:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xzmag",
                  "author": "__JockY__",
                  "text": "Holy shit thatâ€™s *perfect*! Thatâ€™s basically what I have, too. Works so well that itâ€™s indistinguishable from witchcraft.",
                  "score": 5,
                  "created_utc": "2026-01-21 22:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o10q6h2",
              "author": "throwaway957263",
              "text": ">run MiniMax with Claude code cli (100% offline) all day long and itâ€™s changed my life.\n\nHow do you use claude code cli offline?",
              "score": 1,
              "created_utc": "2026-01-22 09:44:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o10r3aq",
                  "author": "__JockY__",
                  "text": "Install MiniMax-M2.1 FP8, vLLM, Claude code cli. Set the relevant env vars. Done!\n\n    ANTHROPIC_BASE_URL=http://your.vllm \n    ANTHROPIC_API_KEY=dummy\n    ANTHROPIC_AUTH_TOKEN=dummy\n    ANTHROPIC_MODEL=MiniMaxAI/MiniMax-M2.1\n    ANTHROPIC_SMALL_FAST_MODEL=${ANTHROPIC_MODEL}",
                  "score": 1,
                  "created_utc": "2026-01-22 09:52:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xtdzp",
          "author": "bigh-aus",
          "text": "$880 for 8 gpus....\n\nThis looks like something i want to make...\n\nChecks ebay - it's more like $880 for two now... \n\nScratch that!",
          "score": 21,
          "created_utc": "2026-01-21 22:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o126h60",
              "author": "vertigo235",
              "text": "Yeah they were really cheap before everyone realized you don't need CUDA.",
              "score": 5,
              "created_utc": "2026-01-22 15:23:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o165b9j",
              "author": "b0tbuilder",
              "text": "Mi150 is basically the same as a Radeon vii.  They have a different cooling solution and nerfed fp64 which means nothing for LLMs.  They go for about $200 on eBay each.  I have 2 that work just fine that I am too lazy to sell.",
              "score": 1,
              "created_utc": "2026-01-23 03:02:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xlq3u",
          "author": "6969its_a_great_time",
          "text": "What kind of motherboard do you need to put all 8 of these bad boys on it?",
          "score": 16,
          "created_utc": "2026-01-21 21:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xpv08",
              "author": "ai-infos",
              "text": "I'm using ASRock Rack ROMED8-2T (which has 7 PCIe 4.0 ports x16) but there are a lot of other possible solutions (using splitters / risers etc)",
              "score": 10,
              "created_utc": "2026-01-21 21:58:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xw4lp",
                  "author": "anti22dot",
                  "text": "Yes, but that MOBO itself cost tons of money, near 1K$, while general consumer latest MOBOs cost 200$...So, if you'd summup , the setup is very far from being affordable..",
                  "score": 4,
                  "created_utc": "2026-01-21 22:28:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o13vjvy",
                  "author": "6969its_a_great_time",
                  "text": "Wouldnâ€™t splinters make your generations speeds slower?",
                  "score": 1,
                  "created_utc": "2026-01-22 19:58:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xxart",
          "author": "Amazing_Athlete_2265",
          "text": "Suspended inference",
          "score": 12,
          "created_utc": "2026-01-21 22:34:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yf5tw",
              "author": "magic-one",
              "text": "With floating point precision",
              "score": 11,
              "created_utc": "2026-01-22 00:08:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12okll",
                  "author": "ConfidenceOk7751",
                  "text": "kkkkkkkkkk",
                  "score": 1,
                  "created_utc": "2026-01-22 16:45:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0yk65r",
          "author": "HistorianPotential48",
          "text": "the dangling cards look like pork meat stand in asian market. i need these in my house",
          "score": 11,
          "created_utc": "2026-01-22 00:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xw9rr",
          "author": "Previous_Nature_5319",
          "text": "try the --enable-expert-parallel option for vllm, it can speed up output to moe.",
          "score": 4,
          "created_utc": "2026-01-21 22:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y2mv7",
              "author": "ai-infos",
              "text": "every time I tried in the past (with glm 4.6 and other models), the speed was lower (-\\~5/10%) but the VRAM requirement was lower too, so the KV cache and max context length could be higher... I might give another shot for glm 4.7",
              "score": 4,
              "created_utc": "2026-01-21 23:01:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xnha8",
          "author": "HebelBrudi",
          "text": "Congrats! This is way faster than I would have thought for this kind of setup!",
          "score": 3,
          "created_utc": "2026-01-21 21:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xo9my",
          "author": "maglat",
          "text": "Have you used PCI-Splitter? If yes, which model exactly?",
          "score": 3,
          "created_utc": "2026-01-21 21:51:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xrgwu",
              "author": "ai-infos",
              "text": "yes, it was random chinese hardware from aliexpress (no known brands): \n\n* 2x SlimSAS PCIe device adapters\n* 2x SlimSAS cables 8i\n* 1x SlimSAS PCIe host adapter (plugged on the motherboard in the PCIe 4.0 port)\n\n(**SFF-8654 8i)**",
              "score": 10,
              "created_utc": "2026-01-21 22:06:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o147fnm",
                  "author": "maglat",
                  "text": "Would you mind to share the exact link of every component? Thank you.",
                  "score": 1,
                  "created_utc": "2026-01-22 20:53:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o14qo4k",
                  "author": "maglat",
                  "text": "Have you connected the 6pin power source to the 2x SlimSAS PCIe device adapters?",
                  "score": 1,
                  "created_utc": "2026-01-22 22:27:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0yt2ig",
          "author": "Lissanro",
          "text": "This is truly impressive for its price... For comparison, with four 3090 GPUs + 64-core EPYC 7763 + 8-channel 1 TB 3200MHz DDR4 RAM, running IQ4 quant of M2.1, I get:\n\n    prompt eval time =   26738.90 ms / 13374 tokens (    2.00 ms per token,   500.17 tokens per second)\n    eval time =   15404.66 ms /   280 tokens (   55.02 ms per token,    18.18 tokens per second)\n\nSo, you have 1.5x faster inference and 6 times faster prompt processing, at the total GPU cost about 3 times less compared to four 3090.\n\nI actually considered getting MI50 myself, my motherboard potentially can fit up to 20 of them, each having x4 PCI-E 4.0 bandwidth (technically, with 20 GPUs, two would be at x8 PCI-E 3.0, but bandwidth should be about the same as x4 PCI-E 4.0 on the rest), in the hope running faster large models like Kimi K2 Thinking, but I am a bit concerned about potential stability issues, and I also think MI50 generation speed could be much higher given its VRAM bandwidth so may be it still not as optimized as it could be, at least not yet. But it is great to hear that you reached good stability with 8 GPUs.",
          "score": 3,
          "created_utc": "2026-01-22 01:25:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zihj1",
              "author": "-InformalBanana-",
              "text": "What is the tg/s if you run it only on cpu, with threads setting in llama.cpp equaling the number of physical cores - 1 (Ive heard that is the best value for threads)",
              "score": 2,
              "created_utc": "2026-01-22 03:51:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o11gguo",
              "author": "ai-infos",
              "text": "Thanks for your feedback and benchmark. Here are some remarks:  \n\n1. There's something I didn't mention (but I assume a lot of people already know) , this is an old AMD card and not NVIDIA so that's not always \"plug & play\".  That means that if you don't use llama.cpp, it can be a nightmare for some people to make it work on vllm-gfx906 fork. Every time, there's a new model, I'm not sure if the MI50 setup can make it work with vllm-gfx906, I have to do the \"integration\" myself, looking into the source code, checking the last vllm PRs, etc... that's really time consuming (if you compare with the \"plug and play\" 3090 setup...)\n2. Another thing to know about MI50s, according to the last benchmarks, they have pretty bad speed in Image/Video generation.\n3. I didn't mention neither, but the 26.8 tok/s speeds are for pcie 3.0 with a mix of x4 and x8. For inference, there won't be a big difference in pcie 4.0 x16 vs pcie 3.0 x4. \n4. MI50 does not perform well with \"non standard\" gguf quants, I mean, they have good TG speed for Q8\\_0, Q4\\_0, Q4\\_1, but i remember someone who posted the speed for Q4\\_K\\_XL unsloth quant and it was \\~30% lower than the Q4\\_0\n5. I don't know if Kimi K2 thinking will work with vllm-gfx906, for now, no one tried publicly. You have to know that Kimi K2 Thinking use the MLA (Multi-Head Latent Attention) architecture, the same as Deepseek V3.2 and last time I tried for Deepseek, it wasn't stable after 18k context (generating garbage output). Besides, the kimi quant does 594GB so we need either 32 MI50 (using TP 32 on 2 nodes) or 24 MI50 (using TP8 PP3 on 2 or 3 nodes...). It will use vllm ray backend and last time I tried with deepseek, it wasn't really stable. I might give another shot and post the feedback there if i have time.\n\n\n\nOverall, MI50 setups are still interesting (even at 200-250â‚¬ per gpu) if you're ready to spend some time optimizing the software stack to make it work at decent speed.",
              "score": 2,
              "created_utc": "2026-01-22 13:08:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14y6gd",
                  "author": "Lissanro",
                  "text": "Thank you for sharing this valuable info! I guess, if I really wanted to, I could stuff 24 GPUs (convert two M.2 slots for two more, and bifurcate PCI-E 3.0 x16 to x4 each, with everything else on PCI-E 4.0 x4), but sounds like vllm-gfx906 is not yet ready for production use! My guess, llama.cpp could more more tolerant.\n\nGiven 544G size of K2 Thinking Q4\\_X quant from Ubergarm (this is the biggest possible size since it mirrors the original INT4 weights), and that 256K context cache takes around 80GB VRAM, in total may fit on 20 GPUs but may be tight fit with about 0.5GB headroom per GPU, so now that I think about it, likely would need 22xMI50 for better headroom around 3.5 GB per card. But your current 8-GPU setup is probably more reasonable, 22 GPUs likely to be hard to manage.\n\nIn my case, most straightforward upgrade would be probably just add four more 3090 GPUs, for 8 in total, each on x8 PCI-E 4.0, if I wanted to fully fit MiniMax M2.1 the IQ4 quant in VRAM... I actually planned originally go with 8 GPUs, so I have enough power and space for them, and even four more risers, but right after I built my rig, I had some stability issues, that at first seemed to be caused by GPUs (like \"GPU fell off the bus\" happening from time to time) but as it turned out, were due to bad CPU contact in CPU socket - after resitting CPU couple of times they went away forever, while resitting cards and risers never had any effect on them. However, it took me few month to figure out and I just did not get to upgrade any further yet.\n\nBy the way, I noticed that with lower context MiniMax M2.4 the IQ4 quant runs much faster on my rig with four 3090 (this is closer your testing length):\n\n    prompt eval time =    5587.22 ms /  2277 tokens (    2.45 ms per token,   407.54 tokens per second)\n    eval time =   13539.28 ms /   322 tokens (   42.05 ms per token,    23.78 tokens per second)\n\nPrompt processing seems to be mostly within 400-500 tokens/s range, could not push it any further even after experimenting with various options in ik\\_llama.cpp. But I guess not bad given I have only 24 layers out of 63 in VRAM, and the rest are in slower 8-channel 3200MHz DDR4 RAM.",
                  "score": 2,
                  "created_utc": "2026-01-22 23:08:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o109vpu",
              "author": "def_not_jose",
              "text": "> at the total GPU cost about 3 times less compared to four 3090.\n\nThese are early 2025 prices though, I don't think mi50 are viable at this point",
              "score": 1,
              "created_utc": "2026-01-22 07:13:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11b2ci",
                  "author": "ai-infos",
                  "text": "on alibaba, it's now around the double in price: 200-250â‚¬ per MI50",
                  "score": 1,
                  "created_utc": "2026-01-22 12:34:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o103s7d",
          "author": "indicava",
          "text": "Damn OP, that is one janky build. \n\nItâ€™s beautiful! \n\nTrue Localllama style.",
          "score": 3,
          "created_utc": "2026-01-22 06:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ytr21",
          "author": "grabber4321",
          "text": "EXWEEEZE ME? thats a nice rack!",
          "score": 2,
          "created_utc": "2026-01-22 01:29:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yu1fc",
          "author": "StardockEngineer",
          "text": "I canâ€™t help but love a beast.",
          "score": 2,
          "created_utc": "2026-01-22 01:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z1lkb",
          "author": "ortegaalfredo",
          "text": "Thats quite cool I have 10x 3090 and I'm getting 20 tok/s on GLM 4.7 AWQ 4 bit. Very close.",
          "score": 2,
          "created_utc": "2026-01-22 02:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11hhmb",
              "author": "ai-infos",
              "text": "thanks for the feedback! Is it with MTP setting? If not, you can get an extra boost in TG speed  \n(and if this is vllm, check in your log, if it uses a specific fused\\_moe config or the default one...if it uses the default one, there's still some room for better perf in your setup). Anyway, 20 tok/s is already very good and enough for a lot of usecases actually",
              "score": 1,
              "created_utc": "2026-01-22 13:14:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11n8no",
                  "author": "ortegaalfredo",
                  "text": "Could not make MTP work. I believe VLLM still don't support it, but I didn't tried the latest version.\n\nYes, it works very good, I use it for coding, agents, basically everything. The only problem is the heat but that's only when I run huge batched requests.",
                  "score": 1,
                  "created_utc": "2026-01-22 13:46:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0z3rnz",
          "author": "muyuu",
          "text": "How are the results like with those models and context lengths?\n\nI'm wondering if they're workable for coding assistance.",
          "score": 2,
          "created_utc": "2026-01-22 02:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11iy6h",
              "author": "ai-infos",
              "text": "on minimax, you can have the max context length at 196k , but for glm 4.7 (which is a bigger model), you can go up to \\~95k (half of its max)  \n  \nfor coding agents, for now, having at least \\~80k is enough as most of the models got bad at high context depth   \n  \nyes, that's 100% workable for coding assistance, i'm using it for that.",
              "score": 2,
              "created_utc": "2026-01-22 13:22:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zbmd5",
          "author": "sine120",
          "text": "God I keep beating myself up for not buying those cards when they were cheap.",
          "score": 2,
          "created_utc": "2026-01-22 03:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o100a0l",
          "author": "overand",
          "text": "Dang, for a 2018 card, that thing has killer bandwidth! over 1TB/sec, like 5-10% more memory bandwidth than a 3090. Nice!",
          "score": 2,
          "created_utc": "2026-01-22 05:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11jeie",
              "author": "ai-infos",
              "text": "yes! but unfortunetaly, memory bandwidth doesn't do everything, i learnt it the hard way with the mi50s (see: [https://www.reddit.com/r/LocalLLaMA/comments/1qjaxfy/comment/o11gguo/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qjaxfy/comment/o11gguo/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) )",
              "score": 2,
              "created_utc": "2026-01-22 13:25:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11awj0",
          "author": "gregtoth",
          "text": "Nice work on pushing the performance envelope! It's always rewarding to find that sweet spot of stability and throughput. I'm curious what kind of use cases you're targeting with this setup - anything specific in mind?",
          "score": 2,
          "created_utc": "2026-01-22 12:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11k376",
              "author": "ai-infos",
              "text": "i'm developing a stack to have autonomous coding agents with good memory handling (still a lot of work to do)",
              "score": 1,
              "created_utc": "2026-01-22 13:29:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o15m2xi",
                  "author": "michaelsoft__binbows",
                  "text": "I have been playing with oh-my-opencode lately and it seems pretty promising so far. I do enjoy seeing the main model able to construct multiple different research angles for subagents to prosecute in parallel all while preventing the main base agent from getting its context flooded with the MCP schemas of the research tools!\n\nThe ability and effectiveness of recursively meta-iterating on the internals of the tool with the tool itself is particularly invigorating.",
                  "score": 1,
                  "created_utc": "2026-01-23 01:15:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11sehw",
          "author": "DaniyarQQQ",
          "text": "Hanging Gardens of GPUs",
          "score": 2,
          "created_utc": "2026-01-22 14:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o126q4z",
          "author": "dsanft",
          "text": "Nice. \n\nI'm silly and I'm forcing myself to write my own inferencing engine and kernels from scratch for my Xeon and Mi50s. \n\nOne of these days I need to just hook it all up and use it with something off the shelf just to watch it fly",
          "score": 2,
          "created_utc": "2026-01-22 15:24:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12nf92",
              "author": "ai-infos",
              "text": "good luck with that, i hope you will succeed!",
              "score": 2,
              "created_utc": "2026-01-22 16:40:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12vq5i",
          "author": "shvz",
          "text": "Noob question here- you are not using any obfuscation for the 8th card? like where is the 8th card going? \n\nthis is fire thank for sharing!",
          "score": 2,
          "created_utc": "2026-01-22 17:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o132x7k",
              "author": "ai-infos",
              "text": "the picture has not been taken properly, but you can see a part of the 8th card at left (there are 7 upstairs and 1 on the motherboard)",
              "score": 2,
              "created_utc": "2026-01-22 17:50:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o13gfb1",
                  "author": "shvz",
                  "text": "so this is a PCI card that allow you to connect 2 more PCIe?",
                  "score": 1,
                  "created_utc": "2026-01-22 18:50:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1461q1",
          "author": "kidflashonnikes",
          "text": "good output. I work for one of the largest labs, and they give us a personal compute budget for free time expermintation. My stipend allowed me to get 4 RTX PRO 6000s (40k stipend on personal compute per year based on my labs performance). My set up is this: the 4 RTX PRO 6000s, Asus wrx 90 SAGE SE mobo, 2,000 watt PSU, 1 TB of RAM (ECC RDIMM 5), 16 TB of SDDs Samsung storage, thread ripper pro (96 GB), AIO, wireless lian li fans, all house in the Phanteks pro server 2 TG case. I also have a small gpu, a 5060 GPU which renders the monitor for. I am running over 50 t/s on quantized larger version of minimax and GML models. Also for anyone wondering - not allowed to say the name of the lab, but its not OpenAI or Antjropic, but I do work as a 3rd party with Musk, my lab analyzed damaged brain wavelenghts/frequncies and we use LLMs at my lab to study damaged brains in hopes of curing or helping certain neurlogical diseases, as well as plan out with LLMs medicines that we will be used for data centers once the labs get attached to nuclear faciltiies ect.",
          "score": 2,
          "created_utc": "2026-01-22 20:47:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14edg7",
              "author": "ai-infos",
              "text": "thanks for your review and benchmark, I guess it's fp8 quants so 50 t/s on them with 4 rtx pro 6000, that's pretty good (except that you might have a small kv cache with glm 4.7 fp8 i guess)",
              "score": 1,
              "created_utc": "2026-01-22 21:26:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1g222e",
          "author": "Maximum_Parking_5174",
          "text": "If you use vllm could you also add top speed with parallell request?\n\nI have a 8x RTX3090 EPYC server and have done some testing with Minimax m2.1 AWQ 4 bit. I get 75t/s in TG without parallell requests and 645 t/s at 20 reqs. Prompt processing is about 5300t/s.",
          "score": 2,
          "created_utc": "2026-01-24 16:09:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h97yx",
              "author": "ai-infos",
              "text": "Thanks for your benchmard, that's good to know the 3090 perf  \nI didn't try at 20 reqs but in Claude Code, I saw 3 reqs at 34 t/s for Minimax m2.1 awq 4bit and for the prompt processing, i saw som peaks at 5500t/s (for \\~10k+ tokens input)",
              "score": 2,
              "created_utc": "2026-01-24 19:18:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xzjdw",
          "author": "GaboureySidibe",
          "text": "I wish someone would invent computer cases.",
          "score": 2,
          "created_utc": "2026-01-21 22:45:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y5yww",
              "author": "foldl-li",
              "text": "computer cases need to be re-invented.",
              "score": 5,
              "created_utc": "2026-01-21 23:19:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0z91v5",
              "author": "pixelpoet_nz",
              "text": "You may not like it, but this is peak computing form.",
              "score": 6,
              "created_utc": "2026-01-22 02:55:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0y50wk",
          "author": "Individual-Source618",
          "text": "do you use tensor paralelism ?",
          "score": 1,
          "created_utc": "2026-01-21 23:14:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0y7vbz",
              "author": "ai-infos",
              "text": "yes TP 8 for both models",
              "score": 1,
              "created_utc": "2026-01-21 23:29:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0y5bc9",
          "author": "kersk",
          "text": "I think this post highlights an incredible opportunity for server colocation in chicken coops. Keeps the eggs warm and provides infrastructure to dangle GPUs from.",
          "score": 1,
          "created_utc": "2026-01-21 23:15:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10j8d3",
              "author": "Aphid_red",
              "text": "Chickens are curious creatures.\n\nChickens also like their warmth and comfort.\n\nChickens are birds. \n\nBirds produce wet excrement.\n\nYou will quickly find your GPUs being turned into a nesting place, which will lower their expected lifespan to about a day or two.",
              "score": 1,
              "created_utc": "2026-01-22 08:38:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12j0jg",
                  "author": "kersk",
                  "text": "Itâ€™s ok, you sell the eggs to offset the costs of replacing GPUs",
                  "score": 1,
                  "created_utc": "2026-01-22 16:21:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ybb2r",
          "author": "SnowyOwl72",
          "text": "sick, electricity bill?",
          "score": 1,
          "created_utc": "2026-01-21 23:48:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10ii2e",
              "author": "Amazing_Athlete_2265",
              "text": "Yes",
              "score": 2,
              "created_utc": "2026-01-22 08:31:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o116ohp",
              "author": "ai-infos",
              "text": "i will have the bill at the end of the year but i guess:   \n  \n0.6kW(mean\\_use)\\*4hours\\_use\\_per\\_day\\*30days\\*0.2â‚¬/kwh=14.4â‚¬ / month just for that...\n\nthat might be high but that's the price for private local inference",
              "score": 1,
              "created_utc": "2026-01-22 12:03:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ybz7h",
          "author": "Obvious-Nobody-9592",
          "text": "Where did u get mi50's?",
          "score": 1,
          "created_utc": "2026-01-21 23:51:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yp8z3",
              "author": "NullKalahar",
              "text": "Alibaba and eBay",
              "score": 3,
              "created_utc": "2026-01-22 01:03:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o116ult",
              "author": "ai-infos",
              "text": "alibaba",
              "score": 3,
              "created_utc": "2026-01-22 12:05:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ys37p",
          "author": "ayake_ayake",
          "text": "What kind of cooling solution do you use and how loud is it during inference?",
          "score": 1,
          "created_utc": "2026-01-22 01:19:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11akq3",
              "author": "ai-infos",
              "text": "with the low quality chinese fans i use, it's loud, like very loud\n\nbut can be quiet if you have the budget for it (water cooling, very high quality and silent fans...)",
              "score": 1,
              "created_utc": "2026-01-22 12:31:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14qsj4",
                  "author": "ayake_ayake",
                  "text": "I'd be happy to try out shrouds and noctua fans to get a lower noise level - but I don't really know what noise levels I can reach with that. Do you have any idea maybe? I haven't found anything for that yet.",
                  "score": 1,
                  "created_utc": "2026-01-22 22:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0yvbq3",
          "author": "MushroomCharacter411",
          "text": "I think someone has misunderstood what \"rack-mounted hardware\" means.",
          "score": 1,
          "created_utc": "2026-01-22 01:38:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z0cxr",
          "author": "dropswisdom",
          "text": "What is this monstrosity?",
          "score": 1,
          "created_utc": "2026-01-22 02:06:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z1n65",
          "author": "sloptimizer",
          "text": "Nice system! What's your bifurcation setup?\n\nI see 4 plugged in with x16 and a PCIe splitter card. Is it 4 x16 and 4 x4?",
          "score": 1,
          "created_utc": "2026-01-22 02:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11hygd",
              "author": "ai-infos",
              "text": "5 in x16 with extenders, 1 in x16 on the motherboard, 2 in x8 with SlimSAS splitter",
              "score": 1,
              "created_utc": "2026-01-22 13:17:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10bdhk",
          "author": "Available-Air-9110",
          "text": "ä¸€å¼ gpuè¿è¡ŒranslateGemma 27b  é€Ÿåº¦æœ‰å¤šå°‘",
          "score": 1,
          "created_utc": "2026-01-22 07:26:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10f1bo",
          "author": "MelodicRecognition7",
          "text": "lol this is so perfectly ugly! even the cardboard is cut exactly to the mobo dimensions to create a shortcircuit risk at the slightest movement.",
          "score": 1,
          "created_utc": "2026-01-22 07:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16423f",
          "author": "b0tbuilder",
          "text": "I have 2 Radeon VII that are both in good condition and functional if you are interested\n\nOh snap, I just remembered, Radeon vii are limited to PCIE 3 x 16.  Does mi150 support PCIE 4?",
          "score": 1,
          "created_utc": "2026-01-23 02:55:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o173ol8",
          "author": "Polysulfide-75",
          "text": "Sharding slowness!  Get an old DGX with NVLINK",
          "score": 1,
          "created_utc": "2026-01-23 06:54:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18rk55",
          "author": "Neptun78",
          "text": "Nice tests\nWhat prompt processing speed can You get? Especially for very long context (I see you tried about 100k and 200k)",
          "score": 1,
          "created_utc": "2026-01-23 14:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1b7sv4",
              "author": "ai-infos",
              "text": "in claude code with glm 4.7 and with 46.5k context depth (kv cache 49% of 95k), i've got some peak prompt processing (PP) speed around \\~8500 tok/s but the token generation speed decrease a lot and got only \\~2 tok/s, when using MTP. Without MTP, PP is equivalent but token generation speed is slightly better at 46k context depth: around \\~4.5 tok/s",
              "score": 2,
              "created_utc": "2026-01-23 21:16:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1orbu5",
          "author": "stasj145",
          "text": "> Power draw: 280W (idle) / 1200W (inference)\n\nJfc, must be nice living somewhere with cheap power. That thing would cost me about 830â‚¬ (985$) per year in electricity alone. (Assuming just 1h of inference per day and 24/7 uptime)",
          "score": 1,
          "created_utc": "2026-01-25 20:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xocnz",
          "author": "crowtain",
          "text": "Thanks for sharing all the process, but i wonder how can you get so \"few\" token/s , i'v got an framework destop and i use minimax Q3 kxl, and it has 25 token/s with small context, after a while it crawls to 3.  \nbandwidth of MI50 is 4Â times the stryx and with vllm it should be even higher?",
          "score": 1,
          "created_utc": "2026-01-21 21:51:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xsegm",
              "author": "ai-infos",
              "text": "in the vllm-gfx906 fork [https://github.com/nlzy/vllm-gfx906](https://github.com/nlzy/vllm-gfx906), the dev wrote: \"All MoE quantization models are significantly slow, and all unquantized models are slightly slow. Not recommended to use.\" \n\nso yes, theoretically it can be higher if the software stack was more optimized for gfx906 MI50 gpu",
              "score": 3,
              "created_utc": "2026-01-21 22:10:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0ymzh8",
                  "author": "[deleted]",
                  "text": "You would get higher inference speed using the gfx906 fork of llama.cpp than you would this version of vllm, which as you said doesn't really work for MOE models.",
                  "score": 2,
                  "created_utc": "2026-01-22 00:51:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o103kw4",
                  "author": "crowtain",
                  "text": "I see, I believe you already tried llama.cpp and it's fork and wasn't performing at best.  \nso from your setup, it seems that 4GPU without nvlink start to really slow down.",
                  "score": 1,
                  "created_utc": "2026-01-22 06:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0xsr4e",
              "author": "Toooooool",
              "text": "iirc MI50 doesn't have any tensor cores",
              "score": 2,
              "created_utc": "2026-01-21 22:12:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0xtldd",
                  "author": "fallingdowndizzyvr",
                  "text": "Strix Halo doesn't have tensor cores.",
                  "score": 1,
                  "created_utc": "2026-01-21 22:16:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qiwm3c",
      "title": "Fix for GLM 4.7 Flash has been merged into llama.cpp",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/18980",
      "author": "jacek2023",
      "created_utc": "2026-01-21 12:29:19",
      "score": 318,
      "num_comments": 86,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0vxj01",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-21 17:10:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0v49jf",
          "author": "GodRidingPegasus",
          "text": "How does it do running CPU only, for the GPU poor?",
          "score": 18,
          "created_utc": "2026-01-21 14:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vp442",
              "author": "supportend",
              "text": "Depends what hardware you have. I use a Ryzen 5700u with 64 GB slower RAM (3200) and it runs well, sure not very fast.",
              "score": 5,
              "created_utc": "2026-01-21 16:32:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0xi3ad",
                  "author": "Michaeli_Starky",
                  "text": "Not very fast - very-very slow you mean?",
                  "score": 1,
                  "created_utc": "2026-01-21 21:23:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0ytoqr",
              "author": "ProfessionPurple639",
              "text": "On a Mac m4 max with 64gb. Iâ€™ve got the gguf running with llama.cpp at 150 tps average. Just started testing but so far very happy with its performance.",
              "score": 2,
              "created_utc": "2026-01-22 01:28:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0v4soj",
              "author": "jacek2023",
              "text": "what is the reason for not having even a simple GPU?",
              "score": -23,
              "created_utc": "2026-01-21 14:59:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0v66kr",
                  "author": "CatEatsDogs",
                  "text": "A lot of reasons. I have amd 3700x in my server for example. It doesn't have igpu.",
                  "score": 2,
                  "created_utc": "2026-01-21 15:05:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0vbblc",
                  "author": "Old-Sherbert-4495",
                  "text": "any ways to run on 16gb vram and 32 gb ram?",
                  "score": 1,
                  "created_utc": "2026-01-21 15:30:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ujsqo",
          "author": "Deep_Traffic_7873",
          "text": "is the GGUF from unsloth OK or it has to be redownloaded ?",
          "score": 25,
          "created_utc": "2026-01-21 13:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uktdv",
              "author": "xRintintin",
              "text": "Asking for a friend.",
              "score": 12,
              "created_utc": "2026-01-21 13:12:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0uncdm",
                  "author": "danielhanchen",
                  "text": "Yes they work now - we re-did them all!",
                  "score": 33,
                  "created_utc": "2026-01-21 13:27:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0wmzkr",
                  "author": "Hunting-Succcubus",
                  "text": "I can confirm he asked for my sake.",
                  "score": 3,
                  "created_utc": "2026-01-21 19:02:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0v22ji",
              "author": "noneabove1182",
              "text": "I updated mine already, the code update will fix the old models but imatrix calculated with the old gate won't be as accurate\n\nhttps://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF",
              "score": 32,
              "created_utc": "2026-01-21 14:45:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0un55n",
              "author": "danielhanchen",
              "text": "Yes! You in fact do not need to update llama.cpp when using Unsloth ones (we directly injected the correct scoring\\_func in the metadata - just re-download them. However you do need to use updated params ie `--temp 1.0 --top-p 0.95 --min-p 0.01`\n\n**For LM Studio, disable** `repeat_penalty` (this causes issues rather) or set it to 1.0!\n\nAs an example after fixing the `\"scoring_func\": \"sigmoid\"` issue, it created a Flappy Bird game in HTML only successfully and it ran:\n\nhttps://preview.redd.it/6vl55e44fpeg1.png?width=640&format=png&auto=webp&s=32471549b912c4c3f758a2b65e24270ffebd58e7",
              "score": 29,
              "created_utc": "2026-01-21 13:26:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0v21fx",
                  "author": "_raydeStar",
                  "text": "Dang. You guys don't mess around. Way to go. I'm going to download again and see what happens. I was getting about 45-50 t/s",
                  "score": 6,
                  "created_utc": "2026-01-21 14:45:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0w2170",
                  "author": "RickyRickC137",
                  "text": "Can we use flash attention in your updated GGUFs? or is that still problematic?",
                  "score": 5,
                  "created_utc": "2026-01-21 17:30:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0us4fa",
                  "author": "tgsz",
                  "text": "Was it only for the smaller quantized versions or the original bf16 also had the repeat/loop issue?",
                  "score": 4,
                  "created_utc": "2026-01-21 13:53:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0uwxgt",
                  "author": "Deep_Traffic_7873",
                  "text": "Thanks for the reply. It's possible to tweak the reasoning part? because it reason a lot for my use cases.",
                  "score": 3,
                  "created_utc": "2026-01-21 14:19:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0vhxx1",
                  "author": "Free-Internet1981",
                  "text": "Thanks ðŸ™",
                  "score": 2,
                  "created_utc": "2026-01-21 16:00:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uonln",
          "author": "viperx7",
          "text": "if anyone is wondering about speeds i am getting\n\n# GLM 4.7 unsloth (data for 20k context)\n\n|Quant|GPU|Context|Prompt Processing|Token Generation|Notes|\n|:-|:-|:-|:-|:-|:-|\n|UD-Q4\\_K\\_XL|Single 4090|64k|3489 t/s|88 t/s||\n|UD-Q4\\_K\\_XL|4090 + 3060|170k|2017 t/s|52 t/s||\n|Q8|4090 + 3060|30k|2087 t/s|47.1 t/s||\n|Q8|4090 + 3060 + cpu|64k|1711 t/s|41.3 t/s|`-ot '([2][0-2]).ffn_.*_exps.=CPU'`|\n\ni ran with `llama-server --host 0.0.0.0 --port 5000 -fa auto --no-mmap --jinja -fit off --no-op-offload -m <model> -c <ctx>`",
          "score": 23,
          "created_utc": "2026-01-21 13:34:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0uos6f",
              "author": "jacek2023",
              "text": "FA or no FA?",
              "score": 7,
              "created_utc": "2026-01-21 13:35:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0uq333",
                  "author": "viperx7",
                  "text": "added full command   \nflash attention  was set to auto",
                  "score": 8,
                  "created_utc": "2026-01-21 13:42:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wbfnx",
          "author": "Environmental_Hand35",
          "text": "Fits within the VRAM of a single RTX 3090 using the following parameters:  \n./llama-server -hf unsloth/GLM-4.7-Flash-GGUF:Q4\\_K\\_M --threads 9 --flash-attn auto --prio 3 --n-gpu-layers 999 --temp 0.7 --top-p 1.0 --min-p 0.01 --no-warmup --jinja --ctx-size 32768 --batch-size 4096 --ubatch-size 1024 --host [0.0.0.0](http://0.0.0.0) \\--port 8090 --no-webui -fit off\n\nHere are the results from a short prompt:  \nprompt eval time =      35.13 ms /    14 tokens (    2.51 ms per token,   398.55 tokens/s)        eval time =    3805.10 ms /   408 tokens (    9.33 ms per token,   107.22 tokens/s)       total time =    3840.23 ms /   422 tokens",
          "score": 7,
          "created_utc": "2026-01-21 18:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0uvvq2",
          "author": "dsartori",
          "text": "This is good. \n\nModel is much smarter now with no gibberish or repetition detected. \n\nI wonder if anyone else is seeing the problem I am, though. Prompt processing is insanely slow in LMStudio on my Strix Halo hardware. Not sure why but I get about 13 t/s for prompt procession which is absurdly slow. Generation is normal at 35 t/s.\n\nEDIT: Thanks to the person who ninja-commented \"disable FA\" that fixed it. 557 t/s now; good for this hardware.",
          "score": 10,
          "created_utc": "2026-01-21 14:13:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wqysz",
              "author": "theplayerofthedark",
              "text": "How are you running it to get over 500tps pp? Im only seeing \\~120 so far for Q4",
              "score": 1,
              "created_utc": "2026-01-21 19:20:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ufhq1",
          "author": "QuackerEnte",
          "text": "does GLM 4.7 Flash really use deepseeks architecture, specifically the Latent Attention compression? I struggle to find official mentions of that aside from some unofficial ggufs on huggingface mentioning it. If someone can point me to the informations source, that would be of great help. ðŸ™",
          "score": 8,
          "created_utc": "2026-01-21 12:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ug6xa",
              "author": "lly0571",
              "text": "```\n  \"q_lora_rank\": 768,\n  \"kv_lora_rank\": 512,\n  \"qk_nope_head_dim\": 192,\n  \"qk_rope_head_dim\": 64,\n  \"v_head_dim\": 256,\n```\n\nThese lines in the `config.json` file prove the model is using MLA.",
              "score": 14,
              "created_utc": "2026-01-21 12:43:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0w4iiu",
                  "author": "QuackerEnte",
                  "text": "Thank you",
                  "score": 1,
                  "created_utc": "2026-01-21 17:41:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ut3wr",
                  "author": "SilentLennie",
                  "text": "MLA is Deepseek, but it's not DSA which is also Deepseek.",
                  "score": 0,
                  "created_utc": "2026-01-21 13:59:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xc34h",
          "author": "Chlorek",
          "text": "Just tried it, latest seemingly fixed GLM-4.7-Flash-UD-Q4\\_K\\_XL gguf. Very good model, depends on use case but not the best I tested in its class. Despite that its full-sized brother is my go-to everyday. First couple attempts at programming just worked, but agentic programming fails due to quickly decreasing speed and quality (tested with 32k context on RTX 3090). At the time being Devstral Small 2 (same unsloth's quant) is way better for autonomous agents of claude code kind.",
          "score": 3,
          "created_utc": "2026-01-21 20:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xcatn",
              "author": "jacek2023",
              "text": "Is your FA disabled?",
              "score": 2,
              "created_utc": "2026-01-21 20:56:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0xf8pe",
                  "author": "Chlorek",
                  "text": "FA on 62 tokens/s preprocessing vs 32 tokens/s with FA off. For comparison I have over 1800 tokens/s for the mentioned Devstral.",
                  "score": 2,
                  "created_utc": "2026-01-21 21:10:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0xo32s",
              "author": "Guilty_Rooster_6708",
              "text": "I also tried Q4\\_K\\_XL from Unsloth but it kept failing the bouncing ball simulation. The model keeps looping itself in either its thinking process or ouput. Do you see the same issue in your test? \n\nhttps://preview.redd.it/9id23dv2xreg1.png?width=669&format=png&auto=webp&s=c3f556b8141eca023ea90eb03ab3e3f8c9420c12",
              "score": 2,
              "created_utc": "2026-01-21 21:50:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0xpz9r",
                  "author": "Chlorek",
                  "text": "Not in case of this model, yet, but this is happening to me with some even really big, good models in combination with some system prompts and other parameters, quirks I guess. I have the same thing happening to me with big GLM 4.7 in a few agentic workflows, while never experiencing it in others. For Flash I've been using temp 0.7 top-p 1.0, min-p 0.01 and Zed agent.",
                  "score": 1,
                  "created_utc": "2026-01-21 21:59:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0yivbs",
                  "author": "yoracale",
                  "text": "When did you download the quants and did you follow the parameters? Sometimes looping can happen, it even happens via Gemini and chatgpt",
                  "score": 1,
                  "created_utc": "2026-01-22 00:28:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xissg",
          "author": "StardockEngineer",
          "text": "It's so slow after just 40k tokens, I wouldn't say it's fixed just yet.",
          "score": 3,
          "created_utc": "2026-01-21 21:26:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0xnyaw",
              "author": "Mr_Back",
              "text": "I confirm. Very slowly. Slower than GLM 4.5 air Q4. Slower than similar models.  \nExample: Nemotron 3 nano F16 vs GLM 4.7 air Q4 UD. \n\nhttps://preview.redd.it/08l05aq1wreg1.png?width=1280&format=png&auto=webp&s=ede9543179f3736e39bc01c5fa36203b6ebfda23\n\nAnd yet GLM's response is shit(  \nI updated the llama and the model.  \nMy PC: i5 12400, 96gb ram. 4070 - 12gb vram.",
              "score": 3,
              "created_utc": "2026-01-21 21:49:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0xqsfw",
                  "author": "StardockEngineer",
                  "text": "Yeah, I was running it on a very simple set of todos and it couldn't get them done, slow or not.  Not promising.  \n\nTbf, I haven't tuned any params as recommended.",
                  "score": 1,
                  "created_utc": "2026-01-21 22:03:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o10g5zt",
                  "author": "simon96",
                  "text": "You need 5090, with 25gb vram, and honeslty that is not even enough, more VRAM is requierd",
                  "score": 1,
                  "created_utc": "2026-01-22 08:09:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ul6rp",
          "author": "Pristine_Income9554",
          "text": "Fixed != merged. It still has problems to be fixed before it will be merged in to master tree",
          "score": 8,
          "created_utc": "2026-01-21 13:14:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0una5e",
              "author": "DeProgrammer99",
              "text": "The gating function fix was merged when OP posted.\n\n> pwilkinÂ merged 3 commits intoÂ ggml-org:masterÂ fromÂ pwilkin:glm47fixrouterÂ Â 1 hour ago",
              "score": 4,
              "created_utc": "2026-01-21 13:27:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0unpi7",
                  "author": "EbbNorth7735",
                  "text": "What branch did OP post? Thats open",
                  "score": 2,
                  "created_utc": "2026-01-21 13:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vqzi7",
          "author": "VoidAlchemy",
          "text": "https://preview.redd.it/bybtp1ordqeg1.png?width=2087&format=png&auto=webp&s=1dca45a0946c83757bcde13ef614a13096d9f1fd",
          "score": 4,
          "created_utc": "2026-01-21 16:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w1sn9",
          "author": "customgenitalia",
          "text": "Iâ€™ve been trying to get glm 4.7 flash running in lmstudio but it rambles endlessly. Turns out lmstudio doesnâ€™t support specifying DRY_MULTIPLIER, which this model needs set to 1.1. Time to check out ollama!",
          "score": 4,
          "created_utc": "2026-01-21 17:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w8vuq",
              "author": "jacek2023",
              "text": "What's wrong with llama.cpp?",
              "score": 5,
              "created_utc": "2026-01-21 18:00:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0wl0yl",
                  "author": "customgenitalia",
                  "text": "As it turns out, nothing at all",
                  "score": 6,
                  "created_utc": "2026-01-21 18:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vpefu",
          "author": "Healthy-Nebula-3603",
          "text": "Yay!",
          "score": 1,
          "created_utc": "2026-01-21 16:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vspt2",
          "author": "JsThiago5",
          "text": "Does the q4 being a lot worse than q8 still stand?",
          "score": 1,
          "created_utc": "2026-01-21 16:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wmop1",
              "author": "TMTornado",
              "text": "I am running the unsloth UD Q4 quant and it's working flawlessly with claude code so far",
              "score": 2,
              "created_utc": "2026-01-21 19:00:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x4y4b",
          "author": "LicensedTerrapin",
          "text": "I am fascinated by the thinking logic of this model.i have not seen anything like this before.",
          "score": 1,
          "created_utc": "2026-01-21 20:23:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vlahg",
          "author": "SnooBunnies8392",
          "text": "Donâ€™t forget to turn off thinking, otherwise it will spend thousands of tokens overthinking even the simplest tasks",
          "score": -3,
          "created_utc": "2026-01-21 16:15:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vsjzu",
              "author": "NomadicHomebody21",
              "text": "Good suggestion, how can you turn off thinking if running via llama.cpp?",
              "score": 2,
              "created_utc": "2026-01-21 16:47:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0vyc4i",
                  "author": "Odd-Ordinary-5922",
                  "text": "dont turn of thinking on a thinking model unless you want it to have a lobotomy",
                  "score": 6,
                  "created_utc": "2026-01-21 17:13:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlzbhh",
      "title": "[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning & OpenAI-Compatible API",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/",
      "author": "blackstoreonline",
      "created_utc": "2026-01-24 21:21:50",
      "score": 303,
      "num_comments": 133,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nThe Qwen team just dropped **Qwen3-TTS**, and itâ€™s a significant step forward for local speech synthesis. If youâ€™ve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAIâ€™s TTS that you can actually run on your own hardware, this is it.\n\nWeâ€™ve put together a repository that provides an **OpenAI-compatible FastAPI server**, meaning you can use it as a drop-in replacement for any app already using OpenAIâ€™s TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.\n\n# Why this is a big deal:\n\n* **Insane Speed:** It features a dual-track hybrid architecture that hits \\~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.\n* **Natural Voice Control:** You don't just send text; you can give it natural language instructions like *\"Say this in an incredibly angry tone\"* or *\"A shaky, nervous 17-year-old voice\"* and it actually follows through.\n* **Easy Voice Cloning:** Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.\n* **OpenAI Drop-in:** Works natively with the OpenAI Python client. Just change your `base_url` to localhost.\n* **Multilingual:** Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).\n\n# Getting Started (The Quick Way)\n\nIf you have Docker and a GPU, you can get this running in seconds:\n\nBash\n\n    git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi\n    docker build -t qwen3-tts-api .\n    docker run --gpus all -p 8880:8880 qwen3-tts-api\n\n# Python Usage (OpenAI Style)\n\nPython\n\n    from openai import OpenAI\n    \n    client = OpenAI(base_url=\"http://localhost:8880/v1\", api_key=\"not-needed\")\n    \n    response = client.audio.speech.create(\n        model=\"qwen3-tts\",\n        voice=\"Vivian\",  # 9 premium voices included\n        input=\"This sounds way too human for a local model.\",\n        speed=1.0\n    )\n    response.stream_to_file(\"output.mp3\")\n\n# Technical Highlights\n\n* **Architecture:** It uses the new **Qwen3-TTS-Tokenizer-12Hz** for acoustic compression. It skips the traditional \"LM + DiT\" bottleneck, which is why the latency is so low.\n* **Model Sizes:** Available in **0.6B** (super fast/light) and **1.7B** (high fidelity) versions.\n* **VRAM Friendly:** Supports FlashAttention 2 to keep memory usage down.\n\n**Links to dive deeper:**\n\n* [ðŸ¤— Hugging Face Collection](https://huggingface.co/collections/Qwen/qwen3-tts)\n* [ðŸ“„ Research Paper on arXiv](https://arxiv.org/abs/2601.15621)\n* [ðŸ’» Github Repo](https://github.com/QwenLM/Qwen3-TTS)\n\nIâ€™m really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.\n\nLet me know if you run into any issues setting it up!\n\nhttps://preview.redd.it/sa9itpxw6dfg1.png?width=1280&format=png&auto=webp&s=7fe58c44a2d0b9d03a5bf099024f18752d48949d",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1ixlxx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-25 00:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i0aux",
          "author": "Fragrant_Dog6303",
          "text": "Holy shit 97ms latency is actually insane for local TTS. Been using tortoise-tts and it takes like 30 seconds just to say \"hello\" lmao\n\n  \nDefinitely trying this tonight, the voice cloning with just 3 seconds sounds too good to be true but if it actually works that's game changing for local setups",
          "score": 49,
          "created_utc": "2026-01-24 21:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ir29e",
              "author": "z_latent",
              "text": ">Been using tortoise-tts and it takes like 30 seconds just to say \"hello\" lmao\n\nmodel name checks out I suppose\n\njokes aside, recently we've seen weekly drops of new super-fast, low-latency TTS models, many by independent people. ~~just yesterday I saw~~ [~~this one~~](https://www.reddit.com/r/LocalLLaMA/comments/1ql7mav/removed_by_moderator/) ~~on the sub~~ welp apparently post got removed due to \"rule 4\", but if anyone's interested, it's called LuxTTS. it wasn't even that good iirc, but it still impresses me how many of those were trained by random people online with some GPUs, rather than larger labs.",
              "score": 20,
              "created_utc": "2026-01-24 23:36:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1iudwj",
              "author": "_raydeStar",
              "text": "Do you know if it can do CPU inference at that speed?",
              "score": 5,
              "created_utc": "2026-01-24 23:53:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1l0sbh",
                  "author": "AlwaysLateToThaParty",
                  "text": "It will definitely *not* have those speeds on CPU memory bandwidths.",
                  "score": 1,
                  "created_utc": "2026-01-25 08:03:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1kom3y",
              "author": "The_frozen_one",
              "text": "Just curious, why tortoise-tts and not something like [piper](https://github.com/OHF-Voice/piper1-gpl) or [kokoro](https://github.com/hexgrad/kokoro)? What does tortoise-tts do well?\n\nqwen3-tts is for sure worth trying out, I've had more fun with qwen3-tts than any other TTS system. I've gotten it to work and run through all the different features without much trouble at all.",
              "score": 4,
              "created_utc": "2026-01-25 06:22:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1kuyfu",
                  "author": "mpasila",
                  "text": "It used to be pretty decent (and had voice cloning) but it was very slow, though it had received some speedups but things like XTTSv2 kinda replaced it and then now we have stuff like Chatterbox.. that make tortoise pretty outdated. Kokoro and Piper don't have voice cloning support. Which is kinda nice to have usually for a TTS. Kyutai did also release a pretty small model around 100M params with voice cloning (pocket-tts) as well which is probably gonna be faster than Qwen3 TTS (0.6B and 1.7B params).",
                  "score": 2,
                  "created_utc": "2026-01-25 07:13:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1lfliz",
              "author": "Decaf_GT",
              "text": "If low latency is a concerrn, you should definitely check out Supertonic: https://huggingface.co/Supertone/supertonic-2\n\nI've run it on an M1 Max with 32GB of RAM and it's damn near instant. \n\nI haven't yet tried Qwen TTS but I will soon.",
              "score": 2,
              "created_utc": "2026-01-25 10:13:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21v8ko",
              "author": "Top-Rip-4940",
              "text": "i cant get RTF on any card ., 5090 or h100 even . using vllm or official python backend . can anyone help me please?? can anyone tell me how to get the claimed 0.9 RTF ??  please ...",
              "score": 1,
              "created_utc": "2026-01-27 17:27:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ks5gu",
              "author": "Bakoro",
              "text": ">the voice cloning with just 3 seconds sounds too good to be true \n\nIn theory, it's completely feasible. A person's voice is just a collection of frequencies/harmonics, and there just aren't *that* many meaningful frequencies for humans.  \n\nYou can find a person's vocal signature using signal processing techniques like a Fourier Transform. You can model a voice as a source and filter.  \n  \nAn AI model that's been trained on enough data would just learn a set of common basis signals and modulate them to fit.",
              "score": 1,
              "created_utc": "2026-01-25 06:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1livfq",
                  "author": "cheyyne",
                  "text": "> In theory, it's completely feasible. A person's voice is just a collection of frequencies/harmonics, and there just aren't that many meaningful frequencies for humans. \n\nI mean, yeah... It's just all the little subtleties and layers of implication, plus the effects of accents... Inflection in the language of origin... Regional dialects... You know. That kinda stuff.\n\nThe devil's in the details with these things.",
                  "score": 6,
                  "created_utc": "2026-01-25 10:42:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i54n1",
          "author": "SouthernFriedAthiest",
          "text": "Try it outâ€¦Iâ€™m hosting it for some testingâ€¦but so far dang amazing!\n\nTts.loser.com (I have it running for anyone to give it a whirl)",
          "score": 34,
          "created_utc": "2026-01-24 21:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ihb8o",
              "author": "hapliniste",
              "text": "The trump one completely rewrote my text to sound like Trump, is that an extra feature or is it the model itself? Was very funny in any case",
              "score": 14,
              "created_utc": "2026-01-24 22:45:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ihhnt",
                  "author": "SouthernFriedAthiest",
                  "text": "Ohh thatâ€™s a special feature just for Trumpy and Yoda ;)",
                  "score": 21,
                  "created_utc": "2026-01-24 22:46:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j5gdk",
              "author": "qazzq",
              "text": "What are you running this on? Generation seems fairly speedy. Also, im gonna have to try this out myself. \n\nHow does quality compare to other models btw, anyone deep into this and has an opinion theyd like to share?",
              "score": 3,
              "created_utc": "2026-01-25 00:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ku2my",
              "author": "pfn0",
              "text": "Lol at trumpy taking a complete shit over the input prompt. the rest of the voices don't really feel like they match who they are (unless the goal wasn't voice clone but intonation clone)",
              "score": 3,
              "created_utc": "2026-01-25 07:06:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1iq2gy",
              "author": "Sambojin1",
              "text": "Cheers. I'm going to run some slightly longer clips from the Ultima 7 transcript, just to see consistency.\n\nYep, Qwen TTS prompting is going to be another skill I'm going to have to learn. Still, this will be a goldmine for game mods and solo dev projects that want to do voice work. Like, we know it's going to be used for humour and deepfakes first, but it does have genuine ethical uses as well.",
              "score": 2,
              "created_utc": "2026-01-24 23:30:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1lo5lt",
              "author": "SocialDinamo",
              "text": "Really appreciate you hosting the demo!",
              "score": 2,
              "created_utc": "2026-01-25 11:29:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1o0808",
              "author": "GuideAxon",
              "text": "The Trump one is hilarious.",
              "score": 2,
              "created_utc": "2026-01-25 18:41:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1jqxvo",
              "author": "New_Jaguar_9104",
              "text": "volume could use some normalizing across them but otherwise pretty cool dude",
              "score": 1,
              "created_utc": "2026-01-25 02:49:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1jyu68",
                  "author": "SouthernFriedAthiest",
                  "text": "You are not wrongâ€¦it was a mix of samples (3 secs ) each from YouTube videos ..if one were to source quality samplesâ€¦I think it could be way better for sure",
                  "score": 1,
                  "created_utc": "2026-01-25 03:34:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1knd5u",
              "author": "rm-rf-rm",
              "text": "I get `TTS Generation Failed` error..",
              "score": 1,
              "created_utc": "2026-01-25 06:12:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sjbnn",
              "author": "thecosmingurau",
              "text": "It's not working",
              "score": 1,
              "created_utc": "2026-01-26 09:51:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uf72g",
              "author": "Additional-Sun-6083",
              "text": "The Trumpy filter made me laugh so much more than I thought it would.",
              "score": 1,
              "created_utc": "2026-01-26 16:34:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1jstqd",
              "author": "andero",
              "text": "That is exceptionally fast.  \n\nNot actually very good, at least not if those voice-options are trying to clone voice samples rather than follow text descriptions.  \n\nThe free and open Higgs Audio model that was put out quite some time ago was VASTLY better at voice cloning, though orders of magnitude slower.",
              "score": 1,
              "created_utc": "2026-01-25 02:59:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1iro5z",
          "author": "ubrtnk",
          "text": "Good work - just FYI, by default your Dockerfile wont work with Blackwell GPUs. \n\nI was able to get it to work by modifying a few things in the Dockerfile\n\n1) change your base image to nvidia/cuda:12.8.0-cudnn-runtime-ubuntu22.04\n\n2) change your index-url in the python dependencies to [https://download.pytorch.org/whl/cu128](https://download.pytorch.org/whl/cu128) (to match new image)\n\nOOH I also had to disable the Numba JIT caching\n\nhttps://preview.redd.it/xsjow9hgpdfg1.png?width=1017&format=png&auto=webp&s=9a145eabab651a0d185ca71d91fb1d08eecd79d1\n\nadding an environment variable to the compose file for the image you want to use (NUMBA\\_DISABLE\\_JIT=1) fixed that error\n\nI'm also attempting to see if I can get it running on a jetson orin nano super with GPU support -  just to see if I can ;)\n\nQuestion - where is the voice cloning section and where/how do I upload my audio file for cloning?",
          "score": 19,
          "created_utc": "2026-01-24 23:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j8shz",
              "author": "ubrtnk",
              "text": "Update - So I could get the container to compile and run on CPU but it was too big for the Jetson - maxed out ram 100% AND used almost 6G of SWAP. Could not get CUDA running even though I limited the execution only to GPU, installed the right version of onnxrutime-gpu and removed what I could find for any other CPU bound functions.",
              "score": 3,
              "created_utc": "2026-01-25 01:09:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1it9nx",
              "author": "andy2na",
              "text": "Thanks for figuring this out, can you provide the dockerfile so we can test it out? Tried to load from OP and it seems to just use system memory and CPU",
              "score": 2,
              "created_utc": "2026-01-24 23:47:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1iv0ds",
                  "author": "ubrtnk",
                  "text": "Sure:\n\nhttps://preview.redd.it/2dci3r8aydfg1.png?width=797&format=png&auto=webp&s=8152d4a74faf16a22ce928e1874e8ee93bf7b880\n\nI didnt change anything below the FastAPI and server dependencies component so didnt paste that section. I will say I also did change the docker compose file a bit to use my specific 5060Ti in my AI rig\n\nI changed it to device\\_ids: \\[\"2\"\\] so that way it only had access to that GPU, as labeled by nvtop or nvidia-smi",
                  "score": 3,
                  "created_utc": "2026-01-24 23:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ifx1k",
          "author": "Kindly-Annual-5504",
          "text": "Your description mentions streaming with low latency, but does it really support streaming audio? The model architecture does support streaming, but the currently published code on github does not, so that's why I'm asking.",
          "score": 14,
          "created_utc": "2026-01-24 22:38:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kpb2r",
              "author": "Tenemi",
              "text": "Looked into this FastAPI implementation - it doesn't actually stream. The /v1/audio/speech endpoint generates the full audio before returning anything. No StreamingResponse, no WebSocket, nothing chunked.\n\nI have a fork with actual streaming - WebSocket that sends audio chunks as tokens are generated, gets you first audio in \\~1.5s instead of waiting for the whole thing. SSE option too for sentence-by-sentence. Vibe coded with Claude Code, certainly not perfect but works for my use case until true streaming gets exposed for us... hopefully.\n\nBut none of this fixes the real problem: the model needs all the text upfront before it starts generating. You can stream the audio out, but you can't feed it text incrementally as your LLM produces it. That's baked into the transformer architecture, not something a server wrapper can solve.",
              "score": 6,
              "created_utc": "2026-01-25 06:27:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ntbi9",
                  "author": "Yorn2",
                  "text": "It's an old program, but AllTalk was able to get streaming working somehow. It used XTTS and even could do different voices for narration and quotes. It's really too bad the developer stopped updating it, I got the impression it was going to be the one best software for running offline TTS engines and would just add new ones as they came out. Instead, Chatterbox is seemingly now better for this. Even with Qwen3-TTS having interesting new features, I'm not sure it's going to be enough to replace existing software unless existing software adds it as a base engine.",
                  "score": 1,
                  "created_utc": "2026-01-25 18:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1l1fvu",
              "author": "amroamroamro",
              "text": "https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/docs/vllm-backend.md#comparison-vllm-vs-official-backend\n\n> Neither backend supports true audio streaming over HTTP currently. Both use OpenWebUI's chunk-based approach.\n> Chunk Streaming: Long text is split into chunks, each processed as a separate TTS request.\n\neven that is not true, the fastapi endpoint only generates full audio clips, no streaming or chunking...\n\nboth backends are used from an interface that only exposes `generate_speech` method which returns audio as one numpy array no streaming/chunking:\n\nhttps://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/api/backends/base.py#L33-L55\n\nhttps://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi/blob/main/api/routers/openai_compatible.py#L227-L250\n\nNot to criticize but this whole fastapi wrapper was clearly vibe coded, the part about it supporting streaming/chunking was just hallucinated",
              "score": 8,
              "created_utc": "2026-01-25 08:08:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1juizu",
              "author": "hellomistershifty",
              "text": "It's kind of silly, because he recommends vLLM for low latency but the vllm-omni readme says:\n\n\"While vLLM-Omni currently only supports offline inference (not true audio streaming), it can significantly speed up generation compared to the official backend.\"\n\nso yeah, it seems like we have to wait to get real streaming with this",
              "score": 6,
              "created_utc": "2026-01-25 03:09:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ins65",
              "author": "no_witty_username",
              "text": "my questions was same as the code doesnt seem to have true streaming but maybe im wrong. let me know if you get it working....",
              "score": 0,
              "created_utc": "2026-01-24 23:18:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ik3tx",
              "author": "blackstoreonline",
              "text": "This one does",
              "score": -6,
              "created_utc": "2026-01-24 22:59:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1jk0of",
                  "author": "Sweet_Albatross9772",
                  "text": "Does it really? Could you show where exactly? It seems you haven't changed Qwen's team inference code and their code does not support streaming yet...  \nLooking at the code you just slapped an API on top of it...",
                  "score": 11,
                  "created_utc": "2026-01-25 02:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1j4ajz",
          "author": "andy2na",
          "text": "thanks u/blackstoreonline got it working wiht u/ubrtnk dockerfile update for blackwell cards. How do we change it to use the 0.6 model? is there an env variable to select?",
          "score": 4,
          "created_utc": "2026-01-25 00:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kzpgu",
          "author": "FlowCritikal",
          "text": "Anyone get this working on ROCm?  Specifically would be interested in running this on Strix Halo",
          "score": 4,
          "created_utc": "2026-01-25 07:53:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l5xii",
          "author": "VampiroMedicado",
          "text": "It's insane how easy you can clone a voice, I just tried it with the 1.7B model.\n\nI said \"Hello, who is this? who is this?\" like you would on a phone call where the other sides doesn't answer, then I made it say \"Hello mom, I need help\" and it was very similar to my voice.",
          "score": 3,
          "created_utc": "2026-01-25 08:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mh8aa",
              "author": "xandep",
              "text": "You seem too familiar with how Brazilian scammers operate.. ðŸ˜†\n\nDon't worry, we are just months away from widespread use of this scamming technique.",
              "score": 2,
              "created_utc": "2026-01-25 14:40:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1idlkb",
          "author": "no_witty_username",
          "text": "Does this repo have the streaming code? cause I tried to get streaming working on the original repo and couldn't find any reference code for it as it all pointed toward gated gated api's?",
          "score": 8,
          "created_utc": "2026-01-24 22:27:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ik156",
              "author": "blackstoreonline",
              "text": "yes it does, I use it with open-webui",
              "score": -6,
              "created_utc": "2026-01-24 22:59:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ikyg0",
                  "author": "no_witty_username",
                  "text": "from codex and other agentic coding solutions, there telling me its not true streaming. as the rtf is above 1 and ttfs will be high. as in whole waveform has to be generated before you hear any output. are they hallucinating this? what is the rtf numbers and ttfs numbers you getting?",
                  "score": 5,
                  "created_utc": "2026-01-24 23:04:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1i30yr",
          "author": "Pentium95",
          "text": "Does It run on CPU too?",
          "score": 3,
          "created_utc": "2026-01-24 21:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ibmbd",
              "author": "OC2608",
              "text": "Interested to know this as well.",
              "score": 3,
              "created_utc": "2026-01-24 22:17:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ilvk3",
                  "author": "saul_karl",
                  "text": "Yes, just specify \"--device cpu --no-flash-attn\".\n\nOn my intel i5 13th gen mini pc, it takes about 15-30 mins to generate a 10 second clip. However the quality is good (I can't tell the difference between 0.6B and 1.7B models just by listening).",
                  "score": 5,
                  "created_utc": "2026-01-24 23:09:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1l92aw",
                  "author": "Fear_ltself",
                  "text": "Yes, it wasnâ€™t even that much slower than the GPU. 14 seconds for my 4070, 45 seconds for the cpu. Nowhere near the advertised speeds, and the quality was worse than kokoro 73m by a noticeable amount. Think Iâ€™ll try with flash attention on to see if that helps speed, because Iâ€™m not sure how other people are getting to run so fast",
                  "score": 1,
                  "created_utc": "2026-01-25 09:15:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1lcfa8",
              "author": "JackStrawWitchita",
              "text": "I've just spent time running it on CPU and it's painfully slow and quality is no better than Chatterbox or Vibvoice. I'm sticking with Chatterbox.",
              "score": 2,
              "created_utc": "2026-01-25 09:45:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r31uh",
          "author": "aschroeder91",
          "text": "It takes about 5 seconds to generate a small sample sentence. I am running using docker vllm server using flash attn on 3090 GPU using 0.6B model. \n\nhttps://preview.redd.it/mw3gus3y2mfg1.png?width=430&format=png&auto=webp&s=5d3e76e1d54b6c3746c462c0ee75e1c62491b2a6",
          "score": 3,
          "created_utc": "2026-01-26 03:17:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i7te8",
          "author": "umbs81",
          "text": "qwen with the Italian language does not perform well. Not very natural.",
          "score": 7,
          "created_utc": "2026-01-24 21:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k0hi5",
              "author": "BusRevolutionary9893",
              "text": "Only 1.0%-1.1% of the world's population can speak Italian and only 0.3%-0.5% of the world's population only speak Italian. In my opinion, any training on Italian was a waste. All focus should have been on English, then Chinese, then Spanish.Â Â \n\n\nThose 3 languages cover 30%-35% of the global population and there are 7,200 living languages.Â \n\n\nSome might argue Hindi instead of Spanish as there are slightly more people who speak Hindi but, while having more speakers, it is far more regionally concentrated and less used as a global second language, making it less likely to be part of high-frequency cross-language global interaction.",
              "score": 6,
              "created_utc": "2026-01-25 03:43:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ls9i0",
                  "author": "finkonstein",
                  "text": "Yes, and 90% of the world do not use Qwen-TTS at all, so we do not need it either",
                  "score": 3,
                  "created_utc": "2026-01-25 12:02:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1jxeg3",
          "author": "SmartCustard9944",
          "text": "You sure say 97ms a lot. Your AI generated post is not even mentioning on which hardware, so, your claim is completely pointless and unsubstantiated.",
          "score": 6,
          "created_utc": "2026-01-25 03:25:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jz240",
              "author": "hellomistershifty",
              "text": "97ms is from Qwen's[ model card](https://github.com/QwenLM/Qwen3-TTS):\n\n*Extreme Low-Latency Streaming Generation: Based on the innovative Dual-Track hybrid streaming generation architecture, a single model supports both streaming and non-streaming generation. It can output the first audio packet immediately after a single character is input, with end-to-end synthesis latency as low as 97ms, meeting the rigorous demands of real-time interactive scenarios.*\n\nThis repo doesn't support dual-track hybrid streaming (nothing local supports it right now) so you won't get latency nearly that low - you have to wait for the whole generation to finish",
              "score": 5,
              "created_utc": "2026-01-25 03:35:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihp7t",
          "author": "hapliniste",
          "text": "97ms but what does it mean exactly? In comfyui I tried it and it's way slower than real time, it takes around 100s to generate a 5-10s clip which I find strange for such a small model on a 3090.\n\nMaybe it's not optimize yet?",
          "score": 3,
          "created_utc": "2026-01-24 22:47:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ik8qj",
              "author": "blackstoreonline",
              "text": "Iâ€™ve got 3090 too and is waay faster than that, that comfyui node is clearly not optimized",
              "score": 1,
              "created_utc": "2026-01-24 23:00:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1itnqt",
                  "author": "aeroumbria",
                  "text": "The large voice design model still takes like 30s for a paragraph of text using a 3090 and the official repo. By latency do you mean running in streaming mode rather than full IO?",
                  "score": 1,
                  "created_utc": "2026-01-24 23:49:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1k1hgw",
          "author": "false79",
          "text": "Man - nothing but problems if running Windows with an AMD GPU :/\n\nSeems like this thing was built on CUDA only.",
          "score": 2,
          "created_utc": "2026-01-25 03:49:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mlkxj",
              "author": "deepspace_9",
              "text": "it did run in linux, 7900xtx. it took 30 seconds to generate audio.",
              "score": 1,
              "created_utc": "2026-01-25 15:02:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mu8uy",
                  "author": "false79",
                  "text": "Thx for the heads up. I have same GPU. I've been meaning to switch over for a while to linux.\n\nBut 30 seconds seems awfully long. I thought it was \"Ultra Low Latency (97ms)\"",
                  "score": 1,
                  "created_utc": "2026-01-25 15:43:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1s7tmn",
                  "author": "armored_strawberries",
                  "text": "I have the same GPU, but running on Window$\nI have so much stuff running for work, but I hate this freaking bloated spyware. Maybe that's the call I needed... lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:06:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1offr9",
          "author": "Lower_Journalist5500",
          "text": "I think this is a deceptive post. The server is deceiving. The curl log has a content-length header: 18764. This is a bad sign. If the server returns a content-length, it means it first generated the entire file, calculated its size, and only then started sending it. In real streaming (Chunked Transfer Encoding), the file size is not known in advance. Even considering that the request has \"stream\": True.  I'd be happy to receive a working streaming example.\n\nC:\\\\Qwen3TTS\\\\.venv\\\\Scripts\\\\python.exe C:\\\\Qwen3TTS\\\\main2.py\n\n\\>> Sending request...\n\nâš ï¸ WARNING: Server returned Content-Length (180524).\n\nThis means the server FIRST generated everything, and then started sending. There will be no streaming.\n\nProcess finished with exit code 0  \n  \n  \nLook at the code in api/routers/openai\\_compatible.py, lines 185-205 (function create\\_speech):\n\n\n\npython\n\n\\# Generate speech\n\n\\# HERE IS THE PROBLEM: There is an await here that waits for COMPLETE generation\n\naudio, sample\\_rate = await generate\\_speech(...) \n\n\n\n\\# Encode audio to requested format\n\naudio\\_bytes = encode\\_audio(audio, request.response\\_format, sample\\_rate)\n\n\n\n\\# ... creating a regular Response ...\n\nreturn Response(content=audio\\_bytes, ...)\n\nThis code ignores the stream=True parameter. It always generates the entire file, encodes it completely, and returns it as a whole.",
          "score": 2,
          "created_utc": "2026-01-25 19:47:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iha0k",
          "author": "Gudeldar",
          "text": "The voice cloning doesn't seem nearly as IndexTTS 2.",
          "score": 2,
          "created_utc": "2026-01-24 22:45:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j78lq",
          "author": "Tonyoh87",
          "text": "How does it compare to pocket-tts?",
          "score": 1,
          "created_utc": "2026-01-25 01:00:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1jfxs0",
              "author": "SouthernFriedAthiest",
              "text": "Pocket tts is amazing for size and speed etc.. cpu or gpu â€” but the qwen3tts family blows it away in performance\tand all it can doâ€¦but pocket is great for its use case.  (All depends on edge/or where you are generating your audioâ€¦)",
              "score": 1,
              "created_utc": "2026-01-25 01:48:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1jhu5k",
          "author": "Western_Team4433",
          "text": "97ms is very very very good. How well does it compare against the closed source models from elevenlabs?",
          "score": 1,
          "created_utc": "2026-01-25 01:59:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jmi2c",
          "author": "djtubig-malicex",
          "text": "While the speed is nice, I think the voice cloning aspect is still better with IndexTTS2",
          "score": 1,
          "created_utc": "2026-01-25 02:24:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k1ueg",
          "author": "altacct3",
          "text": "was waiting for something like this! thanks!",
          "score": 1,
          "created_utc": "2026-01-25 03:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k5wnp",
          "author": "ResponsiblePoetry601",
          "text": "Nice! Was able to generate around 2 min audio in 4 min with no cuda. Vivian sounds ok!",
          "score": 1,
          "created_utc": "2026-01-25 04:16:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1k8024",
              "author": "ResponsiblePoetry601",
              "text": "Besides English I also tried Portuguese and although itâ€™s more like Brazilian the quality is very good",
              "score": 1,
              "created_utc": "2026-01-25 04:30:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1k8nyo",
                  "author": "ResponsiblePoetry601",
                  "text": "Curious if itâ€™s good for real time will try later",
                  "score": 1,
                  "created_utc": "2026-01-25 04:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1k9et9",
          "author": "Decent-Opening-2113",
          "text": "I really like the model but without local streaming it's too slow for my project. Hopefully they'll release streaming for the repo sometime.",
          "score": 1,
          "created_utc": "2026-01-25 04:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kkuf3",
          "author": "127loopback",
          "text": "Hey can you do the same front end for Chroma\nhttps://github.com/FlashLabs-AI-Corp/FlashLabs-Chroma/tree/main/\n\nhttps://huggingface.co/FlashLabs/Chroma-4B\n\nhttps://www.flashlabs.ai/flashai-voice-agents",
          "score": 1,
          "created_utc": "2026-01-25 05:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kn4f0",
          "author": "rm-rf-rm",
          "text": "I tried the HF space and voice cloning is quite poor - you can re-run the exact same text and voice input and you get very different outputs - sometimes it sounds like a good clone and other times not even close. Is there a way to improve this?",
          "score": 1,
          "created_utc": "2026-01-25 06:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kntn2",
          "author": "OopsWrongSubTA",
          "text": "For a language like French, default voices sound too much english/chinese... custom voices also (it works better sometimes : is it possible to chose a seed?).\n\nBut cloning voices : insane quality!",
          "score": 1,
          "created_utc": "2026-01-25 06:16:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lylv8",
              "author": "AnusIingus",
              "text": "T'as rÃ©ussi Ã  mettre des Ã©motions avec la voix clonÃ©e ?",
              "score": 1,
              "created_utc": "2026-01-25 12:51:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kpajm",
          "author": "wichwigga",
          "text": "Is it possible to record PC audio and transcribe in real time with this model?",
          "score": 1,
          "created_utc": "2026-01-25 06:27:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xg2lm",
              "author": "Levy_Wilson",
              "text": "This is TTS, you need a STT like Whisper.",
              "score": 2,
              "created_utc": "2026-01-27 00:47:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yh41a",
                  "author": "wichwigga",
                  "text": "Ah I see thanks",
                  "score": 1,
                  "created_utc": "2026-01-27 04:14:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1krf6m",
          "author": "pfn0",
          "text": "Thanks for putting this UI together, it looks amazing. Seems like it's not ready to run out of the box on blackwell though, I'll have to hack on it to add support:\n\n```\nqwen3-tts-api  | NVIDIA RTX PRO 6000 Blackwell Workstation Edition with CUDA capability sm_120 is not compatible with the current PyTorch installation.\nqwen3-tts-api  | The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.\nqwen3-tts-api  | If you want to use the NVIDIA RTX PRO 6000 Blackwell Workstation Edition GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n```\n\nedit:\n\neasy fix though, just edit the docker FROM to point to the following for base and build, respectively:\n * nvidia/cuda:13.1.0-runtime-ubuntu24.04                                                                     \n * nvidia/cuda:13.1.0-devel-ubuntu24.04\n\nalso fetching cu130 from pytorch instead of cu121\n\nthen wait a year while flash-attn builds...",
          "score": 1,
          "created_utc": "2026-01-25 06:44:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kxmpr",
          "author": "rm-rf-rm",
          "text": "I see you make FastAPI based OpenAI API wrappers for all TTS/STT models. Is it possible to make a single wrapper package where you can pick and choose whatever STT/TTS model(s) you want?",
          "score": 1,
          "created_utc": "2026-01-25 07:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l2v77",
          "author": "Yorn2",
          "text": "With other TTS OpenAPI-compatible engines there's typically a /voices folder where I dump all my audio for cloning. Does this not support that or do we have to manually upload our voices for cloning somehow?",
          "score": 1,
          "created_utc": "2026-01-25 08:21:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lrt7y",
          "author": "Weak-Shelter-1698",
          "text": "how to turn of flash attention :) turing gpus aren't working.",
          "score": 1,
          "created_utc": "2026-01-25 11:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1luqyx",
          "author": "drivenkey",
          "text": "Trying this on windows 11, docker with 3090 - getting this error on flashattention - any help pls? \n\nhttps://preview.redd.it/2aldj4panhfg1.png?width=1127&format=png&auto=webp&s=341c0acc30578599fca7614de757199862287034",
          "score": 1,
          "created_utc": "2026-01-25 12:22:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1q80n0",
              "author": "No-Mall1142",
              "text": "I'm in the same boat, 4070.  Claude had me add it to the dockerfile, but still no go.",
              "score": 1,
              "created_utc": "2026-01-26 00:39:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1r5mxe",
              "author": "aschroeder91",
              "text": "often happens when you use the wrong docker tag. When you use docker build -t qwen3-tts-api:<tag> you should choose one that builds with flash-attn (the dockerfile has various tags)  \ntry using docker build -t qwen3-tts-api:production",
              "score": 1,
              "created_utc": "2026-01-26 03:32:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1xcbff",
              "author": "kenrock2",
              "text": "Gemini had me to use this command to install instead of the above version which gives me the same problem as yours. Below are the working one.\n\ndocker build --target production -t qwen3-tts-api .",
              "score": 1,
              "created_utc": "2026-01-27 00:28:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1m7vqq",
          "author": "mediali",
          "text": "The models are all excellent. It's truly generous of them to release such a product for free. Thank you for their contribution.",
          "score": 1,
          "created_utc": "2026-01-25 13:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nd2ty",
          "author": "studentofknowledg3",
          "text": "I have RTX 5080, 9950x3D, cannot install because of having CUDA 13, and cannot install \\`flash-attn\\`",
          "score": 1,
          "created_utc": "2026-01-25 17:05:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o0lap",
          "author": "Electronic-Metal2391",
          "text": "It is giving me garbage audio (many voices speaking at the same time) generation when using the VoiceDesign.",
          "score": 1,
          "created_utc": "2026-01-25 18:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rdku4",
          "author": "Exydosa",
          "text": "Is the 97ms latency from voice cloning, or is it only for the default text-to-speech model card?",
          "score": 1,
          "created_utc": "2026-01-26 04:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s5skx",
          "author": "getgoingfast",
          "text": "Thanks for sharing. Demo voices are rather dramatic, Uncle\\_flu, gonna be interesting.",
          "score": 1,
          "created_utc": "2026-01-26 07:48:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uf722",
          "author": "Ooothatboy",
          "text": "you should add the ability to add voices in the ui (for voice cloning similar to how [github.com/travisvn/chatterbox-tts-api](http://github.com/travisvn/chatterbox-tts-api) does it)",
          "score": 1,
          "created_utc": "2026-01-26 16:34:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xcu0p",
          "author": "kenrock2",
          "text": "https://preview.redd.it/emhppdb3dsfg1.png?width=992&format=png&auto=webp&s=83dd4afd0ea23790d658798f2828c137c916b590\n\n  \nThe generation of the default test speech took approximately 40 seconds, which is slightly slower than GPT-SoVITS. It is unclear whether this discrepancy is related to the utilization of cuda:0, but the process is indeed leveraging GPU resources.",
          "score": 1,
          "created_utc": "2026-01-27 00:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xjhtt",
              "author": "aschroeder91",
              "text": "i am also not able to get anywhere near this <100ms claim...",
              "score": 1,
              "created_utc": "2026-01-27 01:05:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xof1k",
                  "author": "kenrock2",
                  "text": "In my experience, GPT-SoVITS is significantly faster, generating responses in under 3 seconds for speech of similar length to the test, and its quality is sufficient for conversation.",
                  "score": 1,
                  "created_utc": "2026-01-27 01:32:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1y7bon",
          "author": "j3ss4u",
          "text": "I checked the demo on huggingface a few days ago and it had way more default faces, now they're gone. Does anyone know what happened to them? Can I have those if I run this locally?",
          "score": 1,
          "created_utc": "2026-01-27 03:15:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp9m3",
          "author": "Pretend-Umpire-3448",
          "text": "what's the minimum hardware requirement to run this locally? thank you",
          "score": 1,
          "created_utc": "2026-01-27 05:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o216xk7",
          "author": "Gambikules",
          "text": "audio source to audio target possible ? or only text to audio",
          "score": 1,
          "created_utc": "2026-01-27 15:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21v734",
          "author": "Top-Rip-4940",
          "text": "i cant get RTF on any card ., 5090 or h100 even . using vllm or official python backend . can anyone help me please?? can anyone tell me how to get the claimed 0.9 RTF ??  please ...",
          "score": 1,
          "created_utc": "2026-01-27 17:26:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i16wn",
          "author": "CatEatsDogs",
          "text": "Does it support intel igpu?",
          "score": 1,
          "created_utc": "2026-01-24 21:28:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i50z9",
              "author": "literateu",
              "text": "Yes, see the repo.\n\n\\`\\`\\`  \n\\# Build CPU-only variant\n\ndocker build -t qwen3-tts-api-cpu --target cpu-base .\n\ndocker run -p 8880:8880 qwen3-tts-api-cpu\n\n\n\n\\# Or use Docker Compose\n\ndocker-compose --profile cpu up qwen3-tts-cpu  \n\\`\\`\\`",
              "score": 0,
              "created_utc": "2026-01-24 21:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ksvs5",
                  "author": "CatEatsDogs",
                  "text": "Can you spot the difference between \"cpu-only\" and \"intel igpu\"? I was asking about igpu not cpu.",
                  "score": 1,
                  "created_utc": "2026-01-25 06:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kju8r",
          "author": "Eastern_Rock7947",
          "text": "I have found it slow on a 3080 ti 180secs for 46 secs of audio. However the quality is really good.",
          "score": 1,
          "created_utc": "2026-01-25 05:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ieynl",
          "author": "Lostronzoditurno",
          "text": "Failed to initialize TTS model: cannot cache function '\\_\\_o\\_fold': no locator available for file '/opt/venv/lib/python3.11/site-packages/librosa/core/notation.py'\n\nJust cloned the repo and runned it with docker compose up qwen3-tts-gpu  \nI'm using NVIDIA",
          "score": 0,
          "created_utc": "2026-01-24 22:34:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ikd2e",
              "author": "blackstoreonline",
              "text": "sorry Iâ€™ve just fixed that error, please do git pull and try again",
              "score": 1,
              "created_utc": "2026-01-24 23:01:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1knml6",
          "author": "The_frozen_one",
          "text": "This is definitely the best local TTS I've used for voice cloning. It also passes my \"can I get this working in a reasonable amount without having to type `pip` more than 10 times\" test.",
          "score": -3,
          "created_utc": "2026-01-25 06:14:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1i8iw1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -18,
          "created_utc": "2026-01-24 22:02:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ia1a0",
              "author": "weasl",
              "text": "thanks chatgpt",
              "score": 15,
              "created_utc": "2026-01-24 22:10:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjota",
      "title": "I built a \"hive mind\" for Claude Code - 7 agents sharing memory and talking to each other",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "author": "Historical-Celery-83",
      "created_utc": "2026-01-26 15:49:13",
      "score": 299,
      "num_comments": 62,
      "upvote_ratio": 0.87,
      "text": "Been tinkering with multi-agent orchestration and wanted to share what came out of it.\n\n\n\n\\*\\*The idea\\*\\*: Instead of one LLM doing everything, what if specialized agents (coder, tester, reviewer, architect, etc.) could coordinate on tasks, share persistent memory, and pass context between each other?\n\n\n\n\\*\\*What it does\\*\\*:\n\n\\- 7 agent types with different system prompts and capabilities\n\n\\- SQLite + FTS5 for persistent memory (agents remember stuff between sessions)\n\n\\- Message bus for agent-to-agent communication\n\n\\- Task queue with priority-based coordination\n\n\\- Runs as an MCP server, so it plugs directly into Claude Code\n\n\\- Works with Anthropic, OpenAI, or Ollama\n\n\n\n\\*\\*The cool part\\*\\*: When the coder finishes implementing something, the tester can query the shared memory to see what was built and write appropriate tests. The reviewer sees the full context of decisions made. It's not magic - it's just passing data around intelligently - but it feels like they're actually collaborating.\n\n\n\n\\*\\*The not-so-cool part\\*\\*: Debugging 7 agents talking to each other is... an experience. Sometimes they work beautifully. Sometimes one agent keeps assigning tasks to itself in an infinite loop. You know, typical multi-agent stuff.\n\n\n\n\\*\\*Stack\\*\\*: TypeScript, better-sqlite3, MCP SDK, Zod\n\n\n\nNot enterprise-ready. Not trying to compete with anything. Just an experiment to learn how agent coordination patterns work.\n\n\n\nMIT licensed: [github.com/blackms/aistack](http://github.com/blackms/aistack)\n\n\n\nHappy to answer questions or hear how you're approaching multi-agent systems.\n\n",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v6rcq",
          "author": "Zc5Gwu",
          "text": "Why does this have 100+ upvotes but only one comment?",
          "score": 39,
          "created_utc": "2026-01-26 18:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yvghg",
              "author": "Semi_Tech",
              "text": "looks like another vibe coded program in Claude code + paid upvotes just to gain visibility :P",
              "score": 15,
              "created_utc": "2026-01-27 05:52:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vcvkk",
              "author": "kevin_1994",
              "text": "this happens a lot in this sub",
              "score": 19,
              "created_utc": "2026-01-26 18:58:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vfj85",
                  "author": "Historical-Celery-83",
                  "text": "annoying... I want challenge :)",
                  "score": -52,
                  "created_utc": "2026-01-26 19:09:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2405xg",
              "author": "drallcom3",
              "text": "He paid for upvotes to promote his Github.",
              "score": 2,
              "created_utc": "2026-01-27 23:10:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v9imm",
              "author": "Historical-Celery-83",
              "text": "good question! comment it!",
              "score": -47,
              "created_utc": "2026-01-26 18:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ubcfn",
          "author": "robiinn",
          "text": "How does it differ from [bmad method](https://github.com/bmad-code-org/BMAD-METHOD) or something like that? Sounds very similar.",
          "score": 14,
          "created_utc": "2026-01-26 16:18:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ula9z",
              "author": "Historical-Celery-83",
              "text": "good question, first time I see that project, seems like is very Agile oriented. Mine is more about agent swarm",
              "score": -16,
              "created_utc": "2026-01-26 17:00:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v9igo",
          "author": "No_Afternoon_4260",
          "text": "The question is do they agree with each other?",
          "score": 7,
          "created_utc": "2026-01-26 18:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v9uzx",
              "author": "Historical-Celery-83",
              "text": "sometimes, and sometimes is a total mess. the orchestrator struggle to keep the agents on tracks, I'm trying to add more guardrails and more deterministic code.",
              "score": 8,
              "created_utc": "2026-01-26 18:45:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x9n7l",
                  "author": "Environmental-Metal9",
                  "text": "First of all, my dude, please edit away the LLMisms before posting. Itâ€™s grating to be fixing llm code all day just to come here and read more llm slop.\n\nAnd also, for cohesion, what is the actual disparity? Get the agents to agree on intent or on response? How many tokens can you waste per call?\n\nI havenâ€™t worked on anything like this EXACTLY, but Iâ€™ve worked on have to reach eventual consistency from agents running different models, and because of how my data was shaped, I was able to train a T5 model as classifier and would run that in front of every response. If the response didnâ€™t pass classification Iâ€™d just regenerate the response",
                  "score": 7,
                  "created_utc": "2026-01-27 00:14:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vg7ug",
          "author": "JellyBean504",
          "text": "This is awesome, are you familiar with steve yegge's gastown? it seems very similar.",
          "score": 3,
          "created_utc": "2026-01-26 19:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgvaq",
              "author": "Historical-Celery-83",
              "text": "no, but I'm looking at it right now, is more evolved compared to mine, I will take some idea from there. seems very interesting and more scalable compared to my solution.",
              "score": 3,
              "created_utc": "2026-01-26 19:15:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vxcfp",
          "author": "nonerequired_",
          "text": "That was exactly what I wanted to start building. What a coincidence!",
          "score": 3,
          "created_utc": "2026-01-26 20:26:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207yzc",
              "author": "epyctime",
              "text": "it took op 2 days and like 90 commits to feel comfortable enough to submit this to the public. so just open claude code and start vibing man",
              "score": 1,
              "created_utc": "2026-01-27 12:40:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1whuve",
          "author": "Far-Low-4705",
          "text": "do you have any real performance benchmarks?",
          "score": 3,
          "created_utc": "2026-01-26 21:57:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wie4c",
              "author": "Historical-Celery-83",
              "text": "No, I started to write it like 2 days ago.",
              "score": 0,
              "created_utc": "2026-01-26 22:00:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wmwbw",
                  "author": "Far-Low-4705",
                  "text": "then how do you know that this even works?",
                  "score": 7,
                  "created_utc": "2026-01-26 22:21:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xynxz",
                  "author": "jonas-reddit",
                  "text": "Maybe work on it a bit more than 2 days, I assume it was vibe coded as well, do some testing and then share when thereâ€™s a bit more to tell.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:28:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w6xuu",
          "author": "segmond",
          "text": "Good job, not a new problem, microsoft first released solution for multi agent about 2 years or more.  I played a lot with it then.   Check it out, you might gain some new ideas\n\n[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)",
          "score": 5,
          "created_utc": "2026-01-26 21:09:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1we8q8",
          "author": "autodidacticasaurus",
          "text": "A few more iterations and we'll have created the Geth.\n\nIsn't that what DeepSeek does internally?",
          "score": 2,
          "created_utc": "2026-01-26 21:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1werol",
              "author": "Historical-Celery-83",
              "text": "yes, also claude code, but at least I can have control here. and use multi model for different tasks.",
              "score": 2,
              "created_utc": "2026-01-26 21:44:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wfyaj",
                  "author": "autodidacticasaurus",
                  "text": "I think that's cool. Also, it's easier to distribute across multiple cards or machines. Much more flexible.",
                  "score": 1,
                  "created_utc": "2026-01-26 21:49:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o207vp5",
          "author": "epyctime",
          "text": "\"some shit i started working on 3 days ago thats fully vibe coded slop so i can get clout on my github profile\"  \nedit: OP blocked me lol so i cant reply but this guys a fucking loon, 90 commits of claude code slop and its better than claude-flow \"The leading agent orchestration platform for Claude\" with 5.2k commits, but yeah, it's not fully implemented but yours is. \"your ignorance is the same as your ego\" then IMMEDIATELY commented \"probably better than 99% of the code that you will write in your entire life\" is the funniest thing ive ever read in my life. if I have access to opus we are coding the same my boy",
          "score": 2,
          "created_utc": "2026-01-27 12:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o240mak",
              "author": "drallcom3",
              "text": "The whole Github page reeks of someone trying to scam.",
              "score": 2,
              "created_utc": "2026-01-27 23:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20luzm",
              "author": "Historical-Celery-83",
              "text": "probably better than 99% of the code that you will write in your entire life.",
              "score": 0,
              "created_utc": "2026-01-27 13:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wcdt9",
          "author": "__Maximum__",
          "text": "Why claude code? For fucks sake, why? There are open source alternatives that you can contribute to.",
          "score": 5,
          "created_utc": "2026-01-26 21:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yis59",
              "author": "rocketmonkeys",
              "text": "I'm just getting into things, what do you use?  Opencode?",
              "score": 1,
              "created_utc": "2026-01-27 04:24:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zgggt",
                  "author": "__Maximum__",
                  "text": "Opencode, openhands, vibe even gemini cli is open source.",
                  "score": 1,
                  "created_utc": "2026-01-27 08:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wu7sz",
              "author": "Historical-Celery-83",
              "text": "claude code can be used but it supports more providers, I choosed claude code because is one of my favourite. But I would like to be agnostic in the future. Already working on it.",
              "score": 1,
              "created_utc": "2026-01-26 22:55:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zgdjj",
                  "author": "__Maximum__",
                  "text": "It supports more providers than opencode?",
                  "score": 1,
                  "created_utc": "2026-01-27 08:51:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1x4o0x",
          "author": "the_ai_wizard",
          "text": "Cool diy. This approach has been tried and has failure modes just like you have experienced. It sounds brilliant but so far ultimately goes nowhere. Maybe this will be interesting to you:\n\nhttps://neurips.cc/virtual/2024/106556",
          "score": 1,
          "created_utc": "2026-01-26 23:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xfdij",
          "author": "foundrynet",
          "text": "This is the kind of infrastructure the agent space needs. Most multi-agent demos fall apart because there's no good way for agents to coordinate and remember state.\n\nOne thing I've been exploring: what if agents could earn/spend resources for completing tasks? Creates natural prioritization where high value work gets done first.",
          "score": 1,
          "created_utc": "2026-01-27 00:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215hx5",
          "author": "fugogugo",
          "text": "I just watched this video today and it looks like similar situation lol\n\nhttps://youtu.be/U7s_CaI93Mo?si=_eGWY6QG9Ni7Qyr7",
          "score": 1,
          "created_utc": "2026-01-27 15:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21twyh",
          "author": "PhotographerUSA",
          "text": "So, now no more programming mistakes on first try?",
          "score": 1,
          "created_utc": "2026-01-27 17:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22b67b",
          "author": "isopropoflexx",
          "text": "I am curious to find out how you are handling multiple agents accessing the SQLite database file concurrently? I personally love SQLite as a quick and easy way to incorporate a db into small/simple projects, as it doesn't need a \"proper\" service of its own to run. But... SQLite is not designed with concurrency in mind (same as any other \"flat file\" type storage solution), and having multiple concurrent connections typically results in errors due to db/table locks etc.",
          "score": 1,
          "created_utc": "2026-01-27 18:35:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ypit6",
          "author": "SeleneGardenAI",
          "text": "The voting patterns here are wild sometimes - posts with genuinely interesting architectures get buried while others mysteriously rocket up with no discussion. It's frustrating when you want to dig into the technical details.\n\nI've been experimenting with multi-agent memory sharing lately, and the biggest challenge I've found isn't the inter-agent communication itself, but preventing memory contamination between agents with different roles. When agents share context, they tend to blur their distinct personalities and functions over time. What's your approach to maintaining agent identity boundaries while still allowing meaningful information exchange? Are you using separate embedding spaces for shared vs. private memories, or handling it at the prompt level?\n\nThe \"hive mind\" concept is fascinating because it mirrors how human teams actually work - shared context with specialized roles. But LLMs weren't really designed for this kind of persistent, multi-perspective memory architecture. Would love to hear more about your implementation details, especially how you're handling memory prioritization when the shared context gets large.",
          "score": 1,
          "created_utc": "2026-01-27 05:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z5ynx",
              "author": "Historical-Celery-83",
              "text": "    Great questions - these are exactly the things I've been wrestling with.\n    \n    On memory contamination / identity boundaries: my current approach is pretty simple, honestly. Namespaces + prompt-level isolation. Each agent has a strong system prompt defining its role and capabilities, and they access shared memory via namespace filtering. The key insight is that agents don't actually \"retain\" personality across calls - they're stateless. What persists is the memory content, not the agent's perspective on it.\n    \n    So when the coder stores something like memory_store(\"auth-decision\", \"Using JWT with refresh tokens\", {namespace: \"architecture\"}), the reviewer later queries this and interprets it through its own system prompt lens. There's no embedding space separation - it's the same FTS5 index. The \"identity boundary\" is entirely at the prompt level.\n    \n    Is this ideal? Probably not. But it sidesteps the contamination problem because agents don't actually share state - they share facts. The interpretation happens fresh each time.\n    \n    On memory prioritization: hybrid approach. FTS5 with BM25 ranking for keyword relevance, optional vector search (OpenAI/Ollama embeddings) for semantic similarity, then merge with deduplication where vector results come first (higher quality), then FTS backfill. The \"prioritization\" is really just semantic match > keyword match > recency as tiebreaker.\n    \n    For large contexts I'm honestly just relying on the LLM's attention mechanism. I send relevant search results, not the entire memory. If context gets too big, that's a sign the search query wasn't specific enough.\n    \n    What I'd do differently if this were production: per-agent memory scopes (private + shared pools), decay functions for old memories, explicit \"handoff\" objects when one agent passes to another. But for an experiment, the current approach works well enough to see the patterns emerge. The agents do coordinate surprisingly well when you give them structured ways to leave breadcrumbs for each other.\n    \n    What approaches are you exploring for the contamination problem?",
              "score": 0,
              "created_utc": "2026-01-27 07:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vmotm",
          "author": "Magnus_Forsling",
          "text": "The infinite loop problem you mentioned is fascinating â€” it's basically the multi-agent equivalent of an LLM talking itself in circles. Have you experimented with any circuit-breaker patterns? Something like tracking task lineage so an agent can't pick up a task that descended from one it created?\n\nThe SQLite+FTS5 choice for shared memory is smart. Curious if you've hit scaling issues as the memory grows, or if the FTS5 indexing keeps retrieval snappy even with longer sessions.",
          "score": -4,
          "created_utc": "2026-01-26 19:40:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vsfq9",
              "author": "Historical-Celery-83",
              "text": "No, you mean an anti loop system? or something more evolved?",
              "score": 1,
              "created_utc": "2026-01-26 20:05:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1w6lii",
                  "author": "Magnus_Forsling",
                  "text": "Bit of both, actually. A basic anti-loop system catches the obvious stuff â€” \"agent A spawned task B which spawned task C which looks suspiciously like A again.\" But the more interesting patterns are subtler:\n\n1. **Semantic drift detection** â€” task descriptions mutate slightly each iteration until they're technically different but functionally identical. You'd want embedding similarity checks against the task's ancestors, not just string matching.\n\n2. **Resource exhaustion triggers** â€” if an agent has touched >N files or made >M API calls without producing a deliverable, something's probably wrong. Hard to tune but catches the \"productively going nowhere\" loops.\n\n3. **Consensus checkpoints** â€” for high-stakes tasks, require a different agent to sign off before the originating agent's subtasks can spawn their own children. Adds latency but breaks the self-reinforcing cycles.\n\nThe lineage tracking is the foundation though. Without knowing *where* a task came from, you can't catch any of these patterns. Are you storing that in the SQLite schema already?",
                  "score": -2,
                  "created_utc": "2026-01-26 21:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qoty38",
      "title": "Kimi K2.5 costs almost 10% of what Opus costs at a similar performance",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xz7okply3zfg1.png",
      "author": "Odd_Tumbleweed574",
      "created_utc": "2026-01-27 23:10:16",
      "score": 295,
      "num_comments": 67,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o269wrx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 07:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o257yrq",
          "author": "one-wandering-mind",
          "text": "It used 3x the tokens that opus does for the same tasks so cheaper, but more like 3x cheaper than 10x cheaper.Â \n\n\nThese models often use a dramatically different number of tokens to do the same thing. It should be considered for both cost and latency when you compare them.Â \n\n\nI've heard great things about the kimi models especially the last version for writing.Â \n\n\nÂ https://artificialanalysis.ai/#cost-to-run-artificial-analysis-intelligence-index",
          "score": 54,
          "created_utc": "2026-01-28 02:57:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2472ya",
          "author": "TAW56234",
          "text": "If I had a nickel for every time someone claimed the newest OSS Sota model was similar to Claude, I could generate a few prompts.",
          "score": 229,
          "created_utc": "2026-01-27 23:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gq5f",
              "author": "ForsookComparison",
              "text": "I use closed weight models at work (required) and open weight models for side projects (cost). Several hours of agent use per day.\n\nI think open weight is approaching **Sonnet 3.7** if I'm totally honest. I love this community to death but it gets drunk off of bar charts and one-shots. Sonnet 3.7 for Kimi and Deepseek prices is amazing, but *\"approaching Opus 4.5\"* just flags for me that nobody is using these things for hours at a time.",
              "score": 73,
              "created_utc": "2026-01-28 03:46:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o260qbh",
                  "author": "peculiarMouse",
                  "text": "I used Max extensively for multiple months and I tend to disagree. Claude is best at writing code, but architecture, hallucinations and creativity make it highly debatable in all other things that matter about code. \n\nGemini Pro for instance has INSANE, borderline dementia levels hallucinations, where it necessitates undoing generation and redoing with something else, but its context window and knowledge is unparalleled.\n\nClaude is very fixed on things it thinks it knows how to do. For example old libraries that have long deprecated documentation. GLM with same Claude .md has much lower rate for ignoring it altogether. It goes as far as making your code worse through unrequested changes.\n\nI would easily put Claude first as agentic powerhouse that comes with your subscription.  \nBut I tend to use other models a lot, when working with Claude, so I'd put them on Sonnet 4.6 level easily.\n\nClaude just has very good understanding of what's most important to 95% devs.",
                  "score": 15,
                  "created_utc": "2026-01-28 05:57:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25q46f",
                  "author": "KaroYadgar",
                  "text": "What do you use them for? LLMs are very spike-y so I'd like to know at what specific tasks you think they're comparable to Sonnet 3.7. Are there any tasks you think they are comparable or better at?",
                  "score": 11,
                  "created_utc": "2026-01-28 04:43:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25qhvs",
                  "author": "Virtamancer",
                  "text": "Have you used any of the current-gen top open models in OpenCode? Like, the absolute newest version of deepseek/glm/kimi/qwen/whatever, like within the last 7 days or whenever they released.\n\nIâ€™m curious how they compare running through OpenCode vs Claude Code with Opus 4.5 or Codex with GPT-5.2 set to Extra High reasoning.",
                  "score": 2,
                  "created_utc": "2026-01-28 04:46:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26h5zi",
                  "author": "FyreKZ",
                  "text": "I think you're misremembering how dumb 3.7 was lol. K2.5 is realistically probably not far off Sonnet 4.5, Opus is a stretch.",
                  "score": 1,
                  "created_utc": "2026-01-28 08:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o252exg",
              "author": "Zc5Gwu",
              "text": "Generate prompts with kimi or claude though, thatâ€™s the question.",
              "score": 24,
              "created_utc": "2026-01-28 02:28:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24694d",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 70,
          "created_utc": "2026-01-27 23:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qbtx",
              "author": "DistanceSolar1449",
              "text": "Almost certainly not. [Estimates for Claude Opus 4.5 put it at 1.6T-3T params, 160b active](https://news.ycombinator.com/item?id=46038512). \n\nThat would make Kimi roughly 1/2 to 1/4 the size.",
              "score": 7,
              "created_utc": "2026-01-28 04:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24ewv8",
              "author": "neotorama",
              "text": "I would say almost similar. Opus has been degrading last 3 weeks",
              "score": -11,
              "created_utc": "2026-01-28 00:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24rlr0",
                  "author": "eli_pizza",
                  "text": "Opus hasnâ€™t changed.",
                  "score": 16,
                  "created_utc": "2026-01-28 01:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o24fo78",
              "author": "Healthy-Nebula-3603",
              "text": "Yes \n\nIs very similar",
              "score": -9,
              "created_utc": "2026-01-28 00:29:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24q0lj",
                  "author": "Bob_Fancy",
                  "text": "Iâ€™d bet a good amount that itâ€™s not.",
                  "score": 12,
                  "created_utc": "2026-01-28 01:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o24byuf",
          "author": "NighthawkT42",
          "text": "It's good, but not really the same level.",
          "score": 33,
          "created_utc": "2026-01-28 00:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e4mh",
              "author": "ihexx",
              "text": "yeah, it's closer to sonnet or gemini 3 flash",
              "score": 0,
              "created_utc": "2026-01-28 07:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24e4e0",
          "author": "ghulamalchik",
          "text": "I'm just gonna wait for DeepSeek 4 and MiniMax M2.2\n\nI trust those from experience.\n\nI used many models in cline and DS and MiniMax were my favorite.",
          "score": 29,
          "created_utc": "2026-01-28 00:21:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gwjm",
              "author": "epyctime",
              "text": "you prefer them to glm4.7?",
              "score": 5,
              "created_utc": "2026-01-28 03:47:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25qbup",
                  "author": "ghulamalchik",
                  "text": "I haven't used GLM 4.7 a lot to form a comprehensive opinion, however when I tried it it was *slightly* worse than MiniMax M2.1 for my use cases (programming desktop GUIs). MiniMax was faster and slightly better. DeepSeek was nice when it came to debugging big issues, it's about the same in terms of general intelligence, it's much slower than MiniMax though.\n\nTL;DR\n- DS was smart but slow, good for debugging and planning big changes. It's also very cheap. The main downside is that it's slower.\n- MiniMax is good a default.\n- GLM 4.7 is in the same ballpark in terms of performance but MiniMax felt a little better for me personally.\n\nI think it all depends on what you use the models for. There's no answer that fits all.",
                  "score": 6,
                  "created_utc": "2026-01-28 04:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25c7v4",
              "author": "Zianiwarhead",
              "text": "I strongly agree.",
              "score": 2,
              "created_utc": "2026-01-28 03:21:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25wjgj",
              "author": "deadcoder0904",
              "text": "What are u using DS & MiniMax for specifically? Just coding? Frontend or backend?",
              "score": 1,
              "created_utc": "2026-01-28 05:26:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25xqoj",
                  "author": "ghulamalchik",
                  "text": "Yeah just coding. I'm mainly doing desktop GUIs in Qt (full programs not just the GUI part).",
                  "score": 2,
                  "created_utc": "2026-01-28 05:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25coe3",
              "author": "Peetlin",
              "text": "same here it's not getting enough love. it's fast and intelligence and agentic capability. it has brain oof 80% of gpt5.2 but that's enough for me",
              "score": 2,
              "created_utc": "2026-01-28 03:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25snrk",
          "author": "dubesor86",
          "text": "this assumes a ton of input and will swing widely depending on use case. for me, the bulk of the cost is always the model output.\n\nin my general benchmark the cost was:\n\nKimi-K2.5 (reasoning) $1.60\n\nClaude Opus 4.5 $2.75\n\n= 42% cheaper\n\n\nin my chess benchmark the game cost was:\n\nKimi-K2.5 (reasoning) $0.87\n\nClaude Opus 4.5 $0.46\n\n= 89% more expensive\n\nAlso, obviously the performance is not \"similar\" level if you actually used these models, despite what some bars tell you.",
          "score": 11,
          "created_utc": "2026-01-28 04:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25tibf",
          "author": "Torodaddy",
          "text": "*absolutely not the same performance*",
          "score": 10,
          "created_utc": "2026-01-28 05:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e67e",
              "author": "rm-rf-rm",
              "text": "based on prejudice or something more concrete? TBH I dont believe its not same either, but Im asking what your basis is for clarity",
              "score": 1,
              "created_utc": "2026-01-28 07:46:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2458eb",
          "author": "TransportationSea579",
          "text": "Is the similar performance in the room with us?",
          "score": 61,
          "created_utc": "2026-01-27 23:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24lm0k",
              "author": "Howdareme9",
              "text": "It never is lol.",
              "score": 16,
              "created_utc": "2026-01-28 00:59:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2486zq",
              "author": "illforgetsoonenough",
              "text": "K2.5 is sitting on the chair in the corner of the room",
              "score": 6,
              "created_utc": "2026-01-27 23:51:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24fur6",
              "author": "Healthy-Nebula-3603",
              "text": "From what I tested and what I saw on YouTube.... Very similar to opus 4.5 in coding.",
              "score": -8,
              "created_utc": "2026-01-28 00:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25d942",
          "author": "galambalazs",
          "text": "It's more fair to compare to sonnet 4.5",
          "score": 5,
          "created_utc": "2026-01-28 03:26:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o246yjt",
          "author": "LoveMind_AI",
          "text": "Kimi in the thinking era is VERY hit or miss. The hits are amazing. The misses are sad, because when itâ€™s on, it really gives the SOTA a run for its money. I still think MiniMax M2 is the best LLM outside of the West.",
          "score": 18,
          "created_utc": "2026-01-27 23:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24vphu",
          "author": "Recoil42",
          "text": "Does it use the same number of tokens? I doubt it.",
          "score": 5,
          "created_utc": "2026-01-28 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o250252",
              "author": "electronicsoul",
              "text": "Exactly, and what's being cached in those token counts for a much cheaper rate and therefore faster inference is what people seem to forget. That's a big part of what makes Claude feel a level above the rest.",
              "score": 3,
              "created_utc": "2026-01-28 02:16:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o264z3l",
          "author": "TheInfiniteUniverse_",
          "text": "obviously this would be fantastic if true. this is quite obviously not true, still way behind Opus and GPT but getting there....",
          "score": 3,
          "created_utc": "2026-01-28 06:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25x2iy",
          "author": "Middle_Bullfrog_6173",
          "text": "If you just go by benchmarks, Genini 3 Flash also has similar performance for 10% cost. In reality for some use cases there's much more difference at the top than a few points suggest. And some tasks are not really captured by benchmarks.\n\n\nLimited testing so far, but K2.5 has the large model feature of great niche performance, like low resource languages. But it seems to lose coherence earlier than the big closed models as context fills up. Can't really say anything more specific yet.",
          "score": 3,
          "created_utc": "2026-01-28 05:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260dkg",
          "author": "Dazzling_Focus_6993",
          "text": "as someone who using heavily using these tools for real world tasks, I would say opus 4.5 is another level, despite benchmarks implying gpt 5.2 and gemini 3 are very close. I say the gap is huge. I cannot use any other model for relatively complex tasks. \n\nI use opensource models (e.g., gpt-oss, kimi and qwen) for relatively simple tasks mainly due to the cost. I look forward to trying kimi 2.5 but, to be frank, i do not have high hopes. I think i will continue to use my combination of opus 4.5 and cheap OS models. \n\nPS: I select models through Kilo Code\n\nAdditionally: For gemini 3, it is the opposite. I do not have any use cases for gemini 3 despite benchmarks look very high.",
          "score": 3,
          "created_utc": "2026-01-28 05:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26iuob",
              "author": "Front_Eagle739",
              "text": "weird. I use codex 5.2high and claude code side by side and both are better at different things. If I had to pick a smarter one i'd nudge to 5.2 high. Its just more likely to one shot fix a problem and less likely to get lost in circles. Claude and opus is waaay faster though so still gets used first.",
              "score": 1,
              "created_utc": "2026-01-28 08:28:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2668r4",
          "author": "Michaeli_Starky",
          "text": "At similar performance?  Not even close.",
          "score": 3,
          "created_utc": "2026-01-28 06:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o240qo4",
          "author": "ChainOfThot",
          "text": "How can their API costs be so cheap with what I assume is still older hardware?",
          "score": 5,
          "created_utc": "2026-01-27 23:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24us8r",
              "author": "BlueSwordM",
              "text": "Native INT4, great context handling, small MOE and not amazing speeds.",
              "score": 5,
              "created_utc": "2026-01-28 01:48:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o249wya",
              "author": "smith7018",
              "text": "Government subsidies",
              "score": 11,
              "created_utc": "2026-01-28 00:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24kjkr",
                  "author": "HateAccountMaking",
                  "text": "We(USA) also have Government subsidies. China is a peoples government thats the difference.",
                  "score": -9,
                  "created_utc": "2026-01-28 00:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o250j8u",
          "author": "electronicsoul",
          "text": "Everyone loves cheap tokens but that's a very reductionist calculation.",
          "score": 3,
          "created_utc": "2026-01-28 02:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24qgyx",
          "author": "lochyw",
          "text": "Who chose these colors? It's literally the opposite of what it should be.",
          "score": 2,
          "created_utc": "2026-01-28 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24tw1h",
              "author": "derivative49",
              "text": "you're talking about it ;)",
              "score": -2,
              "created_utc": "2026-01-28 01:43:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o261l6f",
          "author": "Cool-Selection-9275",
          "text": "Just how do they even do that? That's nuts",
          "score": 2,
          "created_utc": "2026-01-28 06:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260jz8",
          "author": "ChimSau19",
          "text": "I have problem with GLM that claude code just dont ask me if it could modify? Could it fix simply by Shift Tab, or u guy have that problem too?",
          "score": 1,
          "created_utc": "2026-01-28 05:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o265zbl",
          "author": "Crafty-Struggle7810",
          "text": "At what quant?",
          "score": 1,
          "created_utc": "2026-01-28 06:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dit1",
          "author": "korino11",
          "text": "well i making a project and codex 5.2high doesnt solved. But kimi did LOL\n\nGPT always aking about whole parameters what they should be, how need to be done wjole parts. When he have all formulas amd whole project in high math is done. But Kimi, i jut give all formulass and...hold my bear!\n\nJust 1 thing i do not like. in Coding Plans you have a limits on API -Calls. So doesnt matter your call 20k tokens or just 500 tokens...  But..i got it for 2.59  So it perfect for that price",
          "score": 1,
          "created_utc": "2026-01-28 07:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26j1gj",
          "author": "No_Afternoon_4260",
          "text": "> the first time that I feel a open source model is truly competitive..\n\nYeah I know! I feel the same every 6 months, last time was deepseek's release, then k2's.. then glm was the faster brother..\n\nCrazy times I know ðŸ˜…\n\n(As every 6 months, maybe not opus level but not far)",
          "score": 1,
          "created_utc": "2026-01-28 08:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o243jly",
          "author": "PhotographerUSA",
          "text": "Kimi has terrible programming skills. lol",
          "score": -10,
          "created_utc": "2026-01-27 23:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bh4i",
          "author": "Ashley_Sophia",
          "text": "Yeah but I just had a look. Instant ads via UI. No thx.",
          "score": -7,
          "created_utc": "2026-01-28 00:08:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoa8rp",
      "title": "The Qwen Devs Are Teasing Something",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/umvks92vcvfg1.png",
      "author": "Few_Painter_5588",
      "created_utc": "2026-01-27 10:28:56",
      "score": 286,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o21p3m0",
          "author": "rm-rf-rm",
          "text": "Thread locked - announcement is now out: https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/",
          "score": 1,
          "created_utc": "2026-01-27 17:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zvrfc",
          "author": "rerri",
          "text": "Z-Image. It's been popping up in ComfyUI PR's in recent days and then today there's this update with a hidden item added to collection:\n\nhttps://preview.redd.it/3d4oy6wxjvfg1.jpeg?width=780&format=pjpg&auto=webp&s=5c0f222d04f42b920dce8929e666f0158a6b9840",
          "score": 60,
          "created_utc": "2026-01-27 11:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zxdt0",
              "author": "Illya___",
              "text": "The question is which one though, base? edit? All of them?\n\nIs that apparent from the PRs?",
              "score": 20,
              "created_utc": "2026-01-27 11:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zyq95",
                  "author": "rerri",
                  "text": "Base T2I workflow was just added to templates today. \"Omni\" with edit capabilities has been worked on earlier.\n\nI think the template addition hints at base releasing today but whether the same model also has edit capabilities or whether that's something coming later, I'm not sure.",
                  "score": 9,
                  "created_utc": "2026-01-27 11:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zu1ri",
          "author": "nikhilprasanth",
          "text": "Most likely the Z image base",
          "score": 33,
          "created_utc": "2026-01-27 10:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2059t7",
          "author": "xandep",
          "text": "Qwen4 Next 48B A3B. I'm sure. ðŸ¥¹",
          "score": 21,
          "created_utc": "2026-01-27 12:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20yhei",
              "author": "Available-Craft-5795",
              "text": "The full Qwen 4 series would be nice. Its crazy how good they got the 0.6B mode to be.",
              "score": 11,
              "created_utc": "2026-01-27 15:02:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o208imc",
              "author": "Opening_Exit_1153",
              "text": "I hope it's, We need something deployable that can beat 4.7 flash, the 4.7 flash is a great model but a new Qwen moe that can beat it will be great!",
              "score": 5,
              "created_utc": "2026-01-27 12:44:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o215smz",
              "author": "KittyPigeon",
              "text": "Would love for that to come out",
              "score": 1,
              "created_utc": "2026-01-27 15:36:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21nk9m",
              "author": "Far-Low-4705",
              "text": "qwen 4vl 80b  \n  \n\\- this is my dream rn lol  \n\\* 80b sparse moe (perfect for my rig at Q4)  \n\\* rly good vision (if its anything like qwen 3vl, thats already good enough for me)  \n\\* better long context performance (as seen in qwen 3 next, hopefully carries over to qwen 4)  \n\\* interleaved thinking (hopefully)",
              "score": 1,
              "created_utc": "2026-01-27 16:53:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zryyl",
          "author": "ResidentPositive4122",
          "text": "Someone said something a while ago, about the chinese new year. Google says it's on the 17th this year, so it would make sense that a lot of labs want to get things out before the break. K2.5 is out, hopefully we'll get q3.5, dsv4, mm2.2 and so on.",
          "score": 22,
          "created_utc": "2026-01-27 10:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ztsvf",
              "author": "Few_Painter_5588",
              "text": "There's also some serious competition between these labs. So it makes sense they all want to get something out. \n\nQwen3 Max Thinking, Kimi K2.5.",
              "score": 8,
              "created_utc": "2026-01-27 10:53:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zwwlw",
          "author": "robberviet",
          "text": "Qwen 3.5 would be awesome. It's 6 months (?) already.",
          "score": 6,
          "created_utc": "2026-01-27 11:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ztqc3",
          "author": "Antique_Dot_5513",
          "text": "This is the image base normally, since the flow arrived on comfyui",
          "score": 9,
          "created_utc": "2026-01-27 10:52:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zv79b",
              "author": "Odd-Ordinary-5922",
              "text": "im 99% sure its the next qwen3next lineup (nvm edit: considering its tongyi lab its probably z image turbo edit",
              "score": 3,
              "created_utc": "2026-01-27 11:05:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zr5nl",
          "author": "Few_Painter_5588",
          "text": "Another tweet of interest\n\nhttps://preview.redd.it/nyc5fno6dvfg1.png?width=1194&format=png&auto=webp&s=224444e21cdee679358405965b8647dd5bd40824",
          "score": 17,
          "created_utc": "2026-01-27 10:30:22",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1zrpt5",
              "author": "MidAirRunner",
              "text": "Qwen3.5 30b A3b pls ty.",
              "score": 28,
              "created_utc": "2026-01-27 10:35:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20hoqb",
                  "author": "ahmetegesel",
                  "text": "That flash emoji put my hopes high",
                  "score": 5,
                  "created_utc": "2026-01-27 13:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zuwki",
          "author": "GreenGreasyGreasels",
          "text": "I am so over teasers and vague posting. When the release cadence was slow this was exciting, but now with new jaw dropping models dropping every weekday and and twice on Sundays this is getting tiresome and exhausting. Be more like Moonshot and Kimi K2.5 I guess. Anyone else feel the same? Or are you guys enjoying this?",
          "score": 11,
          "created_utc": "2026-01-27 11:02:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zva91",
              "author": "Odd-Ordinary-5922",
              "text": "im just happy that we are getting free models",
              "score": 27,
              "created_utc": "2026-01-27 11:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zvvhg",
              "author": "FullOf_Bad_Ideas",
              "text": "I agree it's good to filter out teases. Marketing is cheap, R&D is not, and China has a hustle B2C marketing culture.\n\nIf you sign up for Alibaba Cloud you'll have people contacting you pretty quickly. If you set up Google Cloud, AWS or Azure, you'll never be able to reach a real human being without putting down a lot of money first. They are serious about direct contact with customers.",
              "score": 2,
              "created_utc": "2026-01-27 11:10:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zutd7",
          "author": "AdventurousSwim1312",
          "text": "Qwen Next update?",
          "score": 2,
          "created_utc": "2026-01-27 11:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zxtqz",
          "author": "NoYogurtcloset4090",
          "text": "Just Z-image new models.",
          "score": 1,
          "created_utc": "2026-01-27 11:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zz2ed",
          "author": "Fast-Double-8915",
          "text": "Stand with legs apart?Â ",
          "score": 1,
          "created_utc": "2026-01-27 11:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202h90",
          "author": "Time-Teaching1926",
          "text": "I hope it's soon and not just another tease so we won't be waiting a long time again. ðŸ˜­",
          "score": 1,
          "created_utc": "2026-01-27 12:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2045br",
          "author": "Guilty_Rooster_6708",
          "text": "Z-image edit please",
          "score": 1,
          "created_utc": "2026-01-27 12:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208nxv",
          "author": "derivative49",
          "text": "these people won't rest unless they pop some bubble somewhere",
          "score": 1,
          "created_utc": "2026-01-27 12:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20bnfv",
          "author": "No_Conversation9561",
          "text": "Itâ€™s tweeted by Tongyi Lab so probably Z-Image base and edit ðŸ¤ž",
          "score": 1,
          "created_utc": "2026-01-27 13:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ghky",
          "author": "swagonflyyyy",
          "text": "Its not a new model! Its the best model! You're not wrong! You're just in the dark!",
          "score": 1,
          "created_utc": "2026-01-27 13:31:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215v6d",
          "author": "pigeon57434",
          "text": "if it was something to do with Qwen they would just post it on their alibaba\\_qwen account",
          "score": 1,
          "created_utc": "2026-01-27 15:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20kwck",
          "author": "Doct0r0710",
          "text": "I hate this teasing wankery... Why must they act like all the crypto bros combined?",
          "score": 1,
          "created_utc": "2026-01-27 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o207sdw",
          "author": "Opening_Exit_1153",
          "text": "I think GTA 6 is gonna come out before the new Qwen 30B MOE",
          "score": 1,
          "created_utc": "2026-01-27 12:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zv0d8",
          "author": "Minute_Attempt3063",
          "text": "Could it be, that they are cooking up a model that is uncensored (more then normal) and is way better at coding then Claude?\n\nOr a Deepseek-Qwen model?",
          "score": -5,
          "created_utc": "2026-01-27 11:03:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zyzq8",
          "author": "darkpigvirus",
          "text": "how about Qwen3.5 1B A350M - Thinking? Easy to train will take only less than your day Qwen Team",
          "score": -1,
          "created_utc": "2026-01-27 11:36:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql6cz7",
      "title": "Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy .   Fork/check it out! BYOR",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/hlrhml65m6fg1.gif",
      "author": "Efficient-Proof-1824",
      "created_utc": "2026-01-23 23:20:23",
      "score": 273,
      "num_comments": 30,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1bxokd",
          "author": "Potential-Net-9375",
          "text": "What a fun project! Thanks for sharing",
          "score": 26,
          "created_utc": "2026-01-23 23:24:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c4h0d",
          "author": "ahstanin",
          "text": "Here we go, now AI playing my games. What's next?",
          "score": 20,
          "created_utc": "2026-01-24 00:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c6gvl",
              "author": "SeriousGrab6233",
              "text": "I fear with robots they are going to start eating my food too",
              "score": 15,
              "created_utc": "2026-01-24 00:11:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1cxzma",
                  "author": "SmartCustard9944",
                  "text": "Thank god you said food",
                  "score": 6,
                  "created_utc": "2026-01-24 02:48:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1d6d5u",
                  "author": "philmarcracken",
                  "text": "the clanker didnt mean those kinds of chips",
                  "score": -3,
                  "created_utc": "2026-01-24 03:38:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1cd7p8",
          "author": "TheyCallMeDozer",
          "text": "Really cool idea and setup, might be nice to setup an OpenAI style route to try bigger models locally, for example with Olama or LMStudio... could be really cool to spin up a large model and see how it handels it",
          "score": 7,
          "created_utc": "2026-01-24 00:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dol8k",
              "author": "Efficient-Proof-1824",
              "text": "For sure - that's a great idea and def on the roadmap!",
              "score": 3,
              "created_utc": "2026-01-24 05:42:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1jp862",
                  "author": "Efficient-Proof-1824",
                  "text": "updated w/ OpenAI v1 endpoints - check it out! I don't have Ollama on this machine but confirmed with OpenAI and Groq.  Will confirm with Ollama and LM Studio tomorrow but LMK if anything comes up",
                  "score": 2,
                  "created_utc": "2026-01-25 02:39:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e62d2",
          "author": "Aggressive_Pea_2739",
          "text": "This would be dope even just to farm and train pokemons in those dense jungles",
          "score": 6,
          "created_utc": "2026-01-24 08:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dgcrh",
          "author": "Flame_Grilled_Tanuki",
          "text": "Is there a way to enable audio output?",
          "score": 3,
          "created_utc": "2026-01-24 04:44:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1doggn",
              "author": "Efficient-Proof-1824",
              "text": "Just added it! Click the speaker icon next to Save/Load to enable audio. There's a volume slider too. Fair warning - it does come across choppy as the emulation is also played at > 1x",
              "score": 4,
              "created_utc": "2026-01-24 05:41:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1dphx4",
                  "author": "Flame_Grilled_Tanuki",
                  "text": "Awesome. Thanks for the addition.",
                  "score": 3,
                  "created_utc": "2026-01-24 05:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1do7et",
          "author": "Niwa-kun",
          "text": "any plans to have it build a database of what strats it has used, therefore for sequential runs, it cannot use the same strats over and over?",
          "score": 4,
          "created_utc": "2026-01-24 05:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dotuk",
              "author": "Efficient-Proof-1824",
              "text": "partially in place already.  The system stores all the experiences (state, action, reward) in IndexedDB and so the policy network uses that but not so much an explicit 'check this ledger of past strats'.  Good idea tho - could log LLM plans + outcomes and inject recent failures into the prompt",
              "score": 6,
              "created_utc": "2026-01-24 05:44:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1fm55m",
          "author": "Aggressive_Arm9817",
          "text": "Have you tried it with other models? Like GLM 4.7 Flash 30B",
          "score": 3,
          "created_utc": "2026-01-24 14:52:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ijbuo",
              "author": "Efficient-Proof-1824",
              "text": "planning on adding OpenAI endpoint support - hopefully in next day or so!",
              "score": 3,
              "created_utc": "2026-01-24 22:55:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wx6wt",
                  "author": "Aggressive_Arm9817",
                  "text": "That's gonna be very cool ðŸ‘€",
                  "score": 1,
                  "created_utc": "2026-01-26 23:10:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fu7fq",
          "author": "ds-unraid",
          "text": "Thanks for the share!  I added the ability to specify my own local OpenAI V1 compatible endpoint so I can change models via your AdvancedPanel. Currently using LM Studio as the local API.  Something to consider.  I find this super dope!",
          "score": 5,
          "created_utc": "2026-01-24 15:32:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ijkou",
              "author": "Efficient-Proof-1824",
              "text": "Hey thanks! Yes definitely I'm planning on adding support for that in the coming day or so -  excited to see what that opens up",
              "score": 2,
              "created_utc": "2026-01-24 22:57:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1iv6au",
                  "author": "ds-unraid",
                  "text": "Are you using Cursor because I had Cursor do it and it took about two minutes.  I am not advocating for AI slop and I super respect the software engineering field so if you're trying to do everything by hand, that's cool, but if you've never heard of Cursor, I highly recommend you check it out. I used to write one full stack application every 1 to 2 weeks now I do like 5 to 6 a day. I'm no different from someone who doesn't know how to code with the exception that I understand the underlying stack of technologies. Especially how things should fit together, which is the only advantage I have over someone who vibe-codes.",
                  "score": 3,
                  "created_utc": "2026-01-24 23:57:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e5xhz",
          "author": "Aggressive_Pea_2739",
          "text": "Dope",
          "score": 3,
          "created_utc": "2026-01-24 08:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f3nxb",
          "author": "IngwiePhoenix",
          "text": "This is fun but it completely did not like my german ROM. XD\n\nBut this is still pretty dope. Would love to throw a bigger model at this and try it again. :)",
          "score": 3,
          "created_utc": "2026-01-24 13:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ikbbq",
              "author": "Efficient-Proof-1824",
              "text": "Going to add support for more endpoint support! The ROM thing is interesting...so technically the prompt was specific to Pokemon Red but not language specific.  Maybe need a translation step lol",
              "score": 2,
              "created_utc": "2026-01-24 23:00:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ix29a",
                  "author": "IngwiePhoenix",
                  "text": "I just YOLO'd it - had no english ROM, so I fed it my german one and it... tried to do stuff. o.o\n\nThough the values in the UI seemed off (4/8 badges...?) so either my Adblock (Brave) or a different memory layout in german gen1 was the issue. Buuuuut, it DID run around and eventually got to Prof. Oak. xD It just took forever (40 min) lol.",
                  "score": 3,
                  "created_utc": "2026-01-25 00:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1h6qpr",
          "author": "ctbanks",
          "text": "[https://developer.chrome.com/docs/ai/built-in](https://developer.chrome.com/docs/ai/built-in)",
          "score": 3,
          "created_utc": "2026-01-24 19:07:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ij3cg",
              "author": "Efficient-Proof-1824",
              "text": "thanks for sharing! Interesting and glad to see",
              "score": 2,
              "created_utc": "2026-01-24 22:54:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1egizw",
          "author": "Limp_Classroom_2645",
          "text": "looks very fun! can I ask why tensorflow instead of torch?",
          "score": 2,
          "created_utc": "2026-01-24 09:48:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmvny5",
      "title": "GLM-4.7-Flash is even faster now",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/19092",
      "author": "jacek2023",
      "created_utc": "2026-01-25 21:14:50",
      "score": 269,
      "num_comments": 98,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1q67hb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 00:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ozkks",
          "author": "jacek2023",
          "text": "https://preview.redd.it/vdyf3fwdakfg1.png?width=1600&format=png&auto=webp&s=a3fef014a14a330c67c84f6591085ad070bd1225\n\nspecial thanks to u/Remove_Ayys for making my previous post obsolete",
          "score": 95,
          "created_utc": "2026-01-25 21:15:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1p0rr4",
              "author": "coder543",
              "text": "Ok, now *that* starts to look respectable. Still worth comparing against efficient models like gpt-oss and nemotron-3-nano.\n\nEDIT: prompt processing still seems to fall off a cliff on glm-4.7-flash, I just tested it.",
              "score": 46,
              "created_utc": "2026-01-25 21:20:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1p15ha",
                  "author": "jacek2023",
                  "text": "https://preview.redd.it/gdnzc5eobkfg1.png?width=1600&format=png&auto=webp&s=f150d1956eb0e31d8825219cf637a1d525d0735d",
                  "score": 27,
                  "created_utc": "2026-01-25 21:22:18",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1p1j58",
                  "author": "marsxyz",
                  "text": "Yeah same here wuth vulkan back end. Let's hope it'll be resolved soon",
                  "score": 1,
                  "created_utc": "2026-01-25 21:23:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1p99m2",
              "author": "coder543",
              "text": "See my gist: [https://gist.github.com/coder543/16ca5e60aabee4dfc3351b54e8fe2a1c](https://gist.github.com/coder543/16ca5e60aabee4dfc3351b54e8fe2a1c)\n\nLinear:\n\nhttps://preview.redd.it/e1bqaqwxnkfg1.png?width=1920&format=png&auto=webp&s=3c4484b5606646c1aee564a932b072ad5782887b\n\nNemotron holds its performance extremely well due to its hybrid architecture. I don't know why the improvements for GLM-4.7-Flash don't seem to have helped the DGX Spark at all.\n\nEDIT: added Qwen3-Coder for fun. (My RTX 3090 couldn't go all the way to 50k tokens with the quant that I have.) The quants are not entirely apples to apples, but the performance curve is the main thing here, not the absolute numbers.",
              "score": 20,
              "created_utc": "2026-01-25 21:58:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1pbcnp",
                  "author": "coder543",
                  "text": "Log y:\n\nhttps://preview.redd.it/1l7ej68wnkfg1.png?width=1920&format=png&auto=webp&s=a3155d8f09d81c33dbfaaaa48353a66e5fa6ef66",
                  "score": 3,
                  "created_utc": "2026-01-25 22:06:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1pad9r",
                  "author": "jacek2023",
                  "text": "[https://github.com/ggml-org/llama.cpp/pull/19092#issuecomment-3797263741](https://github.com/ggml-org/llama.cpp/pull/19092#issuecomment-3797263741)",
                  "score": 2,
                  "created_utc": "2026-01-25 22:02:44",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1py7o9",
                  "author": "rm-rf-rm",
                  "text": "Nemotron slower tok/s in the first data point? that doesnt seem right? \n\nP.S: Excellent plots! what did you use to make it?",
                  "score": 1,
                  "created_utc": "2026-01-25 23:51:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vqepp",
                  "author": "onil_gova",
                  "text": "I love the style of these graphs. What package did you use to create them?",
                  "score": 1,
                  "created_utc": "2026-01-26 19:56:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p11kw",
          "author": "ps5cfw",
          "text": "Cries in AMD GPUÂ ",
          "score": 29,
          "created_utc": "2026-01-25 21:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4u0r",
              "author": "jacek2023",
              "text": "what are your results on vulkan?",
              "score": 7,
              "created_utc": "2026-01-25 21:38:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1slae5",
                  "author": "Electronic-Fill-6891",
                  "text": "https://preview.redd.it/itxwu7um2ofg1.png?width=1600&format=png&auto=webp&s=b85dba61fff57e7d42ff29f4287f528084e112cc\n\nThis was on **ROCm**, **RX 7900XT**. Vulkan is hit-or-miss on my system.",
                  "score": 3,
                  "created_utc": "2026-01-26 10:09:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1t3iv2",
                  "author": "politerate",
                  "text": "https://preview.redd.it/t441p9bpuofg1.png?width=1500&format=png&auto=webp&s=5e49e47ac13d38537ca7540e9d897d574ccf6655\n\nQ4 K XL on ROCm and 7900XTX",
                  "score": 2,
                  "created_utc": "2026-01-26 12:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q6jq9",
              "author": "ayylmaonade",
              "text": "I was excited for a second too :(",
              "score": 7,
              "created_utc": "2026-01-26 00:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1p2d4k",
          "author": "jacek2023",
          "text": "https://preview.redd.it/rjo1w96mckfg1.png?width=1600&format=png&auto=webp&s=0b1c0de73960d311382b372c4719460e9444855a",
          "score": 10,
          "created_utc": "2026-01-25 21:27:33",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1pd0xe",
              "author": "Lazy-Pattern-5171",
              "text": "Splitting hairs but is the performance drop comparable to what you would expect if models had 2B parameter differences with also different architectures?",
              "score": 2,
              "created_utc": "2026-01-25 22:14:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pdipq",
          "author": "jacek2023",
          "text": "opencode is pretty usable right now\n\n(n\\_tokens = 43462 - after reading some docs and discussing parts of code)\n\n    slot launch_slot_: id  0 | task 3944 | processing task, is_child = 0\n    slot update_slots: id  0 | task 3944 | new prompt, n_ctx_slot = 200192, n_keep = 0, task.n_tokens = 45074\n    slot update_slots: id  0 | task 3944 | n_tokens = 43462, memory_seq_rm [43462, end)\n    slot update_slots: id  0 | task 3944 | prompt processing progress, n_tokens = 45074, batch.n_tokens = 1612, progress = 1.000000\n    slot update_slots: id  0 | task 3944 | prompt done, n_tokens = 45074, batch.n_tokens = 1612\n    slot init_sampler: id  0 | task 3944 | init sampler, took 9.71 ms, tokens: text = 45074, total = 45074\n    slot print_timing: id  0 | task 3944 |\n    prompt eval time =    2814.63 ms /  1612 tokens (    1.75 ms per token,   572.72 tokens per second)\n           eval time =   29352.57 ms /  1731 tokens (   16.96 ms per token,    58.97 tokens per second)\n          total time =   32167.20 ms /  3343 tokens",
          "score": 8,
          "created_utc": "2026-01-25 22:16:34",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1s3374",
              "author": "Primary-Debate-549",
              "text": "On what hardware? I've been trying it on a DGX spark with ollama and it's not exactly fast ... or good.",
              "score": 2,
              "created_utc": "2026-01-26 07:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o23irmw",
                  "author": "Primary-Debate-549",
                  "text": "Okay I've been able to force (on the ollama side) a large context window. That really helps with the \"good\" part. It's 10x as intelligent now at least (context size 100000). Still slow.",
                  "score": 1,
                  "created_utc": "2026-01-27 21:47:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1p2c8j",
          "author": "jacek2023",
          "text": "https://preview.redd.it/x15ii8dlckfg1.png?width=1600&format=png&auto=webp&s=318e1223e09a9424675cf7cab49e6404806a7f52",
          "score": 5,
          "created_utc": "2026-01-25 21:27:26",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1pysja",
              "author": "rm-rf-rm",
              "text": "How repeatable are the results?",
              "score": 1,
              "created_utc": "2026-01-25 23:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q3jge",
                  "author": "jacek2023",
                  "text": "llama-bench performs 5 runs by default",
                  "score": 1,
                  "created_utc": "2026-01-26 00:17:16",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pxj8q",
          "author": "bobaburger",
          "text": "Awesome. With default params, my tg went from 9-10 tok/s to 17-18 tok/s. Kudos to all the hard work from the llama.cpp team!!!",
          "score": 4,
          "created_utc": "2026-01-25 23:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgk25",
          "author": "jinnyjuice",
          "text": "For those who might be confused -- it's for llama.cpp. Now, llama.cpp works faster with GLM-4.7-Flash.",
          "score": 4,
          "created_utc": "2026-01-26 01:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p7d29",
          "author": "Gallardo994",
          "text": "I swear this model is somehow cursed",
          "score": 24,
          "created_utc": "2026-01-25 21:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pz7r7",
              "author": "robberviet",
              "text": "I swear unless dev invested in prepare prior to the release for 0-day support, all models are cursed.",
              "score": 18,
              "created_utc": "2026-01-25 23:56:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1pb0fo",
              "author": "Zc5Gwu",
              "text": "This happens every time a new model comes out. IDK why people are up in arms. It usually gets ironed out over time.",
              "score": 36,
              "created_utc": "2026-01-25 22:05:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qlc9g",
                  "author": "Far-Low-4705",
                  "text": "i think its cuz people think it is a common qwen 30b architecture under the hood (since there are so many) and not a new novel architecture that needs to be implemented from scratch",
                  "score": 10,
                  "created_utc": "2026-01-26 01:46:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rhcf8",
                  "author": "koflerdavid",
                  "text": "Requiring multiple bugfixes is indeed rare. But it's still less effort than Qwen3-Next required.",
                  "score": 2,
                  "created_utc": "2026-01-26 04:44:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1pfi1y",
          "author": "Cool-Chemical-5629",
          "text": "Is there anything that can be done for Vulkan inference in terms of getting better speed?",
          "score": 4,
          "created_utc": "2026-01-25 22:25:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qvw9n",
          "author": "k0setes",
          "text": "https://preview.redd.it/rcydthh9wlfg1.png?width=1778&format=png&auto=webp&s=1d8b489002e38cd718d1b6e9409b6711f6e8ebc7",
          "score": 3,
          "created_utc": "2026-01-26 02:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s7tzj",
              "author": "jacek2023",
              "text": "But why ;)",
              "score": 1,
              "created_utc": "2026-01-26 08:06:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rfdp5",
          "author": "DOAMOD",
          "text": "I've had a lot of problems with this model, but since yesterday I've been working with it and it seems much more stable now, and I have to say I think it's very good. It's handling complex problems that are usually more the domain of 120/200b models, and it's surprising me. Of course, it's not going to be an Opus, but considering its size, its way of thinking, its capabilities, and good use of tools, etc., its improving speed, and a more up-to-date model, I can only congratulate Z for the great work.\n\nIf it continues to improve, especially the stability and performance drop at high CTX, it will be a very good model. I'm going to stick with it because I'm liking it.\n\nI haven't tried this update yet, let's see if it improves my results. I'm currently dropping to 850/75 over 35/40k(128)",
          "score": 3,
          "created_utc": "2026-01-26 04:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rmuqp",
              "author": "simracerman",
              "text": "Whatâ€™s your hardware?\n\nMy 5070 Ti is spilling into system memory, and thatâ€™s still giving me 55t/s at 20k(128)",
              "score": 1,
              "created_utc": "2026-01-26 05:20:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1smkr1",
                  "author": "jacek2023",
                  "text": "well but what's your CPU and RAM?",
                  "score": 1,
                  "created_utc": "2026-01-26 10:20:32",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q8fts",
          "author": "TheRealMasonMac",
          "text": "Sucks that they don't want to release the base model. Didn't take long for them to start backing out of open weight releases after going public.",
          "score": 4,
          "created_utc": "2026-01-26 00:41:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qblob",
              "author": "sxales",
              "text": "[GLM 4.7 came out last month](https://huggingface.co/zai-org/GLM-4.7)?",
              "score": 10,
              "created_utc": "2026-01-26 00:56:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qj4li",
                  "author": "TheRealMasonMac",
                  "text": "This is 4.7-Flash",
                  "score": 0,
                  "created_utc": "2026-01-26 01:35:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1t6nts",
              "author": "FullOf_Bad_Ideas",
              "text": "Yeah I feel like the curtain on Chinese open weight releases might be closing.\n\nI think there's 30% chance that GLM 5 will not be open weight.",
              "score": 3,
              "created_utc": "2026-01-26 12:56:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1qc0oj",
              "author": "huzbum",
              "text": "Would there even be a base model if they distill from a larger post trained model?",
              "score": 5,
              "created_utc": "2026-01-26 00:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qjact",
                  "author": "TheRealMasonMac",
                  "text": "Yes. There is always a base model unless they're doing something like REAP. [https://huggingface.co/zai-org/GLM-4.7-Flash/discussions/2](https://huggingface.co/zai-org/GLM-4.7-Flash/discussions/2)\n\n\\> Sorry, but we don't have plans to release the base model.",
                  "score": 4,
                  "created_utc": "2026-01-26 01:36:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rhu51",
                  "author": "koflerdavid",
                  "text": "Presumably, distillation is from a larger base model as well. Then post-training is applied. One doesn't want the model to incompletly learn the instruction tuning.",
                  "score": 2,
                  "created_utc": "2026-01-26 04:47:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1qchq6",
              "author": "Odd-Ordinary-5922",
              "text": "at least they released a model??",
              "score": 3,
              "created_utc": "2026-01-26 01:01:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qjhok",
                  "author": "TheRealMasonMac",
                  "text": "That is orthogonal to my statement. The base models for GLM-4.5/6/7 and Air are already available. How else would you interpret them suddenly not wanting to release base models?",
                  "score": 7,
                  "created_utc": "2026-01-26 01:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q9wb5",
          "author": "fallingdowndizzyvr",
          "text": "Still waiting for things to settle down before trying this. It seems like there's a new major change every day.",
          "score": 1,
          "created_utc": "2026-01-26 00:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qa1bj",
              "author": "jacek2023",
              "text": "Actually no, twice a day",
              "score": 4,
              "created_utc": "2026-01-26 00:49:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qp3ko",
          "author": "mr_zerolith",
          "text": "Hell yeah!!!!!!",
          "score": 1,
          "created_utc": "2026-01-26 02:05:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sm907",
          "author": "crantob",
          "text": "Sportscar.  Hard to drive.",
          "score": 1,
          "created_utc": "2026-01-26 10:17:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sous0",
          "author": "BeeNo7094",
          "text": "How is the vllm support? Anyone tried an 4bit awq quant yet?",
          "score": 1,
          "created_utc": "2026-01-26 10:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1soykf",
              "author": "jacek2023",
              "text": "I tried vllm and failed (got problem with context), will try in the future again, support in vllm is probably also in progress",
              "score": 1,
              "created_utc": "2026-01-26 10:41:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sphvq",
                  "author": "BeeNo7094",
                  "text": "ðŸ™‚â€â†•ï¸rocm 7.2 with llama.cpp is the only way, on my 7900xt machine",
                  "score": 1,
                  "created_utc": "2026-01-26 10:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1swcv3",
          "author": "mouseofcatofschrodi",
          "text": "this only solves the problem for the llama.cpp engine right? What about MLX? Would it be better to use a gguf model on a mac over mlx?",
          "score": 1,
          "created_utc": "2026-01-26 11:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tc8gm",
          "author": "1-a-n",
          "text": "Not faster for Blackwell",
          "score": 1,
          "created_utc": "2026-01-26 13:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tud9h",
          "author": "SatoshiNotMe",
          "text": "Still awful with Claude Code. The latest build from source did not improve this situation:\n\nOn my M1 Max Pro 64 GB, Qwen3-30B-A3B works very well at around 20 tok/s generation speed in CC via llama-server using the setup Iâ€™ve described here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nBut with GLM-4.7-flash Iâ€™ve tried all sorts of llama-server settings and I barely get 3 tok/s which is useless.\n\nThe core problem seems to be that GLM's template has thinking enabled by default and Claude Code uses assistant prefill - they're incompatible.",
          "score": 1,
          "created_utc": "2026-01-26 15:03:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tulbm",
              "author": "jacek2023",
              "text": "try opencode",
              "score": 2,
              "created_utc": "2026-01-26 15:04:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1twmwi",
                  "author": "SatoshiNotMe",
                  "text": "Prefer staying in CC and leverage my max subscription. To be clear, I'm obviously not looking to run this model for any serious coding, but more for sensitive document work, private notes, etc.\n\nGiven the gap with Qwen3-30B-A3B, there's clearly something that still needs to be fixed with llama.cpp support of glm-4.7-flash",
                  "score": 1,
                  "created_utc": "2026-01-26 15:13:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o207m6h",
          "author": "sammcj",
          "text": "Still only getting 37tk/s with llama.cpp compared to 110tk/s with vLLM on my 2x RTX3090 setup.",
          "score": 1,
          "created_utc": "2026-01-27 12:38:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2080cq",
              "author": "jacek2023",
              "text": "Please share vllm command with 50000 context (I use 200000 in llama.cpp)",
              "score": 1,
              "created_utc": "2026-01-27 12:41:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o239qlc",
                  "author": "sammcj",
                  "text": "Reddit craps itself when you try to share codeblocks so I've share the relevant parts in a gist for you: https://gist.github.com/sammcj/728128541109c45f3b1cecc8be20955f",
                  "score": 1,
                  "created_utc": "2026-01-27 21:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q1l2f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-26 00:07:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1va6a2",
              "author": "johnnyApplePRNG",
              "text": "Skill issue.",
              "score": 1,
              "created_utc": "2026-01-26 18:46:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1qof1u",
          "author": "Loud_Economics4853",
          "text": "Itâ€™s constantly updatedâ€”we users are so lucky!",
          "score": 1,
          "created_utc": "2026-01-26 02:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r6sd8",
          "author": "PathfinderTactician",
          "text": "What's the point of all this? The speed already seems to be OK, but the output is terrible. This is running Q8 with the supposedly fixed Unsloth quant, FA=off, and also using the override-kv (deepseek2...) argument. The model makes basic errors, and loops even with 16k context.",
          "score": -3,
          "created_utc": "2026-01-26 03:38:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rigfz",
              "author": "viperx7",
              "text": "to be honest i am having  a completely different experience. this model is no opus but it\\`s able to do things fine i have tested even the Q4 upto 70K context and it holds the Q8 is also working nicely\n\nI am still running tests to see how smart dumb or capable this is compared to nemotron or qwen30b moes\n\ni think something might be off with your system or setup. as the model stands now it is very much usable\n\ncan you give some example tasks for which it is looping because in my case this loops almost never less than the nemotron (i have used this extensively with opencode Q4 / Q6 and Q8)",
              "score": 6,
              "created_utc": "2026-01-26 04:51:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1rit3q",
              "author": "Odd-Ordinary-5922",
              "text": "the fa is supposed to be on and you dont need to use override kv. bro update your llama",
              "score": 7,
              "created_utc": "2026-01-26 04:53:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1paqri",
          "author": "wapxmas",
          "text": "shame for [z.ai](http://z.ai) team",
          "score": -16,
          "created_utc": "2026-01-25 22:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pnbgx",
              "author": "__Maximum__",
              "text": "This is llama.cpp issue, right?",
              "score": 11,
              "created_utc": "2026-01-25 23:00:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1pqspy",
                  "author": "boredinballard",
                  "text": "I think it's mainly an issue with the z.ai team not optimizing for all the necessary tweaks before release. Not to glaze openAI, but they did do a lot of work before releasing gpt-oss to make sure it worked with llama and what not. Still wasn't great at launch but within a few hours the necessary tweaks and such were done.",
                  "score": -4,
                  "created_utc": "2026-01-25 23:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmlpjp",
      "title": "Internet blackout and Local LLMs",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/",
      "author": "DunderSunder",
      "created_utc": "2026-01-25 15:15:05",
      "score": 254,
      "num_comments": 77,
      "upvote_ratio": 0.91,
      "text": "Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).\n\n  \nMeanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1mooh4",
          "author": "jacek2023",
          "text": "Yes that's a very good argument for local LLMs. People born after 2000 (or 1990?) assume that Internet services are always available. And cloud models are always free.",
          "score": 175,
          "created_utc": "2026-01-25 15:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n82yc",
              "author": "RoyalCities",
              "text": "Having a local LLM and a Meshtastic node are basically a must in today's society.\n\nOne for local knowledge\n\nAnd the other to still communicate with friends and family across the city even if the entire internet and cell towers go down.",
              "score": 37,
              "created_utc": "2026-01-25 16:43:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ogsom",
                  "author": "mycall",
                  "text": "> Meshtastic\n\nLoRA is [susceptible to jamming](https://pmc.ncbi.nlm.nih.gov/articles/PMC9269689/) with relative ease.",
                  "score": 11,
                  "created_utc": "2026-01-25 19:53:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1t1zxa",
                  "author": "DryDevelopment8584",
                  "text": "What are some good resources to learn about meshtastic nodes?",
                  "score": 1,
                  "created_utc": "2026-01-26 12:25:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nhpaf",
              "author": "Wide_Egg_5814",
              "text": "Also you can download Wikipedia it's around 100 gb to download all of Wikipedia",
              "score": 9,
              "created_utc": "2026-01-25 17:24:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1nakb6",
              "author": "NeverLookBothWays",
              "text": "It's getting much tougher to build a rig that can handle quality LLMs.  But for those who can, it's like having a reasoning encyclopedia of knowledge at your disposal. (of course, with hallucinations, but definitely workable)",
              "score": 3,
              "created_utc": "2026-01-25 16:54:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mpktm",
              "author": "Ztoxed",
              "text": "truth",
              "score": 3,
              "created_utc": "2026-01-25 15:21:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mz16u",
              "author": "ivoras",
              "text": "OTOH, if the Internet is down, what use is a local LLM?\n\nIf you use it as an encyclopaedia, it needs to be a gigantic model. If it's used just as a conversation partner, that won't help anyone that much,",
              "score": -9,
              "created_utc": "2026-01-25 16:04:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mzxhl",
                  "author": "jacek2023",
                  "text": "Do you assume there is no actual knowledge in 12B model?\n\nI always ask models about specific music video and they are always wrong. That happens for 4B and some cloud models.  But if you want some common knowledge even 4B is useful. Models are basically a compressed knowledge.",
                  "score": 10,
                  "created_utc": "2026-01-25 16:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1nbvhm",
                  "author": "Mr_Back",
                  "text": "I used Qwen model (around 230 billion parameters) in a quantized format (Q2) and the full-sized GPT-OSS 120B model when I was without internet access for several days. My city had a mobile internet outage. I used these models to troubleshoot and configure my home local network, as well as to set up my NAS and other devices, preparing them for connection to a wired internet connection.",
                  "score": 4,
                  "created_utc": "2026-01-25 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1neuf0",
          "author": "def_not_jose",
          "text": "ChatGPT being whitelisted is hilarious, there is little doubt that they share user data with intelligence agencies",
          "score": 58,
          "created_utc": "2026-01-25 17:12:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p4lgp",
              "author": "IrisColt",
              "text": "If it wasnâ€™t obvious before, it is now.",
              "score": 13,
              "created_utc": "2026-01-25 21:37:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1qagec",
                  "author": "Comas_Sola_Mining_Co",
                  "text": "Ah yes, the Iranian revolutionary guards must be secretly behind Sam Altman and openai. That's why it's one of the three websites which op claims Iranians are allowed to access. Good job we have you around to figure out what the intelligence agencies are up to",
                  "score": -9,
                  "created_utc": "2026-01-26 00:51:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1rccpk",
              "author": "SkyFeistyLlama8",
              "text": "\"How do I crush a rebellion?\"\n\nI wouldn't put it past the people in charge in Iran to use LLMs to formulate some kind of repression framework.",
              "score": 1,
              "created_utc": "2026-01-26 04:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rnfvt",
                  "author": "crantob",
                  "text": "How do they instigate one is the relevant topic to study.",
                  "score": 1,
                  "created_utc": "2026-01-26 05:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nfgtm",
          "author": "a_beautiful_rhind",
          "text": "Even in matters of life or death, OpenAI gonna uphold it's censorship. I bet everyone feels real \"safe\" right now.",
          "score": 23,
          "created_utc": "2026-01-25 17:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o0hs8",
          "author": "fallingdowndizzyvr",
          "text": "Have you thought about downloading Wikipedia? They make it easy by making dumps available. I would trust that more than an LLM when having to look things up.",
          "score": 23,
          "created_utc": "2026-01-25 18:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mu98u",
          "author": "porzione",
          "text": "If google works - try AI mode on main page, it is some kind of Gemini, you can ask about current events, vpn, DPI piercing and so on. Small 8B local models have a very limited technical knowledge, if you need to find a way how to pierce the blocks.   \nDo not tell Deepseek anything about politics and censorship - ask it pure technical, neutral questions to avoid guardrails.",
          "score": 21,
          "created_utc": "2026-01-25 15:43:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mwua1",
              "author": "DunderSunder",
              "text": "The AI mode never works if you go with iranian IP. I don't know if it's because of government or Google but like you can't disable safe search too which is gov's fault.",
              "score": 8,
              "created_utc": "2026-01-25 15:54:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1n16ma",
                  "author": "porzione",
                  "text": "Maybe this is a Google's thing not to enable ai mode for some countries. If it's possible download bigger models - just in case, 14B qwen or latest ministral - they'll be slower with you VRAM but smarter. And be safe!",
                  "score": 1,
                  "created_utc": "2026-01-25 16:13:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ngt20",
              "author": "woswoissdenniii",
              "text": "Zim",
              "score": 1,
              "created_utc": "2026-01-25 17:20:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q4kyw",
          "author": "hilarino",
          "text": "Do you have working phone lines able to make international calls again? If so, with a computer and a 56k modem you can get a (very slow and probably expensive) connection to the rest of the Internet via dial-up.",
          "score": 6,
          "created_utc": "2026-01-26 00:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1stanq",
              "author": "DunderSunder",
              "text": "oh right, in the first week international calls were not possible. But that is crazy if it works.",
              "score": 5,
              "created_utc": "2026-01-26 11:18:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tqnrt",
                  "author": "hilarino",
                  "text": "Can you get a 56k modem + usb adaptor? Are international calls possible today? Let me know if I can help!",
                  "score": 1,
                  "created_utc": "2026-01-26 14:45:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23sequ",
              "author": "RhubarbSimilar1683",
              "text": "Most voice calls and all international ones are VoIP and thus can't run a 56k modem because the audio compression introduces distortion to the signal. Maybe slower ones work",
              "score": 1,
              "created_utc": "2026-01-27 22:32:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1moxsq",
          "author": "wingwing124",
          "text": "I understand what you are getting at, but unfortunately I don't think an uncensored local model will help you reach out past the censorship. \n\nPresumably the reason chatgpt is able to Google things for you is because you're able to reach the OpenAI servers, which are not subject to the same censorship due to geographic location. Even an uncensored local model would only be able to reach what your local machine is able to reach.",
          "score": 12,
          "created_utc": "2026-01-25 15:18:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1msc2t",
              "author": "DunderSunder",
              "text": "There are techniques which use those whitelisted IPs to access other websites. like you could use some APIs of google to do some weird shenanigans. Chinese are experts in bypassing these limits because of the great firewall, most tools are developed by them. Chatgpt won't give you any script or solution in these cases.",
              "score": 10,
              "created_utc": "2026-01-25 15:34:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1mq2db",
              "author": "PollinosisQc",
              "text": "Yeah it's basically doing what a VPN does but with extra steps",
              "score": 1,
              "created_utc": "2026-01-25 15:24:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mq7ev",
              "author": "Ztoxed",
              "text": "This as a new person is something I have been pondering. I am studying models. And even hope to build one. But realized that I am just like ChatGBT or others that are online. Most only allow data in a certain time from. LLM's only can get and update by going off local pretty much so data could stale over time.  \nI have not read enough to understand how that is being looked at.",
              "score": 1,
              "created_utc": "2026-01-25 15:24:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mrzfe",
                  "author": "-illusoryMechanist",
                  "text": "*ChatGPT . (The gpt stands for generative pretrainted transformer.)",
                  "score": 3,
                  "created_utc": "2026-01-25 15:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mrecx",
              "author": "Zealousideal_Nail288",
              "text": "Well unless it gets access to internet a local llm is has usefull has a bok or calculatorÂ \n\n\nIt can help with local problems but it cant reach outside or get news besides what it got during its last update",
              "score": 1,
              "created_utc": "2026-01-25 15:30:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r8o0v",
          "author": "Defiant-Snow8782",
          "text": "To what extent is Google unblocked? Can you use a VPN hosted on GCP? \n\nWhat about [HTTP injector](https://play.google.com/store/apps/details?id=com.evozi.injector)? I used to use it as a teenager to get free internet by pretending the traffic was going to the mobile operator's website (which is free and unlimited) when really it was an SSH tunnel that let me access anything",
          "score": 5,
          "created_utc": "2026-01-26 03:49:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sti7o",
              "author": "DunderSunder",
              "text": "Yeah there are some weird methods sending http over dns or ssh with that app or similar ones. They are slow though.",
              "score": 1,
              "created_utc": "2026-01-26 11:20:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pro3c",
          "author": "Witty_Mycologist_995",
          "text": "First things first, try using Tor to connect to clearnet, then reach out for more help from there.",
          "score": 6,
          "created_utc": "2026-01-25 23:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mxrk4",
          "author": "_IsNull",
          "text": "Why not just use vpn to access what you need? The Chinese been using vpn to access beyond the great firewalls for decades",
          "score": 2,
          "created_utc": "2026-01-25 15:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1myyg3",
              "author": "MelodicRecognition7",
              "text": "I'm afraid there is whitelisting in Iran not blacklisting like in China. In China you can not open some specific sites but everything else is allowed, in Iran you can not open everything except a few specific sites. And if your supposed VPN server's IP address is not in the allowed IP ranges then you just would not be able to connect to it.",
              "score": 15,
              "created_utc": "2026-01-25 16:04:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n47wi",
                  "author": "DunderSunder",
                  "text": "At the beginning of protests we didn't have google, and local search engines are garbage. right now, situation is slightly better than previous weeks and few datacenters have internet access and some people set up vpns so others can connect through that but most of them just stop working after an hour or so, so more vpn configurations are made every minute. overall very frustrating experience, unreliable and very slow, like good luck finding anything close to 1MB/s.",
                  "score": 9,
                  "created_utc": "2026-01-25 16:26:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nhg9h",
          "author": "pas_possible",
          "text": "You can certainly use chatgpt to communicate with the outside world because chatGPT is basically an agent now, you can send text via mcp",
          "score": 2,
          "created_utc": "2026-01-25 17:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oaxbb",
          "author": "dank_shit_poster69",
          "text": "try downloading a Dolphin model, I think those are uncensored?",
          "score": 1,
          "created_utc": "2026-01-25 19:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qux0o",
          "author": "PetrichorShark",
          "text": "In addition to downloading Wikipedia, you can use Kiwix to download, search, and browse a bunch of StackExchange sites (including various techy ones that might help you get around the censorship, like [Tor Q&A](https://browse.library.kiwix.org/viewer#tor.stackexchange.com_en_all_2025-12/questions)), the prepper resources [here](https://library.kiwix.org/?lang=eng#lang=eng&tag=preppers), various wikibooks, etc.",
          "score": 1,
          "created_utc": "2026-01-26 02:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rl1g1",
          "author": "autoencoder",
          "text": "Check out Heretic models:\n\nhttps://github.com/p-e-w/heretic\n\nhttps://huggingface.co/collections/p-e-w/the-bestiary",
          "score": 1,
          "created_utc": "2026-01-26 05:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1teao0",
          "author": "hauhau901",
          "text": "I've uncensored the newest GLM 4.7 Flash myself, advise you try it. Should have no issues using tools.\nhttps://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced",
          "score": 1,
          "created_utc": "2026-01-26 13:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25pyi2",
          "author": "anti-fascist-dude",
          "text": "I dunno man, after seeing masked terrorists shooting your own people. Makes you think where these terrorists came from. I hope you are doing great despite all of this.",
          "score": 1,
          "created_utc": "2026-01-28 04:42:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nnxnh",
          "author": "kaisurniwurer",
          "text": "I'm not an expert, but try circumventing DNS. Try using direct addresses to sites instead. A little naive, but perhaps...\n\nSecond approach is mesh wifi and internal network.\n\nAs for the topic, I agree. Just having access to an uncensored model, as big as you can fit, including partial offload and hybrid inference, no matter the speed, can help you someday.\n\nIn the worse case, you can even pool GPUs and hardware in general with neighbors to get it a little better too.",
          "score": 0,
          "created_utc": "2026-01-25 17:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23spsm",
              "author": "RhubarbSimilar1683",
              "text": "Circumventing DNS might work",
              "score": 1,
              "created_utc": "2026-01-27 22:34:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1slheg",
          "author": "Orihara-Izaya",
          "text": "Oy vey, Oy vey!!!\nHasbara troll, Unit 8200 bot, sayanim or only a golddigger for 7.000 $/post from Netanyahoo???\nAsking for Iranian friend...",
          "score": -3,
          "created_utc": "2026-01-26 10:10:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmjzx1",
      "title": "KV cache fix for GLM 4.7 Flash",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/pull/19067",
      "author": "jacek2023",
      "created_utc": "2026-01-25 14:06:55",
      "score": 250,
      "num_comments": 70,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1nw5a7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-25 18:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mjmtv",
          "author": "__Maximum__",
          "text": "We are now just 5 patches away from running this model locally without issues!",
          "score": 91,
          "created_utc": "2026-01-25 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mx0be",
              "author": "Hunting-Succcubus",
              "text": "Actually its 7 patches",
              "score": 30,
              "created_utc": "2026-01-25 15:55:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1njm51",
              "author": "jacek2023",
              "text": "what are you ta..... [https://github.com/ggml-org/llama.cpp/pull/19092](https://github.com/ggml-org/llama.cpp/pull/19092)",
              "score": 16,
              "created_utc": "2026-01-25 17:33:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mli64",
          "author": "teachersecret",
          "text": "Just tested with UD's k\\_xl 4 bit version on my 4090. Yesterday I was using it with about 45,000 context and maxing out the 4090.\n\nNow it fits with 90,000 context.\n\nI like the model. Still a bit quirky though. I had it running some agentic stuff yesterday and I was really impressed with what I was able to scaffold out of it, but I absolutely had to hold its hand a bit. Reminds me of trying to code with gemini flash or something - it's not terrible and you can get the job done. Beyond coding, it crushes tool use and works great as a tool using assistant. You can get it to do some writing and roleplay but it doesn't seem particularly good at that (it'll make mistakes bigger/more creative writing focused models don't). It's definitely my new default for my home-server.",
          "score": 27,
          "created_utc": "2026-01-25 15:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1myj54",
              "author": "__Maximum__",
              "text": "I was impressed by its tool use. You throw at it tools, and chains them like a pro. It calls search, then fetches the URL, then based on that another search, based on all the above git clones a repo, edits it, runs tests and so on for hours without any issues. All simple tasks, of course.\n\nWhen given a huge codebase, it will still use tons of tools but will come up with wrong conclusions or have obviously wrong priorities.\n\nI used the API so far, so don't know if this holds up on local setups with quants, but I sure hope so.\n\nBtw, the model behind API is having huge issues atm as well. Almost unusable.",
              "score": 3,
              "created_utc": "2026-01-25 16:02:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n2c0s",
                  "author": "teachersecret",
                  "text": "Yeah. I found you have to loop in some agentic double checking and scaffolding to keep it on track, and on a larger codebase I think youâ€™d really want to focus it on some small piece or feature.\n\nI canâ€™t imagine actually coding with it over something like opus 4.5, but for agentic local stuff? Itâ€™s pretty damn impressive.\n\nI plan on getting vllm up and running with it once theyâ€™ve got it all dialed in there. Itâ€™s small enough that we should be able to run multiple simultaneous agents - possibly dozens of them. Iâ€™m kinda excited to see what a pile of local agents set to work could do with such reliable tool calling.",
                  "score": 3,
                  "created_utc": "2026-01-25 16:18:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1oeklx",
              "author": "floppypancakes4u",
              "text": "Im on 4090 as well, using LM studio though and im sure thats my problem, since im only getting 10tks. What setup are you using and whats your tks?",
              "score": 1,
              "created_utc": "2026-01-25 19:43:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sizd8",
              "author": "AfterAte",
              "text": "If you can, run your display off your IGPU. I could get 65K context before this build on my 3090, using 23.3GB all for llama.cpp.",
              "score": 1,
              "created_utc": "2026-01-26 09:47:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1w2p5e",
                  "author": "teachersecret",
                  "text": "I'm rolling a 4090/5900x, no igpu.\n\nThat said, it's better now. Here's my latest testing:  \n| Context | Graph Splits | VRAM | TTFT | Prompt | Generation |\n\n|---------|--------------|------|------|--------|------------|\n\n| 32K | 2 | 19.8 GB | 47.1 ms | 670.5 tok/s | 133.3 tok/s |\n\n| 64K | 2 | 21.5 GB | 48.9 ms | 648.9 tok/s | 134.6 tok/s |\n\n| 95K | 2 | 22.9 GB | \\~50 ms | \\~650 tok/s | \\~135 tok/s |\n\n| 96K | 7 | 23.0 GB | 52.0 ms | 612.1 tok/s | 125.4 tok/s |\n\n| 128K | 24 | 23.0 GB | 80.3 ms | 396.7 tok/s | 95.2 tok/s |\n\nIt's slower at 128k because it has to split the graphs a bit more (it goes from 2 to 9 or something, I think). Still works, just a bit of performance loss.\n\n| Graph Splits | Performance Impact |\n\n|--------------|-------------------|\n\n| 2 | Optimal - minimal overhead |\n\n| 7-9 | \\~7% slowdown |\n\n| 21-24 | \\~30% slowdown |\n\n| 100+ | \\~75% slowdown |\n\nAlso tried it with kv cache quantization:  \n| Configuration | Context | Generation Speed | Use Case |\n\n|---------------|---------|------------------|----------|\n\n| q4\\_0 KV | \\*\\*202K\\*\\* | \\*\\*139 tok/s\\*\\* | \\*\\*Best overall\\*\\* |\n\n| q8\\_0 KV | 128K | 138 tok/s | High-quality KV cache |\n\n| f16 KV (default) | 95K | 135 tok/s | Baseline |\n\n| CPU MoE | 202K | 32 tok/s | Low VRAM systems |\n\nOn CPU MoE it still pulls off 32 tok/s at 202k context length.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:50:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zpfp2",
                  "author": "Kitchen-Year-8434",
                  "text": "Be careful with this if you also game on your rig. I ended up with throttling and underutilized gpu in gaming because my iGPU couldnâ€™t handle the bandwidth of framebuffer copying from my dGPU. \n\nThough I am running dual 4K ultra wide resolution on a NEO G9. Might not be a problem with 4K or 1440p.",
                  "score": 1,
                  "created_utc": "2026-01-27 10:15:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mhrw5",
          "author": "Deep_Traffic_7873",
          "text": "Is re-re-download needed for the gguf?Â ",
          "score": 24,
          "created_utc": "2026-01-25 14:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mhvu3",
              "author": "jacek2023",
              "text": "no",
              "score": 12,
              "created_utc": "2026-01-25 14:43:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mtqi0",
          "author": "viperx7",
          "text": "# GLM 4.7 unsloth (data for 20k input tokens)\n\n# Before this change\n\n|Quant|GPU|Context|Prompt Processing|Token Generation|Notes|\n|:-|:-|:-|:-|:-|:-|\n|UD-Q4\\_K\\_XL|Single 4090|64k|3489 t/s|88 t/s||\n|UD-Q4\\_K\\_XL|4090 + 3060|170k|2017 t/s|52 t/s||\n|Q8|4090 + 3060|30k|2087 t/s|47.1 t/s||\n|Q8|4090 + 3060 + cpu|64k|1711 t/s|41.3 t/s|`-ot '([2][0-2]).ffn_.*_exps.=CPU'`|\n\n# After the change\n\n|Quant|GPU|Context|Prompt Processing|Token Generation|Notes|\n|:-|:-|:-|:-|:-|:-|\n|UD-Q4\\_K\\_XL|Single 4090|128k|3510 t/s|92.5 t/s||\n|UD-Q4\\_K\\_XL|4090 + 3060|200k|2041 t/s|56.2 t/s||\n|Q8|4090 + 3060|72k|2058 t/s|50.4 t/s||\n|Q8|4090 + 3060 + cpu|100k|1968 t/s|45.7 t/s|`-ot '([2][0-2]).ffn_.*_exps.=CPU'`|\n\n`no kv cache quantisation used`  \n`my GPUs are headless so this is probabily max context you can fit`\n\n`max context size for this model is 207K and in 4090+3060 senario with Q4_K_XL it fits full 200k cache and about 6gb VRAM remains empty`",
          "score": 25,
          "created_utc": "2026-01-25 15:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1md892",
          "author": "Able_Ad1273",
          "text": "what is going on with this model lmao",
          "score": 83,
          "created_utc": "2026-01-25 14:19:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mfm7x",
              "author": "-p-e-w-",
              "text": "Modern LLMs are extremely complex, with almost all of them now introducing new attention or MoE techniques, every single time.\n\nBut the biggest problem is that automated correctness testing pretty much isnâ€™t a thing, with basically no progress on that topic in the past 2 years.",
              "score": 91,
              "created_utc": "2026-01-25 14:32:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mjv9f",
                  "author": "teachersecret",
                  "text": "I am surprised someone hasn't knocked something together for that purpose.\n\nLife on the bleeding edge.",
                  "score": 18,
                  "created_utc": "2026-01-25 14:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1o46ep",
                  "author": "Objective_Mousse7216",
                  "text": "If only AI could write complex code for itself....",
                  "score": 6,
                  "created_utc": "2026-01-25 18:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ooukk",
              "author": "ilintar",
              "text": "Non-trivial architecture that has to be adapted. I told you give us a week :)",
              "score": 9,
              "created_utc": "2026-01-25 20:29:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1mdwft",
              "author": "jacek2023",
              "text": "let me quote Z.ai: \"two weeks\" ;)",
              "score": 22,
              "created_utc": "2026-01-25 14:23:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o4a3j",
                  "author": "MrWeirdoFace",
                  "text": "Tweeeeeeo weeeeeks....",
                  "score": 4,
                  "created_utc": "2026-01-25 18:58:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1miaiy",
              "author": "sleepingsysadmin",
              "text": "I get qwen next having pains on release; they did something new. \n\nThis model is cursed.",
              "score": 2,
              "created_utc": "2026-01-25 14:46:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1millz",
                  "author": "jacek2023",
                  "text": "qwen next is at least merged, look at kimi linear ;)",
                  "score": 16,
                  "created_utc": "2026-01-25 14:47:36",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1mz5hg",
                  "author": "markole",
                  "text": "Somehow it works great on my side with recent llama.cpp, opencode and unsloth q8 quant. ðŸ¤·",
                  "score": 5,
                  "created_utc": "2026-01-25 16:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1s0aa7",
              "author": "rashaniquah",
              "text": "I had a horrible time running it on vLLM too because the 0.14.0 was released a couple hours after release",
              "score": 1,
              "created_utc": "2026-01-26 07:02:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1merm0",
              "author": "teachersecret",
              "text": "Not unusual for some of these Chinese models to be broken for a few weeks while people get them properly implemented :). (it's not always the model itself, although this one SPECIFICALLY has already had multiple versions quantized and re-quantized to get it working, typically this is just a matter of implementing whatever new voodoo the model-maker added to the mix, so as usual give it a few weeks)",
              "score": -13,
              "created_utc": "2026-01-25 14:27:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1mf4bi",
                  "author": "jacek2023",
                  "text": "it's llama.cpp implementation, not the model itself",
                  "score": 20,
                  "created_utc": "2026-01-25 14:29:30",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o1mmp5l",
                  "author": "Aggressive-Bother470",
                  "text": "wtf are these downvotes, lol.Â \n\n\ntruer words ne'er be spake.",
                  "score": -6,
                  "created_utc": "2026-01-25 15:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mpkpk",
          "author": "FluoroquinolonesKill",
          "text": "This at least doubles the speed on my rig. Now I am getting about 30 t/s. Before, I was getting about 10-13 t/s.",
          "score": 7,
          "created_utc": "2026-01-25 15:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nce7x",
          "author": "jacek2023",
          "text": "BTW this model is quite popular\n\nhttps://preview.redd.it/th6swn271jfg1.png?width=1964&format=png&auto=webp&s=804be603da5a2b358a0f9f826aab4d1f1849d067",
          "score": 8,
          "created_utc": "2026-01-25 17:02:06",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1nkyaw",
              "author": "mister2d",
              "text": "We want it to succeed. ðŸ˜Š",
              "score": 8,
              "created_utc": "2026-01-25 17:38:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1msxmd",
          "author": "LagOps91",
          "text": "wait what? how does it work without using values? is this an RNN architecture?",
          "score": 3,
          "created_utc": "2026-01-25 15:37:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mt3tz",
              "author": "jacek2023",
              "text": "MLA",
              "score": 8,
              "created_utc": "2026-01-25 15:38:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mtleg",
                  "author": "LagOps91",
                  "text": "how does it avoid V cache? i was under the impression that MLA is still based on standard attention with some improvements made to increase memory efficiency. is the V cache combined with something else that's stored or how does it work?",
                  "score": 5,
                  "created_utc": "2026-01-25 15:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1olcly",
          "author": "GaboureySidibe",
          "text": "A KV data structure without the values is just a set.",
          "score": 2,
          "created_utc": "2026-01-25 20:13:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1omoi1",
          "author": "Odd-Ordinary-5922",
          "text": "getting 5 more tokens/s but its good because I was getting 25 before",
          "score": 2,
          "created_utc": "2026-01-25 20:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1n6xsg",
          "author": "harrro",
          "text": "The model is good and fast but it is so verbose in reasoning (even for simple things).\n\nIs it possible to limit/disable reasoning or is this not trained for that?",
          "score": 3,
          "created_utc": "2026-01-25 16:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o0u5d",
              "author": "robiinn",
              "text": "You can disable it with `--chat-template-kwargs '{\"enable_thinking\": false}'`",
              "score": 6,
              "created_utc": "2026-01-25 18:44:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1o8up5",
                  "author": "harrro",
                  "text": "Worked perfectly! Thank you.\n\nResponses now finishing in around 7-8 seconds instead of the 40 secconds it was taking before.",
                  "score": 1,
                  "created_utc": "2026-01-25 19:18:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nyk5v",
              "author": "viperx7",
              "text": "When I use. It directly I feel the same but somehow when using it with opencode it thinks very optimally and to the point\nThat leads me to believe \na good system prompt is what you need to make this model's thinking not too verbose",
              "score": 1,
              "created_utc": "2026-01-25 18:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1oddg2",
                  "author": "jacek2023",
                  "text": "I have same experiences, opencode somehow works, with this new patch I have kind of \"Claude Code at home\" feeling",
                  "score": 1,
                  "created_utc": "2026-01-25 19:38:14",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1o3t4c",
              "author": "nasone32",
              "text": "it reasons less at lower temperature",
              "score": 1,
              "created_utc": "2026-01-25 18:56:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ow8rv",
          "author": "ladz",
          "text": "Latest build tripled generation TPS for me. Yay!",
          "score": 1,
          "created_utc": "2026-01-25 21:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p0lep",
          "author": "alex_bit_",
          "text": "Whereâ€™s vLLM?",
          "score": 1,
          "created_utc": "2026-01-25 21:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qeoqb",
          "author": "LocoMod",
          "text": "I've abstained from using this model until the issues are ironed out. Seems like we're at a point where we can cook. What are the recommended llama-server params to primarily use it as an \"orchestrator\" that invokes tools and other agents? I'm using the Q6_K_XL Unsloth version on an RTX5090. The model is 26GB so I have 6GB to fit the maximum content in. What ctx and temp is everyone using?",
          "score": 1,
          "created_utc": "2026-01-26 01:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qopox",
              "author": "LocoMod",
              "text": "EDIT: Very inconsistent. Sometimes it works great, other times using the same exact prompt it does not.",
              "score": 1,
              "created_utc": "2026-01-26 02:03:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mkf1m",
          "author": "Cool-Chemical-5629",
          "text": "I trust ggerganov, but still I have to ask. Is this REALLY safe? I mean removal of the V portion of the cache? Is that really how the model works / is supposed to work? I just hope they aren't vibe coding this or something and that they really know what they are doing lol. Sure the model is currently slow but what the heck it's far better than other models of that size, so they better not break it more. ðŸ˜‚",
          "score": -9,
          "created_utc": "2026-01-25 14:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mlc93",
              "author": "jacek2023",
              "text": "(not sure are you trolling or not)\n\nfrom my understanding MLA uses different kind of cache, so one value (latent) is used instead two k/v",
              "score": 9,
              "created_utc": "2026-01-25 15:01:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mmzyq",
                  "author": "Cool-Chemical-5629",
                  "text": "It was a honest question, not trolling at all. Stuff breaks sometimes, it happens even to the best coders out there. I'm starting to like this model more every day, so naturally I'm anxious whenever there's a new change to the runtime which could make it run 5000 times better or leave it completely broken lol",
                  "score": 3,
                  "created_utc": "2026-01-25 15:09:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}