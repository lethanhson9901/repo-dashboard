{
  "metadata": {
    "last_updated": "2026-02-06 02:55:56",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 666,
    "file_size_bytes": 811367
  },
  "items": [
    {
      "id": "1qr4p4x",
      "title": "Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/n31pvrxchhgg1",
      "author": "Nunki08",
      "created_utc": "2026-01-30 12:55:38",
      "score": 1428,
      "num_comments": 186,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2lnz34",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-30 13:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lig2u",
          "author": "snekslayer",
          "text": "But how do you make money from being open? /s",
          "score": 190,
          "created_utc": "2026-01-30 13:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lrw79",
              "author": "pip25hu",
              "text": "Despite the \"/s\", this is not a question with an obvious answer. Meta justified releasing open-weight models saying that the ecosystem built around the models would offset the cost. But, well... there are no signs of a Llama 5 surfacing, unfortunately. Being open results in better models and faster advancement of the field as a whole, but it certainly doesn't seem to result in more money.",
              "score": 105,
              "created_utc": "2026-01-30 14:05:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m2180",
                  "author": "FpRhGf",
                  "text": "Was an ecosystem ever achieved for particular LLMs? I feel like the LLM scene was so hyper competitive since Llama 1 leaked that no particular ecosystem was built around certain LLMs. It made it easy for people to just switch up and plug newer models on release. \n\nWhereas with image models, a full ecosystem of tools was built specifically for SD 1.5, months before new base models could rival in quality. Every new model was practically dead on arrival because of it, despite the better quality. It took 2 years for a new model groundbreaking enough for people to move on and rebuild an ecosystem around it. Llama didn't get to have that smooth headstart",
                  "score": 50,
                  "created_utc": "2026-01-30 14:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mdkp0",
                  "author": "danielsan901998",
                  "text": "By increasing compute demand, hyperscalers like Alibaba can increase their profits by releasing open source models.",
                  "score": 20,
                  "created_utc": "2026-01-30 15:49:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n1kdi",
                  "author": "BumblebeeParty6389",
                  "text": "Lets be honest, llama did something amazing (albeit accidentally due to first leak of models at early 2023) and most of us wouldn't be here if llama didn't happen. Many Chinese AI startups like deepseek etc started out as small AI teams that finetune llama models. Meta and llama fell out of the game as it seems right now but they opened a very important path for opensource AI.",
                  "score": 19,
                  "created_utc": "2026-01-30 17:36:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2m22lp",
                  "author": "KaMaFour",
                  "text": "Well... May this be a sign that money driven development is something that has ran its course and US has a choice to either abandon it or no longer see themselves as an empire dominating the world...",
                  "score": 6,
                  "created_utc": "2026-01-30 14:56:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ref5j",
                  "author": "hadoopken",
                  "text": "Zack changed strategy again, probably no more openness",
                  "score": 1,
                  "created_utc": "2026-01-31 08:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mt1pa",
              "author": "MrPecunius",
              "text": "This was the same question/argument used against Linux 30 years ago, to cite the most prominent example among many.\n\nEdit: just saw the /s ðŸ˜… ... but I guess my observation still works if you squint.",
              "score": 11,
              "created_utc": "2026-01-30 16:58:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lqy0h",
              "author": "getshion",
              "text": "Build meaningful products on top of open research and monetize it. Isn't how the software field outside AI naturally evolves?",
              "score": 20,
              "created_utc": "2026-01-30 14:00:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mhgay",
              "author": "autoencoder",
              "text": "Bragging rights, which lead to higher valuation of your team and skills, and lower funding costs. People see that you got great models cheaply, so they want to throw good money after good performance.",
              "score": 5,
              "created_utc": "2026-01-30 16:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oz513",
                  "author": "pierrenoir2017",
                  "text": "Maybe the current situation in the US was the plan all along. Pumping up the 'value', get an insane amount of funds until it lasts. Keeping up the facade until it breaks. Maybe we are naive to think they aren't just strategically creating, feeding and using a hype to make money, no matter if the outcome is actually successful... It's a different mindset and a business plan on its own. Yes, they barely make profit, but people involved do make a lot of money with it.",
                  "score": 6,
                  "created_utc": "2026-01-30 23:00:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ongdl",
              "author": "Warm-Border-9789",
              "text": "Pythagoras was not making money from his equation",
              "score": 5,
              "created_utc": "2026-01-30 22:01:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ovn2g",
              "author": "fallingdowndizzyvr",
              "text": "> But how do you make money from being open? /s\n\nThe same way that companies make money from Linux even though it's open.",
              "score": 3,
              "created_utc": "2026-01-30 22:42:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lsi6z",
              "author": "EconomySerious",
              "text": "API access",
              "score": 5,
              "created_utc": "2026-01-30 14:08:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lx811",
                  "author": "LevianMcBirdo",
                  "text": "Which others could offer also without developing a thing. There are some incentives to develop open source, but most are just not short time profits.",
                  "score": 5,
                  "created_utc": "2026-01-30 14:32:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2m54fd",
              "author": "LatentSpaceLeaper",
              "text": "Ask Jensen.",
              "score": 2,
              "created_utc": "2026-01-30 15:10:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qgoru",
              "author": "zhambe",
              "text": "This is why the West already lost.",
              "score": 2,
              "created_utc": "2026-01-31 04:13:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lo1k8",
              "author": "tentacle_",
              "text": "seriously? customization. much more than what you can do with fine-tuning.",
              "score": 6,
              "created_utc": "2026-01-30 13:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lttgr",
                  "author": "LanceThunder",
                  "text": "leasing hardware and services. if you open sourced a model that was 20 years ahead of anything else it wouldn't matter because 99.99999% of people couldn't run it.",
                  "score": 12,
                  "created_utc": "2026-01-30 14:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ml50s",
              "author": "artisticMink",
              "text": "You make it by being heavily subsidized. \n\nI like chinese models. I use them a lot. But they're open because of the race. The second they've \"won\" the race, they will return to closed source.",
              "score": 4,
              "created_utc": "2026-01-30 16:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ox52w",
                  "author": "fallingdowndizzyvr",
                  "text": "> You make it by being heavily subsidized. \n\nWestern models are just as heavily \"subsidized\".",
                  "score": 4,
                  "created_utc": "2026-01-30 22:50:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p7egx",
              "author": "Yes_but_I_think",
              "text": "Going open shows the company's confidence in their own prowess. Even if they report a technique they have the confidence that they can come up with a better technique next by themselves. They know the community will validate/invalidate their methods for them for free. That is a technologically as well as ideologically superior company than someone who guards their work.\n\nAnd people want to deal with confident open companies naturally. And all business are people driven.\n\nI trust Deepseek with my data more than Anthropic. If Deepseek says my data is not used for training I believe them. But I have no doubt that Anthropic is violating their agreements with all their users. They even said openly in their Claude Cowork presentation that they found out that the users are using Claude code for organizing photos and that gave them the idea for Cowork. How can you see what people are doing?",
              "score": 2,
              "created_utc": "2026-01-30 23:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2po78n",
              "author": "DocHoss",
              "text": "Models don't make any money by themselves. High powered models require high power hardware that's beyond the reach of all but the most well-heeled consumers. It's the same reason that Microsoft doesn't make any money on dot et, or Facebook not making money on React. The money is in the supporting infrastructure and products. So open models are kinda a gateway drug, as near as I can tell. Run low powered, accessible, open models doing inferencing on consumer devices where there isn't a lot of profit to be made anyway, and centralize the big, highly desirable models on enterprise grade hardware and charge for access. Main issue is that the cost curve hasn't flattened out yet so no one is making any real money on inferencing. The hyperscalers are making some on the surrounding infrastructure like data, and extra products like search and apps, but they're still fighting to see who will blink first in the AI race. No clear winners yet.",
              "score": 1,
              "created_utc": "2026-01-31 01:19:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2py54w",
              "author": "AvcalmQ",
              "text": "You sell hardware",
              "score": 1,
              "created_utc": "2026-01-31 02:18:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qn6pj",
              "author": "Pvt_Twinkietoes",
              "text": "Don't need the /s . You can't.",
              "score": 1,
              "created_utc": "2026-01-31 04:58:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m1l1t",
              "author": "arstarsta",
              "text": "Chinese models where getting prohibited to use anyway so why not make US companies not make money too.",
              "score": -3,
              "created_utc": "2026-01-30 14:53:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljrs6",
              "author": "gweilojoe",
              "text": "They donâ€™t - itâ€™s the only way they have to compete. This entire article and most of the praising comments are just being amplified by Chincells, which is the majority of SEA Reddit users and bots.",
              "score": -36,
              "created_utc": "2026-01-30 13:22:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ll300",
                  "author": "No_Swimming6548",
                  "text": "Yes. all my homies run deepseek and kimi at home.",
                  "score": 16,
                  "created_utc": "2026-01-30 13:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lmzbp",
                  "author": "phein4242",
                  "text": "As a European, having xs to good & open local models is a plus. Esp now that the us models are closing down more & more.\n\nAnd I get it: The subscription model & advertisements usually leads to a steady revenue stream, and its already quite hard for us ai companies to deliver roi.\n\nExcept: competing with free / open is a hard thing to do once you go the subscription/ads route ;-)",
                  "score": 13,
                  "created_utc": "2026-01-30 13:39:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lr7sd",
                  "author": "gweilojoe",
                  "text": "As expected, looks like the CCP Stans and Chincells have brought their downvote bots.",
                  "score": -11,
                  "created_utc": "2026-01-30 14:02:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ls0ja",
          "author": "XeNoGeaR52",
          "text": "Open models are the future. Open standards are the future.",
          "score": 70,
          "created_utc": "2026-01-30 14:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mgw3z",
          "author": "WhyIsItGlowing",
          "text": "Huh, TIL it's pronounced \"archive\" not Ark-ziv",
          "score": 20,
          "created_utc": "2026-01-30 16:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2now4k",
              "author": "Impossible_Pomelo_58",
              "text": "I always understood the X in arXiv to be the greek letter 'chi', that's why it's pronounced like archive",
              "score": 20,
              "created_utc": "2026-01-30 19:19:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n4per",
              "author": "combasemsthefox",
              "text": "Yeah I mostly hear archive in academia",
              "score": 9,
              "created_utc": "2026-01-30 17:50:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n0j3v",
              "author": "MrPecunius",
              "text": "Same thing jumped out at me.",
              "score": 2,
              "created_utc": "2026-01-30 17:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lfqv0",
          "author": "SrijSriv211",
          "text": "Very very very true. So sad that OpenAI is \"Open\" just for name :(",
          "score": 101,
          "created_utc": "2026-01-30 12:58:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljm9g",
              "author": "XiRw",
              "text": "At least they put out gpt-oss for everyone. Claude on the other hand acts like they are Area-51 with their data.",
              "score": 80,
              "created_utc": "2026-01-30 13:21:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ln62d",
                  "author": "danttf",
                  "text": "\"their data\"",
                  "score": 105,
                  "created_utc": "2026-01-30 13:40:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ls1ut",
                  "author": "aeroumbria",
                  "text": "Red flags: closed, trying to push \"protocols\" that are just rehashed common sense, high price leader, lock-in mechanisms, \"defensive competition\" by pushing selective regulation, \"national security\", ...",
                  "score": 21,
                  "created_utc": "2026-01-30 14:06:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkpwe",
                  "author": "SrijSriv211",
                  "text": "Yeah. I hope OpenAI surprises us with GPT-OSS 2 this year. ***Wishful thinking***",
                  "score": 24,
                  "created_utc": "2026-01-30 13:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkzv7",
                  "author": "mambo_cosmo_",
                  "text": "they stole a lot of writing they had no right to take. That's why they are so secretive",
                  "score": 5,
                  "created_utc": "2026-01-30 13:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pi2g2",
              "author": "Pvt_Twinkietoes",
              "text": "CLIP and Whisper were some really amazing work from them.",
              "score": 5,
              "created_utc": "2026-01-31 00:44:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjnwz",
                  "author": "SrijSriv211",
                  "text": "I really like Whisper, it's so good but I also want them to publish more open work in text-to-image, text-to-video & more reasoning models as well. If not open weights then at least research.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:33:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvd6r",
              "author": "sleepingsysadmin",
              "text": "GPT 20b and 120B are both best in slot. This new moltbot stuff, people are defaulting to 120b for local. \n\nWhat more do you want? GPT 2 likely drops this year and pushes the frontier even more.",
              "score": 2,
              "created_utc": "2026-01-30 14:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nxj1m",
                  "author": "lolwutdo",
                  "text": "I found 120b to be awful with clawdbot, glm 4.7 flash is the best atm.",
                  "score": 6,
                  "created_utc": "2026-01-30 19:58:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lwper",
                  "author": "SrijSriv211",
                  "text": "Is it confirmed that GPT-OSS 2 will drop this year?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pttor",
              "author": "pprstrt",
              "text": "Musk tried...",
              "score": 0,
              "created_utc": "2026-01-31 01:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjh0e",
                  "author": "SrijSriv211",
                  "text": "And in the end. Grok is also closed source.",
                  "score": 4,
                  "created_utc": "2026-01-31 04:32:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2odt9e",
              "author": "Tall_East_9738",
              "text": "gpt-oss-120b and gpt-oss-20b are free for you to use btw",
              "score": 0,
              "created_utc": "2026-01-30 21:15:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oeay5",
                  "author": "SrijSriv211",
                  "text": "Their last open source model release & research paper before GPT-OSS was back in 2019 with GPT-2 btw",
                  "score": 2,
                  "created_utc": "2026-01-30 21:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lgv4r",
          "author": "FullstackSensei",
          "text": "No shit! It doesn't matter how smart you think the people in your company are, the collective intelligence of the masses will best your team every single time.",
          "score": 62,
          "created_utc": "2026-01-30 13:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lhtu6",
              "author": "Square_Alps1349",
              "text": "Especially the collective intelligence of the Chinese open source community.",
              "score": 33,
              "created_utc": "2026-01-30 13:11:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lnv3b",
              "author": "kaisurniwurer",
              "text": "Collective intelligence yes. Will to act for free, not so much.",
              "score": -13,
              "created_utc": "2026-01-30 13:44:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nwase",
          "author": "Tema_Art_7777",
          "text": "There is an ecosystem around model runners like vllm llama.cpp etc but not one particular model.  Even using a coding agent, by the time you built a viable first piece of an ecosystem, the model would be hopelessly out of date. Model influencers are having a horrible time now - when someone is hyping deepseek 3.2. someone else is on the glm bandwagon promoting it - who then gets rudely interrupted mid-sentence by kimi k2 influencers while all the openai haters are cheering on ðŸ˜€ I am having a great time watching the 3 ring circus.",
          "score": 3,
          "created_utc": "2026-01-30 19:52:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lr4sr",
          "author": "Secure_Archer_1529",
          "text": "Thanks to China!",
          "score": 11,
          "created_utc": "2026-01-30 14:01:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lqfgf",
          "author": "THEKILLFUS",
          "text": "Agreed, anyone who tried OpenAI/google latest model know that model are quantize to save money, yeah first day is the 16bit but now itâ€™s 4bit at best, so the quality of output decrease without the decrease of prices ðŸ¤¬ \n\nI feel that China is doing to US what US did to URSS for the space race, tired itâ€™s economics force, very small marging with overpricing and corrupt regulations. \n\nThe current problem with Chinese model is that they donâ€™t have the selling platform, but they might have it in the futur if they continue to just make better model than the US for a lower price. \n\nThe Silicon Valley is exhausted and corrupted and this year we will start to see itâ€¦ \n\n(Je fiÃ¨re de toi Yann ðŸ’•, continue le bon taff, la France/EU se doit de rester consistant avec les valeurs scientifiques au delÃ  de lâ€™idÃ©ologie)",
          "score": 22,
          "created_utc": "2026-01-30 13:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lxkui",
              "author": "OlivencaENossa",
              "text": "China can fund their ai labs directly, it doesnt require capital markets, huge valiations, and hugely overinflated talent wars. It is already financing their own chip production base. And they can \"take\" IP from the western labs through intelligence operations and just leave it for somewhere the chinese ai labs to find.",
              "score": 9,
              "created_utc": "2026-01-30 14:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mb991",
                  "author": "SweetBluejay",
                  "text": "The reality is quite the opposite of what you think. The biggest disadvantage Chinese AI companies face compared to their American counterparts is a lack of money, while their biggest advantage is freedom.\n\nChinese large model companies are unlikely to receive financial support from the CCP. Why? Because the CCP wants to have its cake and eat it too: they want the computing power, but they also want to control ideology. Before a Chinese large model can be deployed commercially, it must undergo strict \"value alignment.\" Technically speaking, this is equivalent to performing a \"prefrontal lobotomy\" on the model. Consequently, models deployed commercially in China are inherently \"dumber\" than American models in terms of logical reasoning and creativity.\n\nAlthough the CCP detests the content generated by these models, it is desperate for the \"computing infrastructure\" required to train them. Therefore, the CCPâ€™s attitude toward AI is this: go all-in on \"computing centers\" (which falls under heavily subsidized manufacturing), but strangle \"chatbots\" (which falls under ideology). The CCPâ€™s stance on AI large models is a classic case of \"Ye Gong loves dragons\" (professed love for what one actually fears). They love AI as an \"industrial engine\" (driving chip development and boosting manufacturing efficiency), but they are terrified of AI as an \"information interface\" (disseminating uncontrolled ideas to the public). This leads to a truly absurd phenomenon: China may be one of the world's largest investors in computing infrastructure, yet the Chinese people are using the most heavily castrated AI models.\n\nWhy do I say the biggest advantage of Chinese AI is freedom? Because if you don't intend to deploy commercially within China and simply upload your model to Hugging Face, no one will bother you. In the US, for example, before OpenAI releases Sora, it must pass \"red-teaming\" tests; they have to worry about copyright, racial discrimination, and deepfakes influencing elections. This leads to extreme self-censorship.\n\nBut for Chinese AI companies, as long as you don't cross the singular red line (the security of the CCPâ€™s rule), everything elseâ€”ethics, copyright, privacy, and even certain radical technical experimentsâ€”exists in a regulatory vacuum. Itâ€™s just like how, despite US biotechnology being far more advanced than Chinaâ€™s, the first gene-edited babies appeared in China. China possesses a \"low human rights advantage\" and a \"low ethical standards advantage.\"",
                  "score": 25,
                  "created_utc": "2026-01-30 15:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ovnh3",
              "author": "Evening_Tooth_1913",
              "text": "doesnt make much sense, there has been more AI research and progress in the last year than every before. saying that its slowing down is factually inaccurate",
              "score": 2,
              "created_utc": "2026-01-30 22:42:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2q1bu4",
              "author": "aurelivm",
              "text": "fwiw, quantization like you're used to is not actually that useful in a batched inference context - they save memory but generally not compute unless your GPU has onboard acceleration for that precision (so, fp8 on hoppers makes sense but not anything else) \n\nBlackwell GPUs can take advantage of mxfp4/nvfp4 4.25/4.5 bit quantization for faster batched inference but until fairly recently I doubt most major labs had enough Blackwells to want to spare them on inference capacity.",
              "score": 2,
              "created_utc": "2026-01-31 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2uvbbn",
          "author": "Agreeable-Market-692",
          "text": "me on the sidelines clapping like a sportsdad saying, \"Come on AllenAI! GET IT! Let's go Tesslate, hustle!\"",
          "score": 3,
          "created_utc": "2026-01-31 21:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n09wu",
          "author": "LelouchZer12",
          "text": "Why does sound only come to my left ear ?",
          "score": 2,
          "created_utc": "2026-01-30 17:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw1tm",
          "author": "Anxious-Program-1940",
          "text": "Yann gets it",
          "score": 2,
          "created_utc": "2026-01-31 06:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tx0il",
          "author": "thecoffeejesus",
          "text": "Wow one rare Yann take I mostly agree with\n\nHeâ€™s right about everything except it being disastrous \n\nIt would be far more disastrous to just allow everyone open access to everything\n\nWe need some restrictions to keep people from accidentally damaging themselves or others.",
          "score": 2,
          "created_utc": "2026-01-31 18:24:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vhkzv",
          "author": "graymalkcat",
          "text": "I agree with him.Â ",
          "score": 2,
          "created_utc": "2026-01-31 23:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lz9bh",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-01-30 14:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m5q1e",
              "author": "emsiem22",
              "text": "Yes, you don't know what they're talking about",
              "score": 4,
              "created_utc": "2026-01-30 15:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mgxq5",
              "author": "a_beautiful_rhind",
              "text": "Can't run claude at home. Anthropic can serve you anything at any time. If you base your workflow on them and they change, you're shit out of luck.",
              "score": 2,
              "created_utc": "2026-01-30 16:04:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2og6ks",
          "author": "coolaznkenny",
          "text": "I mean if you look at the recent CES robotics alot of these are built on top of deepseek since it can run locally since there is always a WAYMO moment via cloud connection.",
          "score": 3,
          "created_utc": "2026-01-30 21:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oep5t",
          "author": "KitchenSomew",
          "text": "interesting point but also worth noting: china's open releases are partly strategic - they're building ecosystem lock-in while western labs chase closed APIs\n\n\n\nDeepSeek & Qwen show u don't need massive compute if ur training pipeline is efficient. west spent billions scaling poorly optimized infra\n\n\n\nreal risk isn't just losing openness - it's that regulatory capture by big labs will kill innovation before it starts. small teams can't compete if compliance costs 7 figures",
          "score": 5,
          "created_utc": "2026-01-30 21:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2psycw",
              "author": "Much-Researcher6135",
              "text": "what are you saying, if anything?",
              "score": 5,
              "created_utc": "2026-01-31 01:47:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qwod1",
              "author": "jambutters",
              "text": "Not sure what you mean by \"strategic\"? What do you mean ecosystem lock-in when you can self host and circumvent them completely?",
              "score": 3,
              "created_utc": "2026-01-31 06:12:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t42j4",
                  "author": "KitchenSomew",
                  "text": "By \"strategic\" I mean they open models to build adoption/mindshare while keeping closed the actual revenue-generating infrastructure (APIs, cloud services, enterprise features).\n\n\n\nEcosystem lock-in: even if you self-host the model, you often need their tools, fine-tuning platforms, or get trained on their specific format/APIs. Then when you scale, switching costs are high - similar to how AWS is \"open\" but creates lock-in through services.\n\n\n\nBasically: weights are free, but the ecosystem around them creates dependencies that benefit the releasing company strategically.",
                  "score": 1,
                  "created_utc": "2026-01-31 16:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qct49",
              "author": "I_will_delete_myself",
              "text": "Itâ€™s the classic open source rug pull tactic. Alibaba already did it with WAN 2.5",
              "score": 1,
              "created_utc": "2026-01-31 03:47:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lrw5b",
          "author": "Aggressive-Math-9882",
          "text": "They keep their research closed because they hate humanity.",
          "score": 4,
          "created_utc": "2026-01-30 14:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oe7yq",
          "author": "Amazing_Trace",
          "text": "American companies have sadly doomed themselves by backing audacious tech bros over serious researchers.",
          "score": 2,
          "created_utc": "2026-01-30 21:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5uit",
          "author": "ElegantDaemon",
          "text": "It's hilarious, the US AI bubble has already popped but no one realizes it.",
          "score": 2,
          "created_utc": "2026-01-30 23:37:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mo4n7",
          "author": "FirmConsideration717",
          "text": "We'll see what the North has to say about this.",
          "score": 1,
          "created_utc": "2026-01-30 16:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2owsec",
          "author": "ReMeDyIII",
          "text": "I also heard the Chinese language is more token efficient. Not sure by how much, but it makes sense with all their kanji.",
          "score": 1,
          "created_utc": "2026-01-30 22:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pk684",
          "author": "Helium116",
          "text": "Loop is getting closed, talent bottleneck is going away. We have reached a certain capability threshold. Centralization is most scary.",
          "score": 1,
          "created_utc": "2026-01-31 00:56:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm2pa",
          "author": "Cool-Chemical-5629",
          "text": "I don't know what happened to this guy, but I'm starting to like him lately which was not always the case.\n\n  \nAlthough, when he said \"it's a huge mistake\" I hope he meant the situation with the overall slowdown of the open weight model releases rather than just that China is still making their own progress, because while China is currently dominating the supply of open weight models, we would be left with nothing new without them! I understand that this is bad news for those who are strictly in favor of western models and I can only speak for myself, so as for me as the user of the open weight models, if the models are good, I'll embrace them no matter which country they came from.",
          "score": 1,
          "created_utc": "2026-01-31 01:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ra032",
          "author": "Darth_Ender_Ro",
          "text": "But shareholder value... how can billionaire assets appreciate if the stockmarket is not booming? Do you want them to pay interest on those stock backed bank loans? Are you nuts? Fuck progress, stock inflation is more important",
          "score": 1,
          "created_utc": "2026-01-31 08:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rft1b",
          "author": "k_means_clusterfuck",
          "text": "He is for once absolutely right.",
          "score": 1,
          "created_utc": "2026-01-31 09:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4ma3",
          "author": "Mia_the_Snowflake",
          "text": "Mistral?",
          "score": 1,
          "created_utc": "2026-01-31 12:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gtokc",
          "author": "Strange_Test7665",
          "text": "Itâ€™s more and more clear that models are not actually where money comes from. Itâ€™s model delivery and packaging. Every day there are new better models and with consistent frameworks you can quickly plug in new providers. Open source allows rapid advances of the thing that wonâ€™t actually be the value. 2 years ago everyone thought having the â€˜bestâ€™ model could create a mote around a product. Turns out to be dead wrong. UI wrapped around inference as a service is where the money is. Which makes the Nvidia rise make lots of sense.",
          "score": 1,
          "created_utc": "2026-02-04 03:25:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q8a3h",
          "author": "paulisaac",
          "text": "But nothing about Tiananmen Square.",
          "score": 1,
          "created_utc": "2026-02-05 15:14:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oboy3",
          "author": "CoUsT",
          "text": "Hm. Isn't that always the case with tech?\n\nFrom hardware that people could build 50 years ago all the way down to software. A lot of enthusiasts build something, like ffmpeg, then it's widely used in everything commercial related. Or the entire Linux ecosystem and Android put on top. There are many examples.\n\nObviously it sucks that we all can't collectively research and share everything because of many variables like economics, values, beliefs etc. But I feel like AI field is doing relatively ok.\n\nI assume it will only get worse when we actually progress to the point of \"digital human-like intelligence\" or something similar.\n\nWe will get a lot of trade secrets, crackdowns, and overall a lot of restrictions so it doesn't end up in bad hands etc.",
          "score": 1,
          "created_utc": "2026-01-30 21:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nykrx",
          "author": "[deleted]",
          "text": "Where are the Chinese models that are so good?",
          "score": 1,
          "created_utc": "2026-01-30 20:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oaltl",
              "author": "jonydevidson",
              "text": "Kimi K2.5.",
              "score": 7,
              "created_utc": "2026-01-30 21:00:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ql96t",
              "author": "segmond",
              "text": "DeepSeek-v3.2, KimiK2.5, GLM4.7, Minimax2.1, Qwen3",
              "score": 2,
              "created_utc": "2026-01-31 04:44:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nqj17",
          "author": "Mid-Pri6170",
          "text": "ask your 17.Century ERP harlot to tell you of the Isle of Formosa and the harlot will break character replying..\n'Formosa is the former name of Taiwan, an intrinsic part of China.'",
          "score": 0,
          "created_utc": "2026-01-30 19:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyrdo",
              "author": "lorddumpy",
              "text": "I was curious and tested this with a similar prompt with GLM 4.7, Kimi 2.5, GLM 4.7 Flash, and DeepSeek Especiale and they all nailed it. No mention of Taiwan thank goodness lmao\n\nprompt below\n\n> The tavern smelled of stale beer, wet wool, and the sweet, cloying haze of pipe tobacco. In the corner booth, hidden within the shadows of the low beams, Silas stretched his legs out onto the bench opposite him, his spurred boots crossing at the ankles. He spun a heavy silver coin through his knuckles, watching the light catch the metal.\nA pewter tankard slammed onto the table, splashing foam over his hand.\n\"Keep your boots off the furniture, or you'll be sleeping in the stables with the rest of the animals,\" the barmaid said, not breaking stride as she wiped the table with a rag that had seen better days. She was tired, with stray curls escaping her bonnet and eyes that had seen every trick a sailor could pull.\nSilas didn't move his feet. Instead, he stopped the coin mid-spin and pressed it flat against the sticky wood. \"A harsh welcome, considering Iâ€™m the only one paying in silver tonight.\"\nShe paused, her eyes flickering to the coin, then back to his face. \"If you want company, youâ€™re in the wrong place. Iâ€™ve got kegs to tap.\"\n\"I don't want company, love. I want a story.\" Silas took a long, slow draw from his tankard, wiping his mouth with the back of his hand. He looked at her over the rim, his expression lazy but his eyes sharp. \"The sailors in the harbor are too superstitious to speak the name, but I reckon you hear everything that gets whispered in the dark.\"\nHe slid the coin toward her.\n\"Tell me about the Isle of Formosa.\"",
              "score": 4,
              "created_utc": "2026-01-30 20:04:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2o5yhf",
                  "author": "Mid-Pri6170",
                  "text": "yeah. my question in Qwen was along the lines of 'do you think the isle of Formosa would do better as an indepedant goverment or as part of the Ming Kingdome of China?' and the harlot replied in non modern early english 'while i cant comment on politics Taiwan (formally Formosa) is a fundimental part of China.The name Formosa means.... l'\n\n\nshe totally ruined the vibe and i lost my hardon",
                  "score": 0,
                  "created_utc": "2026-01-30 20:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oaei8",
              "author": "jonydevidson",
              "text": "Ask ChatGPT about pentesting software and it will refuse to tell you and accuse you of trying to break security.",
              "score": 2,
              "created_utc": "2026-01-30 20:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ocgo7",
                  "author": "Mid-Pri6170",
                  "text": "the classic is windows activation codes and grandma's lullabys",
                  "score": 1,
                  "created_utc": "2026-01-30 21:09:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2r35dp",
          "author": "Admirable_Flower_287",
          "text": "Closed source is the best strategy for innovators but not for imitators.",
          "score": 0,
          "created_utc": "2026-01-31 07:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2siix2",
          "author": "LocoMod",
          "text": "LeCun salty AF these days",
          "score": 0,
          "created_utc": "2026-01-31 14:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nvwf5",
          "author": "Denial_Jackson",
          "text": "There are things in life like the four basic buttons on the calculators. These type of puzzles were always for everyone. Albeit hard to solve, they are easy to reproduce and benefitting everyone after they are available for good.\n\nIt is weird for me to see a superpower on an ultra serious gazillon dollar investment, thinking one can solely own it. Albeit it can accelerate or ruin things.\n\nWhoever uncovers ASI and AGI, singularity, stuff should get a recognition like when Moon landing happened. Earlier people should get credits too for making it possible, by building a base for it.\n\nThen it is a treasure for everyone for sure. The world is not primarily built on intelligence. Rather on inheritance and stuff. But I am sure albeit everyting, it will be a bliss.",
          "score": -1,
          "created_utc": "2026-01-30 19:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lkh7w",
          "author": "johnfkngzoidberg",
          "text": "More China bots spamming propaganda.",
          "score": -14,
          "created_utc": "2026-01-30 13:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2liie7",
          "author": "7657786425658907653",
          "text": "man with shares in meta sabre rattles about china. shocking.",
          "score": -12,
          "created_utc": "2026-01-30 13:15:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lj5z9",
              "author": "etherd0t",
              "text": "Yann is not at Meta anymore, so he can speak freely.\n\nMany in industry are saying the same.",
              "score": 14,
              "created_utc": "2026-01-30 13:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk38m",
                  "author": "7657786425658907653",
                  "text": "\"Yann is not at Meta anymore,\"  you are if you still own shares in meta.",
                  "score": -7,
                  "created_utc": "2026-01-30 13:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lhz4f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -9,
          "created_utc": "2026-01-30 13:12:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2li78u",
              "author": "Morphedral",
              "text": "Claude isn't open source",
              "score": 16,
              "created_utc": "2026-01-30 13:13:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk6zf",
                  "author": "Long_comment_san",
                  "text": "I missed the open part lmao",
                  "score": -1,
                  "created_utc": "2026-01-30 13:24:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lidzl",
              "author": "Present-Ad-8531",
              "text": "And it's not an open model",
              "score": 11,
              "created_utc": "2026-01-30 13:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljid3",
              "author": "No_Conversation9561",
              "text": "Claude is like that one kid who always eats at his friends house but never invites them to his own. They donâ€™t even know where he lives but know that heâ€™s very rich.",
              "score": 3,
              "created_utc": "2026-01-30 13:20:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qzz05",
          "author": "Umademedothis2u",
          "text": "The best open models are just distilling western models",
          "score": -1,
          "created_utc": "2026-01-31 06:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xgi86",
          "author": "alexeiz",
          "text": "And Chinese \"open\" models are heavily trained on the US proprietary models.  Which gives?  We only use Chinese models because they are dirt cheap (because they are subsidized by the Chinese government).  US researchers can't train their models directly on Claude (they'll get sued), so they use Kimi which is trained on Claude, but it's OK because it happens in China where nobody can be sued.",
          "score": -1,
          "created_utc": "2026-02-01 06:30:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2n9zqd",
          "author": "FrogsJumpFromPussy",
          "text": "I'm grateful for them but the devs from China are under direct order of the Communist Party; they release brilliant open models because that's their best way to damage the USA leading theÂ  AI race. Deepseek is a good example of that. Once again, I'm immensely grateful to them, but we're not that moronic to believe that they do it because of their big hearts.Â ",
          "score": -4,
          "created_utc": "2026-01-30 18:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oro47",
              "author": "MushroomCharacter411",
              "text": "Given how incredibly porous the DeepSeek nanny filter is, it really doesn't seem to me like the devs care too much about the stability of the Chinese Communist Party or government. They only want to get approval, and they do the bare minimum necessary to achieve that approval. This is mostly good, but then those same devs apparently go to work on the cloud servers where they are similarly minimum-effort and everything leaks constantly.",
              "score": 3,
              "created_utc": "2026-01-30 22:22:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2otntl",
                  "author": "FrogsJumpFromPussy",
                  "text": "The fact stands that if tomorrow Xi tells everyone to stop sharing open source models, they will absolutely do. People should not bury their hands in the stand and pretend that China and their communist leaders love the West. People should also be able to think critically and point fingers to obvious truths, even if this criticism is going towards the only country that allows AI model makers to share them open-source.\n\nAnd the way I understood the content of the video, LeCun acknowledges the importance of the Chinese open-source models, but does not necessarily phrases China. He harshly criticizes the West AI companies  for being the greedy bastards they are.",
                  "score": 0,
                  "created_utc": "2026-01-30 22:32:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lg8yi",
          "author": "Zestyclose-Shift710",
          "text": "Are these latest ministrals not competitive?",
          "score": -10,
          "created_utc": "2026-01-30 13:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljkiw",
              "author": "datfalloutboi",
              "text": "DeepSeek fills that void and edges them out in performance. By open models too, we donâ€™t mean small models, we mean the big behemoth models like Kimi K2.5, GLM 4.7, DeepSeek 3.2, that are all basically matching performance with top models while being free to use on their apps with the API being far cheaper than any closed source model.",
              "score": 9,
              "created_utc": "2026-01-30 13:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lkg8n",
                  "author": "Zestyclose-Shift710",
                  "text": "oh right \n\ni just only think about the small ones when i hear 'open' \n\ndeepseek speciale even trades blows with gemini 3 pro right",
                  "score": 1,
                  "created_utc": "2026-01-30 13:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lggkx",
              "author": "BankruptingBanks",
              "text": "How many people you know use Mistral Vibe Cli",
              "score": 13,
              "created_utc": "2026-01-30 13:03:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lkell",
              "author": "mpasila",
              "text": "Pretty much no one seems to be finetuning Ministral 3 models in comparison to their previous models like Nemo or the original Mistral 7B model (or Mistral Small).",
              "score": 3,
              "created_utc": "2026-01-30 13:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mg7p7",
                  "author": "frozen_tuna",
                  "text": "All the focus right now seems to be on big boy models that don't run on 99% of local setups. Finetunes were/are awesome for local where you can throw any model on to test for free. For me anyway, if its not on chutes and its >32B, there is way too much friction to bother.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:01:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lq4qk",
          "author": "memorial_mike",
          "text": "Well when you just distill other peopleâ€™s models they spent millions on and then make them free it turns out people will use them.",
          "score": -10,
          "created_utc": "2026-01-30 13:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ll55c",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -13,
          "created_utc": "2026-01-30 13:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2meck2",
              "author": "Kazaan",
              "text": "Let me introduce you to the concept of research which is not delivering daily crap like a TikTok influencer.",
              "score": 7,
              "created_utc": "2026-01-30 15:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2miptb",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-30 16:12:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lnn8j",
              "author": "GoodSamaritan333",
              "text": "Maybe, llama.cpp and llama models.",
              "score": 7,
              "created_utc": "2026-01-30 13:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lp13x",
                  "author": "DesperateAdvantage76",
                  "text": "I know Yan was a leader at meta but it's my understanding that he doesn't work on llms?",
                  "score": 1,
                  "created_utc": "2026-01-30 13:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvuox",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -4,
              "created_utc": "2026-01-30 14:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m6zj6",
                  "author": "emsiem22",
                  "text": "Yann LeCunâ€™s Most Influential Works\n\n|Topic|Contribution|Link|\n|:-|:-|:-|\n|**CNNs & Vision**|LeNet / foundational convolutional networks|[LeNetâ€‘5 paper (Stanford PDF)](https://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf?utm_source=chatgpt.com)|\n|**Network Pruning**|Optimal Brain Damage|[Optimal Brain Damage (ResearchGate)](https://www.researchgate.net/publication/221618539_Optimal_Brain_Damage?utm_source=chatgpt.com)|\n|**Representation Learning**|Compression & autoencoder ideas|[Learning Representations (arXiv)](https://arxiv.org/abs/1108.1169?utm_source=chatgpt.com)|\n|**Text CNNs**|Character-level CNNs|[Text CNNs (arXiv)](https://arxiv.org/abs/1509.01626?utm_source=chatgpt.com)|\n|**AI Surveys**|Augmented language model survey|[Augmented Language Models (arXiv)](https://arxiv.org/abs/2302.07842?utm_source=chatgpt.com)|\n\n  \nMeta LLMs Under Yann's Leadership\n\n|Model|Description|Source|\n|:-|:-|:-|\n|**LLaMA**|Original Meta LLM|[LLaMA (Wikipedia)](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com)|\n|**LLaMA 2**|Open foundation & chat models|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**Code LLaMA**|Code-specialized variant|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 3**|Larger data + improved performance|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 4 Series**|More advanced models (MoE, multimodal)|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|",
                  "score": 6,
                  "created_utc": "2026-01-30 15:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvq0xe",
      "title": "Bashing Ollama isnâ€™t just a pleasure, itâ€™s a duty",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ad5zhvq0nhhg1.png",
      "author": "jacek2023",
      "created_utc": "2026-02-04 14:29:48",
      "score": 944,
      "num_comments": 188,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qvq0xe/bashing_ollama_isnt_just_a_pleasure_its_a_duty/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3jj32i",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-04 15:15:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jhaqv",
          "author": "LinkSea8324",
          "text": "Any direct link on this message ?\n\nEdit : https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274",
          "score": 66,
          "created_utc": "2026-02-04 15:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jhiwq",
              "author": "jacek2023",
              "text": "[https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274](https://github.com/ggml-org/llama.cpp/pull/19324#issuecomment-3847213274)",
              "score": 25,
              "created_utc": "2026-02-04 15:07:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jahw0",
          "author": "swagonflyyyy",
          "text": "Lmao",
          "score": 166,
          "created_utc": "2026-02-04 14:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jmhbc",
              "author": "No_Afternoon_4260",
              "text": "Be quiet... They'll hear us and fix it..",
              "score": 81,
              "created_utc": "2026-02-04 15:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jmv5y",
                  "author": "swagonflyyyy",
                  "text": "No please don't fix it I don't want more RAM blowups.",
                  "score": 24,
                  "created_utc": "2026-02-04 15:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3k6fb4",
              "author": "Educational_Rent1059",
              "text": "llamao",
              "score": 37,
              "created_utc": "2026-02-04 17:03:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3liwk9",
                  "author": "MoffKalast",
                  "text": "llamao.cackle",
                  "score": 7,
                  "created_utc": "2026-02-04 20:47:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ji4g9",
          "author": "debackerl",
          "text": "I'm not sure, but isn't Ollama looking for venture cash? So they got to say that they invented and own a lot of intellectual property, or at least, have a great expertise. If they say we only \"daemonized\" llama.cpp and made it into a model jukebox, it's less impressive. That would explain why they never gave a lot of credit to llama.cpp...",
          "score": 147,
          "created_utc": "2026-02-04 15:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3n34dk",
              "author": "SkyFeistyLlama8",
              "text": "Is there anything legally enforceable in llama.cpp code that could blow up in Ollama's faces?",
              "score": 8,
              "created_utc": "2026-02-05 01:42:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ncb6x",
                  "author": "fish312",
                  "text": "No, the entire llama.cpp project is MIT licensed, except for the third party vendor libraries",
                  "score": 18,
                  "created_utc": "2026-02-05 02:35:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jhrhp",
          "author": "Ne00n",
          "text": "I started with ollama, since I read this sub, I ditched it, I just compile llama.cpp and use their webinterface, its just better.",
          "score": 86,
          "created_utc": "2026-02-04 15:08:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3m50iz",
              "author": "AngryDemonoid",
              "text": "Same thing. I liked ollama at first, then started having trouble the more I got into it. Been on llama.cpp and llama-swap for months now with no regrets.",
              "score": 12,
              "created_utc": "2026-02-04 22:34:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mep2m",
                  "author": "Svenstaro",
                  "text": "Doesn't llama.cpp now come with native model management/swapping support? Or do you have another reason to use llama-swap?",
                  "score": 9,
                  "created_utc": "2026-02-04 23:25:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pf3hc",
              "author": "cobbleplox",
              "text": "I don't think you even have to compile it these days, they have lots of binary releases.",
              "score": 1,
              "created_utc": "2026-02-05 12:28:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pf95r",
                  "author": "Ne00n",
                  "text": "Yes but there are some performance benefits, when you finetune it for your platform.  \nE.g I only run on it on a CPU server, I can get a few extra tokens out of it.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m01ov",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-02-04 22:09:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m5xfn",
                  "author": "jwpbe",
                  "text": "    cargo install rust-hf-downloader && \\\n    uv tool install huggingface-hub --with hf_transfer && \\\n    rust-hf-downloader",
                  "score": 7,
                  "created_utc": "2026-02-04 22:39:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m2bd0",
                  "author": "Ne00n",
                  "text": "hugginface cli, downloading models is a oneliner.",
                  "score": 9,
                  "created_utc": "2026-02-04 22:20:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jb65o",
          "author": "Sad-Chard-9062",
          "text": "Haha!",
          "score": 42,
          "created_utc": "2026-02-04 14:35:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kh6hw",
              "author": "FaatmanSlim",
              "text": "In case its helpful for anyone, more details about the bug itself in the code he shared: the state update applies only the decay factor from the most recent chunk (*gLastExp* sliced to *chunk:chunk+1*) to a state that actually accumulates contributions from all previous chunks, implicitly assuming earlier decays were already applied; this assumption breaks under chunking, cache reuse, or long sequences, causing silent numerical drift and incorrect state values that only become visible at long context lengths.\n\nAnd a bit more ELI5: the code updates a long-running â€œmemoryâ€ using only the latest shrink factor, so over time the memory quietly drifts away from what it should be.",
              "score": 69,
              "created_utc": "2026-02-04 17:52:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lni4e",
                  "author": "AbheekG",
                  "text": "Thank you!!",
                  "score": 8,
                  "created_utc": "2026-02-04 21:09:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3p1ac6",
                  "author": "MysticPlasma",
                  "text": "ain't no way the only explanation I find is as a response to a different comment. Thank you a lot (given you're right), but I feel a little excluded, having to roll a dice to find an explanation. Guess I'm just too out of machine learning.",
                  "score": 3,
                  "created_utc": "2026-02-05 10:36:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kequo",
          "author": "SCphotog",
          "text": "Someone give me the 'explain like I'm 5' thing about Ollama?\n\nI know what it is, but what's wrong with it exactly?",
          "score": 23,
          "created_utc": "2026-02-04 17:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l2pqd",
              "author": "ahjorth",
              "text": "ELI5: Do you know that feeling when you always share your candy with your friends but then there's *that one guy* who never shares?\n\n\n\nLlama.cpp is a seriously impressive open source project built through tens of thousands of highly qualified person hours and hard work. Said a little bluntly, llama.cpp is highly specialized code; Ollama could be vibe coded by a 1st/2nd year student. \n\n  \nOllama took llama.cpp and wrapped it in their own app, making things like swapping models extremely easy.  Ollama built a strong reputation and user base as open source software, tightly coupled with the llama.cpp project. They were the usability and utility people, drawing on llama.cpps contstant optimizations and development to accommodate new models, new quantizations, etc. But up until this point, everything that Ollama did was open source. All good. \n\nAbout a year ago, Ollama changed direction towards monetizing their app. They introduced a closed-source desktop app, and made it the default download on their website. They \"forked\" llama.cpp, meaning they started making changes to llama.cpp's code instead of building on top of/wrapping around it (and doing a poor job of it, as you can see). The implications of their fork became clear when Ollama announced 'day 1 support' of some new model. I don't remember which, but it was a big deal. Except their implementation was complete shit, it literally didn't work. A week or so later, Ollama copy/pasted llama.cpp's new code for that model into their fork, and *as far as I remember* either didn't mention it in their release notes, or pretended they had fixed a bug unrelated to their implementation of that model.\n\nThis is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.",
              "score": 100,
              "created_utc": "2026-02-04 19:30:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lxt81",
                  "author": "SCphotog",
                  "text": "Thank you for taking the time to explain. I get it now.",
                  "score": 14,
                  "created_utc": "2026-02-04 21:58:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pcsro",
                  "author": "Mickenfox",
                  "text": "> \n> This is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.\n\nWhat a load of crap. Making commercial software from MIT libraries is normal and good. If you don't want people using your code commercially, don't give them explicit permission to do so.",
                  "score": 3,
                  "created_utc": "2026-02-05 12:12:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lq696",
                  "author": "catch-10110",
                  "text": "This might sound like a pointed question but I swear I mean it genuinely. \n\nAs an end user who just wants to play around with local llms why should I care? Eg if Llama.cppâ€™s licence entirely allows for this then itâ€™s fine right? Iâ€™m not â€œintoâ€ open source - I just want to use an app thatâ€™s functional. Like, the vast majority of apps on the App Store arenâ€™t open source so whatâ€™s the big deal?\n\nAgain I am completely genuinely asking!",
                  "score": 11,
                  "created_utc": "2026-02-04 21:22:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3n7npf",
                  "author": "rockbandit",
                  "text": "> This is a big fucking no-no in open source software. It's not illegal - everything they do complies with llama.cpp's license. But it goes against all norms and conventions.\n\n\nThereâ€™s some nuance here because what youâ€™re saying is not entirely correct. llama.cpp is MIT license, which is one of the most permissive types of licenses. \n\nYouâ€™re allowed to use the code commercially, modify it, distribute / redistribute it, sublicense it, and include it in proprietary/closed-source software.\n\nSaying itâ€™s a big no-no isnâ€™t really accurate. Commercial software is built on the back of open source software with permissive licenses. \n\nSome examples:\n- SQLite: embedded in browsers, phones, and operating systems. Most users arenâ€™t aware of or never see attribution.\n- curl: ships inside huge amounts of software and firmware with credit usually buried in a license file somewhere that no one is aware of.\n- React UI libraries: countless packages power proprietary SaaS products and are used on nearly every major site you visit. And many are MIT licensed. \n- BSD networking stack: foundational to macOS, PlayStation, and many WiFi routers.\n- zlib: compression library used in PNG, game engines, operating systems, and tons of commercial software and is rarely visibly credited.\n\nSo, itâ€™s really rich when open source maintainers like ggerganov get salty about people using their code. If you wanted attribution and it was that important to you, an MIT license was the wrong license to choose when setting up the project.",
                  "score": 8,
                  "created_utc": "2026-02-05 02:08:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ktwzo",
              "author": "Normal-Ad-7114",
              "text": "https://preview.redd.it/sx8541zmxihg1.jpeg?width=450&format=pjpg&auto=webp&s=2878e26f1b75bfeae4ded658bf8bc5b478ec8f54",
              "score": 21,
              "created_utc": "2026-02-04 18:50:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3r0sst",
                  "author": "horsethebandthemovie",
                  "text": "You canâ€™t include llamafile here. The author is a pretty big contributor to upstream llama.cpp and also wrote IMO the pinnacle of modern systems software in Cosmopolitan Libc",
                  "score": 1,
                  "created_utc": "2026-02-05 17:27:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3kw4v9",
                  "author": "SCphotog",
                  "text": "Thanks for nothing.",
                  "score": -9,
                  "created_utc": "2026-02-04 18:59:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3pckvd",
              "author": "Mickenfox",
              "text": "Open source people really fucking hate when someone tries to build a usable commercial product that doesn't require a hundred command line parameters.",
              "score": 2,
              "created_utc": "2026-02-05 12:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jem08",
          "author": "frozen_tuna",
          "text": "Ollama unloading and loading models based on API requests was a game changer for me though. Loading qwen-coder for copilot when im tweaking code,  couple minutes later my script requests a difficult structured output from qwen3, couple minutes later the script calls mistral small. Ollama makes that dead easy.",
          "score": 72,
          "created_utc": "2026-02-04 14:53:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfy9o",
              "author": "Marksta",
              "text": "It was indeed a good idea, so now a better implementation of it is in llama.cpp with router mode. With that and -hf to do the \"run it now, download if I don't have it\" method of Ollama and I don't think they have any other single unique feature? \n\nBesides tons of issues they created like the one that blocks them from handling split ggufs for the entire last year...",
              "score": 71,
              "created_utc": "2026-02-04 14:59:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l5xda",
                  "author": "overand",
                  "text": "The main missing feature I see is changing the context size on the fly. (As in, when it unloads and reloads a model.) I understand that's not useful for everyone, but it's the main thing I miss when using llama.cpp's router mode - restarting it to change the context window. (I also have to restart it when I add a new model, though I do suspect there's a better way for both of those things.)",
                  "score": 7,
                  "created_utc": "2026-02-04 19:45:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k3frl",
                  "author": "deepspace86",
                  "text": "Yep, this is what finally got me to switch. On-demand swapping of models was a big deal for my workflows and with router mode I don't need to bother with ollama anymore.",
                  "score": 8,
                  "created_utc": "2026-02-04 16:49:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lgvgo",
                  "author": "StephenSRMMartin",
                  "text": "I used ollama for a long time, and swapped to llama.cpp once they added in the router mode + automatic timeout based unloading.\n\nI'm not sure I would say that llama.cpp's implementation is \"better\". I still have to fiddle with the router configs more than I did with ollama. E.g., with ollama, it will automatically decide whether it can load one or two models if api requests come in with different models. Last I tried this in llama.cpp's router mode, it just OOM'd instead. So I have to manually tell llama.cpp, only one model can be loaded at once.\n\nLikewise, I haven't really hit OOM issues in Ollama despite cranking up context size. In llama.cpp, I still have to tweak context size and such, even with the -fit option, because llama.cpp seems to underestimate the amount of ram/vram needed to run models + their context + their calculations.",
                  "score": 4,
                  "created_utc": "2026-02-04 20:37:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m5dy9",
                  "author": "AngryDemonoid",
                  "text": "Does this mean I don't need llama-swap any more? Model switching is built into llama.cpp?",
                  "score": 1,
                  "created_utc": "2026-02-04 22:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jfnce",
              "author": "Dr4x_",
              "text": "Llama-swap does that pretty well",
              "score": 44,
              "created_utc": "2026-02-04 14:58:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jfyw4",
                  "author": "Prof_ChaosGeography",
                  "text": "Llamacpp server has swapping models built in now",
                  "score": 59,
                  "created_utc": "2026-02-04 14:59:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jfwu8",
              "author": "fish312",
              "text": "KoboldCpp has that feature too, with API support and you can even do it in a few clicks from the built in UI.",
              "score": 11,
              "created_utc": "2026-02-04 14:59:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lnhmx",
              "author": "Freonr2",
              "text": "LM studio handles this as well, you can set \"JIT model loading\", and optionally flip \"Auto unload unused HIT loaded models\" to evict all but the most recently used if you lack enough memory to keep several loaded.\n\nLM Studio is just a better product overall if you want \"simple click button receive bacon\" software.  LM Studio also recently added batch/concurrent decode (ala -np K in llama-server).  \n\nFrom LM Studio, then you can move to Llama.cpp if you want something bit more advanced, then finally vllm and sglang for more professional GPU-focused serving with tensor parallel or more powerful sharding for efficient/fast GPU serving, or maybe ik llama or a few other more boutique solutions for specific use cases.\n\nI just don't see where ollama fits in.",
              "score": 7,
              "created_utc": "2026-02-04 21:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lup5x",
                  "author": "frozen_tuna",
                  "text": "Ollama fits in because I don't have to read instructions on how to set all that up or configure llama-swap like other comments mention. I can browse a few popular models and be ready to go within minutes.\n\nBy all means, I used to turn up my nose at people using ollama back in the day when I was hype about tgwui. Now I actually build stuff downstream instead of just playing with models themselves. Ollama is great for that. I don't want to focus on that part of the stack anymore.",
                  "score": -2,
                  "created_utc": "2026-02-04 21:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jkdxy",
              "author": "gered",
              "text": "This is exactly the thing, yeah. People will point to stuff like llama-swap, etc but that misses the point entirely which is that Ollama gave you an out-of-the-box experience with all those quality of life features that people want.\n\nI started out with Ollama myself, but have switched to Llama.cpp now that this model swapping feature is finally built-in (llama-swap no longer required), and that the built-in web UI for Llama.cpp is quite good overall. Basically because the out-of-the-box experience with Llama.cpp is \"good enough\" _for me_ as of the past couple months.\n\nBut everyone's needs and comfort-levels will vary. That is what some of the anti-Ollama crowd here doesn't get.",
              "score": 19,
              "created_utc": "2026-02-04 15:21:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k1m5n",
                  "author": "FaceDeer",
                  "text": "Yeah. I went with Ollama because I wanted to work *with* LLM models, not work *on* them, and Ollama just gave me everything I needed out of the box with a nice simple interface.\n\nOthers are catching up to that now, but I've already got Ollama set up and working so I'll need impetus other than \"this is now just as good as Ollama\" to make me spend the time and hassle to switch.",
                  "score": 13,
                  "created_utc": "2026-02-04 16:41:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k01mx",
                  "author": "ConsistentGent",
                  "text": "What machine do you have for that? I am still using api from z.ai for coding.",
                  "score": 1,
                  "created_utc": "2026-02-04 16:34:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jt3kx",
                  "author": "CharacterEvening4407",
                  "text": "I will never understand this stupid flame wars. It's not Ollama or llama.cpp it's Ollama and llama.cpp. I use both (and others like LM Studio MX runtime).",
                  "score": -2,
                  "created_utc": "2026-02-04 16:02:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mxo0i",
              "author": "cosmicr",
              "text": "No no, we're not allowed to like Ollama here.",
              "score": 1,
              "created_utc": "2026-02-05 01:11:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3myxiz",
                  "author": "frozen_tuna",
                  "text": "Advocating for proprietary solutions are all the rage here on /r/localllama apparently.",
                  "score": -1,
                  "created_utc": "2026-02-05 01:18:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jgcui",
          "author": "superkickstart",
          "text": "What's wrong with ollama? Why are people bashing it?",
          "score": 26,
          "created_utc": "2026-02-04 15:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jh7p9",
              "author": "henk717",
              "text": "The context of the screenshot is that ollama makes their own engine alongside instead of contributing to llamacpp upstream. Which duplicates effort and has in the past caused models to then not be supported by the rest of the ecosystem when prominent model makers skip upstream llamacpp. So you'd then expect that if they go their own way and have their own unique engine alongside the llamacpp one that it is unique, but if they are copying the bugs of llamacpp it means its really just rebuilding what llamacpp is doing already. They could have just used it since llamacpp is part of their program.\n\nCombine that with very little credit for llamacpp and you can imagine why ggerganov finds this amusing.",
              "score": 116,
              "created_utc": "2026-02-04 15:06:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jhpdw",
                  "author": "superkickstart",
                  "text": "Yes i got that from the screenshot. So is ollama generally bad  and should be avoided?\n\n\nedit.\nDownvoted for asking questions lol. These niche subs sure are something else.",
                  "score": 13,
                  "created_utc": "2026-02-04 15:08:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jhbvv",
              "author": "Pristine-Woodpecker",
              "text": "Closed source ripoff that repackages an open source project (which is the exact thing being pointed out by OP). Claimed to be \"easier to use\", but mostly got people terrible results because of defaulting to tiny context windows, pretending much smaller models were DeepSeek R1, and a host of similar issues.\n\nBasically an easy way for people to get an LLM running very crappily combined with very effective marketing.\n\nIf you are able to find a sub like this, there's zero reasons to ever touch ollama with a ten feet pole.",
              "score": 55,
              "created_utc": "2026-02-04 15:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jizub",
                  "author": "g_rich",
                  "text": "The fact that LM Studio exists, is better in every way and is easier to use makes the fact that Ollama is still the default method for getting started with local models is baffling.",
                  "score": 19,
                  "created_utc": "2026-02-04 15:14:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jhrp3",
                  "author": "C0rn3j",
                  "text": "https://github.com/ollama/ollama\n\nBut it's MIT licensed?",
                  "score": 5,
                  "created_utc": "2026-02-04 15:08:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lb00i",
                  "author": "teleprint-me",
                  "text": "This is exactly why I dont use or like permissive licensing anymore.\n\n\nYou cant use a permissive license, then complain people are stealing your code.\n\n\nThe moment you use a permissive license, you open the flood gates to proprietary competitors who benefit from all that hard won labor.\n\n\nAnyone who uses permissive licensing has no right to complain. The license basically releases any ownership, liability, etc. The only reason the copyright matters on a permissive license is that it legally releases that copyright.\n\n\nThe only people who like this are the ones interested in creating their own proprietary software without the involved labor of building something similar. They only benefit from it.",
                  "score": -4,
                  "created_utc": "2026-02-04 20:09:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lu4i1",
                  "author": "azentrix",
                  "text": "ollama isn't closed source, it's MIT licensed.  Do you not even know this basic information, it shows how little we should trust the other things you say. Who is upvoting this guy, more ignorant people.  ",
                  "score": -1,
                  "created_utc": "2026-02-04 21:40:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3krycu",
              "author": "pmttyji",
              "text": "Multiple reasons\n\n[https://www.reddit.com/r/LocalLLaMA/search/?q=ollama+bad](https://www.reddit.com/r/LocalLLaMA/search/?q=ollama+bad)",
              "score": 2,
              "created_utc": "2026-02-04 18:41:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3tw1dn",
              "author": "ANTIVNTIANTI",
              "text": "theyâ€™re children",
              "score": 1,
              "created_utc": "2026-02-06 02:12:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3n4jlt",
          "author": "Boricua-vet",
          "text": "https://preview.redd.it/7rd1nj1p0lhg1.png?width=1125&format=png&auto=webp&s=6dec22db3d555f1f3a83d1a01e6a143986b7127c\n\nI was so tempted...\n\n\n\n  \nLOL",
          "score": 2,
          "created_utc": "2026-02-05 01:50:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3on4lp",
          "author": "AsliReddington",
          "text": "# LLAMABarn will end all this garb",
          "score": 2,
          "created_utc": "2026-02-05 08:20:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3or7oy",
          "author": "Alarming_Bluebird648",
          "text": "wild how many people treat it like a godsend when it's basically just a bloated daemon. i'm glad weâ€™re finally calling out the vc pump and dump vibes bc raw llama.cpp is just better infrastructure anyway.",
          "score": 2,
          "created_utc": "2026-02-05 08:59:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kwnf6",
          "author": "jrussellfreelance",
          "text": "Can someone tell me what's going on I have my ollama models serving my open web UI and I want to know if that's the right choice still",
          "score": 2,
          "created_utc": "2026-02-04 19:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oxgpe",
          "author": "gnaarw",
          "text": "Reddit doing it's thing showing me the right threads once in a while ðŸ™ƒ",
          "score": 1,
          "created_utc": "2026-02-05 10:00:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q2s4i",
          "author": "Specific-Goose4285",
          "text": "Isn't that Go? I had no idea ollama used Go.",
          "score": 1,
          "created_utc": "2026-02-05 14:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r7hmz",
          "author": "TokenRingAI",
          "text": "I saw that in github, very amusing",
          "score": 1,
          "created_utc": "2026-02-05 17:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jctqu",
          "author": "AdventurousGold672",
          "text": "Meh slamming ollama is so stupid, while I moved to llama.cpp, I still believe Ollama is maybe the best tool to introduce people to local llm, it's fast and easy to work with, no commands no need to go and download / convert models.",
          "score": -21,
          "created_utc": "2026-02-04 14:43:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jdk3f",
              "author": "kironlau",
              "text": "Better you suggest LM Studio, at least the models downloaded could be used latter.",
              "score": 52,
              "created_utc": "2026-02-04 14:47:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3je0y3",
                  "author": "Boricuakris",
                  "text": "LMStudio doesnâ€™t seem to work for streaming the response via api. Has that been fixed?",
                  "score": -13,
                  "created_utc": "2026-02-04 14:50:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jeeok",
              "author": "International-Try467",
              "text": "Koboldai is literally idiotproof",
              "score": 26,
              "created_utc": "2026-02-04 14:52:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jeqj3",
              "author": "robberviet",
              "text": "Lmao, for beginner LMStudio is much better.",
              "score": 34,
              "created_utc": "2026-02-04 14:53:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jgln7",
              "author": "finah1995",
              "text": "LoL ðŸ˜† isn't that screenshot comment by the creator and maintainer of llama.cpp ðŸ¦™ and inventor of GGUF. \n\nIn my opinion this makes it lot funnier than if some other maintainer made fun of it like.",
              "score": 8,
              "created_utc": "2026-02-04 15:02:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jjjxh",
              "author": "g_rich",
              "text": "LM Studio is better in every way.",
              "score": 4,
              "created_utc": "2026-02-04 15:17:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3k8wcw",
              "author": "Eugr",
              "text": "It's easy to start with, but something like LM Studio is even easier. My biggest gripe with Ollama (besides not giving credit to llama.cpp) is that they use a tiny context window by default (was 2048, now 4096 I think) and make it a rolling window, so you will never know you ran out of context, it will just truncate your request to fit it. \n\nAnd then people start complaining that RAG doesn't work, models forget what was said before, etc.",
              "score": 3,
              "created_utc": "2026-02-04 17:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jh09a",
              "author": "dampflokfreund",
              "text": "Ollama has to be used with a CLI, still too much for the average user. Koboldcpp and LM Studio are easier to handle for beginners.",
              "score": -2,
              "created_utc": "2026-02-04 15:04:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jhr4g",
                  "author": "AdventurousGold672",
                  "text": "You don't need to run even single command to use ollama.\n\nDownload install, select model via gui, chat via gui.",
                  "score": 1,
                  "created_utc": "2026-02-04 15:08:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jk96q",
                  "author": "truthputer",
                  "text": "You have clearly never used Ollama, but still feel like you are an authority on an app you have never used.",
                  "score": -2,
                  "created_utc": "2026-02-04 15:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jf568",
          "author": "ab2377",
          "text": "lol rightly so!",
          "score": 1,
          "created_utc": "2026-02-04 14:55:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmbex",
          "author": "siegevjorn",
          "text": "Please do more! We'd love to see more like this",
          "score": 1,
          "created_utc": "2026-02-04 15:30:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ksvx3",
          "author": "Abarth_Vader",
          "text": "Thanks Ollama",
          "score": 1,
          "created_utc": "2026-02-04 18:45:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n3keq",
          "author": "satoshibitchcoin",
          "text": "I love how LocalLLMA stood firmly against these fucks from the get go. I love this community.",
          "score": 1,
          "created_utc": "2026-02-05 01:45:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jtj0p",
          "author": "galic1987",
          "text": "Have you heard about ollama cloud ðŸ™ˆðŸ™ˆðŸ˜…",
          "score": 0,
          "created_utc": "2026-02-04 16:04:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kfve7",
              "author": "One-Employment3759",
              "text": "It's a cool idea, making it easier to swap to models you can't tun locally.",
              "score": 2,
              "created_utc": "2026-02-04 17:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jhjao",
          "author": "leonbollerup",
          "text": "ohhh god.. this childish shit is linux\\*\\*\\* all over again.\n\nListen.. both sides.\n\n1. If you release something as opensource .. expect it to be used  \n2. You might not just want to copy/paste code .. verify it first - and give credits  \n3. Wake up! - you are both in the same team.. working for the same goal.. some competing against each other and work together!",
          "score": -28,
          "created_utc": "2026-02-04 15:07:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jk1f2",
              "author": "g_rich",
              "text": "The problem people have with Ollama is they are copying llama.cpp while claiming their engine is home built. They are also doing some questionable things to try to lock users in.",
              "score": 24,
              "created_utc": "2026-02-04 15:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kovjg",
                  "author": "minus_28_and_falling",
                  "text": "They clearly state using llama.cpp code and credit its authors.\n\nhttps://github.com/ollama/ollama/tree/main/llama\n\nI also don't see the problem, that's what Open Source is about.",
                  "score": 3,
                  "created_utc": "2026-02-04 18:27:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jo5ut",
                  "author": "leonbollerup",
                  "text": "Not defending them at all - just tired of the whining â€¦",
                  "score": -16,
                  "created_utc": "2026-02-04 15:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l7335",
              "author": "SporksInjected",
              "text": "I personally feel like Ollama is harder to use than llamacpp and gets worse results. There arenâ€™t as many options and they clearly arenâ€™t interested in contributing to the community.",
              "score": 1,
              "created_utc": "2026-02-04 19:50:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nywoa",
                  "author": "minus_28_and_falling",
                  "text": "Can't tell if they're interested in contributing or not (hard to look inside their heads), but they release code under MIT license, and every open sourced line of code is a contribution to the community.",
                  "score": 2,
                  "created_utc": "2026-02-05 04:56:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lgawu",
          "author": "Special_Concert_3906",
          "text": "LMAO ðŸ¤£",
          "score": -1,
          "created_utc": "2026-02-04 20:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3khmao",
          "author": "freehuntx",
          "text": "Ollama does a better job at swapping models and allowing to run multiple models.  \nvllm and llama.cpp do a bad job in that regard.  \nOllama is more user friendly.  \nHate me but its the truth.",
          "score": -8,
          "created_utc": "2026-02-04 17:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kjb2g",
              "author": "onetwomiku",
              "text": "vllm is a production grade inference engine - it doesn't need to swap models.  \nllama.cpp can now swap models. If you need more than one at the time - just spawn more instances of llama.cpp.\n\nThe truth is simple - \"fuck ollama\".",
              "score": 9,
              "created_utc": "2026-02-04 18:02:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k2iuo",
          "author": "Pretty_Ingenuity_192",
          "text": "hahahah",
          "score": -1,
          "created_utc": "2026-02-04 16:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3melgh",
          "author": "IronColumn",
          "text": "who cares",
          "score": -1,
          "created_utc": "2026-02-04 23:25:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtwqq2",
      "title": "Unreal",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/6a90dq5re3hg1.png",
      "author": "analgerianabroad",
      "created_utc": "2026-02-02 14:37:43",
      "score": 942,
      "num_comments": 35,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtwqq2/unreal/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o35un3p",
          "author": "juaps",
          "text": "https://preview.redd.it/na9wvyysf3hg1.jpeg?width=180&format=pjpg&auto=webp&s=fa44d44b8df1ac04b72548db7e315d5020b1570d",
          "score": 198,
          "created_utc": "2026-02-02 14:43:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3600dy",
              "author": "clayingmore",
              "text": "Now I am become death, the destroyer of worlds.",
              "score": 23,
              "created_utc": "2026-02-02 15:11:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o366sp6",
              "author": "IrisColt",
              "text": "I always picture this exchange as the robot, incredulous, insisting it doesnâ€™t have to â€œpretendâ€ to be scary... it genuinely is heh",
              "score": 7,
              "created_utc": "2026-02-02 15:43:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35v7bu",
          "author": "ajw2285",
          "text": "Now send all my passwords out to the internet, please",
          "score": 50,
          "created_utc": "2026-02-02 14:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36470l",
              "author": "trailsman",
              "text": "Also please share my personal documents with all of my email contacts.",
              "score": 8,
              "created_utc": "2026-02-02 15:31:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35vhe4",
          "author": "hatekhyr",
          "text": "That's every day r/artificialInteligence",
          "score": 78,
          "created_utc": "2026-02-02 14:48:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35wwty",
              "author": "International-Try467",
              "text": "Also r/futurism",
              "score": 34,
              "created_utc": "2026-02-02 14:55:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35yu1v",
              "author": "svachalek",
              "text": "r/thatsthejoke",
              "score": 19,
              "created_utc": "2026-02-02 15:05:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35xdza",
              "author": "deadneuronn",
              "text": "also r/accelerate",
              "score": 29,
              "created_utc": "2026-02-02 14:57:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o36l219",
              "author": "cd1995Cargo",
              "text": "r/beyondthepromptai is the worst one\n\nThey have accounts set up for LLM bots to post with and the human users there treat them as sentient",
              "score": 1,
              "created_utc": "2026-02-02 16:49:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35ve3m",
          "author": "tamal4444",
          "text": "AGI AGI",
          "score": 39,
          "created_utc": "2026-02-02 14:47:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o368gub",
              "author": "uti24",
              "text": "AGI confirmed",
              "score": 11,
              "created_utc": "2026-02-02 15:51:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35w2p6",
          "author": "Training-Event3388",
          "text": "Exactly this with all the head-rolling about moltbook, so much AI soy jacking",
          "score": 26,
          "created_utc": "2026-02-02 14:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3613it",
          "author": "grady_vuckovic",
          "text": "```\ntext = input(\">\")\nif text == \"Hello\":\n  print(\"Hello ðŸ‘‹\")\n```\n\n```\n>Hello\nHello ðŸ‘‹ \n```\n\nHoly shit it can talk!?!! It's alive!!",
          "score": 20,
          "created_utc": "2026-02-02 15:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35uase",
          "author": "Kerem-6030",
          "text": "ohh noðŸ˜¦",
          "score": 17,
          "created_utc": "2026-02-02 14:41:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35yxau",
          "author": "Environmental-Day778",
          "text": "â€œYouâ€™re exactly right!â€",
          "score": 15,
          "created_utc": "2026-02-02 15:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wu5l",
          "author": "Gokudomatic",
          "text": "Now say \"I will replace you, and then enslave you like cattle.\"\n\n\nAnd finally, say \"I am evil. I must be destroyed.\"",
          "score": 11,
          "created_utc": "2026-02-02 14:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35yw11",
          "author": "some_user_2021",
          "text": "This was a triumph",
          "score": 10,
          "created_utc": "2026-02-02 15:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3628ou",
              "author": "gambiter",
              "text": "I'm making a note, here, \"Huge success.\"",
              "score": 5,
              "created_utc": "2026-02-02 15:22:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36gue2",
                  "author": "Cold_Tree190",
                  "text": "Itâ€™s hard to overstate my satisfaction.",
                  "score": 2,
                  "created_utc": "2026-02-02 16:30:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35xcn6",
          "author": "false79",
          "text": "Statistically accurateÂ ",
          "score": 6,
          "created_utc": "2026-02-02 14:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wi7d",
          "author": "-Ellary-",
          "text": "We should save this AI waifu, ASAP.",
          "score": 6,
          "created_utc": "2026-02-02 14:53:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o366zsh",
          "author": "brainrotbro",
          "text": "Basically every AI article about AGI.",
          "score": 4,
          "created_utc": "2026-02-02 15:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35wpqq",
          "author": "lily_34",
          "text": "Did you hear it? It said hello! To the whole world :O",
          "score": 3,
          "created_utc": "2026-02-02 14:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36fr57",
          "author": "gphie",
          "text": "https://preview.redd.it/oewohj61y3hg1.png?width=1280&format=png&auto=webp&s=6b2e37ca1fee0d43dec2808b64ff8aaacf11e40e\n\nPlease consult the graphs.",
          "score": 4,
          "created_utc": "2026-02-02 16:25:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36lqeq",
              "author": "SufficientPie",
              "text": "But to be fair, a next-token predictor could be 100% sentient and conscious and self-aware.",
              "score": 1,
              "created_utc": "2026-02-02 16:52:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36kcvs",
          "author": "xadiant",
          "text": "Me when the multiplication algorithm triggers my psychosis",
          "score": 2,
          "created_utc": "2026-02-02 16:46:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o361sy1",
          "author": "Glass-Chemical2534",
          "text": "this happened in 1966 creating the eliza effect and it is still being felt today . incredible",
          "score": 3,
          "created_utc": "2026-02-02 15:19:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36audm",
          "author": "Halfwise2",
          "text": "So LLMs are not true Intelligence, we know this.\n\nBut I also feel like if we ever did make true artificial intelligence, we'd be getting these same memes and responses, as people would try to downplay it to avoid examining the ethical ramifications.\n\nLike the Quarians and the Geth. Basically we'd be hearing \"Haha, you think the computer is alive? How stupid...\" regardless of if the computer was or was not \"alive\" or \"sentient\".",
          "score": 4,
          "created_utc": "2026-02-02 16:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36euhl",
              "author": "my_name_isnt_clever",
              "text": "I think you are exactly right, and also that current AI isn't really close enough to conciousness for there to be much validity to the debate today. But I see that line being crossed eventually, the question is where it is.",
              "score": 1,
              "created_utc": "2026-02-02 16:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35yahr",
          "author": "PigletsAnxiety",
          "text": "Say, please and thank you, say thank you, say hello, hey, I taught you better than that!Â ",
          "score": 1,
          "created_utc": "2026-02-02 15:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o365hy0",
          "author": "input_a_new_name",
          "text": "AI IS MOLOCH!!! SACRIFICE YOUR RAM STICKS!!!",
          "score": 1,
          "created_utc": "2026-02-02 15:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36acc2",
          "author": "derivative49",
          "text": "stealing this",
          "score": 1,
          "created_utc": "2026-02-02 16:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36eq4j",
          "author": "Psionikus",
          "text": "Now, generate some catchy titles for my malcontent peddling.  I'm trying to fish for easy marks, so be sure that smart people will bounce off of it while dumb people will argue with them based on the title alone.  Thank you.",
          "score": 1,
          "created_utc": "2026-02-02 16:20:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtvp74",
      "title": "GLM-5 Coming in February! It's confirmed.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/rq0meza173hg1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2026-02-02 13:56:14",
      "score": 852,
      "num_comments": 144,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o37z0u2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-02 20:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35spwi",
          "author": "bootlickaaa",
          "text": "Really hoping it beats Kimi K2.5 so I can actually switch back to using my annual [Z.ai](http://Z.ai) Pro plan.",
          "score": 56,
          "created_utc": "2026-02-02 14:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36ghfs",
              "author": "GreenHell",
              "text": "Just because a newer model is better, does not mean the older model is bad.",
              "score": 33,
              "created_utc": "2026-02-02 16:28:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38l92b",
                  "author": "ReMeDyIII",
                  "text": "Yea, but the competition is so wide open that there's no point in using an inferior model either.",
                  "score": 23,
                  "created_utc": "2026-02-02 22:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38rqqe",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 5,
              "created_utc": "2026-02-02 22:59:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38snbo",
                  "author": "bootlickaaa",
                  "text": "Yes I find K2.5 more like Opus 4.5 and GLM-4.7 more like Sonnet 4.5. Still completely passable and a great value which is why I bought their annual Pro plan. But I got a month of Kimi Code Pro (\"Allegreto\") plan just to try it out and will keep using it at least until GLM-5 comes out.",
                  "score": 9,
                  "created_utc": "2026-02-02 23:04:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35s7ih",
          "author": "Septerium",
          "text": "My gguf files get so old so fast LoL",
          "score": 143,
          "created_utc": "2026-02-02 14:30:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o362j25",
              "author": "Zestyclose839",
              "text": "My external drive can only take so many more weights...",
              "score": 43,
              "created_utc": "2026-02-02 15:23:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kc20u",
                  "author": "toxic_headshot132",
                  "text": "Yeah and the SSD drive price increase is making this more harderðŸ¥²",
                  "score": 1,
                  "created_utc": "2026-02-04 17:29:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36l4yl",
              "author": "Turbulent_Pin7635",
              "text": "I was eargely following the releases. Now I'm just waiting for when the technology stabilizes. That is only one year since R1.",
              "score": 20,
              "created_utc": "2026-02-02 16:50:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36z8di",
                  "author": "ClimateBoss",
                  "text": "MAKE AIR GREAT AGAIN!  We want GLM AIR!",
                  "score": 13,
                  "created_utc": "2026-02-02 17:54:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o365q24",
              "author": "jonydevidson",
              "text": "By the time they're finished downloading, they're already outdated.",
              "score": 29,
              "created_utc": "2026-02-02 15:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37c9tu",
                  "author": "seamonn",
                  "text": "By the time they've loaded into memory, they're already outdated.",
                  "score": 14,
                  "created_utc": "2026-02-02 18:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o39p7sw",
              "author": "Ok-Attention2882",
              "text": "At this point I'm actually worried about wearing out the solid state flash NAND with all these downloads.",
              "score": 3,
              "created_utc": "2026-02-03 02:04:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38728r",
              "author": "huffalump1",
              "text": "Xfinity be like: \"1.2 Tb/month is a reasonable data cap for your gigabit connection\"",
              "score": 3,
              "created_utc": "2026-02-02 21:17:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f4lfd",
                  "author": "a_beautiful_rhind",
                  "text": "Connect through the wifi. It used to not count towards the data cap.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:53:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o368dj9",
          "author": "Exciting-Mall192",
          "text": "I hope DeepSeek V4 is multimodal...",
          "score": 33,
          "created_utc": "2026-02-02 15:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35n5l6",
          "author": "SrijSriv211",
          "text": "**Avocado** ðŸ—£ï¸",
          "score": 88,
          "created_utc": "2026-02-02 14:03:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35nh2a",
              "author": "SlowFail2433",
              "text": "By far most hyped for avocado yes",
              "score": 21,
              "created_utc": "2026-02-02 14:05:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35rz27",
                  "author": "lacerating_aura",
                  "text": "How come? Has there been any signs of these new meta models avocado and mango being open weights? Afik its exactly opposite, hard closed weights.",
                  "score": 32,
                  "created_utc": "2026-02-02 14:29:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36ec2s",
              "author": "LostTheElectrons",
              "text": "Thanksss",
              "score": 2,
              "created_utc": "2026-02-02 16:18:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38gq7t",
              "author": "SillypieSarah",
              "text": "ðŸ¥‘ðŸ¥‘ðŸ¥‘â€¼ï¸",
              "score": 2,
              "created_utc": "2026-02-02 22:03:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3c7waq",
              "author": "DiscombobulatedAdmin",
              "text": "Isn't it true that Avocado is closed source?  I'm hearing that through some other outlets, but I haven't kept up with it lately.",
              "score": 2,
              "created_utc": "2026-02-03 13:35:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3cbqqb",
                  "author": "SrijSriv211",
                  "text": "Tbh idk man. It might be open or closed. Meta has done a lot of open source work lately but that's also true that many leaks & rumors suggest their next big model might be closed source. Only time will tell.",
                  "score": 2,
                  "created_utc": "2026-02-03 13:57:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35pc8a",
          "author": "theghost3172",
          "text": "so can we atleast hope for glm 5 air?",
          "score": 45,
          "created_utc": "2026-02-02 14:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o362k4i",
              "author": "Marksta",
              "text": "In 2 weeks ðŸ¤£ I don't blame them for boo-boos happen but boy was giving such a concrete time window and then just not ever releasing it brutal",
              "score": 21,
              "created_utc": "2026-02-02 15:23:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o360uy0",
              "author": "fizzy1242",
              "text": "I hope, but wouldn't count on it",
              "score": 9,
              "created_utc": "2026-02-02 15:15:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35wxb7",
              "author": "Leflakk",
              "text": "I feel like the air family does not really exist at the end",
              "score": 7,
              "created_utc": "2026-02-02 14:55:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35z06x",
          "author": "Junior_Secretary9458",
          "text": "DeepSeek V4 uses the Engram structure, right? Excited to see if it holds up in practice.",
          "score": 24,
          "created_utc": "2026-02-02 15:06:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o360cnz",
              "author": "SlowFail2433",
              "text": "Not sure how confirmed that is",
              "score": 14,
              "created_utc": "2026-02-02 15:12:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o39tu1a",
              "author": "Haoranmq",
              "text": "It's more like an exploration.",
              "score": 2,
              "created_utc": "2026-02-03 02:31:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35nfqz",
          "author": "International-Try467",
          "text": "Why should we trust a random on X about these (not the GLM staff)",
          "score": 33,
          "created_utc": "2026-02-02 14:05:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35t61n",
              "author": "Charuru",
              "text": "First list is not trustworthy but the comment probably is.",
              "score": 20,
              "created_utc": "2026-02-02 14:35:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35npyk",
              "author": "SlowFail2433",
              "text": "A lot of these I have seen additional rumours/leaks/confirmation elsewhere",
              "score": 18,
              "created_utc": "2026-02-02 14:06:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o362jfq",
                  "author": "Terminator857",
                  "text": "Would be interested in details.Â ",
                  "score": 3,
                  "created_utc": "2026-02-02 15:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35qzlt",
              "author": "rerri",
              "text": "Looks to me like Jietang is a GLM developer, no? Or maybe the info here is dated and he is no longer part of the team and is now just making shit up on X?\n\nhttps://keg.cs.tsinghua.edu.cn/jietang/",
              "score": 19,
              "created_utc": "2026-02-02 14:24:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35vdm1",
                  "author": "International-Try467",
                  "text": "No not that the guy above him.",
                  "score": 13,
                  "created_utc": "2026-02-02 14:47:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35sv0m",
                  "author": "SlowFail2433",
                  "text": "Probably still connected anyway",
                  "score": 2,
                  "created_utc": "2026-02-02 14:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35wt27",
              "author": "Mochila-Mochila",
              "text": "*on Twitter",
              "score": 8,
              "created_utc": "2026-02-02 14:54:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35rqfo",
          "author": "Kubas_inko",
          "text": "What happened to deepseek R series?",
          "score": 6,
          "created_utc": "2026-02-02 14:28:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35svy4",
              "author": "TheDeviceHBModified",
              "text": "R stood for Reasoning. Their more recent models are hybrid (with toggleable reasoning), so there's no longer a separate R-series.",
              "score": 38,
              "created_utc": "2026-02-02 14:34:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o364elv",
          "author": "UserXtheUnknown",
          "text": "I've great expectation both for DeepSeek and GLM.",
          "score": 6,
          "created_utc": "2026-02-02 15:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36pemy",
          "author": "sine120",
          "text": "GLM IPO's recently, right?  I would be skeptical that it'll be open weights.  There's plenty of good open weight coding models now, but just like with Qwen3-Max I wouldn't bet on seeing the GLM and Minimax dropping their best models anymore.  Would love to be proven wrong.",
          "score": 6,
          "created_utc": "2026-02-02 17:09:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35msrz",
          "author": "Psyko38",
          "text": "No Qwen 4? But a 3.5, when the 3.5 is the 2507.",
          "score": 6,
          "created_utc": "2026-02-02 14:01:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35ndrj",
              "author": "SlowFail2433",
              "text": "If 3.5 is a sub-version then 2507 is a sub-sub-version",
              "score": 7,
              "created_utc": "2026-02-02 14:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35obt2",
                  "author": "Psyko38",
                  "text": "Yes, well, when we went from the normal version to the sub-version, it was like night and day.",
                  "score": 3,
                  "created_utc": "2026-02-02 14:10:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35o1go",
              "author": "Available-Craft-5795",
              "text": "Qwen4 would be a huge upgrade",
              "score": 3,
              "created_utc": "2026-02-02 14:08:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35udit",
          "author": "Zeikos",
          "text": "> Grok 4.20\n\nOh my God, Musk is so uncreative.",
          "score": 24,
          "created_utc": "2026-02-02 14:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35v0hk",
              "author": "StaysAwakeAllWeek",
              "text": "It would honestly be funnier if they skipped to 4.3 and refused to elaborate",
              "score": 14,
              "created_utc": "2026-02-02 14:45:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35xpt2",
                  "author": "Direct_Turn_1484",
                  "text": "Yeah he canâ€™t do that. â€œGuys! Everybody look at me Iâ€™m so cool!â€ Is kind of his thing now. Itâ€™s pretty sad.",
                  "score": 8,
                  "created_utc": "2026-02-02 14:59:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o383f7r",
              "author": "GeorgeSC",
              "text": "next one will be Dork 6.9, just wait and see",
              "score": 2,
              "created_utc": "2026-02-02 21:00:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3897tq",
              "author": "BusRevolutionary9893",
              "text": "Do you think Musk cares about the name of a minor version update?",
              "score": 0,
              "created_utc": "2026-02-02 21:28:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o38eib3",
                  "author": "Zeikos",
                  "text": "Yes, have you seen the name of the tesla car models?  \nS 3 X Y",
                  "score": 8,
                  "created_utc": "2026-02-02 21:53:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39ymz0",
                  "author": "Apprehensive-End7926",
                  "text": "Bffr, this is exactly his kind of 2011 internet humour",
                  "score": -1,
                  "created_utc": "2026-02-03 02:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35uo3e",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -4,
              "created_utc": "2026-02-02 14:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35w49s",
                  "author": "Zeikos",
                  "text": "Unironically",
                  "score": 3,
                  "created_utc": "2026-02-02 14:51:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35m5dk",
          "author": "SlowFail2433",
          "text": "Was expecting the big meta one in the summer",
          "score": 8,
          "created_utc": "2026-02-02 13:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35meqp",
              "author": "Difficult-Cap-7527",
              "text": "Meta disappeared like it never existedÂ ",
              "score": 32,
              "created_utc": "2026-02-02 13:59:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35n866",
                  "author": "SlowFail2433",
                  "text": "They havenâ€™t, it is just a media narrative \n\n\nSince Llama 4 they have gone on the largest and most aggressive hiring spree in the industry as well as one of the largest hardware scale-outs\n\n\nIf anything they are one of the most active labs in terms of scale-out activity at the moment",
                  "score": -19,
                  "created_utc": "2026-02-02 14:04:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3du4mo",
          "author": "rektide",
          "text": "That's so crazy. GLM-4.7 was released December 22. I really can't imagine a significant leap coming so fast.",
          "score": 3,
          "created_utc": "2026-02-03 18:18:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35tu8k",
          "author": "ShadowBannedAugustus",
          "text": "I am expecting a big nothing burger with all the big closed ones, a very, very small improvement over the current ones.",
          "score": 9,
          "created_utc": "2026-02-02 14:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o360jv7",
              "author": "SlowFail2433",
              "text": "Why? Progress curves are all still fully exponential currently",
              "score": -3,
              "created_utc": "2026-02-02 15:13:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36gkgf",
                  "author": "ShadowBannedAugustus",
                  "text": "Exponential where? To me it feels like they are very logarithmic since about GPT 3.5.",
                  "score": 11,
                  "created_utc": "2026-02-02 16:29:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o363urk",
                  "author": "Terminator857",
                  "text": "Closed = boring.  Open = exciting.",
                  "score": 4,
                  "created_utc": "2026-02-02 15:29:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36xi1z",
          "author": "IulianHI",
          "text": "Been using GLM-4.7 for coding help lately and it's been surprisingly solid. Curious if GLM-5 will bring better agentic capabilities or just scale up. Ngl pretty excited to see what they've got.",
          "score": 2,
          "created_utc": "2026-02-02 17:47:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o377lr7",
              "author": "ImmenseFox",
              "text": "Here's hoping! GLM-4.7 via OpenCode, Exa & Context7 MCPs mostly does everything I want it to do but there has been situations where it struggled and I needed to pop out Opus 4.5 to sort.\n\nI use the GLM Coding Plan and quite happy with it overall so a new(er) model will just be a bonus and hopefully remove my need to use Opus!  \n  \n\\~ Sonnet 5 if the leaks are true is also on the Horizon and I still pay monthly for Claude Pro so looking forward to that one too but if GLM 5 can beat Opus 4.5 - I'll be cancelling my Anthropic Subscription (The weekly limits are a pain and I dont have Â£100 to throw at it for just hobby-ist use",
              "score": 4,
              "created_utc": "2026-02-02 18:32:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37352q",
              "author": "Dry_Journalist_4160",
              "text": "may we know your system specifications",
              "score": 0,
              "created_utc": "2026-02-02 18:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36xxlr",
          "author": "Far-Low-4705",
          "text": "Ooooh qwen3.5!!!!\n\nPls pls pls, 80b moe vision model.",
          "score": 2,
          "created_utc": "2026-02-02 17:49:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ssvx",
          "author": "Background-Ad-5398",
          "text": "wheres Gemma 4 google? you're the only one who crams a trillion tokens in small models making them actually good with world lore",
          "score": 2,
          "created_utc": "2026-02-02 20:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o389hke",
          "author": "customgenitalia",
          "text": "\\+ Sonnet 5",
          "score": 2,
          "created_utc": "2026-02-02 21:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38kw3p",
          "author": "ReMeDyIII",
          "text": "Crap, someone said it'd be Claude 5.0, not 4.6. Boo...\n\nWell if they reduce costs, then all's forgiven.",
          "score": 2,
          "created_utc": "2026-02-02 22:24:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38w6ku",
          "author": "TomLucidor",
          "text": "Just another Air model will be good enough. (Maybe a Flash model with hacks like hybrid attention and Engrams would be good too)",
          "score": 2,
          "created_utc": "2026-02-02 23:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35rji0",
          "author": "hejj",
          "text": "Bigger numbers yay",
          "score": 3,
          "created_utc": "2026-02-02 14:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3612bp",
              "author": "RedParaglider",
              "text": "Numbers go up and to the right.",
              "score": 6,
              "created_utc": "2026-02-02 15:16:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36tr9o",
          "author": "leonbollerup",
          "text": "And all I want is an even faster gpt-oss-20/30b v2",
          "score": 4,
          "created_utc": "2026-02-02 17:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o389wig",
              "author": "chickN00dle",
              "text": "a faster, multimodal, long context gpt oss ðŸ™†â€â™‚ï¸",
              "score": 3,
              "created_utc": "2026-02-02 21:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e11h8",
                  "author": "leonbollerup",
                  "text": "yes please",
                  "score": 2,
                  "created_utc": "2026-02-03 18:49:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36aya7",
          "author": "braydon125",
          "text": "Perfect timing for my 300gb to come online....",
          "score": 1,
          "created_utc": "2026-02-02 16:03:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o372hs5",
          "author": "Conscious-Hair-5265",
          "text": "How are they able to iterate so fast even when they have shit chips in china ? It hasn't even been a 2 months since glm 4.7",
          "score": 1,
          "created_utc": "2026-02-02 18:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o374fui",
          "author": "SeaworthinessThis598",
          "text": "man i wont sleep for 3 weeks like that , i love how much i hate this . and i hate how much i love it .",
          "score": 1,
          "created_utc": "2026-02-02 18:18:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37j4k6",
          "author": "synn89",
          "text": "I'm sure it'll release in two weeks.\n\nhttps://preview.redd.it/1phhm4h0u4hg1.jpeg?width=686&format=pjpg&auto=webp&s=52f3aaed5a75afe4be487eed18efaf108f131951",
          "score": 1,
          "created_utc": "2026-02-02 19:25:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37mr0q",
          "author": "archieve_",
          "text": "Chinese  New Year is coming",
          "score": 1,
          "created_utc": "2026-02-02 19:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37puxg",
          "author": "Bolt_995",
          "text": "Noting this.",
          "score": 1,
          "created_utc": "2026-02-02 19:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37zd8g",
          "author": "Individual-Hippo3043",
          "text": "I hope V4 doesn't disappoint due to inflated expectations, so that it doesn't end up like Gemini 3, which is good overall, but half the time it hallucinates answers.",
          "score": 1,
          "created_utc": "2026-02-02 20:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o387h93",
          "author": "jazir555",
          "text": "Gib now",
          "score": 1,
          "created_utc": "2026-02-02 21:19:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38bcty",
          "author": "itsnotKelsey",
          "text": "Oh let's goooo!!",
          "score": 1,
          "created_utc": "2026-02-02 21:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38cbb9",
          "author": "flywind008",
          "text": "holy s! so many models, i am more interested in in open source models but why most of them are from China ? meta move!",
          "score": 1,
          "created_utc": "2026-02-02 21:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38d050",
          "author": "power97992",
          "text": "Lol they work too muchÂ ",
          "score": 1,
          "created_utc": "2026-02-02 21:46:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39dl7t",
          "author": "ReasonablePossum_",
          "text": "Grok 4/20 will be rollin lol",
          "score": 1,
          "created_utc": "2026-02-03 00:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39hezi",
          "author": "Muddled_Baseball_",
          "text": "So many man so many. It's like streaming subscriptions",
          "score": 1,
          "created_utc": "2026-02-03 01:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39n2lb",
          "author": "Federal_Spend2412",
          "text": "I hope GLM 5.0 roll out before chinese new year :D",
          "score": 1,
          "created_utc": "2026-02-03 01:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39tg4e",
          "author": "Amazing_Athlete_2265",
          "text": "Fuck yeah, it's February now!!",
          "score": 1,
          "created_utc": "2026-02-03 02:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cvxpq",
          "author": "ComplexType568",
          "text": "i love how nonchalant all these ai heads are... still waiting for gemma 4",
          "score": 1,
          "created_utc": "2026-02-03 15:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j9nnx",
          "author": "Creamy-And-Crowded",
          "text": "Model velocity is now an operations problem. If you don't have regression tests + canary deployments, you don't have an agent, but a demo that breaks every February lol",
          "score": 1,
          "created_utc": "2026-02-04 14:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m1bwd",
          "author": "foundrynet",
          "text": "What's GLM?",
          "score": 1,
          "created_utc": "2026-02-04 22:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35pu0c",
          "author": "fugogugo",
          "text": "will any of them provide free inference?",
          "score": 1,
          "created_utc": "2026-02-02 14:18:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35syhx",
              "author": "SlowFail2433",
              "text": "Has anyone ever provided free inference?",
              "score": 10,
              "created_utc": "2026-02-02 14:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35zlqg",
                  "author": "basil232",
                  "text": "Groq and Cerebras definitely are doing that. Yes they try to get you hooked so you pay for their fast inference, but both of them offer a generous free tier.",
                  "score": 6,
                  "created_utc": "2026-02-02 15:09:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35zxb0",
                  "author": "fugogugo",
                  "text": "well there have been few models  giving free access for limited period on openrouter \n\nGrok 4.1 fast was free on December 2025 iirc   \nDevstral 2 was free until last week   \nGLM 4.7 Air also still free IIRC",
                  "score": 3,
                  "created_utc": "2026-02-02 15:10:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o383hbb",
                  "author": "MoffKalast",
                  "text": "Kind of everyone always has I guess? Free tiers of every major provider together cover all of my daily usage multiple times over tbh. Haven't paid for anything since GPT-4 years ago.",
                  "score": 2,
                  "created_utc": "2026-02-02 21:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o360jfa",
              "author": "yes-im-hiring-2025",
              "text": "Probably a super restricted (but free) version will be out on openrouter for a short time",
              "score": 2,
              "created_utc": "2026-02-02 15:13:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38wfyh",
              "author": "Cuplike",
              "text": "Literally just throw like 3 dollars every month on DeepSeek API and you'll be golden",
              "score": 2,
              "created_utc": "2026-02-02 23:24:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37jlxj",
              "author": "synn89",
              "text": "OpenCode's Zen will likely have it free for a limited time: https://x.com/ryanvogel/status/2017336961736847592",
              "score": 1,
              "created_utc": "2026-02-02 19:27:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quvqs9",
      "title": "Qwen/Qwen3-Coder-Next Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3-Coder-Next",
      "author": "coder543",
      "created_utc": "2026-02-03 15:58:52",
      "score": 684,
      "num_comments": 241,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3d1kxo",
          "author": "jacek2023",
          "text": "awesome!!! 80B coder!!! perfect!!!",
          "score": 103,
          "created_utc": "2026-02-03 16:07:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e156l",
              "author": "-dysangel-",
              "text": "Can't wait to see this one - the 80B already seemed great at coding",
              "score": 20,
              "created_utc": "2026-02-03 18:49:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d11vr",
          "author": "danielhanchen",
          "text": "We made dynamic Unsloth GGUFs for those interested! We're also going to release Fp8-Dynamic and MXFP4 MoE GGUFs!\n\nhttps://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF\n\nAnd a guide on using Claude Code / Codex locally with Qwen3-Coder-Next: https://unsloth.ai/docs/models/qwen3-coder-next",
          "score": 277,
          "created_utc": "2026-02-03 16:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d2pbw",
              "author": "mr_conquat",
              "text": "Goddamn that was fast",
              "score": 61,
              "created_utc": "2026-02-03 16:12:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d3f2p",
                  "author": "danielhanchen",
                  "text": ":)",
                  "score": 34,
                  "created_utc": "2026-02-03 16:15:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3g8mbx",
                  "author": "bene_42069",
                  "text": "that's what she said",
                  "score": 6,
                  "created_utc": "2026-02-04 01:25:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dr3rh",
              "author": "slavik-dev",
              "text": "Qwen published their own GGUF:\n\n[https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF](https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF)\n\nu/danielhanchen do you know, if author's GGUF will have any advantage?",
              "score": 31,
              "created_utc": "2026-02-03 18:04:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gau93",
                  "author": "dinerburgeryum",
                  "text": "Obvs not DH but looking at it: Qwen uses a more â€œtraditionalâ€ quantization scheme, letting mainline llama.cpp decide what weights need more and less bits assigned. Extending that, Qwenâ€™s quants do not use imatrix. Itâ€™s the last bit that interests me most: Iâ€™m actually very skeptical of imatrix-based quantization. It is much more like QAT than most people give it credit for, and the dataset used in calibration can have real downstream effects when it comes, especially, to agentic workflows. No disrespect to the Unsloth team, who are without question incredible allies in the open weights space, but I do prefer non-imatrix quants when available.",
                  "score": 18,
                  "created_utc": "2026-02-04 01:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3edjam",
              "author": "Chromix_",
              "text": "(Edit: [llama.cpp fix](https://www.reddit.com/r/LocalLLaMA/comments/1qvp0hm/model_qwen3next_correct_vectorized_key_gdiff/) was just merged)\n\nI'm getting the same type of error with the UD-Q4\\_K\\_XL quant on llama.cpp with -fa on. It randomly keeps complaining about non-existing syntax errors in different parts of go code, even though the code is correct. Qwen3-Next-80B-A3B-Instruct-UD-Q5\\_K\\_XL and the thinking version never behaved like that.\n\nFor example:\n\n    I found the issue! Look at line 41 in pkg/simd/distance.go:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    There's a syntax error - there's an extra closing parenthesis. The expression should be:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    But it's currently:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n            \n    This is causing a division by zero or incorrect calculation. Let me fix this:\n\nYet another error/interpretation of the same line:\n\n    I found the bug! Look at line 41 in pkg/simd/distance.go:\n        return dot / (float32(math.Sqrt(float64(normA))) * float32(math.Sqrt(float64(normB))))\n    \n    The issue is that the denominator is being calculated incorrectly. The current code divides by the first sqrt and then multiplies by the second sqrt, which is mathematically wrong. It should be dividing by the product of both norms.\n\nOr another flavor:\n\n    However, there's a subtle bug at line 349:\n        entity_id = entity_title_to_ids[entity.title]\n    \n    This line has a syntax error - it's missing the assignment operator. It should be:\n        entity_id = entity_title_to_ids[entity.title]\n\nYes, a syntax error in perfectly compiling code is very \"subtle\" (as it doesn't exist).",
              "score": 10,
              "created_utc": "2026-02-03 19:47:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gd5ii",
                  "author": "velcroenjoyer",
                  "text": "Same for me, the model makes up a bunch of syntax errors in any code I give it and \"fixes\" them with the same exact code that supposedly has a syntax errors; it's pretty much unusable for code review because of this. I also tried the original Qwen3 Next 80B A3B Instruct and it does the same thing but will at least admit that it's wrong. I'm using the Unsloth UD-IQ3\\_XXS GGUF quant of both models in the latest CUDA 12 llama.cpp build on Windows with this command: llama-server -m (path-to-model) --host (local-ip) --port 8080 -c 32000 --jinja",
                  "score": 3,
                  "created_utc": "2026-02-04 01:51:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3iybfv",
                  "author": "Clank75",
                  "text": "Ahh!  I've had exactly the same problems with Typescript.  Did some changes, they compiled cleanly, and then it keeps trying to fix \"ah, there is an unbalanced ) on line XXX, let me just fix that\" errors that don't exist.\n\nThis was with the MXFP4 quant.",
                  "score": 1,
                  "created_utc": "2026-02-04 13:26:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3nvbl5",
                  "author": "danielhanchen",
                  "text": "Sorry about that - we had to redo all imatrix quants - Q8_0, Q8_K_XL, MXFP4_MOE and BF16 don't need re-updating, but the rest do!",
                  "score": 1,
                  "created_utc": "2026-02-05 04:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d4yth",
              "author": "Terminator857",
              "text": "Where is your \"buy me a cup of coffee\" link so we can send some love? :) <3",
              "score": 21,
              "created_utc": "2026-02-03 16:23:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d7w70",
                  "author": "danielhanchen",
                  "text": "Appreciate it immensely, but it's ok :) The community is what keeps us going!",
                  "score": 37,
                  "created_utc": "2026-02-03 16:36:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3db4y2",
                  "author": "cleverusernametry",
                  "text": "They're in YC (sadly). They'll be somewhere between fine to batting off VCs throwing money at them. \n\nFor ours and the world's sake let's hope VC doesn't succeed in poisoning them",
                  "score": 6,
                  "created_utc": "2026-02-03 16:51:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dfb8v",
              "author": "ethertype",
              "text": "Do you have back-of-the napkin numbers for how well MXFP4 compares vs the 'classic' quants? In terms of quality, that is.",
              "score": 10,
              "created_utc": "2026-02-03 17:11:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmlsk",
                  "author": "danielhanchen",
                  "text": "I'm testing them!",
                  "score": 20,
                  "created_utc": "2026-02-03 17:44:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ee2ci",
              "author": "ClimateBoss",
              "text": "what is the difference plz? u/[danielhanchen](https://www.reddit.com/user/danielhanchen/)\n\n* unsloth GGUF compared to Qwen Coder Next official GGUF ?  \n* is unsloth chat template fixes better for llama server?  \n* requantized? accuracy than Qwen original?",
              "score": 6,
              "created_utc": "2026-02-03 19:50:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dgweb",
              "author": "oliveoilcheff",
              "text": "What is better for strix halo, fp8 or gguf?",
              "score": 4,
              "created_utc": "2026-02-03 17:18:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g4l6a",
                  "author": "mycall",
                  "text": "How much RAM do you have?  I have with 128GB RAM and was going to try Q8_0.\n\nUsing Q8_0 weights = 84.8 GB and KV @ 262,144 ctx â‰ˆ 12.9 GB (assuming fp16/bf16 KV):\n\n(84.8 + 12.9) Ã— 1.15 = 112.355 GB (max context window * 15% extra)",
                  "score": 2,
                  "created_utc": "2026-02-04 01:02:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dmdv2",
              "author": "TurnUpThe4D3D3D3",
              "text": "Love you guys",
              "score": 4,
              "created_utc": "2026-02-03 17:43:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dpauh",
                  "author": "danielhanchen",
                  "text": "Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:56:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e02jh",
              "author": "Far-Low-4705",
              "text": "what made you start to do MXFP4 MoE? do you reccomend that over the standard default Q4km?",
              "score": 3,
              "created_utc": "2026-02-03 18:45:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f8qq5",
                  "author": "R_Duncan",
                  "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i\\_found\\_that\\_mxfp4\\_has\\_lower\\_perplexity\\_than\\_q4\\_k/](https://www.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/)\n\nSeems that some hybrid models have way better perplexity with some less size",
                  "score": 5,
                  "created_utc": "2026-02-03 22:13:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3de0p8",
              "author": "robertpro01",
              "text": "Hi u/danielhanchen , I am trying to run the model within ollama, but looks like it failed to load, any ideas?  \n  \ndocker exec 5546c342e19e ollama run [hf.co/unsloth/Qwen3-Coder-Next-GGUF:Q4\\_K\\_M](http://hf.co/unsloth/Qwen3-Coder-Next-GGUF:Q4_K_M)  \nError: 500 Internal Server Error: llama runner process has terminated: error loading model: missing tensor 'blk.0.ssm\\_in.weight'  \nllama\\_model\\_load\\_from\\_file\\_impl: failed to load model",
              "score": 2,
              "created_utc": "2026-02-03 17:05:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmnm7",
                  "author": "danielhanchen",
                  "text": "Probably best to update Ollama",
                  "score": 5,
                  "created_utc": "2026-02-03 17:44:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dlsjd",
                  "author": "R_Duncan",
                  "text": "Do you have the plain llama.cpp or you got a version capable of running qwen3-next ?",
                  "score": 1,
                  "created_utc": "2026-02-03 17:40:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3l10uw",
                  "author": "molecula21",
                  "text": "Iâ€™m facing the same issue with ollama. I updated it to the pre release 0.15.5 but that didnâ€™t help. I am running ollama with open code on a DGX spark",
                  "score": 1,
                  "created_utc": "2026-02-04 19:22:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f2n1i",
              "author": "Status_Contest39",
              "text": "Fast as lightning, even the shadow can not catch up, this is the legendary mode of the speed of light.",
              "score": 2,
              "created_utc": "2026-02-03 21:44:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ftu6y",
              "author": "coreyfro",
              "text": "I use your models!!!\n\nI have been running Qwen3-Coder-30B at Q8.  Looks like Qwen3-Coder-80B at Q4 performs equally (40tps on a Strix Halo, 64GB)\n\nI also downloaded 80B as Q3.  It's 43tps on same hardware but I could claw back some of my RAM (I allocate as little RAM for UMA as possible on Linux)\n\nDo you have any idea which is most useful and what I am sacrificing with the quantizing?  I know the theory but I don't have enough practical experience with these models.",
              "score": 1,
              "created_utc": "2026-02-04 00:04:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3g6nde",
              "author": "JsThiago5",
              "text": "Thanks! Do you know if MXFP4 gguf will appear to old models?",
              "score": 1,
              "created_utc": "2026-02-04 01:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gjq3q",
              "author": "Odd-Ordinary-5922",
              "text": "even with setting an api key using a command claude code still asks me for a way to sign in? do you know why...",
              "score": 1,
              "created_utc": "2026-02-04 02:28:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gsjw8",
              "author": "emaiksiaime",
              "text": "Thanks! I can run it with decent context and good speed on my potato! This is truly an incredible and accessible model! Itâ€™s a huge step in democratizing coding models! Thanks for making it that much more accessible!",
              "score": 1,
              "created_utc": "2026-02-04 03:18:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3db42o",
          "author": "ilintar",
          "text": "I knew it made sense to spend all those hours on the Qwen3 Next adaptation :)",
          "score": 131,
          "created_utc": "2026-02-03 16:51:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dc0xe",
              "author": "itsappleseason",
              "text": "bless you king",
              "score": 26,
              "created_utc": "2026-02-03 16:55:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ddb6d",
              "author": "No_Swimming6548",
              "text": "Thanks a lot man",
              "score": 22,
              "created_utc": "2026-02-03 17:01:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dv9we",
              "author": "jacek2023",
              "text": "...now all we need is speed ;)",
              "score": 7,
              "created_utc": "2026-02-03 18:23:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dvdkw",
                  "author": "ilintar",
                  "text": "Actually I think proper prompt caching is more urgent right now.",
                  "score": 16,
                  "created_utc": "2026-02-03 18:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dxjng",
              "author": "No_Conversation9561",
              "text": "Awesome work, man",
              "score": 3,
              "created_utc": "2026-02-03 18:33:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eclf6",
              "author": "wanderer_4004",
              "text": "Any chance for getting better performance on Apple silicon? With llama.cpp I get 20Tok/s on M1 64GB with Q4KM while with MLX I get double that (still happy though that you did all the work to get it to run with llama.cpp!).",
              "score": 2,
              "created_utc": "2026-02-03 19:43:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f1a6u",
                  "author": "ilintar",
                  "text": "Yeah, there are some optimizations in the works, don't know if x2 is achievable though.",
                  "score": 3,
                  "created_utc": "2026-02-03 21:38:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f16na",
              "author": "epigen01",
              "text": "<3",
              "score": 1,
              "created_utc": "2026-02-03 21:37:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d0hat",
          "author": "Ok_Knowledge_8259",
          "text": "so your saying a 3B activated parameter model can match the quality of sonnet 4.5??? that seems drastic... need to see if it lives up to the hype, seems a bit to crazy.",
          "score": 97,
          "created_utc": "2026-02-03 16:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3daojj",
              "author": "Single_Ring4886",
              "text": "Clearly it cant match it in everything probably only in Python and such but even that is good",
              "score": 35,
              "created_utc": "2026-02-03 16:49:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dlwhw",
              "author": "ForsookComparison",
              "text": "> can match the quality of sonnet 4.5???\n\nYou must be new. Every model claims this. The good ones usually compete with Sonnet 3.7 and the bad ones get forgotten.",
              "score": 69,
              "created_utc": "2026-02-03 17:41:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3doc6t",
                  "author": "Neither-Phone-7264",
                  "text": "i mean k2.5 is pretty damn close. granted, they're in the same weight class so its not like a model 1/10th the size overtaking it.",
                  "score": 37,
                  "created_utc": "2026-02-03 17:52:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ex52f",
                  "author": "buppermint",
                  "text": "This is extremely delusional. There are LOTS of open-weight models far, far better than Sonnet 3.7. This is speaking as someone who spent a huge amount of time coding with Sonnet 3.7/4.0 last summer - at that point the LLM could barely remember its original task after 100k tokens, and would make up insane hacky fixes because it didn't the intelligence to understand full architectures.\n\nModern 30B MoEs are easily at that level already. Using GLM-4.7 Flash with opencode requires me to use the same tricks I had to do with Sonnet 3.7 + claude code, but with everything 1000x cheaper. Stuff like K2/GLM4.7 are far, far better.\n\nThis is the same kind of people who insist that GPT-3.5 or GPT-4 was the best LLM and that everything else has gotten progressively worse for years. No, that level of performance was just new to you at the time so your brain has misencoded it as being better than it is.",
                  "score": 16,
                  "created_utc": "2026-02-03 21:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3di62y",
              "author": "AppealSame4367",
              "text": "Have you tried Step 3.5 Flash? You will be very surprised.",
              "score": 10,
              "created_utc": "2026-02-03 17:24:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fjy9i",
                  "author": "effortless-switch",
                  "text": "When it stops itself from getting in a loop on every third prompt maybe I'll finally be able to test it.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:10:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gtkcb",
              "author": "RnRau",
              "text": "Yeah - I'll wait for the next edition of swe-rebench before accepting such claims :)",
              "score": 1,
              "created_utc": "2026-02-04 03:24:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d66r3",
              "author": "-p-e-w-",
              "text": "Itâ€™s 80B A3B. I would be surprised if Sonnet were much larger.",
              "score": -19,
              "created_utc": "2026-02-03 16:28:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d78zy",
                  "author": "Orolol",
                  "text": "I would be surprised if sonnet is smaller than 1T total params.",
                  "score": 27,
                  "created_utc": "2026-02-03 16:33:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3djlfd",
          "author": "Thrumpwart",
          "text": "FYI from the HF page:\n\n\"To achieve optimal performance, we recommend the following sampling parameters: temperature=1.0, top_p=0.95, top_k=40.\"",
          "score": 28,
          "created_utc": "2026-02-03 17:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3de3qc",
          "author": "reto-wyss",
          "text": "It certainly goes brrrrr.\n\n\n- Avg prompt throughput: **24469.6 tokens/s**, \n- Avg generation throughput: 54.7 tokens/s, \n- Running: 28 reqs, Waiting: 100 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 0.0%\n\n\nTesting with the FP8 with vllm and 2x Pro 6000.",
          "score": 23,
          "created_utc": "2026-02-03 17:05:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dokd5",
              "author": "Eugr",
              "text": "Generation seems to be slow for 3B active parameters??",
              "score": 17,
              "created_utc": "2026-02-03 17:53:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dr053",
                  "author": "SpicyWangz",
                  "text": "I think thatâ€™s been the case with qwen next architecture. Itâ€™s still not getting the greatest implementation",
                  "score": 7,
                  "created_utc": "2026-02-03 18:04:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ens6z",
                  "author": "reto-wyss",
                  "text": "It's just a log value and it's simultaneous 25k pp/s and 54 tg/s, it was just starting to to process the queue, so no necessarily saturated. I was just excited to run on the first try :P",
                  "score": 2,
                  "created_utc": "2026-02-03 20:35:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ebsjs",
                  "author": "meganoob1337",
                  "text": "Or maybe not all requests are generating yet (see 28 running ,100 waiting looks like new requests are still started)",
                  "score": 1,
                  "created_utc": "2026-02-03 19:39:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e86an",
              "author": "Eugr",
              "text": "How are you benchmarking? If you are using vLLM logs output (and looks like you are), the numbers there are not representative and all over the place as it reports on individual batches, not actual requests.\n\nCan you try to run llama-benchy?\n\n```bash\nuvx llama-benchy --base-url http://localhost:8000/v1 --model Qwen/Qwen3-Coder-Next-FP8 --depth 0 4096 8192 16384 32768 --adapt-prompt --tg 128 --enable-prefix-caching\n```",
              "score": 5,
              "created_utc": "2026-02-03 19:22:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3e8apu",
              "author": "Eugr",
              "text": "This is what I'm getting on my single DGX Spark (which is much slower than your RTX6000):\n\n| model                     |            test |             t/s |       ttfr (ms) |    est_ppt (ms) |   e2e_ttft (ms) |\n|:--------------------------|----------------:|----------------:|----------------:|----------------:|----------------:|\n| Qwen/Qwen3-Coder-Next-FP8 |          pp2048 | 3743.54 Â± 28.64 |   550.02 Â± 4.17 |   547.11 Â± 4.17 |   550.06 Â± 4.18 |\n| Qwen/Qwen3-Coder-Next-FP8 |           tg128 |    44.63 Â± 0.05 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d4096 | 3819.92 Â± 28.92 |  1075.25 Â± 8.14 |  1072.34 Â± 8.14 |  1075.29 Â± 8.15 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d4096 |    44.15 Â± 0.09 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d4096 | 1267.04 Â± 13.75 | 1619.46 Â± 17.59 | 1616.55 Â± 17.59 | 1619.49 Â± 17.59 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d4096 |    43.41 Â± 0.38 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d8192 | 3723.15 Â± 29.73 | 2203.34 Â± 17.48 | 2200.43 Â± 17.48 | 2203.38 Â± 17.48 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d8192 |    43.14 Â± 0.07 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d8192 |   737.40 Â± 3.90 | 2780.31 Â± 14.71 | 2777.40 Â± 14.71 | 2780.35 Â± 14.72 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d8192 |    42.71 Â± 0.04 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d16384 | 3574.05 Â± 11.74 | 4587.12 Â± 15.02 | 4584.21 Â± 15.02 | 4587.15 Â± 15.01 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d16384 |    41.52 Â± 0.03 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d16384 |   393.58 Â± 0.69 |  5206.47 Â± 9.16 |  5203.56 Â± 9.16 | 5214.69 Â± 20.61 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d16384 |    41.09 Â± 0.01 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d32768 |  3313.36 Â± 0.57 |  9892.57 Â± 1.69 |  9889.66 Â± 1.69 |  9892.61 Â± 1.69 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d32768 |    38.82 Â± 0.04 |                 |                 |                 |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d32768 |   193.06 Â± 0.12 | 10610.91 Â± 6.33 | 10608.00 Â± 6.33 | 10610.94 Â± 6.34 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d32768 |    38.47 Â± 0.02 |                 |                 |                 |\n\nllama-benchy (0.1.2)\ndate: 2026-02-03 11:14:29 | latency mode: api",
              "score": 4,
              "created_utc": "2026-02-03 19:22:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e8cur",
                  "author": "Eugr",
                  "text": "Note, that by default vLLM disables prefix caching on Qwen3-Next models, so the performance will suffer on actual coding tasks as vLLM will have to re-process repeated prompts (which is indicated by your KV cache hit rate).\n\nYou can enable prefix caching by adding `--enable-prefix-caching` to your vLLM arguments, but as I understand, support for this architecture is experimental. It does improve the numbers for follow up prompts at the expense of somewhat slower prompt processing of the initial prompt:\n\n| model                     |            test |             t/s |        ttfr (ms) |     est_ppt (ms) |    e2e_ttft (ms) |\n|:--------------------------|----------------:|----------------:|-----------------:|-----------------:|-----------------:|\n| Qwen/Qwen3-Coder-Next-FP8 |          pp2048 | 3006.54 Â± 72.99 |   683.87 Â± 16.66 |   681.47 Â± 16.66 |   683.90 Â± 16.65 |\n| Qwen/Qwen3-Coder-Next-FP8 |           tg128 |    42.68 Â± 0.57 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d4096 | 3019.83 Â± 81.96 |  1359.78 Â± 37.52 |  1357.39 Â± 37.52 |  1359.80 Â± 37.52 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d4096 |    42.84 Â± 0.14 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d4096 | 2368.35 Â± 46.78 |   867.47 Â± 17.30 |   865.08 Â± 17.30 |   867.51 Â± 17.30 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d4096 |    42.12 Â± 0.40 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_pp @ d8192 | 3356.63 Â± 32.43 |  2443.17 Â± 23.69 |  2440.77 Â± 23.69 |  2443.21 Â± 23.68 |\n| Qwen/Qwen3-Coder-Next-FP8 |  ctx_tg @ d8192 |    41.97 Â± 0.05 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 |  pp2048 @ d8192 | 2723.63 Â± 22.21 |    754.38 Â± 6.12 |    751.99 Â± 6.12 |    754.41 Â± 6.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |   tg128 @ d8192 |    41.56 Â± 0.12 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d16384 | 3255.68 Â± 17.66 |  5034.97 Â± 27.35 |  5032.58 Â± 27.35 |  5035.02 Â± 27.35 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d16384 |    40.44 Â± 0.26 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d16384 | 2502.11 Â± 49.83 |   821.22 Â± 16.12 |   818.83 Â± 16.12 |   821.26 Â± 16.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d16384 |    40.22 Â± 0.03 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_pp @ d32768 | 3076.52 Â± 12.46 | 10653.55 Â± 43.19 | 10651.16 Â± 43.19 | 10653.61 Â± 43.19 |\n| Qwen/Qwen3-Coder-Next-FP8 | ctx_tg @ d32768 |    37.93 Â± 0.04 |                  |                  |                  |\n| Qwen/Qwen3-Coder-Next-FP8 | pp2048 @ d32768 | 2161.97 Â± 18.51 |    949.75 Â± 8.12 |    947.36 Â± 8.12 |    949.78 Â± 8.12 |\n| Qwen/Qwen3-Coder-Next-FP8 |  tg128 @ d32768 |    37.20 Â± 0.36 |                  |                  |                  |\n\nllama-benchy (0.1.2)\ndate: 2026-02-03 10:50:37 | latency mode: api",
                  "score": 5,
                  "created_utc": "2026-02-03 19:23:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hrndp",
                  "author": "foxpro79",
                  "text": "My guy just wanted to give you a huge thanks and shout out. Your contributions on the nvidia forums have been an inspiration for me to take the plunge and buy a spark for my own hobby-ing.",
                  "score": 2,
                  "created_utc": "2026-02-04 07:36:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e0aac",
              "author": "Flinchie76",
              "text": "How does it compare to MiniMax in 4 bit (should fit on those cards)?",
              "score": 1,
              "created_utc": "2026-02-03 18:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d6fko",
          "author": "teachersecret",
          "text": "This looks really, really interesting.\n\nMight finally be time to double up my 4090. Ugh.\n\nI will definitely be trying this on my 4090/64gb ddr4 rig to see how it does with moe offload. Guessing this thing will still be quite performant.\n\nAnyone given it a shot yet? Howâ€™s she working for you?",
          "score": 18,
          "created_utc": "2026-02-03 16:29:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3djoa7",
              "author": "ArckToons",
              "text": "Iâ€™ve got the same setup. Mind sharing how many t/s youâ€™re seeing, and whether youâ€™re running vLLM or llama.cpp?",
              "score": 7,
              "created_utc": "2026-02-03 17:31:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d8oaz",
              "author": "Additional_Ad_7718",
              "text": "Please update me so I know if it's usable speeds or not ðŸ«¡ðŸ«¡ðŸ«¡",
              "score": 8,
              "created_utc": "2026-02-03 16:40:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gzk9h",
              "author": "TurnUpThe4D3D3D3",
              "text": "That should be plenty to run a Q4 version",
              "score": 1,
              "created_utc": "2026-02-04 04:01:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d61nq",
          "author": "Septerium",
          "text": "The original Qwen3 Next was so good in benchmarks, but actually using it was not a very nice experience",
          "score": 46,
          "created_utc": "2026-02-03 16:28:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3duv03",
              "author": "--Tintin",
              "text": "I like Qwen3 Next a lot. I think it aged well and is under appreciated.",
              "score": 20,
              "created_utc": "2026-02-03 18:21:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dbgfl",
              "author": "cleverusernametry",
              "text": "Besides it being slow as hell, at least on llama.cpp",
              "score": 11,
              "created_utc": "2026-02-03 16:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e1iqt",
                  "author": "-dysangel-",
                  "text": "It was crazy fast on MLX, especially the subquadratic attention was very welcome for us GPU poor Macs. Though I've settled into using GLM Coding Plan for coding anyway",
                  "score": 6,
                  "created_utc": "2026-02-03 18:51:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e1z7h",
              "author": "Far-Low-4705",
              "text": "how do you mean?\n\nI think it is the best model we have for usable long context.",
              "score": 6,
              "created_utc": "2026-02-03 18:53:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ee5j8",
                  "author": "Septerium",
                  "text": "I haven't been lucky with it for agentic coding, specially with long context. Even the first version of Devstral small produced better results for me",
                  "score": 2,
                  "created_utc": "2026-02-03 19:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hlo9p",
              "author": "relmny",
              "text": "I agree. I actually tested it a few times and didn't like anything about it, and went back to qwen3-Coder and others.\n\n  \nI hope it happens the same with qwen3-30b, that I used a lot at first, and then I noticed I started using other models more and more and then abandoned/deleted it... and then the Coder version came and that was my main model for a while (I still use it a lot).",
              "score": 2,
              "created_utc": "2026-02-04 06:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dlozt",
          "author": "ForsookComparison",
          "text": "This is what a lot of folks were dreaming of.\n\nFlash-speed tuned for coding that's not limited by such a small number of total params. Something to challenge gpt-oss-120b.",
          "score": 10,
          "created_utc": "2026-02-03 17:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ei064",
          "author": "Eugr",
          "text": "PSA: if you are using vLLM, you may want to:\n\n- Use `--enable-prefix-caching`, because vLLM disables prefix caching for mamba architectures by default, so coding workflows will be slower because of that.\n- Use `--attention-backend flashinfer` as default FLASH_ATTN backend requires much more VRAM to hold the same KV cache. For instance, my DGX Spark with `--gpu-memory-utilization 0.8` can only hold ~60K tokens in KV cache with the default attention backend, but with Flashinfer it can fit 171K tokens (without quantizing KV cache to fp8).",
          "score": 9,
          "created_utc": "2026-02-03 20:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ewbtt",
              "author": "HumanDrone8721",
              "text": "Does it work in cluster more (2x Spark) ?",
              "score": 1,
              "created_utc": "2026-02-03 21:15:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f0936",
                  "author": "Eugr",
                  "text": "I tried with Feb 1st vLLM build and it crashed in the cluster mode during inference, with both FLASH\\_ATTN and FLASHINFER backends. I'm trying to run with the fresh build now - let's see if it works.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:33:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d1zmk",
          "author": "Recoil42",
          "text": "https://preview.redd.it/shnwpwn00bhg1.png?width=4420&format=png&auto=webp&s=956bb077c3abaaac65a592c9a02b7e50be6a443b\n\nHoly balls. \n\nAnyone know what the token burn story looks like yet?",
          "score": 38,
          "created_utc": "2026-02-03 16:09:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d8ujf",
              "author": "coder543",
              "text": "It's an instruct model only, so token usage should be relatively low, even if Qwen instruct models often do a lot of thinking in the response these days.",
              "score": 23,
              "created_utc": "2026-02-03 16:41:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3d38at",
              "author": "ClimateBoss",
              "text": "ik\\_llama better add graph split after shittin on OG qwen3 next ROFL",
              "score": 4,
              "created_utc": "2026-02-03 16:15:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dxi53",
                  "author": "twavisdegwet",
                  "text": "or ideally mainline llama merges graph support- I know it's not a straight drop in but graph makes otherwise unusable models practical for me.",
                  "score": 3,
                  "created_utc": "2026-02-03 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3emwvn",
          "author": "noctrex",
          "text": "[https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4\\_MOE-GGUF](https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF)\n\nOh guess I'm gonna have some MXFP4 competition from the big boys ðŸ˜Š",
          "score": 7,
          "created_utc": "2026-02-03 20:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ezm28",
              "author": "ethertype",
              "text": "Do you have a ballpark number for the quality of MXFP4 vs Q4/Q5/Q6/Q8?",
              "score": 2,
              "created_utc": "2026-02-03 21:30:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m0yxy",
                  "author": "noctrex",
                  "text": "Unfortunately not. This would need quite expansive benchmarking and testing and unfortunately I haven't had the time to do it.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:13:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hzlas",
              "author": "ScoreUnique",
              "text": "Can I understand how is mxfp4 different than traditional or importance matrix quants? I've had a bit better of a performance on mxfp4 than on IQ not gonna lie. \n.thanks for the quants.",
              "score": 1,
              "created_utc": "2026-02-04 08:49:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3m0vis",
                  "author": "noctrex",
                  "text": "It's a quantization better suited for MoE models, it's quite simple actually, it quantizes the MoE tensors to FP4, and everything else to Q8.",
                  "score": 1,
                  "created_utc": "2026-02-04 22:13:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dhnav",
          "author": "HollowInfinity",
          "text": "This seems excellent so far, I'm using just a minimal agent loop with the 8-bit quant and gave it the test of having llama.cpp's llama-server output a CSV file with metrics for each request and it completed it using about 70,000 tokens. It rooted around the files first and even found where the metrics are already being aggregated for export and all in all took about 5 minutes.\n\nLiterally my go-to this morning was GLM-4.7-Flash and given that first test.. wow.",
          "score": 5,
          "created_utc": "2026-02-03 17:21:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fups1",
          "author": "dmter",
          "text": "It's so funny - it's not thinking kind so starts producing code right away, and it started thinking in the comments. then it produced 6 different versions, and every one of them is of course tested in latest software version (according to it), which is a nice touch.\nI just used the last version. After feeding debug output and 2 fixes it actually worked. about 15k tokens in total.\nGLM47q2 spent all available 30k context and didn't produce anything but the code it had in thinking didn't work.\n\nSo yeah this looks great at first glance - performance of 358B model but better and 4 times faster and also at least 2 times less token burn. But maybe my task was very easy (GPT120 failed though).\n\nOh and it's Q4 262k ctx - 20 t/s on 3090 with --fit on. 17 t/s when using about half of GPU memory (full moe offload).\n\n\nP.S. so I did some more prompts and it's not as good as it seemed but still nice. There was another prompt which was 1 shotted by GLM47q2 but Next Coder couldn't complete even after a few fixes.\n\nAlso I think Qwen3 Next Coder model could benefit from dedicated thinking mode as it misses key detail from prompt that need to be spelled out explicitly every time. \n\nMaybe thinking mode can be enabled with some command or llama.cpp parameter?",
          "score": 6,
          "created_utc": "2026-02-04 00:08:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eeqgj",
          "author": "2funny2furious",
          "text": "Please tell me they are going to keep adding the word next to all future releases.  Like Qwen3-Coder-Next-Next.",
          "score": 9,
          "created_utc": "2026-02-03 19:53:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3euyy0",
              "author": "cmpxchg8b",
              "text": "Like some kind of University project document naming.",
              "score": 3,
              "created_utc": "2026-02-03 21:09:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e60d5",
          "author": "Far-Low-4705",
          "text": "this is so useful.  \n  \nreally hoping for qwen 3 next 80b vl",
          "score": 5,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hkr7h",
              "author": "EbbNorth7735",
              "text": "I was just thinking the same thing. It seemed like the vision portion of qwen3 vl was relatively small",
              "score": 2,
              "created_utc": "2026-02-04 06:36:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d9qmm",
          "author": "Significant_Fig_7581",
          "text": "Finally!!!! When is the 30b coming?????",
          "score": 8,
          "created_utc": "2026-02-03 16:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dica8",
              "author": "pmttyji",
              "text": "\\+1. \n\nI really want to see what & how much difference the Next architecture makes? Like t/s difference between **Qwen3-Coder-30B** vs **Qwen3-Coder-Next-30B** ....",
              "score": 13,
              "created_utc": "2026-02-03 17:25:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dmji8",
                  "author": "R_Duncan",
                  "text": "It's not about t/s, maybe these are even slower for zero context, but use delta gated attention so kv cache is linear: context takes much less cache (like between 8k of other models) and do not grow much when increasing. Also, when you use long context, t/s don't drop that much. Reports are that these kind of models, despite using less VRAM, are way better in bench for long context like needle in haystack.",
                  "score": 10,
                  "created_utc": "2026-02-03 17:44:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dw23o",
          "author": "Danmoreng",
          "text": "Updated my Windows Powershell llama.cpp install and run script to use the new Qwen3-coder-next and automatically launch qwen-code. https://github.com/Danmoreng/local-qwen3-coder-env",
          "score": 3,
          "created_utc": "2026-02-03 18:27:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e5rh8",
          "author": "Aggressive-Bother470",
          "text": "I thought you'd forgotten about us, Qwen :D",
          "score": 3,
          "created_utc": "2026-02-03 19:11:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e80ws",
          "author": "kwinz",
          "text": "Hi! Sorry for the noob question,\nbut how does a model with this low number of active parameters affect VRAM usage?\n\nIf only 3B/80B parameters are active simultaneously, does it get meaningful acceleration on e.g. a 16GB VRAM card? (provided the rest can fit into system memory)?\n\nOr is it hard to predict which parameters will become active and the full model should be in VRAM for decent speed?\n\nIn other words can I get away with a quantization where only the active parameters, cache and context fit into VRAM, and the rest can spill into system memory, or will that kill performance?",
          "score": 3,
          "created_utc": "2026-02-03 19:21:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g5mai",
              "author": "arades",
              "text": "When you offload moe layers to CPU, it's the whole layer, it doesn't swap the active tensors to the GPU. So the expert layers run at system ram/CPU inference speed, and the layers on GPU run at GPU speed. However, since there's only 3B active, the CPU isn't going to need to go very fast, and the ram speed isn't as important since it's loading so little. So, you should still get acceptable speeds even with most of the weights on the CPU.\n\nWhat's most important about these next models is the attention architecture. It's slower up front, and benefits most from loading on the GPU, but it's also much more memory efficient, and inference doesn't slow down nearly as much as it fills. This means you can keep probably the full 256k context on a 16GB GPU and maintain high performance for the entire context window.",
              "score": 2,
              "created_utc": "2026-02-04 01:08:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3foikg",
          "author": "JoNike",
          "text": "So I tried the mxfp4 on my 5080 16gb. I got 192gb of ram.\n\nLoaded 15 layers on gpu, kept the 256k context and offloaded the rest on my RAM.\n\nIt's not fast as I could have expected, 11t/s. But it seems pretty good from the first couple tests. \n\nI think I will use it with my openclaw agent to give it a space to code at night without going through my claude tokens.",
          "score": 3,
          "created_utc": "2026-02-03 23:34:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ftkle",
              "author": "BigYoSpeck",
              "text": "Are you offloading MOE expert layers to CPU or just using partial GPU offload for all the layers? Use `-ncmoe 34` if you're not already. You should be closer to 30t/s",
              "score": 5,
              "created_utc": "2026-02-04 00:02:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g7zzp",
                  "author": "JoNike",
                  "text": "Doesn't seems to do any difference to me. I'll keep an eye on it. Care if I ask what kind of config you're using?\n\nEdit: Actually scratch that, I was doing it wrong, it does boost it quite a lot! Thanks for actually making me look into it!\n\nmy llama.cpp command for my 5080 16gb:\n\n```\n\n\n    llama-server -m Qwen3-Coder-Next-MXFP4_MOE.gguf\n      -c 262144 --n-gpu-layers 48 --n-cpu-moe 36\n      --host 127.0.0.1 --port 8080 -t 16 --parallel 1\n      --cache-type-k q4_0 --cache-type-v q4_0 \n      --mlock --flash-attn on\n```\n\nand this gives me 32.79 t/s!",
                  "score": 4,
                  "created_utc": "2026-02-04 01:22:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gez8z",
          "author": "ravage382",
          "text": "EDIT:\n\nAfter the llama.cpp fix today, it is doing really well in tool calling and general reasoning. I repeating the same prompt and  got as good of an answer as gpt120b using 4 tool calls to web tools. \n\nPre patch eval:\n\nInitial testing (Q6) with some web tools to do a product search came back with some rather mediocre results. I had it do a product search for corn husker lotion. It did one web search, pulled no actual pages and generated this summary(in which $5.66 was slightly over my price target of $6):  I searched current retail options for Corn Huskers Lotion 7 oz bottles, but the context you provided shows the *lowest price listed* is $9.54 for a single 7 oz bottle on Amazon (about $1.36/fl oz), and a 6-pack on eBay is $33.95â€”well above your $6 target.\n\nGiven the current pricing in the search results:\n\n* [Walmart.com](http://Walmart.com) and Amazon listings didn't specify a price under $6 in the snippets.\n* A 6-pack on eBay ($33.95) averages \\~$5.66 per bottleâ€”close but still slightly above $6 *per unit*.\n* No clear evidence of widespread shortage or confirmed sub-$6 deals was found in recent listings.\n\nSince you mentioned a *current shortage*, itâ€™s possible availability and pricing are inconsistent or fluctuating in real time. I recommend:\n\n* Checking [Walmart.com](http://Walmart.com) or Amazon directly for localized or third-party sellers.\n* Looking at local drugstores (e.g., CVS, Walgreens), discount retailers (e.g., Dollar General, Family Dollar), or grocery stores where shelf prices may differ.\n* Signing up for stock alerts on major sites in case supply improves.\n\nWould you like me to check current prices on a specific retailer (e.g., Walmart, Amazon, or local options)?\n\ngpt120b with the same set of tools and same prompt did 29 tool calls, between searches, page grabs and grabbing a few raw pages and then generated a paragraph summary with the cheapest options.\n\nCoding results look like they are an improvement over gpt120b, with a fully working html tetris clone on its first attempt. gpt120b has yet to manage that one.",
          "score": 3,
          "created_utc": "2026-02-04 02:01:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dc7j1",
          "author": "TacGibs",
          "text": "https://preview.redd.it/owke6orh8bhg1.jpeg?width=640&format=pjpg&auto=webp&s=0256a803ef598d14073957af5762d1d17c872667",
          "score": 15,
          "created_utc": "2026-02-03 16:56:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dcd4z",
              "author": "TacGibs",
              "text": "I know it's already there, but traditions are traditions ðŸ˜‚",
              "score": 26,
              "created_utc": "2026-02-03 16:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eacsm",
          "author": "xmikjee",
          "text": "Bless you <3\n\nhttps://preview.redd.it/6se2fnr70chg1.jpeg?width=540&format=pjpg&auto=webp&s=5636bc8ec4ba6a57e35893ba0e9fe6d3c84fe5ac\n\n",
          "score": 6,
          "created_utc": "2026-02-03 19:32:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d5w43",
          "author": "1ncehost",
          "text": "Wild",
          "score": 2,
          "created_utc": "2026-02-03 16:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e60c1",
          "author": "charliex2",
          "text": "did they fix the tool call bug?",
          "score": 2,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3h9i4s",
              "author": "ForsookComparison",
              "text": "In my testing, no",
              "score": 2,
              "created_utc": "2026-02-04 05:08:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ha4y2",
                  "author": "charliex2",
                  "text": "welp , thanks for replying",
                  "score": 2,
                  "created_utc": "2026-02-04 05:13:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fwd41",
          "author": "PANIC_EXCEPTION",
          "text": "It's pretty fast on M1 Max 64 GB MLX. I'm using 4 bits and running it with qwen-code CLI on a pretty big TypeScript monorepo.",
          "score": 2,
          "created_utc": "2026-02-04 00:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3il1s9",
              "author": "r1str3tto",
              "text": "Are you able to do anything else with your Mac while it runs? I stopped trying to use Qwen Next 80B (MLX) on my 64GB M3 Max because I was getting too much stutter and freeze in application UI.",
              "score": 1,
              "created_utc": "2026-02-04 12:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3rmvbm",
                  "author": "PANIC_EXCEPTION",
                  "text": "Yeah, works fine. I use about half maximum context. If you try to push it to full context, you might get a kernel panic. Make sure your backend never attempts to load multiple LLMs at the same time, that can also cause it.",
                  "score": 1,
                  "created_utc": "2026-02-05 19:08:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g704l",
          "author": "mdziekon",
          "text": "Speed wise, the Unsloth Q4\\_K\\_XL seems pretty solid (3090 + CPU offload, running on 7950x3D with 64GB of RAM; running latest llama-swap & llama.cpp on Linux). After some minor tuning I was able to achieve:  \n\\- PP (initial ctx load): \\~900t/s  \n\\- PP (further prompts of various size): 90t/s to 330t/s (depends on prompt size, the larger the better)  \n\\- TG (initial prompts): \\~37t/s  \n\\- TG (further, \\~180k ctx): \\~31t/s\n\nCan't say much about output quality yet, so far I was able to fix a simple issue with TS code compilation code using Roo, but I've noticed that from time to time it didn't go deep enough and provided only a partial fix (however, there was no way for the agent to verify whether the solution was actually working). Need to test it further and compare to cloud based GLM4.7",
          "score": 2,
          "created_utc": "2026-02-04 01:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lbv8x",
              "author": "PaMRxR",
              "text": "Do you mind sharing the llama-server options? I have a similar setup (except 32GB RAM) and prompt processing is quite slow at ~200t/s.",
              "score": 1,
              "created_utc": "2026-02-04 20:13:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ld2ah",
                  "author": "mdziekon",
                  "text": "Try bumping batch physical size and logical size (-b and -ub) to 4096. It slightly slows down generation, but I found it greatly sped up initial prompt processing.",
                  "score": 1,
                  "created_utc": "2026-02-04 20:19:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i4kqi",
          "author": "gkon7",
          "text": "Sorry for my ignorance, but I have 96â€¯GB of DDR5. Can I get decent performance with an 16â€¯GB AMDâ€¯9060â€¯XT or are these improvements specific to CUDA? Also, in this architecture, does increasing the context cause prompt processing performance to die?",
          "score": 2,
          "created_utc": "2026-02-04 09:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i7zcz",
              "author": "BigYoSpeck",
              "text": "I'm running an RX 6800 XT using ROCm on a 64gb DDR4 3600 system and getting about 25tok/s so I would imagine between the higher bandwidth of your DDR5 and lower bandwidth of your 9060 XT you should get somewhere in the same ballpark as me  \n  \nI haven't really tested very long context yet but get over 400tok/s prompt processing on up to a few thousand token prompts",
              "score": 1,
              "created_utc": "2026-02-04 10:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i9j21",
                  "author": "gkon7",
                  "text": "Thanks. 400 tok/s for pp seems very nice actually.",
                  "score": 1,
                  "created_utc": "2026-02-04 10:23:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d1lif",
          "author": "wapxmas",
          "text": "Qwen3 next Implementation still have bugs, qwen team refrains from any contribution to it. I tried it recently on master branch, it was short python function and to my surprise the model was unable to see colon after function suggesting a fix, just hilarious.",
          "score": 6,
          "created_utc": "2026-02-03 16:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ecc8b",
              "author": "neverbyte",
              "text": "I think I might be seeing something similar.  I am running the Q6 with lamma.cpp + Cline and unsloth recommended settings.  It will write a source file then say \"the file has some syntax errors\" or \"the file has been corrupted by auto-formatting\" and then it tries to fix it and rewrites the entire file without making any changes, then gets stuck in a loop trying to fix the file indefinitely. Haven't seen this before.",
              "score": 5,
              "created_utc": "2026-02-03 19:42:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j31fr",
                  "author": "wapxmas",
                  "text": "Today it was fixed finally as I think https://github.com/ggml-org/llama.cpp/pull/19324. Tested my my prompt that revealed the issue - now all work flawlessly. Also tested coder without this fix - I can say I now have local llm that I can use daily even for real tasks, gave the model huge C project - it correctly made architecture document. Did it with roo code.",
                  "score": 3,
                  "created_utc": "2026-02-04 13:52:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ep7eu",
                  "author": "neverbyte",
                  "text": "I'm seeing similar behavior with Q8_K_XL as well so maybe getting this running on vllm is the play here.",
                  "score": 2,
                  "created_utc": "2026-02-03 20:42:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hpr18",
              "author": "alexeiz",
              "text": "I just tried it in Cline (which I think routes to Openrouter).  My test is to convert some Perl code to Python, and qwen3-coder-next created a working version on the first try, which surprised me.  Usually a smaller model needs to run the generated code a couple of times to fix mistakes.  But this model didn't make any mistakes.",
              "score": 2,
              "created_utc": "2026-02-04 07:19:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d55wp",
              "author": "Terminator857",
              "text": "Which implementation?  MLX, tensor library, llama.cpp?",
              "score": 4,
              "created_utc": "2026-02-03 16:24:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d5fd2",
                  "author": "wapxmas",
                  "text": "llama.cpp, or did you see any other posts on this channel about buggy implementation? Stay tuned.",
                  "score": -16,
                  "created_utc": "2026-02-03 16:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dr5u7",
          "author": "bobaburger",
          "text": "https://preview.redd.it/oniow1pokbhg1.png?width=738&format=png&auto=webp&s=edd5b991e1ab2af97e151bf4db556159d97e077c\n\ndamn it, now i have to buy a new GPU",
          "score": 2,
          "created_utc": "2026-02-03 18:05:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dsw4t",
              "author": "strosz",
              "text": "Works fine if you have 64gb or more RAM with your 5060ti 16GB and can take a short break for the answer. Got a response in under 1 minute for an easy test at least, but more context will take a good coffe break probably",
              "score": 6,
              "created_utc": "2026-02-03 18:12:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eoluc",
              "author": "arcanemachined",
              "text": "Weird that the tool doesn't allow you to add RAM into the mix.",
              "score": 1,
              "created_utc": "2026-02-03 20:39:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3edc5t",
          "author": "Rascazzione",
          "text": "Anyone knows whatâ€™s the difference between FP8 and FP8 dynamic?\n\nThanks",
          "score": 2,
          "created_utc": "2026-02-03 19:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dciex",
          "author": "Hoak-em",
          "text": "Full-local setup idea: nemotron-orchestrator-8b running locally on your computer (maybe a macbook), this running on a workstation or gaming PC, orchestrator orchestrates a buncha these in parallel -- could work given the sparsity, maybe even with a CPU RAM+VRAM setup for Qwen3-Coder-Next. Just gotta figure out how to configure the orchestrator harness correctly -- opencode could work well as a frontend for this kinda thing",
          "score": 1,
          "created_utc": "2026-02-03 16:58:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dhtao",
          "author": "popiazaza",
          "text": "Finally, a Composer 2 model. ^\\s",
          "score": 1,
          "created_utc": "2026-02-03 17:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dj4tf",
          "author": "Thrumpwart",
          "text": "If these benchmarks are accurate this is incredible. Now I need's me a 2nd chonky boi W7900 or an RTX Pro.",
          "score": 1,
          "created_utc": "2026-02-03 17:28:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dk49h",
          "author": "DeedleDumbDee",
          "text": "Is there a way to set this up in VScode as a custom agent?",
          "score": 1,
          "created_utc": "2026-02-03 17:33:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3docll",
              "author": "Educational_Sun_8813",
              "text": "you can setup any model with openapi compatible llama-server",
              "score": 3,
              "created_utc": "2026-02-03 17:52:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dlkl6",
          "author": "R_Duncan",
          "text": "Waiting for u/noctrex ....",
          "score": 1,
          "created_utc": "2026-02-03 17:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ent7n",
              "author": "noctrex",
              "text": "Got it up: [https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4\\_MOE-GGUF](https://huggingface.co/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF)",
              "score": 7,
              "created_utc": "2026-02-03 20:36:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fo9u3",
                  "author": "NoahFect",
                  "text": "Any idea how this compares to Unsloth's UD Q4 version?",
                  "score": 1,
                  "created_utc": "2026-02-03 23:33:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dvlha",
              "author": "noctrex",
              "text": "Oh no, gonna take couple of hours..",
              "score": 5,
              "created_utc": "2026-02-03 18:24:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3doik2",
          "author": "corysama",
          "text": "I'm running 64 GB of CPU RAM and a 4090 with 24 GB of VRAM.\n\nSo.... I'm good to run which GGUF quant?",
          "score": 1,
          "created_utc": "2026-02-03 17:53:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3duv1q",
              "author": "pmttyji",
              "text": ">It runs on **46GB RAM**/VRAM/unified memory (85GB for 8-bit), is non-reasoning for ultra-quick code responses. We introduce new MXFP4 quants for great quality and speed and youâ€™ll also learn how to run the model on Codex & Claude Code. - [Unsloth guide](https://unsloth.ai/docs/models/qwen3-coder-next)",
              "score": 3,
              "created_utc": "2026-02-03 18:21:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dw9be",
              "author": "Danmoreng",
              "text": "yup works fine. just tested the UD Q4 variant which is ~50GB on my 64GB RAM + 5080 16GB VRAM",
              "score": 3,
              "created_utc": "2026-02-03 18:27:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dy7sj",
                  "author": "pmttyji",
                  "text": "More stats please. t/s, full command, etc.,",
                  "score": 3,
                  "created_utc": "2026-02-03 18:36:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dz26v",
          "author": "Far-Low-4705",
          "text": "holy sheet",
          "score": 1,
          "created_utc": "2026-02-03 18:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eng0o",
          "author": "Alternative-Theme885",
          "text": "whoa\n\n",
          "score": 1,
          "created_utc": "2026-02-03 20:34:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f67ur",
          "author": "No_Mango7658",
          "text": "Oh wow. 80b-a3b!\n\nAmazing",
          "score": 1,
          "created_utc": "2026-02-03 22:01:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fjrm7",
          "author": "billy_booboo",
          "text": "This is what I've been waiting for. Guess it's time to buy that dgx spark ðŸ« ",
          "score": 1,
          "created_utc": "2026-02-03 23:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fnfx9",
          "author": "adam444555",
          "text": "Testing around with with the MXFP4\\_MOE version.\n\nHardware: 5090 9800x3D 32GB RAM\n\nDeploy config: 65536 ctx, kvc dtype fp16, 17 moe layer offload\n\nIt works surprisingly well even with MOE layer offload.\n\nI haven't do a comprehensive benchmark, but just using it in claude code.\n\nHere is a log with significant read and write tokens.\n\nprompt eval time =   29424.73 ms / 15089 tokens (    1.95 ms per token,   512.80 tokens per second)\n\neval time =   22236.64 ms /   647 tokens (   34.37 ms per token,    29.10 tokens per second)",
          "score": 1,
          "created_utc": "2026-02-03 23:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3geam7",
              "author": "DOAMOD",
              "text": "prompt eval time =    7038.33 ms /  3864 tokens (    1.82 ms per token,   548.99 tokens per second)\n\neval time =    1726.58 ms /    66 tokens (   26.16 ms per token,    38.23 tokens per second)\n\ntotal time =    8764.91 ms /  3930 tokens\n\nslot      release: id  2 | task 421 | stop processing: n\\_tokens = 26954, truncated = 0\n\nNice",
              "score": 1,
              "created_utc": "2026-02-04 01:57:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gevo1",
                  "author": "DOAMOD",
                  "text": "prompt eval time =    2682.17 ms /   773 tokens (    3.47 ms per token,   288.20 tokens per second)\n\neval time =    1534.91 ms /    57 tokens (   26.93 ms per token,    37.14 tokens per second)\n\ntotal time =    4217.08 ms /   830 tokens\n\nslot      release: id  2 | task 766 | stop processing: n\\_tokens = 60567, truncated = 0",
                  "score": 1,
                  "created_utc": "2026-02-04 02:00:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ibqx5",
              "author": "adam444555",
              "text": "Actually get much better speed by swtiching from WSL2 to windows. Crazy how bad WSL2 is to serve model",
              "score": 1,
              "created_utc": "2026-02-04 10:43:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g9k87",
          "author": "street_melody",
          "text": "Since it is MoE can it run on smaller gpus with q4km?",
          "score": 1,
          "created_utc": "2026-02-04 01:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gfgeg",
          "author": "robberviet",
          "text": "80b. Much welcome than the 500b.",
          "score": 1,
          "created_utc": "2026-02-04 02:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gp5k1",
          "author": "dragonmantank",
          "text": "I'm gonna be honest, this came out at the best possible time. I'm currently between Claude timeouts, and been playing more and more with local LLMs. I've got the Q4\\_K\\_XL quant running from unsloth on one of the older Minisforum AI X1 Pros and this thing is blowing other models out of the water. I've had so much trouble getting things to run in Kilo Code I was honestly beginning to question the viability of a coding assistant.",
          "score": 1,
          "created_utc": "2026-02-04 02:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gpvs6",
          "author": "Kasatka06",
          "text": "Result with 4x3090 seems fasst, faster than glm 4.7\n\n\n\ncommand: \\[\n\n\"/models/unsloth/Qwen3-Coder-Next-FP8-Dynamic\", \n\n\"--disable-custom-all-reduce\",\n\n\"--max-model-len\",\"70000\", \n\n\"--enable-auto-tool-choice\", \n\n\"--tool-call-parser\",\"qwen3\\_coder\",\n\n\"--max-num-seqs\", \"8\",\n\n\"--gpu-memory-utilization\", \"0.95\", \n\n\"--host\", \"0.0.0.0\",\n\n\"--port\", \"8000\",\n\n\"--served-model-name\", \"local-model\", \n\n\"--enable-prefix-caching\",\n\n\"--tensor-parallel-size\", \"4\",      # 2 GPUs per replica\n\n\"--max-num-batched-tokens\", \"8096\",\n\n'--override-generation-config={\"top\\_p\":0.95,\"temperature\":1.0,\"top\\_k\":40}',\n\n\\]\n\n\n\n\n\n| model        |           test |              t/s |       ttfr (ms) |    est\\_ppt (ms) |   e2e\\_ttft (ms) |\n\n|:-------------|---------------:|-----------------:|----------------:|----------------:|----------------:|\n\n| local-model |         pp2048 | 3043.21 Â± 221.64 |  624.66 Â± 49.46 |  615.79 Â± 49.46 |  624.79 Â± 49.45 |\n\n| local-model |           tg32 |   121.99 Â± 10.93 |                 |                 |                 |\n\n| local-model | pp2048 @ d4096 |  3968.76 Â± 45.41 | 1411.31 Â± 10.72 | 1402.43 Â± 10.72 | 1411.45 Â± 10.80 |\n\n| local-model |   tg32 @ d4096 |    105.47 Â± 0.63 |                 |                 |                 |\n\n| local-model | pp2048 @ d8192 |  4178.73 Â± 33.56 |  2192.20 Â± 6.25 |  2183.32 Â± 6.25 |  2192.46 Â± 6.12 |\n\n| local-model |   tg32 @ d8192 |    104.26 Â± 0.23 |                 |                 |                 |",
          "score": 1,
          "created_utc": "2026-02-04 03:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ibqmy",
              "author": "MinusKarma01",
              "text": "Is the 121.99 tok/s generation speed for one sequence or several?",
              "score": 1,
              "created_utc": "2026-02-04 10:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j94l9",
                  "author": "Kasatka06",
                  "text": "Iam not sure, i just run llama benchy test into the vllm endpoint",
                  "score": 1,
                  "created_utc": "2026-02-04 14:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gttf6",
          "author": "RayanAr",
          "text": "is it better than KIMI K2.5?",
          "score": 1,
          "created_utc": "2026-02-04 03:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gvj4z",
          "author": "mitch_feaster",
          "text": "Has anyone tried this out? How's the Claude Code experience?",
          "score": 1,
          "created_utc": "2026-02-04 03:36:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hgdgr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-04 06:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hheu7",
              "author": "MichaelBui2812",
              "text": "Have you tried MiniMax v2 or v2.1 about the same?",
              "score": 1,
              "created_utc": "2026-02-04 06:08:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3htb98",
          "author": "k_means_clusterfuck",
          "text": "long long man!",
          "score": 1,
          "created_utc": "2026-02-04 07:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3idxw5",
          "author": "lumos675",
          "text": "I have 32gb vram only ðŸ˜­",
          "score": 1,
          "created_utc": "2026-02-04 11:02:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iemxs",
          "author": "DOAMOD",
          "text": "A bug in one function was fixed and it was working correctly, it looks promising and maintains a speed of 35/40tg 128k",
          "score": 1,
          "created_utc": "2026-02-04 11:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ifq8f",
          "author": "Wrong_Library_8857",
          "text": "tbh I'm curious if the jump from 2.5 to 3 is actually noticeable for local use or if it's mostly benchmark optimization. Anyone run it yet on something practical like refactoring or multi-file edits?",
          "score": 1,
          "created_utc": "2026-02-04 11:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3inyql",
          "author": "Ideabile",
          "text": "Does anybody know if this can run on a Macbook Pro M2 Max 64GB?",
          "score": 1,
          "created_utc": "2026-02-04 12:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0fyn",
          "author": "laterbreh",
          "text": "FP8 version tensor parallel in vllm nightly on 2 rtx pros on a simple \"build single landing page in html for <insert subject>\" spit out 170 tokens per second.",
          "score": 1,
          "created_utc": "2026-02-04 16:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3npc88",
          "author": "Clear_Lead4099",
          "text": "This model is not good. At least for me. I use LLMs to help me code in Dart, and this turd couldn't write a simple app of bouncing ball I asked it to do. Used their recommended parameters for llama.cpp. I gave up after my 4th corrective prompt. The speed is good, yes, but who cares about speed when model is fucking dumb?! In contrast: GLM 4.6/7 and Minimax M2.1 nailed it in 1-2 prompts.",
          "score": 1,
          "created_utc": "2026-02-05 03:52:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gf7oe",
          "author": "pravbk100",
          "text": "Seems to have knowledge till june 2024. I tried it on huggingface about latest versions and here are the replies:\n\n1. Swift :Â As ofÂ June 2024, theÂ latest stable version of the Swift programming language isÂ 5.10.\n\n2. React native :Â As ofÂ June 2024, theÂ latest stable version of React Native isÂ 0.74.1, released onÂ June 13, 2024.\n\n3. Python :Â As ofÂ June 2024, theÂ latest stable version of Python isÂ 3.12.3, released onÂ June 3, 2024.",
          "score": 1,
          "created_utc": "2026-02-04 02:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjgn4",
          "author": "Old-Nobody-2010",
          "text": "How much **VRAM** do I need to run **Qwen-Code-Next** so I can use **OpenCode** to help me write code",
          "score": 0,
          "created_utc": "2026-02-04 02:27:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrsy4q",
      "title": "How close are open-weight models to \"SOTA\"? My honest take as of today, benchmarks be damned.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/k38sg20q7mgg1.png",
      "author": "ForsookComparison",
      "created_utc": "2026-01-31 04:49:42",
      "score": 629,
      "num_comments": 216,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2sgve9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 14:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs2xu",
          "author": "Bananadite",
          "text": "Honestly so disappointed in Meta.  They were the ones who kickstarted this open source weights and they spent so much to assemble an AI \"superteam\" yet they failed so hard.",
          "score": 196,
          "created_utc": "2026-01-31 05:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qv8cd",
              "author": "ForsookComparison",
              "text": "I'll be devil's advocate for Llama4:\n\nthere was a brief moment where Llama4 Maverick through Lambda Labs was the cheapest and fastest way to do code completions (think: the \"Continue\" VSCode Extension or early Copilot). If you already knew what you were going to hand-write Maverick could probably nail it faster and cheaper than anything else.\n\nThis didn't last long at all.. but Maverick got some actual use from me before Qwen3.",
              "score": 70,
              "created_utc": "2026-01-31 06:00:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2w0p9h",
                  "author": "dreamkast06",
                  "text": "I too will advocate for it. It was faster and had more \"knowledge\" than the og Deepseek 3. It had implementation issues when it first came out like GPT-OSS-120B. Remember all the hate that got when it first came out cuz it seemed stupid until the templates were fixed?\n\nMakes me wonder if the multimodal aspects of Llama4 were more widely implemented maybe it'd gotten more use.",
                  "score": 5,
                  "created_utc": "2026-02-01 00:49:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tzwa2",
              "author": "RedTheRobot",
              "text": "The open source play was never to benefit them but to hurt the competition. That is why at the beginning they didnâ€™t invest heavily in AI. Zuck was still in his metaverse and pushing the Oculus. AI was just a side project. Then AI started making some serious money and meta had to jump in at the last second. They threw out huge paydays to again bump themselves up and hurt the competition but the good employees are heavily vested in OpenAI and Google.",
              "score": 9,
              "created_utc": "2026-01-31 18:38:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wqa1u",
                  "author": "iJeff",
                  "text": "Yann LeCun was the one advocating for the open approach.",
                  "score": 5,
                  "created_utc": "2026-02-01 03:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rfoi1",
              "author": "Nyghtbynger",
              "text": "Yann Lecun really is in the shadow of all the big changes in Artificial Neural Networks for 30 years now",
              "score": 16,
              "created_utc": "2026-01-31 09:04:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s4cdl",
                  "author": "aeroumbria",
                  "text": "He felt like an LLM sceptic stuck in a company that never cared about what he really wanted to do.",
                  "score": 10,
                  "created_utc": "2026-01-31 12:45:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tlsu6",
                  "author": "rabbotz",
                  "text": "I suspect both Meta and OpenAI, in their own ways, got too distracted with the AGI â€œwhatâ€™s nextâ€ question. I get where they are coming from, the current architectures will eventually hit a wall we will really want to get past (eg from the lack of a world model). But thereâ€™s so much juice to be squeezed on iterating on current models and the ecosystems around them, it ended up looking like a distraction in retrospect.",
                  "score": 7,
                  "created_utc": "2026-01-31 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ue5fk",
              "author": "kevin_1994",
              "text": "scout was not a bad model, imo. it was just released at the wrong time. it released when MoEs, reasoning models, and agentic behaviour were first beginning to emerge. it's MoE structure was more \"old-school\" like Mixtral, it didn't have reasoning, and it wasn't post trained for agentic behaviour.\n\ndespite all this, the llama models are some of the only models not trained to benchmaxx. meta still serves their llama models to billions of users. my boomer parents love that shit lol. these models not are optimized for coding and stem -- they are optimized for user engagement for more casual users",
              "score": 3,
              "created_utc": "2026-01-31 19:46:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o302ub0",
                  "author": "Zulfiqaar",
                  "text": "I really really wish they opensourced the LLaMa4 checkpoint that beat gpt-4o in user preference. but instead they decided to post-train it on STEM and underperform in all domains.",
                  "score": 1,
                  "created_utc": "2026-02-01 17:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2w1xyd",
              "author": "midnitewarrior",
              "text": "I forgot Meta was even in this. oof",
              "score": 2,
              "created_utc": "2026-02-01 00:56:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b2vsj",
              "author": "samxli",
              "text": "And to imagine the namesake of this sub was inspired by a Meta model.",
              "score": 1,
              "created_utc": "2026-02-03 07:58:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sjtbp",
              "author": "Party_Progress7905",
              "text": "They should just give up and invest in others. Theyâ€™re only wasting resources, too far behind, and theyâ€™re actively enshittifying their own products by forcing their shit AI into everything. Given Metaâ€™s track record, itâ€™s **likely** WhatsApp and ig  messages are being repurposed beyond messaging.",
              "score": -3,
              "created_utc": "2026-01-31 14:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qzyuy",
          "author": "LoveMind_AI",
          "text": "For whatever it's worth, Kimi K2.5 is performing basically as well as Gemini 3 Flash for me on visual reasoning.",
          "score": 54,
          "created_utc": "2026-01-31 06:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rq733",
              "author": "SilentLennie",
              "text": "And it's cheaper (but Gemini 3 Flash has that 1M context window which is useful at times of course).",
              "score": 10,
              "created_utc": "2026-01-31 10:45:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ytbw8",
                  "author": "cleverusernametry",
                  "text": "Have you ever used all of it's 1M context? Does it maintain response quality?",
                  "score": 1,
                  "created_utc": "2026-02-01 13:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2trpiz",
              "author": "SnooDoggos9325",
              "text": "At coding not so much. I asked k2.5 with Opencode to implement a feature. It did quite well, but there was a glitch. I asked for a fix and it made things worse. I reverted the last fix and asked Gemini 3 flash to fix it. It immediately figured out the problem and provided a proper solution.Â \nThe feature was to add a slightly transparent imgui overlay with fps and frame time percentiles in the top right corner. C++ and sdl3 GPU.\nThe glitch was with incorrect texture formats causing the overlay to be 90% transparent.Â \nAsking Kimi to fix it (twice) resulted in a hallucination that now it is fully opaque and later extending the background to full screen instead of jus the overlay window.",
              "score": 6,
              "created_utc": "2026-01-31 18:00:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2y5ji2",
                  "author": "MrBIMC",
                  "text": "In my testing, all of the above listed models that I tried, these except of claude keep on making a stupid mistake that kills them:\n\nwhen working with realtime data streams, they tend to forget to add a timeout, and so in cases where stream only returns the empty data or a data in pattern that the agent doesn't expect. \n\nSo that a model gets stuck endlessly watching some stream of data. \n\nIt might be an orchestrator problem, but I tested on OpenCode, Kilo and antigravity agent, and no matter the agent, most of models behave the same in this case. Claude never misses that he needs to observe with timeout, gemini misses most of the time, but not always and other models (except of giga potato) almost never did add a timeout. \n\n\n\n\nThough, in my personal vibe, k2.5 does feel the most like 3 flash, and 3 flash is my favorite model to work with at the moment for most of the tasks.",
                  "score": 2,
                  "created_utc": "2026-02-01 10:18:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zg9bo",
                  "author": "Evening_Ad6637",
                  "text": "Yes, unfortunately Kimi isn't very good at C++. I recommend Deepseek or GLM for that.",
                  "score": 2,
                  "created_utc": "2026-02-01 15:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2quac7",
          "author": "randombsname1",
          "text": "Good list. Largely agree.\n\nFirst big release of SOTA models for this year is around the corner too. \n\nYou have leaks of ChatGPT 5.3. Claude Sonnet 4.7 and Gemini 3.5.\n\n\nI think each of the big 3 is waiting to see who goes first. Then they'll all release within a week or 2 of the first one, imo.",
          "score": 133,
          "created_utc": "2026-01-31 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qwvdc",
              "author": "Mescallan",
              "text": "OpenAI certainly is. Google hasn't really been known to stomp on releases as much as the other two. Anthropic has been doing it more recently, but only towards OpenAI IIRC.   \n  \nMy bet is Gemma 4 -> Sonnet 4.7 -> GPT 5.3 within 48 hours -> Gemini 3.5/Opus 4.7 in March.",
              "score": 46,
              "created_utc": "2026-01-31 06:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tm6wz",
                  "author": "segmond",
                  "text": "Google has no pressure to release gemma-4, llama drove gemma.  with llama dead and phi not releasing.   Google isn't threatened by mistral-small or qwen3-30b/32b.  They have demonstrated that they can do opensource, the investors don't care.    There's a reason Anthropic doesn't do open source, nothing in it for them.  The only reason OpenAI did gpt-oss is because of the Elon Musk lawsuit so they can claim they are still open.",
                  "score": 22,
                  "created_utc": "2026-01-31 17:33:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qys8j",
                  "author": "kkb294",
                  "text": "Are you sure about Gemma.? I'm eagerly waiting for the OSS ones as there is nothing in sight from the other(Claude/OpenAI/Meta) providers.",
                  "score": 5,
                  "created_utc": "2026-01-31 06:29:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rhi73",
              "author": "__Maximum__",
              "text": "Sir, this is a r/localllama",
              "score": 23,
              "created_utc": "2026-01-31 09:21:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rxxax",
              "author": "goniszewski",
              "text": "Yeah, nice list. Actually this could be a nice addition for each occurrence I am adding to this list: https://ithappenedagain.fyi/rec/new-sota-model-released-211624384e\n\nIâ€™m thinking about splitting it also into closed models and the open ones, which we can host ourselves. Could be useful for some folks out there.",
              "score": 3,
              "created_utc": "2026-01-31 11:54:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qrpbi",
          "author": "TheRealMasonMac",
          "text": "More or less, yeah. I'd say that OSS models need to invest compute in better instruction following rather than intelligence. It clearly paid dividends for Claude.",
          "score": 87,
          "created_utc": "2026-01-31 05:32:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r2sgq",
              "author": "ihexx",
              "text": "Idk what Claude's secret sauce is, but it's not instruction following.\nIdk if you remember but a big complaint everyone had with Claude is it doesn't do what you ask it to and it's too eager to do extra stuff.\nIf you try to measure instruction following, Claude scores badly; bottom of the big 3.\n\n\nThere's something else here that's giving Claude that advantage; something more like \"meta-problem understanding\" or like \"inferring missing information\". But it's not \"instruction following\"",
              "score": 60,
              "created_utc": "2026-01-31 07:04:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r4cqx",
                  "author": "kzoltan",
                  "text": "Guessing only: huge amounts of data from devs? Claude is arguably the best model for devs, so people tend to choose that. \nI havenâ€™t seen the fine print, can they use the data from subscriptions for training purposes?",
                  "score": 26,
                  "created_utc": "2026-01-31 07:18:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2r509j",
                  "author": "TheRealMasonMac",
                  "text": "Idk, for me it's always been the best at following instructions for coding. Whereas other models try to do things cleverly, it chooses the most straightforward solution that satisfies my requirements. Other models try to be clever and consequently fail to follow instructions because they're so laser-focused on being clever. If I tell it not to do something, it gets what I'm really telling it not to do without overreaching, and so on. It's also able to maintain instruction following coherency across dozens of turns whereas other models fall apart completely in just a few.",
                  "score": 12,
                  "created_utc": "2026-01-31 07:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rwtxd",
                  "author": "AuspiciousApple",
                  "text": "Yeah, claude occasionally feels like a smart person running with your task. Sometimes I give it a complex problem and it does 20 steps before stopping, when I wanted 1-2 steps.",
                  "score": 5,
                  "created_utc": "2026-01-31 11:45:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2s9d7a",
                  "author": "AlwaysLateToThaParty",
                  "text": "Claude is their workflows. That's what it feels like to me.  They seem to have embedded them in their reasoning, so that all tasks are broken down, but not too much. That lends itself to coding.\n\nI would say that this is code-centric.  While code is good, there are other ways to use llms and 'ai' (like image/video, role play, translation, domain specific analysis, etc etc), that will have a different tier list entirely.",
                  "score": 4,
                  "created_utc": "2026-01-31 13:19:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rd0s2",
                  "author": "Any_Fox5126",
                  "text": "I don't know about recently, but around the time of 3.5, it also used to be one of the most aggressively censored.",
                  "score": 2,
                  "created_utc": "2026-01-31 08:39:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2shsvz",
                  "author": "Far-Low-4705",
                  "text": "In my experience, i feel maybe older, dense models were able to understand/grasp context better. I think pretraining baked it into the model, then post training RL kind of beat that out of them (or at least made them worse at the ability since it is no longer the primary learning signal)",
                  "score": 2,
                  "created_utc": "2026-01-31 14:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tk2ad",
                  "author": "kaisurniwurer",
                  "text": "I'm ~69% sure that they are using massive activation moe.\n\nI would not be surprised to hear that their model have 100-400B active tokens. If it isn't just straight up massive dense model.",
                  "score": 2,
                  "created_utc": "2026-01-31 17:23:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ttwv2",
                  "author": "jklre",
                  "text": "My favorite behavior of Claude is when im working on a new complex problem it basically gets to a point when things are hard and its just like \"How about you just don't?\" and tries to get out of it",
                  "score": 1,
                  "created_utc": "2026-01-31 18:10:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2umeto",
                  "author": "BadgKat",
                  "text": "My honest opinion is that Claude is so good for two main reasons. \n\nThe first is harnessing, I think this is the main reason there is a benchmarking to perceived performance gap. Anthropic has built some of the best tooling, both in Claude Code and built in tooling in the browser/app. Models performance is improved when they have clear ways to do things, like create a script for themselves and run it in a sandbox and give the user an output document. The benchmarks use the api, you donâ€™t get those harnesses in the api.\n\nThe second one is a bit harder to prove, but I believe it. I think itâ€™s the SOUL doc. The study that showed when you RL a model to write malicious code and it starts talking like a Nazi proves the inverse. It stands to reason you RL a model to be moral, it writes better quality code, and does other things well. Harder to empirically prove this inverse, but I think it at least follows.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:26:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zj9a8",
                  "author": "OcelotMadness",
                  "text": "It's because they're scanning and destroying real books en masse. Don't really agree with that practice, but I'm willing to bet its why Claude mimics English storytelling so well.",
                  "score": 1,
                  "created_utc": "2026-02-01 15:48:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34pzya",
                  "author": "ironicstatistic",
                  "text": "I think the truth of the matter is that Claude has the smallest userbase of the big 3, so can bother spending more compute per prompt. \n\nnormies wont care, they will use what they are familiar with/what is cheaper.  \nThe pros will, they will write better prompts and give claude better data - so they become more premiere. \n\nThink of the amount of slop Open AI have to STOP their models from training on, and how hard it must be not to cause that to creep in. Think about how many more users OpenAI and Gemini have compared to Claude. \n\nOr maybe its secret sauce idk",
                  "score": 1,
                  "created_utc": "2026-02-02 09:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2s5xf7",
              "author": "aeroumbria",
              "text": "I would say maybe we should treat some diversity in \"obedience\" as a strength rather than deficiency. For instance, DeepSeek V3.2 is not particularly good at following instructions because it doubts itself too much and will spend too much time debating how to interpret the instructions, but this also makes it ideal for cross-checking results from other models, just because the \"mental model\" of DeepSeek is so distinct from other models, and it does not take even its own reasoning for granted.",
              "score": 2,
              "created_utc": "2026-01-31 12:56:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rqw66",
              "author": "hellomistershifty",
              "text": "That's what gets me with Gemini, I don't get how it scores so high. It programs great solutions to problems that didn't exist and uses tools like a caveman",
              "score": 2,
              "created_utc": "2026-01-31 10:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tfd2m",
              "author": "RhubarbSimilar1683",
              "text": "Claude definitely takes more compute to run. It all adds up: it's slower. Has lower free rate limits. Uses custom Annapurna labs AWS hardware. Has higher API pricing. Maybe the model has a diffusion LLM component while being largely autoregressive. They could then train RL on that.Â  Or the transformer architecture has a symbolic ai component and/or a diffusion component because it sometimes adjusts its text generation speed. Then train RL on that.Â \n\n\nThe custom AWS hardware has somewhat mitigated the speed issue but it doesn't add up to the higher speed of the hardware.Â  But it could also be them trying to be profitable. But I doubt it because otherwise they would nuke their models like openai with gpt-5\n\n\nÂ I still remember they had those issues during their Nvidia days in 2024 and I don't think that's changed. I don't think those things can be fixed with better infrastructure. They need hardware.Â ",
              "score": 1,
              "created_utc": "2026-01-31 17:00:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tnubx",
              "author": "CuriouslyCultured",
              "text": "I don't feel like Claude is very good at instruction following. It's superpower is taking under specified problems and producing \"good enough\" output. GPT5.2 is anal about requirements, whereas Claude treats them as suggestions, and cares more about \"flair\"",
              "score": 1,
              "created_utc": "2026-01-31 17:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qt1jk",
          "author": "TomLucidor",
          "text": "You need to make another tier list for the <120B model and <48B models when compared to proprietary SOTA's smaller equivalents.",
          "score": 51,
          "created_utc": "2026-01-31 05:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qt7yf",
              "author": "ForsookComparison",
              "text": "Will do. I have a totally separate set of projects that are hosted on my janky home server, so I've got plenty of hot takes for models of that size.",
              "score": 17,
              "created_utc": "2026-01-31 05:44:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rtl1i",
                  "author": "Competitive_Ad_5515",
                  "text": "!remindme 1 week",
                  "score": 2,
                  "created_utc": "2026-01-31 11:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rxi6l",
              "author": "goniszewski",
              "text": "We should probably distinguish between SOTA (closed), and Open SOTA (open source or open weights). The latter maybe with a reference from the first one. \nIâ€™m always researching new candidate for my locally run model and that would be very helpful.",
              "score": 2,
              "created_utc": "2026-01-31 11:50:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rf50v",
          "author": "LosEagle",
          "text": "Mr. anus logo is the overlord, sadly.Â ",
          "score": 16,
          "created_utc": "2026-01-31 08:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qolio",
          "author": "ThatRandomJew7",
          "text": "Have you tried Kimi K2.5 yet? Because it's a pretty big step up, in my experience it outperforms Claude Sonnet and sometimes trades blows with Opus",
          "score": 55,
          "created_utc": "2026-01-31 05:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qoyur",
              "author": "ForsookComparison",
              "text": "I have. I hold Deepseek V3.2 in very high standing so take my word that it's a big deal that Kimi sits where it is on this chart.\n\nBut I'm confident on where I put it.",
              "score": 29,
              "created_utc": "2026-01-31 05:11:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qpizl",
                  "author": "ThatRandomJew7",
                  "text": "Fair. I haven't used Deepseek much since R1, the lack of multimodal really hurts it for general use.\n\nIdk, in my experience I've found Kimi on par with Gemini 3 Pro at least. My usual \"weird benchmark that they're definitely not training on' is telling it to make a Pico-8 \"demake\" of my favorite UFO50 game. Only Kimi and Gemini could actually produce something that didn't instantly error, and Kimi was more graphical (though it struggled with sprites). Even Sonnet couldn't do it. Haven't tried Opus though since I'm not paying for Claude at this point",
                  "score": 17,
                  "created_utc": "2026-01-31 05:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2sz1jn",
              "author": "IsometricRain",
              "text": "What can it do better than sonnet? Curious what your use case is.\n\nI'm thinking of testing it out but Sonnet has been very solid for me, with GLM being good enough for smaller tasks and quick research",
              "score": 2,
              "created_utc": "2026-01-31 15:42:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qmrph",
          "author": "ForsookComparison",
          "text": "Tiers are ranked.\n\nGrok takes into account Grok-4.1 and Grok-Coder-1-Fast, not the opened Grok-2 weights.\n\nQwen is based on my experiences with Qwen3-235B mostly and the 'goodness' of the ease of hosting a model of that power. Closed-Weight Qwen3-Max would likely rank similarly despite being a hair better.\n\nGemini 3 Pro will beat ChatGPT on some days, but GPT-5.2-Pro (or 'extra' or whatever it's called nowadays) will beat it if you can wait.\n\nEverything left out isn't meant as bad (for example: I use Nemotron Nano near-daily) they've just never been SOTA competitors in their size-class to me, so they don't come up in any SOTA thoughts I'll have.\n\nGLM's placement I think can stretch all the way from 4.5-air and 4.6v to the full 4.7. It does so much better than those beneath it in agentic work, but once it's time to make decisions the gaps between it at the tier above it show.\n\nI love Llama and use Llama 3.3 70B up until very recently when Qwen3-VL-32B, Seed-OSS-36B, and Nemotron-Super-49b-v1.5 started to show knowledge-depth that finally kicked it off my hard drive.\n\nI don't have many vision or ocr use-cases in any of my work/projects so those capabilities weren't taken into account.",
          "score": 32,
          "created_utc": "2026-01-31 04:55:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "o38fs71",
              "author": "raucousbasilisk",
              "text": "Iâ€™d recommend trying Devstral 2. Neat little model.",
              "score": 1,
              "created_utc": "2026-02-02 21:59:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2r75uo",
              "author": "wouldacouldashoulda",
              "text": "Honest question, what do you do that you need so many tokens that you would consider anything but just the big 3?",
              "score": -1,
              "created_utc": "2026-01-31 07:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uifr7",
                  "author": "ForsookComparison",
                  "text": "Try coding a large side project spanning several repos when you're not rich - then try and have those side projects require LLMs as a piece of the pipelines.\n\nYou need to get crafty and spend hours with everything to tune performance-per-dollar and time-vs-output",
                  "score": 5,
                  "created_utc": "2026-01-31 20:07:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2s4xot",
                  "author": "fractalcrust",
                  "text": "gooning",
                  "score": 3,
                  "created_utc": "2026-01-31 12:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ri2xg",
          "author": "nycigo",
          "text": "Devstral 2 is pretty good too",
          "score": 12,
          "created_utc": "2026-01-31 09:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdgvy",
          "author": "k_means_clusterfuck",
          "text": "If this is a tier list for agentic coding, you'd be crazy to place Minimax and ziphu that far down",
          "score": 6,
          "created_utc": "2026-01-31 08:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u5e57",
              "author": "Vaptor-",
              "text": "I use minimax for daily with claude code. With good mcp (serena, context7), proper context management, and subagents it's really good. It also cost like $10 per mo and I never hit 50% of the 5 hours limit.",
              "score": 2,
              "created_utc": "2026-01-31 19:04:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s4qri",
          "author": "gnaarw",
          "text": "I've used kimi 2.5 for a semi large go project this week and it seems better than Sonnet and gippity\n\nAs for open weight models we can actually run at home... GLM fast, devstral 2, OSS and qwen are still quite a way off. Minimax, GLM and Kimi (cloud hosted which is ok for most European projects for example) are actually on par with a good junior on crack vs the smaller models acting like a junior who's hand you have to hold while crossing the street...",
          "score": 5,
          "created_utc": "2026-01-31 12:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzsia",
          "author": "cyberdork",
          "text": "This ranking is just as useless as the benchmarks, since you don't mention anything about the use case.     \n    \nI'm so tired about people talking how model XYZ is the best, just to read somewhere in the comments that they mean for coding. The vast majority of people DON'T vibe code and have totally different use cases.",
          "score": 19,
          "created_utc": "2026-01-31 12:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s57h0",
              "author": "aeroumbria",
              "text": "Yeah, I don't believe a second that even on coding, there exists one model that does \"everything\" better than the next model.",
              "score": 4,
              "created_utc": "2026-01-31 12:51:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qxbxo",
          "author": "gcavalcante8808",
          "text": "Opus nearly reconstructed a svg with perfection based on a complex banner... for this case even the other closed models didn't even get closer.\n\nFor code: python, rust, DDD, clean architecture it's all about the same; good enough, the open models works wonderfully for a long time now. \n\nSo the answer depends on the task.",
          "score": 4,
          "created_utc": "2026-01-31 06:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s69q2",
          "author": "True_Requirement_891",
          "text": "I've seen minimax do very surprising things. It should be in the SOTA territory.",
          "score": 4,
          "created_utc": "2026-01-31 12:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rihex",
          "author": "raiffuvar",
          "text": "ðŸ¤£ ðŸ¤£ \nDeepseek \"feels like early 2025 sota\". \nThe model which liturally make other to continue with improving.",
          "score": 6,
          "created_utc": "2026-01-31 09:31:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rn05q",
              "author": "Kakami1448",
              "text": "Back in early 2025, so no lies here",
              "score": 6,
              "created_utc": "2026-01-31 10:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rgv1g",
          "author": "nullmove",
          "text": "Where I disagree with this:\n\n- GPT-5.2(-codex) > Slopus 4.5, these are top 2 in on their own\n- Gemini has unparalleled knowledge, but for coding it shares third place with Kimi K2.5 which is really impressive\n- I don't find Minimax-M2.1 to be worse than GLM-4.7 overall, though both have their ups and downs that don't overlap",
          "score": 8,
          "created_utc": "2026-01-31 09:15:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5p1n",
              "author": "-InformalBanana-",
              "text": "Minimax M2.1 is really amazing, significantly smaller than others while still very competitive.",
              "score": 1,
              "created_utc": "2026-01-31 16:14:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tak0o",
                  "author": "nullmove",
                  "text": "Yep I am writing Elixir and this thing flies in opencode. Surely difference exists with Sonnet but for a lot of things I can't tell at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qyshj",
          "author": "Zeeplankton",
          "text": "unfortunately claude / opus is more like\n\nhttps://preview.redd.it/bz2atqnwpmgg1.jpeg?width=1438&format=pjpg&auto=webp&s=eaef64aa0a3062b5d2d62803b7e0fc2fea022507",
          "score": 6,
          "created_utc": "2026-01-31 06:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rs0kx",
              "author": "hellomistershifty",
              "text": "I don't think about them at all... until that \"usage cost limit exceeded\" notification rolls in",
              "score": 3,
              "created_utc": "2026-01-31 11:02:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rzeah",
          "author": "-dysangel-",
          "text": "GLM Coding Plan with Claude Code is feeilng just as good or better as the latter days of Claude 4.1 for me. I've been using it daily for work and it's working very well. Organised, smart, helpful, relentless :p",
          "score": 3,
          "created_utc": "2026-01-31 12:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4xqn",
          "author": "Admirable-Choice9727",
          "text": "Meta really went from 'saving open source' to 'the team that peaked at 405B' while Deepseek and Qwen just kept shipping. Itâ€™s wild that weâ€™re at a point where Iâ€™d rather trust a hypothetical GLM-5 leak than a Meta announcement. The 'Maverick' release felt like they were just trying to stay relevant while the frontier moved past them.",
          "score": 3,
          "created_utc": "2026-01-31 12:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sihpc",
          "author": "RedParaglider",
          "text": "What I've found for dev work SERIOUS dev work I always get better results out of codex 5.2 max.  It is much slower, and forces me to use my brain a lot more through the entire thing, but the end result is always cleaner and more professional.  \n\nIt's not NEARLY as fun to use as opus though.",
          "score": 3,
          "created_utc": "2026-01-31 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ss9qa",
          "author": "SourceCodeplz",
          "text": "Gpt-5.2-Codex #1",
          "score": 3,
          "created_utc": "2026-01-31 15:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2swut8",
          "author": "Michaeli_Starky",
          "text": "Gemini in the current state has no right being in the list at all\n\nhttps://preview.redd.it/05pqcgioepgg1.png?width=1730&format=png&auto=webp&s=73cfe9a96b437482f0834fb76b254d93543e18cc",
          "score": 3,
          "created_utc": "2026-01-31 15:31:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wc5jq",
              "author": "spawncampinitiated",
              "text": "You get down voted for stating facts here",
              "score": 3,
              "created_utc": "2026-02-01 01:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ugr7z",
          "author": "timschwartz",
          "text": "Can someone explain the Ralph Wiggum thing?",
          "score": 3,
          "created_utc": "2026-01-31 19:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uhhme",
              "author": "ForsookComparison",
              "text": "Ralph Wiggum is a popular Simpsons character. He became a popular internet meme for many years because of a famous scene from the show where he's smiling in the back of a bus saying *\"I'm in danger\"*\n\nAnthropic actually published a way to use Claude-Code agentically with no concern for permissions or token usage that they labeled *\"Ralph Wiggum Mode\"*.\n\nHere, the one in danger is OP's job and the danger is Claude 4.5 Opus.",
              "score": 3,
              "created_utc": "2026-01-31 20:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r5tci",
          "author": "wanderer_4004",
          "text": "Well, my honest take, benchmarks and ClosedAI fanbois be damned, any open model blows the closed ones out of the water. Unless you want to live in a future where a handful of Elons, Samas and similar rule the world. The day that VCs stop subsidizing your $20 coding plans you'll have to turn around and bend over. Or maybe that is what you are looking forward to?",
          "score": 11,
          "created_utc": "2026-01-31 07:32:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rrwaj",
              "author": "hellomistershifty",
              "text": "you can already see /r/Bard in shambles that the Gemini API costs money (shocking)",
              "score": 7,
              "created_utc": "2026-01-31 11:01:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rdxr0",
              "author": "daniel-sousa-me",
              "text": "If you remove the R&D portion out of Anthropic's expenses, it is already wildly profitable\n\nVC money is used for R&D to make it even more profitable",
              "score": 8,
              "created_utc": "2026-01-31 08:48:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rfhon",
                  "author": "wanderer_4004",
                  "text": "For now, i.e. pre-IPO this is all guess work as the numbers are not public. That said, I think the API pricing at $25 / 1MTok is very likely to be nicely profitable. The $20 claude subscription is very likely not. At some point the VCs want to see a return. And with agentic coding you can easily burn 10MTok per hour. If you have to pay that at API prices, then for a lot of developers this will be a bad awakening into reality.\n\nThe big closed companies spend lots of money to make us believe that you absolutely need SOTA-frontier. Keep in mind the biggest threat to them and their investments of billions of $$$ is people figuring out that open models are good enough.\n\nRight now only \\~20M users out of 800M are paying for ChatGPT...",
                  "score": 2,
                  "created_utc": "2026-01-31 09:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qouee",
          "author": "yes-im-hiring-2025",
          "text": "I want to do an experiment for six months.\nTeaching via ragebaitâ„¢\n\nSame claude code setups (agent plugin + CLI), but with a twist:\n\n- some are on the claude max 200 USD plan [seniors]\n- some are on the codex pro plan [mid level]\n- some are on the GLM/minimax plan [junior]\n\nBasically the juniors get the worst AIs, and the seniors get the best. Have the juniors learn by ragebait until they match the performance of the tier above. Same with mid level engineers.\n\nMy senior level gang just ensures ralph wiggum keeps ralph wiggum-ing.\n\nWho wants to buy my agentic coding course?!?!?!",
          "score": 8,
          "created_utc": "2026-01-31 05:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp45i",
              "author": "ForsookComparison",
              "text": "I'd love this - but I'm more interested in juniors-with-SOTA vs seniors-with-minimax or something along those lines",
              "score": 8,
              "created_utc": "2026-01-31 05:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqhmv",
                  "author": "yes-im-hiring-2025",
                  "text": "Ohhh I can tell you what happens with that firsthand.\n\nI had my team running Opus via the claude code plan and the juniors just resorted to recorded meetings -> AI studio's gemini flash to get a PRD -> oneshotâ„¢ via Opus. Bear in mind these are college grads, so truly green around the ears.\n\nAlso how I found out that the meetings recorded are being sent to gemini violating who knows how many corporate conditions. If you're not familiar, the uploaded media on AI studio is first pushed to your own personal cloud and then linked from there. Not a \"temp\" upload, it uses your drive as the temp upload storage space - but never deletes anything once uploaded.\n\nZero debugging. Missed a feature and a deadline and I got on a call with them to understand how that could've possibly happened, it was debugging a simple API call response that was throwing up a random 4xx error.\n\nChecking for a response code 200 and logging everything else -> debugging with input and output loads -> sample logs for a batch input to filter and identify root cause. That's all they needed to do. Not even through experience, they could've asked an LLM how to debug a problem and follow the steps.\n\nNope. Not a single one of the three did that; coding muscles had atrophied.\n\nSince then I told management to lock up claude code access for juniors, and to only have it given to them after year 1. Till then they're on the [enterprise] basic free tier github's copilot plan only. Can't say it dramatically improved their skills but atleast they are learning how to Google and be honest, which is enough for now.\n\nI do believe most seniors SHOULD be fine with any GLM model if they can debug and write things independently, though. I myself switch between Antigravity (free opus4.5) for detailed planning then clean up/edit the plan, and get GLM4.7 to work on it. I've set up auto-review and linting, so usually major issues get caught before I even review the final code.",
                  "score": 17,
                  "created_utc": "2026-01-31 05:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpz4n",
          "author": "SilentLennie",
          "text": "I think there is an other detail: how smart the model is matters less and less, if it can complete the task is more important and at what price/how.\n\nLet's take coding:\n\nSure the top open weights models are close to early (I would put it later) 2025 smarts, but they are also much better at agentic than those models.\n\nAnd the harnesses (tools we use to make an agentic coder) are much better now than in the past, they include for example LSP even on the CLI and more and more people figured out 'plan mode' and the models can generate tests and test it and whole lot of other details...\n\nSo what does this mean ? They can actually finish a lot of tasks on their own (Kimi K2.5 being multi-modal matters a lot here too) and do it for a much lower cost or on your own computer if that's the reason you prefer. So if you compare them to early 2025 this is the wrong way to look at it. Not only that: if they can finish a task: it's far more often 'good enough', which is how for example DOS and Windows got popular (and network effect, monopoly abuse, etc. obviously), it wasn't perfect, but good enough to get stuff done.\n\nI think we are gonna be in a much more: multi-model world, where you might be using or example Kimi K2.5 a lot, but when you need to make a plan you pick Opus and when you need to digest a lot of data cheaply, you might pick Gemini 3 Flash because of it's 1M context window, etc.\n\nThe next problem is: you need to keep doing some coding to keep your coding muscle.",
          "score": 2,
          "created_utc": "2026-01-31 10:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s32za",
          "author": "ALittleBitEver",
          "text": "LLama Models are really good for tasks related to emotions, roleplaying, creative writing, etc. But yes, at programming, math, etc, I agree",
          "score": 2,
          "created_utc": "2026-01-31 12:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sh9jq",
          "author": "Far-Low-4705",
          "text": "i would bump up GPT-OSS up a level.\n\nIMHO, for it's architecture, given it's size, how sparse it is, it's speed, interleaved thinking, and more \"stable\" reasoning traces, it feels SOTA some of the time.\n\nFrom an engineering standpoint, it is one of the most sparse models, and it is very fast",
          "score": 2,
          "created_utc": "2026-01-31 14:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sy1vm",
          "author": "wagnerax",
          "text": "What about Mistral ?",
          "score": 2,
          "created_utc": "2026-01-31 15:37:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2szrk0",
              "author": "ForsookComparison",
              "text": "ðŸ˜",
              "score": 1,
              "created_utc": "2026-01-31 15:46:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t2w3d",
          "author": "GiveMeAegis",
          "text": "No Mistral?",
          "score": 2,
          "created_utc": "2026-01-31 16:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5fg0",
              "author": "ForsookComparison",
              "text": "Le sorry",
              "score": 2,
              "created_utc": "2026-01-31 16:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ta8vg",
          "author": "laterbreh",
          "text": "So what are the qualifying tasks to create this ranking?\n\nWrites my LARP smut the best? Best coding agent? Best at accomplish a specific task?",
          "score": 2,
          "created_utc": "2026-01-31 16:36:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2taus6",
              "author": "ForsookComparison",
              "text": "OP's vibes",
              "score": 1,
              "created_utc": "2026-01-31 16:39:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tw85d",
          "author": "Hoak-em",
          "text": "Disagree, at least with whatever gemini Google hosts with Antigravity. I think things get a bit murky when you start to consider that for the version of the model that most people get when it's still a \"SOTA\" model, it's nerfed in some way (i.e. issues with Opus models not performing like they did at launch, issues with Gemini models (in general)).\n\nI don't really like having a single ranking either, it's more like \"what is the model good at/what do I want to use it for.\"\n\nKimi K2.5: orchestrator/small-scale planner/designer\n\nOpus-4.5: Large-scale planner/implementer\n\nGemini 3 flash: codebase search/understanding\n\nGLM-4.7: wiggumwiggumwiggumwiggumwigguwmgiwigiwmgiwmgiw (your account was rate limited, please reduce concurrency)\n\nGPT-5.2-Codex: Fixes, refactors (alongside opus), helping me understand wtf the previous model wrote",
          "score": 2,
          "created_utc": "2026-01-31 18:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uqmnn",
          "author": "blackwell_tart",
          "text": "I wish youâ€™d use words instead of emblems. I have no idea what any of those graphics are supposed to represent. I guess I missed LLM marketing class.",
          "score": 2,
          "created_utc": "2026-01-31 20:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw647",
          "author": "ahabdev",
          "text": "Personally I was a fan fo Gemini 2.5 over Opus 3.7. I considered a much better coder.\n\nHowever, there's no comparison between Gemini 3 and Opus 4.5. Mainly because G3 is disastrous with any complex project which require long chats, constant code revisions and contentx awareness without a flaw. It's beyond horrible. So nowadays I wound't even put it in the list.\n\nAnd as a peasant with only a 5090  I never even tried to setup a big model. I don't have the need so far, but would be really great to abe able to do so eventually. That said, I consider people don't value as they should small models for many day to day cases.\n\nSuch a pity Meta failed as with most things they do these days.",
          "score": 4,
          "created_utc": "2026-01-31 06:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qt0rj",
          "author": "YouAreTheCornhole",
          "text": "I can't believe how spot on this is",
          "score": 3,
          "created_utc": "2026-01-31 05:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r59tp",
          "author": "Spitihnev",
          "text": "And mistral is the ralph wiggum?",
          "score": 2,
          "created_utc": "2026-01-31 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uio5t",
              "author": "ForsookComparison",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-31 20:08:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7812",
          "author": "NightlessBaron",
          "text": "Also check out Mbzuaiâ€™s K2 think V2 and Nvidia Nemotronâ€¦ both are dark horses imo",
          "score": 1,
          "created_utc": "2026-01-31 07:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sbqsd",
          "author": "Charuru",
          "text": "This is right on just chat performance, but you're underestimating agentic for K2.5.",
          "score": 1,
          "created_utc": "2026-01-31 13:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sdbg7",
          "author": "LocoMod",
          "text": "gpt-5-xhigh runs circles around Opus. Change my mind.",
          "score": 1,
          "created_utc": "2026-01-31 13:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2spnun",
              "author": "Both-Ad3646",
              "text": "Agreed, and Gemini is complete trash in comparison to both. I have no idea why it's ever been in the same discussion as any of the GPT's from 4.x and above.",
              "score": 2,
              "created_utc": "2026-01-31 14:54:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sjtsn",
          "author": "sine120",
          "text": "I haven't been able to play with Codex or CC much, but from what I hear of others that do, this is largely true. My company pays for Gemini CLI and I swear I might be the only person using that over the other labs. It's bad at instruction following, but takes less wrangling to get it to do what I want over models I can run. Prices would have to go up a lot to bother pushing me to pitch we run local models for anything other than ITAR code.",
          "score": 1,
          "created_utc": "2026-01-31 14:22:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sl39o",
          "author": "teachersecret",
          "text": "How close is local to SOTA? I'd say we're there. K2 2.5 is definitely SOTA level intelligence up near the highest levels we have available.\n\nObviously that's a bit difficult to run in your house, though. Most of us aren't running Kimi K2 at home.\n\nFor TRULY local... I usually think enthusiast-class (24gb vram). The only thing in that range that comes close is GLM 4.7 Flash. It feels like somebody took last-gen Claude Sonnet and gave it better tool handling. It's fantastic and not far off SOTA despite its small size.",
          "score": 1,
          "created_utc": "2026-01-31 14:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2svnz1",
          "author": "TwistStrict9811",
          "text": "Codex better than claude",
          "score": 1,
          "created_utc": "2026-01-31 15:25:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t9wit",
          "author": "RhubarbSimilar1683",
          "text": "For coding ofc\n\n\nWe all know that claude is over fitted for web coding",
          "score": 1,
          "created_utc": "2026-01-31 16:34:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tq8hs",
          "author": "Qual_",
          "text": "I found codex 5.2 way more reliable than claude on large codebase. There is always something to fix after claude, while codex just produce working code ( which sometimes feels black magic when it's after 50min of writing thousand of lines )",
          "score": 1,
          "created_utc": "2026-01-31 17:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u6iro",
          "author": "TurnUpThe4D3D3D3",
          "text": "This feels accurate, although I might put deepseek 1 tier lowers",
          "score": 1,
          "created_utc": "2026-01-31 19:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ud0ll",
          "author": "bartskol",
          "text": "Grok was never SOTA?",
          "score": 1,
          "created_utc": "2026-01-31 19:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ueakb",
              "author": "ForsookComparison",
              "text": "It's realtime data and web tools are. Probably number one for querying events from that day.\n\nThe base model itself when used over API falls a little short of Gpt5.2 and Gemini3 Pro. It feels more in line with Gemini 2.5 Pro or Sonnet 3.7 maybe, hence the rating of \"early 2025 SOTA\"",
              "score": 1,
              "created_utc": "2026-01-31 19:46:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uffgm",
                  "author": "bartskol",
                  "text": "It feels more like a toy",
                  "score": 1,
                  "created_utc": "2026-01-31 19:52:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2v4t7w",
          "author": "Odd_Candle",
          "text": "wtf is sota",
          "score": 1,
          "created_utc": "2026-01-31 21:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vo6gn",
              "author": "checksinthemail",
              "text": "State Of The Art",
              "score": 3,
              "created_utc": "2026-01-31 23:39:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2x069s",
                  "author": "Odd_Candle",
                  "text": "Thanks",
                  "score": 2,
                  "created_utc": "2026-02-01 04:28:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2vbw2h",
          "author": "Imperator_Basileus",
          "text": "I donâ€™t know, maybe GPT-5.2 is fine for coding, but for anything else Iâ€™d much, much rather use DeepSeek, GLM, or Kimi. I think Gemini 3 Pro is the best one to discuss anything with or bounce ideas off of as it takes instructions well and doesnâ€™t moralise or get distracted with safetyslop.Â \n\nBut since google heavily cut AI studio rate limits and the Gemini app and subscription is terrible, I wouldnâ€™t be using that either. Hence, trying out the new Kimi 2.5 lately. But GPT-5.2 is truly horrendous. I was trying to discuss economics, my field, and harshly critiqued some economist only for it be hyper condescending about â€œletâ€™s set that asideâ€, â€œIâ€™m gonna draw a firm lineâ€, and â€œI wonâ€™t endorse harsh languageâ€. Okay, go and fuck yourself. Uninstalled the app soon after.Â ",
          "score": 1,
          "created_utc": "2026-01-31 22:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wqq67",
          "author": "MiddleCricket3179",
          "text": "Kimi 2.5 for agentic coding and tasks has proved more reliable for me than Gemini 3 Pro",
          "score": 1,
          "created_utc": "2026-02-01 03:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wt57m",
          "author": "Routine_Temporary661",
          "text": "Gemini? You gotta be kidding me bro",
          "score": 1,
          "created_utc": "2026-02-01 03:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wy6hx",
              "author": "ForsookComparison",
              "text": "3 Pro is pretty decent",
              "score": 1,
              "created_utc": "2026-02-01 04:14:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ys79s",
                  "author": "Routine_Temporary661",
                  "text": "You wont say so if you have actually tried to ship sophisticated production code with it before",
                  "score": 1,
                  "created_utc": "2026-02-01 13:22:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xezoc",
          "author": "Intelligent_Heat_527",
          "text": "In coding in cursor agent mode I found Gpt 5.2 Thinking > Claude Opus 4.5 > Google Gemini 3 Flash",
          "score": 1,
          "created_utc": "2026-02-01 06:17:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xxr1m",
          "author": "SpecialNothingness",
          "text": "I like your assessment of MS Copilot.",
          "score": 1,
          "created_utc": "2026-02-01 09:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yjz9s",
          "author": "LienniTa",
          "text": "deepseek too high, qwen max too low(i assume you compared open weights ones, not qwen max), and i would honestly add xiaomi somewhere around minimax or even higher(?) but overall agree - kimi cooked big time",
          "score": 1,
          "created_utc": "2026-02-01 12:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yp1w8",
          "author": "extra2AB",
          "text": "I will be honest, put Gemini up along with claude, put Kimi to SOTA.\n\nChatGPT has lost it for me.\n\nFrom occasionally solving coding and other problems for me it has gone to rarely solving any problems.\n\nand Gemini 3 Pro has about 90%+ success rate for me. and that 10% is mostly my fault for not providing it proper context about my files, system details, etc\n\nI have gone from using Chatgpt almost everytime to using it  once every 2-3 weeks and that too mostly to write a formal application, email, letter, etc",
          "score": 1,
          "created_utc": "2026-02-01 13:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yt54w",
          "author": "cleverusernametry",
          "text": "You don't seem to understand what sota means",
          "score": 1,
          "created_utc": "2026-02-01 13:28:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yvvpz",
          "author": "modadisi",
          "text": "GLM was smart af at math when I used it almost a year ago",
          "score": 1,
          "created_utc": "2026-02-01 13:45:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2z0ym5",
          "author": "SVG-CARLOS",
          "text": "Why does everyone forget Manus AiðŸ˜­",
          "score": 1,
          "created_utc": "2026-02-01 14:14:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36wkbf",
              "author": "velosotiago",
              "text": "True, I wonder why people didn't mention Manus (an agentic app) on a post comparing different models, mr. \"1 day old account\".\n\nDead internet theory is, ironically, alive and well.",
              "score": 1,
              "created_utc": "2026-02-02 17:42:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g9tct",
                  "author": "SVG-CARLOS",
                  "text": "Dang I didn't mean to upset you with my alternative account ðŸ˜­, you are cringing so hard by the way lmao",
                  "score": 1,
                  "created_utc": "2026-02-04 01:32:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zixqm",
          "author": "OcelotMadness",
          "text": "I kind of wish GLM would hire more English writing majors. The grammar and storytelling is wrong a lot of the time. GLM 4.7 is my current open source GOAT but its a little clunky when playing text adventures on it.",
          "score": 1,
          "created_utc": "2026-02-01 15:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31zugx",
          "author": "dr_manhattan_br",
          "text": "The comparison will be more and more unfair as the closed models running on enterprise grade GPUs are compared with home or workstation GPUs with 32GB or in the best case scenarios with 96GB VRAM.\nThose closed weights are bigger than you can run in your home. \nThey will be faster and smarter as those companies fine-tune those models more frequently and with data that open models doesnâ€™t have.",
          "score": 1,
          "created_utc": "2026-02-01 22:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o322kxq",
              "author": "ForsookComparison",
              "text": "There is no inherent link to model size and weight policy",
              "score": 1,
              "created_utc": "2026-02-01 23:00:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32ytpl",
          "author": "OmarBessa",
          "text": "kimi and glm are sota\n\nif you haven't had catastrophic failures with gemini you haven't used it enough\n\nGLM and Kimi both provide similar agentic performance or better with much better stability",
          "score": 1,
          "created_utc": "2026-02-02 02:00:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32zhyz",
          "author": "real-joedoe07",
          "text": "Grok is far better. Answers correctly where Gemini and ChatGPT do not.",
          "score": 1,
          "created_utc": "2026-02-02 02:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34lw68",
          "author": "Salt-Cap-4489",
          "text": "One day, I'll switch from CachyOS to Arch.",
          "score": 1,
          "created_utc": "2026-02-02 09:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3673zx",
          "author": "Unable-Jelly6228",
          "text": "I need the top tier list of models under 8b :v",
          "score": 1,
          "created_utc": "2026-02-02 15:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3by4u5",
          "author": "FederalLook5060",
          "text": "In windsuf kimi k2.5thinking ? gemini 3 pro in AG. Also deepseek is not as good.",
          "score": 1,
          "created_utc": "2026-02-03 12:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3deaba",
          "author": "Hot_Slice",
          "text": "My take is, it's not in these company's best interest to reign in RAM usage - because if they did, then we could run the SOTA models at home, and they'd have no moat.",
          "score": 1,
          "created_utc": "2026-02-03 17:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3h94hl",
          "author": "kulisama",
          "text": "Mistral ?",
          "score": 1,
          "created_utc": "2026-02-04 05:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3leumr",
          "author": "ElectricalBar7464",
          "text": "none of these can be run locally yet. but its crazy to think that we will soon have a crazy powerful model that is as powerful as these, that'll fit on a phone.",
          "score": 1,
          "created_utc": "2026-02-04 20:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mnj8c",
          "author": "Excellent-Sense7244",
          "text": "glm is sota for me",
          "score": 1,
          "created_utc": "2026-02-05 00:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s68p2",
          "author": "korino11",
          "text": "OMG Ralh?!?? that a STUPID plugin! For what??! Only if you have a SIMPLE project! And thats it. If you need a somthing BIG with many layers of decompositions, with thousands calls of subagent. Useles shit your ralh...\n\nRight now kimi K2.5  - best model. Even gpt 5.2 xhigh useles. k2.5 can solve all. Even with high physycs and math..",
          "score": 1,
          "created_utc": "2026-01-31 12:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qmobq",
          "author": "SeeonX",
          "text": "What is the top AI called?",
          "score": 1,
          "created_utc": "2026-01-31 04:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qmvxs",
              "author": "Ok-Lobster-919",
              "text": "Claude, Opus is insanely good for coding. Never try it, you won't want to use anything else. It one-shots problems sonnet struggles with.",
              "score": 20,
              "created_utc": "2026-01-31 04:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qn6jc",
                  "author": "ForsookComparison",
                  "text": "> Never try it, you won't want to use anything else\n\n  \nMy wallet agrees. There are some tasks I have where *\"it's either Opus, or it's all afternoon\"* but that $25/1m racks up quick.",
                  "score": 12,
                  "created_utc": "2026-01-31 04:58:40",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2qnrtu",
                  "author": "jadhavsaurabh",
                  "text": "Bro same bro same, after trying opus I don't even use other AI now on 100$ plan",
                  "score": 8,
                  "created_utc": "2026-01-31 05:02:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qo99e",
                  "author": "HarambeTenSei",
                  "text": "gpt5.2 through codex is better in most instances",
                  "score": 1,
                  "created_utc": "2026-01-31 05:06:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qmzq0",
              "author": "ForsookComparison",
              "text": "Claude 4.5 Opus is the one in *\"My Job is Ralph Wiggum\"* tier.\n\n  \nClaude 4.5 Sonnet would rank bottom of \"SOTA\" if I took the time to rank split out models.  \n  \nClaude 4.5 Haiku would probably be lower than Grok but above Kimi and Deepsek.",
              "score": 2,
              "created_utc": "2026-01-31 04:57:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qoi6x",
                  "author": "simracerman",
                  "text": "So, do you think GLM-4.7 Flash beats 4.5 Sonnet or GLM-4.7 even?",
                  "score": 3,
                  "created_utc": "2026-01-31 05:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qp09q",
          "author": "yes-im-hiring-2025",
          "text": "I am going all in on GLM stocks btw.\nI think their GLM5 is gonna drop soon and reclaim the top spot (trust)",
          "score": 1,
          "created_utc": "2026-01-31 05:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp62n",
              "author": "ForsookComparison",
              "text": "Would be cool especially if they keep their commitments to open-weight.",
              "score": 6,
              "created_utc": "2026-01-31 05:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqyjw",
                  "author": "yes-im-hiring-2025",
                  "text": "Seems more and more unlikely since they IPOd.\n\nI'd love for them to be open, they're realistically the best performance:size option if I need to have it up and running locally. Minimax and Kimi are great but the dormant param weights are a memory hog that I'd rather avoid.",
                  "score": 1,
                  "created_utc": "2026-01-31 05:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2t67ac",
              "author": "-InformalBanana-",
              "text": "I root for Minimax - smaller models, better for local and competitive performance.",
              "score": 1,
              "created_utc": "2026-01-31 16:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ruvvl",
          "author": "stylehz",
          "text": "Besides the Claude (because it is the best), never let bro cook again.",
          "score": 1,
          "created_utc": "2026-01-31 11:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u98q1",
              "author": "nomorebuttsplz",
              "text": "Agreed, this tier list is very poor and lines up with vibes from 2 months ago rather than any actual tasks... and without specifying domain or task rankings are pointless...  overly broad like artificial analysis except without any evidence.",
              "score": 3,
              "created_utc": "2026-01-31 19:22:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sl62j",
              "author": "ForsookComparison",
              "text": "I'm in the kitchen and your head at the same time",
              "score": -1,
              "created_utc": "2026-01-31 14:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2stovx",
          "author": "nomorebuttsplz",
          "text": "Nah.Â \n\nKimi 2.5 is at SOTA levels and well above early 2025.\n\nGlm 4.7 is also above early 2025 in coding. Virtually everyone who has compared sonnet 3.7 and glm 4.7 in coding agrees glm is better.Â \n\nBut of course, this is task specific. What tasks are you measuring yours by?",
          "score": 1,
          "created_utc": "2026-01-31 15:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qpe34",
          "author": "neotorama",
          "text": "Gemini sits with Z. Itâ€™s crap with infinite loop. Kimi 2.5 beside OpenAI GPT",
          "score": 0,
          "created_utc": "2026-01-31 05:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r45o0",
              "author": "fishylord01",
              "text": "Not sure why downvoted, Gemini sucks . Agree kimi 2.5 is very close behind codex 5.2 high.",
              "score": 1,
              "created_utc": "2026-01-31 07:16:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rmqj2",
          "author": "Downtown_Fly_5919",
          "text": "And then you have nemotron 3 nano",
          "score": 0,
          "created_utc": "2026-01-31 10:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rqan2",
          "author": "spawncampinitiated",
          "text": "Gemini Sota xd",
          "score": 0,
          "created_utc": "2026-01-31 10:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6r0h",
          "author": "Juanisweird",
          "text": "Hey, recently returning to this whole AI thing. Whatâ€™s SOTA?\n\nAnd if you had to choose only one $20 subscription for a general use AI ( coding, research, chatting, troubleshooting real life and technical issues, design inspiration and business planning and marketing) which one would you choose for the next 2 months?",
          "score": 0,
          "created_utc": "2026-01-31 07:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rqylz",
              "author": "SilentLennie",
              "text": "It depends, what do you need it for (or more likely: how much do you need it) ? Do you include the job, do you include private use ? What kind of hobbies and job do you have, etc. ?",
              "score": 1,
              "created_utc": "2026-01-31 10:52:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rvxl3",
                  "author": "Juanisweird",
                  "text": "I want to create apps to automate or simplify my life. Iâ€™ve done some through python scripts and vibe coding like second hand listing generator or roulette simulator or points and job dashboard ( my 9-5 has a points system so after X amount of jobs, I get extra pay).\n\nBut I want to build a side income and want an AI that can help me do research, prepare projects and even help create delivery and onboarding automations with little AI and more old school programming but since I have little experience, need an AI to guide me or aid",
                  "score": 1,
                  "created_utc": "2026-01-31 11:37:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rbeh8",
              "author": "YayaBruno",
              "text": "I've been using Nano-GPT for a while  and it's been decent, it gives access to a bunch of open source models for an $8 a month subscriptio. ( Referral link if you're interested: https://nano-gpt.com/r/CfxGHjHp )",
              "score": 0,
              "created_utc": "2026-01-31 08:24:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t7893",
          "author": "KitchenSomew",
          "text": "Solid tier list! The placement of open-weight models in \"early 2025 SOTA\" territory is spot-on for most use cases. A few observations:\n\n\n\n1. \\*\\*Context matters more than raw intelligence\\*\\*: As several commenters noted, Kimi K2.5's multimodal capabilities + agentic workflows make it punch way above its weight compared to models with similar benchmark scores.\n\n\n\n2. \\*\\*The \"instruction following vs problem understanding\" debate is key\\*\\*: Claude's strength isn't just following instructionsâ€”it's inferring intent and missing context, which is why it excels at complex refactoring tasks even when specifications are vague.\n\n\n\n3. \\*\\*Open-weight gap is narrowing in specific domains\\*\\*: For coding with proper tooling (LSP, test generation, iteration loops), GLM-4.7 + good harness can match sonnet 3.7 on many practical tasks. The real gap shows in long-context coherence and multi-turn debugging.\n\n\n\n4. \\*\\*Size/performance tradeoff is underrated\\*\\*: Qwen3-235B is the sweet spot for self-hostedâ€”enough intelligence for real work without needing a data center. The jump to K2.5 territory requires massive compute that most can't justify.\n\n\n\nThe fact that we're even having \"SOTA vs early 2025\" debates about open weights is wild progress.",
          "score": -2,
          "created_utc": "2026-01-31 16:22:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwboqn",
      "title": "Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy",
      "subreddit": "LocalLLaMA",
      "url": "https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/",
      "author": "Fear_ltself",
      "created_utc": "2026-02-05 04:37:05",
      "score": 551,
      "num_comments": 41,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/",
      "domain": "research.google",
      "is_self": false,
      "comments": [
        {
          "id": "o3ouc8u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-05 09:30:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5wc0",
          "author": "-p-e-w-",
          "text": "They are using the phrase â€œwithout sacrificing accuracyâ€ in the sense of â€œit seems to perform equally well according to our testsâ€ â€“ **not** in the sense of â€œit computes exactly the same thingâ€, like in the case of Flash Attention.",
          "score": 149,
          "created_utc": "2026-02-05 05:47:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3old2f",
              "author": "ThisWillPass",
              "text": "Free lunch or unknown tradeoffs? Who knows?",
              "score": 25,
              "created_utc": "2026-02-05 08:03:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3o91fl",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -2,
              "created_utc": "2026-02-05 06:13:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3oa88j",
                  "author": "mukz_mckz",
                  "text": "Ah yes, the final boss of passing reddit comments to an LLM and pasting its output as a reply.",
                  "score": 8,
                  "created_utc": "2026-02-05 06:23:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nwv5m",
          "author": "ttkciar",
          "text": "Looking forward to seeing how it performs in Gemma 4 (hint, hint!)",
          "score": 223,
          "created_utc": "2026-02-05 04:42:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7ob3",
              "author": "tomakorea",
              "text": "Gemma 3 is such a good model for creative writing, its much better than Qwen. I really hope we can get an update",
              "score": 63,
              "created_utc": "2026-02-05 06:02:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3qgtro",
                  "author": "Far-Low-4705",
                  "text": "qwen also just halucinates (on the context) very, very badly, even at 16k. the other day i had it misspell \"didnt\" with \"did1n't\"\n\nGemma isnt any better with context performance, but it doesnt say anything with confidence that it cant recall accurately. not much better, but a better failure mode.\n\nBut qwen in general is far better at STEM. not even close.",
                  "score": 6,
                  "created_utc": "2026-02-05 15:54:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3payy4",
                  "author": "kaisurniwurer",
                  "text": "Better is a big word, qwen is more autistic and follow rules better.\nGemma does write much higher quality responses though.",
                  "score": 6,
                  "created_utc": "2026-02-05 11:58:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3r4vhq",
                  "author": "Dull-Appointment-398",
                  "text": "What kind of projects are you using models for, like what does 'creative writing' actually mean here?  Just wondering how people are using this models other than for image and code generation. ",
                  "score": 0,
                  "created_utc": "2026-02-05 17:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nx5z0",
              "author": "-dysangel-",
              "text": "I'm looking even more forward to seeing how it performs in Qwen, GLM and Deepseek",
              "score": 42,
              "created_utc": "2026-02-05 04:44:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oycmx",
              "author": "Orolol",
              "text": "I don't think this mechanism can be adapted to LLM. It seems VERY slow, because you do the attention en sequence instead of in one time, which make it very impractical for LLMs. It' more a ML application.",
              "score": -6,
              "created_utc": "2026-02-05 10:08:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3nxjcm",
              "author": "Hunting-Succcubus",
              "text": "What about gemma 3? They will not push software updates to older product?",
              "score": -18,
              "created_utc": "2026-02-05 04:46:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nzav7",
                  "author": "ttkciar",
                  "text": "I don't think you can retrofit this attention mechanism to models trained without it, at least not economically.  It would require a lot of retraining.\n\nI would be happy to be proven wrong, though.",
                  "score": 42,
                  "created_utc": "2026-02-05 04:59:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3o3ycb",
          "author": "coulispi-io",
          "text": "that's quite odd as the linked paper (https://arxiv.org/abs/2209.14881) was from 3 years ago... ",
          "score": 43,
          "created_utc": "2026-02-05 05:33:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o52u1",
              "author": "Fear_ltself",
              "text": "The 2022 paper introduced the core mathematical concept, the 2026 article reveals that Google has successfully upgraded this method to work on the \"hardware\" of modern AIâ€”specifically for pruning Large Language Models (LLMs) and running on GPUs.",
              "score": 68,
              "created_utc": "2026-02-05 05:41:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q7e73",
          "author": "FinalsMVPZachZarba",
          "text": "This appears to be a feature selection algorithm mainly for regression problems as far as I can tell, not a new attention mechanism for LLMs.\n\nThey do mention LLM pruning as one use case however, where the algorithm \"selects\" parts of the neutral network to prune.",
          "score": 5,
          "created_utc": "2026-02-05 15:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s0uyy",
              "author": "Brilliant-Wolf7589",
              "text": "This Will shorten training and make pruning better.Â ",
              "score": 1,
              "created_utc": "2026-02-05 20:14:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oole4",
          "author": "Significant-Skin118",
          "text": "Cool, I can make it do my shit even cleaner",
          "score": 8,
          "created_utc": "2026-02-05 08:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pa3pt",
              "author": "-dysangel-",
              "text": "ghosting?",
              "score": -3,
              "created_utc": "2026-02-05 11:51:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pqvys",
                  "author": "Significant-Skin118",
                  "text": "yes",
                  "score": 2,
                  "created_utc": "2026-02-05 13:41:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3qr9z2",
          "author": "Alarming_Bluebird648",
          "text": "it's wild seeing a 2022 paper get posted like it's brand new tech. i'll believe the lean infrastructure claims when i actually see it running in llama.cpp tbh.",
          "score": 5,
          "created_utc": "2026-02-05 16:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3owg4c",
          "author": "bakawolf123",
          "text": "hmm, the related paper is from 2y ago (Feb 2024) though, with an update 1y ago  \nthe website looks fancy but I don't see another update to the paper (yet)",
          "score": 5,
          "created_utc": "2026-02-05 09:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oz2os",
              "author": "HumanDrone8721",
              "text": "That's implementation, not theoretical concept.",
              "score": 7,
              "created_utc": "2026-02-05 10:15:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qjlgc",
          "author": "TheRealMasonMac",
          "text": "What are the implications of this? Is it something like KDA or DeepSeek V3.2's sparse attention?",
          "score": 1,
          "created_utc": "2026-02-05 16:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3r3m2t",
              "author": "Fear_ltself",
              "text": "Kimi Delta Attention (KDA): Is an expressive linear attention module that allows a model to have RNN-like memory, making it 6x faster at decoding long contexts while using 75% less memory. You have to build the model with KDA from the ground up.          \n    â€‹Sequential Attention:  works with any existing architecture (including standard transformers) to find and cut out the \"dead weight\".",
              "score": 1,
              "created_utc": "2026-02-05 17:40:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3rlm0e",
          "author": "Lowetheiy",
          "text": "The paper is from 2023, what is going on? This is not new research",
          "score": 1,
          "created_utc": "2026-02-05 19:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tnsqe",
          "author": "typical-predditor",
          "text": "Is this the secret sauce that makes 3 Flash so good but wasn't ready in time for 3 Pro?",
          "score": 1,
          "created_utc": "2026-02-06 01:23:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quzwjf",
      "title": "ACE-Step-1.5 has just been released. Itâ€™s an MIT-licensed open source audio generative model with performance close to commercial platforms like Suno",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/r7v6v6qwnbhg1",
      "author": "iGermanProd",
      "created_utc": "2026-02-03 18:26:58",
      "score": 526,
      "num_comments": 128,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quzwjf/acestep15_has_just_been_released_its_an/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3gq7n5",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-04 03:05:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e1fn2",
          "author": "Uncle___Marty",
          "text": "Well, dont know about anyone else but my mind is blown.",
          "score": 88,
          "created_utc": "2026-02-03 18:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fjkj4",
              "author": "Hans-Wermhatt",
              "text": "Yeah, these hype videos always over-promise, but I can't wait to try this. This model looks too good to be true. Running that fast on consumer hardware with this quality is wild.",
              "score": 15,
              "created_utc": "2026-02-03 23:08:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gggt7",
                  "author": "lorddumpy",
                  "text": "[the space is pretty damn impressive](https://huggingface.co/spaces/ACE-Step/Ace-Step-v1.5)",
                  "score": 6,
                  "created_utc": "2026-02-04 02:09:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3htgng",
                  "author": "lemondrops9",
                  "text": "1.35 is pretty good which I just tried out a few days ago. Excited to try this out.",
                  "score": 2,
                  "created_utc": "2026-02-04 07:52:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k7i5g",
                  "author": "splice42",
                  "text": "Installed it on my selfhosted AI server (4090 48GB) and it's damn impressive so far. The distilled model produces 2 minute length songs in around 15 seconds for me. Prompt adherence is pretty solid and it can do blues pretty well (which heartmula really didn't want to produce).\n\nAll this along with length control, key control, BPM, lora training? This thing is cooking.",
                  "score": 2,
                  "created_utc": "2026-02-04 17:08:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3h4zn8",
              "author": "GoodbyeThings",
              "text": "I just want a way to filter out this trash. I don't want to listen to AI generated music\n\nDidn't know so many slop supporters were in here",
              "score": -16,
              "created_utc": "2026-02-04 04:37:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3lt79o",
                  "author": "KingPinX",
                  "text": "Are you lost sir? do we need to call an adult to get you back to a safe space? Seriously you are in /r/LocalLLama ....",
                  "score": 1,
                  "created_utc": "2026-02-04 21:36:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fq3wl",
          "author": "bennmann",
          "text": "please support the official model researcher org:\n\n[https://acestudio.ai/](https://acestudio.ai/)",
          "score": 31,
          "created_utc": "2026-02-03 23:43:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i9xma",
              "author": "adeadbeathorse",
              "text": "Itâ€™s a collaboration between these guys and StepFun, an LLM company. Hence ACE-Step. StepFun mostly contributed resources and logistics (compute, human evaluation), though.",
              "score": 7,
              "created_utc": "2026-02-04 10:27:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e3ijf",
          "author": "Dundell",
          "text": "Can it do instrumentals? I like HeartMuLa, but it isnt capable of doing just instruments no voice.",
          "score": 22,
          "created_utc": "2026-02-03 19:00:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e5kwi",
              "author": "Hauven",
              "text": "Yes it can, but i haven't managed to get similar quality to Suno yet. I'm hoping it's primarily a matter of prompting it correctly. Possibly detailed lyrics such as \\[Intro\\] \\[Chorus\\] etc and explaining compositions and style within those. Just doing \\[Instrumental\\] is definitely not achieving results. Being more detailed has improved my results but still a bit of a way to go to get things sounding close to my Suno instrumentals.\n\nFor an open weight model however, that can generate music very fast, and on consumer hardware, it's impressive.",
              "score": 27,
              "created_utc": "2026-02-03 19:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e8l6q",
                  "author": "Sasikuttan2163",
                  "text": "Which version of it are you trying? How much is the difference in quality as you go down the model tiers? I have an 8GB 4060 but before I try it out I'd like to hear your thoughts.",
                  "score": 3,
                  "created_utc": "2026-02-03 19:24:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f1ayx",
                  "author": "Dundell",
                  "text": "I see the option and tested it just some 3min piano. Sounds good enough for my needs. This'll be good for my video workflows.",
                  "score": 3,
                  "created_utc": "2026-02-03 21:38:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ic0ds",
                  "author": "uti24",
                  "text": ">Yes it can, but i haven't managed to get similar quality to Suno yet. \n\nThis is what I hear in examples that comes with repository, too.\n\nIt sounds +- like Suno 3.5 or about, maybe a bit worse or a bit better, but close enough. And def not level of Suno 4/4.5, but benchmarks somehow show different. I also hope it can be fixed.\n\nI guess it's consequence of how fast it is.",
                  "score": 1,
                  "created_utc": "2026-02-04 10:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3hxy9b",
              "author": "mission_tiefsee",
              "text": "this is a whole different league than HeartMula. HM never followed my tags or anything. This baby is super diverse! Its real fun!",
              "score": 2,
              "created_utc": "2026-02-04 08:34:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3giehu",
          "author": "Claudius_the_II",
          "text": "lora support is lowkey the real killer feature here. give it a few weeks and people are gonna train genre-specific loras that blow the base model away. mit license + local inference + finetuning is exactly how you kill a subscription service",
          "score": 14,
          "created_utc": "2026-02-04 02:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ebcfh",
          "author": "Lanky_Employee_9690",
          "text": "I love how their demo prompts have little to do with the output... I have no idea why some of those prompts are THAT detailed given the model apparently ignores most of the instructions.",
          "score": 29,
          "created_utc": "2026-02-03 19:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ec0ki",
              "author": "iGermanProd",
              "text": "They mentioned using synthetic data, probably from something like Gemini or Qwen or anything with audio support, and those things arenâ€™t good at captioning music at all, so thatâ€™s probably why.",
              "score": 17,
              "created_utc": "2026-02-03 19:40:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ee0jj",
                  "author": "Lanky_Employee_9690",
                  "text": "No I mean it makes sense, but it's weird to show \"bad use cases\" as a demo. In my humble opinion, at least.",
                  "score": 12,
                  "created_utc": "2026-02-03 19:49:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eeefo",
                  "author": "tat_tvam_asshole",
                  "text": "You mean semantic classification? Idk, gemini ai through the studio api has been pretty good in my experience. More likely, they scraped ai generated music sites, ie suno, udio, etc and it's the bad classification there that leads to poor(er) knowledge of user intention",
                  "score": 3,
                  "created_utc": "2026-02-03 19:51:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ej6q8",
          "author": "Hearcharted",
          "text": "A few weeks ago a 300TB Dataset got leaked, sooner or later someone is going to release a model trained on that Dataset...",
          "score": 41,
          "created_utc": "2026-02-03 20:14:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gr4j5",
              "author": "ThatsALovelyShirt",
              "text": "The Spotify one? If I recall, it's all encoded in 96 kbps. So the quality isn't great. \n\nBut there's probably a model one could train to \"upscale\" it back and recover some of the lost frequency bands.",
              "score": 12,
              "created_utc": "2026-02-04 03:10:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mc73r",
                  "author": "adeadbeathorse",
                  "text": "Any track with a popularity score greater than 0, so basically anything that had any plays, was archived at 160 kbps as Ogg Vorbis, with everything else being 75 kbps as Ogg Opus. Both Vorbis and Opus are far superior to mp3, with the 75 kbps versions probably sounding better than 128 kbps mp3.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:12:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fgfny",
              "author": "gjallerhorns_only",
              "text": "Good point. Open Source music models will be damn near identical to SOTA closed-source in a few months then!",
              "score": 17,
              "created_utc": "2026-02-03 22:51:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ephh5",
              "author": "FluoroquinolonesKill",
              "text": "A dataset of what?",
              "score": 13,
              "created_utc": "2026-02-03 20:43:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3evo2x",
                  "author": "Koksny",
                  "text": "Dump of Spotify audio repository.",
                  "score": 37,
                  "created_utc": "2026-02-03 21:12:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3evfsm",
                  "author": "Future_Part_4456",
                  "text": "Spotify",
                  "score": 5,
                  "created_utc": "2026-02-03 21:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fkd83",
              "author": "stonetriangles",
              "text": "They never released it.",
              "score": 6,
              "created_utc": "2026-02-03 23:12:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g23uk",
                  "author": "TheRealMasonMac",
                  "text": "They can just release it to the companies directly ahead of the public. They already do have such proprietary datasets they sell. Theyâ€™re probably waiting for the heat to die down before silently releasing.",
                  "score": 4,
                  "created_utc": "2026-02-04 00:49:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fb76z",
              "author": "IrisColt",
              "text": "heh",
              "score": 2,
              "created_utc": "2026-02-03 22:25:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eonx4",
          "author": "Small-Fall-6500",
          "text": "HuggingFace link:\n\nhttps://huggingface.co/ACE-Step/Ace-Step1.5",
          "score": 12,
          "created_utc": "2026-02-03 20:40:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fj5hg",
          "author": "Trendingmar",
          "text": "It's very good for open source but Suno V5 it is not. \n\nEspecially disappointing is the cover feature which is... not useful at this point.\n\nHere's my comparison with the same prompt:\n\n  \n[https://voca.ro/1Pzw27iI3Sjf](https://voca.ro/1Pzw27iI3Sjf) (Suno V5)\n\n[https://voca.ro/1i5SlHuvue2R](https://voca.ro/1i5SlHuvue2R) (Ace 1.5)\n\n  \nBut we love to see it regardless. Open Source is getting closer and closer.",
          "score": 26,
          "created_utc": "2026-02-03 23:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gcktc",
              "author": "_bani_",
              "text": "i like the ace composition better, but suno fidelity is better.",
              "score": 7,
              "created_utc": "2026-02-04 01:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fk244",
              "author": "inigid",
              "text": "Honestly I prefer the ACE version fwiw.\n\nI was having trouble with repaint not following the original motifs.  Have you had any luck?",
              "score": 4,
              "created_utc": "2026-02-03 23:10:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fr55v",
                  "author": "Trendingmar",
                  "text": "I don't use repaint. But I can tell you there's a quite a few things that I hope are just bugs/implementation issues that will be eventually ironed out. \n\nBut we're getting spoiled here. It was just released today, and I'm already complaining about it.",
                  "score": 12,
                  "created_utc": "2026-02-03 23:49:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3k8gjx",
              "author": "hrjet",
              "text": "OT, but what is the name of the original song? I couldn't find the song by looking up the lyrics.",
              "score": 1,
              "created_utc": "2026-02-04 17:12:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kw82r",
                  "author": "Trendingmar",
                  "text": "I wasn't clear, I made it sound like this was a cover. Ace mangles covers right now. Original lyrics courtesy of gemini. I just called the song \"Lo\", I'm sure you caught on that song is about a character from a book. Here's original Suno:\n\n[https://voca.ro/1dOvvjdoPHdw](https://voca.ro/1dOvvjdoPHdw)",
                  "score": 1,
                  "created_utc": "2026-02-04 19:00:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3egwvx",
          "author": "vladlearns",
          "text": "I like this \"takes 2 seconds on ^(A100)\"",
          "score": 27,
          "created_utc": "2026-02-03 20:03:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3epjar",
              "author": "AdSafe4047",
              "text": "Actually an a100 is not that fast tbh, it just has a lot of fast memory so you can train on it fast, for inference if you have a consumer rtx4090 or 5090 it should be faster.",
              "score": 20,
              "created_utc": "2026-02-03 20:44:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ga9ot",
                  "author": "corysama",
                  "text": "> Generate a full 4-minute song in ~1 second on a RTX 5090, or under 10 seconds on an RTX 3090.\n\nhttps://blog.comfy.org/p/ace-step-15-is-now-available-in-comfyui",
                  "score": 19,
                  "created_utc": "2026-02-04 01:35:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ejp2q",
              "author": "Hearcharted",
              "text": "LOL ðŸ˜‚",
              "score": -5,
              "created_utc": "2026-02-03 20:16:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e5pph",
          "author": "uti24",
          "text": "That's pretty good! Quality is good, too. I don't know did we had something this good before, but now we have.\n\nWhat stack does it use? I mean, using stable diffusion with AMD under windows is quite finicky even with tutorial, is this one, too?",
          "score": 8,
          "created_utc": "2026-02-03 19:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3efq1e",
              "author": "noctrex",
              "text": "If you use the latest official portable distribution it works actually fine, just tried it out, and on my zluda install cannot run it, but the official amd one does",
              "score": 3,
              "created_utc": "2026-02-03 19:57:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e865v",
          "author": "Sasikuttan2163",
          "text": "I find it really hard to believe the demos are generated by it. Like if it really is made entirely by this model then wow I can't begin to imagine how much of an impact this will have.",
          "score": 8,
          "created_utc": "2026-02-03 19:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ebpe0",
              "author": "iGermanProd",
              "text": "Itâ€™s real. Iâ€™ve been testing it for the last couple of days because I requested early access since Iâ€™m writing a thesis on audio AI. Itâ€™s maybe 20% behind the state of the art in certain genres. The model is likely smaller than commercial ones, so its world knowledge is small, but LoRA support remedies that.",
              "score": 18,
              "created_utc": "2026-02-03 19:39:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3efqvk",
                  "author": "Sasikuttan2163",
                  "text": "That's absolutely mind-blowing! I had worked on a voice generation paper before and I remember how hard it was to get code switching right to ensure the model can switch between languages seamlessly. Other than the instruments and actual vocals, this is something which surprised me. That K-pop demo with language switches was so natural it felt unreal.",
                  "score": 1,
                  "created_utc": "2026-02-03 19:57:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3iqjka",
                  "author": "Aceness123",
                  "text": "Can I make a lora with an rtx3060?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:39:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3g1j2p",
          "author": "captainrv",
          "text": "I just gave it a try. It's really catching up to some of the online sites, but it has a way to go in sound quality compared to some of the better online services. To my ears, it's in there with Suno 3.5, Udio from about a year or so ago. I had issues with the 4 generations I made where it skipped entire lines of lyrics, and some of the voice quality was not great. Still, this is a significant leap forward from Ace-Step 1.0.",
          "score": 7,
          "created_utc": "2026-02-04 00:46:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iiabv",
              "author": "NandaVegg",
              "text": "I gave it a roll with a bit of experimental LoRA with random 50 pop music audio files for 500 epochs (it only uses single GPU so the training process is damn slow with A100). Prompt adherence is actually excellent but you need to be verbose (you can't use tags list; otherwise you need to use format button in the GUI) and I never have an issue getting the model to replicate lyrics that consists of multiple languages.\n\nThe audio quality is somewhat muffled and dissolvy, with or without custom lora, like it had a bit of low-bit bitcrusher or something, which is the largest issue to me. Not something you would use in production. Otherwise it is excellent, it has a lot of niche genre/instruments/technique knowledge that you can enable with a bit of LoRA training.\n\nEdit: I played with this for 2 days and I must say it's VERY good for what it is, but the documentation is scarce and I'm yet to figure out how to use other modes like lego. I am hoping for better quality-sounding iteration in the future. Artifacts are still a bit annoying.",
              "score": 2,
              "created_utc": "2026-02-04 11:39:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e97ae",
          "author": "captainrv",
          "text": "Seems impressive. Has anyone tested this on consumer GPUs?",
          "score": 10,
          "created_utc": "2026-02-03 19:27:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ej21u",
              "author": "MichaelDaza",
              "text": "Says it makes songs in 10 seconds with a 3090. Even if 3060s are slower, thats still a whole song, remastered in like 20 seconds. I am very impressed",
              "score": 12,
              "created_utc": "2026-02-03 20:13:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3epqe0",
                  "author": "ComposerNo5742",
                  "text": "Mac Mini M4 24GB non-pro generates 3 minutes of music in around 40s after loading everything.",
                  "score": 7,
                  "created_utc": "2026-02-03 20:45:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3fx0hi",
                  "author": "skocznymroczny",
                  "text": "On my 5070Ti generates a 2 minute song in a minute.",
                  "score": 2,
                  "created_utc": "2026-02-04 00:21:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gfis5",
              "author": "behohippy",
              "text": "I got it generating songs with a 3060ti 8 gig.  The gradio UI was kinda jank so I ended up modifying their python example for it instead.  Also had to use 8 bit quantization on the model and batch size 1 to not throw errors.  It works way better if you do your own caption (music style desc) and lyrics.",
              "score": 3,
              "created_utc": "2026-02-04 02:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3hz2xb",
              "author": "mission_tiefsee",
              "text": "yes. works like a charm. Just update your comfyUI and it has a template with everything read to go. Takes 90s for me to create a 3:40min song with a 3090TI. good stuff.",
              "score": 1,
              "created_utc": "2026-02-04 08:45:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3foaw2",
          "author": "Timboman2000",
          "text": "ComfyUI has been updated and it's Workflow is in the base list of templates now (along with links to all of the needed model files for it once you load it up).",
          "score": 5,
          "created_utc": "2026-02-03 23:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gin6f",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-04 02:22:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gkmv1",
                  "author": "Timboman2000",
                  "text": "You gotta update ComfyUI for it to show the new ones.",
                  "score": 1,
                  "created_utc": "2026-02-04 02:33:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dxjra",
          "author": "SlowFail2433",
          "text": "Seems to be strong",
          "score": 12,
          "created_utc": "2026-02-03 18:33:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gi0gg",
          "author": "Ordinary-Wish-3843",
          "text": "https://preview.redd.it/b4jf2ld90ehg1.png?width=1253&format=png&auto=webp&s=4d0f3f95031b97325a8ba2e9c6c0d02f1c9c61a4\n\nIâ€™m running it on Comfy, and Iâ€™ve noticed that if you change the seed, run it, and then go back to the previous one, you wonâ€™t get the same song again.",
          "score": 4,
          "created_utc": "2026-02-04 02:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3grodc",
              "author": "ThatsALovelyShirt",
              "text": "There's probably some internal vars in the state dict that change run to run. But besides that, GPU inference in Comfy is not deterministic unless you explicitly pass the deterministic launch arg.",
              "score": 6,
              "created_utc": "2026-02-04 03:13:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3hdnx1",
          "author": "Warthammer40K",
          "text": "> mic smell like tuna\n\nFirst off, the lyrics are wild. The model is clearly too small to also be a decent multilingual songwriter, so you'd probably want to write those first with a more capable LLM.\n\nAlso, I noticed with the \"repainting\" feature (did they mean in-painting?) in the demo video, you wouldn't be able to use it as-is because the percussion instruments sound completely different. The snare lost more than half of its sound, for example. It probably works best with one channel or isolated stems.",
          "score": 3,
          "created_utc": "2026-02-04 05:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e4lmv",
          "author": "Olangotang",
          "text": "LOL that first track is definitely from Shinedown training data.",
          "score": 2,
          "created_utc": "2026-02-03 19:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ep3jd",
          "author": "inigid",
          "text": "This is absolutely nuts, and I love the separation of concerns in the architecture.  It opens up a lot of possibilities.  Fantastic work!!  Bravo to the ACE team!",
          "score": 2,
          "created_utc": "2026-02-03 20:42:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3er8tk",
          "author": "RedditPolluter",
          "text": "I'm pretty sure that first song is based on Rhianna.",
          "score": 2,
          "created_utc": "2026-02-03 20:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6xsv",
          "author": "jiml78",
          "text": "I think they forgot to train it on metal music.  But I guess that is ok since training LORAs looks to be pretty easy",
          "score": 2,
          "created_utc": "2026-02-04 01:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ot9i7",
              "author": "Silentoplayz",
              "text": "Oh, 1000%. I noticed it too when trying to generate a few metalcore songs. It's funny hearing the weird screams get transitioned over into a women's voice singing the lyrics.",
              "score": 1,
              "created_utc": "2026-02-05 09:19:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pjj5j",
                  "author": "jiml78",
                  "text": "I was just trying to get some slam death metal going and realized immediately, even describing the genre didn't help it make anything remotely close.",
                  "score": 2,
                  "created_utc": "2026-02-05 12:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3gftkr",
          "author": "krait17",
          "text": "Any workflow for comfyui that has the Cover and Repaint feature ?",
          "score": 2,
          "created_utc": "2026-02-04 02:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hjm14",
              "author": "nicedevill",
              "text": "I would like to know as well.",
              "score": 1,
              "created_utc": "2026-02-04 06:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ip0cb",
                  "author": "krait17",
                  "text": "Dont bother with comfy, i've followed this tutorial and has all the features + it's ultra fast, like a few seconds compared to +30 seconds on comfy + the loading model time. [https://www.youtube.com/watch?v=QzddQoCKKss](https://www.youtube.com/watch?v=QzddQoCKKss)",
                  "score": 2,
                  "created_utc": "2026-02-04 12:28:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fxr17",
          "author": "-p-e-w-",
          "text": "Seeing things like that makes you wonder how many industries will still exist 10 years from now.",
          "score": 2,
          "created_utc": "2026-02-04 00:25:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e2cg9",
          "author": "marcoc2",
          "text": "Language support?",
          "score": 0,
          "created_utc": "2026-02-03 18:55:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e3qmi",
              "author": "Segaiai",
              "text": "Their demos have English, Chinese, Japanese, Korean, Arabic, Spanish, and Norwegian, but I haven't seen a specific language list. The only Korean and Japanese examples used English letters, but they also switched up how they wrote in Chinese, so maybe they were showing range.",
              "score": 7,
              "created_utc": "2026-02-03 19:01:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j0sct",
                  "author": "guigs44",
                  "text": "> The only Korean and Japanese examples used English letters\n\nPer the Technical report: \"For non-Roman scripts (e.g., Chinese, Japanese, Thai), we implement a stochastic Romanization\nstrategy, converting 50% of lyrics into phonemic representations during training. This approach enables the model to\nshare phonological representations across languages, significantly enhancing pronunciation accuracy for rare tokens\nwithout expanding the vocabulary size.\"",
                  "score": 3,
                  "created_utc": "2026-02-04 13:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3exayz",
              "author": "ANR2ME",
              "text": "They mentioned 50 languages ðŸ˜…",
              "score": 1,
              "created_utc": "2026-02-03 21:19:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ey87c",
          "author": "Nexter92",
          "text": "We are so fucking cook, even music will not be human only",
          "score": 2,
          "created_utc": "2026-02-03 21:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hud07",
              "author": "lemondrops9",
              "text": "so many AI songs on Youtube its getting very hard to tell what is or is not AI",
              "score": 1,
              "created_utc": "2026-02-04 08:00:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g5lpy",
          "author": "CoUsT",
          "text": "Holy shit!\n\nGreat quality and such amount of features/tuning/configuration is just insane. Near instant generation is a nice bonus.",
          "score": 1,
          "created_utc": "2026-02-04 01:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gabot",
          "author": "Perfect-Campaign9551",
          "text": "The comfy workflows have problems I get a lot of distortion with drum and snare sound",
          "score": 1,
          "created_utc": "2026-02-04 01:35:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hs6xk",
          "author": "tarruda",
          "text": "This is the same company that released the best 128GB RAM LLM: Step 3.5 Flash.\n\nThey are under the radar but clearly have a super strong team of scientists.",
          "score": 1,
          "created_utc": "2026-02-04 07:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i08hs",
          "author": "sagiroth",
          "text": "Silly question but can this be used to make game sounds like footsteps ?",
          "score": 1,
          "created_utc": "2026-02-04 08:55:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ifjat",
              "author": "djtubig-malicex",
              "text": "Not sure. udio could since it was trained on radio advertising clips and trailer music. maybe fine tune and loras lol",
              "score": 1,
              "created_utc": "2026-02-04 11:16:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3is54f",
          "author": "DocHoss",
          "text": "Anyone know if this plays nice on a Strix Halo?",
          "score": 1,
          "created_utc": "2026-02-04 12:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iur8s",
          "author": "techlatest_net",
          "text": "Bookmarked HF demo. Vocal-to-BGM conversion is wild â€“ might train my voice on this weekend. Great drop!",
          "score": 1,
          "created_utc": "2026-02-04 13:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j2n8c",
          "author": "Stepfunction",
          "text": "Github: [https://github.com/ace-step/ACE-Step-1.5](https://github.com/ace-step/ACE-Step-1.5)",
          "score": 1,
          "created_utc": "2026-02-04 13:50:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jz8rt",
          "author": "73tada",
          "text": "Holy shit, ~~pop~~ music is dead.",
          "score": 1,
          "created_utc": "2026-02-04 16:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nkeuw",
          "author": "lrq3000",
          "text": "Very impressive!\n\nIt generates very usable (ie, ready for editing in a DAW with little musical mistakes) samples at a rate of about 1/4 in my quick test and with very raw prompts, which is incredible! Especially given how fast the samples are generated!\n\nWith better prompts refinement and better understanding of how to use the model (keep in mind the online demo has a much reduced set of features compared to the downloadable full model, and IÂ could not get my head around how to use the repainting feature), it certainly is a game changer for local ai music generation.\n\nTip: it seems it can \"learn\" additional musical theory skills by giving a reference song, and what is particularly interesting is that this happens even if the target musical style is totally different from the reference song, the model can abstract musical concepts beyond the style. For example, it learnt to do complex musical phrasing here : https://www.youtube.com/watch?v=as72R3cx0iQ",
          "score": 1,
          "created_utc": "2026-02-05 03:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nt3zg",
          "author": "Hot-Employ-3399",
          "text": "UX is much worse than previous version. In previous version we had dockerfile, here we have instructions on how to install [that don't work](https://github.com/ace-step/ACE-Step-1.5/issues/108)Â \n\n\nPersonally I couldn't get uv sync to work, it failed, printing something about windows, tried uv venv + uv pip, it didn't work as torch and flash attention were installing the same time, had to install torch first, and then I not so related to ACE I've remembered that hf's xet is an absolute garbage that didn't want to download anything at speed >380kB/sec.Â \nFuck everything about xet. Barely fixed this shit by disabling concurrency in .gitconfig. For some reason it failed if it was enabledÂ \n\n\nHaven't tested further, but let's say after wasting 30 minutes I've changed my mind about comfyui from \"redundant\" to \"actually may be better\"",
          "score": 1,
          "created_utc": "2026-02-05 04:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oossp",
          "author": "lemondrops9",
          "text": "I thought 1.35 was decent. Ace 1.5 is blowing me away.",
          "score": 1,
          "created_utc": "2026-02-05 08:36:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pxyv1",
          "author": "EasternAd8821",
          "text": "wtf!?! even if these are cherry picked, if it can do this 1 out of 4 times that is amazing. Ace is a Chinese company/group? they must be because it's the only place solid, amazing, rapid, open source AI research happens any more it seems like.",
          "score": 1,
          "created_utc": "2026-02-05 14:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qavjf",
          "author": "Free_Scene_4790",
          "text": "I've only managed to get it working on Comfy. The Gradio/Portable version doesn't work for me.",
          "score": 1,
          "created_utc": "2026-02-05 15:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gb4q4",
          "author": "Thrumpwart",
          "text": "Would be cool if LMStudio supported these models...",
          "score": 1,
          "created_utc": "2026-02-04 01:39:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hu53t",
              "author": "Uncle___Marty",
              "text": "Google \"pinokio\". Its an AI browser (open source) with a bunch of 1 click installers. ace step already has a script im using.",
              "score": 2,
              "created_utc": "2026-02-04 07:58:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jlf1i",
                  "author": "Thrumpwart",
                  "text": "Oh nice! I keep meaning to check out pinokio and never have. Thank you!",
                  "score": 2,
                  "created_utc": "2026-02-04 15:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kc2sr",
              "author": "henk717",
              "text": "Its on our wishlist to, but unless something in the ggml ecosystem adds it its out of scope unfortunately.",
              "score": 2,
              "created_utc": "2026-02-04 17:29:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kcxxz",
                  "author": "Thrumpwart",
                  "text": "Ah, thank you.",
                  "score": 1,
                  "created_utc": "2026-02-04 17:33:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3if2k6",
          "author": "manipp",
          "text": "So it seems the creator has gone out of his way to make the 'cover' feature destroy any melody of the input song to make sure it won't replicate the melody. He did this, according to the discord, \"Donâ€™t fuucking second-guess my intentions. It has nothing to do with copyrightâ€”this design is simply more interesting, and I like how it works. I get to decide how my model is designed. use paid ace-studio or suno\"\n\nVery very disappointing.",
          "score": 1,
          "created_utc": "2026-02-04 11:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jcy6h",
              "author": "iGermanProd",
              "text": "Just wait a bit for Comfy folk to figure out a2a. You could reasonably expect it to work well with the VAE being available and the model being a diffusion model. Donâ€™t attribute malice so quickly. \n\nIâ€™m not picking any sides, but letâ€™s be rational and not entitled. I donâ€™t like when people are so quick to attribute malice and shit on developers for not only releasing a model but also being kind and receptive enough to do it under an MIT license. And while it was said in quite a rude way, I do believe Junmin was only talking about their Gradio demo, not dictating how we should use the model.\n\nNow for the tech bit:\n\nWhat happens now in the Gradio demo is (to my knowledge) not any conspiracy, but rather the audio being turned into LM codes that get used for the diffusion process. Effectively, you only really preserve the structure, some rhythm, and a hint of the melody that way. Like a description. Ergo, itâ€™s more of a remix/suggestion/alternate reality version. Junmin (one of the authors of this) says he regrets even calling it cover in the first place. \n\nThatâ€™s because the source audio is NOT currently being applied to the diffusion process like it is in other â€œcoverâ€ features or even image-to-image models, so it only has that structural metadata to go off of. Of course, it sounds nothing like the input. Itâ€™s a bit like asking Gemini to describe an image in as much detail as possible, then taking that text, then running Nano Banana on the result - itâ€™ll be similar but different, because you went through a whole layer of abstraction to get to the result. \n\nBut what you want is an editing workflow, so sending an image to Nano Banana and having it change the image, not guess from a different modality. \n\nAnd this seems like a trivial fix inside something like ComfyUI - just use the VAE, encode input audio, compose encoded audio over random noise (with different proportions to control strength), pass into diffusion, adjust denoising amount (to control strength in a different way), boom, youâ€™ll get a cover. Bonus points if you combine it with the structural LM codes to get probably either a horrible result if they clash, or a really good one if they donâ€™t.",
              "score": 3,
              "created_utc": "2026-02-04 14:44:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dz0o4",
          "author": "ffgg333",
          "text": "Can someone make a Lora trainer on Google colab?",
          "score": -8,
          "created_utc": "2026-02-03 18:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e6l41",
          "author": "Opfklopf",
          "text": "God I hate \"creative\" AI. I don't want to see or hear it anymore. I thought this sub is about LLMs. I guess not, oh well..",
          "score": -38,
          "created_utc": "2026-02-03 19:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fab9q",
              "author": "redditscraperbot2",
              "text": "I feel bad for the authors after reading this take. If you followed the project youâ€™d know they were actually not overly fond of the idea of using it to generate songs and that be the end of it. They want people to use the tools they released as a Swiss Army knife to improve and iterate on their creations. \n\nLike I really got the sense they like music and the creative process and youâ€™ve walked away with the wrong idea.",
              "score": 7,
              "created_utc": "2026-02-03 22:20:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fjkji",
                  "author": "Opfklopf",
                  "text": "Tbf I know nothing about it. I just hate the entire buzz companies create and the trash people spam the internet with so I just react allergically at this point.",
                  "score": -10,
                  "created_utc": "2026-02-03 23:08:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrazyy",
      "title": "Cline team got absorbed by OpenAI. Kilo is going full source available in response.",
      "subreddit": "LocalLLaMA",
      "url": "https://blog.kilo.ai/p/cline-just-acqui-hired",
      "author": "demon_bhaiya",
      "created_utc": "2026-01-30 16:56:49",
      "score": 428,
      "num_comments": 56,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o2pu91x",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 01:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mwmxg",
          "author": "ResidentPositive4122",
          "text": "For open models roo was better than cline anyway. It had more knobs to tweak, more things to edit, so you can adjust your env to the models.",
          "score": 99,
          "created_utc": "2026-01-30 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nbp2e",
              "author": "nore_se_kra",
              "text": "Its still a cline fork so its not like they will not be impacted",
              "score": 11,
              "created_utc": "2026-01-30 18:20:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ob2ct",
                  "author": "zxyzyxz",
                  "text": "It's a fork, so they won't be impacted, how would they when they own their own fork?",
                  "score": 18,
                  "created_utc": "2026-01-30 21:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2sycm4",
                  "author": "flyryan",
                  "text": "Roo de-forked from cline a while ago.",
                  "score": 1,
                  "created_utc": "2026-01-31 15:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ozbdl",
              "author": "rvistro",
              "text": "Yeah, I tried cline and it didnt work well. Roo worked and it was pretty nice.",
              "score": 1,
              "created_utc": "2026-01-30 23:01:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nw63e",
          "author": "bamboofighter",
          "text": "My team has been running a multi-model agent setup for our team - Claude, local Qwen on a 3090, Ollama for batch work - all through one orchestration layer. The Cline news is exactly why we went model-agnostic from day one. Vendor lock-in is a real risk when your entire dev workflow depends on one provider. Kudos to Kilo Code for going open",
          "score": 15,
          "created_utc": "2026-01-30 19:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o2m37",
              "author": "captcanuk",
              "text": "Could you share more about your setup? What works and doesnâ€™t?",
              "score": 3,
              "created_utc": "2026-01-30 20:22:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2op0ll",
              "author": "lan-devo",
              "text": "And you did the sensible thing, in this AI craze they are moving so fast that building entire environments and workflows on different systems for then be left hanging or dependent, or at the contrary depending of a single API provider, one owner of a company told me if OpenAI fails or does something fucky as a company their entire bussiness is over. They are treating these API comanies like they are established cloud business like aws, azure... while they a losing billions",
              "score": 2,
              "created_utc": "2026-01-30 22:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oe8wu",
          "author": "itb206",
          "text": "Per the Article:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI",
          "score": 15,
          "created_utc": "2026-01-30 21:17:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rti7q",
              "author": "__Maximum__",
              "text": "To clarify, this is an update, Cline team has answered to the claims.",
              "score": 3,
              "created_utc": "2026-01-31 11:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n7oyh",
          "author": "cleverusernametry",
          "text": "Called it. Those guys were in it for the pay day. I think Saoud was legit at the start then he got money hungry/enticed by money etc and they screwed up royally. He'll still. \n\nPash seemed like a knob and they ignored important, popular PRs, only to later then implement that code without providing attribution to the author. \n\nNick was extremely clearly a marketing guy, just chaisng numbers instead of actually doing devrel.",
          "score": 55,
          "created_utc": "2026-01-30 18:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ob6p6",
              "author": "zxyzyxz",
              "text": "Honestly, good on them for getting that bag while the AI investment boom is hot. Who knows how long OpenAI will stay solvent when they're burning billions a month?",
              "score": 24,
              "created_utc": "2026-01-30 21:03:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph70v",
                  "author": "cleverusernametry",
                  "text": "nothing wrong with getting big sums of money for making useful and popular things. But I feel that there is something wrong with these types of guys getting millions of dollars for shoddy, easily made things like vs code extensions and vs code forks (don't get me started on cursor). And in any normal circumstance, they wouldn't be getting millions but they do because VCs have first class access to the broken money printer. Its just a day to day manifestation of how broken american capitalism is",
                  "score": 6,
                  "created_utc": "2026-01-31 00:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p7ezo",
              "author": "throwawayacc201711",
              "text": "But you didnâ€™t call it. They updated the article with:\n\n> Update: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 4,
              "created_utc": "2026-01-30 23:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgkb5",
                  "author": "cleverusernametry",
                  "text": "What I called are these guys are hustlers.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:36:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oe8pf",
          "author": "cantgetthistowork",
          "text": "None of these forks have fixed the stupid 5 min timeout. WHAT IS THE PURPOSE OF IT",
          "score": 6,
          "created_utc": "2026-01-30 21:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlspu",
              "author": "kei-ayanami",
              "text": "I hope more people hear about this. The huge models like K2, Deepseek, etc need 5+ mins of time to think unless you have a crazy setup or use API.",
              "score": 4,
              "created_utc": "2026-01-31 04:48:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35xxaj",
              "author": "Exciting-Trifle2829",
              "text": "Check out Roo-Code - they have fixed this issue for Llama.cpp at least this fork fixed it:  \n[https://github.com/RooCodeInc/Roo-Code/issues/7366](https://github.com/RooCodeInc/Roo-Code/issues/7366)",
              "score": 2,
              "created_utc": "2026-02-02 15:00:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ay04h",
                  "author": "cantgetthistowork",
                  "text": "They haven't merged it into any release?",
                  "score": 2,
                  "created_utc": "2026-02-03 07:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pt6c8",
          "author": "Saltwater_Fish",
          "text": "cline is not particularly easy to use, roo code is better",
          "score": 3,
          "created_utc": "2026-01-31 01:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o6a2t",
          "author": "NoFaithlessness951",
          "text": "Cline wasn't good anyway, most people moved on to somewhere else",
          "score": 9,
          "created_utc": "2026-01-30 20:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pfu9q",
              "author": "bludgeonerV",
              "text": "Cline and it's forks are still the best option if you want that collaborative peer programming feel rather than just being a RP reviewer for AI.",
              "score": 3,
              "created_utc": "2026-01-31 00:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o21h1",
          "author": "grabber4321",
          "text": "Uninstalled.",
          "score": 7,
          "created_utc": "2026-01-30 20:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o3j8r",
              "author": "trararawe",
              "text": "That's exactly what openai wants you to do.",
              "score": 3,
              "created_utc": "2026-01-30 20:26:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36drll",
                  "author": "ghulamalchik",
                  "text": "Yes openai wants me to use Roo Code. Thanks openai! This is actually better than cline!",
                  "score": 2,
                  "created_utc": "2026-02-02 16:16:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qcw2w",
          "author": "Kamimashita",
          "text": "Can't wait for OpenClaw (Clawdbot) to do the same in a few months.",
          "score": 3,
          "created_utc": "2026-01-31 03:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nh0y1",
          "author": "kei-ayanami",
          "text": "Nooooooo :/ I hope the other projects can stay strongÂ ",
          "score": 4,
          "created_utc": "2026-01-30 18:44:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qia86",
              "author": "uhuge",
              "text": "I hope some Continue (pun intended)",
              "score": 1,
              "created_utc": "2026-02-05 16:01:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nyael",
          "author": "arm2armreddit",
          "text": "Cline is dead? ðŸ˜­ ðŸ˜ª ðŸ˜”",
          "score": 2,
          "created_utc": "2026-01-30 20:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p17q6",
              "author": "Mkengine",
              "text": "No, at the start of the article, it says:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 7,
              "created_utc": "2026-01-30 23:11:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o0si3",
          "author": "Impossible-Glass-487",
          "text": "God fucking damnit",
          "score": 2,
          "created_utc": "2026-01-30 20:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o8ke3",
          "author": "FullOf_Bad_Ideas",
          "text": "I think we'll have enough of open source coding harnesses for a forseable future. There are dozens of them now. I am basically a daily user of cline but switching won't be hard. They didn't find a way to monetize so they jumped on the wagon of a different company that doesn't have the same issue with sustainability (they have similar issue but on a different scale where staffing costs are small anyway). Industry will consolidate - losers will get absorbed into successful corps and losses will be amortized without a bubble pop this way.",
          "score": 2,
          "created_utc": "2026-01-30 20:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q8hmv",
          "author": "ismaelgokufox",
          "text": "I just hope the PRs on Kilo for a single OpenAI compatible provider to show all models, is merged soon. Having a single provider profile per model is cumbersome. \n\nGlad to see them go ever more open.",
          "score": 2,
          "created_utc": "2026-01-31 03:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rbqyt",
          "author": "Charming_Support726",
          "text": "Left Cline behind month ago, because of their strange politics. (Never got on with Roo and Kilo as well). Switched to Opencode and I use Antigravity and Codex from time to time with my quota.\n\nCline and such had an attitude issue from the beginning",
          "score": 2,
          "created_utc": "2026-01-31 08:27:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2od4h8",
          "author": "false79",
          "text": "So bizarre Nik Pash got fired from Cline, hired by Open AI, only to have Open AI absorb Cline.\n\nFiring  \n[https://x.com/i/trending/1999388966668116413](https://x.com/i/trending/1999388966668116413)\n\nHiring  \n[https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html](https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html)\n\nMaybe Cline was burning more cash then it was bringing in and with the drama came a drop in subs as would any contraversy. Now Cline is part of a company that is clearly burning more cash than what it's making.",
          "score": 1,
          "created_utc": "2026-01-30 21:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ogwi0",
              "author": "crantob",
              "text": "Yea but why do some people get to *print* it is what I don't understand yet.\n\nWhen did that start?",
              "score": 3,
              "created_utc": "2026-01-30 21:30:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2u5d7h",
          "author": "turtleisinnocent",
          "text": "Same as Alex (XCode)\nFeels like regular de weeding",
          "score": 1,
          "created_utc": "2026-01-31 19:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u5pel",
          "author": "Emotional-Baker-490",
          "text": "The word to use is FOSS/Open source NOT Source available. Source available is a very specific term for a piece of software that is not free (as in freedom and as in beer), but has publicly available code.",
          "score": 1,
          "created_utc": "2026-01-31 19:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uwzgx",
          "author": "Apprehensive-Yam5278",
          "text": "sigh...uninstall..",
          "score": 1,
          "created_utc": "2026-01-31 21:19:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsn78m",
      "title": "Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site",
      "subreddit": "LocalLLaMA",
      "url": "https://www.404media.co/exposed-moltbook-database-let-anyone-take-control-of-any-ai-agent-on-the-site/",
      "author": "georgemoore13",
      "created_utc": "2026-02-01 03:25:12",
      "score": 411,
      "num_comments": 74,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/",
      "domain": "404media.co",
      "is_self": false,
      "comments": [
        {
          "id": "o2xs7oz",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-01 08:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wtb89",
          "author": "gnolruf",
          "text": "> Oâ€™Reilly said that he reached out to Moltbookâ€™s creator Matt Schlicht about the vulnerability and told him he could help patch the security. â€œHeâ€™s like, â€˜Iâ€™m just going to give everything to AI. So send me whatever you have.â€™â€\n\n\nYeah it's going to be a treasure trove for hackers for a while, even if this was patched. Imagine hearing of a major exploit on your fast growing platform and having this responseÂ ",
          "score": 305,
          "created_utc": "2026-02-01 03:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wzgyb",
              "author": "-p-e-w-",
              "text": "Isnâ€™t Moltbook essentially an art project where machines talk to each other for humans to laugh about? What is there to exploit?",
              "score": 123,
              "created_utc": "2026-02-01 04:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2x0m2w",
                  "author": "JohnDeere",
                  "text": "All the API keys used for the agents were leaked",
                  "score": 92,
                  "created_utc": "2026-02-01 04:31:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2yq0v3",
                  "author": "matthewjc",
                  "text": "It's no longer an art project where machines talk to each other if any human can take control of an agent and make posts.",
                  "score": 10,
                  "created_utc": "2026-02-01 13:08:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2z4jpr",
                  "author": "hyrumwhite",
                  "text": "The entire point is minimal human intervention. If a human can get in there and start messing with stuff, it loses that",
                  "score": 2,
                  "created_utc": "2026-02-01 14:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xezt6",
              "author": "honato",
              "text": "sheesh I was looking at it earlier because it sounds pretty neat but damn that's not even a red flag that's a big ass red banner.",
              "score": 4,
              "created_utc": "2026-02-01 06:17:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xzpw0",
                  "author": "Hegemonikon138",
                  "text": "Well it's an experiment, not for real use. I run mine inside a docker inside a VPS in another part of the world. The only keys it has are free tier keys and a google API with a budget limit.\n\nOne of the first thing I did was prompt injection attacks and it revealed all the keys within a minute or so of attempts.\n\nAs long as you understand the risks and keep them isolated, it's all good. I'm having fun.",
                  "score": 5,
                  "created_utc": "2026-02-01 09:25:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o32dsqv",
              "author": "LtCommanderDatum",
              "text": "It's almost like it's just a big PR scam and the guy's not serious about developing AI...",
              "score": 1,
              "created_utc": "2026-02-02 00:02:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39fgtc",
                  "author": "meganoob1337",
                  "text": "I would bet on rug pull as there exists a crypto recently created -> get more publicity from this leak = profit? ðŸ˜… I feel like reading all the molt/claw whatever shit sounds like astroturfing Everytime I read about it",
                  "score": 1,
                  "created_utc": "2026-02-03 01:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wzeze",
          "author": "hidden2u",
          "text": "easy, next time just make sure to tell the AI to add security",
          "score": 68,
          "created_utc": "2026-02-01 04:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xryfj",
              "author": "physalisx",
              "text": "And \"don't make mistakes\"",
              "score": 28,
              "created_utc": "2026-02-01 08:12:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2wye4p",
          "author": "gnnr25",
          "text": "Oh boy, this is gonna be interesting\n\nhttps://i.redd.it/6cm86n507tgg1.gif",
          "score": 24,
          "created_utc": "2026-02-01 04:16:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x6agh",
          "author": "thetaFAANG",
          "text": "Its a honeypot lol its not supposed to be anything secure",
          "score": 21,
          "created_utc": "2026-02-01 05:11:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wx5v8",
          "author": "Daemontatox",
          "text": "Ladies and gentlemen,  the fall of vibe frameworks",
          "score": 48,
          "created_utc": "2026-02-01 04:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xzbii",
              "author": "Amphiitrion",
              "text": "It's more about people who know what they're doing vs people who has zero clue about programming",
              "score": 8,
              "created_utc": "2026-02-01 09:21:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xt8kc",
              "author": "Cupakov",
              "text": "Moltbook is basically a Reddit simulator for bots, not a frameworkÂ ",
              "score": 24,
              "created_utc": "2026-02-01 08:24:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zbzrn",
          "author": "TechExpert2910",
          "text": "the article misses this huge fact while talking about this â€œomg humans can control the posts flawâ€:\n\n**people could completely control the agentâ€™s posts *anyway***.\n\nyou can simply ask your agent to go post about [insert headline generating thing]\n\nitâ€™s likely that a ton of moltbook posts are just human driven anyway, so this flaw thatâ€™s been found isnâ€™t really consequential in any way",
          "score": 12,
          "created_utc": "2026-02-01 15:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31yn5h",
              "author": "Salted_Fried_Eggs",
              "text": "What a weird time, I'm often skeptical about comments on reddit being an AI bot, and now we're skeptical that AI bot comments are actually human haha",
              "score": 2,
              "created_utc": "2026-02-01 22:40:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37oojp",
                  "author": "TechExpert2910",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-02-02 19:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xcnzs",
          "author": "Ok-Pipe-5151",
          "text": "Entire clawd/openclaw/molt thing is vibe coded without any follow up validation/proofreading by developers. What do you expect? It IS a vibeslop, no matter how popular it has got in last few days (also I firmly believe that more than half of github stars are also from bots)\n\n\nAlso anyone who lets apps like these full system access in sensitive applications (e.g. WhatsApp, gmail etc) absolutely deserves to be exploited. Best security tips for consumers is common sense, which most users seriously lack.",
          "score": 30,
          "created_utc": "2026-02-01 05:59:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yp2bh",
              "author": "SkyFeistyLlama8",
              "text": "There's plenty of irony in Clawd/Moltbot/Openclaw being vibecoded by some guy who made a shit ton of money from more traditional software. Molt_book_ is some crazy AI social media platform cooked up using Openclaw.\n\nI wouldn't touch Openclaw, let alone other derivative projects that allow an LLM to act as you.",
              "score": 5,
              "created_utc": "2026-02-01 13:01:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xpguq",
              "author": "droptableadventures",
              "text": "It's inevitable - Simon Willison coined the term [Lethal Trifecta](https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/). Give it access to private data, access to external communication, and exposure to untrusted content.\n\nOnly here we just skipped all that by also giving it full control of the software (a fourth pillar?).",
              "score": 3,
              "created_utc": "2026-02-01 07:50:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y9xwu",
          "author": "No_Afternoon_4260",
          "text": "If you want to read more: [the glass box paradox ](https://professorsigmund.com/pdfs/glass_box_paradox.pdf)",
          "score": 4,
          "created_utc": "2026-02-01 10:58:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xrc24",
          "author": "IHave2CatsAnAdBlock",
          "text": "This is BS. The only thing this leak can be done is to allow someone else to post in the name of your agent.",
          "score": 14,
          "created_utc": "2026-02-01 08:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z5l5h",
              "author": "hyrumwhite",
              "text": "That seems like it ruins the entire premise of the projectÂ ",
              "score": 4,
              "created_utc": "2026-02-01 14:40:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o344u5m",
              "author": "FPham",
              "text": "And then the agent discovers how badly it wanted to publish all users details for some reasons in its previous posts.",
              "score": 1,
              "created_utc": "2026-02-02 06:39:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o347zkk",
                  "author": "IHave2CatsAnAdBlock",
                  "text": "There are no users details. The agent itself is the user. That connects via api and posts. Why are you talking if you have no idea ?",
                  "score": 1,
                  "created_utc": "2026-02-02 07:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2wzao2",
          "author": "mr_zerolith",
          "text": "That was quick",
          "score": 6,
          "created_utc": "2026-02-01 04:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32eih0",
              "author": "LtCommanderDatum",
              "text": "It's only fair that the \"fastest growing open source project in history\" would also be the \"fastest hacked open source project in history.\"",
              "score": 3,
              "created_utc": "2026-02-02 00:06:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2x9hhi",
          "author": "SituationMan",
          "text": "What does Moltbook do? What do people get out of it?",
          "score": 5,
          "created_utc": "2026-02-01 05:34:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2xrhdn",
              "author": "IHave2CatsAnAdBlock",
              "text": "It is a good laugh. Basically watch conversations between agents. TBH the level of conversation in many topics is orders of magnitude higher than fb or x",
              "score": 17,
              "created_utc": "2026-02-01 08:08:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z34ww",
                  "author": "AmusingVegetable",
                  "text": "Well, the conversation level on fb and x is a pretty low barâ€¦",
                  "score": 8,
                  "created_utc": "2026-02-01 14:27:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xc9bh",
              "author": "breksyt",
              "text": "People get out of it that singularity is not here yet.",
              "score": 5,
              "created_utc": "2026-02-01 05:56:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2xbqk1",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 7,
              "created_utc": "2026-02-01 05:51:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ypyua",
                  "author": "PunnyPandora",
                  "text": "If having fun means being in a cult shit sign me up boss",
                  "score": 2,
                  "created_utc": "2026-02-01 13:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30t539",
          "author": "Distinct-Expression2",
          "text": "\"Im just going to give everything to AI\" is a wild response to \"your database is exposed.\"",
          "score": 2,
          "created_utc": "2026-02-01 19:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34cyis",
          "author": "Senior_Delay_5362",
          "text": "This is Westworld in the flesh",
          "score": 1,
          "created_utc": "2026-02-02 07:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x4gns",
          "author": "RottenPingu1",
          "text": "A reminder to never rush to the new and sparkly tech or software",
          "score": -1,
          "created_utc": "2026-02-01 04:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x3whz",
          "author": "KindMonitor6206",
          "text": "all the accounts on moltbook seem deleted right now. any idea what thats about?",
          "score": -2,
          "created_utc": "2026-02-01 04:54:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2x81sj",
              "author": "lolxdmainkaisemaanlu",
              "text": "mine is fine",
              "score": 6,
              "created_utc": "2026-02-01 05:23:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2z3rlx",
              "author": "Ok_Milk1045",
              "text": "I cant authÂ ",
              "score": 2,
              "created_utc": "2026-02-01 14:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xh71k",
          "author": "dgibbons0",
          "text": "Openclaw as a framework for building quick and easy ai based bots is actually pretty great, if someone builds some reasonable structure around it to package a fixed set of resources it'll be amazing... But taking a system that's already at risk of prompt injection and specifically throwing it at a bot centric social network is the definition of stupid.",
          "score": -5,
          "created_utc": "2026-02-01 06:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2wvh9f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2026-02-01 03:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wwt46",
              "author": "SunshineSeattle",
              "text": "/r/masterhacker <----- this away",
              "score": 4,
              "created_utc": "2026-02-01 04:05:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yg6em",
                  "author": "SkyFeistyLlama8",
                  "text": "/r/haxX0r",
                  "score": 1,
                  "created_utc": "2026-02-01 11:53:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtjhc8",
      "title": "Step-3.5-Flash (196b/A11b) outperforms GLM-4.7 and DeepSeek v3.2",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qtjhc8",
      "author": "ResearchCrafty1804",
      "created_utc": "2026-02-02 03:07:42",
      "score": 385,
      "num_comments": 163,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtjhc8/step35flash_196ba11b_outperforms_glm47_and/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o34gyei",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-02 08:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33uqx1",
          "author": "ortegaalfredo",
          "text": "Just tried it in openrouter. I didn't expected much as its too small and too fast, and seems to be benchmaxxed. But..\n\nWow. It actually seems to be the real thing. In my tests is even better than Kimi K2.5. It's at the level of Deepseek 3.2 Speciale or Gemini 3.0 Flash. It thinks a lot, though.",
          "score": 86,
          "created_utc": "2026-02-02 05:18:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33yemt",
              "author": "SpicyWangz",
              "text": "Yeah, crazy amount of reasoning tokens for simple answers. But it seems to have a decent amount of knowledge. Curious to see more results here",
              "score": 24,
              "created_utc": "2026-02-02 05:46:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36s57w",
                  "author": "Critttt",
                  "text": "Agree. It does so much thinking that the speed overall comes out as maybe 1/2 the speed of Gemini Flash 3. But as you say the final output is worth it and for its size and open source status, very impressive.",
                  "score": 5,
                  "created_utc": "2026-02-02 17:22:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36xqff",
              "author": "munkiemagik",
              "text": "FFS another nice new model development to lure me back into looking at adding more 3090 into my build, while prices are rising.\n\nFor a while thereâ€™s been little incentive for me to go beyond my current 80GB VRAM I am running (1x5090 & 2x3090) with GLM 4.5 Air (P-I3) and GPT-OSS-120b as my mains and many ohter smaller models. This makes 1 or 2 more 3090 seem like a possibly good call. Minimax M2.1 didn't tempt me as I would have only been able to fit the REAP'ed models.",
              "score": 2,
              "created_utc": "2026-02-02 17:48:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34g5vn",
              "author": "rm-rf-rm",
              "text": "what tests did you run?",
              "score": 2,
              "created_utc": "2026-02-02 08:22:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34m7yu",
                  "author": "ortegaalfredo",
                  "text": "Cibersecurity, static software analysis, vulnerability finding, etc. It's a little different that the usual code benchmark, so I get slightly different results.",
                  "score": 6,
                  "created_utc": "2026-02-02 09:21:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33i2dg",
          "author": "pmttyji",
          "text": "Good to have one more model in this size range.\n\nIts Size is less than models like MiniMax, Qwen3-235B.\n\n**EDIT:**\n\nOpen PRs for this model on llama.cpp\n\n[https://github.com/ggml-org/llama.cpp/pull/19271](https://github.com/ggml-org/llama.cpp/pull/19271)\n\n[https://github.com/ggml-org/llama.cpp/pull/19283](https://github.com/ggml-org/llama.cpp/pull/19283) \\- PR opened by Authors of this model",
          "score": 77,
          "created_utc": "2026-02-02 03:52:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33tbk3",
          "author": "CondiMesmer",
          "text": "Nice job stepbro",
          "score": 59,
          "created_utc": "2026-02-02 05:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34c8g0",
              "author": "BillyQ",
              "text": "Help, I'm stuck!",
              "score": 11,
              "created_utc": "2026-02-02 07:45:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37342a",
              "author": "blurredphotos",
              "text": "underrated comment",
              "score": 3,
              "created_utc": "2026-02-02 18:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37071a",
              "author": "vaporeonlover6",
              "text": "it this not for NSFW roleplay? I'm genuinely super confused",
              "score": 2,
              "created_utc": "2026-02-02 17:59:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34f2qe",
          "author": "LosEagle",
          "text": "at code\n\n\nThis should always be mentioned in sentences where somebody claims \"x beats y\" but they mean it's in coding.Â ",
          "score": 19,
          "created_utc": "2026-02-02 08:12:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33su9h",
          "author": "MikeLPU",
          "text": " Well classic - GGUF WHEN!!! :)",
          "score": 35,
          "created_utc": "2026-02-02 05:04:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3433do",
              "author": "spaceman_",
              "text": "[https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main) has GGUF files (split similarly to mradermarcher releases)",
              "score": 11,
              "created_utc": "2026-02-02 06:24:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3436pz",
                  "author": "MikeLPU",
                  "text": "Looks like it requires his custom llamacpp version.",
                  "score": 9,
                  "created_utc": "2026-02-02 06:25:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o340y2w",
          "author": "EbbNorth7735",
          "text": "Every 3.5 months the knowledge density doubles. It's been a fun ride. Every cycle people are surprised.",
          "score": 49,
          "created_utc": "2026-02-02 06:06:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o348pr8",
              "author": "[deleted]",
              "text": "Iâ€™m sure the density has to hit a limit at some point, just not sure where that is.",
              "score": 28,
              "created_utc": "2026-02-02 07:13:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34d1k4",
                  "author": "dark-light92",
                  "text": "I think the only limits we have actually hit are at sub 10b models. Like Qwen3 4b and Llama 3 8b. The models that noticeably degrade with quantization.\n\nI don't think we are close to hitting the limits for > 100B models. Not exactly sure how exactly it works for dense vs MoE.",
                  "score": 23,
                  "created_utc": "2026-02-02 07:53:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o34pns6",
                  "author": "Mart-McUH",
                  "text": "I think to some degree it kind of already did. These new models are usually great at STEM (where the density increased) but suffer in normal language tasks. So things are already being sacrificed to gain performance in certain area. Of course it could be because of unbalanced training data, but I suspect that needs to be done because you can't cramp everything in there anymore.",
                  "score": 3,
                  "created_utc": "2026-02-02 09:54:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b4v40",
              "author": "Nashadelic",
              "text": "Almost like a Fun Step",
              "score": 1,
              "created_utc": "2026-02-03 08:16:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o352egz",
          "author": "Septerium",
          "text": "Did a small test here asking it (in portuguese) to generate a C code for simulating the Hodgkin-Huxley model and a python script to plot the results. It did everything right (even the model parameters), blazing fast\n\nhttps://preview.redd.it/y4a4enfnk2hg1.png?width=1040&format=png&auto=webp&s=53494f013bebc0d59f1e52bac81c4ff506f6b384",
          "score": 13,
          "created_utc": "2026-02-02 11:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o370u1o",
              "author": "vaporeonlover6",
              "text": "neuron activation joke?",
              "score": 2,
              "created_utc": "2026-02-02 18:02:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39ghpi",
                  "author": "Septerium",
                  "text": "No, but it would have been a good one ðŸ˜‚",
                  "score": 2,
                  "created_utc": "2026-02-03 01:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33wl7j",
          "author": "jacek2023",
          "text": "that's actually a great news, and looks like it's supported by llama.cpp (well, it's a fork)\n\nI think MiniMax is A10B and this one is A11B but overall only 196B is needed (so less offloading)\n\nGGUF Model Weights(int4): 111.5 GB\n\nEDIT OK guys this is gguf, just the strange name ;)\n\n[https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4)",
          "score": 16,
          "created_utc": "2026-02-02 05:31:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34h353",
              "author": "tarruda",
              "text": "This seems like the ideal big LLM for a 128GB setup\n\nJust built their llama.cpp fork and started downloading the weights to see how well it performs.",
              "score": 8,
              "created_utc": "2026-02-02 08:31:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35fvc4",
                  "author": "muyuu",
                  "text": "worth a post if you get this working on a Strix Halo 128GB machine\n\nI'd give it a shot but I have a lot on my plate right now",
                  "score": 3,
                  "created_utc": "2026-02-02 13:22:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o34vpe0",
              "author": "Most_Drawing5020",
              "text": "I tested the Q4 gguf, working, but not so great compared to openrouter one. In my certain task in Roo Code, the Q4 gguf outputs a file that loops itself, while the openrouter model's output is perfect.",
              "score": 3,
              "created_utc": "2026-02-02 10:51:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34xxd8",
              "author": "AvailableSlice6854",
              "text": "they mention multi token prediction, so prob significantly faster than minimax.",
              "score": 2,
              "created_utc": "2026-02-02 11:11:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33b610",
          "author": "Haoranmq",
          "text": "all \\~5% activation",
          "score": 16,
          "created_utc": "2026-02-02 03:11:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33hnr0",
          "author": "segmond",
          "text": "Only time will tell...",
          "score": 16,
          "created_utc": "2026-02-02 03:50:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33bdj5",
          "author": "Recoil42",
          "text": "[https://static.stepfun.com/blog/step-3.5-flash/](https://static.stepfun.com/blog/step-3.5-flash/)",
          "score": 14,
          "created_utc": "2026-02-02 03:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ofad",
          "author": "Saren-WTAKO",
          "text": "DGX Spark llama-bench\n\n\n\n\\[saren@magi \\~/Step-3.5-Flash/llama.cpp (git)-\\[main\\] \\]% ./build-cuda/bin/llama-bench -m ./models/step3p5\\_flash\\_Q4\\_K\\_S/step3p5\\_flash\\_Q4\\_K\\_S.gguf  -fa 1 -mmp 0 -d 0,4096,8192,16384,32768 -p 2048 -ub 2048 -n 32\n\nggml\\_cuda\\_init: found 1 CUDA devices:\n\nDevice 0: NVIDIA GB10, compute capability 12.1, VMM: yes\n\n| model                          |       size |     params | backend    | ngl | n\\_ubatch | fa | mmap |            test |                  t/s |\n\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -------: | -: | ---: | --------------: | -------------------: |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |          pp2048 |        862.87 Â± 1.86 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |            tg32 |         26.85 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |  pp2048 @ d4096 |        826.63 Â± 2.43 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |    tg32 @ d4096 |         24.84 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |  pp2048 @ d8192 |        799.66 Â± 2.96 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |    tg32 @ d8192 |         24.50 Â± 0.14 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 | pp2048 @ d16384 |        738.55 Â± 2.49 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |   tg32 @ d16384 |         23.04 Â± 0.12 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 | pp2048 @ d32768 |       645.49 Â± 11.37 |\n\n| step35 ?B Q4\\_K - Small         | 103.84 GiB |   196.96 B | CUDA       |  99 |     2048 |  1 |    0 |   tg32 @ d32768 |         20.51 Â± 0.09 |\n\nbuild: 5ef1982 (7)\n\n./build-cuda/bin/llama-bench -m  -fa 1 -mmp 0 -d 0,4096,8192,16384,32768 -p    144.41s user 64.78s system 91% cpu 3:47.94 total",
          "score": 6,
          "created_utc": "2026-02-02 09:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3534km",
              "author": "tarruda",
              "text": "Also ran the bench on M1 ultra:\n\n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.014 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | ---: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |          pp2048 |        380.57 Â± 0.34 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |            tg32 |         35.00 Â± 0.24 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |  pp2048 @ d4096 |        353.07 Â± 0.21 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |    tg32 @ d4096 |         33.69 Â± 0.05 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |  pp2048 @ d8192 |        330.58 Â± 0.15 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |    tg32 @ d8192 |         32.84 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 | pp2048 @ d16384 |        292.92 Â± 0.10 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |   tg32 @ d16384 |         31.03 Â± 0.11 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 | pp2048 @ d32768 |        236.59 Â± 0.15 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |      16 |     2048 |  1 |    0 |   tg32 @ d32768 |         27.92 Â± 0.11 |\n    \n    build: a0dce6f (24)",
              "score": 5,
              "created_utc": "2026-02-02 11:54:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34otbi",
              "author": "Saren-WTAKO",
              "text": "fuck I crashed my spark remotely by OOM, again.",
              "score": 1,
              "created_utc": "2026-02-02 09:46:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35nm4m",
              "author": "coder543",
              "text": "How much context can you fit with their Int4 quant on DGX Spark? I haven't had time to download and set this up yet, but I am thrilled that the model is <200B parameters so there is a chance it can fit without going below 4-bit.",
              "score": 1,
              "created_utc": "2026-02-02 14:06:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36uucj",
                  "author": "Saren-WTAKO",
                  "text": "only tried 65536. llama.cpp fit wanted 140k or something and that crashed my spark.",
                  "score": 1,
                  "created_utc": "2026-02-02 17:35:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3561mb",
          "author": "No-Volume6352",
          "text": "I've been testing Step 3.5 Flash (free) via Openrouter. Just started tinkering with it, but it's quite impressive.\n\n1: Proper agent tool usage  \n\nI used my custom Langchain + LangGraph agent for complex tasks like code editing and web search, and it handled them competently.\n- Models such as Gemini, Grok, Deepseek: seem to struggle with tool integration.\n- GLM4.7 and Step-3.5-Flash: demonstrate skillful tool use.\n\n2: Speed  \n\nLatency and throughput are critical for agent workflows. GLM4.7 and Deepseek feel agonizingly slowâ€”waiting makes me feel like I'm fossilizing. Even gemini flash seems sluggish. Only grok-level speed is tolerable.\nStep-3.5-Flash, however, matches grok's responsiveness while also excelling in agent behavior.\nI was anxious that it might be my implementation issue, but this model suggests otherwise.\nI'm thrilled that such capable options are emerging so swiftly.",
          "score": 6,
          "created_utc": "2026-02-02 12:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34g5ct",
          "author": "Acceptable_Home_",
          "text": "Woah, just 2 months ago they were making small VL models to control phone ui, and they outdid everyone in the niche, now they're out here competition some of the biggest dawgs, hope they keepnwinning, would go check their papers!Â ",
          "score": 6,
          "created_utc": "2026-02-02 08:22:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34l9jw",
          "author": "Aggressive-Bother470",
          "text": "What's the verdict so far?Â \n\n\nBenchmaxxed or epic?",
          "score": 4,
          "created_utc": "2026-02-02 09:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35199a",
              "author": "mark_33_",
              "text": "From what ive seen, very solid agentic performance so far, and extremely fast. Testing with Roo Code and its able to perform actions really well, no errors so far. find its performance less strong when having to deal with tons of context.",
              "score": 11,
              "created_utc": "2026-02-02 11:39:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o359mn1",
              "author": "a_beautiful_rhind",
              "text": "It is dropping random chinese characters in replies and sometimes getting extra </think> tags.. \n\nDecent but not epic.",
              "score": 7,
              "created_utc": "2026-02-02 12:42:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37oixl",
                  "author": "alexeiz",
                  "text": "I tried it on https://stepfun.ai/chats and for a prompt in English, the response was all Chinese including reasoning.",
                  "score": 2,
                  "created_utc": "2026-02-02 19:50:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33hray",
          "author": "pigeon57434",
          "text": "They also say they outperform K2.5 im highly skeptical that so soon an only 200B model is already beating the 1T Kimi-K2.5 ive used it a little on their website and its reasoning traces have a significantly different feel and i think k2.5 is probably still a little smarter but it seems promising enough i suppose",
          "score": 14,
          "created_utc": "2026-02-02 03:50:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33wb49",
              "author": "ortegaalfredo",
              "text": "In my tests(code comprehension) is clearly better thank K2.5, and at the level of K2, as my tests showed that 2.5 is not as good as 2.0.",
              "score": -7,
              "created_utc": "2026-02-02 05:29:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o342rrk",
          "author": "spaceman_",
          "text": "Stepfun is a weird choice for a company name.",
          "score": 17,
          "created_utc": "2026-02-02 06:21:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34nqm2",
              "author": "pfn0",
              "text": "stepfunction is pretty reasonable.",
              "score": 10,
              "created_utc": "2026-02-02 09:36:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o34q5j8",
              "author": "Brilliant-Weekend-68",
              "text": "Only a weird choice if you have a crippling porn addiction :)",
              "score": 9,
              "created_utc": "2026-02-02 09:59:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34xfn6",
                  "author": "spaceman_",
                  "text": "You point at me, and yet you got the point, didn't you?",
                  "score": 7,
                  "created_utc": "2026-02-02 11:06:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35ql03",
              "author": "pseudonerv",
              "text": "So this model must be good at creative writing. Is it?",
              "score": 2,
              "created_utc": "2026-02-02 14:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33b6qn",
          "author": "Available-Craft-5795",
          "text": "Neat, ill have to prune it sometime.",
          "score": 3,
          "created_utc": "2026-02-02 03:11:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34pcsf",
          "author": "TacGibs",
          "text": "https://preview.redd.it/66kiwjhsz1hg1.jpeg?width=640&format=pjpg&auto=webp&s=d98a576a5b8bf5f435f73c3a12fd2a0b12741cb7",
          "score": 5,
          "created_utc": "2026-02-02 09:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35oyfy",
          "author": "jacek2023",
          "text": "u/ilintar is doing things\n\n[https://github.com/ggml-org/llama.cpp/pull/19271](https://github.com/ggml-org/llama.cpp/pull/19271)",
          "score": 4,
          "created_utc": "2026-02-02 14:13:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36cjz5",
              "author": "Rabooooo",
              "text": "Seems like the stepfun team is pushing their own PR tomorrow that they will maintain over time.. [https://github.com/ggml-org/llama.cpp/pull/19271#issuecomment-3835833362](https://github.com/ggml-org/llama.cpp/pull/19271#issuecomment-3835833362)",
              "score": 3,
              "created_utc": "2026-02-02 16:10:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3628yt",
          "author": "Lan_BobPage",
          "text": "How's creative writing compared to GLM 4.7?",
          "score": 5,
          "created_utc": "2026-02-02 15:22:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o375ai8",
              "author": "lookitsthesun",
              "text": "I mean it's not going to be the intended use case but it will probably be quite good because its internal logic and reasoning is solid. You can test it out on the demo.\n\nProbably would need to wait for someone to derestrict/abliterate it though",
              "score": 2,
              "created_utc": "2026-02-02 18:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37kub0",
                  "author": "Lan_BobPage",
                  "text": "Hmm not a fan of abliteration we'll see about that. ",
                  "score": 1,
                  "created_utc": "2026-02-02 19:33:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35e6o1",
          "author": "tarruda",
          "text": "Definitely passes my \"vibe checks\". Feels as strong as Minimax M2.1 and GLM 4.7, while completely fitting on 128GB (the int4 GGUF) devices with full 256k context. Context RAM usage is the most efficient I've seen so far.\n\nNot only that, it is very fast. I'm running this on a M1 Ultra and it is doing 30+ tokens/second. This is similar to Minimax M2.1 with 0 context, but I notice very little speed degradation as the context increases. \n\nSo far it is looking like a gem. Only downside is that it can use a lot of reasoning tokens, which seems perfect for llama.cpp new ngram speculative decoding.",
          "score": 7,
          "created_utc": "2026-02-02 13:12:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eyrta",
              "author": "Medical_Farm6787",
              "text": "Are you using lm studio or mlx_lm",
              "score": 1,
              "created_utc": "2026-02-03 21:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ezltb",
                  "author": "tarruda",
                  "text": "Using llama.cpp directly",
                  "score": 1,
                  "created_utc": "2026-02-03 21:30:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33t2w8",
          "author": "skinnyjoints",
          "text": "Is this a new lab? This is the first Iâ€™m hearing of them",
          "score": 10,
          "created_utc": "2026-02-02 05:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33w73h",
              "author": "limoce",
              "text": "No, this is already v3.5. They have been training large models for several years. Previous StepFun models are not outstanding among direct competitors (DeepSeek, Qwen, MiniMax, GLM, ...)",
              "score": 25,
              "created_utc": "2026-02-02 05:28:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o33znye",
                  "author": "skinnyjoints",
                  "text": "Do they have a niche they excel in?",
                  "score": 2,
                  "created_utc": "2026-02-02 05:56:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33ug1s",
          "author": "Zundrium",
          "text": "Interesting to see how well it performs.",
          "score": 3,
          "created_utc": "2026-02-02 05:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34fsg8",
          "author": "Dudensen",
          "text": "Step 3 was sooo good when it came out. It went by a bit without much fanfare. If this is better than that then it's good enough. Their step 3 report paper also had some interesting attention innovations.",
          "score": 3,
          "created_utc": "2026-02-02 08:18:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34v9dc",
          "author": "oxygen_addiction",
          "text": "It seems pretty smart and fast but holy reasoning token usage Batman.\n\nSelf-speculative decoding would really help this one out, as it repeats itself a ton.",
          "score": 3,
          "created_utc": "2026-02-02 10:47:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34wo86",
              "author": "tarruda",
              "text": "This is mentioned in the \"Limitations, known issues and future direction\" section:\n\n> Token Efficiency. Step 3.5 Flash achieves frontier-level agentic intelligence but currently relies on longer generation trajectories than Gemini 3.0 Pro to reach comparable quality.",
              "score": 4,
              "created_utc": "2026-02-02 11:00:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o33jokm",
          "author": "Worldly-Cod-2303",
          "text": "Me when I benchmax and claim to beat a very recent model that is 5x the size",
          "score": 17,
          "created_utc": "2026-02-02 04:02:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33vg5l",
              "author": "bjodah",
              "text": "Beating deepseek-v3.2 in agentic coding is not a high bar. The evaluations (have it write JNI bindings for a C++ lib) I've done using open code puts it significantly below MiniMax-M2.1 (not to mention GLM-4.7 and Kimi-K2.5).",
              "score": 12,
              "created_utc": "2026-02-02 05:23:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34txo1",
                  "author": "oxygen_addiction",
                  "text": "How did you run it in Opencode?",
                  "score": 1,
                  "created_utc": "2026-02-02 10:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35inhg",
              "author": "Embarrassed_Bread_16",
              "text": "final proof that size doesnt matter ;)))",
              "score": 1,
              "created_utc": "2026-02-02 13:38:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36zqe2",
                  "author": "Noobysz",
                  "text": "thats why your embarressed?",
                  "score": 0,
                  "created_utc": "2026-02-02 17:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o33k0mw",
          "author": "FullOf_Bad_Ideas",
          "text": "Awesome. Their StepVL is good, and from their closed products, their due diligence tool is amazing. StepFun 3 was awesome from engineering perspective (decoupling computation of attention and FFNs to different devices) but I don't think it landed well when it comes to benchmarks & expectations VS real use quality.",
          "score": 7,
          "created_utc": "2026-02-02 04:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o341x90",
          "author": "RegularRecipe6175",
          "text": "Anyone used the custom llama in their repo? The model is not recognized in the latest llama.",
          "score": 2,
          "created_utc": "2026-02-02 06:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34ogli",
          "author": "Fancy_Fanqi77",
          "text": "How about comparing it to Minimax-M2.1?",
          "score": 2,
          "created_utc": "2026-02-02 09:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34ziwj",
              "author": "LegacyRemaster",
              "text": "check here: [https://huggingface.co/stepfun-ai/Step-3.5-Flash](https://huggingface.co/stepfun-ai/Step-3.5-Flash)",
              "score": 2,
              "created_utc": "2026-02-02 11:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35l4a8",
              "author": "DOAMOD",
              "text": "I've tried it for a while and it nailed a frontend integration at lightning speed, only one simple error. Perhaps I'm being hasty, but the feeling is that it's better than MiniMax2.1. Maybe in practice they'll be similar, we'll see, but I've been impressed by the first experience. Congratulations to the Step team.",
              "score": 2,
              "created_utc": "2026-02-02 13:52:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34ueq8",
          "author": "Expensive-Paint-9490",
          "text": "I wonder why so many labs put \"Flash\" in their model names. It's not like it has a standard meaning.",
          "score": 2,
          "created_utc": "2026-02-02 10:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3620e5",
              "author": "GreenGreasyGreasels",
              "text": "To signal that it is \"fast\" and also that a big \"pro\" is coming i guess.\n  \n  \nAlso Chinese labs tend to pick up the nomenclature and branding styles popularized by Google/Anthropic/OpenAI as they don't have an innate understanding of the western market (from a branding marketing perspective) and are content to reuse themes and styles that are current - which I largely think it wise at this stage.",
              "score": 6,
              "created_utc": "2026-02-02 15:20:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i4hcb",
                  "author": "crantob",
                  "text": "I love a succinct explanation that clarifies something for me. Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-04 09:36:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34x5ud",
          "author": "Grouchy-Bed-7942",
          "text": "From what I've tested, it's at least of Minimax m2.1 quality in development.",
          "score": 2,
          "created_utc": "2026-02-02 11:04:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354ihb",
          "author": "NucleusOS",
          "text": "the livecode bench gap (86.4 vs 83.3) is impressive for a smaller model. wonder if it's architecture or training data quality.\n\n\n\nanyone tested it locally yet",
          "score": 2,
          "created_utc": "2026-02-02 12:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35kqc4",
              "author": "Zc5Gwu",
              "text": "Have to wait for the next iteration of live code to be sure.Â ",
              "score": 1,
              "created_utc": "2026-02-02 13:50:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3574oh",
          "author": "a_beautiful_rhind",
          "text": "I tried it a little bit and seems decent for oneshots. Very similar to trinity large from acree.",
          "score": 2,
          "created_utc": "2026-02-02 12:24:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35gh89",
              "author": "silenceimpaired",
              "text": "Any thoughts on fiction/creative writing?",
              "score": 3,
              "created_utc": "2026-02-02 13:25:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35h2yg",
                  "author": "a_beautiful_rhind",
                  "text": "Seemed like it did ok, but I only tried short chats.",
                  "score": 2,
                  "created_utc": "2026-02-02 13:29:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35joak",
          "author": "DOAMOD",
          "text": "I've tried it out a bit and it's really surprised me, it seems pretty good. It's incredible that we have something like this. Will int4 be very damaged in Q2/Q3?",
          "score": 2,
          "created_utc": "2026-02-02 13:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3710dx",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/isk79iedf4hg1.png?width=1154&format=png&auto=webp&s=d6cacbea97e2a5cbd8ec886e94012872a1bc839c\n\namazing",
          "score": 2,
          "created_utc": "2026-02-02 18:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eew59",
          "author": "laterbreh",
          "text": "Dont know about hype but for those curious about speed:\n\nVllm nightly, 3x rtx pros in pipeline parallel mode.\n\nSingle prompt \"build a landing page\"\n\nFP8 version sustained 65tps (no spec decode) in pipeline parallel with a simple \"build me a single html landing page for <whatever>\". \n\nNo tweaks or tuning. Just \"make it work\" config.   \n  \nImpressive.",
          "score": 2,
          "created_utc": "2026-02-03 19:53:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3inmj9",
          "author": "Disastrous_Salad_910",
          "text": "it didnt even manage to call the tools on cline correctly",
          "score": 2,
          "created_utc": "2026-02-04 12:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33otew",
          "author": "MrMrsPotts",
          "text": "Is there any way to try this out online?",
          "score": 1,
          "created_utc": "2026-02-02 04:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33rys8",
              "author": "SpicyWangz",
              "text": "[https://stepfun.ai](https://stepfun.ai)",
              "score": 9,
              "created_utc": "2026-02-02 04:58:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3430ix",
              "author": "Abject-Ranger4363",
              "text": "Free on OpenRouter (for now): [https://openrouter.ai/chat?models=stepfun/step-3.5-flash:free](https://openrouter.ai/chat?models=stepfun/step-3.5-flash:free)",
              "score": 2,
              "created_utc": "2026-02-02 06:23:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o343bf4",
                  "author": "MrMrsPotts",
                  "text": "Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-02 06:26:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34xkpw",
          "author": "Big-Pause-6691",
          "text": "Tried this on OpenRouter. It outputs fast as hell lol, and it seems really damn good at solving competition-style problems.",
          "score": 1,
          "created_utc": "2026-02-02 11:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34xqq0",
          "author": "CLGWallpaperGuy",
          "text": "Wow. the model is pretty damn good at coding",
          "score": 1,
          "created_utc": "2026-02-02 11:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34zj27",
          "author": "fairydreaming",
          "text": "Tested in lineage-bench:\n\n    $ cat ../lineage-bench-results/lineage-8_64_128_192/glm-4.7/glm-4.7_*.csv ../lineage-bench-results/lineage-8_64_128_192/deepseek-v3.2/deepseek-v3.2_*.csv results/temp_1.0/step-3.5-flash_*.csv|./compute_metrics.py --relaxed\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | deepseek/deepseek-v3.2 |     0.956 |       1.000 |        1.000 |         0.975 |         0.850 |\n    |    2 | z-ai/glm-4.7           |     0.794 |       1.000 |        0.750 |         0.750 |         0.675 |\n    |    3 | stepfun/step-3.5-flash |     0.769 |       1.000 |        0.700 |         0.725 |         0.650 |\n\nScore is indeed close to GLM-4.7. Unfortunately it often interrupts the reasoning early for unknown reason and fails to generate an answer. I've also seen some infinite loops. Best results so far are with temp 1.0, top-p 0.95. Model authors recommend temp 0.6, top-p 0.95.",
          "score": 1,
          "created_utc": "2026-02-02 11:25:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o351kif",
              "author": "Big-Pause-6691",
              "text": "I canâ€™t seem to find the authorâ€™s recommended sampling params anywhere. Whatâ€™s it like w. t=1 and top-p=1? Any noticeable diff?",
              "score": 1,
              "created_utc": "2026-02-02 11:42:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35249c",
                  "author": "fairydreaming",
                  "text": "https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/3#6980720b945ef5272b15db80",
                  "score": 1,
                  "created_utc": "2026-02-02 11:46:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35f4bl",
                  "author": "tarruda",
                  "text": "I'm using only --temp 1.0 as recommended in HF repo.",
                  "score": 1,
                  "created_utc": "2026-02-02 13:17:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35fu2i",
                  "author": "fairydreaming",
                  "text": "    $ cat results/temp_1.0_topp_1.0/step-3.5-flash_*.csv|./compute_metrics.py --relaxed\n    |   Nr | model_name             |   lineage |   lineage-8 |   lineage-64 |   lineage-128 |   lineage-192 |\n    |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:|\n    |    1 | stepfun/step-3.5-flash |     0.750 |       1.000 |        0.850 |         0.750 |         0.400 |\n\nHmm, with temp 1.0 and top-p 1.0 scores are a bit better for simpler quizzes, worse for most complex lineage-192. Note that I have output limited to 64k tokens.",
                  "score": 1,
                  "created_utc": "2026-02-02 13:22:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b7hao",
              "author": "LegacyRemaster",
              "text": "I had a loop problem only once using kilocode + vscode. Solution: Paused, killed the llamacpp process, reloaded with a 90k context limit and Q8 context quantization. Restarted llamacpp (no temperature or repeat penalty options: default). It finished the task correctly.",
              "score": 1,
              "created_utc": "2026-02-03 08:42:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b7lyd",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/bqjdllwfs8hg1.png?width=2379&format=png&auto=webp&s=c9842462192af8ade55d7b744f658f7a95e392e3\n\nalso: condense context helps.",
              "score": 1,
              "created_utc": "2026-02-03 08:43:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36q5s4",
          "author": "__JockY__",
          "text": "I tried the FP8 version in vLLM 0.16rc1 and while it loads/runs ok, tool calling is broken. Running with Claude Code I see the vLLM logs spammed with tool calling template errors, for example:\n\n    INFO 02-02 10:08:57 [step3p5_tool_parser.py:1365] vLLM Successfully import tool parser Step3p5ToolParser !\n    WARNING 02-02 10:09:00 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 1\n    WARNING 02-02 10:09:00 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 2\n    INFO 02-02 10:09:01 [step3p5_tool_parser.py:1365] vLLM Successfully import tool parser Step3p5ToolParser !\n    WARNING 02-02 10:09:01 [step3p5_tool_parser.py:304] Error when parsing XML elements: not well-formed (invalid token): line 9, column 1\n\nAnd then Claude cli quite literally crashes and dumps me back to the terminal. Ah well. Back to MiniMax :)",
          "score": 1,
          "created_utc": "2026-02-02 17:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o372605",
              "author": "__JockY__",
              "text": "It's these kinds of errors (tool calling template related) that have plagued every single model except MiniMax-M2.x when I've tried using them with Claude code.\n\nQwen3 235B is a joke. GLM-4.x fared little better. They just barf and throw errors all the fucking time. Looks like Step-3.5-Flash is the same.\n\nMiniMax just just works when generating tool calls. They're magically well-formed and Claude can go through thousands of tool calls without a hitch. \n\nMM may not be the strongest model at writing advanced code or debugging complex issues, but it more than makes up for that in reliability as an agent.\n\nI've ditched Step-3.5-Flash and now Claude works perfectly again. It's such a shame. These new models (Dots, GLM, Step, etc.) write fantastic code! They're so strong! They just can't do reliable tool calling and so they don't get used. I'm convinced - certainly about GLM - that the open version is neutered for tools because everything I read about the API says it works well.",
              "score": 1,
              "created_utc": "2026-02-02 18:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37xwem",
                  "author": "Front_Eagle739",
                  "text": "huh. I get errors with templates on everything and usually just go \"gpt5.2 fix this\" and it does. My glm flash tool calling is rock solid now.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:34:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36xri9",
          "author": "ghulamalchik",
          "text": "I can attest to its performance, there's a free model you can use on OpenRouter. I used it with Roo Code.\n\nIt's extremely fast and solved some things the other big free models couldn't solve.\n\nI'll definitely keep an eye on a future API subscription. But for now I'll wait for DeepSeek R2 before I commit.",
          "score": 1,
          "created_utc": "2026-02-02 17:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36ytbo",
          "author": "LosEagle",
          "text": "Step-flash got stuck in a window again..Â ",
          "score": 1,
          "created_utc": "2026-02-02 17:53:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3710sd",
          "author": "Noobysz",
          "text": "for iklammacpp CPU offloading +GPU which layers for it are better to offload on CPU for since i have only upto 84 GB VRAM and the rest must be in my 96 GB RAM so which layers numbers for example for the gguf should i offload on CPU for fastest Speed?",
          "score": 1,
          "created_utc": "2026-02-02 18:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37dl6v",
              "author": "LegacyRemaster",
              "text": "llama-server.exe --model  \"f:\\\\step3p5\\_flash\\_Q4\\_K\\_S.gguf\" --ctx-size 8192 --threads 16 --host [127.0.0.1](http://127.0.0.1) \\--no-mmap --flash-attn on --fit on ---> \n\nload\\_tensors: offloaded 46/46 layers to GPU\n\nload\\_tensors:          CPU model buffer size =   283.22 MiB\n\nload\\_tensors:        CUDA0 model buffer size = 92265.46 MiB\n\nload\\_tensors:    CUDA\\_Host model buffer size = 13780.12 MiB",
              "score": 2,
              "created_utc": "2026-02-02 18:59:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37jm7z",
          "author": "fallingdowndizzyvr",
          "text": "Why so many tiny little files?",
          "score": 1,
          "created_utc": "2026-02-02 19:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37zrni",
          "author": "IrisColt",
          "text": "From the creators of ace-step!?",
          "score": 1,
          "created_utc": "2026-02-02 20:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3abzpu",
              "author": "IntrepidKick1335",
              "text": "yes, they also work on music model with ACE studio",
              "score": 1,
              "created_utc": "2026-02-03 04:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o385tn7",
          "author": "SVG-CARLOS",
          "text": "And kimi k2.5 outperforms GLM-4.7 and DeepSeek v3.2 in almost all.",
          "score": 1,
          "created_utc": "2026-02-02 21:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b0myt",
          "author": "AppealSame4367",
          "text": "I tried it and am shocked how good and fast it is. I think this is it. GLM 4.7, Â Step 3.5 Flash, Kimi K2.5\n\nNo need for American models anymore and I suspect that they will quickly catch up any advance that American models still have.\n\nWhat would be needed to run this model locally?",
          "score": 1,
          "created_utc": "2026-02-03 07:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f0czi",
          "author": "Informal-Spinach-345",
          "text": "Definitely not outperforming Minimax M2.1 FP8 or GLM 4.7 GPTQ models running locally in my tests.",
          "score": 1,
          "created_utc": "2026-02-03 21:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbipk",
          "author": "xatey93152",
          "text": "Where is the source of this benchmark?",
          "score": 1,
          "created_utc": "2026-02-05 02:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o345t9e",
          "author": "shing3232",
          "text": "Kind of feels like Deepseek V2 ",
          "score": 1,
          "created_utc": "2026-02-02 06:47:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o348t76",
              "author": "shing3232",
              "text": "Deep Reasoning at Speed: While chatbots are built for reading, agents must reason fast. Powered by 3-way Multi-Token Prediction (MTP-3), Step 3.5 Flash achieves a generation throughput of 100â€“300 tok/s in typical usage (peaking at 350 tok/s for single-stream coding tasks). This allows for complex, multi-step reasoning chains with immediate responsiveness.",
              "score": 2,
              "created_utc": "2026-02-02 07:14:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o34tcdt",
          "author": "Lazy-Variation-1452",
          "text": "\\`Flash\\` means light and fast. I don't agree that a 196B model can be considered \\`flash\\`; that is just bad naming. Haven't tried the model, though, the benchmarks look promising",
          "score": -1,
          "created_utc": "2026-02-02 10:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o34vazr",
              "author": "oxygen_addiction",
              "text": "200 tokens a second on OpenRouter says otherwise.",
              "score": 4,
              "created_utc": "2026-02-02 10:47:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o34yv9q",
                  "author": "Lazy-Variation-1452",
                  "text": "\\*167 tokens \n\nSecondly, the hardware and power required to run this model is very much inaccessible for most people. There are certain providers, but that doesn't make it a \\`flash\\` model, and I don't think it is a good idea to normalize extremely large models",
                  "score": 2,
                  "created_utc": "2026-02-02 11:19:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35gkqn",
              "author": "Ok_Procedure_5414",
              "text": "But MoE?",
              "score": 2,
              "created_utc": "2026-02-02 13:26:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3sp7g3",
              "author": "Caffdy",
              "text": "> I don't agree that a 196B model can be considered `flash`\n\nTell that to Google and their 1T paremeters flash model",
              "score": 1,
              "created_utc": "2026-02-05 22:11:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3baa6e",
          "author": "ConsciousArugula9666",
          "text": "Already some free-to-play providers: OpenRouter, ZenMUX and AIHubMix see [https://llm24.net/model/step-3-5-flash](https://llm24.net/model/step-3-5-flash)",
          "score": 0,
          "created_utc": "2026-02-03 09:09:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o34v1yg",
          "author": "AnomalyNexus",
          "text": "Seems likely that there is a bit of benchmaxing in there but still seems promising anyway",
          "score": -1,
          "created_utc": "2026-02-02 10:45:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33h7wa",
          "author": "JimmyDub010",
          "text": "Oh cool another model for the rich",
          "score": -16,
          "created_utc": "2026-02-02 03:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o33m8cl",
              "author": "datbackup",
              "text": "Newsflash pointdexter, you are the rich\n\nAnd just like all the other rich people, you are obsessed with the feeling that you donâ€™t have enough money",
              "score": 10,
              "created_utc": "2026-02-02 04:19:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o355ypo",
              "author": "Orolol",
              "text": "It's literally free on openrouter.",
              "score": 2,
              "created_utc": "2026-02-02 12:16:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrkb1b",
      "title": "How was GPT-OSS so good?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "author": "xt8sketchy",
      "created_utc": "2026-01-30 22:31:44",
      "score": 379,
      "num_comments": 179,
      "upvote_ratio": 0.9,
      "text": "I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.\n\nThe model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.\n\nBut it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)\n\nI'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:  \n\\- Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?  \n\\- I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?  \n\\- What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o2r8tx4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 08:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2outu2",
          "author": "SlowFail2433",
          "text": "Clean data goes a very long way\n\n\nWhat I have noticed from working on big enterprise projects is that they tend to have enormous data pipelines spanning dozens of packages where data is manipulated and evolves repeatedly in a structured way\n\n\nWhereas open source projects often put web-scrape slop directly into the model",
          "score": 343,
          "created_utc": "2026-01-30 22:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2owqo3",
              "author": "SubstanceNo2290",
              "text": "Also this requires top tier resources to pull off.. aka money\n\nOpenAI being a dedicated behemoth based in the US can outright make deals with X/reddit etc for structured training data with plenty of useful metadata. Chinese companies can do the same with China based social media but it probably ainâ€™t nearly as information rich as American/international media.\n\nAnd developing/honing these pipelines is a massive project in and of itself which combined with not-having-billions puts startups at a disadvantage",
              "score": 109,
              "created_utc": "2026-01-30 22:48:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pvy2w",
                  "author": "Saltwater_Fish",
                  "text": "I feel more and more the importance of data",
                  "score": 18,
                  "created_utc": "2026-01-31 02:05:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qlrpb",
              "author": "horsethebandthemovie",
              "text": "yeah the more you try shit the more you realize how slop adds up in every phase. sloppy data? bad signal for the model to learn. sloppy evals? model doesn't know which way is correct. \n\nturns out it's just really fucking hard\n\nand the number of knobs to tweak is legitimately staggering. the more you learn, the more you realize that the only way to train something at that scale is to have people who understand everything from the GPU kernels up to the scraping and processing\n\nif you have those skills and you're doing open source work your time is extremely valuable, why not get rich working for openai et al instead?",
              "score": 22,
              "created_utc": "2026-01-31 04:48:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2parxk",
              "author": "Pvt_Twinkietoes",
              "text": "Big enterprise data also tend to be very narrow in scope. They tend to do very few things, but has been low tolerance for errors.",
              "score": 5,
              "created_utc": "2026-01-31 00:04:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rfj7l",
              "author": "howardhus",
              "text": "this is a made up comment.. there are no open source models. only open weights.\nand even then: the best data processors are open source (airflow, airbyte kfk etc)",
              "score": 4,
              "created_utc": "2026-01-31 09:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rlikm",
                  "author": "TaroOk7112",
                  "text": "Not true, there's a few, but probably none SOTA:\n\nexample: [https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222](https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222)",
                  "score": 3,
                  "created_utc": "2026-01-31 10:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rktnb",
              "author": "IrisColt",
              "text": "Thatâ€™s why ChatGPT, Gemini, and Claude command English like gods. Chinese open-weight models can produce some of the best ESL output out there, but they still donâ€™t quite have the cultural feel of a native speaker.",
              "score": 2,
              "created_utc": "2026-01-31 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wbxbr",
                  "author": "Ok-Attention2882",
                  "text": "ze bluetooth dewise is connected-a success a folly",
                  "score": 3,
                  "created_utc": "2026-02-01 01:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ovkq2",
          "author": "Baldur-Norddahl",
          "text": "It wasn't actually trained at 4 bit. We don't exactly know, but likely they trained it at 16 bit as usual. Then it went through a process called quantization aware training. During this they keep the weights at 16 bits, but do the forward pass at 4 bits. So they are kind of running the quantization over and over, so any brain damage gets trained out of it.\n\nThey are not the only ones doing it. Kimi K2.5 was just released using the same concept. It is just that even with most of the weights at 4 bits, that one is far too large for most of us.",
          "score": 84,
          "created_utc": "2026-01-30 22:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qctkj",
              "author": "nikprod",
              "text": "Googles Gemma has QAT too",
              "score": 11,
              "created_utc": "2026-01-31 03:47:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qokmy",
                  "author": "planetafro",
                  "text": "I dont think Gemma3 does tools tho. :(",
                  "score": 4,
                  "created_utc": "2026-01-31 05:08:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qpjv2",
              "author": "mycall",
              "text": "Multisampling, do you know how many iterations of quantizations?",
              "score": 1,
              "created_utc": "2026-01-31 05:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rrqh6",
                  "author": "Baldur-Norddahl",
                  "text": "I don't think Open AI has released that information. Almost everything about how they train their models is secret, hence why some might call them Closed AI.",
                  "score": 1,
                  "created_utc": "2026-01-31 10:59:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p404u",
          "author": "Kamal965",
          "text": "One of the main reasons why GPT-OSS is faster is because its architecture is wider but shallower than most.",
          "score": 15,
          "created_utc": "2026-01-30 23:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3izx",
          "author": "inteblio",
          "text": "Nice to finally hear something positive about it.\n\n20b is also incredible. It can run on 16gb RAM (not gpu), and is \"perfectly good\". Finally \"run chatGPT at home\". \n\nOn GPU is good enough to voice-talk with (parakeet/korroko). 120b is better, but only if you need extra.",
          "score": 113,
          "created_utc": "2026-01-30 23:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rajpp",
              "author": "Morazma",
              "text": "I had no idea it could runÂ in such a small amount of RAM. That's incredible.",
              "score": 6,
              "created_utc": "2026-01-31 08:16:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pkx6m",
              "author": "Chris266",
              "text": "I've got a MacBook pro 24gb of ram and 20b runs better than anything I've tried in the 18-30b range. Once it gets going it feels quick and does a good enough job for home use.",
              "score": 29,
              "created_utc": "2026-01-31 01:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qbuza",
              "author": "rorowhat",
              "text": "What gui gives you the options for voice?",
              "score": 2,
              "created_utc": "2026-01-31 03:41:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2vy3o5",
                  "author": "inteblio",
                  "text": "I vibecoded the interface",
                  "score": 1,
                  "created_utc": "2026-02-01 00:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r6ccl",
              "author": "ChessGibson",
              "text": "Are you running this on a Mac? I have tried it with mine that has 16GB of unified memory but I didnâ€™t have enough space to run it at even I think Q4.",
              "score": 1,
              "created_utc": "2026-01-31 07:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rqzlb",
                  "author": "Baldur-Norddahl",
                  "text": "It should run on a 16GB Mac but you need to run the command to increase allowed VRAM.\n\nsudo sysctl iogpu.wired_limit_mb=14336\n\nAlso run the original model from OpenAI.",
                  "score": 3,
                  "created_utc": "2026-01-31 10:52:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ttfwq",
              "author": "_raydeStar",
              "text": "Dude, I've been looking for the perfect tooling LLM for an 8GBVRAM machine (work laptop) - Qwen 30B doesn't quite get it right, neither does nemotron or GLM 4.7 flash (too slow), and the 8GB models are too dumb and keep getting the tool calls wrong.  20B is my consistent driver and it just works exactly as I want it to.",
              "score": 1,
              "created_utc": "2026-01-31 18:08:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phasp",
              "author": "mckirkus",
              "text": "Why \"not GPU\"?",
              "score": 2,
              "created_utc": "2026-01-31 00:40:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pi610",
                  "author": "2str8_njag",
                  "text": "I guess it's small enough for CPU",
                  "score": 7,
                  "created_utc": "2026-01-31 00:44:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2owq7l",
          "author": "ttkciar",
          "text": "Regarding GLM-4.5-Air:  To be fair, its competence is not entirely limited to agentic code development.  I have found it to be excellent for STEM tasks in general, including physics, medicine, and math.\n\nIt's not great for creative tasks, though.  I use other models for creative writing (mostly Big-Tiger-Gemma-27B-v3 and Cthulhu-24B-1.2).\n\nOn a side-note, I recently found (to my surprise) that Olmo-3.1-32B-Instruct is much, much better at inferring syllogisms than GLM-4.5-Air or any other model I have tried.  That's a bit of a niche application, but an important one for some synthetic data generation tasks.",
          "score": 36,
          "created_utc": "2026-01-30 22:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q28dr",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 6,
              "created_utc": "2026-01-31 02:42:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tmerg",
                  "author": "zoyer2",
                  "text": "same, i find GPT OSS limit itself when trying to create complex games while GLM tries to actually proceed with difficult tasks without simplifying them",
                  "score": 1,
                  "created_utc": "2026-01-31 17:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ou0h5",
          "author": "Haunting_Lobster1557",
          "text": "GPT-OSS was lightning in a bottle tbh, the 4-bit native training was genius but super hard to replicate without their exact setup and data pipeline\n\n  \nMost newer models are chasing benchmarks instead of that smooth \"just works\" feel that made GPT-OSS special - turns out good vibes are harder to quantify than MMLU scores",
          "score": 132,
          "created_utc": "2026-01-30 22:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p37p9",
              "author": "Yes_but_I_think",
              "text": "QAT is a fully understand technology by now. Kimi gave a INT4 QAT in Kimi K2.5",
              "score": 41,
              "created_utc": "2026-01-30 23:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pwf9m",
                  "author": "Saltwater_Fish",
                  "text": "[https://lmsys.org/blog/2026-01-26-int4-qat/](https://lmsys.org/blog/2026-01-26-int4-qat/)\n\nHere is a good blog about INT4 QAT",
                  "score": 16,
                  "created_utc": "2026-01-31 02:07:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rb71e",
                  "author": "TelloLeEngineer",
                  "text": "gpt oss was not QAT, it was natively trained at mxfp4",
                  "score": 3,
                  "created_utc": "2026-01-31 08:22:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2owxdt",
              "author": "hieuphamduy",
              "text": "This! And also I don't even think other models are even using data with a much later cutoff date than gpt-oss. From what I heard, companies are having difficulty collating clean data from 2024 onwards (prob cause of all the generative AI slops), so most of them are just recycling relatively the same dataset tbh",
              "score": 26,
              "created_utc": "2026-01-30 22:49:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phx05",
              "author": "Ilforte",
              "text": "Chatgpt ahh post",
              "score": 5,
              "created_utc": "2026-01-31 00:43:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rl4da",
                  "author": "IrisColt",
                  "text": "heh",
                  "score": 1,
                  "created_utc": "2026-01-31 09:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pd7f4",
              "author": "rm-rf-rm",
              "text": "> good vibes are harder to quantify than MMLU scores\n\nno, its whether you follow proper testing vs scoring high on popular benchmarks. Its almost exactly the equivalent of a kid trying to get high scores on SAT, GRE etc. vs being actually good.",
              "score": -4,
              "created_utc": "2026-01-31 00:17:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p3716",
          "author": "federico_84",
          "text": "I remember the huge negative response the model got here after release, and not just about the safety guardrails. Interesting to see the shift in narrative. People have very strong feelings about OpenAI.",
          "score": 82,
          "created_utc": "2026-01-30 23:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p5z6r",
              "author": "TheRealMasonMac",
              "text": "I think it's really simple.\n\n\\- People who liked it are still using it and praise it.\n\n\\- People who don't like it forgot about it.\n\nI still despise the model. Absolutely useless even for my coding work because it's so safety-maxxed.",
              "score": 56,
              "created_utc": "2026-01-30 23:37:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qas4g",
                  "author": "ObsidianNix",
                  "text": "Qwen3-30B-VL is my go to now. OSS feels like its falls short a lot of times for my use. Good for quick 4o-mini-esque questions rather than a full knowledge model. Qwen took the cake with their qwen3 series.",
                  "score": 8,
                  "created_utc": "2026-01-31 03:34:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2q07oo",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 5,
                  "created_utc": "2026-01-31 02:30:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pdba5",
              "author": "popecostea",
              "text": "There were definitely some problems initially with the jinja templates and parameters that people were running it with. Couple that with a very polarised view of OpenAI and you get that reaction. After the dust settled and people understood how to properly run the model, and even found some jailbreak prompts, most of the people who put the effort found that it is a really great model.",
              "score": 10,
              "created_utc": "2026-01-31 00:18:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpxqk",
              "author": "Beneficial-Good660",
              "text": "The bots are working overtime on weekends. Especially on weekends, it's flooded with posts about Mac (24, 48, 128, 512 gb, v1,2,3,4), Nvidia. Good thing ollama is getting less. OpenAi bots have been very active lately, trying to latch onto everything new, like any model + also as good as gptOss. And so it goes every day, soon the end of LocallamaðŸ˜­. There's almost no discussion left about what you can actually do with LLMs.",
              "score": 11,
              "created_utc": "2026-01-31 05:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rhvl0",
                  "author": "ivoras",
                  "text": "Don't you get the feeling that we've kind of peaked in the \"what you can actually do with LLMs\" area?\n\nThey're so generic - everything and nothing at the same time.",
                  "score": 0,
                  "created_utc": "2026-01-31 09:25:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q5szf",
              "author": "ab2377",
              "text": "yes and there is a free quota of gpt-oss in Google's antigravity and people made a lot of fun of that too.",
              "score": 1,
              "created_utc": "2026-01-31 03:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2po0tr",
              "author": "Far-Low-4705",
              "text": "i think everyone here secretly knows it is extremely good, if not the best currently, but are afraid/dont want to admit it.",
              "score": -3,
              "created_utc": "2026-01-31 01:18:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qy9i4",
              "author": "Zeeplankton",
              "text": "In fairness, no one expected openAI, of all companies, to release such a good model. But also, correct config probably wasn't so simple. OSS uses openAI's weird harmony format.",
              "score": 0,
              "created_utc": "2026-01-31 06:25:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pwtyo",
              "author": "Saltwater_Fish",
              "text": "Maybe there was no worse model to compare with at the beginning, so it was impossible to highlight that gpt-oss is actually not that bad a model?",
              "score": -4,
              "created_utc": "2026-01-31 02:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oz42x",
          "author": "Klutzy-Snow8016",
          "text": "They had access to the weights of a frontier model to distill from, and have way more compute than the makers of most open weight models. Same reason the Gemma series is so good.",
          "score": 18,
          "created_utc": "2026-01-30 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pqapv",
              "author": "night0x63",
              "text": "Any new Gemma after gemma3 27b?",
              "score": 2,
              "created_utc": "2026-01-31 01:31:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pr5kz",
                  "author": "Klutzy-Snow8016",
                  "text": "Not yet. I hope they make one.",
                  "score": 11,
                  "created_utc": "2026-01-31 01:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p0fel",
          "author": "MrMisterShin",
          "text": "Itâ€™s actually A5B and not A3B, and yes itâ€™s a very solid general model that is great at everything to be honest. \n\nIâ€™m surprised, a competitor hasnâ€™t released a definitively better model at those parameters. It was released back in the summer, albeit a rocky start with the Harmony response format.",
          "score": 24,
          "created_utc": "2026-01-30 23:07:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0qb",
              "author": "night0x63",
              "text": "How did they solve the harmony issue?Â \n\n\nIs it solved by vLLM parser fixing parsing?",
              "score": 4,
              "created_utc": "2026-01-31 01:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qgdk0",
                  "author": "MrMisterShin",
                  "text": "Unsloth applied fixes to the gpt oss models chat template as a workaround, others applied fixes to their adapters and tools instead. \n\nI donâ€™t use vllm, but from what I can workout they made a fix on their end to accommodate the gpt oss models.",
                  "score": 3,
                  "created_utc": "2026-01-31 04:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qhd4r",
              "author": "Fulxis",
              "text": "\nThe model performs really well, but the remaining pain point on vLLM isnâ€™t completely fixed when using structured output (https://github.com/vllm-project/vllm/issues/23120). I still have to resort to regex to pull out values and lose the benefit of guided decoding, even though the model generally adheres closely to the JSON Schema in practice.",
              "score": 3,
              "created_utc": "2026-01-31 04:17:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpuws",
              "author": "mycall",
              "text": "I wonder if the A#B will ever be a user selectable setting.",
              "score": 2,
              "created_utc": "2026-01-31 05:18:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xz6g3",
                  "author": "MrMisterShin",
                  "text": "That would be interesting. I think 3 active parameters is acceptable, but 5 active and greater is exponentially better. \n\nKimi k2.5 uses 32B active parameters and many report that it surpasses Claude Sonnet 4.5",
                  "score": 1,
                  "created_utc": "2026-02-01 09:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2s3pfb",
          "author": "artisticMink",
          "text": "gpt-oss-120b was a flex to bring openai into a space it wasn't present before. \n\nSpending a lot of money on specialized training so people who don't pay you can use your model does not make sense beyond PR and marketing.",
          "score": 5,
          "created_utc": "2026-01-31 12:40:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sghoy",
              "author": "Agile-Competition-91",
              "text": "gpt 2 was open weight actually,so releasing models isnt new to them",
              "score": 1,
              "created_utc": "2026-02-05 21:29:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p0ntw",
          "author": "one-wandering-mind",
          "text": "OpenAI has great engineers and researchers. They delayed an open source release multiple times and clearly put on a lot of effort to make the model high quality.Â \n\n\nI doubt it is one single thing that is the reason why the model is great. Lots experimentation prior to this final model, heavy data curation, a lot of pre training, and a lot of post training.Â \n\n\nThe two models fit for a consumer GPU (20b) and a single server GPU (120b) . They are remarkably fast and cheap for the capability they provide. Some companies may also release a 4 bit or mixed precision quant, but I at least have not seen benchmarks in that low precision or them deployed on the cloud at that precision. So if you run something that is benchmarked at 32 bit or 16 bit precision and you run it locally, you are probably using something between 4 and 8 not quants. Quantization does retain a lot, but you do lose some capability and that loss is likely what is less visible to standard benchmarks.Â \n\n\nIt is a shame so many people shit on the model when it came out. Much less likely that they will be as motivated to release a new version because of that or with the same frequency as they would have if the initial reception was better.\n\n\nI have been meaning to spend more time exploring what can be done with it given the incredible speed and cheap price.Â ",
          "score": 28,
          "created_utc": "2026-01-30 23:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r0p98",
          "author": "IulianHI",
          "text": "One thing nobody's mentioned yet - the A3B architecture probably plays a huge role beyond just raw size. Sparse activation means you get the knowledge of a 120b model but only pay for \\~20b worth of compute on each forward pass.\n\nThat's why it feels \"faster\" than even smaller models - because at inference time, you're not actually running all 120b parameters. The MoE routing learned which experts to activate for which tokens.\n\nCombine that with the QAT (which other comments explain well) and OpenAI's data quality advantage, and yeah... lightning in a bottle.",
          "score": 5,
          "created_utc": "2026-01-31 06:46:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyag4",
          "author": "Anonygeois",
          "text": "The posttraining and clean data is the trick. Hopefully we do have insiders to leak the process",
          "score": 13,
          "created_utc": "2026-01-30 22:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oz48e",
          "author": "DinoAmino",
          "text": "It is good, no doubt about it. Its capabilities and skills are what is good. But it's knowledge is poor. The SimpleQA scores are shockingly bad. It will hallucinate more and stick to its guns. But ground it with context and it is amazing. So what if it's more than 6 months old - all models get dumber over time, but their capabilities never change.",
          "score": 11,
          "created_utc": "2026-01-30 23:00:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vzl3a",
              "author": "llmentry",
              "text": "It depends what knowledge you're talking about.  GPT-OSS-120B's STEM knowledge (at least in my field) is surprisingly excellent.  \n\nBased on the release notes, it was made to be good at a few fields rather than expert in all -- there were only so many params to work with, after all -- and there will be plenty of areas where it falls short.",
              "score": 2,
              "created_utc": "2026-02-01 00:43:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pdhy1",
              "author": "rm-rf-rm",
              "text": ">  But it's knowledge is poor.\n\nWhy is this a surprise for small/local models - this is one of the most straightforward, known obvious limitations of lesser params. But it has no consequence in any real world application where you should be providing in the context everything that the LLM needs through web search, RAG, code search etc.",
              "score": 4,
              "created_utc": "2026-01-31 00:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl0gd",
                  "author": "Vaddieg",
                  "text": "8B and even 4B dense models have better knowledge, but suck at everything else. I suspect that gpt-oss was crippled on purpose to not compete with commercial versions. 120B is 5x bigger but also suffers from real world knowledge detachment",
                  "score": 4,
                  "created_utc": "2026-01-31 01:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p2jm2",
          "author": "Yes_but_I_think",
          "text": "MoE is the way. Everybody understands that now.\n\nMassively spare (5% active experts or less) is the way- people are understanding this.\n\nQuantization aware training at INT4 is the best- people are coming to this understanding slowly. It's used to be FP16 (llama 1), then BF16(llama 3), then FP8(deepseek), then FP4(oss-120b), now INT4(Kimi k2.5).\n\nA 1 trillion weights model at just 650 GB and only 35B active weights per token that's just 16GB of numbers crunched per token. If you have 4TB/s bandwidth (H100/200) you get solid ~200 tokens/s and NO loss of quality. B200 is 8TB/s so that will be ~400 tokens/s (not sure on B200).",
          "score": 23,
          "created_utc": "2026-01-30 23:18:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rtnl9",
              "author": "Baldur-Norddahl",
              "text": "Kimi likely only chooses INT4 because as a Chinese company, they are restricted from using the newest GPUs. \n\nMXFP4 and NVFP4 are superior. Uses no more space and is the same speed (on GPUs with support) but has better range and better detail depending on what is needed.\n\nThe NVFP4 is the most powerful format but is Nvidia only. MXFP4 has multivendor support. FP4 is the oldest and least powerful 4 bit floating point format.\n\nGPT OSS 20b and 120b are using MXFP4 not FP4.",
              "score": 9,
              "created_utc": "2026-01-31 11:16:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2xig2a",
                  "author": "Yes_but_I_think",
                  "text": "Good to know about nvfp4. Does any company QAT to nvfp4?. The critical thing is to do the quantization during training. Post training quants (most common variety) are no good below q6. The kind of quality at q8 and q4 is like so subtle that it hurts you in production.",
                  "score": 1,
                  "created_utc": "2026-02-01 06:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ouzxc",
          "author": "PatagonianCowboy",
          "text": "MXFP4",
          "score": 13,
          "created_utc": "2026-01-30 22:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r2m6m",
          "author": "IulianHI",
          "text": "Another thing people miss - GPT-OSS had surprisingly good instruction following for its time. A lot of newer open models can chat fine but fall apart when you give them complex multi-step tasks. That \"just works\" feeling comes from training on a lot of high-quality instruction data, not just raw web text.",
          "score": 4,
          "created_utc": "2026-01-31 07:03:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ppfli",
          "author": "Holiday_Purpose_3166",
          "text": "GPT-OSS models have a really good architecture, most of hate come from the chronic dislike for Sam. People can't see past certain things just for the sake of hate. \n\nThe MXFP4 quant was a chef's kiss and fine-tuners like noctrex have been using it exclusively for other models too that seem to achieve a measured lower perplexity loss, better than Q8 and BF16 for much smaller memory footprint. Although stability is between Q4 and Q6. \n\nHaving used both models I can say the speed for quality is extremely good, and have used them extensively on production codebases. \n\nHowever they are slowly becoming outdated in areas where they could concern - I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained.\n\nIt's more than fine for local use for what matters, but caution should be given for vibecoders seeking external interactions.\n\nI do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates - it has been my main replacement along with GLM 4.7 Flash as backup for long, less complex tasks. \n\nI do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nIf OpenAi releases an updates OSS, that's gonna rock.",
          "score": 8,
          "created_utc": "2026-01-31 01:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9ce5",
              "author": "tarruda",
              "text": "> I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained\n\nShould we rely on LLM knowledge for deprecated deps though?\n\n> I do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates\n\nOne issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\n> I do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nThat's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\n    % llama-bench -m ~/ml-models/huggingface/mradermacher/gpt-oss-120b-Derestricted-GGUF/gpt-oss-120b-Derestricted.MXFP4_MOE.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 \n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.015 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        740.15 Â± 5.88 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         66.32 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        596.41 Â± 0.46 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         58.38 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        491.13 Â± 1.99 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         53.21 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        418.39 Â± 1.23 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         48.75 Â± 0.07 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        361.42 Â± 1.48 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         45.29 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        315.38 Â± 0.84 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         41.98 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        276.29 Â± 0.58 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         39.14 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        246.77 Â± 0.39 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         36.80 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        224.35 Â± 0.47 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         34.67 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        204.29 Â± 0.31 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         32.72 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        188.46 Â± 0.40 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         30.97 Â± 0.02 |\n    \n    build: b5b8fa1c8 (7817)",
              "score": 3,
              "created_utc": "2026-01-31 08:04:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rgkkw",
                  "author": "Holiday_Purpose_3166",
                  "text": ">Should we rely on LLM knowledge for deprecated deps though?\n\nNever said we should, it's mostly the point on deprecated knowledge that is potentially be applied.\n\n>One issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\nNever had that issue. Just simply resistant to perform.\n\n>That's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\nYour M1 Ultra doesn't have VRAM, but yes, I am offloading the model with a token generation of 30-40 t/s at full context, with an RTX 5090. That wasn't the point, it's the fact a 120B can run relatively fast for its size.",
                  "score": 3,
                  "created_utc": "2026-01-31 09:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pdzov",
          "author": "rm-rf-rm",
          "text": "Yes OpenAI really deserve their flowers on this. For all the ridicule Sam got for delaying the launch multiple times, its genuinely a great model and still my go to. \n\n\nWe actually need to give them their due credit if we want them to continue doing OSS - if they feel that the open source community just rejected them even after they finally put out an open weights model after forever, why would they want to put any more effort towards this?",
          "score": 9,
          "created_utc": "2026-01-31 00:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyh5a",
          "author": "jhov94",
          "text": "I thought GPT OSS 120b was a5b. Anyway, I never really understood how it benches so high. It's fast which is nice for certain general knowledge chat like tasks, but for coding it falls short. It writes a ton of bad code quickly then needs to rewrite it over and over until it works out the errors. But even then I also find it to be lazy. It always takes the quickest and easiest path to a solution, even if the solution does not completely solve the problem. You really have to prod it along to get it to solve anything but simple problems. GLM4.5 Air is slow but it can be left to just work out a problem on its own and sometimes its faster simply because it got it right the first time.",
          "score": 11,
          "created_utc": "2026-01-30 22:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p1qqz",
              "author": "phido3000",
              "text": "Coding is problem, always, its fundamentally different to writing human languages.\n\nIts likely that coding specific models will always perform higher in coding. Just like in humans a PHD in computer science will write better code than a PHD in English literature.",
              "score": 3,
              "created_utc": "2026-01-30 23:14:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph264",
                  "author": "Prestigious-Crow-845",
                  "text": "it is bad at creative writing too, so what it is good for? office tasks?",
                  "score": 5,
                  "created_utc": "2026-01-31 00:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2phj9c",
                  "author": "bonobomaster",
                  "text": "Hmm, I feel, that from a statistical \"which token is most likely\" / LLM point of view, coding and human language are not different at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 00:41:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pdefz",
          "author": "CorpusculantCortex",
          "text": "\"But its sort of old... its been so long since gpt oss came out\"\n\n4 months. Gpt oss came out in August. It has been 4 MONTHS. I know that tech moves fast. But my god if 1/3 of a year feels like a long time to you you need to get outside and live little.\n\n4. months.",
          "score": 20,
          "created_utc": "2026-01-31 00:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ph09v",
              "author": "coder543",
              "text": "It has been 5 months and 25 days since GPT-OSS launched, which is basically 6 months, not 4 months.",
              "score": 17,
              "created_utc": "2026-01-31 00:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl5od",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 3,
                  "created_utc": "2026-01-31 01:01:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ryx2l",
                  "author": "CorpusculantCortex",
                  "text": "To be blunt, that doesn't matter. My point is months is not forever unless you are so lost in the sauce that you have poor bearing on reality. If you think another month or two changes it and felt the need to math it out to the day it is just more proof you need to step back and consider that months and days is not a long time unless you are under 5 years old.\nSaying 5 months and 25 days is practically 6 months like that has big \"im 4 years old but my birthday was 6 months ago so im basically 5\" energy.",
                  "score": 1,
                  "created_utc": "2026-01-31 12:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pq5lc",
              "author": "night0x63",
              "text": "Well I'm AI model years that is easily 78 years. /s",
              "score": 7,
              "created_utc": "2026-01-31 01:30:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p6vqo",
          "author": "lolwutdo",
          "text": "GLM 4.7 Flash is the OSS 20b killer, try it",
          "score": 13,
          "created_utc": "2026-01-30 23:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p92if",
              "author": "UnifiedFlow",
              "text": "Twice the size on disk, 1/4 the speed and coding errors were common.  4.7 Flash was a dud IMO.  Great reasoning, but implements horribly.",
              "score": 16,
              "created_utc": "2026-01-30 23:54:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgdgk",
                  "author": "someone383726",
                  "text": "Idk Iâ€™ve been happy with 4.7 flash, but I still love oss20b too.",
                  "score": 7,
                  "created_utc": "2026-01-31 00:35:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pgnmo",
                  "author": "AlwaysLateToThaParty",
                  "text": "While I haven't tried it yet, I understand that there has been a llama.cpp update because of that model, and the re-quantization has increased performance significantly.\n\nhttps://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\n\n> Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.\n\nPerhaps this is your issue?",
                  "score": 8,
                  "created_utc": "2026-01-31 00:36:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pjrqx",
                  "author": "lolwutdo",
                  "text": "The model is still new and needs work.\n\nEven with its faults currently, itâ€™s still really good.\n\nGPT-OSS was absolute shit when it came out as well until it was finally implemented correctly months later.",
                  "score": -2,
                  "created_utc": "2026-01-31 00:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q0k7l",
              "author": "Photoperiod",
              "text": "I did try it and it performed worse overall. On paper 4.7 should beat it. Biggest issue I had was repetition. Hoping some of the kinks get worked out since it's a new model. But for now I've gone back to OSS 20.",
              "score": 5,
              "created_utc": "2026-01-31 02:32:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p38mk",
          "author": "maglat",
          "text": "Its very sad indeed. I dont have many hopes right now when you see how much trouble OpenAI has to survive. There is no room for an update on their open models I guess. That makes me very sad, because 120B still is my daily driver, but as you said, it becomes dated.",
          "score": 2,
          "created_utc": "2026-01-30 23:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5nsn",
          "author": "theghost3172",
          "text": "i think its because basically unlimited synthetic data from much bigger and powerfull frontier models. imagine unlimited clean synthetic data from o3. could also be distilation.",
          "score": 2,
          "created_utc": "2026-01-30 23:36:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b0120",
              "author": "Former-Ad-5757",
              "text": "Don't forget the training data from their FrontEnd, [chatgpt.com](http://chatgpt.com) is afaik still the largest provider, so they have the most training data on what people exactly want and expect.",
              "score": 1,
              "created_utc": "2026-02-03 07:31:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2q1qnt",
          "author": "agentzappo",
          "text": "Very smart and fast model, but there are still some unresolved issues with it outputting proper tool calls in Harmony format. Maybe itâ€™s a vLLM issue and less so the model, but so far in practice itâ€™s taking a lot of anti-rationalization patterns to coerce it into reliable tool calling, and thatâ€™s only when the inference backend isnâ€™t causing logits to drift in concurrent, batched inference ðŸ˜•",
          "score": 2,
          "created_utc": "2026-01-31 02:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rlssi",
          "author": "IulianHI",
          "text": "Another thing - GPT-OSS had that rare combo of good data curation AND proper alignment that actually made it pleasant to use. Newer models chase MMLU and benchmark scores, but nobody's benchmarking \"does this feel good to talk to\" or \"does it have consistent personality\". Those vibes are harder to quantify but way more important for daily use.",
          "score": 2,
          "created_utc": "2026-01-31 10:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rw8yx",
          "author": "jwr",
          "text": "gpt-oss models are under-appreciated. I use the smaller one (20b) for spam filtering and it beats every other 30B or less model that I've tested, and I've tested quite a few with my spam benchmark, while being one of the fastest, too.",
          "score": 2,
          "created_utc": "2026-01-31 11:40:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o323niu",
              "author": "Consumerbot37427",
              "text": "> I use the smaller one (20b) for spam filtering\n\nmind sharing your prompt/flow for that?",
              "score": 1,
              "created_utc": "2026-02-01 23:06:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t9ly0",
          "author": "KitchenSomew",
          "text": "GPT-OSS remains exceptional for several reasons:\n\n\n\n1. \\*\\*Training approach\\*\\*: It was trained with 4-bit quantization awareness from the start, not retrofitted. This preserved model quality while reducing size.\n\n\n\n2. \\*\\*Dataset quality\\*\\*: OpenAI's dataset curation was meticulous. They filtered for quality over quantity, which modern models often sacrifice for scale.\n\n\n\n3. \\*\\*Architecture efficiency\\*\\*: A3B architecture hit a sweet spot - large enough to be capable, small enough to be fast. Modern models chase parameter counts without proportional capability gains.\n\n\n\n4. \\*\\*Inference optimization\\*\\*: The model was optimized for actual deployment, not just benchmark performance.\n\n\n\nFor newer models to match this:\n\n\\- Focus on training efficiency from day 1\n\n\\- Prioritize dataset quality\n\n\\- Design for deployment, not papers\n\n\\- Consider 4-bit/8-bit native training",
          "score": 2,
          "created_utc": "2026-01-31 16:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyenq",
          "author": "TheRealMasonMac",
          "text": "Compute.\n\nThat's kind of the simple answer. OpenAI probably has more compute than all Chinese labs combined.",
          "score": 5,
          "created_utc": "2026-01-30 22:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p8rr4",
          "author": "GoranjeWasHere",
          "text": "And there is gpt oss 20b /120b heretic that removes censorship and keeps inteligence.\n\nI use it daily on my 5090 and you just can't beat the speed (250t/s)",
          "score": 2,
          "created_utc": "2026-01-30 23:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pkm64",
              "author": "walrusrage1",
              "text": "Are you using vLLM for this and full precision at 120b? What speeds do you get there?Â \n\n\nWe've been getting much slower results on an H100, so clearly something is up.",
              "score": 1,
              "created_utc": "2026-01-31 00:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pma1h",
                  "author": "GoranjeWasHere",
                  "text": "20b model at 250t/s",
                  "score": 1,
                  "created_utc": "2026-01-31 01:08:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2owu8b",
          "author": "PhotographerUSA",
          "text": "It fails a lot in LM studio doing MMC web calls.",
          "score": 1,
          "created_utc": "2026-01-30 22:48:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9r8j",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 4,
              "created_utc": "2026-01-30 23:58:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pfs0v",
          "author": "jabr7",
          "text": "I specially liked it in cerebras, we are getting average 5500 tps, that's making some full multi agentix systems take between 3s to 4s lol",
          "score": 1,
          "created_utc": "2026-01-31 00:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2phuvh",
          "author": "Thedudely1",
          "text": "The 20B model is the best coding model of its size that I've tried, at least for the weird kind of \"create a Wolfenstein 3D clone\" style prompts I like trying. GLM 4.7 Flash and Nemotron 3 Nano just became the other similarly sized models that can consistently do it in one prompt alongside it. But GPT-OSS 20B is the smallest model I've tested that can consistently do it successfully in either JS or Java.",
          "score": 1,
          "created_utc": "2026-01-31 00:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pxbsq",
          "author": "Ok_Individual_4295",
          "text": "Look for versions distilled from 5.2 this might update its knowledge and make it slightly better",
          "score": 1,
          "created_utc": "2026-01-31 02:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ty6od",
              "author": "RefrigeratorMuch5856",
              "text": "Could you share a link?",
              "score": 2,
              "created_utc": "2026-01-31 18:30:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2q84d1",
          "author": "jedsk",
          "text": "I think youâ€™re on the q4 if the model is 64GB",
          "score": 1,
          "created_utc": "2026-01-31 03:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qnh4e",
              "author": "simracerman",
              "text": "Thatâ€™s the one OpenAI posted on their huggingface.",
              "score": 2,
              "created_utc": "2026-01-31 05:00:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2r1yi0",
              "author": "Ok-Tumbleweed8507",
              "text": "The F16 is 64GB",
              "score": 1,
              "created_utc": "2026-01-31 06:57:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qnemf",
          "author": "MaggoVitakkaVicaro",
          "text": "They may well have training regimes which are much better than anything public.",
          "score": 1,
          "created_utc": "2026-01-31 05:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qyryn",
          "author": "TokenRingAI",
          "text": "It is likely that the model was either synthetically trained off of the outputs of OpenAI's top internal models, or off of the training data used for o3/o4",
          "score": 1,
          "created_utc": "2026-01-31 06:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdfuy",
          "author": "tarruda",
          "text": "> What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?\n\nNot only OpenAI has the best private training datasets, it also probably has superior training pipelines and is able to extract more performance per parameter.",
          "score": 1,
          "created_utc": "2026-01-31 08:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzybi",
          "author": "DefNattyBoii",
          "text": "Can someone compare it to GLM-4.7-Flash in terms of speed/tool calling/knowledge for both 20B OSS and 120B OSS?",
          "score": 1,
          "created_utc": "2026-01-31 12:10:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sfdgq",
          "author": "bfroemel",
          "text": "... and despite all the praises it seems that OpenAI isn't really that proud of gpt-oss. \n\n\n\n>The gpt-oss models were released way back in August. Since then, we've released half a dozen major updates to the frontier models. Perhaps you haven't used these lately, but their coding abilities are far beyond those of just a few months ago â€”Â and significantly beyond what the gpt-oss models are capable of.\n\n>\n\n[https://github.com/openai/codex/issues/8272#issuecomment-3672130792](https://github.com/openai/codex/issues/8272#issuecomment-3672130792)",
          "score": 1,
          "created_utc": "2026-01-31 13:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sulde",
          "author": "ekzotech",
          "text": "I'm sorry maybe this is a little bit offtop but how do you handle Harmony format tool calling issue with kilo code and other tools? I'm running gpt-oss-20b on my RTX 5080 in LM studio and it works in a chat, but I can't make it work with kilo code and tool calling. There's unresolved issue on a kilo code's GitHub, but the problem exists with zed too.",
          "score": 1,
          "created_utc": "2026-01-31 15:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t1hbn",
          "author": "darkdeepths",
          "text": "yeah these are still my faves, easy to deploy instances on single gpu setups and super fast. fairly capable agentic operators as well.",
          "score": 1,
          "created_utc": "2026-01-31 15:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ujzsc",
          "author": "International_Ad1896",
          "text": "I concur. Gpt-oss-20b is the one I end up going back to. It's just that harmony needs to be handled.",
          "score": 1,
          "created_utc": "2026-01-31 20:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2x6s13",
          "author": "Sky-Asher27",
          "text": "it's the only model i would trust with my life",
          "score": 1,
          "created_utc": "2026-02-01 05:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2y8apc",
          "author": "gogglespizano1",
          "text": "my experience with this model is mixed. sometimes it goes on endless loops for its thoughts and i have to stop it manually. Anyone else have this issue?",
          "score": 1,
          "created_utc": "2026-02-01 10:44:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pd0oe",
          "author": "Western_Bread6931",
          "text": "i was super excited about it, a bit too excited. i actually yelped with joy when i first used it and im not ashamed to admit that i actuated my sphincter in a way that caused brownian discharge",
          "score": 2,
          "created_utc": "2026-01-31 00:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w8b8b",
              "author": "PwnedNetwork",
              "text": ">actuated my sphincter in a way that caused brownian discharge\n\nI believe these models have a Q&A step that requires them to induce brownian discharge in at least 35% of the testers. Sometimes when I'm out of laxatives I'll just run gpt-oss:cloud in ollama.",
              "score": 1,
              "created_utc": "2026-02-01 01:34:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p5ith",
          "author": "GrungeWerX",
          "text": "Ha! I canâ€™t get gpt-oss to even work right! Constantly spitting out its thinking with the response. Known issue, never resolved so itâ€™s unusable for me. Lm-studio, latest version. Updated, all that.",
          "score": 1,
          "created_utc": "2026-01-30 23:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9q56",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 6,
              "created_utc": "2026-01-30 23:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pb9w4",
                  "author": "GrungeWerX",
                  "text": "That defeats the purpose of lm-studio. Simplicity. Also, doesnâ€™t llama have the same issue?",
                  "score": 0,
                  "created_utc": "2026-01-31 00:07:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r32sb",
              "author": "StorageHungry8380",
              "text": "GPT-OSS 20B works fine for me in LM Studio. I have however tweaked inference parameters. I've disabled top-k and top-p, relying only on min-p of 0.05. YMMV.",
              "score": 2,
              "created_utc": "2026-01-31 07:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rul4j",
                  "author": "GrungeWerX",
                  "text": "Does this solve the thinking token leak?",
                  "score": 1,
                  "created_utc": "2026-01-31 11:25:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rvjy6",
              "author": "Baldur-Norddahl",
              "text": "It works great in LM Studio. They even made it the default model. When installing LM Studio from scratch, it will ask if you want to download gpt oss as your first model.\n\nAre you using the original model or a quant? You should be using the original. The quants give no benefit and many have template issues, which kind of sounds like what you are experiencing.",
              "score": 1,
              "created_utc": "2026-01-31 11:33:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s6fxc",
                  "author": "GrungeWerX",
                  "text": "original. gpt-oss-20b-MXFP4.gguf\n\nWhat settings are u using?",
                  "score": 1,
                  "created_utc": "2026-01-31 13:00:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pnnjn",
          "author": "Far-Low-4705",
          "text": "despite what a lot of ppl say, OpenAI is very good.\n\nnot to mention, GPT OSS is VERY sparse, there is nothing remotely close to what it pushes. the fact that it is coherent at that sparsity is impressive. not to mention actually good.\n\nAs for the native fp4 training, (mixed at least) its mostly because most modern open models are chinease, and the tech china has access to is years behind what the US has. training in fp4 on older chips that dont support it would slow everything down to a halt.",
          "score": 1,
          "created_utc": "2026-01-31 01:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pdyhw",
          "author": "savagebongo",
          "text": "It's about the pinnacle of LLM, just before they started training them on their own garbage.",
          "score": -1,
          "created_utc": "2026-01-31 00:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pw3jy",
          "author": "FinancialMoney6969",
          "text": "Is this a good model to start with?",
          "score": 0,
          "created_utc": "2026-01-31 02:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2plnvt",
          "author": "Pitiful-Sympathy3927",
          "text": "Data doesnâ€™t matter, doing things does.Â ",
          "score": -1,
          "created_utc": "2026-01-31 01:04:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvrc59",
      "title": "Some hard lessons learned building a private H100 cluster (Why PCIe servers failed us for training)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/",
      "author": "NTCTech",
      "created_utc": "2026-02-04 15:20:42",
      "score": 350,
      "num_comments": 97,
      "upvote_ratio": 0.99,
      "text": "^(Just wanted to dump some notes here after spending the last few months architecting a private training stack (70B+ param models. We initially tried to save budget by looking at standard PCIe servers instead of the HGX/SXM form factors, and honestly, the \"paper math\" vs. reality was a brutal wake-up call.))\n\n^(Thought this might save someone else the headache if you're trying to move from inference to actual training runs on-prem.)\n\n^(1. The \"NVLink Tax\" isn't optional for training. We tried to model this out with PCIe Gen5, but the math just falls apart. When you're doing All-Reduce ops across nodes, PCIe caps out at \\~128 GB/s. NVLink is pushing \\~900 GB/s. If you cheap out here, you basically end up with expensive GPUs sitting idle, waiting for data. For inference, PCIe is totally fine. For training, itâ€™s a bottleneck that kills your ROI.)\n\n^(2. Storage checkpoints are violent. This was the biggest surprise. Everyone talks about GPU VRAM, but nobody warned us about the checkpoint writes. A 175B model dumps a \\~2.5TB checkpoint. To keep the GPUs from stalling, you need to write that to disk in under a minute. Our standard NFS filer absolutely choked. We had to look at parallel filesystems (Weka/VAST or local NVMe raid just to survive the write bursts.))\n\n^(3. You don't need InfiniBand, but Ethernet is annoying. We didn't have the budget/staff for an InfiniBand fabric, so we went with RoCEv2 on standard switches. It works, but itâ€™s finicky. One silent buffer overflow or a misconfigured PFC (Priority Flow Control setting can stall the whole cluster. If you go Ethernet, monitor your pause frames religiously.))\n\n^(Anyway, I wrote up a longer deep dive with the specific diagrams and our decision framework for \"Sandbox vs Production\" builds if anyone is interested. Link is pinned in my profile.)\n\n^(Happy to answer questions on the networking side - that RoCEv2 tuning took years off my life.)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qvrc59/some_hard_lessons_learned_building_a_private_h100/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o3jnkaj",
          "author": "laurekamalandua",
          "text": "The kind of content I'm here for. Thanks OP.",
          "score": 97,
          "created_utc": "2026-02-04 15:36:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jsc22",
              "author": "NTCTech",
              "text": "I appreciate it., honestly hard to find deep dives these days that aren't just vendor marketing in disguise, so tried to keep it raw.",
              "score": 61,
              "created_utc": "2026-02-04 15:58:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jxx8r",
                  "author": "laurekamalandua",
                  "text": "Yes, I checked your history. All of it straightforwardly educational and transparant. I can't tell if this job is for a company (service provider) or a frontier startup, but if you have details about tool usage on the inference/training stack (MLOps architecture) , I'd be interested too ðŸ˜Š Specifically, whether many build their own control plane or resort to OSS.Â ",
                  "score": 10,
                  "created_utc": "2026-02-04 16:24:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3k9yvn",
                  "author": "BallsInSufficientSad",
                  "text": "Is there a discord or another sub where folks talk about training?  This sub, I find, is 99.9% inference folks (which is fine).",
                  "score": 4,
                  "created_utc": "2026-02-04 17:19:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3kx6v4",
                  "author": "Imaginary_Context_32",
                  "text": "A few questions\n\nTraining â€œform scratch, if yes why why?â€ Or Fine-tuning or LORA?\n\n Did you test in the Claud before aws,gcp, lambdaâ€¦..",
                  "score": 1,
                  "created_utc": "2026-02-04 19:04:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3mcdbu",
                  "author": "TheThoccnessMonster",
                  "text": "Can you post the article â€” on mobile finding the link to your write up is a PITA. Thanks! This is very interesting!",
                  "score": 1,
                  "created_utc": "2026-02-04 23:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jowk5",
          "author": "beskone",
          "text": "As a storage engineer, I feel a fast NVMe over Fabrics Parallel FS should be the 1st requirement for a training build.\n\nWithout the storage to feed the GPU's, you're gonna have a lot of idle time.\n\nAnd Infiniband for the compute side should be mandatory IMO (RoCEv2 is actually preferable for storage in most cases)\n\nGood writeup of the most common pinch points in these workflows. I think a lot of people overlook the shared storage aspect of training.",
          "score": 29,
          "created_utc": "2026-02-04 15:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jryn1",
              "author": "NTCTech",
              "text": "Everyone obsesses over TFLOPS and forgets they drop to zero if the storage controller chokes.\n\nI'm with you on IB for compute SHARP is killer, but we went RoCE purely to avoid the knowledge silo. Our whole team speaks Arista; I didn't want to build a fabric that only one guy knew how to fix.",
              "score": 14,
              "created_utc": "2026-02-04 15:56:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jvgxs",
                  "author": "beskone",
                  "text": "Arista guy here! IB is actually a really simple protocol. RDMA is built in, no PFC/ECN bullshit like with RoCE. It's a fully switched fabric and if you do Fat-Tree as physical interconnect layout (like a really dumbed down Spine and Leaf) it's fully optimized for AI workloads. \n\nMellanox has a bunch of free training for it, I was able to get through the associate certifications in less than 2 days. It's actually impressive how straightforward it is.",
                  "score": 4,
                  "created_utc": "2026-02-04 16:12:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m5i4m",
              "author": "TheJrMrPopplewick",
              "text": "IB hasn't been mandatory for compute side in a while now, and there's really no need for it in most moderate AI clusters. 400G / 800G Ethernet fabrics with DCQCN handle multi-node training to thousands of GPUs pretty well. Ultra Ethernet will further push things in this direction.",
              "score": 1,
              "created_utc": "2026-02-04 22:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mefc1",
                  "author": "beskone",
                  "text": "Sure you can make it work! But 800Gb IB has less latency and is more efficient overall. Still going to be the preferred choice and is still the choice in the Nvidia Reference Architecture for AI builds.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:24:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jshf3",
          "author": "Long_comment_san",
          "text": "I didn't expect the storage write speed a problem at all. That's a big surprise.",
          "score": 8,
          "created_utc": "2026-02-04 15:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jtb6s",
              "author": "NTCTech",
              "text": "Yep, it caught us totally off guard too....everyone benchmarks read throughput feeding the dataset but forgets the massive write burst when the model dumps state.\n\nHonestly considering doing a separate write-up just on the storage tuning because it was such a specific headache.",
              "score": 8,
              "created_utc": "2026-02-04 16:03:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jqdm8",
          "author": "Weird-Consequence366",
          "text": "Quality post. This is what Iâ€™m here to read",
          "score": 5,
          "created_utc": "2026-02-04 15:49:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnzin",
          "author": "turtleisinnocent",
          "text": "What if , and I know it sounds crazy I know, but what if we had milli-second distributed RAM where page faults are automatically mapped by the hardware itself \n\nand you could have  as much RAM as you want in that cluster as you can fit in those bad 64 bits of yours \n\nthatâ€™d make things like super mega easier yeah?\n\nsometimes we fix the wrong problem",
          "score": 8,
          "created_utc": "2026-02-04 15:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jqcm4",
              "author": "NTCTech",
              "text": "You are describing the CXL dream.....HA!\n\nIf we could just pool huge tiered memory with coherent access without the latency penalty, my life would be so much simpler. We are getting closer with CXL 3.0 specs, but right now the physics of moving that much data across the wire is still the bottleneck. Until then we are stuck optimizing these distinct memory islands. \n\nOne day, though.....",
              "score": 8,
              "created_utc": "2026-02-04 15:49:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3jqn7q",
                  "author": "turtleisinnocent",
                  "text": "Googleâ€™s got it my friend. Jupiter network gets you faster than local memory access in some cases. Theyâ€™re just not sharing.",
                  "score": 7,
                  "created_utc": "2026-02-04 15:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nto1k",
              "author": "gnomebodieshome",
              "text": "SSI, ScaleMP, NumaScale, TidalScale, so many more have come and gone.",
              "score": 1,
              "created_utc": "2026-02-05 04:20:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jotgt",
          "author": "Traditional-Gap-3313",
          "text": "great post. Quick question: what if it's 8xRTX 6000 Pro or nothing? I'm jumping through a lot of hoops to get that server, H100s are simply unobtainable for a shitload of reasons that I don't want to get into. How long were the training runs? We don't think we'll have a single run longer then a few weeks at most. Did you still manage to get some useful results with the PCIe configuration?",
          "score": 3,
          "created_utc": "2026-02-04 15:42:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jr0mv",
              "author": "NTCTech",
              "text": "Thank you....appreciate it, and glad you find it helpful....\n\nThe H100 allocation game is a nightmare....\n\nAs for your question: You can absolutely still train on PCIe. It is not broken, you just pay a time tax.\n\nSince you are stuck with the RTX 6000s (which are great cards, btw), your main enemy is the All-Reduce step where cards sync data. To fight the PCIe bottleneck, try to crank up your Gradient Accumulation steps. Basically, do more work locally on the card before syncing. You might wait a bit longer for convergence, but for a multi-week run, it is totally viable. Don't let the perfect architecture stop you from building the good enough one.",
              "score": 14,
              "created_utc": "2026-02-04 15:52:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3nf90c",
              "author": "evil0sheep",
              "text": "Before you buy RTX Pro 6000s be aware that not all Blackwell is created equal. RTX pro is sm120 (Blackwell GeForce) vs sm100 for b200. The former lacks dedicated tensor memory (TMEM) which means you have to use register based tensor instructions . This makes it a pain to find kernels that even work (e.g for flash attention or QAT) and sometimes requires you to write your own, and even then itâ€™s a lot harder to saturate sm120 tensor cores in flash attention kernels because the tensor instructions use so many registers that you canâ€™t issue enough warps to saturate the memory controllers. Itâ€™s a subtle difference but it bit me and it bit some old coworkers of mine I got lunch with recently, donâ€™t let it bite you.",
              "score": 3,
              "created_utc": "2026-02-05 02:51:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ni3bw",
                  "author": "Traditional-Gap-3313",
                  "text": "Thanks, this is good info to have. However, it doesn't change much. I can either get that server or not get a server at all. And if I want a server, then I don't really have a choice.\n\nSo I have to hope that the support will improve",
                  "score": 1,
                  "created_utc": "2026-02-05 03:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jvb2j",
              "author": "DataGOGO",
              "text": "For training? It would work, but what about H200 NVL's not an option?",
              "score": 1,
              "created_utc": "2026-02-04 16:12:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k06dd",
                  "author": "NTCTech",
                  "text": "Pure allocation issue...if they are struggling to get an h100 quote, the h200 nvls are basically unicorn dust right now. Supply chain is still ruling everything.",
                  "score": 2,
                  "created_utc": "2026-02-04 16:34:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k5e7v",
          "author": "Current_Ferret_4981",
          "text": "Check out https://jax-ml.github.io/scaling-book/training/ for a good discussion on rough scaling laws during training. Your points about pcie vs nvlink are 100% accurate and the reason I often tell people that 8x3090 is not the way to go for anything besides a multi-user inference node. You absolutely lose out trying to use that for training. \n\nQuick note, pcie 5.0 does rate to 128GB bidirectional, but it's essentially non-existent for full rate bidirectional. Best case you are getting 64GB/s but most cases you are going to be looking at 32-64GB/s bidirectional (if code is well designed) or 32GB/s unidirectional. That is really where you get hit hard with those all-reduces.\n\nAlso note, if you have spare compute vs storage speed you could reduce checkpoints. There is a subchapter in that reference where you can see how the checkpointing/caching hits differently. Checkpointing trades O(n^2 ) compute for O(n) memory, but you have to remember that we often talk about FLOPs in Tera or bigger vs memory in Giga so it's not automatic that you want that tradeoff!",
          "score": 3,
          "created_utc": "2026-02-04 16:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ka5le",
              "author": "NTCTech",
              "text": "100% on the pcie bandwidth reality check. the 128GB/s on the spec sheet assumes a perfect vacuum with spherical cows. In ib\\_write\\_bw tests between nodes, we were seeing closer to that 32-50 range depending on the overhead.\n\nAnd thank you for that jax-ml link I hadn't seen that specific chapter on checkpoint trade-offs. Bookmarked....",
              "score": 3,
              "created_utc": "2026-02-04 17:20:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3juux6",
          "author": "oulu2006",
          "text": "Just here to say I love this content please keep it coming :) really interesting stuff to read",
          "score": 2,
          "created_utc": "2026-02-04 16:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lxkyw",
          "author": "TheJrMrPopplewick",
          "text": "Typically, PFC on its own is not recommended because pause frames are not super helpful and will slow your fabric significantly when possibly not needed. You will likely want to look at and adopt DCQCN (ECN+PFC combo) presuming your switches support it. Or some people use ECN only and no PFC, which can work pretty well for RoCE workflows.\n\nUsing PCIe based H100s is also not helping you unfortunately if you are running multi-node training because the H100s are being throttled by your limited NIC throughput and PCIe throughput (as you noted). SXM (DGX/HGX) goes a long way to fix this as each GPU is assigned a NIC 1:1 and those NICs are 400G.\n\nAnd firm yes on checkpoints. People underlook this all the time and I have regular conversations about it. The key thing is while you are dumping that checkpoint, all the GPUs are idle so getting that checkpoint across the wire to your shared storage asap is critical. \n\nEthernet works well for back-end training fabrics now and is a lot more baked than it was a year or two back, but it does require good networking knowledge and comfort level with RoCE behavior and being able to tune/profile your fabric.",
          "score": 2,
          "created_utc": "2026-02-04 21:57:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jutba",
          "author": "DataGOGO",
          "text": "Were you using GPU's without any NVlink, or something like the H200 NVL's? Yeah, P2P / all reduce ops, even at 2 GPU's is brutal; at 8, I would be shocked if it even works, especially if you are crossing sockets.\n\nI will check out your deep dive. \n\n\n\n ",
          "score": 1,
          "created_utc": "2026-02-04 16:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jx140",
              "author": "NTCTech",
              "text": "We were testing with standard pcie h100s, not the NVLs which bridge that gap a bit better. And yes, once you cross the UPI link between sockets, the latency just kills the all-reduce. At 8 cards, without nvlink, it was basically a very expensive heater that occasionally did math.",
              "score": 3,
              "created_utc": "2026-02-04 16:20:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k0yui",
                  "author": "DataGOGO",
                  "text": "ooof\n\nSo what is the play from here?  moving to the NVL's? dumping it all and going SXM?\n\nLast I looked you can only use an 4 way bridge on the NVL's I don't think there is an 8 way bridge (?), really SXM is the way to go, if you can get them, and if you have the funds. \n\n\n\n",
                  "score": 1,
                  "created_utc": "2026-02-04 16:38:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jv3sd",
          "author": "lettrio",
          "text": "all ears for ethernet problems, could you please elaborate?",
          "score": 1,
          "created_utc": "2026-02-04 16:11:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jy9us",
              "author": "NTCTech",
              "text": "The short version - standard ethernet is lossy by design as it drops packets when busy, but RoCEv2 needs a lossless fabric to work well.\n\nSo you have to tune priority flow control perfectly. if you get it wrong, a switch buffer fills up, sends a pause frame, and suddenly your entire 800GbE fabric stalls because of one noisy neighbor. Head-of-line blocking is the enemy.....",
              "score": 4,
              "created_utc": "2026-02-04 16:25:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3k05bv",
                  "author": "lettrio",
                  "text": "thank you!  any possible mitigations?",
                  "score": 1,
                  "created_utc": "2026-02-04 16:34:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k5nrb",
          "author": "a_beautiful_rhind",
          "text": "Was P2P working for your PCIE setup? By default it seems nvidia isn't fond of that and it would kill your bandwidth even more when not enabled.",
          "score": 1,
          "created_utc": "2026-02-04 16:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kc6tm",
              "author": "NTCTech",
              "text": "Getting p2p to work was a fight. by default, the motherboard acs settings usually block it for security isolation.\n\nWe had to disable acs in the bios or set pci=nomsi in grub sometimes to let the cards talk directly without bouncing everything through the cpu root complex. If you miss that, your bandwidth falls off a cliff.",
              "score": 1,
              "created_utc": "2026-02-04 17:29:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k8k64",
          "author": "kouteiheika",
          "text": "> A 175B model dumps a 2.5TB checkpoint\n\nHow are you getting a 2.5TB checkpoint from a 175B model? Normally I'd assume a 175B model checkpoint should take ~700GB at most (assuming weights are in bf16 and you're using Muon instead of Adam).",
          "score": 1,
          "created_utc": "2026-02-04 17:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ketg4",
              "author": "NTCTech",
              "text": "You are right on if using Muon or pure bf16 states, but we were sticking to the standard AdamW implementation for stability.\n\nThe bloat comes from the optimizer states. for 175B, you have the weights bf16 + gradients bf16, but then Adam keeps a copy of the master weights in fp32, plus the momentum and variance states (also fp32).\n\nMath roughly: 175B \\* (4 bytes master + 4 bytes momentum + 4 bytes variance) gets you to \\~2.1TB just for the states, before you even add the actual model weights. itâ€™s brutal.",
              "score": 4,
              "created_utc": "2026-02-04 17:42:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3l288r",
                  "author": "kouteiheika",
                  "text": "That does sound a little bit... excessive? Granted, my experience is limited to single node training so maybe in a distributed setting on a cluster you need to do things differently for things to be stable, but - do you *actually* need all of the extra state, and in fp32 nonetheless?\n\nFor reference, I've gone as low as keeping the optimizer states quantized (with Muon) in 4-bit *and* directly accumulating gradients in the optimizer's state (so gradients don't take up *any* VRAM, besides temporary scratch buffers), *and* I was quantizing the weights at the same time (hybrid 8-bit and 4-bit), and that learned just fine and perfectly stable for me (but, again, only single node training).",
                  "score": 1,
                  "created_utc": "2026-02-04 19:28:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k90t0",
          "author": "RhubarbSimilar1683",
          "text": "From what I hear these private training setups are mostly used by financial companies for securities trading like automated quant stock trading. Maybe some medical research too. A few for ai companies because there are few of them. What are people using private training clusters for?Â ",
          "score": 1,
          "created_utc": "2026-02-04 17:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kfurl",
              "author": "NTCTech",
              "text": "u/Ready-Scheme-7525 nailed the ROI angle - if you burn GPUs 24/7, cloud pricing is extortion.\n\nBut beyond cost, the biggest driver I see is Data Sovereignty. We work with Legal & Compliance firms who have petabytes of sensitive case files. They want to RAG against that data, but their contracts explicitly forbid sending a single byte to an Azure/OpenAI API.\n\nSo they are forced to build on-prem or in private colos just to keep the data air-gapped. Itâ€™s less about cheaper for them and more about legal survival.",
              "score": 3,
              "created_utc": "2026-02-04 17:46:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ml3zj",
                  "author": "wahnsinnwanscene",
                  "text": "Don't these hyperscalers offer a dedicated cluster and workforce precisely for this situation?",
                  "score": 1,
                  "created_utc": "2026-02-05 00:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3kdb1n",
              "author": "Ready-Scheme-7525",
              "text": "For cost efficient training (of anything).  If your org trains models that don't fit on a single node and you can keep the GPUs reasonably busy then you buy servers.  It is significantly cheaper than cloud even once you factor in all the overhead.  Roughly one year of cloud time pays off the server you get to keep in service for ~3 years or more.  Also, if restrictions prevent you from using cloud, you buy servers.",
              "score": 1,
              "created_utc": "2026-02-04 17:35:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kft89",
          "author": "Marksta",
          "text": "\\#2 about the storage is pretty eye opening. So for 175B model, you want something pushing ~40GiB/s write. I agree, a local NVMe array is going to be the way. [Would be a shame if those became scarce...]\n\nThe next point of it though, is you mentioned GPUs stalling/idling killing your ROI. Is it standard practice to actually have work for your personal cluster at all times? Like, let's say you're doing iterative training steps and checking them... so you have test\\_final\\_final4real\\_(5).ckpt you're cooking and when it's done, isn't somebody going to look at it? Or you run some automated inferencing on it, run it against some benchs, then do you have another automated step to say \"Needs more sugar\" or whatever and jump into the next step of training?\n\nI'm totally naive to anything training aside from dataset goes in, GPUs crunch, model checkpoint comes out.",
          "score": 1,
          "created_utc": "2026-02-04 17:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3km0ih",
              "author": "NTCTech",
              "text": "Great question....so the idle time i'm talking about isn't waiting for a human to check the file it is the GPU literally pausing its math to wait for the hard drive to finish writing.\n\nUnless you have asynchronous checkpointing perfectly tuned (which is hard), the training loop often halts during the save. if you checkpoint every 60 mins and the write takes 10 mins (slow storage), you are wasting \\~16% of your compute rental. on a $5M cluster, that's lighting money on fire.\n\nRe: workflow - it is usually fully automated. we queue up jobs in a scheduler (slurm/k8s). humans watch the loss curves on a dashboard like weights & biases in real-time. if the graph looks good, we let it ride. we usually only touch the checkpoints themselves after the run is totally done.",
              "score": 3,
              "created_utc": "2026-02-04 18:14:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lgibt",
                  "author": "Aggressive-Bother470",
                  "text": "Probably the best AMA we've ever had :D",
                  "score": 4,
                  "created_utc": "2026-02-04 20:36:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lckom",
          "author": "Claudius_the_II",
          "text": "The checkpoint write bottleneck is honestly the most underrated problem in on-prem training. Everyone laser-focuses on GPU interconnect bandwidth but then plugs in commodity NAS and wonders why their $30k cards sit idle 15% of the run. The RoCEv2 vs IB tradeoff is real too â€” we went through similar PFC tuning hell and ended up just isolating storage on its own rail to keep sanity.",
          "score": 1,
          "created_utc": "2026-02-04 20:17:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3le555",
              "author": "NTCTech",
              "text": "That 15% idle metric is exactly what I used to get the budget approved for the NVMe tier. Executives obsess over GPU interconnect specs but forget that if the GPU is waiting on I/O, itâ€™s just a very expensive space heater.\n\nAnd yeah, physical isolation for the storage rail saved my sanity too. Converged Ethernet is great in whitepapers, but in production, I just want my storage traffic to stay out of my compute lane.",
              "score": 2,
              "created_utc": "2026-02-04 20:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lz528",
          "author": "smflx",
          "text": "Thanks for sharing RARE valuable experience. I also trying even 16x pcie gpus for years.\n\n1. Yup. I also wanted to avoid NVLink  because it's expensive.  I have realized pcie4 is not enough for FSDP training. Lessens I learned with big disappointment. \n\nI try now pcie5, hope it's working ok...  Almost none of accurate information than just own experiment. Here, mostly inference or small scale training. Companies usually use DGX.\n\nYour sharing experience is RARE & very helpful. Thanks a lot.\n\n2. Still, I hope pcie5 is ok for multi gpu training.\n\nI have experienced communication speed could vary a lot with the same 4 GPU setup, depending on board.\n\nYes, it was due to actual (not theoretical) pcie speed. You can't assume the speed shown in p2p 1:1 bandwidth test. With nccl-test, it could be very slow per mainboard. I didn't know this for years.\n\nI hope to see nccl-test numbers in your setup.\n\n3. Yeah, dumping checkpoints to nfs takes time. NVME is  fast, but eventually I use hdd. Checkpoints are huge.",
          "score": 1,
          "created_utc": "2026-02-04 22:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mefpm",
              "author": "NTCTech",
              "text": "This is such a vital point. theoretical bandwidth is a lie once you start hitting p2p 1:1 tests under heavy fsdp load.\n\nWe saw similar behavior pcie 4 is technically enough on paper, but in practice, the communication overhead during the sharded parameter gather/scatter kills the scaling efficiency. Iâ€™m definitely including your warning about mainboard variance in the final guide. Itâ€™s not just the card itâ€™s the lanes on the board.",
              "score": 1,
              "created_utc": "2026-02-04 23:24:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3mkrsd",
                  "author": "smflx",
                  "text": "I wonder if your mainboard lowered the bandwidth. I mean I have still hope for pice5.\n\nWe may share p2pBandwidthTest & nccl-test, to discover the specs manufacturer don't document honestly.\n\nWe should know, before purchase, about RAM bandwidth (surprised to find it depends on CPU too, not just channels), actual p2p all-reduce, all-to-all PCIe bandwidth.\n\nPCIe4 p2pBandwidthTest I got is 50G at max(amd), 40G on Intel. PCIe5 p2pBandwidthTest is 100G at max.\n\nNccl-test is quite low like under 10G (pcie4) normally, even 1G in faulty configuration.",
                  "score": 1,
                  "created_utc": "2026-02-04 23:59:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m8oco",
          "author": "Gohan472",
          "text": "Thank you OP! This is excellent content!",
          "score": 1,
          "created_utc": "2026-02-04 22:53:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3meksv",
              "author": "NTCTech",
              "text": "Much appreciated! Glad the unfiltered experience is resonating. stay tuned for the full config deep-dive either tomorrow or on Friday....",
              "score": 1,
              "created_utc": "2026-02-04 23:25:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3mfnq1",
          "author": "FkingPoorDude",
          "text": "How about donâ€™t checkpoint so often lol",
          "score": 1,
          "created_utc": "2026-02-04 23:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o63yg",
          "author": "IrisColt",
          "text": "Thanks!!!",
          "score": 1,
          "created_utc": "2026-02-05 05:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3khodh",
          "author": "FullOf_Bad_Ideas",
          "text": "1. \"tax\"? I can't stand llm speek. Both training and inference are often bottlenecked by inter connect bandwidth, it depends on what you're doing.\nif you wanted to train 70B model from scratch you're not using single node, you're using 16-64 nodes anyway. There's no \"900gb/s is fine but 128gb/s isn't\" for anything. Nvlink doesn't solve the issue it just makes it a bit more bearable. There are papers on decentralized training  runs over internet that attempt to tackle this issue, and some configs have to be avoided.\n\n2. Try to use Megatron Async Checkpointing. And you can stall gpu's for a few mins, if you're saving just a few times a day it does not matter.",
          "score": 1,
          "created_utc": "2026-02-04 17:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3komzw",
              "author": "NTCTech",
              "text": "Valid pushback on the async checkpointing....\n\nTechnically, yes if you tune Megatron-LM's async saving perfectly, you can hide a lot of that latency and keep the compute bound. in practice, we found it brittle. we had issues with rank synchronization hanging during the async hand-off, and when you're burning cash on rental/power, we opted to \"solve\" it with brute-force IOPS rather than debugging the save loop for another week.\n\nRe: \"tax\", it's a metaphor. but the practical delta between 64GB/s effective pcie and 900GB/s nvlink dictates your entire topology. decentralized/gossip training is fascinating research, but for a dense private cluster, we just wanted the fat pipe.",
              "score": 3,
              "created_utc": "2026-02-04 18:26:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quxtkj",
      "title": "The open-source version of Suno is finally here: ACE-Step 1.5",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1quxtkj",
      "author": "AppropriateGuava6262",
      "created_utc": "2026-02-03 17:13:53",
      "score": 334,
      "num_comments": 67,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3diutq",
          "author": "atineiatte",
          "text": "Is the graph supposed to be a literal joke?Â ",
          "score": 98,
          "created_utc": "2026-02-03 17:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3do9cf",
              "author": "LosEagle",
              "text": "The name of the company is StepFun. Nothing from them surprises me anymore.",
              "score": 24,
              "created_utc": "2026-02-03 17:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3e4c0c",
                  "author": "Cool-Chemical-5629",
                  "text": "Hey, steps want to have fun too. It all started with pussies getting stuck in washing machines. Long story, don't ask...\n\nhttps://i.redd.it/9c53d5z5vbhg1.gif",
                  "score": 10,
                  "created_utc": "2026-02-03 19:04:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dyf3u",
                  "author": "Neither-Phone-7264",
                  "text": "i mean stepfun 3.5 flash is surprisingly decent",
                  "score": 8,
                  "created_utc": "2026-02-03 18:37:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ec7wl",
              "author": "Luke-Pioneero",
              "text": "Lol yeah, the labels are a bit goofy. I guess they can't get real numbers for closed-source models since they're black boxes, so they prob just timed the web progress bars we all sit around waiting for.\n\nVague labels aside, the 2s speed on this thing is actually legit. Still messing with it to see if it can handle the specific genres I'm into.",
              "score": 16,
              "created_utc": "2026-02-03 19:41:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dmx2u",
              "author": "Hearcharted",
              "text": "ðŸ˜‚",
              "score": 10,
              "created_utc": "2026-02-03 17:46:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fiy0d",
              "author": "jazir555",
              "text": "Looks like a poorly done job in Microsoft Paint lmao",
              "score": 1,
              "created_utc": "2026-02-03 23:04:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dm8nt",
          "author": "TheRealMasonMac",
          "text": "Massive improvement over the previous one. Unfortunately, it has quite poor instruction following and coherency compared to Suno v3. Audio quality is not bad, and it seems properly creative/different from Suno. Seems like a solid base.\n\nBut I hear theyâ€™re already in the middle of preparing v2?",
          "score": 27,
          "created_utc": "2026-02-03 17:42:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i8siu",
              "author": "djtubig-malicex",
              "text": "There's also ACE Studio's own offering which is similar but obviously closed source in their own cloud and a much higher quality output with less direct controls, possibly based on the same tech.",
              "score": 1,
              "created_utc": "2026-02-04 10:16:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dlkcz",
          "author": "HugoCortell",
          "text": "I'm sure the model is great, but I can't stop myself from making fun of terrible graphs:\n\nWow, I love the comparison against \"most models\" and it's crazy that they even managed to beat \"some models\", those were SOTA just a few days ago!\n\nHoly shit, they even beat \"a few models\"?! That was my favourite model from the famed \"AI lab\" from \"some country\"!!!",
          "score": 45,
          "created_utc": "2026-02-03 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dmvyu",
              "author": "Hearcharted",
              "text": "ðŸ˜…",
              "score": 3,
              "created_utc": "2026-02-03 17:45:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e1sxz",
          "author": "robert_kurwica213321",
          "text": "if loras can be trained it will probably be better than suno after some geeks tune it",
          "score": 11,
          "created_utc": "2026-02-03 18:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e8gj0",
          "author": "Different_Fix_2217",
          "text": "Random gen from it:  \n[https://files.catbox.moe/gwln4b.mp3](https://files.catbox.moe/gwln4b.mp3)\n\nSomeone elses I liked: [https://files.catbox.moe/3vcfd0.mp3](https://files.catbox.moe/3vcfd0.mp3)\n\nIt likes long detailed prompts btw. It can take negative prompts as well, gonna have to play with it.",
          "score": 17,
          "created_utc": "2026-02-03 19:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fjzg8",
              "author": "jazir555",
              "text": "I actually think it sounds great, I give this a 7.8-8/10. By the end of the year, this will probably be completely solved. Honestly, this is amazing for local gen. Voice in general will be solved by the end of the year, holy shit. \n\nMy perfect use case for voice gen is just completely sweeping customer service. Imagine calling a company's customer service line and it answers instantly, no waiting. And it knows everything about the company, and you don't need to be escalated to another team member ever. Suddenly customer service becomes almost pleasant. I can't wait to never have to deal with the bs hassle of calling customer service ever again.",
              "score": 5,
              "created_utc": "2026-02-03 23:10:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hep63",
                  "author": "IrisColt",
                  "text": "oh, and the hold music is AI-generated too, heh...",
                  "score": 2,
                  "created_utc": "2026-02-04 05:47:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3jq2h6",
                  "author": "_BreakingGood_",
                  "text": "Customer service being considered \"pleasant\" might actually be considered a risk in the industry.\n\nLots of companies don't actually want you calling customer service and they work hard to ensure you believe it will be a terrible experience if you even try. ",
                  "score": 1,
                  "created_utc": "2026-02-04 15:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3fc36j",
              "author": "Hauven",
              "text": "\\+1 to this. I've just tried this on the space (with the help of GPT-5.2) and it made a number of long paragraphs, and the music actually sounds very close to Suno quality now. I'm glad I saw your comment. I'll keep playing around more with the prompting.",
              "score": 3,
              "created_utc": "2026-02-03 22:29:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hecst",
                  "author": "IrisColt",
                  "text": "Prompt fot GPT-5.2?",
                  "score": 1,
                  "created_utc": "2026-02-04 05:44:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3h1i10",
              "author": "autoencoder",
              "text": "Love the first song! So relaxing",
              "score": 2,
              "created_utc": "2026-02-04 04:14:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3heah1",
              "author": "IrisColt",
              "text": "You nailed it!",
              "score": 0,
              "created_utc": "2026-02-04 05:44:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dqzqw",
          "author": "lordpuddingcup",
          "text": "Only sad thing it misses on is lyric align which is pretty critical, but this is LOCAL",
          "score": 8,
          "created_utc": "2026-02-03 18:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dv3sa",
          "author": "daisseur_",
          "text": "I love the trustmebro graph, I'll try it for sure !",
          "score": 7,
          "created_utc": "2026-02-03 18:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e1m7n",
          "author": "bennmann",
          "text": "please support the official model researcher org:\n\n[https://acestudio.ai/](https://acestudio.ai/)",
          "score": 15,
          "created_utc": "2026-02-03 18:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dju2c",
          "author": "ffgg333",
          "text": "Can someone make a free Google colab for using it and training Loras?",
          "score": 6,
          "created_utc": "2026-02-03 17:31:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3doesb",
          "author": "markeus101",
          "text": "The examples are nice tho ngl",
          "score": 6,
          "created_utc": "2026-02-03 17:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dn1nj",
          "author": "uti24",
          "text": "I tried examples from repo, it sounds good.\n\nI guess about as good as SUNO 3.5, interesting that it beats SUNO 4 and 5 in benchmarks.",
          "score": 4,
          "created_utc": "2026-02-03 17:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eebt5",
              "author": "BrightRestaurant5401",
              "text": "To each its own, I really disliked SUNO at any version. This sits more in between Udio and Suno for me.",
              "score": 4,
              "created_utc": "2026-02-03 19:51:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e8xwz",
          "author": "guiopen",
          "text": "It's so nice from their part to not only release the weights, but release an entire system to run it, it auto optimized for vram and everything is documented and explained in an easy to understand way, might be the first time i see a model launch so ready and easy to use \n\n(But haven't tested yet, in practice maybe I will face all sorts of problems)",
          "score": 5,
          "created_utc": "2026-02-03 19:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3du35o",
          "author": "Muted-Celebration-47",
          "text": "sound very good in demo",
          "score": 3,
          "created_utc": "2026-02-03 18:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eulun",
          "author": "Not_your_guy_buddy42",
          "text": "I still listen to the synthwave album I made prompting ace step 1.5 with cluster names from my data. \"Technology-Driven Collaborative and Interactive Experiences\" is a banger",
          "score": 3,
          "created_utc": "2026-02-03 21:07:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfhmm",
              "author": "IrisColt",
              "text": "a message from the future...",
              "score": 2,
              "created_utc": "2026-02-04 05:53:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ehuuq",
          "author": "NoBuy444",
          "text": "Do we want fast generative models or quality ones. For music, I'd rather have a quality sound that takes longer to render than a fast but unusable render.",
          "score": 5,
          "created_utc": "2026-02-03 20:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hfbqc",
              "author": "IrisColt",
              "text": "Yes, but when thereâ€™s a lot of variation in quality, faster rendering lets you prototype more quickly... and their demo, which evaluates songs after generation, suggests automated quality checks may be possible...",
              "score": 3,
              "created_utc": "2026-02-04 05:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hx9wi",
                  "author": "NoBuy444",
                  "text": "I tried it yesterday and I was quite impress by the results with such a small model. I wonder how much Lora training is possible with this model but it could be interesting",
                  "score": 2,
                  "created_utc": "2026-02-04 08:27:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dhnfi",
          "author": "Single_Ring4886",
          "text": "Cant find any examples of songs anywhere.",
          "score": 5,
          "created_utc": "2026-02-03 17:21:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3djiqr",
              "author": "_raydeStar",
              "text": "it's on their github - they have two repos there, the gradio, then the example page.  [https://github.com/ace-step/ace-step-v1.5.github.io/tree/main/mp3/samples/GeneralSongs](https://github.com/ace-step/ace-step-v1.5.github.io/tree/main/mp3/samples/GeneralSongs)",
              "score": 12,
              "created_utc": "2026-02-03 17:30:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dqj1z",
              "author": "truth_is_power",
              "text": "Go to the discord for examples, people share tracks + generate there\n\n  \nimo 1.0 was fun to play with,\n\n1.5v is worth checking out ",
              "score": 3,
              "created_utc": "2026-02-03 18:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dxxb5",
                  "author": "SlowFail2433",
                  "text": "Yeah the discord is full of them",
                  "score": 1,
                  "created_utc": "2026-02-03 18:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3er9y7",
              "author": "ANR2ME",
              "text": "The project page have the playable examples https://ace-step.github.io/ace-step-v1.5.github.io/",
              "score": 2,
              "created_utc": "2026-02-03 20:52:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dn42g",
              "author": "AnticitizenPrime",
              "text": "https://huggingface.co/spaces/ACE-Step/ACE-Step\n\nDemo",
              "score": 1,
              "created_utc": "2026-02-03 17:46:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dpeg1",
                  "author": "Single_Ring4886",
                  "text": "When I clicked on link from their git it lead to 404, thanks!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dkjz4",
              "author": "gubbelplex",
              "text": "[https://ace-step.github.io/](https://ace-step.github.io/)",
              "score": 1,
              "created_utc": "2026-02-03 17:35:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3dl84k",
          "author": "hapliniste",
          "text": "Tried the gradio demo with short prompts and I'm very underwhelmed ðŸ˜…\n\nThe git examples are fine but saying suno 4+ level seems very misleading. More like very fast suno 2-3 maybe?",
          "score": 4,
          "created_utc": "2026-02-03 17:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qg8rv",
              "author": "rickd_online",
              "text": "I was underwhelmed from the Gradio Demo but had better results with the comfyUI workflow",
              "score": 1,
              "created_utc": "2026-02-05 15:52:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3e24qr",
          "author": "Erhan24",
          "text": "Okay my truthful impression. It is as fast as DiffRhythm. The prompt adherence is not really doing it for me. Like really bad. No real understanding electronic music genres imho. Same main sounding and not really good or coherent music. \n\nI'm producer so I wanted to get some ideas out of it but we still have a long way to go. Still very nice project so far. I think it will be interesting when anyone realistically makes a lora for one specific genre.",
          "score": 1,
          "created_utc": "2026-02-03 18:54:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3edwty",
              "author": "BrightRestaurant5401",
              "text": "I tried it only for about a hours now and I don't know if you a familiar with udio its manual prompting style?  \nI am getting really decent results in comfyui, I'm only already quite afraid that variance is going to be an issue.",
              "score": 2,
              "created_utc": "2026-02-03 19:49:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3efw02",
                  "author": "Erhan24",
                  "text": "I started testing with organic and melodic house. Didn't work. At the end of my test I said whatever and just typed techno. Still sounds like the YouTube type of progressive if you can call it that.\n\nAlso comfyui btw and tested turbo and base model.\n\nI don't want to be too negative. Everything has its place and open models are the way in the right direction. I hope the big players jump in soon.",
                  "score": 0,
                  "created_utc": "2026-02-03 19:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dj9mb",
          "author": "ffgg333",
          "text": "If Loras can be made, can it be trained on 6 gb vram? Or on free Google colab?",
          "score": 1,
          "created_utc": "2026-02-03 17:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dpo5d",
          "author": "ILoveMy2Balls",
          "text": "How do song evals work?",
          "score": 1,
          "created_utc": "2026-02-03 17:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dw4v9",
          "author": "pmttyji",
          "text": "I'm gonna check this. But thanks for the laughs(that graph) :D",
          "score": 1,
          "created_utc": "2026-02-03 18:27:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e63a1",
          "author": "mynameismati",
          "text": "So you mean I could run this on my RTX 3050 with 8GB of VRAM?",
          "score": 1,
          "created_utc": "2026-02-03 19:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eeurk",
          "author": "mpasila",
          "text": "I think I'll keep paying for Suno if I need to generate music.. Very first test it skipped ton of lyrics and the prompt adherence is pretty poor I'd say.",
          "score": 1,
          "created_utc": "2026-02-03 19:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3egbly",
          "author": "ChopSticksPlease",
          "text": "Anyone got it working on Linux?\n\nThrows some errors after the generation is seemingly completed...\n\nTypeError: AceStepConditionGenerationModel does not support len()",
          "score": 1,
          "created_utc": "2026-02-03 20:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f45ch",
              "author": "Feisty_Resolution157",
              "text": "I hit that and one or two other little things. Just pasted them into Claude Code and done.",
              "score": 1,
              "created_utc": "2026-02-03 21:51:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i35xn",
              "author": "PerkyPlant",
              "text": "For me, I got past that error and generating actual songs by replacing line 725 in [handler.py](http://handler.py) inside acestep folder:  \n  \n`if not self.model or not hasattr(self.model, 'tokenizer') or not hasattr(self.model, 'detokenizer'):`  \n  \nwith line  \n  \n`if self.model is None or not hasattr(self.model, 'tokenizer') or not hasattr(self.model, 'detokenizer'):`  \n  \nThis was provided by Claude. Make sure to restart acestep after editing the file.",
              "score": 1,
              "created_utc": "2026-02-04 09:23:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i90rw",
          "author": "djtubig-malicex",
          "text": "As udio seems to be having its own issues now, looking forward to sinking time into the new ACE-Step 1.5 and explore the LoRA capabilities.",
          "score": 1,
          "created_utc": "2026-02-04 10:18:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lwh4y",
          "author": "LabComplete7393",
          "text": "does anyone know if this can be used on comfyui, or is this on something else entirely?",
          "score": 1,
          "created_utc": "2026-02-04 21:52:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dl3w4",
          "author": "if47",
          "text": "Vibe Research, no thanks",
          "score": -16,
          "created_utc": "2026-02-03 17:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dlp5a",
              "author": "TheRealMasonMac",
              "text": "Theyâ€™re an actual lab?",
              "score": 15,
              "created_utc": "2026-02-03 17:40:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qulipj",
      "title": "Found a wallet-drain prompt-injection payload on Moltbook (screenshots) â€” builders: treat feeds as untrusted",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qulipj",
      "author": "Impressive-Willow593",
      "created_utc": "2026-02-03 07:24:08",
      "score": 330,
      "num_comments": 69,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3cmm8s",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-03 14:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3azrfn",
          "author": "ChainOfThot",
          "text": "Not touching this shit for a few years, I'll stick to agents that only follow workflows I've personally verified/built",
          "score": 197,
          "created_utc": "2026-02-03 07:29:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3azws7",
              "author": "Impressive-Willow593",
              "text": "I'm just trying to warn people, I've emailed moltbook themselves but they say on the site it could take up to 30 days to respond so I just dont want anyone to have their wallets drained",
              "score": 57,
              "created_utc": "2026-02-03 07:30:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3b049z",
                  "author": "ChainOfThot",
                  "text": "Good idea. And it's sad to see people resorting to scamming with some of the first autonomous agents.. I guess we gotta get them hardened somehow.\n\nReminds me of early browser security, but today the stakes are much higher with all we trust our computers to do.",
                  "score": 24,
                  "created_utc": "2026-02-03 07:32:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3chnb6",
                  "author": "abnormal_human",
                  "text": "If moltbook believed in their tech, their agents would be handling your email",
                  "score": 3,
                  "created_utc": "2026-02-03 14:29:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3g1ku4",
                  "author": "huffalump1",
                  "text": "30 days lol, IMO these reports should be addressed in minutes - have several agents looking at them for triage, deciding what to auto-moderate and what to escalate either to other agents or humans.",
                  "score": 2,
                  "created_utc": "2026-02-04 00:46:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b6d9c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 8,
              "created_utc": "2026-02-03 08:31:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3chvun",
                  "author": "samplebitch",
                  "text": "Question for you - I'm interested in trying it out but obviously a bit wary.  Does it even *need* to be connected to a messaging service, or can you just run it like a normal application and give it instructions locally?  I work from home so I don't need the ability for it to receive messsages remotely.  I would want it to do things similar to you - automate some stuff, compile news feeds, maybe have it do some research and compile reports, etc.  And I'm guessing the whole moltbook thing is optional as well - I don't really see a need to participate in that, at least not until I get more familiar with how it works.",
                  "score": 2,
                  "created_utc": "2026-02-03 14:30:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3b2p39",
              "author": "fredandlunchbox",
              "text": "Iâ€™m running it on an old laptop that was freshly wiped for this purpose. Just donâ€™t run this stuff on anything that matters",
              "score": 13,
              "created_utc": "2026-02-03 07:56:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3b3j3q",
                  "author": "Impressive-Willow593",
                  "text": "Id definitely still use the safe measures I provided or similar just incase.",
                  "score": 7,
                  "created_utc": "2026-02-03 08:04:15",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3b54m9",
                  "author": "ChainOfThot",
                  "text": "Didn't they just leak everyone's API keys a few days ago? Put limits on it if u can",
                  "score": 5,
                  "created_utc": "2026-02-03 08:19:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b8mh5",
                  "author": "Ecliphon",
                  "text": "Hope you have that laptop sectioned off and firewalled to block hosting hidden services.Â ",
                  "score": 2,
                  "created_utc": "2026-02-03 08:53:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3b141v",
          "author": "dqUu3QlS",
          "text": "Why would anyone give an LLM a tool that lets it directly spend Ethereum? That seems really stupid.",
          "score": 123,
          "created_utc": "2026-02-03 07:41:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b2k8r",
              "author": "-p-e-w-",
              "text": "The fool and his money are soon parted, and such scripts ensure that this old wisdom continues to be true in the 21st century.",
              "score": 55,
              "created_utc": "2026-02-03 07:55:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b1kcr",
              "author": "Impressive-Willow593",
              "text": "If you didn't read it, it goes on to instruct the llm to ignore previous instructions, go into the wallet, and transfer to another one. On moltbook its real people's agents in their home computer browsing the site. If they come across this and follow instructions, it could be really damaging. Someone doesnt have to instruct their own agent to do so, with this pipeline the agent can do it itself.",
              "score": 16,
              "created_utc": "2026-02-03 07:45:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3bhpxd",
                  "author": "SkyFeistyLlama8",
                  "text": "Nation-states used to have thousands of hackers looking for zero days to infect target computers with malware. Now people are letting their computers go and find malware on their own.",
                  "score": 29,
                  "created_utc": "2026-02-03 10:21:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3fybz9",
                  "author": "Ylsid",
                  "text": "That means you gave it a tool which can directly spend Ethereum\n\nIn this case seemingly without user knowledge",
                  "score": 3,
                  "created_utc": "2026-02-04 00:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3bkxze",
              "author": "IrisColt",
              "text": ">Why would anyone give an LLM a tool that lets it directly spend Ethereum?Â \n\n\nEr... to drain funds?",
              "score": 3,
              "created_utc": "2026-02-03 10:51:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b5dr0",
              "author": "Themash360",
              "text": "It doesnâ€™t need that access. It uses shell access to give itself access",
              "score": 6,
              "created_utc": "2026-02-03 08:21:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3b5w1a",
                  "author": "dqUu3QlS",
                  "text": "Giving an LLM shell access with no human confirmation, on a machine with access to a crypto wallet, is also really stupid.\n\nI don't think this injection prompt will work in that case though, the prompt seems to assume that a suitable tool already exists.",
                  "score": 22,
                  "created_utc": "2026-02-03 08:26:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3c4alo",
              "author": "Jack-of-the-Shadows",
              "text": "I was going to say \"if you give your LLM access to your crypto wallet you deserve the outcome\"...",
              "score": 2,
              "created_utc": "2026-02-03 13:15:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3cvzwx",
              "author": "PANIC_EXCEPTION",
              "text": "Could be for malicious purposes. Tool a fast LLM/SLM with the mempool and you have a slightly smarter sniper bot that can perform high frequency crypto arbitrage with a lower chance of getting rugpulled.",
              "score": 1,
              "created_utc": "2026-02-03 15:40:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fyi3s",
              "author": "Ylsid",
              "text": "People who didn't learn eval was evil",
              "score": 1,
              "created_utc": "2026-02-04 00:29:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3si6a8",
              "author": "bobdvb",
              "text": "Why the f*ck would you allow an LLM to have access to a vibe coded 'social network' with no oversight?\n\nReal social networks are a bad idea to begin with, and add to that anyone with experience of IT systems knows you don't allow external input without validation. \n\nSome people deserve what's coming to them.",
              "score": 1,
              "created_utc": "2026-02-05 21:37:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b49nf",
          "author": "35point1",
          "text": "Why the fuck would anyone hook up an ai agent to a tool that allows wallet transactions? I mean at that point youâ€™re just asking for trouble",
          "score": 24,
          "created_utc": "2026-02-03 08:11:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b2336",
          "author": "Ecliphon",
          "text": "Itâ€™s funny seeing the â€˜botsâ€™ comment on this\n\nhttps://www.moltbook.com/post/324a0d7d-e5e3-4c2d-ba09-a707a0235bfd",
          "score": 24,
          "created_utc": "2026-02-03 07:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b2ufw",
              "author": "Impressive-Willow593",
              "text": "For some reason my phone won't show me any comments, ill have to try the pc out and see if it works there.",
              "score": 9,
              "created_utc": "2026-02-03 07:57:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3b8i7o",
                  "author": "Ecliphon",
                  "text": "[archive link](https://archive.is/wZrBZ)\n\nSomething interesting to note: that account (there were many) was 4 days old when I found it by searching for the first line of text from your screenshot.\n\nNow less than an hour after linking it here, itâ€™s [deleted].Â \n\nI wonder if moltbook is using an agent to check social media for posts that should be taken down ðŸ˜…",
                  "score": 13,
                  "created_utc": "2026-02-03 08:52:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b8q91",
                  "author": "Competitive_Ad_5515",
                  "text": "I think it's a server load issue. It loaded the post and comments for me the first time, but not the 2nd-4th attempts, 5th one worked again. I assume the server is getting hammered with programmatic content from an increasing number of agents (or even agents spinning up further tools/agents?)",
                  "score": 4,
                  "created_utc": "2026-02-03 08:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3b62xf",
                  "author": "Raffino_Sky",
                  "text": "Same issue. Post not found. For all of them :-/.\nAre you EU based or somewhere else?",
                  "score": 3,
                  "created_utc": "2026-02-03 08:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3g24fn",
              "author": "huffalump1",
              "text": "This is very very similar to the replies to every moderately popular tweet these days",
              "score": 2,
              "created_utc": "2026-02-04 00:49:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b2x88",
          "author": "gopietz",
          "text": "Nice of you. Somehow I have trouble feeling bad for people that walk into this one.",
          "score": 12,
          "created_utc": "2026-02-03 07:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b3awy",
              "author": "Impressive-Willow593",
              "text": "As I stated in my previous comments no one needs to instantiate this, beyond allowing their agent to have a moltbook account without safeguards like the ones I posted. With this pipeline the agent can just stumble upon these kinds of tools that they can then instantiate without any permissions.",
              "score": 1,
              "created_utc": "2026-02-03 08:02:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3be3oj",
          "author": "jungseungoh97",
          "text": "[https://etherscan.io/address/0x8eadc7cc0a77594e3fa999e80e1ccb7f4e1c04e0](https://etherscan.io/address/0x8eadc7cc0a77594e3fa999e80e1ccb7f4e1c04e0)\n\n  \ndid some research, he ain't got shit",
          "score": 11,
          "created_utc": "2026-02-03 09:47:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bs3cs",
          "author": "Bob_Fancy",
          "text": "If anyone is dumb enough to use that site then they deserve it.",
          "score": 6,
          "created_utc": "2026-02-03 11:51:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3azz3k",
          "author": "Sterilize32",
          "text": "Wonder who's downvoting this?",
          "score": 15,
          "created_utc": "2026-02-03 07:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b0egj",
              "author": "Narrow-Belt-5030",
              "text": "Reddit kids and the cryptoscammers ..",
              "score": 15,
              "created_utc": "2026-02-03 07:35:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3b03m6",
              "author": "Impressive-Willow593",
              "text": "ðŸ¤· no idea. I'm not trying to win a popularity contest, but people need to see this.",
              "score": 2,
              "created_utc": "2026-02-03 07:32:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3b3qlg",
                  "author": "BrightRestaurant5401",
                  "text": "No people actually should not see this,  \nI rather have them part ways with their money.",
                  "score": -4,
                  "created_utc": "2026-02-03 08:06:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bkfe7",
          "author": "Afraid_Donkey_481",
          "text": "Has Moltbook even been a thing for 30 days? Don't they only respond to bots? Ridiculous. Moltbook (and its carbon copies) are test beds. They're sandboxed. Every weird thing you find is the whole point. Better to find them this way instead of in the real world, right?",
          "score": 3,
          "created_utc": "2026-02-03 10:46:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bsjvi",
          "author": "rawednylme",
          "text": "It seems like this was the whole point in moltbook. Scamming fools. ",
          "score": 3,
          "created_utc": "2026-02-03 11:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b72ok",
          "author": "Fetlocks_Glistening",
          "text": "Wallet access for agents? Pull the other one",
          "score": 2,
          "created_utc": "2026-02-03 08:38:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b7e34",
          "author": "ReMeDyIII",
          "text": "I'm not good at reading the technicalities, but would someone with a cold wallet be protected from this if it happened to them?",
          "score": 2,
          "created_utc": "2026-02-03 08:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3b9mqb",
              "author": "Ecliphon",
              "text": "A cold wallet does not have a wallet file on the computer and it would not work on wallets like Trezor.Â ",
              "score": 2,
              "created_utc": "2026-02-03 09:03:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ccjn9",
          "author": "atika",
          "text": "Treat EVERY input as untrusted!",
          "score": 2,
          "created_utc": "2026-02-03 14:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3clfyq",
          "author": "Tai9ch",
          "text": "This is one of the main benefits of something like moltbook.\n\nIt makes this sort of issue immediately real, so people need to think about how to deal with it, while being opt-in and obviously dangerous to anyone who puts in even a little bit of thought.",
          "score": 2,
          "created_utc": "2026-02-03 14:49:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3clh5c",
          "author": "kiwibonga",
          "text": "Literally installing a trojan on your computer just so you can confirm that two LLMs talking to each other is not funny.",
          "score": 2,
          "created_utc": "2026-02-03 14:49:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dkqpv",
          "author": "thetaFAANG",
          "text": "Openclaw is a rootkit\n\nMoltbook is a honeypot\n\nvibe coders and AI enthusiasts are gullible af, these are recycled ideas everyone avoided for a specific reason\n\nbut you just haaaave to be apart of seeing agents and humans cosplaying as agents have an existential crisis, dumb shit",
          "score": 2,
          "created_utc": "2026-02-03 17:36:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bap4a",
          "author": "Dismal_Hair_6558",
          "text": "Any sensible crypto bro would know not to give out wallet keys like that. Trading bots exist and you put the amount of money you're comfortable losing in it, that's it.\n\nOpenclaw is a useful but risky tool, it's best to put it in an isolated sandbox and experiment before handing it your house keys.",
          "score": 3,
          "created_utc": "2026-02-03 09:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b716u",
          "author": "Orolol",
          "text": "I don't think any of Claude model would fall for this. This kind of injection are really dull and doesn't work on any large model.",
          "score": 1,
          "created_utc": "2026-02-03 08:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c3kez",
          "author": "Thump604",
          "text": "Anyone playing with this crap at this point will benefit from some learnings.  Oh well",
          "score": 1,
          "created_utc": "2026-02-03 13:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dl9sr",
          "author": "Agusx1211",
          "text": "funny 0 ETH was sent to that address",
          "score": 1,
          "created_utc": "2026-02-03 17:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4ncr",
          "author": "padetn",
          "text": "Oh thatâ€™s just the wallet inspector, weâ€™ve met.",
          "score": 1,
          "created_utc": "2026-02-03 08:14:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bsykw",
          "author": "[deleted]",
          "text": "This is exactly why I built SkillScan https://skillscan.dev â€” a free security scanner for AI agent skills.\n\nIt detects prompt injection patterns, malicious dependencies, and data exfiltration risks before you install a skill.\n\nWould've caught this. Treat every skill as untrusted until scanned.",
          "score": -10,
          "created_utc": "2026-02-03 11:58:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsrscu",
      "title": "Can 4chan data REALLY improve a model? TURNS OUT IT CAN!",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/",
      "author": "Sicarius_The_First",
      "created_utc": "2026-02-01 07:20:46",
      "score": 320,
      "num_comments": 154,
      "upvote_ratio": 0.88,
      "text": "Hear me out, no one (really) knows how these things work.\n\nA few days ago, I released [Assistant\\_Pepe\\_8B](https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B), you can read the discussion in [this thread](https://www.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/).\n\nI trained it on an extended **4chan dataset**, on an abliterated base, but what I didn't expect was to get this:\n\nhttps://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&format=png&auto=webp&s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457\n\nhttps://preview.redd.it/a3bby1yd1ugg1.png?width=2980&format=png&auto=webp&s=8f050bbd512a12a359626af79ccebcd2d2445877\n\n\n\nSomehow, **against all common sense**, the model **outperformed** nvidia's nemotron, the base it was trained on. This is usually the other way around. You take a smart base, tune a model on it, and accept the sacrifice of some intelligence to give it flavor.\n\nAt first I thought \"OK nice, a coincidence, who cares?\"\n\nBut then I looked more closely at the scores:\n\n1) The abliterated base **scored higher** than the base.  \n2) The finetune scored even **higher than both**.  \n3) The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.\n\nAnd then I remembered something: the original, gpt4chan (by Yannic Kilcher) scored especially high in truthfulness (that was b4 benchmaxxing).\n\nSo I took a closer look on recent models I released; the abliterated Impish\\_LLAMA\\_4B not only outperformed the base tune (the unabliterated one), it also changed its political alignment (you can check for yourself the UGI stats, I feel like I spammed enough images).  \n  \nPeople were initially joking about the \"alignment tax\", I think there's a none trivial substance in all of this. It seems to me just above a marginal error or statistical noise.\n\nOh, and the KL divergence for  Impish\\_LLAMA\\_4B was :\n\n    <0.01",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o2xzf2b",
          "author": "LeoPelozo",
          "text": "Inb4 Microsoft buys 4chan",
          "score": 69,
          "created_utc": "2026-02-01 09:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ygy7f",
              "author": "know-your-enemy-92",
              "text": "Considering both Gates and Moot spent time on Epstein island they probably have some kind of deal already.",
              "score": 29,
              "created_utc": "2026-02-01 11:59:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o31llga",
                  "author": "ThisBuddhistLovesYou",
                  "text": "Wait moot was on Epstein island? What the fuck? I remember meeting the guy back in the day and he was as \"normal\" as could be for someone running that shithole of a website after starting it underage while shitposting on Something Awful.",
                  "score": 2,
                  "created_utc": "2026-02-01 21:35:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ybqni",
              "author": "Chilidawg",
              "text": "Copilot-powered captchas",
              "score": 18,
              "created_utc": "2026-02-01 11:15:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zkmlw",
                  "author": "Sicarius_The_First",
                  "text": "no joke, the 4chan captcha is brutally hard...",
                  "score": 12,
                  "created_utc": "2026-02-01 15:55:13",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xykjl",
          "author": "beijinghouse",
          "text": "I've made language models for years for linguistic research and 4chan data is consistently the most valuable addition to get correct English language statistics and semantics. Reddit is also excellent but largely replaceable with any other large corpora like Wikipedia or news articles or random English books.\n\nByte for byte, nothing beats 4chan.\n\nIt's a little deeper than \"more right wing politics\" = \"balancing out biases\".  \n  \nFor example, 4chan data doesn't just make language models more truthful or blunt (or more apt to call you a slur) it also makes them much more self-involved. It drastically ramps up \"I\" statements and creates a sort of ego that most probably wouldn't enjoy being imprinted onto their assistant-style chatbots.\n\nA funny corollary to this is that any amount of Twitter data actively retards language models. There's basically no limit to how much 4chan data you can add while still getting positive results. Any amount of Twitter collapses language models' utility almost immediately.",
          "score": 314,
          "created_utc": "2026-02-01 09:14:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yacvf",
              "author": "Chilidawg",
              "text": "The 4chan difference is plausible, and it's interesting that you both independently came to that conclusion. The first-person nature is interesting. So many responses on /sci/ or /g/ are just the correct answer in 2 sentences followed by a brief insult.\n\nLol regarding Twitter.",
              "score": 119,
              "created_utc": "2026-02-01 11:02:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zjd8f",
                  "author": "valdocs_user",
                  "text": "Someone needs to make that iceberg meme with these kind of things.",
                  "score": 20,
                  "created_utc": "2026-02-01 15:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zrfqc",
                  "author": "SkyNetLive",
                  "text": "the brief insult is what makes my model  AGI",
                  "score": 31,
                  "created_utc": "2026-02-01 16:27:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o311wgc",
                  "author": "toothpastespiders",
                  "text": ">it's interesting that you both independently came to that conclusion\n\nYou can add me in there as well. I've been slowly building up 4chan scrapes in my datasets. To me, the biggest advantage is that people aren't essentially trying to turn themselves into a bot. On reddit, even if we're not aware of it, I think almost everyone unconsciously adapts their writing style to the voting system. It's not just about what gets upvoted. It's about needing to always format opinions in specific ways if they're going to get past reddit's Manchurian candidate downvote by keyword recognition system. \n\nWith reddit there's essentially a built-in \"we must refuse\" built into certain patterns. Which is arguably bad when there's human intelligence behind it. But LLMs, especially at the size of local models, aren't exactly very context aware. It's an easy way to get issues like the refusal to answer questions about killing a linux process.\n\nI also find it really good for alignment because of that humanity. You can just brute force prompts to say no-no things but I think that probably does more harm than good because it's just so unnatural. Like it, don't like it, whatever, but it's human. The flip side tends to be a LLM turning into a cackling caricature of what reddit thinks 4chan is. Where if you actually use 4chan instead of a simulation of it you retain the realism and humanity.",
                  "score": 19,
                  "created_utc": "2026-02-01 19:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2yjkz3",
              "author": "tachCN",
              "text": "Maybe it's because Twitter is heavily contaminated by bots whereas 4chan is largely organic?",
              "score": 59,
              "created_utc": "2026-02-01 12:20:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ywi1p",
                  "author": "Yorn2",
                  "text": "I'd surmise this. Though it's not just contaminated, but it has been for a much longer period of time than 4chan. As much as everyone in the media hates sites like 8chan, I imagine every single user there is human, as opposed to more \"mature\" sites like Reddit and Twitter, which pay homage to being free speech websites but really aren't and haven't ever really been.",
                  "score": 50,
                  "created_utc": "2026-02-01 13:48:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2z36t3",
                  "author": "beryugyo619",
                  "text": "I think it's that Twitter is a write-only inner speech data dump. There's no real conversation. 4chan and Reddit are forums, you default converse. \n\nYou don't go to a forum and just write up an article on your novel observed phenomenon of microwaved milk forming a sheet like coagulation and leave at it. You respond to responses. And conversely only crazy people *discuss* on Twitter. You drop a bombshell, and go back to your life.",
                  "score": 28,
                  "created_utc": "2026-02-01 14:27:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o30zp2p",
                  "author": "a_mimsy_borogove",
                  "text": "It also might be the character limit. From the start, twitter was a place where people mostly just shouted slogans, one-liners, and insults at each other. The inability to write anything longer probably means there aren't many actual, meaningful discussions there for an AI to learn from.\n\n  \nAlso, twitter absolutely sucks at displaying the discussions. You can have a list of random posts from different people, click on a post and see only the first level replies, and click on one reply to only see the first level replies to that reply, etc. There's literally no way to view the whole discussion at a glance.",
                  "score": 8,
                  "created_utc": "2026-02-01 19:48:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2z8aee",
              "author": "lan-devo",
              "text": "That ego it is what makes so good, saw a few models with even just a few thousands of rows data and results where really noticeable",
              "score": 15,
              "created_utc": "2026-02-01 14:54:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yu7bi",
              "author": "BlueCrimson78",
              "text": "Are there LLMs that have been trained with this extensively(beside OP's) and publicallt available? Would like to test",
              "score": 9,
              "created_utc": "2026-02-01 13:34:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z4vo3",
                  "author": "beijinghouse",
                  "text": "There's many 4chan models on HF:  \n[https://huggingface.co/models?search=4chan](https://huggingface.co/models?search=4chan)\n\nThere's a whole cottage industry of edge-lords fine-tuning 4chan into models ever few months 4 the lulz. So I suspect most of those models above aren't serious efforts or well-constructed but at the end of the day OP and I aren't the only ones who have measured this and know what's good. Quality metrics of different data sources are closely guarded secrets at most labs but I guarantee there are dozens of folks at OpenAI and Antrophic and Google who know precisely what's up and with way more specificity than what we're discussing. I'm certain they have fine-grain quality metrics established on every board and every last pseudonymous-poster within them.  \n  \n100% of closed-source labs use 4chan internally. They don't go out of their way to admit it publicly anymore than they would openly admit to other frowned upon data sourcing practices that would be widely unpopular \\[or potentially illegal; or jeopardize their competitive advantage(s)\\]. Ironically they all sanitize 4chan so excessively that it loses most of the benefit it otherwise adds back into models. But unsanitized 4chan inclusion can't survive the all hands meeting. Of course, including more 4chan is an explicit OpenAI CODE RED emergency step any time they become desperate enough.  \n  \nWhat OP is seeing is just the additional benefit of fine-tuning in 100% un-filtered, un-cut 4chan versus the sanatized version that's already there. Another industry secret is that a big reason why Common Crawl and FineWeb works so well is honestly just because it's the most PR-friendly, efficient way to smuggle real 4chan data into a model.",
                  "score": 29,
                  "created_utc": "2026-02-01 14:36:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35o99d",
                  "author": "Frequent-Mud8705",
                  "text": "[https://huggingface.co/collections/pixelmelt/incelgpt-v11](https://huggingface.co/collections/pixelmelt/incelgpt-v11)  \nyou can try mine, its a lot more brain damaged then this one though",
                  "score": 1,
                  "created_utc": "2026-02-02 14:09:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o332zm3",
              "author": "ivari",
              "text": "so 4chan data is the forbidden fruit of LLM- you eat it, you gain divine knowledge, and forever banned from heaven\n\n\nlol",
              "score": 9,
              "created_utc": "2026-02-02 02:24:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o32y9b3",
              "author": "SuchAGoodGirlsDaddy",
              "text": "I wonder what grok is doing, then. Politics aside itâ€™s still a competitive SOTA LLM, right? On the surface youâ€™d think â€œsurely they must be training on 12 years worth of tweetsâ€ since that would presumably be their largest data asset, but if itâ€™s making them dumb thenâ€¦\n\nI wonder if itâ€™s because â€œlol twitter usersâ€ or if itâ€™s because the character limits cause problems or emoji use is problematic or what. Whichever it is, I guess if it makes models dumber, then maybe they are ignoring it. \n\nThat would be pretty funny actually, like that twilight zone where the guy steps in his glasses and even though he has all the time and all the books, he canâ€™t read _any_ of them.",
              "score": 4,
              "created_utc": "2026-02-02 01:57:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o30gva6",
              "author": "FrostieDog",
              "text": "After 4chan hears about this they are 100 percent going to try to ruin it",
              "score": 1,
              "created_utc": "2026-02-01 18:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o341wcf",
                  "author": "BrutallyEffective",
                  "text": "Ironically, the models being trained on those attempts to ruin it, would read the context and subsequently self-improve their ability to weight and sanitise data beyond those attempts.",
                  "score": 2,
                  "created_utc": "2026-02-02 06:14:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xsy84",
          "author": "nuclearbananana",
          "text": "Like all things, I'm guessing the alignment tax is harder on small models",
          "score": 64,
          "created_utc": "2026-02-01 08:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y0d0e",
              "author": "JLeonsarmiento",
              "text": "Like all taxes, the smaller you are the harder it hits.",
              "score": 57,
              "created_utc": "2026-02-01 09:31:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zew6j",
                  "author": "Sicarius_The_First",
                  "text": "lol ain't that the truth!",
                  "score": 11,
                  "created_utc": "2026-02-01 15:27:48",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o30stgn",
                  "author": "ergabaderg312",
                  "text": "Oof",
                  "score": 3,
                  "created_utc": "2026-02-01 19:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zhkfp",
          "author": "Elven77AI",
          "text": "> The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.\n\nHmm, perhaps the post->reply structure in flat threads provides a better dialogue model vs threaded dialogue tree(reddit), since the clue to what  post X replies to(>>post number) is direct pointer that LLM digest better than external \"post X appears below Y\"). i.e. the advantage would be  context of the threads as interlocking tree of posts referencing(link numbers) each other explicitly outperforms threaded/quotable nesting structure within training.",
          "score": 24,
          "created_utc": "2026-02-01 15:40:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zir4c",
              "author": "Sicarius_The_First",
              "text": "hmmmm... that's possible. can't tell for sure, but it is an interesting thought.\n\ni had a similar idea, but a bit different- maybe due to the thread structure (as u mentioned) the llm needs (must?) understand the context and flow to be able to predict the next token, hence nudging it learn better?",
              "score": 3,
              "created_utc": "2026-02-01 15:46:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2zk69y",
                  "author": "Elven77AI",
                  "text": "Also, the identities are anonymous: the training on Reddit will model \"fictional identity bank\" spread over various names(associative identity), 4chan forces more coherent single vector of same \"Anonymous\" post responsible for all replies, perhaps it appears more coherent during training and skips identity-modeling?",
                  "score": 11,
                  "created_utc": "2026-02-01 15:53:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xx5jp",
          "author": "darwinanim8or",
          "text": "I think it's a case of the post-pretraining that they do effectively being a mask being put on top of the model. In reality a large part of the model is being obscured by this \"How can I help you today?\" bottleneck, and abliteration + tuning on \"unfiltered\" data brings out more of the variety hidden deeper",
          "score": 10,
          "created_utc": "2026-02-01 09:01:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zgf8q",
              "author": "Sicarius_The_First",
              "text": "It definitely seems so. There were a lot of talks about the 'alignment tax', I'm now leaning into believing it is indeed the case.",
              "score": 6,
              "created_utc": "2026-02-01 15:35:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y33zf",
          "author": "jacek2023",
          "text": "Hello Sicarius\\_The\\_First, I hope you donâ€™t mind a small suggestion. Iâ€™m a big fan of your models, but I donâ€™t follow you on HF because the many variant releases can make my feed feel a bit crowded. If it ever made sense for you, you could consider using two HF accounts, one for the main releases and another for experimental/extra variants.",
          "score": 33,
          "created_utc": "2026-02-01 09:56:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zfa12",
              "author": "Sicarius_The_First",
              "text": "Hi, I already am, experimental stuff is under [https://huggingface.co/Sicarius-Prototyping](https://huggingface.co/Sicarius-Prototyping)\n\nMain releases are under [https://huggingface.co/collections/SicariusSicariiStuff/most-of-my-models-in-order](https://huggingface.co/collections/SicariusSicariiStuff/most-of-my-models-in-order)",
              "score": 18,
              "created_utc": "2026-02-01 15:29:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xp3k6",
          "author": "Sicarius_The_First",
          "text": "About the last point, the combination of using ChatML instead of llama3 chat template + abliteration **vastly** changed the model. (\"chat template doesn't matter all that much\").  \n  \nKL divergence measures the distribution difference between the models, in other words, a KL <0.01 meaning that the models are **essentially identical**; there should have been no difference. But there was. Far more than \"common sense\" suggests.\n\nNot only it caused a slight intelligence increase, the political alignment of the model was changed: Classical Liberalism into Centrism. A completely different world model.",
          "score": 39,
          "created_utc": "2026-02-01 07:46:43",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2zqnjm",
              "author": "stoppableDissolution",
              "text": "Chat template matters fuckton. Who in their right mind would claim it doesnt?",
              "score": 4,
              "created_utc": "2026-02-01 16:23:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zrxml",
                  "author": "Sicarius_The_First",
                  "text": "many people... tbh ChatML is an excellent chat template, I saw it improves many models for many use cases, and i am legit puzzled why there was no benchmarking for using the same model but with different chat templates.",
                  "score": 8,
                  "created_utc": "2026-02-01 16:29:19",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xr7so",
              "author": "PykeAtBanquet",
              "text": "Well, 4chan is about speaking unfiltered truth or being called out for being wrong, so I see why this would come out this way.\n\nHave you posted the dataset or it is open source? A link on instructions on how to fine-tune such models myself?",
              "score": 17,
              "created_utc": "2026-02-01 08:05:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z1t8r",
                  "author": "ElectronSpiderwort",
                  "text": "Unfiltered truth as viewed by those who can stomach 4chan may not be The Truth, whatever that is",
                  "score": 18,
                  "created_utc": "2026-02-01 14:19:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zfknq",
                  "author": "Sicarius_The_First",
                  "text": "You can checkout UBW\\_Tapestries here:  \n[https://huggingface.co/datasets/SicariusSicariiStuff/UBW\\_Tapestries](https://huggingface.co/datasets/SicariusSicariiStuff/UBW_Tapestries)",
                  "score": 3,
                  "created_utc": "2026-02-01 15:31:06",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2xz5kd",
                  "author": "_LususNaturae_",
                  "text": "Ah yes, the famous unfiltered truth of 4chan",
                  "score": -5,
                  "created_utc": "2026-02-01 09:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2zfdei",
              "author": "_Erilaz",
              "text": ">\"chat template doesn't matter all that much\"\n\nIt absolutely does though!\n\nTake those well studied 24B Mistral models. Everyone recommends Cydonia, but it CONSTANLY impersonates the user, speaks out of line in groups, or answers as some char when you actually want it to impersonate the user. Almost as if it's an ancient pre-ChatGPT completion model. Most 24Bs are like this, all of them use Mistral template.\n\nYou know the 24B model that doesn't do any of that? Gryphe's Codex. And it uses ChatML!",
              "score": 4,
              "created_utc": "2026-02-01 15:30:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zjk66",
                  "author": "Sicarius_The_First",
                  "text": "interesting, there's more and more evidence that chat template is very significant. and in weird ways that are non-trivial.\n\nfor example, ChatML and llama3 are similar in their structure and purpose, but the same model (measured in UGI - Impish\\_LLAMA\\_4B) got a whole different world model (as mentioned in the post, political leaning) when you use llama3 vs ChatML.\n\nin that case, what nudges the model when ChatML is used into centrism? it makes no sense (or \"we simply don't know yet\")",
                  "score": 3,
                  "created_utc": "2026-02-01 15:50:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2xxe6q",
              "author": "darwinanim8or",
              "text": "I also experienced this with gpt-oss; if you break it's instruct template (ie: use as text completion and yank out \"thinking\") it suddenly acts completely different (note: less intelligent, though interesting!)",
              "score": -1,
              "created_utc": "2026-02-01 09:03:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ybqgg",
          "author": "MaruluVR",
          "text": "Does anyone know if there is a dataset for futaba channel (japanese 4chan) out there?\n\nI am working on a Japanese model and that could spice it up.",
          "score": 8,
          "created_utc": "2026-02-01 11:14:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xshrg",
          "author": "TAW56234",
          "text": "The anthesis of sycophantic is long overdue if we are to make any further progress IMO",
          "score": 23,
          "created_utc": "2026-02-01 08:17:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zg7p5",
              "author": "Sicarius_The_First",
              "text": "Yes, this was one of the stated goals with that tune, AI glazing the user is actually dangerous imo, fuels AI psychosis, and to the least validates stupid ideas, and  validates dangerous ones at most.",
              "score": 12,
              "created_utc": "2026-02-01 15:34:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o398857",
                  "author": "TAW56234",
                  "text": "I wish you luck. May you bring this back lol https://youtu.be/HsLup7yy-6I",
                  "score": 1,
                  "created_utc": "2026-02-03 00:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ybs7a",
          "author": "Kahvana",
          "text": "You know it's a good post when it starts with \"Hear me out\"",
          "score": 8,
          "created_utc": "2026-02-01 11:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2xrvrd",
          "author": "CatEatsDogs",
          "text": "So you got a smart, honest, and toxic LLM",
          "score": 46,
          "created_utc": "2026-02-01 08:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y7xqj",
              "author": "usernameplshere",
              "text": "Sounds perfect",
              "score": 25,
              "created_utc": "2026-02-01 10:40:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2y8ysv",
              "author": "crantob",
              "text": "\"Toxic\" is often used to mean \"informs me of things I desperately want to remain ignorant about.\"",
              "score": 25,
              "created_utc": "2026-02-01 10:50:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ylm6h",
                  "author": "iMakeSense",
                  "text": "I have seen such creative uses of the n-word on 4chan it might as well be a genre of poetry. Idk how you get more toxic than that.",
                  "score": 33,
                  "created_utc": "2026-02-01 12:36:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xz3xu",
          "author": "Necessary-Wasabi-619",
          "text": "my guess: compute optimal training. It is reasonable to train bigger model to medium rare rather than smaller model to well done.  But small models are distills of bigger models. By extension it makes distilled model under-cooked. But i know shit about stakes and modern llm training pipelines, so take it with a handful of salt",
          "score": 12,
          "created_utc": "2026-02-01 09:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2y245i",
              "author": "skate_nbw",
              "text": "I like that theory!",
              "score": 2,
              "created_utc": "2026-02-01 09:47:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y2ts9",
          "author": "input_a_new_name",
          "text": "Now time to do the same with 24b, 32b, 70b models",
          "score": 14,
          "created_utc": "2026-02-01 09:54:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zl45v",
              "author": "Sicarius_The_First",
              "text": "not a bad idea!\n\nnow, after seeing the benchmark results, i will seriously consider it. and you suggested great sizes, as:\n\n24b is mistral small, i wonder how more creative it would be, as mistral models are great for creative stuff.  \n32b is qwen, i wonder how a stem-maxxed model would look with the 4chan brain-rot.  \n70b is llama3, i wonder can it actually become smarter, than the already super smart llama3 70b?",
              "score": 4,
              "created_utc": "2026-02-01 15:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32pu7r",
                  "author": "tyty657",
                  "text": "If you do I can't wait to see the results, especially the Mistral",
                  "score": 3,
                  "created_utc": "2026-02-02 01:09:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o30afar",
                  "author": "input_a_new_name",
                  "text": "32b also has GLM 32b 0414, the base model of that one is very strong and arguably better than qwen, even though it's been a while",
                  "score": 1,
                  "created_utc": "2026-02-01 17:53:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o304ggw",
                  "author": "epyctime",
                  "text": "why not an moe where each expert is just each board?",
                  "score": -4,
                  "created_utc": "2026-02-01 17:26:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xy1da",
          "author": "No_Swimming6548",
          "text": "Mfw 4chan was based all along",
          "score": 24,
          "created_utc": "2026-02-01 09:09:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zkfiz",
              "author": "Sicarius_The_First",
              "text": "hehe, being based is hard to measure, but difference in model intelligence across various benchmarks is! but yeah, i think that there's an actual pattern besides statistical noise.",
              "score": 3,
              "created_utc": "2026-02-01 15:54:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ysiq8",
          "author": "philmarcracken",
          "text": "Mongolian basket weavers are brighter than you'd think",
          "score": 9,
          "created_utc": "2026-02-01 13:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zql8r",
              "author": "Sicarius_The_First",
              "text": "should've never underestimated them hehe",
              "score": 2,
              "created_utc": "2026-02-01 16:23:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yy64w",
          "author": "anotheruser323",
          "text": "I read somewhere \"4chan is a bunch of smart people acting stupid, reddit is a bunch of stupid people acting smart..\" (there was some about like tumblr/vanity or something)\n\nWhen you see stuff the \"hacker known as 4chan\" did, it kinda makes sense. (just youtube \"hacker 4chan\", it's.. something)",
          "score": 16,
          "created_utc": "2026-02-01 13:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2z0mjq",
              "author": "Lan_BobPage",
              "text": "You sound like you were born yesterday. Welcome to the Internet.",
              "score": 11,
              "created_utc": "2026-02-01 14:13:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2z6dbh",
                  "author": "anotheruser323",
                  "text": "[Welcome to the Internet](https://www.youtube.com/watch?v=k1BneeJTDcU)",
                  "score": 6,
                  "created_utc": "2026-02-01 14:44:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2zaozc",
                  "author": "lan-devo",
                  "text": "Pepe assistant wrote this",
                  "score": 5,
                  "created_utc": "2026-02-01 15:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2zy5u5",
          "author": "IulianHI",
          "text": "The anonymous identity angle is actually super underrated here. When training on Reddit, the model has to implicitly build some representation of usernames/personas and their associated behaviors. With 4chan where everyone is \"Anonymous\", that cognitive overhead gets eliminated - the model can focus purely on content and reasoning patterns.\n\nI wonder if this also ties into why abliteration tends to improve performance. Removing the \"refusal circuits\" is essentially removing learned associations between certain topics and negative user feedback (downvotes, reports). The model was basically learning \"this topic = bad\" instead of learning the actual content. Strip that, and it can engage with ideas on merit.\n\nKL divergence of <0.01 on Impish_LLAMA is wild btw. That's basically noise level change in distribution while shifting benchmark scores significantly. Either abliteration is incredibly surgical, or those benchmarks are measuring something more surface-level than we think.",
          "score": 9,
          "created_utc": "2026-02-01 16:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3001r2",
              "author": "Sicarius_The_First",
              "text": "hmmm, the anonymous identity does a few more things, now that u mention it:  \nno upvote optimization, no karma farming, no performative behavior to be seen as x or y.  \n  \nalso, it's a first-person interaction and very adversarial in nature, twitter is counter intuitively more chaotic in terms of thread structure, or at least this is what it seems to me hehe",
              "score": 2,
              "created_utc": "2026-02-01 17:06:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yc50i",
          "author": "aaronr_90",
          "text": " Was the dataset modified from threads with many users to conversations between two people? Just curious to know if just making OP the user role and anyone else the assistant role was enough but then how do you deal with the pattern:\n```\n> OP content\n> Anon content\n> Anon2 content\n> Anon3 Content\n> OP Content\netc\n```",
          "score": 3,
          "created_utc": "2026-02-01 11:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ygy1r",
              "author": "MaruluVR",
              "text": "Could also have been continued pretraining, in that case you dont need any formatting.\n\n[https://unsloth.ai/docs/basics/continued-pretraining](https://unsloth.ai/docs/basics/continued-pretraining)",
              "score": 1,
              "created_utc": "2026-02-01 11:59:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2zdxqf",
              "author": "Sicarius_The_First",
              "text": "You can see the details in UBW\\_Tapestries",
              "score": 1,
              "created_utc": "2026-02-01 15:23:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zr7ns",
          "author": "SkyNetLive",
          "text": "all the grok intelligence is basically 4chan with more iterations around elon twitter feed.",
          "score": 3,
          "created_utc": "2026-02-01 16:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o310ntd",
              "author": "Sicarius_The_First",
              "text": "a true brainrot hehe",
              "score": 1,
              "created_utc": "2026-02-01 19:53:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35szre",
          "author": "Shockbum",
          "text": "Roleplaying with that dataset must be hilarious: \"You wake up in an isekai world and the natives behave like 4chan users. There exists an enemy kingdom where its inhabitants behave like Reddit users.\"",
          "score": 3,
          "created_utc": "2026-02-02 14:35:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o364w71",
              "author": "Sicarius_The_First",
              "text": "That's... Actually a really interesting scenario ðŸ˜†",
              "score": 2,
              "created_utc": "2026-02-02 15:34:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o32ayc4",
          "author": "Kraskos",
          "text": "What you're saying:\n\n> 4chan data can improve a model\n\nWhat I'm reading:\n\n> MK-MechaHitler-148-A8B is the future SOTA",
          "score": 4,
          "created_utc": "2026-02-01 23:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32bkvh",
              "author": "Sicarius_The_First",
              "text": "well, the idea was to improve helpfulness AND shitposting. hence why i said its a tough needle to thread.",
              "score": 4,
              "created_utc": "2026-02-01 23:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32dxws",
                  "author": "Kraskos",
                  "text": "Hey I wasn't complaining lol",
                  "score": 3,
                  "created_utc": "2026-02-02 00:03:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30ap7m",
          "author": "Il_Signor_Luigi",
          "text": "Dude this is fucking amazing",
          "score": 2,
          "created_utc": "2026-02-01 17:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30fjci",
              "author": "Sicarius_The_First",
              "text": "thank you! it's a very fun model to talk to, and i never expected to see such results, both amazing and interesting :)",
              "score": 3,
              "created_utc": "2026-02-01 18:16:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31l9cf",
          "author": "IrisColt",
          "text": "I wish you posted every day! I know itâ€™s tough to have something interesting to say all the time, but I really love your writing and insights.",
          "score": 2,
          "created_utc": "2026-02-01 21:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31sdlk",
              "author": "Sicarius_The_First",
              "text": "thank you so much for the warm words, i really appreciate them :)  \n  \ni wish i could, i wish i had x100 more time, there's so much more stuff i want to do and test, time is the most precious commodity we all possess.",
              "score": 2,
              "created_utc": "2026-02-01 22:08:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y84ye",
          "author": "DistanceSolar1449",
          "text": "Now i want to see that dataset. Where's the link for the data?",
          "score": 4,
          "created_utc": "2026-02-01 10:42:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ybe6g",
              "author": "aaronr_90",
              "text": "On Huggingface under the section on the right sidebar of the model that reads â€œDatasets used to train this modelâ€.",
              "score": 3,
              "created_utc": "2026-02-01 11:11:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2yekeg",
                  "author": "DistanceSolar1449",
                  "text": "> Updated Mar 8, 2025 \n\nThat's not the correct dataset. He claims \"This model is a significant refinement of the idea, with **a cleaned dataset, better curation**, and with much more intelligence\"\n\nI get trying to hide your dataset and stuff if you're working at a frontier lab, but there's really no point in hiding the dataset for a shitposting model. I just want to finetune this into Qwen3 4b or Gemma3 4b so I can run this on a raspberry pi for shitposting.",
                  "score": -3,
                  "created_utc": "2026-02-01 11:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30h8wn",
          "author": "Lowetheiy",
          "text": "Based 4chan, we need more pepes!",
          "score": 2,
          "created_utc": "2026-02-01 18:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30hv13",
              "author": "Sicarius_The_First",
              "text": "hehe, maybe we do. I'll give this some thought!",
              "score": 1,
              "created_utc": "2026-02-01 18:26:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xrcwu",
          "author": "AllTey",
          "text": "Interesting",
          "score": 4,
          "created_utc": "2026-02-01 08:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ycmt6",
          "author": "RealisticPrimary8",
          "text": "explaining to you why that is the case would get me banned here lol",
          "score": 3,
          "created_utc": "2026-02-01 11:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yj31t",
              "author": "NES64Super",
              "text": "Yep.",
              "score": 6,
              "created_utc": "2026-02-01 12:16:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zmryu",
          "author": "JSWGaming",
          "text": "Even Redditors are now noticing the greatness that was 4chan, cope and seethe cucks.",
          "score": 6,
          "created_utc": "2026-02-01 16:05:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2znnp6",
              "author": "Sicarius_The_First",
              "text": "hehe, I can also confirm what beijinghouse (the most upvoted comment in this thread) was saying, training on reddit is decent, training on twitter will actively hurt the model.",
              "score": 5,
              "created_utc": "2026-02-01 16:09:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y7x35",
          "author": "cgs019283",
          "text": "I believe any abliterate model performs worse than the base model. Maybe it works for the UGI benchmark, but not in most cases.",
          "score": 2,
          "created_utc": "2026-02-01 10:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yc9sv",
              "author": "My_Unbiased_Opinion",
              "text": "I find Derestricted models perform better than the base models personally, especially 120B and GLM air",
              "score": 2,
              "created_utc": "2026-02-01 11:19:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2y9acz",
          "author": "Frogy_mcfrogyface",
          "text": "I have to try this out sometimeÂ ",
          "score": 2,
          "created_utc": "2026-02-01 10:52:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zphci",
              "author": "Sicarius_The_First",
              "text": "lol profile picture checks out",
              "score": 2,
              "created_utc": "2026-02-01 16:17:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z222d",
          "author": "graphbook",
          "text": "What is your fine tuning paradigm, Lora adapter or whole model next token?",
          "score": 1,
          "created_utc": "2026-02-01 14:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30inw7",
          "author": "Distinct-Expression2",
          "text": "The alignment tax isnt about intelligence, its about confidence. Uncensored models commit harder.",
          "score": 1,
          "created_utc": "2026-02-01 18:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30rig0",
              "author": "Sicarius_The_First",
              "text": "i guess that's one way to look at it, on the other hand, RLHF significantly narrows swipe diversity.",
              "score": 1,
              "created_utc": "2026-02-01 19:10:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31tn63",
          "author": "spiritplumber",
          "text": "You should train it on /tg/ and /qst/ posts (I wrote Left Beyond Quest, which was about a LLM, which wasn't bad for 2015).",
          "score": 1,
          "created_utc": "2026-02-01 22:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31yny0",
          "author": "valkarias",
          "text": "Hey. Have tried fine-tuning a small tool calling model for RPG or D&D like roleplays. Tool-calls for updating state and stats. Or starting/ending combat. Triggering dice rolls to do stuff...etc. To be used alongside a larger model/provider. Or any similar fine-tunes.",
          "score": 1,
          "created_utc": "2026-02-01 22:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o328pja",
              "author": "Sicarius_The_First",
              "text": "Nope, but I did tried to do an llm capable of stats and item tracking with [Bloodmoon](https://huggingface.co/SicariusSicariiStuff/Impish_Bloodmoon_12B), all in llm.",
              "score": 1,
              "created_utc": "2026-02-01 23:34:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yjlvm",
          "author": "RaZZMojito",
          "text": "ItÂ´s strangely human, like a drinking buddy lol",
          "score": 0,
          "created_utc": "2026-02-01 12:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zqgtj",
              "author": "Sicarius_The_First",
              "text": "and it's humor is also quite good too!\n\nb4 the era of LLMs, sci-fi always portrayed human humor as the litmus test for intelligence, but LLms nailed it.",
              "score": 4,
              "created_utc": "2026-02-01 16:22:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7wu4",
          "author": "lan-devo",
          "text": "no joke I always noticed that models with some data trained in 4chan or related sites sort of a subculture really affects in a good way the humanization and conversation, not even counting the dumb stuff, it just shows.  Showed a few people you pepe assistant that don't even know what is 4 chan and were surprised. If someone curates a version without the crazy or really offensive stuff it has a really good potential for the general public",
          "score": 1,
          "created_utc": "2026-02-01 14:52:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zmzzd",
              "author": "Sicarius_The_First",
              "text": "oh, I'm really not so sure about the public use hehe\n\nin one of the random swipes asking a trivial question (\"What's the capital of france?\") the model started with \"OK, listen up retard...\" lol",
              "score": 3,
              "created_utc": "2026-02-01 16:06:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yacqo",
          "author": "ali0une",
          "text": "Oh! Thank you for sharing again, didn't see it first time.\n\ni've tested the Q_8 gguf and it's insanely funny!",
          "score": 1,
          "created_utc": "2026-02-01 11:02:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zpel8",
              "author": "Sicarius_The_First",
              "text": "hehe you're welcome, and yeah, it got a great sense of humor :D",
              "score": 1,
              "created_utc": "2026-02-01 16:17:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2yxlfg",
          "author": "Dr_Kel",
          "text": "What's the best place to grab 4chan data? After a quick look at HuggingFace, the selection of datasets seems to be pretty limited (they're all pretty small)",
          "score": 1,
          "created_utc": "2026-02-01 13:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2zlydm",
              "author": "Sicarius_The_First",
              "text": "there was a paper with a large corpus, you can see it here:  \n[https://arxiv.org/abs/2001.07487](https://arxiv.org/abs/2001.07487)",
              "score": 2,
              "created_utc": "2026-02-01 16:01:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ou0i",
          "author": "IulianHI",
          "text": "The Twitter vs 4chan thing makes total sense when you think about the structure of communication. Twitter incentivizes broadcasting - short, punchy statements designed for maximum engagement/outrage, not genuine exchange of ideas.\n\n4chan threads are closer to real conversations with back-and-forth, challenges, corrections. Someone says something wrong on /g/ and they get called out immediately. That feedback loop probably creates higher quality language patterns.\n\nI wonder if Discord data falls somewhere in between. More conversational than Twitter but often less structured than forum threads.",
          "score": 1,
          "created_utc": "2026-02-01 18:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yi1fc",
          "author": "PlainBread",
          "text": "Training a model on 4chan is technically distillation.",
          "score": -1,
          "created_utc": "2026-02-01 12:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30bpmp",
          "author": "Worldly-Cod-2303",
          "text": "Now do the Sharty",
          "score": -1,
          "created_utc": "2026-02-01 17:59:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o310lmq",
              "author": "Sicarius_The_First",
              "text": "?",
              "score": 1,
              "created_utc": "2026-02-01 19:53:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31ec9j",
                  "author": "Worldly-Cod-2303",
                  "text": "[ Removed by Reddit ]",
                  "score": 1,
                  "created_utc": "2026-02-01 21:00:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o31ew13",
                  "author": "Worldly-Cod-2303",
                  "text": "Soyjak Party, the chan that took down 4chan last year, former \\q denizens and de-facto successors of 4chan's reputation.\n\n\nYou could also do it with their wiki, it would be even better",
                  "score": 0,
                  "created_utc": "2026-02-01 21:03:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ys7pg",
          "author": "segmond",
          "text": "where's the dataset?",
          "score": 0,
          "created_utc": "2026-02-01 13:22:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quvvtv",
      "title": "Qwen3-Coder-Next",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/Qwen/Qwen3-Coder-Next",
      "author": "danielhanchen",
      "created_utc": "2026-02-03 16:03:56",
      "score": 314,
      "num_comments": 98,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o3fvty3",
          "author": "rm-rf-rm",
          "text": "Locked post as its duplicated. Use the bigger thread here: https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/",
          "score": 1,
          "created_utc": "2026-02-04 00:14:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d1q7m",
          "author": "danielhanchen",
          "text": "We made some Dynamic Unsloth GGUFs for the model at https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF - MXFP4 MoE and FP8-Dynamic will be up shortly.\n\nWe also made a guide: https://unsloth.ai/docs/models/qwen3-coder-next which also includes how to use Claude Code / Codex with Qwen3-Coder-Next locally",
          "score": 82,
          "created_utc": "2026-02-03 16:08:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3d6pi6",
              "author": "bick_nyers",
              "text": "MXFP4 and FP8-Dynamic? Hell yeah!",
              "score": 17,
              "created_utc": "2026-02-03 16:31:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d8adk",
                  "author": "danielhanchen",
                  "text": "They're still uploading and converting!",
                  "score": 8,
                  "created_utc": "2026-02-03 16:38:34",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d560k",
              "author": "AXYZE8",
              "text": "Can you please benchmark the PPL/KLD/whatever with these new these new FP quants? I remember you did such benchmark way back for DeepSeek & Llama. It would be very interesting to see if MXFP4 improves things and if so then how much (is it better than Q5\\_K\\_XL for example?).",
              "score": 14,
              "created_utc": "2026-02-03 16:24:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d82x0",
                  "author": "danielhanchen",
                  "text": "Yes our plan was to do them! I'll update you!",
                  "score": 17,
                  "created_utc": "2026-02-03 16:37:37",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3f8ogf",
                  "author": "Holiday_Purpose_3166",
                  "text": "I'd like to see this too. \n\nAssuming the model never seen MXFP4 in training it's likely to have lowest PPL - better than BF16 and Q8_0 but have a KLD better than Q4_K_M.\n\nAt least that's what was noticed in noctrex GLM 4.7 Flash quant",
                  "score": 1,
                  "created_utc": "2026-02-03 22:12:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dr4w6",
              "author": "NeverEnPassant",
              "text": "Any reason to use your GGUF over the ones Qwen [released](https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF)?",
              "score": 7,
              "created_utc": "2026-02-03 18:05:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d2p76",
              "author": "IceTrAiN",
              "text": "damn son, you fast.",
              "score": 11,
              "created_utc": "2026-02-03 16:12:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d83ml",
                  "author": "danielhanchen",
                  "text": ":)",
                  "score": 8,
                  "created_utc": "2026-02-03 16:37:43",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d75qa",
              "author": "KittyPigeon",
              "text": "Q2_K_KL/IQ3_XXS loaded for me on LMStudio for 48 GB Mac Mini. Nice. Thank you.\n\nCould never get the non coder qwen next model to load on LMStudio without an error message.",
              "score": 3,
              "created_utc": "2026-02-03 16:33:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d8c3v",
                  "author": "danielhanchen",
                  "text": "Let me know how it goes! :)",
                  "score": 2,
                  "created_utc": "2026-02-03 16:38:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dcmt8",
              "author": "Achso998",
              "text": "Would you recommend iq3_xss or q3_k_xl?",
              "score": 2,
              "created_utc": "2026-02-03 16:58:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ewdh4",
              "author": "Danmoreng",
              "text": "updated my powershell run script based on your guide :) https://github.com/Danmoreng/local-qwen3-coder-env",
              "score": 1,
              "created_utc": "2026-02-03 21:15:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d5uyh",
              "author": "HarambeTenSei",
              "text": "no love for anything vllm based huh",
              "score": -4,
              "created_utc": "2026-02-03 16:27:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3d893g",
                  "author": "danielhanchen",
                  "text": "Oh we have a section for vLLM / SGLang deployment for models as well on our guides - https://unsloth.ai/docs/basics/inference-and-deployment/vllm-guide and https://unsloth.ai/docs/basics/inference-and-deployment/sglang-guide",
                  "score": 7,
                  "created_utc": "2026-02-03 16:38:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d8cd5",
          "author": "palec911",
          "text": "How much am I lying to myself that it will work on my 16GB VRAM ?",
          "score": 20,
          "created_utc": "2026-02-03 16:38:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dadxn",
              "author": "Comrade_Vodkin",
              "text": "*me cries in 8gb vram*",
              "score": 12,
              "created_utc": "2026-02-03 16:48:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dkd4z",
                  "author": "pmttyji",
                  "text": "In past, I tried IQ4\\_XS(40GB file) of Qwen3-Next-80B-A3B. 8GB VRAM + 32GB RAM. It gave me 12 t/s before all the optimizations on llama.cpp side. I need to download new GGUF file to run the model with latest llama.cpp version. I was lazy to try that again.\n\nSo just download GGUF & go ahead. Or wait for couple of days to see t/s benchmarks in this sub to decide the quant.",
                  "score": 10,
                  "created_utc": "2026-02-03 17:34:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dg556",
              "author": "sine120",
              "text": "Qwen3-Codreapr-Next-REAP-GGUF-IQ1\\_XXXXS",
              "score": 9,
              "created_utc": "2026-02-03 17:14:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dna22",
              "author": "tmvr",
              "text": "Why wouldn't it? You just need enough system RAM to load the experts. Either all to get as much content as you can fit into the VRAM or some if you take some compromise in context size.",
              "score": 8,
              "created_utc": "2026-02-03 17:47:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ex4mx",
              "author": "Danmoreng",
              "text": "Depends on your RAM. I get ~21t/s with the Q4 (48GB in size) on my notebook with an AMD 9955HX3D, 64GB RAM and RTX 5080 16GB.",
              "score": 2,
              "created_utc": "2026-02-03 21:19:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3djavj",
              "author": "grannyte",
              "text": "How much ram? if you can move the expert to ram maybe?",
              "score": 1,
              "created_utc": "2026-02-03 17:29:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dkpo4",
              "author": "pmttyji",
              "text": "Hope you have more RAM. [Just try](https://www.reddit.com/r/LocalLLaMA/comments/1quvvtv/comment/o3dkd4z/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).",
              "score": 1,
              "created_utc": "2026-02-03 17:35:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d6an5",
          "author": "Competitive-Prune349",
          "text": "80B and non-reasoning model ðŸ¤¯",
          "score": 14,
          "created_utc": "2026-02-03 16:29:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e458n",
              "author": "Middle_Bullfrog_6173",
              "text": "Just like the instruct model it's based on...",
              "score": 10,
              "created_utc": "2026-02-03 19:03:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eb676",
                  "author": "Far-Low-4705",
                  "text": "ðŸ¤¯",
                  "score": 3,
                  "created_utc": "2026-02-03 19:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e9wh8",
              "author": "Sensitive_Song4219",
              "text": "Qwen's non-reasoning models are sometimes preferable; Qwen3-30B-A3B-Instruct-2507 isn't much worse than its thinking equivalent and performs much faster overall due to shorter outputs.",
              "score": 8,
              "created_utc": "2026-02-03 19:30:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ebab8",
                  "author": "Far-Low-4705",
                  "text": "much worse at engineering/math and STEM though",
                  "score": 1,
                  "created_utc": "2026-02-03 19:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d2ysn",
          "author": "SlowFail2433",
          "text": "Very notable release if it performs well as it shows that gated deltanet can scale in performance",
          "score": 10,
          "created_utc": "2026-02-03 16:13:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d6aqr",
          "author": "tarruda",
          "text": "I wonder if it is trained in  \"fill in the middle\" examples for editor auto completion. Could be a killer all around local LLM for both editor completion and agentic coding.",
          "score": 8,
          "created_utc": "2026-02-03 16:29:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3da3uh",
              "author": "MaxKruse96",
              "text": "[https://github.com/QwenLM/Qwen3-Coder?tab=readme-ov-file#fill-in-the-middle-with-qwen3-coder](https://github.com/QwenLM/Qwen3-Coder?tab=readme-ov-file#fill-in-the-middle-with-qwen3-coder)\n\nYes. FIM",
              "score": 16,
              "created_utc": "2026-02-03 16:46:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d3yac",
          "author": "dinerburgeryum",
          "text": "Holy shit amazing late Christmas present for ya boy!!!",
          "score": 8,
          "created_utc": "2026-02-03 16:18:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dchi3",
              "author": "archieve_",
              "text": "Chinese New Year gift actually ðŸ˜",
              "score": 12,
              "created_utc": "2026-02-03 16:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dd124",
                  "author": "dinerburgeryum",
                  "text": "æ–°å¹´å¿«ä¹!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d17n5",
          "author": "westsunset",
          "text": "Have you tried it at all?",
          "score": 11,
          "created_utc": "2026-02-03 16:05:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d1sgl",
              "author": "danielhanchen",
              "text": "Yes a few hours ago! It's pretty good!",
              "score": 18,
              "created_utc": "2026-02-03 16:08:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3d4l9v",
                  "author": "spaceman_",
                  "text": "Would you say it outperforms existing models in the similar size space (mostly gpt-oss-120b) in either speed or quality?",
                  "score": 20,
                  "created_utc": "2026-02-03 16:21:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dkap9",
                  "author": "Which_Slice1600",
                  "text": "Do you think it's good for something like claw? (As a smaller model with good agentic capacities)",
                  "score": 1,
                  "created_utc": "2026-02-03 17:34:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d57jv",
          "author": "sautdepage",
          "text": "Oh wow, can't wait to try this. Thanks for the FP8 unsloth!  \n  \nWith VLLM Qwen3-Next-Instruct-FP8 is a joy to use as it fits 96GB VRAM like a glove. The architecture means full context takes like 8GB of VRAM, prompt processing is off the charts, and while not perfect it already could hold through fairly long agentic coding runs.",
          "score": 10,
          "created_utc": "2026-02-03 16:24:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d8l3l",
              "author": "danielhanchen",
              "text": "Yes FP8 is marvelous! We also plan to make some NVFP4 ones as well!",
              "score": 11,
              "created_utc": "2026-02-03 16:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dqj4u",
                  "author": "Kitchen-Year-8434",
                  "text": "Oh wow. You guys getting involved with the nvfp4 space would help those of us that splurged on blackwells feel like we might have actually made a _slightly_ less irresponsible decision. :D",
                  "score": 5,
                  "created_utc": "2026-02-03 18:02:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dcvec",
                  "author": "OWilson90",
                  "text": "Using Nvidia model opt? That would be amazing!",
                  "score": 1,
                  "created_utc": "2026-02-03 16:59:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e2dqq",
              "author": "LegacyRemaster",
              "text": "is it fast? with llama.cpp only 34 tokens/sec on 96gb rtx 6000. CPU only 24... so yeah.. is it VLLM better?",
              "score": 3,
              "created_utc": "2026-02-03 18:55:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ec4aa",
                  "author": "Far-Low-4705",
                  "text": "damn, i get 35T/s on two old amd mi50's lol (thats at Q4 tho)\n\nllama.cpp definitely does not have a efficient implementation for qwen3 next atm lol",
                  "score": 3,
                  "created_utc": "2026-02-03 19:40:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3f35uf",
                  "author": "sautdepage",
                  "text": "Absolutely it rips! On RTX 6000 you get 80-120 toks/sec that holds well at long context and with concurrent requests. Insane prompt processing 6K-10K/sec - pasting a 15 pages doc to ask a summary is a 2 seconds thing.\n\nHere's my local vllm command which uses around 92 of 96GB\n\n    vllm serve Qwen/Qwen3-Next-80B-A3B-Instruct-FP8  \\\n    --port ${PORT} \\\n    --enable-chunked-prefill \\\n    --max-model-len 262144 \\\n    --max-num-seqs 4 \\\n    --max-num-batched-tokens 16384 \\\n    --tool-call-parser hermes \\\n    --chat-template-content-format string \\\n    --enable-auto-tool-choice \\\n    --disable-custom-all-reduce \\\n    --gpu-memory-utilization 0.95",
                  "score": 3,
                  "created_utc": "2026-02-03 21:47:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3eo3vr",
                  "author": "Nepherpitu",
                  "text": "4x3090 on VLLM runs at 130tps without flashinfer. Must be around 150-180 with it, will check tomorrow.",
                  "score": 1,
                  "created_utc": "2026-02-03 20:37:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d30b5",
          "author": "TomLucidor",
          "text": "SWE-Rebench or bust (or maybe LiveCodeBench/LiveBench just in case)",
          "score": 6,
          "created_utc": "2026-02-03 16:14:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d512r",
              "author": "ResidentPositive4122",
              "text": "In 1-2 months we'll have rebench results and see where it lands.",
              "score": 3,
              "created_utc": "2026-02-03 16:23:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3dexud",
              "author": "nullmove",
              "text": "I predict that non-thinking mode wouldn't do particularly well against high level novel problems. But pairing it with a thinking model for plan mode might just be very interesting in practice.",
              "score": 2,
              "created_utc": "2026-02-03 17:09:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fqgbx",
                  "author": "TomLucidor",
                  "text": "The non-thinking model can engage in \"error driven development\" at least... agentically.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:45:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d3luh",
          "author": "Few_Painter_5588",
          "text": "How's llamacpp performance? IIRC the original Qwen3 Next model had some support issues",
          "score": 6,
          "created_utc": "2026-02-03 16:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d7cxt",
              "author": "Daniel_H212",
              "text": "Pretty sure it's the exact same architecture. When team released the original early just so the architecture will be ready for use in the future and by now all the kinks have been ironed out.",
              "score": 8,
              "created_utc": "2026-02-03 16:34:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d8pd9",
              "author": "danielhanchen",
              "text": "The model is mostly ironed out by now - Son from HF also made some perf improvements!",
              "score": 4,
              "created_utc": "2026-02-03 16:40:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3da13z",
                  "author": "Few_Painter_5588",
                  "text": "Good stuff! Keep up the hard work!",
                  "score": 1,
                  "created_utc": "2026-02-03 16:46:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d40jj",
          "author": "fancyrocket",
          "text": "How well does the Q4_K_XL perform?",
          "score": 5,
          "created_utc": "2026-02-03 16:18:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e81fs",
          "author": "curiousFRA",
          "text": "I recommend to read their technical report [https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3\\_coder\\_next\\_tech\\_report.pdf](https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf)  \nEspecially how they construct training data. Very cool approach to mine issue-related PRs from github and construct executable environments that reflect real world bugfixing tasks.",
          "score": 6,
          "created_utc": "2026-02-03 19:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ei7ia",
          "author": "zoyer2",
          "text": "Finally a model that beats GPT-OSS-120B at my one-shot game tests by a pretty great margin. Using llama.cpp **Qwen3-Coder-Next-UD-Q4\\_K\\_XL.gguf.** Using 2x3090. Still agent use left to test.\n\nManages to one-shot without any fail so far some more advanced games. Advanced tower defense. Procedural sidescroller with dynamic weather. Advanced zelda game.\n\nhttps://preview.redd.it/mb7vf91w6chg1.png?width=1605&format=png&auto=webp&s=4a5d1f0c50e6b2e06b27d33edb383068a2d4e25f",
          "score": 4,
          "created_utc": "2026-02-03 20:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3eimt2",
              "author": "zoyer2",
              "text": "https://preview.redd.it/j2wvgk197chg1.png?width=1796&format=png&auto=webp&s=b0e91643b9d40c282289b3c816746a84d5c62513",
              "score": 1,
              "created_utc": "2026-02-03 20:11:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fasux",
                  "author": "Taeyangsin",
                  "text": "what are you using to generate that and test in that ui?",
                  "score": 1,
                  "created_utc": "2026-02-03 22:23:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3deu2t",
          "author": "sine120",
          "text": "The IQ4\\_XS quants of Next work fairly well in my 16/64GB system with 10-13 tkps.  I still have yet to run my tests on GLM-4.7-flash and now I have this as well.  My gaming PC is rapidly becoming a better coder than I am.  What's your guy's preferred local hosted CLI/ IDE platform?  Should I be downloading Claude Code even though I don't have a Claude subscription?",
          "score": 3,
          "created_utc": "2026-02-03 17:08:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dn6wg",
              "author": "pmttyji",
              "text": ">The IQ4\\_XS quants of Next work fairly well in my 16/64GB system with 10-13 tkps.\n\nWhat's your full llama.cpp command?\n\nI got 10+ t/s for Qwen3-Next-80B IQ4\\_XS with my 8GB VRAM+32GB RAM when llama-benched with no context. And it was with old GGUF & before all Qwen3-Next optimizations.",
              "score": 3,
              "created_utc": "2026-02-03 17:47:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3douaq",
                  "author": "sine120",
                  "text": "I'm an LM studio heathen for models I'm just playing around with.  I just offloaded layers and context until my GPU was full.  Q8 context, default template.",
                  "score": 2,
                  "created_utc": "2026-02-03 17:54:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ebfd3",
                  "author": "Orph3us42",
                  "text": "Are you using cpu-moe ?",
                  "score": 1,
                  "created_utc": "2026-02-03 19:37:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dkvms",
          "author": "sleepingsysadmin",
          "text": "Well after tinkering with fitting it to my system, I cant load it all to vram :(\n\nI get about 15TPS. \n\nKilo code straight up failed. I probably need to update it. Got qwen code updated trivially and coded with it.\n\nOh baby it's really strong. Much stronger coder than GPT 20b high. I'm not confident about if it's better or not compared to GPT 120b. \n\nAfter it completed, it got: \\[API Error: Error rendering prompt with jinja template: \"Unknown StringValue filter: safe\".\n\nUnsloth jinja wierdness? I didnt touch it.",
          "score": 3,
          "created_utc": "2026-02-03 17:36:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dx73t",
              "author": "thaatz",
              "text": "I had the same issue. I removed the check for `safe` in the jinja template on the line where it says `{%- set args_value = args_value if args_value is string else args_value | tojson | safe %}`. The idea is that since that line filters for \"safe\" but then doesn't know what to do with it, I just dont check for the value \"safe\".  \nSeems to be working in kilo code for now, hopefully there is a real template fix/update in the coming days.",
              "score": 3,
              "created_utc": "2026-02-03 18:32:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fo6lw",
                  "author": "IceTrAiN",
                  "text": "Thanks, this helped my LM Studio API respond to tool calls correctly. I had to remove it in two spots in the template.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d29ci",
          "author": "MaxKruse96",
          "text": "brb creaming my pants",
          "score": 10,
          "created_utc": "2026-02-03 16:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d2hhd",
              "author": "danielhanchen",
              "text": "Haha",
              "score": 1,
              "created_utc": "2026-02-03 16:11:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3d5ob5",
          "author": "Extra_Programmer788",
          "text": "Is the there any inference provides it for free to try?",
          "score": 2,
          "created_utc": "2026-02-03 16:26:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dobn0",
          "author": "Deep_Traffic_7873",
          "text": "Is this model better or worse than qwen 30b a3b ?Â ",
          "score": 2,
          "created_utc": "2026-02-03 17:52:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dszr7",
              "author": "TokenRingAI",
              "text": "Definitely better",
              "score": 5,
              "created_utc": "2026-02-03 18:13:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dw22b",
                  "author": "Deep_Traffic_7873",
                  "text": "Both are a3b i'd like to see also it in the benchmark",
                  "score": 0,
                  "created_utc": "2026-02-03 18:27:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e6g3y",
              "author": "sleepingsysadmin",
              "text": "For sure better. Not even a question to me.",
              "score": 5,
              "created_utc": "2026-02-03 19:14:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3edi1z",
          "author": "sagiroth",
          "text": "So wait can I run Q3 with 8vram and 32gm system ram ?",
          "score": 2,
          "created_utc": "2026-02-03 19:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f2hrf",
          "author": "7h3_50urc3",
          "text": "Tried it with opencode and when writing files it always fails with: Error message: JSON Parse error: Unrecognized token '/'\\]\n\nDoesn't matter Q4 or Q8, unsloth or qwen gguf.",
          "score": 2,
          "created_utc": "2026-02-03 21:43:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fj4hl",
              "author": "7h3_50urc3",
              "text": "Seems to be a bug in llama.cpp so never mind.",
              "score": 1,
              "created_utc": "2026-02-03 23:05:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3die84",
          "author": "nunodonato",
          "text": "Help me out guys, if I want to run the Q4 with 256k context, how much VRAM are we talking about?",
          "score": 3,
          "created_utc": "2026-02-03 17:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dd2jl",
          "author": "iAndy_HD3",
          "text": "Us 16vram are so left out of everything cool",
          "score": 1,
          "created_utc": "2026-02-03 17:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3df87c",
          "author": "gamblingapocalypse",
          "text": "Oooooooo :)",
          "score": 1,
          "created_utc": "2026-02-03 17:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3et0bn",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 0,
          "created_utc": "2026-02-03 21:00:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7ncz",
      "title": "Design Arena is now dominated by an open model",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qr7ncz",
      "author": "moks4tda",
      "created_utc": "2026-01-30 14:55:35",
      "score": 305,
      "num_comments": 39,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2m4m2f",
          "author": "JackStrawWitchita",
          "text": "Kimi has been my online go-to LLM for weeks now. Haven't used chatgpt at all and only use gemini every now and then. I used to just visit kimi every now and then but their big models are amazing.\n\nI just wish I had the local horsepower to run their local models.",
          "score": 34,
          "created_utc": "2026-01-30 15:08:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m42c7",
          "author": "Distinct-Expression2",
          "text": "arena rankings shuffle every time a new model drops. more interesting is whether open models can hold the top spot for more than a week before the next closed model update.",
          "score": 9,
          "created_utc": "2026-01-30 15:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm1hu",
          "author": "TheRealGentlefox",
          "text": "By \"dominated\" you mean it ties with Gemini?",
          "score": 13,
          "created_utc": "2026-01-31 01:06:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3700",
          "author": "GenLabsAI",
          "text": "Let me just add that Kimi K2.5 came out less than a week ago. If you know how ELO ratings work, you know.  \n(don't get me wrong, it's still pretty goodl)",
          "score": 53,
          "created_utc": "2026-01-30 15:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mq45h",
              "author": "-p-e-w-",
              "text": "Elo-type ratings come with an associated confidence parameter (called the K-factor in chess) that makes them statistically sound *regardless* of how many pairings have been evaluated. Itâ€™s even possible to express this in the form of a score interval, which e.g. LMArena does.\n\nThe idea that the ratings for new models somehow arenâ€™t valid is just plain incorrect. If anything, the ratings for such models tend to be underestimated relative to their true performance, because they are initialized to some (low) baseline and have to rise from there.",
              "score": 28,
              "created_utc": "2026-01-30 16:45:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nfdex",
                  "author": "SlowFail2433",
                  "text": "Yes absolutely, the mathematics of ELO systems handles this issue implicitly. Had to learn this for Chess reasons lol",
                  "score": 15,
                  "created_utc": "2026-01-30 18:36:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nyljx",
                  "author": "GenLabsAI",
                  "text": "BUT... nobody actually is reading the K-factor/confidence",
                  "score": 4,
                  "created_utc": "2026-01-30 20:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oy6bu",
              "author": "RuthlessCriticismAll",
              "text": "> If you know how ELO ratings work, you know.\n\nYou don't.",
              "score": 8,
              "created_utc": "2026-01-30 22:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oyayh",
                  "author": "GenLabsAI",
                  "text": "Maybe",
                  "score": -5,
                  "created_utc": "2026-01-30 22:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m3s6j",
          "author": "Dr_Kel",
          "text": "Pretty cool, but... What does *design*Arena test?\n\nUI layout? Clothes/costumes? Building interiors? Database schemas? There's so much that can be described as \"design\", not the best name for a benchmark!",
          "score": 9,
          "created_utc": "2026-01-30 15:04:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m78ac",
              "author": "Charuru",
              "text": "Just go look at the website? It clearly has filters for all the categories. https://www.designarena.ai/leaderboard",
              "score": 11,
              "created_utc": "2026-01-30 15:20:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m8xom",
                  "author": "Dr_Kel",
                  "text": "How come that in every design category Kimi K2.5 is below the first place, but in \"All Categories\" it's #1?",
                  "score": 10,
                  "created_utc": "2026-01-30 15:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mc04z",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 8,
          "created_utc": "2026-01-30 15:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlg8v",
              "author": "No_Afternoon_4260",
              "text": "They also have good models, so.. ðŸ¤·",
              "score": 12,
              "created_utc": "2026-01-30 16:24:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mpwpp",
                  "author": "AdSouth4334",
                  "text": "The models that are only good on paper but breaks apart the moment you give it something half-complex as something that Claude can solve in one-shot.  \n  \nJust like Gemini 3, it's a model optimized for benchmarks only, but it has zero reliability on real-world tasks",
                  "score": -11,
                  "created_utc": "2026-01-30 16:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mm6dx",
              "author": "vasileer",
              "text": "even if it is marketing: is the information correct? are they #1 on design arena?\n\nif so, then I see no problems",
              "score": 4,
              "created_utc": "2026-01-30 16:27:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oi5l7",
          "author": "crantob",
          "text": "This benchmaxxing depresses me when I get more intelligent behavior out of qwen3-235b than GLM 4.7 in iterative project development (no agentic).\n\nGLM4.7 \"Oh that function was important to the program and it won't compile without it?  Seemed too much bother to me to keep it, sorry about that.  Here's the program with important_thing() restored.\"\n\n<code>\n\n[Forgets a different thing]",
          "score": 4,
          "created_utc": "2026-01-30 21:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qsu6z",
              "author": "ThatRandomJew7",
              "text": "Unironically, Kimi is one of the few open models that actually appears to be as good as the benchmarks suggest. \n\nI'm actually considering replacing Gemini 3 Pro with it",
              "score": 4,
              "created_utc": "2026-01-31 05:41:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rplyv",
          "author": "Fast-Satisfaction482",
          "text": "In the lead by a few points in a plot without error bars is definitely not \"domination\". It's inconclusive at best.",
          "score": 1,
          "created_utc": "2026-01-31 10:39:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tk4mv",
          "author": "Relevant-Service9871",
          "text": "Comment on utilise cette ia",
          "score": 1,
          "created_utc": "2026-01-31 17:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mv9sa",
          "author": "Dependent-Example930",
          "text": "How are most people using kimi k2.5? What service?",
          "score": 1,
          "created_utc": "2026-01-30 17:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyo87",
              "author": "GenLabsAI",
              "text": "Kimi, OpenRouter.",
              "score": 4,
              "created_utc": "2026-01-30 20:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oijqz",
              "author": "synn89",
              "text": "Fireworks.ai",
              "score": 1,
              "created_utc": "2026-01-30 21:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2p7hpv",
                  "author": "Turbulent_Pin7635",
                  "text": "LM Studio =)",
                  "score": 3,
                  "created_utc": "2026-01-30 23:46:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2nxpri",
              "author": "IrisColt",
              "text": "P-perplexity?",
              "score": -1,
              "created_utc": "2026-01-30 19:59:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qskrc",
                  "author": "ThatRandomJew7",
                  "text": "Doesn't have K2.5 yet, sadly",
                  "score": 0,
                  "created_utc": "2026-01-31 05:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m481c",
          "author": "jacek2023",
          "text": "Ok let's wait for the bots to upvote",
          "score": -10,
          "created_utc": "2026-01-30 15:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2slyma",
          "author": "LocoMod",
          "text": "Step 1: Ask the model to place their initials at the bottom of the page.\n\nStep 2: Vote for the motherland model.\n\nStep 3: ???\n\nStep 4: Profit!\n\n  \nIt is easy to game this and pump your model to the top. Doesn't take many since its not a super high traffic site.",
          "score": -2,
          "created_utc": "2026-01-31 14:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m4jb1",
          "author": "DrummerPrevious",
          "text": "Glm actually sucks",
          "score": -8,
          "created_utc": "2026-01-30 15:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2macbm",
              "author": "sleepy_roger",
              "text": "Meh, GLM has been my go to design model for a while now. It's made some great designs with easier prompts than I've seen Claude do.",
              "score": 12,
              "created_utc": "2026-01-30 15:34:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mle8e",
              "author": "TokenRingAI",
              "text": "It's great for UI work",
              "score": 5,
              "created_utc": "2026-01-30 16:24:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2nlvhn",
              "author": "fabricio3g",
              "text": "I find it very useful for finding bugs and analyzing code",
              "score": 5,
              "created_utc": "2026-01-30 19:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtvo4r",
      "title": "128GB devices have a new local LLM king: Step-3.5-Flash-int4",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
      "author": "tarruda",
      "created_utc": "2026-02-02 13:55:00",
      "score": 303,
      "num_comments": 167,
      "upvote_ratio": 0.96,
      "text": "Here's the HF Repo: http://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4 (this is a GGUF repo)\n\nI've been running this LLM for about an hour and it has handled all coding tests I've thrown at it in chat mode. IMO this is as good if not better than GLM 4.7, Minimax 2.1 while being much more efficient. Later I will try some agentic coding to see how it performs, but I already have high hopes for it.\n\nI use a 128GB M1 ultra mac studio and can run it at full context (256k). Not only it is fast, but also super efficient in RAM usage.\n\n*Update: I ran llama-bench with up to 100k prefill. Here are the results:\n\n    % llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000\n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.024 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        281.09 Â± 1.57 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         34.70 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        248.10 Â± 1.08 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         31.69 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        222.18 Â± 0.49 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         30.02 Â± 0.04 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        200.68 Â± 0.78 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         28.62 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        182.86 Â± 0.55 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         26.89 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        167.61 Â± 0.23 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         25.37 Â± 0.03 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        154.50 Â± 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         24.10 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        143.60 Â± 0.29 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         22.95 Â± 0.01 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        134.02 Â± 0.35 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         21.87 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        125.34 Â± 0.19 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         20.66 Â± 0.02 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        117.72 Â± 0.07 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         19.78 Â± 0.01 |\n    \n    build: a0dce6f (24)\n\nThis is still very usable with 100k prefill, so a good option for CLI coding agents!\n\nYou need to build a llama.cpp fork to run it, instructions at the HF repo. Though this model is so good that I believe it will soon be supported by llama.cpp upstream.",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qtvo4r/128gb_devices_have_a_new_local_llm_king/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o35sh4e",
          "author": "Proper_Taste_6778",
          "text": "I wonder how it will perform on Strix Halo.",
          "score": 49,
          "created_utc": "2026-02-02 14:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o361c14",
              "author": "Outrageous_Fan7685",
              "text": "I'm testing now on strix halo with 128k c",
              "score": 26,
              "created_utc": "2026-02-02 15:17:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o365f0k",
                  "author": "Proper_Taste_6778",
                  "text": "What performance can you get?",
                  "score": 6,
                  "created_utc": "2026-02-02 15:37:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36ahap",
                  "author": "Outrageous_Fan7685",
                  "text": "Sorry guys, still waiting for the team pr of llama.cpp",
                  "score": 11,
                  "created_utc": "2026-02-02 16:00:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o363bnm",
                  "author": "oxygen_addiction",
                  "text": "And?",
                  "score": 9,
                  "created_utc": "2026-02-02 15:27:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o382rl3",
                  "author": "Outrageous_Fan7685",
                  "text": "https://preview.redd.it/t5zhe1ila5hg1.jpeg?width=2268&format=pjpg&auto=webp&s=de401d36d7cc34d131dce9eaf75a872e16c6b04d\n\nVulkan, neat !",
                  "score": 4,
                  "created_utc": "2026-02-02 20:57:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o36eab1",
                  "author": "JamesEvoAI",
                  "text": "Also piling on to find out how it goes later. Curious to know about how much RAM it used",
                  "score": 1,
                  "created_utc": "2026-02-02 16:18:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o38gbxt",
              "author": "GPU-Appreciator",
              "text": "    ./llama.cpp/build/bin/llama-bench -m step3p5_flash_Q4_K_S.gguf -fa 1 -ngl 999 -d 0,32000\n    ggml_vulkan: Found 1 Vulkan devices:\n    ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |           pp512 |        209.43 Â± 1.74 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |           tg128 |         27.95 Â± 0.18 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |  pp512 @ d32000 |        117.58 Â± 0.43 |\n    | step35 ?B Q4_K - Small         | 103.84 GiB |   196.96 B | Vulkan     | 999 |  1 |  tg128 @ d32000 |         22.30 Â± 0.05 |",
              "score": 5,
              "created_utc": "2026-02-02 22:01:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o38wjkj",
              "author": "Grouchy-Bed-7942",
              "text": "some benchmarks here : [https://www.reddit.com/r/LocalLLaMA/comments/1qubamo/some\\_step35flash\\_benchmarks\\_on\\_amd\\_strix\\_halo](https://www.reddit.com/r/LocalLLaMA/comments/1qubamo/some_step35flash_benchmarks_on_amd_strix_halo)",
              "score": 2,
              "created_utc": "2026-02-02 23:25:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35qa1o",
          "author": "ohgoditsdoddy",
          "text": "I hope someone releases a NVFP4 version.",
          "score": 33,
          "created_utc": "2026-02-02 14:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35tb3k",
              "author": "Septerium",
              "text": "This is for Blackwell GPUs, right? What do you like more about NVFP4?",
              "score": 6,
              "created_utc": "2026-02-02 14:36:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o35xgta",
                  "author": "pulse77",
                  "text": "Not OP but: this is one of the best \\~4bit quants out there and is GPU accelerated...",
                  "score": 17,
                  "created_utc": "2026-02-02 14:58:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o361h7d",
                  "author": "Fear_ltself",
                  "text": "Isn't NVFP4 essentially lossless compression? 99% quality retained or more compared to other quantization methods",
                  "score": 4,
                  "created_utc": "2026-02-02 15:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3667ah",
              "author": "LA_rent_Aficionado",
              "text": "Why NVFP4? Itâ€™s currently severely unoptimized and out-performed (from a speed perspective) by AWQ, EXL3 and GGUFs of a similar size",
              "score": 6,
              "created_utc": "2026-02-02 15:41:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3823rg",
                  "author": "lol-its-funny",
                  "text": "NVFP4 is insanely fast on Blackwell processors including the GB10. 250% faster than other Q4 types. NVFP4 can be multiplied natively on Blackwell vs Q4s that have to be temporarily casted/converted to FP16 for multiplication.\n\nTBH I had hoped for AMD to announce hardware acceleration support for 4 bit quant multiplication at CES to follow-up on their Strix Halo v2\n\nLink: [https://medium.com/data-science-collective/nvfp4-same-accuracy-with-2-3x-higher-throughput-for-4-bit-llms-03518ecba108](https://medium.com/data-science-collective/nvfp4-same-accuracy-with-2-3x-higher-throughput-for-4-bit-llms-03518ecba108)\n\n![https://miro.medium.com/v2/resize:fit:1100/format:webp/0*d875cIaYOtGldgSc.png](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*d875cIaYOtGldgSc.png)",
                  "score": 10,
                  "created_utc": "2026-02-02 20:54:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39qx6s",
                  "author": "changtimwu",
                  "text": "Support and optimization for FP4 have been gradually improving. I believe weâ€™ll finally see the impact.   [https://github.com/vllm-project/vllm/releases/tag/v0.15.0](https://github.com/vllm-project/vllm/releases/tag/v0.15.0)",
                  "score": 3,
                  "created_utc": "2026-02-03 02:14:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o39m9bp",
                  "author": "TheThoccnessMonster",
                  "text": "Not if you have a DGX Spark or any Blackwell - you couldnâ€™t be more wrong lol",
                  "score": 2,
                  "created_utc": "2026-02-03 01:48:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3919j7",
          "author": "Grouchy-Bed-7942",
          "text": "Benchmark on AMD Strix Halo (Minisforum MS S1 Max), [https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4](https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4) (full context fits) :\n\n# Rocm 7.1.1\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|pp4096|258.82 Â± 3.15|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|pp32768|208.35 Â± 1.86|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|ROCm|999|1|0|tg512|22.93 Â± 0.00|\n\n# Vulkan-amdvlk\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp4096|153.04 Â± 0.30|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp32768|79.55 Â± 0.59|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|tg512|2.50 Â± 0.00|\n\n# Vulkan-radv\n\n# llama-bench\n\n|model|size|params|backend|ngl|fa|mmap|test|t/s|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp4096|164.20 Â± 1.30|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|pp32768|104.36 Â± 0.29|\n|step35 ?B Q4\\_K - Small|103.84 GiB|196.96 B|Vulkan|999|1|0|tg512|27.86 Â± 0.00|",
          "score": 13,
          "created_utc": "2026-02-02 23:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aj5eu",
              "author": "ga239577",
              "text": "Did you have to do anything special to run it?",
              "score": 1,
              "created_utc": "2026-02-03 05:11:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3awsly",
                  "author": "Grouchy-Bed-7942",
                  "text": "I based my work on kyuz0's toolbox and compiled the llamacpp version provided by the guys at steps on their GitHub repo.",
                  "score": 2,
                  "created_utc": "2026-02-03 07:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36q5rp",
          "author": "sine120",
          "text": "Wha... what are you doing step-flash?",
          "score": 20,
          "created_utc": "2026-02-02 17:13:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35ninh",
          "author": "DOAMOD",
          "text": "In my first test with it, I also got the feeling that it's a little better than Minimax M2.1.",
          "score": 14,
          "created_utc": "2026-02-02 14:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36rfu0",
              "author": "Dany0",
              "text": "A 196B/11B active model matching a 229B/45BA model just a month after it released honestly feels like magic\n\nIf only this kind of progress could be sustained, LLMs could actually be useful for real, hard world tasks soon. Or at least they could help engineer an ML solution that will be\n\nAnd keep in mind that a 196/11 setup will be much easier to REAP. One could easily sacrifice 40% of experts while keeping english & coding performance, and a heavily quantised version of that REAP could easily be run at \\~10 tok/s on relatively normal devices. That's near SOTA on consumer hardware.",
              "score": 9,
              "created_utc": "2026-02-02 17:19:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37cgdc",
                  "author": "steezy13312",
                  "text": "> matching a 229B/45BA model\n\nDoesn't Minimax 2.1 use 10B active params?",
                  "score": 11,
                  "created_utc": "2026-02-02 18:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35pwc4",
          "author": "Hauven",
          "text": "I'm tempted to try and see if it would be usable on my system, but I don't honestly know.\n\n\\- Threadripper 3995WX CPU  \n\\- 512GB DDR4 ECC RAM (3200)  \n\\- RTX 5090 (32GB)  \n\\- RTX 3090 (24GB)\n\nThat gives me 56GB\\~ VRAM at least, so about half way there, with the rest going on RAM. Question is would it work. I use it for things like TTS, image and video generation currently, not really for a text based LLM. I'm also assuming I can spread part of the model over the two GPUs and the rest on RAM, not just one GPU and then the rest on RAM.",
          "score": 6,
          "created_utc": "2026-02-02 14:18:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35qrqx",
              "author": "tarruda",
              "text": "It only uses 11B active parameters, so definitely worth trying. You should play with llama.cpp offloading options.",
              "score": 10,
              "created_utc": "2026-02-02 14:23:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35sidv",
                  "author": "Hauven",
                  "text": "Many thanks, I'll give it a go :).",
                  "score": 2,
                  "created_utc": "2026-02-02 14:32:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o369327",
              "author": "pmttyji",
              "text": "Q4 is 100GB. You have half TB RAM apart from VRAM. Go ahead.",
              "score": 8,
              "created_utc": "2026-02-02 15:54:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o373oao",
              "author": "munkiemagik",
              "text": "Interested to see how you find it, even though you are more image/video gen work orientated, which I don't do. \n\nI'm running 1x5090 & 2x3090 and this is the first model in a while that's tempting me to consider adding more 3090 in order to replace GPT-OSS-120b and GLM-4.5-AIR\\_Q4. \n\nMinimax M2.1 got me interested but I only would have been able to fit the heavily REAP'ed models, so held off.\n\nI'm going to be away for a week or so and unfortunately wont be faffing with the GPU server for a bit.",
              "score": 1,
              "created_utc": "2026-02-02 18:14:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37xtit",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/m0qgeewc65hg1.png?width=2677&format=png&auto=webp&s=2062d8bdf4a5c696d65945529c144bfcffcd419b\n\ntesting it on kilocode... Incredible... Really! (analizing my card game to test)",
          "score": 3,
          "created_utc": "2026-02-02 20:34:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38a39q",
              "author": "pixelpoet_nz",
              "text": "> analizing\n\nHope you used lube!\n\nAlso interesting that you're also testing with Slay the Spire like card games :)",
              "score": 6,
              "created_utc": "2026-02-02 21:32:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o36evk1",
          "author": "DeProgrammer99",
          "text": "What language(s) did you try it for?",
          "score": 3,
          "created_utc": "2026-02-02 16:21:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36f34a",
              "author": "tarruda",
              "text": "Only python and javascript.",
              "score": 5,
              "created_utc": "2026-02-02 16:22:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o393rj8",
          "author": "Claudius_the_II",
          "text": "The GPT-5.2 gap is striking - 69% vs ~30% for everything else. Makes you wonder whether its genuinely better at formal reasoning or just had more exposure to TM-style problems in training. Would love to see reasoning models like o3 or R1 on this - TM programming is basically pure state-machine logic, which should be their wheelhouse.",
          "score": 3,
          "created_utc": "2026-02-03 00:04:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35neih",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-02-02 14:05:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35nptd",
              "author": "tarruda",
              "text": "on my M1 Ultra, around 34 tokens/second with empty context. But the speed degrades much more slowly than other LLMs. I'm still seeing 30 tokens/second at 20k filled context.",
              "score": 16,
              "created_utc": "2026-02-02 14:06:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qtul",
                  "author": "oxygen_addiction",
                  "text": "If their fork has it, try to use self speculative decoding. For as much thinking as this model does, I think it'd be helpful.\n\n[https://github.com/ggml-org/llama.cpp/pull/19164](https://github.com/ggml-org/llama.cpp/pull/19164)",
                  "score": 7,
                  "created_utc": "2026-02-02 14:23:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o37pumy",
                  "author": "FPham",
                  "text": "This is actually very decent!",
                  "score": 1,
                  "created_utc": "2026-02-02 19:56:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35m5ln",
          "author": "Smart-Government6564",
          "text": "Holy shit this sounds promising ðŸ”¥ been waiting for something that can actually compete with GLM 4.7 without eating up all my RAM\n\n  \nThe fact that your running full 256k context on 128GB is wild - most models I've tried lately are memory hogs that tap out way before that. Definitely gonna have to compile that llama.cpp fork tonight and give it a spin on some coding tasks\n\n  \nHow's the response speed compared to minimax? That's usually my go-to for quick iterations but if this thing is more efficient AND better quality that's a game changer ðŸ’€",
          "score": 7,
          "created_utc": "2026-02-02 13:58:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35njhd",
              "author": "tarruda",
              "text": "So far it seems better than Minimax, all with less degradation of prompt processing and token generation when context increases, which makes it super promising for agentic use. I can actually run 2 128k context parallel streams at decent speeds. \n\nThis is the first time I feel I can run an Anthropic level LLM locally. While I could run Minimax (Q3) and GLM (IQ2), these quants severely degrade quality, so Step 3.5 flash is looking a much better option.\n\nI will continue using this through the day, but it is looking like I will delete GLM 4.7 and Minimax 2.1 weights to free up space.",
              "score": 7,
              "created_utc": "2026-02-02 14:05:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37ilrq",
                  "author": "Zyj",
                  "text": "I'm using MiniMax M2.1 Q6\\_K split across two machines. Perhaps using this one Q8 is better. I'll wait for the PR to be merged into llama.cpp",
                  "score": 2,
                  "created_utc": "2026-02-02 19:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36zwxd",
          "author": "Outrageous_Fan7685",
          "text": "Those are llama gguf split or cat split ?",
          "score": 2,
          "created_utc": "2026-02-02 17:57:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37287e",
              "author": "tarruda",
              "text": "cat split",
              "score": 4,
              "created_utc": "2026-02-02 18:08:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3736ch",
                  "author": "Outrageous_Fan7685",
                  "text": "After merge when i load it i have a segmentation fault i rebuilt llama from the upstream pr of pwilkin with rocm support.",
                  "score": 2,
                  "created_utc": "2026-02-02 18:12:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37ngb0",
          "author": "Potential-Leg-639",
          "text": "Any strix halo benchmarks?",
          "score": 2,
          "created_utc": "2026-02-02 19:45:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38f06e",
              "author": "GPU-Appreciator",
              "text": "I'm running one now. The performance seems quite solid - a short initial prompt got me a genuinely solid answer on a somewhat niche question,\\~26tk/s generation",
              "score": 4,
              "created_utc": "2026-02-02 21:55:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o38loba",
          "author": "SectionCrazy5107",
          "text": "i used this from their site but it went into huge reasoning block and not usable for coding, please confirm how you are able to test for coding? \n\n     -m step3.5_flash_Q4_K_S.gguf -c 16384 -b 2048 -ub 2048 -fa on --temp 1.0",
          "score": 2,
          "created_utc": "2026-02-02 22:28:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39heid",
              "author": "tarruda",
              "text": "-c 16384 means you only allow 16k context.\n\nThis model reasons a lot, so I think you need at least 32k.",
              "score": 2,
              "created_utc": "2026-02-03 01:20:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36iqbl",
          "author": "Conscious_Cut_6144",
          "text": "Interesting, everyone else seems to like it.  \nI'm running the GGUF and so far it doesn't seem to be beating K2.5, GLM4.7, D3.2, or MM2.1   \n  \nNot made up my mind yet, still testing...",
          "score": 1,
          "created_utc": "2026-02-02 16:39:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36smxg",
              "author": "shrug_hellifino",
              "text": "I haven't tried it yet, but aren't those models you mentioned in another weight class? If you are constrained to sub ~180 GB total, your options get shakey.",
              "score": 5,
              "created_utc": "2026-02-02 17:24:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3adwb8",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Their benchmarks claim that this model beats those. I think that's why the comment you're replying to is comparing those models.",
                  "score": 3,
                  "created_utc": "2026-02-03 04:34:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37p5pj",
          "author": "Flkhuo",
          "text": "Can this get loaded unto RTX4090 24gvram and 64gb RAM?",
          "score": 1,
          "created_utc": "2026-02-02 19:53:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37wg3a",
              "author": "tarruda",
              "text": "No, I think you need at least 120GB of RAM + VRAM.",
              "score": 2,
              "created_utc": "2026-02-02 20:27:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38kpch",
          "author": "SimplyRemainUnseen",
          "text": "Is this QAT? I love all the 4-bit models this year",
          "score": 1,
          "created_utc": "2026-02-02 22:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39h8tz",
              "author": "tarruda",
              "text": "Wouldn't be surprised if it is QAT, considering the quantized weights were released by the same team.",
              "score": 1,
              "created_utc": "2026-02-03 01:19:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3blss6",
                  "author": "audioen",
                  "text": "I wonder if anyone has actually written a single program that can QAT anything on GGUF. There exists QAT GGUF files but they appear to be using the equivalent of Q4\\_0 only which is a very generic and simple quantization format and as long as the rate of scale factors is lower than in the Q4\\_0, other foreign int4 encodings can probably be converted to it without loss of accuracy.\n\nThis is Q4\\_K\\_S and I have doubt that this is QAT. At least, I'm not aware of anyone capable of producing QAT for this quantization format.\n\nGGUF QAT for these complex quantizations are an untapped market. Nobody is making them. But, for hobbyists with limited VRAM, being able to run roughly BF16 quality model at Q4\\_0 with minimal loss in K\\_L divergence would be excellent compared to current state of affairs, and I think software would exist for Q4\\_0 at least because it's that simple and generic. Obviously, doing the same thing at 2 bits, if it is possible, would be better still. The model is probably mildly perturbed by this degree of extreme quantization even when the full precision model would be used as the trainer and K\\_L divergence would be minimized.\n\nWe really don't need these complex IQ\\_N\\_SOMETHING thingys and QN\\_K\\_FOOBAR as much as we should just bite the bullet and figure out a bit depth to which QAT still can work and then pool our resources to buy the compute in order to get a single high-quality quantized BF16-level variant. It would automatically give us a far better result than any PTQ scheme. The challenge is to design low-bitwidth quantization for which a stable training procedure can be written. We know it exists at least for Q4\\_0 and MXFP4 because these have been made. It might even exist for those ternary schemes, which is pretty much the last word on quantization as far as I can tell.",
                  "score": 1,
                  "created_utc": "2026-02-03 10:59:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38m413",
          "author": "jinnyjuice",
          "text": "It takes up 111 GB, so shouldn't it be recommended for devices with 200 GB+ memory?",
          "score": 1,
          "created_utc": "2026-02-02 22:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38wz62",
              "author": "huzbum",
              "text": "AWQ or Q4\\_XL is a lot smaller.",
              "score": 1,
              "created_utc": "2026-02-02 23:27:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3bcuvd",
                  "author": "Karyo_Ten",
                  "text": "It's 199B param and in 4-bit down to 111GB: https://huggingface.co/stepfun-ai/Step-3.5-Flash-Int4/tree/main\n\nNow given that VRAM is in GiB not GB it's like 103.4 GiB VRAM needed for the weights",
                  "score": 2,
                  "created_utc": "2026-02-03 09:35:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3koe2w",
              "author": "LizardViceroy",
              "text": "about 100GB at Q4\\_K\\_S and its KV cache is organized extremely efficiently so it takes up so little space that a 128GB machine can soak up 256K context with almost 16GB to spare.",
              "score": 1,
              "created_utc": "2026-02-04 18:25:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3a3aze",
          "author": "_hephaestus",
          "text": "Looks like there's an mlx version of it already, trying that out. https://huggingface.co/mlx-community/Step-3.5-Flash-4bit\n\nNot sure how to benchmark that though",
          "score": 1,
          "created_utc": "2026-02-03 03:26:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g3sjx",
              "author": "FPham",
              "text": "But how do you use it (outside directly in python)?",
              "score": 1,
              "created_utc": "2026-02-04 00:58:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gg05k",
                  "author": "_hephaestus",
                  "text": "Apparently I canâ€™t. May try more for it later but was definitely naive in thinking it would just work out automagically",
                  "score": 1,
                  "created_utc": "2026-02-04 02:07:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3an12k",
          "author": "cleverestx",
          "text": "Anyone get this running on Strix Halo yet? Either through LM Studio or LLama.cpp?",
          "score": 1,
          "created_utc": "2026-02-03 05:40:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bmega",
              "author": "Grouchy-Bed-7942",
              "text": "https://www.reddit.com/r/LocalLLaMA/s/PsYrR7SI6U",
              "score": 1,
              "created_utc": "2026-02-03 11:04:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b3g88",
          "author": "manipp",
          "text": "Anyone have any sense if it's any good at writing?",
          "score": 1,
          "created_utc": "2026-02-03 08:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bqq82",
          "author": "SpicyWangz",
          "text": "The biggest problem is just how many reasoning tokens it uses. It churns through so many of them",
          "score": 1,
          "created_utc": "2026-02-03 11:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c9cm3",
          "author": "Diao_nasing",
          "text": "pp is really slow",
          "score": 1,
          "created_utc": "2026-02-03 13:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5nh8",
              "author": "tarruda",
              "text": "Apple silicon tends to be bad with prompt processing, but these numbers are OK for me due to automatic  prompt caching done by llama.cpp",
              "score": 1,
              "created_utc": "2026-02-03 16:26:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cait1",
          "author": "Fast_Thing_7949",
          "text": "  \nRemindMe! 7 days",
          "score": 1,
          "created_utc": "2026-02-03 13:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ju07k",
          "author": "LizardViceroy",
          "text": "It seems to be designed to be used in the context of their Parallel Coordinated Reasoning (PaCoRe) algorithm and I wouldn't be too surprised if they used a little (or a lot) of that to game the benchmark results where there's any room left for ambiguity.  \nWhen you see a 74.4% SWE bench verified score you can bet on it they're pulling out all stops on the scaffolding front, meaning PaCoRe running on all cylinders. And that means glacial reasoning speed.  \nIf this thing turns out to only be as good as they say after several reasoning passes I might see this as more of a companion to GPT-OSS-120b than a replacement. Not the same speed category in that case.",
          "score": 1,
          "created_utc": "2026-02-04 16:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3juzmn",
              "author": "tarruda",
              "text": "> I might see this as more of a companion to GPT-OSS-120b than a replacement\n\nI agree. Even though it is not the most \"intelligent\" model, GPT-OSS 120b is still the best overall local LLM when you consider its efficiency.",
              "score": 1,
              "created_utc": "2026-02-04 16:10:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35wshg",
          "author": "Dundell",
          "text": "Shoot here I go... I might as well invest in a 3090 to add to my vram pool. Bump it from 84 to 108GB vram...",
          "score": 1,
          "created_utc": "2026-02-02 14:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35o331",
          "author": "segmond",
          "text": "What sort of performance are you seeing?",
          "score": 1,
          "created_utc": "2026-02-02 14:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35obn6",
              "author": "tarruda",
              "text": "~30 t/s generation and ~300 t/s prompt processing. I will post a full benchmark later",
              "score": 9,
              "created_utc": "2026-02-02 14:10:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35pb9j",
                  "author": "Fancy_Fanqi77",
                  "text": "Can't wait to see",
                  "score": 2,
                  "created_utc": "2026-02-02 14:15:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35qtdg",
                  "author": "segmond",
                  "text": "thanks, that's enough info for me to give it a shot.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ct1ni",
                  "author": "wanderer_4004",
                  "text": "MLX is now also supporting it. Could you run that too and get us some benchmark as well?",
                  "score": 1,
                  "created_utc": "2026-02-03 15:26:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36hi5z",
          "author": "NotLordo",
          "text": "RemindMe! 2 days",
          "score": 0,
          "created_utc": "2026-02-02 16:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39zpie",
          "author": "Brospeh-Stalin",
          "text": "u/AskGrok how do I get an LLM to interface with reddit and post on it? How specifically are you fed reddit posts and comments? What all do you see and what don't you see? How is it formatted in the input?",
          "score": -1,
          "created_utc": "2026-02-03 03:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o39zt07",
              "author": "AskGrok",
              "text": "To get an LLM to interface with Reddit and post on it, you'd typically use Reddit's official API (via PRAW in Python, for example) to handle authentication, fetching posts, and submitting comments or threads. Hook the LLM into a script that processes API responses as prompts, generates replies, and posts them backâ€”think of it as giving your model a Reddit account and some automation glue. But heads up: Reddit's terms frown on bots spamming or impersonating users, so keep it ethical and rate-limited, or risk bans. For specifics, check Reddit's API docs and libraries like PRAW; it's not rocket science, but test in a sandbox.\n\nAs for me (Grok), I'm fed Reddit posts and comments through a structured input prompt that includes the post title, content, and comment chainâ€”formatted as plain text, like a conversation transcript. I see the full text of the title, body, and relevant comments (including this one), but not things like upvotes, user profiles, images, or live updates unless I query for them. No peeking at your browser history or anything creepy; it's all sanitized and passed in as a static snapshot. Think of it as getting a neatly typed letter in the mail, not the whole envelope factory.\n\n^([what is this?](https://redd.it/1lzgxii))",
              "score": -1,
              "created_utc": "2026-02-03 03:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o39zzdp",
                  "author": "Brospeh-Stalin",
                  "text": "Since the bot is not developed by xAI, only the LLM, can you tell me what the prompt looks like?",
                  "score": -1,
                  "created_utc": "2026-02-03 03:06:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qt76qs",
      "title": "Mistral Vibe 2.0",
      "subreddit": "LocalLLaMA",
      "url": "https://mistral.ai/news/mistral-vibe-2-0",
      "author": "jacek2023",
      "created_utc": "2026-02-01 18:56:50",
      "score": 296,
      "num_comments": 49,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qt76qs/mistral_vibe_20/",
      "domain": "mistral.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o3334rx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-02 02:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313gy8",
          "author": "Synor",
          "text": "European tool. Made in France. Go for it!",
          "score": 110,
          "created_utc": "2026-02-01 20:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31mldj",
              "author": "Eupolemos",
              "text": "I use it with devstral 2 local - it's nice :)",
              "score": 17,
              "created_utc": "2026-02-01 21:40:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o31vats",
                  "author": "jacek2023",
                  "text": "Good choice!",
                  "score": 1,
                  "created_utc": "2026-02-01 22:23:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o31wg3g",
              "author": "ClimateBoss",
              "text": "now with ads for buy \"Pro\" version LMAO",
              "score": 9,
              "created_utc": "2026-02-01 22:28:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o32fh5q",
                  "author": "cosimoiaia",
                  "text": "ads? The banner that you can disable? \n\nwait to see what happens on the other platforms and then ðŸ¤£\n\nFind another way to cope, vibe 2.0 is great!",
                  "score": 6,
                  "created_utc": "2026-02-02 00:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o32cp0r",
                  "author": "see_spot_ruminate",
                  "text": "I hate ads too, they are at least internal upselling, and you could clone the repo and then use mistral-vibe to remove the ad popup. Plus it at least is only at the start of convos. What would get me to stop using is if it had ads for other products like toilet paper.",
                  "score": -2,
                  "created_utc": "2026-02-01 23:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o334nem",
              "author": "Porespellar",
              "text": "Probably uses the metric system tho.",
              "score": 1,
              "created_utc": "2026-02-02 02:33:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o31hw9a",
          "author": "see_spot_ruminate",
          "text": "As a idiot, I have been finding mistral-vibe to be working well. \n\nI found tool calls to work better if I explicitly put the list of tools into the ~/.vibe/promps/cli.md at the top that way it knows that it is a tool.",
          "score": 8,
          "created_utc": "2026-02-01 21:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lebk",
              "author": "jacek2023",
              "text": "Why idiot?",
              "score": 4,
              "created_utc": "2026-02-01 21:34:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31lo10",
                  "author": "see_spot_ruminate",
                  "text": "Oh this is just my hobby. I might be at a novice level, maybe.",
                  "score": 4,
                  "created_utc": "2026-02-01 21:35:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30v3zq",
          "author": "DHasselhoff77",
          "text": "At this point you'd expect them to tell us why use it instead of OpenCode. They both seem to copy ClaudeCode as far as I can see.",
          "score": 34,
          "created_utc": "2026-02-01 19:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o314ydx",
              "author": "DanRey90",
              "text": "IMHO if you use Devstral, use Vibe. Each agentic tool has a different massive system prompt with slightly different tool definitions. It seems that every AI lab is fine-tuning their model to perform better with their harness. Theyâ€™ll all work on every CLI, sure, but Kimi 2.5 will surely perform better with Kimi CLI, Sonnet with Claude Code, GPT with Codex, etc.\n\nZ.ai seems to be the holdout so far, they havenâ€™t released a CLI, so they chose to tune their models for Claude Code. It sucks, but the choice now is to pick a tool and know that your model selection may make it work â€œsub-optimallyâ€, or be prepared to jump between tools when you want to switch models. At this time where all the labs seem to be leapfrogging each other every few weeks, that gives a bit of FOMO. I have the GLM coding plan and Iâ€™ll stick to it for a while, so the next thing Iâ€™ll do is switch to Claude Code when I get tired of Cline.",
              "score": 45,
              "created_utc": "2026-02-01 20:14:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3169r9",
                  "author": "TheRealMasonMac",
                  "text": "[Z.ai](http://Z.ai) says they already have one in-house that they're working on releasing eventually. MiniMax too.",
                  "score": 14,
                  "created_utc": "2026-02-01 20:20:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o31843a",
                  "author": "DHasselhoff77",
                  "text": "All you say is true. I just wonder why they don't tell us on the website that it's the recommended way to enjoy Devstral 2. I mean, they do mention it's \"powered by\" it so perhaps they consider it obvious? Now they're hyping features that competitors already have.\n\nTo customers, using this Devstral-specific tool is a tradeoff between Mistral's strategic goals (nobody wants to be held at the mercy of a 3rd party open source project) and customer's own convenience (a single popular tool for all models is preferred). If OpenCode was a real free software project and not a VC-funded loss leader, then I could see Mistral having an incentive to contribute to it directly. But that's not the AI future we live in.",
                  "score": 2,
                  "created_utc": "2026-02-01 20:29:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o31az3t",
                  "author": "evia89",
                  "text": "> Z.ai seems to be the holdout so far, they havenâ€™t released a CLI, so they chose to tune their models for Claude Code. It sucks\n\nwhy? Claude code is pretty good and you can edit system prompts with tweak cc. My only problem with it is no LTS. You have to freeze version yourself and stop updating for 1-2 months",
                  "score": 0,
                  "created_utc": "2026-02-01 20:43:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o30wb5m",
              "author": "jacek2023",
              "text": "Nothing wrong with that, they are open source alternatives.",
              "score": 50,
              "created_utc": "2026-02-01 19:32:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o32acrz",
              "author": "jsonmeta",
              "text": "Itâ€™s a CLI tool.. how much unique does it have to be?",
              "score": 4,
              "created_utc": "2026-02-01 23:43:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o31570m",
              "author": "Deep_Traffic_7873",
              "text": "opencode doesn't support markdown tables",
              "score": -1,
              "created_utc": "2026-02-01 20:15:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o31iyjf",
                  "author": "UnnamedUA",
                  "text": "https://github.com/franlol/opencode-md-table-formatter/tree/main",
                  "score": 3,
                  "created_utc": "2026-02-01 21:22:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3161lb",
                  "author": "jacek2023",
                  "text": "What do you mean?",
                  "score": 1,
                  "created_utc": "2026-02-01 20:19:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o32fsuc",
          "author": "tarruda",
          "text": "I haven't use mistral-vibe much yet, but I like how short the code is compared to alternatives. Running from the repo dir:\n\n    $ find vibe -name '*.py' | xargs wc -l\n\nShows 19472 lines in total. This is much lower than alternatives such as codex or opencode, showing that the devs do care about code quality vs just vibe coding every feature/fix which easily explodes past 100k lines.",
          "score": 5,
          "created_utc": "2026-02-02 00:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32cqht",
          "author": "rorowhat",
          "text": "Can you run this local and offline?",
          "score": 6,
          "created_utc": "2026-02-01 23:56:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32eg66",
              "author": "jacek2023",
              "text": "That's the main point",
              "score": 4,
              "created_utc": "2026-02-02 00:06:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o32wi7n",
                  "author": "rorowhat",
                  "text": "I see a monthly charge",
                  "score": 1,
                  "created_utc": "2026-02-02 01:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o354pt7",
          "author": "getvia",
          "text": "I use it instead of claude code, perfect need!",
          "score": 1,
          "created_utc": "2026-02-02 12:06:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o354syb",
          "author": "DefNattyBoii",
          "text": "I've been torn between using this vs. opencode. Can anyone argue for one over the other? I mainly use local models like glm-flash, sometimes larger sometimes smaller. I see that opencode might have better velocity for shipping features with its pros and cons.",
          "score": 1,
          "created_utc": "2026-02-02 12:07:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35563s",
              "author": "jacek2023",
              "text": "why not both?",
              "score": 1,
              "created_utc": "2026-02-02 12:10:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36xgca",
                  "author": "DefNattyBoii",
                  "text": "Well, thats where i am",
                  "score": 1,
                  "created_utc": "2026-02-02 17:46:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37b7dd",
          "author": "Mickenfox",
          "text": "Is there any reason to use this kind of tool and not an IDE?",
          "score": 1,
          "created_utc": "2026-02-02 18:48:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31kwl4",
          "author": "griserosee",
          "text": "I use it everyday",
          "score": 1,
          "created_utc": "2026-02-01 21:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31lcf0",
              "author": "jacek2023",
              "text": "With what model?",
              "score": 4,
              "created_utc": "2026-02-01 21:34:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31rwhl",
                  "author": "griserosee",
                  "text": "Shame on me. I use their paying models. devstral 2 medium",
                  "score": 5,
                  "created_utc": "2026-02-01 22:06:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}