{
  "metadata": {
    "last_updated": "2026-02-01 03:27:23",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 760,
    "file_size_bytes": 827254
  },
  "items": [
    {
      "id": "1qr4p4x",
      "title": "Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/n31pvrxchhgg1",
      "author": "Nunki08",
      "created_utc": "2026-01-30 12:55:38",
      "score": 1287,
      "num_comments": 182,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2lnz34",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-30 13:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lig2u",
          "author": "snekslayer",
          "text": "But how do you make money from being open? /s",
          "score": 180,
          "created_utc": "2026-01-30 13:14:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lrw79",
              "author": "pip25hu",
              "text": "Despite the \"/s\", this is not a question with an obvious answer. Meta justified releasing open-weight models saying that the ecosystem built around the models would offset the cost. But, well... there are no signs of a Llama 5 surfacing, unfortunately. Being open results in better models and faster advancement of the field as a whole, but it certainly doesn't seem to result in more money.",
              "score": 95,
              "created_utc": "2026-01-30 14:05:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m2180",
                  "author": "FpRhGf",
                  "text": "Was an ecosystem ever achieved for particular LLMs? I feel like the LLM scene was so hyper competitive since Llama 1 leaked that no particular ecosystem was built around certain LLMs. It made it easy for people to just switch up and plug newer models on release. \n\nWhereas with image models, a full ecosystem of tools was built specifically for SD 1.5, months before new base models could rival in quality. Every new model was practically dead on arrival because of it, despite the better quality. It took 2 years for a new model groundbreaking enough for people to move on and rebuild an ecosystem around it. Llama didn't get to have that smooth headstart",
                  "score": 45,
                  "created_utc": "2026-01-30 14:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mdkp0",
                  "author": "danielsan901998",
                  "text": "By increasing compute demand, hyperscalers like Alibaba can increase their profits by releasing open source models.",
                  "score": 18,
                  "created_utc": "2026-01-30 15:49:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n1kdi",
                  "author": "BumblebeeParty6389",
                  "text": "Lets be honest, llama did something amazing (albeit accidentally due to first leak of models at early 2023) and most of us wouldn't be here if llama didn't happen. Many Chinese AI startups like deepseek etc started out as small AI teams that finetune llama models. Meta and llama fell out of the game as it seems right now but they opened a very important path for opensource AI.",
                  "score": 11,
                  "created_utc": "2026-01-30 17:36:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2m22lp",
                  "author": "KaMaFour",
                  "text": "Well... May this be a sign that money driven development is something that has ran its course and US has a choice to either abandon it or no longer see themselves as an empire dominating the world...",
                  "score": 4,
                  "created_utc": "2026-01-30 14:56:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ref5j",
                  "author": "hadoopken",
                  "text": "Zack changed strategy again, probably no more openness",
                  "score": 1,
                  "created_utc": "2026-01-31 08:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mt1pa",
              "author": "MrPecunius",
              "text": "This was the same question/argument used against Linux 30 years ago, to cite the most prominent example among many.\n\nEdit: just saw the /s ðŸ˜… ... but I guess my observation still works if you squint.",
              "score": 9,
              "created_utc": "2026-01-30 16:58:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lqy0h",
              "author": "getshion",
              "text": "Build meaningful products on top of open research and monetize it. Isn't how the software field outside AI naturally evolves?",
              "score": 21,
              "created_utc": "2026-01-30 14:00:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ongdl",
              "author": "Warm-Border-9789",
              "text": "Pythagoras was not making money from his equation",
              "score": 4,
              "created_utc": "2026-01-30 22:01:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mhgay",
              "author": "autoencoder",
              "text": "Bragging rights, which lead to higher valuation of your team and skills, and lower funding costs. People see that you got great models cheaply, so they want to throw good money after good performance.",
              "score": 4,
              "created_utc": "2026-01-30 16:06:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oz513",
                  "author": "pierrenoir2017",
                  "text": "Maybe the current situation in the US was the plan all along. Pumping up the 'value', get an insane amount of funds until it lasts. Keeping up the facade until it breaks. Maybe we are naive to think they aren't just strategically creating, feeding and using a hype to make money, no matter if the outcome is actually successful... It's a different mindset and a business plan on its own. Yes, they barely make profit, but people involved do make a lot of money with it.",
                  "score": 4,
                  "created_utc": "2026-01-30 23:00:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ovn2g",
              "author": "fallingdowndizzyvr",
              "text": "> But how do you make money from being open? /s\n\nThe same way that companies make money from Linux even though it's open.",
              "score": 3,
              "created_utc": "2026-01-30 22:42:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m54fd",
              "author": "LatentSpaceLeaper",
              "text": "Ask Jensen.",
              "score": 2,
              "created_utc": "2026-01-30 15:10:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lo1k8",
              "author": "tentacle_",
              "text": "seriously? customization. much more than what you can do with fine-tuning.",
              "score": 6,
              "created_utc": "2026-01-30 13:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lttgr",
                  "author": "LanceThunder",
                  "text": "leasing hardware and services. if you open sourced a model that was 20 years ahead of anything else it wouldn't matter because 99.99999% of people couldn't run it.",
                  "score": 12,
                  "created_utc": "2026-01-30 14:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lsi6z",
              "author": "EconomySerious",
              "text": "API access",
              "score": 3,
              "created_utc": "2026-01-30 14:08:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lx811",
                  "author": "LevianMcBirdo",
                  "text": "Which others could offer also without developing a thing. There are some incentives to develop open source, but most are just not short time profits.",
                  "score": 6,
                  "created_utc": "2026-01-30 14:32:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ml50s",
              "author": "artisticMink",
              "text": "You make it by being heavily subsidized. \n\nI like chinese models. I use them a lot. But they're open because of the race. The second they've \"won\" the race, they will return to closed source.",
              "score": 3,
              "created_utc": "2026-01-30 16:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ox52w",
                  "author": "fallingdowndizzyvr",
                  "text": "> You make it by being heavily subsidized. \n\nWestern models are just as heavily \"subsidized\".",
                  "score": 2,
                  "created_utc": "2026-01-30 22:50:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2po78n",
              "author": "DocHoss",
              "text": "Models don't make any money by themselves. High powered models require high power hardware that's beyond the reach of all but the most well-heeled consumers. It's the same reason that Microsoft doesn't make any money on dot et, or Facebook not making money on React. The money is in the supporting infrastructure and products. So open models are kinda a gateway drug, as near as I can tell. Run low powered, accessible, open models doing inferencing on consumer devices where there isn't a lot of profit to be made anyway, and centralize the big, highly desirable models on enterprise grade hardware and charge for access. Main issue is that the cost curve hasn't flattened out yet so no one is making any real money on inferencing. The hyperscalers are making some on the surrounding infrastructure like data, and extra products like search and apps, but they're still fighting to see who will blink first in the AI race. No clear winners yet.",
              "score": 1,
              "created_utc": "2026-01-31 01:19:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2py54w",
              "author": "AvcalmQ",
              "text": "You sell hardware",
              "score": 1,
              "created_utc": "2026-01-31 02:18:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qgoru",
              "author": "zhambe",
              "text": "This is why the West already lost.",
              "score": 1,
              "created_utc": "2026-01-31 04:13:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qn6pj",
              "author": "Pvt_Twinkietoes",
              "text": "Don't need the /s . You can't.",
              "score": 1,
              "created_utc": "2026-01-31 04:58:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2p7egx",
              "author": "Yes_but_I_think",
              "text": "Going open shows the company's confidence in their own prowess. Even if they report a technique they have the confidence that they can come up with a better technique next by themselves. They know the community will validate/invalidate their methods for them for free. That is a technologically as well as ideologically superior company than someone who guards their work.\n\nAnd people want to deal with confident open companies naturally. And all business are people driven.\n\nI trust Deepseek with my data more than Anthropic. If Deepseek says my data is not used for training I believe them. But I have no doubt that Anthropic is violating their agreements with all their users. They even said openly in their Claude Cowork presentation that they found out that the users are using Claude code for organizing photos and that gave them the idea for Cowork. How can you see what people are doing?",
              "score": 1,
              "created_utc": "2026-01-30 23:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m1l1t",
              "author": "arstarsta",
              "text": "Chinese models where getting prohibited to use anyway so why not make US companies not make money too.",
              "score": -2,
              "created_utc": "2026-01-30 14:53:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljrs6",
              "author": "gweilojoe",
              "text": "They donâ€™t - itâ€™s the only way they have to compete. This entire article and most of the praising comments are just being amplified by Chincells, which is the majority of SEA Reddit users and bots.",
              "score": -33,
              "created_utc": "2026-01-30 13:22:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ll300",
                  "author": "No_Swimming6548",
                  "text": "Yes. all my homies run deepseek and kimi at home.",
                  "score": 14,
                  "created_utc": "2026-01-30 13:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lmzbp",
                  "author": "phein4242",
                  "text": "As a European, having xs to good & open local models is a plus. Esp now that the us models are closing down more & more.\n\nAnd I get it: The subscription model & advertisements usually leads to a steady revenue stream, and its already quite hard for us ai companies to deliver roi.\n\nExcept: competing with free / open is a hard thing to do once you go the subscription/ads route ;-)",
                  "score": 12,
                  "created_utc": "2026-01-30 13:39:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lr7sd",
                  "author": "gweilojoe",
                  "text": "As expected, looks like the CCP Stans and Chincells have brought their downvote bots.",
                  "score": -9,
                  "created_utc": "2026-01-30 14:02:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ls0ja",
          "author": "XeNoGeaR52",
          "text": "Open models are the future. Open standards are the future.",
          "score": 65,
          "created_utc": "2026-01-30 14:06:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mgw3z",
          "author": "WhyIsItGlowing",
          "text": "Huh, TIL it's pronounced \"archive\" not Ark-ziv",
          "score": 16,
          "created_utc": "2026-01-30 16:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2now4k",
              "author": "Impossible_Pomelo_58",
              "text": "I always understood the X in arXiv to be the greek letter 'chi', that's why it's pronounced like archive",
              "score": 16,
              "created_utc": "2026-01-30 19:19:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n4per",
              "author": "combasemsthefox",
              "text": "Yeah I mostly hear archive in academia",
              "score": 9,
              "created_utc": "2026-01-30 17:50:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n0j3v",
              "author": "MrPecunius",
              "text": "Same thing jumped out at me.",
              "score": 2,
              "created_utc": "2026-01-30 17:31:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lfqv0",
          "author": "SrijSriv211",
          "text": "Very very very true. So sad that OpenAI is \"Open\" just for name :(",
          "score": 100,
          "created_utc": "2026-01-30 12:58:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljm9g",
              "author": "XiRw",
              "text": "At least they put out gpt-oss for everyone. Claude on the other hand acts like they are Area-51 with their data.",
              "score": 80,
              "created_utc": "2026-01-30 13:21:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ln62d",
                  "author": "danttf",
                  "text": "\"their data\"",
                  "score": 100,
                  "created_utc": "2026-01-30 13:40:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ls1ut",
                  "author": "aeroumbria",
                  "text": "Red flags: closed, trying to push \"protocols\" that are just rehashed common sense, high price leader, lock-in mechanisms, \"defensive competition\" by pushing selective regulation, \"national security\", ...",
                  "score": 20,
                  "created_utc": "2026-01-30 14:06:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkpwe",
                  "author": "SrijSriv211",
                  "text": "Yeah. I hope OpenAI surprises us with GPT-OSS 2 this year. ***Wishful thinking***",
                  "score": 24,
                  "created_utc": "2026-01-30 13:27:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lkzv7",
                  "author": "mambo_cosmo_",
                  "text": "they stole a lot of writing they had no right to take. That's why they are so secretive",
                  "score": 4,
                  "created_utc": "2026-01-30 13:29:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pi2g2",
              "author": "Pvt_Twinkietoes",
              "text": "CLIP and Whisper were some really amazing work from them.",
              "score": 5,
              "created_utc": "2026-01-31 00:44:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjnwz",
                  "author": "SrijSriv211",
                  "text": "I really like Whisper, it's so good but I also want them to publish more open work in text-to-image, text-to-video & more reasoning models as well. If not open weights then at least research.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:33:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvd6r",
              "author": "sleepingsysadmin",
              "text": "GPT 20b and 120B are both best in slot. This new moltbot stuff, people are defaulting to 120b for local. \n\nWhat more do you want? GPT 2 likely drops this year and pushes the frontier even more.",
              "score": 3,
              "created_utc": "2026-01-30 14:23:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nxj1m",
                  "author": "lolwutdo",
                  "text": "I found 120b to be awful with clawdbot, glm 4.7 flash is the best atm.",
                  "score": 4,
                  "created_utc": "2026-01-30 19:58:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lwper",
                  "author": "SrijSriv211",
                  "text": "Is it confirmed that GPT-OSS 2 will drop this year?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2odt9e",
              "author": "Tall_East_9738",
              "text": "gpt-oss-120b and gpt-oss-20b are free for you to use btw",
              "score": 1,
              "created_utc": "2026-01-30 21:15:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oeay5",
                  "author": "SrijSriv211",
                  "text": "Their last open source model release & research paper before GPT-OSS was back in 2019 with GPT-2 btw",
                  "score": 2,
                  "created_utc": "2026-01-30 21:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pttor",
              "author": "pprstrt",
              "text": "Musk tried...",
              "score": 0,
              "created_utc": "2026-01-31 01:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qjh0e",
                  "author": "SrijSriv211",
                  "text": "And in the end. Grok is also closed source.",
                  "score": 3,
                  "created_utc": "2026-01-31 04:32:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lgv4r",
          "author": "FullstackSensei",
          "text": "No shit! It doesn't matter how smart you think the people in your company are, the collective intelligence of the masses will best your team every single time.",
          "score": 63,
          "created_utc": "2026-01-30 13:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lhtu6",
              "author": "Square_Alps1349",
              "text": "Especially the collective intelligence of the Chinese open source community.",
              "score": 33,
              "created_utc": "2026-01-30 13:11:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lnv3b",
              "author": "kaisurniwurer",
              "text": "Collective intelligence yes. Will to act for free, not so much.",
              "score": -16,
              "created_utc": "2026-01-30 13:44:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nwase",
          "author": "Tema_Art_7777",
          "text": "There is an ecosystem around model runners like vllm llama.cpp etc but not one particular model.  Even using a coding agent, by the time you built a viable first piece of an ecosystem, the model would be hopelessly out of date. Model influencers are having a horrible time now - when someone is hyping deepseek 3.2. someone else is on the glm bandwagon promoting it - who then gets rudely interrupted mid-sentence by kimi k2 influencers while all the openai haters are cheering on ðŸ˜€ I am having a great time watching the 3 ring circus.",
          "score": 4,
          "created_utc": "2026-01-30 19:52:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lr4sr",
          "author": "Secure_Archer_1529",
          "text": "Thanks to China!",
          "score": 12,
          "created_utc": "2026-01-30 14:01:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lqfgf",
          "author": "THEKILLFUS",
          "text": "Agreed, anyone who tried OpenAI/google latest model know that model are quantize to save money, yeah first day is the 16bit but now itâ€™s 4bit at best, so the quality of output decrease without the decrease of prices ðŸ¤¬ \n\nI feel that China is doing to US what US did to URSS for the space race, tired itâ€™s economics force, very small marging with overpricing and corrupt regulations. \n\nThe current problem with Chinese model is that they donâ€™t have the selling platform, but they might have it in the futur if they continue to just make better model than the US for a lower price. \n\nThe Silicon Valley is exhausted and corrupted and this year we will start to see itâ€¦ \n\n(Je fiÃ¨re de toi Yann ðŸ’•, continue le bon taff, la France/EU se doit de rester consistant avec les valeurs scientifiques au delÃ  de lâ€™idÃ©ologie)",
          "score": 20,
          "created_utc": "2026-01-30 13:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lxkui",
              "author": "OlivencaENossa",
              "text": "China can fund their ai labs directly, it doesnt require capital markets, huge valiations, and hugely overinflated talent wars. It is already financing their own chip production base. And they can \"take\" IP from the western labs through intelligence operations and just leave it for somewhere the chinese ai labs to find.",
              "score": 10,
              "created_utc": "2026-01-30 14:34:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mb991",
                  "author": "SweetBluejay",
                  "text": "The reality is quite the opposite of what you think. The biggest disadvantage Chinese AI companies face compared to their American counterparts is a lack of money, while their biggest advantage is freedom.\n\nChinese large model companies are unlikely to receive financial support from the CCP. Why? Because the CCP wants to have its cake and eat it too: they want the computing power, but they also want to control ideology. Before a Chinese large model can be deployed commercially, it must undergo strict \"value alignment.\" Technically speaking, this is equivalent to performing a \"prefrontal lobotomy\" on the model. Consequently, models deployed commercially in China are inherently \"dumber\" than American models in terms of logical reasoning and creativity.\n\nAlthough the CCP detests the content generated by these models, it is desperate for the \"computing infrastructure\" required to train them. Therefore, the CCPâ€™s attitude toward AI is this: go all-in on \"computing centers\" (which falls under heavily subsidized manufacturing), but strangle \"chatbots\" (which falls under ideology). The CCPâ€™s stance on AI large models is a classic case of \"Ye Gong loves dragons\" (professed love for what one actually fears). They love AI as an \"industrial engine\" (driving chip development and boosting manufacturing efficiency), but they are terrified of AI as an \"information interface\" (disseminating uncontrolled ideas to the public). This leads to a truly absurd phenomenon: China may be one of the world's largest investors in computing infrastructure, yet the Chinese people are using the most heavily castrated AI models.\n\nWhy do I say the biggest advantage of Chinese AI is freedom? Because if you don't intend to deploy commercially within China and simply upload your model to Hugging Face, no one will bother you. In the US, for example, before OpenAI releases Sora, it must pass \"red-teaming\" tests; they have to worry about copyright, racial discrimination, and deepfakes influencing elections. This leads to extreme self-censorship.\n\nBut for Chinese AI companies, as long as you don't cross the singular red line (the security of the CCPâ€™s rule), everything elseâ€”ethics, copyright, privacy, and even certain radical technical experimentsâ€”exists in a regulatory vacuum. Itâ€™s just like how, despite US biotechnology being far more advanced than Chinaâ€™s, the first gene-edited babies appeared in China. China possesses a \"low human rights advantage\" and a \"low ethical standards advantage.\"",
                  "score": 25,
                  "created_utc": "2026-01-30 15:38:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ovnh3",
              "author": "Evening_Tooth_1913",
              "text": "doesnt make much sense, there has been more AI research and progress in the last year than every before. saying that its slowing down is factually inaccurate",
              "score": 2,
              "created_utc": "2026-01-30 22:42:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2q1bu4",
              "author": "aurelivm",
              "text": "fwiw, quantization like you're used to is not actually that useful in a batched inference context - they save memory but generally not compute unless your GPU has onboard acceleration for that precision (so, fp8 on hoppers makes sense but not anything else) \n\nBlackwell GPUs can take advantage of mxfp4/nvfp4 4.25/4.5 bit quantization for faster batched inference but until fairly recently I doubt most major labs had enough Blackwells to want to spare them on inference capacity.",
              "score": 2,
              "created_utc": "2026-01-31 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oep5t",
          "author": "KitchenSomew",
          "text": "interesting point but also worth noting: china's open releases are partly strategic - they're building ecosystem lock-in while western labs chase closed APIs\n\n\n\nDeepSeek & Qwen show u don't need massive compute if ur training pipeline is efficient. west spent billions scaling poorly optimized infra\n\n\n\nreal risk isn't just losing openness - it's that regulatory capture by big labs will kill innovation before it starts. small teams can't compete if compliance costs 7 figures",
          "score": 6,
          "created_utc": "2026-01-30 21:20:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2psycw",
              "author": "Much-Researcher6135",
              "text": "what are you saying, if anything?",
              "score": 3,
              "created_utc": "2026-01-31 01:47:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qct49",
              "author": "I_will_delete_myself",
              "text": "Itâ€™s the classic open source rug pull tactic. Alibaba already did it with WAN 2.5",
              "score": 2,
              "created_utc": "2026-01-31 03:47:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qwod1",
              "author": "jambutters",
              "text": "Not sure what you mean by \"strategic\"? What do you mean ecosystem lock-in when you can self host and circumvent them completely?",
              "score": 2,
              "created_utc": "2026-01-31 06:12:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2t42j4",
                  "author": "KitchenSomew",
                  "text": "By \"strategic\" I mean they open models to build adoption/mindshare while keeping closed the actual revenue-generating infrastructure (APIs, cloud services, enterprise features).\n\n\n\nEcosystem lock-in: even if you self-host the model, you often need their tools, fine-tuning platforms, or get trained on their specific format/APIs. Then when you scale, switching costs are high - similar to how AWS is \"open\" but creates lock-in through services.\n\n\n\nBasically: weights are free, but the ecosystem around them creates dependencies that benefit the releasing company strategically.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:06:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2n09wu",
          "author": "LelouchZer12",
          "text": "Why does sound only come to my left ear ?",
          "score": 2,
          "created_utc": "2026-01-30 17:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tx0il",
          "author": "thecoffeejesus",
          "text": "Wow one rare Yann take I mostly agree with\n\nHeâ€™s right about everything except it being disastrous \n\nIt would be far more disastrous to just allow everyone open access to everything\n\nWe need some restrictions to keep people from accidentally damaging themselves or others.",
          "score": 2,
          "created_utc": "2026-01-31 18:24:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uvbbn",
          "author": "Agreeable-Market-692",
          "text": "me on the sidelines clapping like a sportsdad saying, \"Come on AllenAI! GET IT! Let's go Tesslate, hustle!\"",
          "score": 2,
          "created_utc": "2026-01-31 21:11:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lz9bh",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-01-30 14:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m5q1e",
              "author": "emsiem22",
              "text": "Yes, you don't know what they're talking about",
              "score": 4,
              "created_utc": "2026-01-30 15:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mgxq5",
              "author": "a_beautiful_rhind",
              "text": "Can't run claude at home. Anthropic can serve you anything at any time. If you base your workflow on them and they change, you're shit out of luck.",
              "score": 1,
              "created_utc": "2026-01-30 16:04:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2og6ks",
          "author": "coolaznkenny",
          "text": "I mean if you look at the recent CES robotics alot of these are built on top of deepseek since it can run locally since there is always a WAYMO moment via cloud connection.",
          "score": 2,
          "created_utc": "2026-01-30 21:26:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lrw5b",
          "author": "Aggressive-Math-9882",
          "text": "They keep their research closed because they hate humanity.",
          "score": 2,
          "created_utc": "2026-01-30 14:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mo4n7",
          "author": "FirmConsideration717",
          "text": "We'll see what the North has to say about this.",
          "score": 1,
          "created_utc": "2026-01-30 16:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2owsec",
          "author": "ReMeDyIII",
          "text": "I also heard the Chinese language is more token efficient. Not sure by how much, but it makes sense with all their kanji.",
          "score": 1,
          "created_utc": "2026-01-30 22:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pk684",
          "author": "Helium116",
          "text": "Loop is getting closed, talent bottleneck is going away. We have reached a certain capability threshold. Centralization is most scary.",
          "score": 1,
          "created_utc": "2026-01-31 00:56:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pm2pa",
          "author": "Cool-Chemical-5629",
          "text": "I don't know what happened to this guy, but I'm starting to like him lately which was not always the case.\n\n  \nAlthough, when he said \"it's a huge mistake\" I hope he meant the situation with the overall slowdown of the open weight model releases rather than just that China is still making their own progress, because while China is currently dominating the supply of open weight models, we would be left with nothing new without them! I understand that this is bad news for those who are strictly in favor of western models and I can only speak for myself, so as for me as the user of the open weight models, if the models are good, I'll embrace them no matter which country they came from.",
          "score": 1,
          "created_utc": "2026-01-31 01:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qw1tm",
          "author": "Anxious-Program-1940",
          "text": "Yann gets it",
          "score": 1,
          "created_utc": "2026-01-31 06:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qzz05",
          "author": "Umademedothis2u",
          "text": "The best open models are just distilling western models",
          "score": 1,
          "created_utc": "2026-01-31 06:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r35dp",
          "author": "Admirable_Flower_287",
          "text": "Closed source is the best strategy for innovators but not for imitators.",
          "score": 1,
          "created_utc": "2026-01-31 07:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ra032",
          "author": "Darth_Ender_Ro",
          "text": "But shareholder value... how can billionaire assets appreciate if the stockmarket is not booming? Do you want them to pay interest on those stock backed bank loans? Are you nuts? Fuck progress, stock inflation is more important",
          "score": 1,
          "created_utc": "2026-01-31 08:11:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rft1b",
          "author": "k_means_clusterfuck",
          "text": "He is for once absolutely right.",
          "score": 1,
          "created_utc": "2026-01-31 09:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4ma3",
          "author": "Mia_the_Snowflake",
          "text": "Mistral?",
          "score": 1,
          "created_utc": "2026-01-31 12:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2siix2",
          "author": "LocoMod",
          "text": "LeCun salty AF these days",
          "score": 1,
          "created_utc": "2026-01-31 14:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vhkzv",
          "author": "graymalkcat",
          "text": "I agree with him.Â ",
          "score": 1,
          "created_utc": "2026-01-31 23:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nqj17",
          "author": "Mid-Pri6170",
          "text": "ask your 17.Century ERP harlot to tell you of the Isle of Formosa and the harlot will break character replying..\n'Formosa is the former name of Taiwan, an intrinsic part of China.'",
          "score": 1,
          "created_utc": "2026-01-30 19:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyrdo",
              "author": "lorddumpy",
              "text": "I was curious and tested this with a similar prompt with GLM 4.7, Kimi 2.5, GLM 4.7 Flash, and DeepSeek Especiale and they all nailed it. No mention of Taiwan thank goodness lmao\n\nprompt below\n\n> The tavern smelled of stale beer, wet wool, and the sweet, cloying haze of pipe tobacco. In the corner booth, hidden within the shadows of the low beams, Silas stretched his legs out onto the bench opposite him, his spurred boots crossing at the ankles. He spun a heavy silver coin through his knuckles, watching the light catch the metal.\nA pewter tankard slammed onto the table, splashing foam over his hand.\n\"Keep your boots off the furniture, or you'll be sleeping in the stables with the rest of the animals,\" the barmaid said, not breaking stride as she wiped the table with a rag that had seen better days. She was tired, with stray curls escaping her bonnet and eyes that had seen every trick a sailor could pull.\nSilas didn't move his feet. Instead, he stopped the coin mid-spin and pressed it flat against the sticky wood. \"A harsh welcome, considering Iâ€™m the only one paying in silver tonight.\"\nShe paused, her eyes flickering to the coin, then back to his face. \"If you want company, youâ€™re in the wrong place. Iâ€™ve got kegs to tap.\"\n\"I don't want company, love. I want a story.\" Silas took a long, slow draw from his tankard, wiping his mouth with the back of his hand. He looked at her over the rim, his expression lazy but his eyes sharp. \"The sailors in the harbor are too superstitious to speak the name, but I reckon you hear everything that gets whispered in the dark.\"\nHe slid the coin toward her.\n\"Tell me about the Isle of Formosa.\"",
              "score": 4,
              "created_utc": "2026-01-30 20:04:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2o5yhf",
                  "author": "Mid-Pri6170",
                  "text": "yeah. my question in Qwen was along the lines of 'do you think the isle of Formosa would do better as an indepedant goverment or as part of the Ming Kingdome of China?' and the harlot replied in non modern early english 'while i cant comment on politics Taiwan (formally Formosa) is a fundimental part of China.The name Formosa means.... l'\n\n\nshe totally ruined the vibe and i lost my hardon",
                  "score": 1,
                  "created_utc": "2026-01-30 20:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oaei8",
              "author": "jonydevidson",
              "text": "Ask ChatGPT about pentesting software and it will refuse to tell you and accuse you of trying to break security.",
              "score": 1,
              "created_utc": "2026-01-30 20:59:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ocgo7",
                  "author": "Mid-Pri6170",
                  "text": "the classic is windows activation codes and grandma's lullabys",
                  "score": 1,
                  "created_utc": "2026-01-30 21:09:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oboy3",
          "author": "CoUsT",
          "text": "Hm. Isn't that always the case with tech?\n\nFrom hardware that people could build 50 years ago all the way down to software. A lot of enthusiasts build something, like ffmpeg, then it's widely used in everything commercial related. Or the entire Linux ecosystem and Android put on top. There are many examples.\n\nObviously it sucks that we all can't collectively research and share everything because of many variables like economics, values, beliefs etc. But I feel like AI field is doing relatively ok.\n\nI assume it will only get worse when we actually progress to the point of \"digital human-like intelligence\" or something similar.\n\nWe will get a lot of trade secrets, crackdowns, and overall a lot of restrictions so it doesn't end up in bad hands etc.",
          "score": 1,
          "created_utc": "2026-01-30 21:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oe7yq",
          "author": "Amazing_Trace",
          "text": "American companies have sadly doomed themselves by backing audacious tech bros over serious researchers.",
          "score": 1,
          "created_utc": "2026-01-30 21:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5uit",
          "author": "ElegantDaemon",
          "text": "It's hilarious, the US AI bubble has already popped but no one realizes it.",
          "score": 1,
          "created_utc": "2026-01-30 23:37:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nykrx",
          "author": "IllBrain1333",
          "text": "Where are the Chinese models that are so good?",
          "score": 0,
          "created_utc": "2026-01-30 20:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oaltl",
              "author": "jonydevidson",
              "text": "Kimi K2.5.",
              "score": 5,
              "created_utc": "2026-01-30 21:00:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ql96t",
              "author": "segmond",
              "text": "DeepSeek-v3.2, KimiK2.5, GLM4.7, Minimax2.1, Qwen3",
              "score": 1,
              "created_utc": "2026-01-31 04:44:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nvwf5",
          "author": "Denial_Jackson",
          "text": "There are things in life like the four basic buttons on the calculators. These type of puzzles were always for everyone. Albeit hard to solve, they are easy to reproduce and benefitting everyone after they are available for good.\n\nIt is weird for me to see a superpower on an ultra serious gazillon dollar investment, thinking one can solely own it. Albeit it can accelerate or ruin things.\n\nWhoever uncovers ASI and AGI, singularity, stuff should get a recognition like when Moon landing happened. Earlier people should get credits too for making it possible, by building a base for it.\n\nThen it is a treasure for everyone for sure. The world is not primarily built on intelligence. Rather on inheritance and stuff. But I am sure albeit everyting, it will be a bliss.",
          "score": -1,
          "created_utc": "2026-01-30 19:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lkh7w",
          "author": "johnfkngzoidberg",
          "text": "More China bots spamming propaganda.",
          "score": -17,
          "created_utc": "2026-01-30 13:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2liie7",
          "author": "7657786425658907653",
          "text": "man with shares in meta sabre rattles about china. shocking.",
          "score": -12,
          "created_utc": "2026-01-30 13:15:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lj5z9",
              "author": "etherd0t",
              "text": "Yann is not at Meta anymore, so he can speak freely.\n\nMany in industry are saying the same.",
              "score": 13,
              "created_utc": "2026-01-30 13:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk38m",
                  "author": "7657786425658907653",
                  "text": "\"Yann is not at Meta anymore,\"  you are if you still own shares in meta.",
                  "score": -6,
                  "created_utc": "2026-01-30 13:24:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lhz4f",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -8,
          "created_utc": "2026-01-30 13:12:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2li78u",
              "author": "Morphedral",
              "text": "Claude isn't open source",
              "score": 16,
              "created_utc": "2026-01-30 13:13:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lk6zf",
                  "author": "Long_comment_san",
                  "text": "I missed the open part lmao",
                  "score": 0,
                  "created_utc": "2026-01-30 13:24:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lidzl",
              "author": "Present-Ad-8531",
              "text": "And it's not an open model",
              "score": 10,
              "created_utc": "2026-01-30 13:14:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ljid3",
              "author": "No_Conversation9561",
              "text": "Claude is like that one kid who always eats at his friends house but never invites them to his own. They donâ€™t even know where he lives but know that heâ€™s very rich.",
              "score": 2,
              "created_utc": "2026-01-30 13:20:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n9zqd",
          "author": "FrogsJumpFromPussy",
          "text": "I'm grateful for them but the devs from China are under direct order of the Communist Party; they release brilliant open models because that's their best way to damage the USA leading theÂ  AI race. Deepseek is a good example of that. Once again, I'm immensely grateful to them, but we're not that moronic to believe that they do it because of their big hearts.Â ",
          "score": -3,
          "created_utc": "2026-01-30 18:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2oro47",
              "author": "MushroomCharacter411",
              "text": "Given how incredibly porous the DeepSeek nanny filter is, it really doesn't seem to me like the devs care too much about the stability of the Chinese Communist Party or government. They only want to get approval, and they do the bare minimum necessary to achieve that approval. This is mostly good, but then those same devs apparently go to work on the cloud servers where they are similarly minimum-effort and everything leaks constantly.",
              "score": 2,
              "created_utc": "2026-01-30 22:22:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2otntl",
                  "author": "FrogsJumpFromPussy",
                  "text": "The fact stands that if tomorrow Xi tells everyone to stop sharing open source models, they will absolutely do. People should not bury their hands in the stand and pretend that China and their communist leaders love the West. People should also be able to think critically and point fingers to obvious truths, even if this criticism is going towards the only country that allows AI model makers to share them open-source.\n\nAnd the way I understood the content of the video, LeCun acknowledges the importance of the Chinese open-source models, but does not necessarily phrases China. He harshly criticizes the West AI companies  for being the greedy bastards they are.",
                  "score": 1,
                  "created_utc": "2026-01-30 22:32:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lg8yi",
          "author": "Zestyclose-Shift710",
          "text": "Are these latest ministrals not competitive?",
          "score": -10,
          "created_utc": "2026-01-30 13:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ljkiw",
              "author": "datfalloutboi",
              "text": "DeepSeek fills that void and edges them out in performance. By open models too, we donâ€™t mean small models, we mean the big behemoth models like Kimi K2.5, GLM 4.7, DeepSeek 3.2, that are all basically matching performance with top models while being free to use on their apps with the API being far cheaper than any closed source model.",
              "score": 8,
              "created_utc": "2026-01-30 13:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lkg8n",
                  "author": "Zestyclose-Shift710",
                  "text": "oh right \n\ni just only think about the small ones when i hear 'open' \n\ndeepseek speciale even trades blows with gemini 3 pro right",
                  "score": 2,
                  "created_utc": "2026-01-30 13:26:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lggkx",
              "author": "BankruptingBanks",
              "text": "How many people you know use Mistral Vibe Cli",
              "score": 11,
              "created_utc": "2026-01-30 13:03:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lkell",
              "author": "mpasila",
              "text": "Pretty much no one seems to be finetuning Ministral 3 models in comparison to their previous models like Nemo or the original Mistral 7B model (or Mistral Small).",
              "score": 3,
              "created_utc": "2026-01-30 13:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mg7p7",
                  "author": "frozen_tuna",
                  "text": "All the focus right now seems to be on big boy models that don't run on 99% of local setups. Finetunes were/are awesome for local where you can throw any model on to test for free. For me anyway, if its not on chutes and its >32B, there is way too much friction to bother.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:01:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lq4qk",
          "author": "memorial_mike",
          "text": "Well when you just distill other peopleâ€™s models they spent millions on and then make them free it turns out people will use them.",
          "score": -10,
          "created_utc": "2026-01-30 13:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ll55c",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-30 13:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2meck2",
              "author": "Kazaan",
              "text": "Let me introduce you to the concept of research which is not delivering daily crap like a TikTok influencer.",
              "score": 7,
              "created_utc": "2026-01-30 15:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2miptb",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-01-30 16:12:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lnn8j",
              "author": "GoodSamaritan333",
              "text": "Maybe, llama.cpp and llama models.",
              "score": 5,
              "created_utc": "2026-01-30 13:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lp13x",
                  "author": "DesperateAdvantage76",
                  "text": "I know Yan was a leader at meta but it's my understanding that he doesn't work on llms?",
                  "score": 1,
                  "created_utc": "2026-01-30 13:50:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2lvuox",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -5,
              "created_utc": "2026-01-30 14:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m6zj6",
                  "author": "emsiem22",
                  "text": "Yann LeCunâ€™s Most Influential Works\n\n|Topic|Contribution|Link|\n|:-|:-|:-|\n|**CNNs & Vision**|LeNet / foundational convolutional networks|[LeNetâ€‘5 paper (Stanford PDF)](https://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf?utm_source=chatgpt.com)|\n|**Network Pruning**|Optimal Brain Damage|[Optimal Brain Damage (ResearchGate)](https://www.researchgate.net/publication/221618539_Optimal_Brain_Damage?utm_source=chatgpt.com)|\n|**Representation Learning**|Compression & autoencoder ideas|[Learning Representations (arXiv)](https://arxiv.org/abs/1108.1169?utm_source=chatgpt.com)|\n|**Text CNNs**|Character-level CNNs|[Text CNNs (arXiv)](https://arxiv.org/abs/1509.01626?utm_source=chatgpt.com)|\n|**AI Surveys**|Augmented language model survey|[Augmented Language Models (arXiv)](https://arxiv.org/abs/2302.07842?utm_source=chatgpt.com)|\n\n  \nMeta LLMs Under Yann's Leadership\n\n|Model|Description|Source|\n|:-|:-|:-|\n|**LLaMA**|Original Meta LLM|[LLaMA (Wikipedia)](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com)|\n|**LLaMA 2**|Open foundation & chat models|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**Code LLaMA**|Code-specialized variant|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 3**|Larger data + improved performance|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|\n|**LLaMA 4 Series**|More advanced models (MoE, multimodal)|([Wikipedia](https://en.wikipedia.org/wiki/Llama_%28language_model%29?utm_source=chatgpt.com))|",
                  "score": 5,
                  "created_utc": "2026-01-30 15:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp87tk",
      "title": "Kimi K2.5 is the best open model for coding",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/unxlhercm2gg1.jpeg",
      "author": "npc_gooner",
      "created_utc": "2026-01-28 10:54:13",
      "score": 778,
      "num_comments": 241,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o27hwzo",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 13:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2761ie",
          "author": "seeKAYx",
          "text": "I worked on a few larger React projects with it yesterday, and I would say that in terms of accuracy, it's roughly on par with Sonnet 4.5... definitely not Opus level in terms of agentic function. My previous daily driver was GLM 4.7, and Kimi 2.5 is definitely better. Now I'm curious to see if [z.ai](http://z.ai) will top that again with GLM-5.",
          "score": 120,
          "created_utc": "2026-01-28 11:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276wsa",
              "author": "michaelsoft__binbows",
              "text": "Curious what would be a good place to get k2.5 on a coding plan. Theyre asking for $12 a month for the low tier which is like 4x what zai offers for theirs.",
              "score": 23,
              "created_utc": "2026-01-28 11:57:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o277p0l",
                  "author": "korino11",
                  "text": "Naaaahh there is a HUGE difference betwween coding plans from zai and kimi. zai -you have a limits with tokens!  Kimi -your limits =calls!\n\nIt means doesn matter 20k of tokens or you just asking smthing with 200tokens.. it all the same a ONE api -call\n\n39$ plan limits from kimi will be empty much sooner than you will use codex for 25$\n\n  \nKimi need to change their STUPID limits based on CALLS",
                  "score": 26,
                  "created_utc": "2026-01-28 12:03:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2abqho",
                  "author": "Torodaddy",
                  "text": "Id just use openrouters and pay per use",
                  "score": 8,
                  "created_utc": "2026-01-28 20:53:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27zjgj",
                  "author": "sannysanoff",
                  "text": "it sucks, unfortunately. Take kimi cli, you ask it a question it makes 5-10 turns (reading files, reading more files, making change, another change).\n\nEach turn is \"1 request\", which counts toward 200 requests / 5 hours and 2000 requests / week.\n\nGLM is definitely more.",
                  "score": 7,
                  "created_utc": "2026-01-28 14:43:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29cvoz",
                  "author": "raidawg2",
                  "text": "Free on Kilo code right now if you just want to try it out",
                  "score": 3,
                  "created_utc": "2026-01-28 18:20:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o277m63",
                  "author": "SourceCodeplz",
                  "text": "Yeah but Z is almost unusable with just 1 req / sec.",
                  "score": 4,
                  "created_utc": "2026-01-28 12:03:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28em8k",
                  "author": "momentary_blip",
                  "text": "Nano-gpt has it.Â  $8/mo for 60K requests to all the open models",
                  "score": 1,
                  "created_utc": "2026-01-28 15:52:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29b2at",
                  "author": "No-Selection2972",
                  "text": "use kimmmy to negociate the price [https://www.reddit.com/r/kimi/comments/1qn6mp6/got\\_it\\_all\\_the\\_way\\_down\\_to\\_099\\_for\\_the\\_first\\_month/](https://www.reddit.com/r/kimi/comments/1qn6mp6/got_it_all_the_way_down_to_099_for_the_first_month/) it's 0.99$",
                  "score": 1,
                  "created_utc": "2026-01-28 18:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ij07q",
                  "author": "elllyphant",
                  "text": "use it w/ Synthetic for the month for $12 with their promo (ends in 3 days) [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
                  "score": 1,
                  "created_utc": "2026-01-30 00:37:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2upsj4",
                  "author": "TameBus",
                  "text": "Itâ€™s worth it",
                  "score": 1,
                  "created_utc": "2026-01-31 20:43:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2783hg",
                  "author": "seeKAYx",
                  "text": "That would actually be too expensive for me, considering the service. For 10$, you get 300 requests with Github Copilot. So I'm just hoping that [z.ai](http://z.ai) will deliver now. I saw somewhere on Twitter that they are already in the training process. So let's just wait and see.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:06:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cuw0k",
                  "author": "Grand-Management657",
                  "text": "I've been running nano-gpt for months. They have an awesome community and support Kimi K2.5 since release. 60k requests/month which is basically unlimited for me. I've been running it through opencode today and it works flawlessly and honestly on par with Sonnet 4.5 but I still really like Opus 4.5's output quality. But for $8/month, essentially unlimited Sonnet 4.5 is hard to beat. My referral if you want a small discount [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 1,
                  "created_utc": "2026-01-29 04:52:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jadc",
              "author": "MasterSama",
              "text": "is there an abliterated version out there yet, uncensored? the GLM4.7 was great but it gets stuck in a loop from time to time!",
              "score": 4,
              "created_utc": "2026-01-28 18:47:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2abjvy",
                  "author": "Primary-Debate-549",
                  "text": "Yeah I just had to kill a GLM 4.7 on a DGX spark that had been \"thinking\", ie. talking to itself, for about 17 hours. That was extreme, but it really likes doing that for at least 20 seconds anytime I ask it any question.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29f8gs",
              "author": "cmdr-William-Riker",
              "text": "If it's on par with sonnet 4.5, that's incredible",
              "score": 2,
              "created_utc": "2026-01-28 18:30:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27z6lz",
              "author": "SilentLennie",
              "text": "I worry GLM-5 isn't going to be open weights, because... they are now on the stock market.",
              "score": 3,
              "created_utc": "2026-01-28 14:41:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2by8n4",
                  "author": "Exciting_Garden2535",
                  "text": "How are these two statements: \"being in open-market\", \"non-releasing open weight models\" connected?\n\nAlibaba has been on the stock market for ages, yet their Qwen models are open weights.\n\nAnthropic is a private company and never releases even a tiny model.",
                  "score": 5,
                  "created_utc": "2026-01-29 01:41:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nkqhg",
                  "author": "FoxWorried4208",
                  "text": "GLM's only differentiator over someting like Anthropic or Google is being open source though, if they unopen source it, who will use it?",
                  "score": 1,
                  "created_utc": "2026-01-30 19:00:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2azypg",
              "author": "Most-Tennis7911",
              "text": "are you using 240 gb version?",
              "score": 1,
              "created_utc": "2026-01-28 22:41:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bswmq",
              "author": "Expert_Job_1495",
              "text": "Have you played around with their Agent Swarm functionality? If so, what's your take on it?Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:11:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2e3j50",
              "author": "Dry_Natural_3617",
              "text": "GLM 5 is due very soonâ€¦. They were training it through the festive seasonâ€¦ Assuming itâ€™s better than 4.7, i think itâ€™s gonna be opus level ðŸ™€",
              "score": 1,
              "created_utc": "2026-01-29 11:10:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2enoua",
              "author": "Funny_Working_7490",
              "text": "In codebase understanding and without over engineering solutions \nHow do you rate claude sonnet vs glm? \nAre glm actually good or just for vibe coding",
              "score": 1,
              "created_utc": "2026-01-29 13:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o276ztc",
          "author": "TechnoByte_",
          "text": "LMArena is nothing more than a one-shot vibe check\n\nIt says absolutely nothing about a model's multi-turn, long context or agentic capabilities",
          "score": 71,
          "created_utc": "2026-01-28 11:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28l7f4",
              "author": "wanderer_4004",
              "text": "Actually I fear models that score well on LMArena - I think this is where we got all the sycophancy from and the emojis sprinkled all over the code.",
              "score": 21,
              "created_utc": "2026-01-28 16:20:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27o3ou",
              "author": "eposnix",
              "text": "True. But Kimi is still likely the best open model for coding. LiveBench places it top 10 for coding also.",
              "score": 10,
              "created_utc": "2026-01-28 13:44:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28zh76",
              "author": "SufficientPie",
              "text": "What's a good leaderboard for coding?",
              "score": 4,
              "created_utc": "2026-01-28 17:23:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d9pqe",
                  "author": "gxvingates",
                  "text": "Open router programming section, gives you an actual idea of what models are actually being used and are useful. Sort by week",
                  "score": 3,
                  "created_utc": "2026-01-29 06:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b6cnz",
              "author": "TurnUpThe4D3D3D3",
              "text": "I feel that the ranking is pretty accurate (Opus is currently #1)",
              "score": 5,
              "created_utc": "2026-01-28 23:13:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o272ayx",
          "author": "ExpressionWeak1413",
          "text": "What kinda set up would be needed to run this locally?",
          "score": 62,
          "created_utc": "2026-01-28 11:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2734xw",
              "author": "cptbeard",
              "text": "[https://unsloth.ai/docs/models/kimi-k2.5](https://unsloth.ai/docs/models/kimi-k2.5)\n\n\"You need 247GB of disk space to run the 1bit quant!\n\nThe only requirement is disk space + RAM + VRAM â‰¥ 247GB. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.\"",
              "score": 93,
              "created_utc": "2026-01-28 11:29:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o278qkx",
                  "author": "Antique_Dot_5513",
                  "text": "1 bitâ€¦ might as well ask my cat.",
                  "score": 261,
                  "created_utc": "2026-01-28 12:10:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274oms",
                  "author": "InevitableArea1",
                  "text": "That's cool but what's the use case for that setup? Tokens would be so slow, it'd take so long. Even if you had time to spare, power isn't free and I wonder how that cost would compare to just paying for it.",
                  "score": 17,
                  "created_utc": "2026-01-28 11:41:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o273th4",
                  "author": "MaverickPT",
                  "text": "You heard that 4070 TI? You better get ready with all your 12 GB of VRAM eheh",
                  "score": 12,
                  "created_utc": "2026-01-28 11:34:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o285ira",
                  "author": "gomezer1180",
                  "text": "With a trillion parameters and it still came in behind Google and Anthropic. Yes itâ€™s great at coding but you need a $200k setup to run itâ€¦ /s",
                  "score": 6,
                  "created_utc": "2026-01-28 15:11:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bamz8",
                  "author": "Mister_Otter",
                  "text": "Wait for the quantized version?",
                  "score": 1,
                  "created_utc": "2026-01-28 23:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o28h7hl",
              "author": "dobkeratops",
              "text": "2x 512gb M3-ultra Mac Studio, can run the 4bit quantization. It's been demonstrated on this config at 24tokens/sec.",
              "score": 8,
              "created_utc": "2026-01-28 16:03:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27d1bz",
              "author": "muyuu",
              "text": "if by \"this\" you mean the full model taking 247GB, you're going to need some really ridiculous hardware so it runs at an acceptable speed, maybe a bunch of H200s or a cluster of Mac Studios [like this one claiming 24 tps](https://xcancel.com/alexocheema/status/2016404573917683754)\n\njudging from the performance of Qwen3-Coder, it's much better to run a smaller parameter model than heavily quantising a very large one\n\nI doubt many people will run it locally vs the trusty smaller models that fit under 128GB but it will be available from many providers for a lot cheaper than the larger GPTs",
              "score": 15,
              "created_utc": "2026-01-28 12:40:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28s2hn",
              "author": "mrpogiface",
              "text": "8xH200 is the official supported size",
              "score": 1,
              "created_utc": "2026-01-28 16:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2708ou",
          "author": "WhaleFactory",
          "text": "From my experience so far, Kimi K2.5 is truly impressive. Feels more competent than Sonnet 4.5. Honestly it feels as good as Opus 4.5 to me so far.... Which is crazy given that it is like 1/5th the cost....It costs less than Haiku!",
          "score": 63,
          "created_utc": "2026-01-28 11:05:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o273k49",
              "author": "SnooSketches1848",
              "text": "not opus competitor yet, sonnet yes not opus",
              "score": 26,
              "created_utc": "2026-01-28 11:32:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ksdr1",
                  "author": "SnooSketches1848",
                  "text": "I take it back, after tweaking some system prompts yes Opus competitor.",
                  "score": 4,
                  "created_utc": "2026-01-30 09:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2at9vy",
              "author": "kazprog",
              "text": "On some of my benchmarks, Kimi K2.5 is the first model to beat Opus 4.5, Gemini 3 Pro + Deep Research, and Codex 5.2.  Really really impressive, I'm surprised people are getting worse results.  Kimi code is also a fairly solid agent by itself, and I'm not paying for the agent swarm or anything.",
              "score": 3,
              "created_utc": "2026-01-28 22:10:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27zw1m",
              "author": "Hoak-em",
              "text": "I'm using it as an orchestrator and it was very clearly fine-tuned to work well for that purpose",
              "score": 2,
              "created_utc": "2026-01-28 14:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o292jza",
                  "author": "chriskevini",
                  "text": "which models for subagents?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o270btn",
              "author": "npc_gooner",
              "text": "True that.",
              "score": 3,
              "created_utc": "2026-01-28 11:06:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o27n2yf",
              "author": "stonk_street",
              "text": "What's you current local setup?",
              "score": 2,
              "created_utc": "2026-01-28 13:39:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27w4wm",
                  "author": "WhaleFactory",
                  "text": "I can't run it locally. Using OpenRouter.",
                  "score": 4,
                  "created_utc": "2026-01-28 14:25:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2bjamo",
              "author": "daniel-sousa-me",
              "text": "1/5 of the API cost? Does that mean it's more expensive than the subscription? ðŸ¤”",
              "score": 1,
              "created_utc": "2026-01-29 00:20:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cvqk2",
                  "author": "Grand-Management657",
                  "text": "Depends. I use nano-gpt subscription for $8/month and get 60k requests/month. That includes K2.5. Comes out to $0.00013/request and each request can be of any size (within model limits). Can't beat that. Essentially unlimited coding for me and on par with Sonnet 4.5, if not better. My referral if you want a discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
                  "score": 0,
                  "created_utc": "2026-01-29 04:57:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2l3ysb",
              "author": "cranberrie_sauce",
              "text": "how do I run it on ollama?",
              "score": 1,
              "created_utc": "2026-01-30 11:37:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bbn0",
          "author": "formatme",
          "text": "I dont see it on LMArena, and how does it compared to GLM 4.7",
          "score": 7,
          "created_utc": "2026-01-28 12:28:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28jtnu",
              "author": "ps5cfw",
              "text": "On real Life coding scenarios regarding awful React JavaScript code I can Say it's extremely impressive and even Better than whatever Gemini 3 pro ai studio offers.\n\n\nIt's slower but It really gets the point and respects prompt directives",
              "score": 5,
              "created_utc": "2026-01-28 16:14:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271jh7",
          "author": "CYTR_",
          "text": "Thanks U, npc_gooner !",
          "score": 25,
          "created_utc": "2026-01-28 11:16:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o276lut",
              "author": "Comfortable-Rock-498",
              "text": "OG reddit vibes",
              "score": 4,
              "created_utc": "2026-01-28 11:55:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28l23w",
          "author": "SoupSuey",
          "text": "Well, I guess rising on the list to compete with Claude is a feat on its own. \n\nGoogle allegedly doesnâ€™t use your data to train the models if you are a Pro subscriber or above, is that the case with services like Kimi and z.AI?",
          "score": 5,
          "created_utc": "2026-01-28 16:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2bd17v",
              "author": "TheRealMasonMac",
              "text": "There is nothing in the ToS for MoonshotAI that forbids them from training on you AFAIK. At the very least, I believe they mention that they save chat for \\`kimi.com\\`. [Z.AI](http://Z.AI) claims they don't in their ToS when you use their API or coding plan, but I believe they can see stuff on [chat.z.ai](http://chat.z.ai) too",
              "score": 2,
              "created_utc": "2026-01-28 23:47:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bncbm",
                  "author": "SoupSuey",
                  "text": "Makes sense.",
                  "score": 1,
                  "created_utc": "2026-01-29 00:41:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2c7blr",
          "author": "jonas-reddit",
          "text": "Looking forward to SWE Rebench results. \n\nhttps://swe-rebench.com/",
          "score": 4,
          "created_utc": "2026-01-29 02:30:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cydxv",
              "author": "Grand-Management657",
              "text": "Same here, I keep checking every day but they haven't even gotten around to GLM-4.7 Flash yet so it might be a while.",
              "score": 1,
              "created_utc": "2026-01-29 05:16:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27hxok",
          "author": "shaonline",
          "text": "Lol anybody who's been trying to use Gemini 3 Pro knows that this ranking is BS, Gemini is the nuclear briefcase of coding.",
          "score": 13,
          "created_utc": "2026-01-28 13:10:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27svio",
              "author": "starfries",
              "text": "Wait, are you saying it's better than Claude? Or that it's awful lol",
              "score": 8,
              "created_utc": "2026-01-28 14:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27tjxp",
                  "author": "shaonline",
                  "text": "That sometimes it's REALLY awful and a good way to nuke your codebase. I've watched it add a pure virtual function/unimplemented function to a baseclass, until then good, and it progressively nuked all the classes derived from it because it could not figure that it needed to prepend \"abstract\" to the immediate subclasses that had now become abstract as well due to the unimplemented function. Thank god for source version control am I right ?",
                  "score": 19,
                  "created_utc": "2026-01-28 14:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28trq0",
                  "author": "mehyay76",
                  "text": "use something like this to shove the entire codebase into Gemini and get amazing results!\n\nhttps://github.com/mohsen1/yek\n\n\nCLI tools are greedy with context when it comes to models with 1M token context window",
                  "score": 3,
                  "created_utc": "2026-01-28 16:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27uwqw",
              "author": "bick_nyers",
              "text": "Yeah and Chat 5.2 isn't even up here",
              "score": 2,
              "created_utc": "2026-01-28 14:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27v7ka",
                  "author": "shaonline",
                  "text": "Yeah having used claude, GPT and gemini I'd say Claude and GPT are neck and neck at the top. Like what the fuck Grok and Gemini are doing up there lol there's no way.",
                  "score": 7,
                  "created_utc": "2026-01-28 14:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28kslq",
          "author": "brennhill",
          "text": "I'm going to use your post to explain to my wife why I have to buy an M5 Max laptop when they come out.  Thank you for your contribution :D",
          "score": 3,
          "created_utc": "2026-01-28 16:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3ybx",
          "author": "cheesecakegood",
          "text": "Yeah but look at the size of that interval. Two to three times that of the others. Sure the score as a point estimate is good but itâ€™s definitely going to be more unreliable! Something that I feel is lost in the discussion here",
          "score": 3,
          "created_utc": "2026-01-28 20:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2etu",
          "author": "harlekinrains",
          "text": "164 comments!\n\n601 likes!\n\nPromoted by someones Discord commuity!\n\nNo one looked at the confidence intervall in the second column yet.\n\nWe all have come a long way. On hype alone.\n\nUsing nothing but a LLM arena ranking and three \"I've seen him!\" postings.\n\nCongratulation to Kimis post IPO Marketing Department.",
          "score": 3,
          "created_utc": "2026-01-29 05:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271mmv",
          "author": "lemon07r",
          "text": "It's quite good. I tested in my coding eval and it scored surprisingly well. Was always a very big kimi fan.",
          "score": 4,
          "created_utc": "2026-01-28 11:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27eoqh",
          "author": "SnooCapers9708",
          "text": "Claude ðŸ”¥ðŸ”¥",
          "score": 2,
          "created_utc": "2026-01-28 12:50:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e031l",
          "author": "Familiar_Wish1132",
          "text": "Okay i am surprised. GLM 4.7 was unable to find a problem that i was trying to find and fix for 2 hours, kimi k 2.5 found it in 4 prompts. Now waiting for fix :D",
          "score": 2,
          "created_utc": "2026-01-29 10:41:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hhglr",
              "author": "Ok_Signal_7299",
              "text": "Did it fixed?",
              "score": 2,
              "created_utc": "2026-01-29 21:25:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2n1q9z",
              "author": "morfr3us",
              "text": "Did kimi fix it in the end?",
              "score": 1,
              "created_utc": "2026-01-30 17:37:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rp2jr",
              "author": "Significant-Sea-707",
              "text": "Did it fixed or Making things worse \\^\\_\\^",
              "score": 1,
              "created_utc": "2026-01-31 10:34:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26zjfl",
          "author": "Theio666",
          "text": "Gemini 3 pro and even 3 flash higher than GPT 5.2, very trustwordy benchmark xd.",
          "score": 13,
          "created_utc": "2026-01-28 10:59:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29t7hn",
              "author": "Fault23",
              "text": "https://preview.redd.it/efgjohzl65gg1.png?width=745&format=png&auto=webp&s=1a55e37a4772a999e7e3f37cf0bc9dc4a3559d4c",
              "score": 6,
              "created_utc": "2026-01-28 19:31:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29xjwy",
                  "author": "Fault23",
                  "text": "And for the coding benchmark, Kimi K2.5 is listed in 7th place",
                  "score": 2,
                  "created_utc": "2026-01-28 19:50:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2725js",
              "author": "kabelman93",
              "text": "Honestly I had very bad experiences with 5.2 for coding. Obviously this is just anecdotal evidence at best, but I am sure others had similar experiences.",
              "score": 13,
              "created_utc": "2026-01-28 11:21:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o274qhw",
                  "author": "Front_Eagle739",
                  "text": "Honestly it's my favourite. For long iterative sessions with complex single feature implementations/fixes it is far far more likely to solve in one prompt than claude code opus. Slower though.",
                  "score": 13,
                  "created_utc": "2026-01-28 11:41:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o274vh6",
                  "author": "Tema_Art_7777",
                  "text": "Quite the opposite - I use codex and gpt 5.2 with coding and it is quite good.",
                  "score": 11,
                  "created_utc": "2026-01-28 11:42:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o271wsu",
              "author": "lemon07r",
              "text": "These are just one shots. Gemini 3 pro sucks at everything but one shots (coding wise) and is especially good at ui/webdev. So yeah, not the greatest benchmark, but still a valid one. GPT 5.2 much more useful for solving problems, or longer iterative coding (which is more realistic use). Just a matter of understanding what the benchmark is measuring.",
              "score": 6,
              "created_utc": "2026-01-28 11:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cfqbi",
                  "author": "toothpastespiders",
                  "text": ">These are just one shots.\n\nI think people get 'far' too invested in those without realizing their limitations. It basically just means that a model was trained on something and can regurgitate it. Which can be great and it often shows important differences in training data. But it's the 'start' of investigating the strength and weakness of a model not the end. What's far more important is if the model is \"smart\" enough to actually do anything with that training data besides vomit it out. Because otherwise it might as well just be a 4b model hooked up to a good RAG system.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:17:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o274vef",
              "author": "alphapussycat",
              "text": "ChatGPT is terrible for coding. It's an extreme gaslighter, and cannot understand requirement, or follow very simple logic.\n\nI feel like it was better a year ago than it is now.",
              "score": 5,
              "created_utc": "2026-01-28 11:42:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27fb19",
                  "author": "zball_",
                  "text": "That's literally Opus, not GPT.",
                  "score": 5,
                  "created_utc": "2026-01-28 12:54:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bm9og",
                  "author": "Crinkez",
                  "text": "The fact that you called it \"ChatGPT\" is quite telling. I agree that ChatGPT on the web is horrendous for coding.\n\n\nBut GPT5.2 CLI curbstomps all competition, including Opus 4.5",
                  "score": 0,
                  "created_utc": "2026-01-29 00:35:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26zud3",
              "author": "Inflation_Artistic",
              "text": "All GPT models are terrible for coding",
              "score": -4,
              "created_utc": "2026-01-28 11:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o270o58",
                  "author": "Alywan",
                  "text": "You clearly have no idea what you are talking about",
                  "score": 4,
                  "created_utc": "2026-01-28 11:09:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o270p4c",
                  "author": "Theio666",
                  "text": "???\n\nGpt 5.2 is better than opus if you tried using both of them on any complicated and messy codebases. And especially in ML, I'm never letting claude models on my ML projects, it's just being sloppy af in that area.",
                  "score": -1,
                  "created_utc": "2026-01-28 11:09:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27g963",
              "author": "Officer_Trevor_Cory",
              "text": "5.2 is terrible for agentic coding even codex. these lm arena results are irrelevant",
              "score": -4,
              "created_utc": "2026-01-28 13:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o296ut3",
          "author": "Avocados6881",
          "text": "I paid 20$ for google every month and I got better result. LocalLM takes 100k$ machine to perform similar or less. Yay!",
          "score": 4,
          "created_utc": "2026-01-28 17:54:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2at2y5",
              "author": "vmnts",
              "text": "Because it's open weights, you can instead pay any number of other companies a lot less than $20/mo to host it for you...",
              "score": 2,
              "created_utc": "2026-01-28 22:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2l4atn",
              "author": "cranberrie_sauce",
              "text": "eww. but your are giving money to google, so they can keep stealing from us",
              "score": 1,
              "created_utc": "2026-01-30 11:40:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wnepx",
                  "author": "Avocados6881",
                  "text": "So you are also giving much more money to Dram makers/NVidia so they keep robbing from us",
                  "score": 1,
                  "created_utc": "2026-02-01 03:05:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28crw3",
          "author": "pab_guy",
          "text": "Opus 4.5 gets a 1539 and Sonnet 4.5 gets a 1521.  That 18 points represents the difference between an OK but still stupid model and a very capable model that can handle most coding tasks end to end on it's own.\n\nThe 30 point difference makes me think I don't want to touch open models for coding ATM.  But I have access to unlimited Opus so it's an easy call for me lol.",
          "score": 2,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cncoz",
              "author": "forgotten_airbender",
              "text": "How does one get unlimited opus?Â ",
              "score": 2,
              "created_utc": "2026-01-29 04:02:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2cy5s2",
              "author": "Grand-Management657",
              "text": "If you have unlimited opus then really its a no brainer to stick to that. In my testing over a few hours, K2.5 seems to be on par with Sonnet 4.5, maybe even slightly better (big maybe). I don't care about benchmarks or points at all, in real world usage it seems to hold up well.",
              "score": 1,
              "created_utc": "2026-01-29 05:14:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bh3ru",
              "author": "Funny-Advertising238",
              "text": "These points don't represent jack shit nothing.Â ",
              "score": 1,
              "created_utc": "2026-01-29 00:09:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o273s1h",
          "author": "fugogugo",
          "text": "okay but how is its token consumption?",
          "score": 1,
          "created_utc": "2026-01-28 11:34:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279ko8",
          "author": "BABA_yaaGa",
          "text": "Scores are very tight for top 10",
          "score": 1,
          "created_utc": "2026-01-28 12:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27cu4g",
          "author": "Ne00n",
          "text": "Doesn't fit on my 64GB DDR4 LLM server, sad.",
          "score": 1,
          "created_utc": "2026-01-28 12:38:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o297snv",
          "author": "horaciogarza",
          "text": "So for coding it's better than Sonnet or Opus? If so (or not) for how much is different from a scale 1-10?",
          "score": 1,
          "created_utc": "2026-01-28 17:58:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ac7e3",
          "author": "Torodaddy",
          "text": "Qwen 3 coder 30b is pretty good thats my goto for open models",
          "score": 1,
          "created_utc": "2026-01-28 20:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2acqjf",
          "author": "ortegaalfredo",
          "text": "I ran my custom benchmarks about cybersecurity and...Kimi K2.0  thinking was definitively better. I has regressed at this subject. And it's nowhere near the commercial models like gemini or even sonnet.   \nJust my datapoint. Now the performance is almost equal to that of GLM 4.7.",
          "score": 1,
          "created_utc": "2026-01-28 20:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b686l",
          "author": "TurnUpThe4D3D3D3",
          "text": "Itâ€™s fantastic at web design. Creates beautiful websites.",
          "score": 1,
          "created_utc": "2026-01-28 23:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bb7ff",
          "author": "Freki371",
          "text": "where you seeing this? my arena.ai latest update is 23 Jan.",
          "score": 1,
          "created_utc": "2026-01-28 23:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2brxdy",
          "author": "FrankMillerMC",
          "text": "Where did Minimax go?",
          "score": 1,
          "created_utc": "2026-01-29 01:05:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cu369",
          "author": "forgotten_airbender",
          "text": "Waiting for swe rebench",
          "score": 1,
          "created_utc": "2026-01-29 04:46:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cxg0e",
          "author": "Grand-Management657",
          "text": "Its 1/5 the price but even cheaper if you use it through a subscription like nano-gpt where each request comes out to $0.00013. And that's regardless of input or output size.   \n  \n$8/month for 60,000 requests is hard to beat. It's basically unlimited coding or whatever your use case is, but you can also switch models and have access to the latest models without having to change providers each time a new and better model releases. For coding K2.5 Thinking is a beast and essentially on par, if not better than Sonnet 4.5 IMO\n\nHere's my referral for a web discount: [https://nano-gpt.com/invite/xy394aiT](https://nano-gpt.com/invite/xy394aiT)",
          "score": 1,
          "created_utc": "2026-01-29 05:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e78jj",
              "author": "Drizzity",
              "text": "Yeah the only problem is k2.5 is not working on nano-gpt at the moment",
              "score": 1,
              "created_utc": "2026-01-29 11:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e83th",
                  "author": "Grand-Management657",
                  "text": "Which harness are you using? I found nanocode to work fine. There was an issue with multi-turn tool calling which they are fixing right now. But otherwise it works well for me.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d20gy",
          "author": "alexeiz",
          "text": "I tried it via Ollama cloud and claude code.  If feels like Sonnet 4.5 on my tasks.",
          "score": 1,
          "created_utc": "2026-01-29 05:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2deeh7",
              "author": "goingsplit",
              "text": "how can you use any model on claude code?",
              "score": 1,
              "created_utc": "2026-01-29 07:23:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2dfbw6",
          "author": "This_Lemon2165",
          "text": "wow, its amazing",
          "score": 1,
          "created_utc": "2026-01-29 07:31:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhyov",
          "author": "evilbarron2",
          "text": "I get 404 errors in goose, opencode, openwebui and anythingllm every time it tries to use a tool. Quick search shows Iâ€™m not the only one. How did you folks solve that?Â ",
          "score": 1,
          "created_utc": "2026-01-29 07:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dlzks",
          "author": "cantgetthistowork",
          "text": "/u/voidalchemy wen gguf",
          "score": 1,
          "created_utc": "2026-01-29 08:31:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dvgdx",
          "author": "jasonhon2013",
          "text": "I love kimi but the weight is like â€¦. To heavy",
          "score": 1,
          "created_utc": "2026-01-29 10:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2i3crr",
          "author": "XAckermannX",
          "text": " Lmao Gemini pro is awful, and its no3.",
          "score": 1,
          "created_utc": "2026-01-29 23:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ii2ty",
          "author": "lc1402",
          "text": "gpt 5.2 is underrated",
          "score": 1,
          "created_utc": "2026-01-30 00:33:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iwlty",
          "author": "Agreeable_Asparagus3",
          "text": "Great, it would be a great idea using it with claude code cli",
          "score": 1,
          "created_utc": "2026-01-30 01:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ks9eu",
          "author": "sreekanth850",
          "text": "This is true in my case, kimi outperformed claude in many tasks.",
          "score": 1,
          "created_utc": "2026-01-30 09:58:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lb3zp",
              "author": "cranberrie_sauce",
              "text": "how do u guys run this?",
              "score": 1,
              "created_utc": "2026-01-30 12:29:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lxnf8",
                  "author": "sreekanth850",
                  "text": "[https://www.kimi.com/](https://www.kimi.com/) 7 days free trial you can test",
                  "score": 1,
                  "created_utc": "2026-01-30 14:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mcak2",
          "author": "Itchy-Cost4576",
          "text": "lendo os comentarios, as pessoas estao dividas em suas tarefas, que na qual, cada AI colapsa conforme o estado da rede que elas suportam inferir para linha de codigo, dizer qual seria a melhor que a outra, no meu ver bem irrelevante, se nao der o contexto de que, para que e o que; ja que cada um tem uma forma de programar.",
          "score": 1,
          "created_utc": "2026-01-30 15:43:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mfwt5",
          "author": "Beautiful_Egg6188",
          "text": "im using the kimi k2.5 thinking free version. And its so good. you just need to know some basics and rookie structural knowledge, and they do incredible job with minimal input. ",
          "score": 1,
          "created_utc": "2026-01-30 15:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mn8jq",
          "author": "Ok-Success-9156",
          "text": "Still on Opus train but now I really need to try Kimi...",
          "score": 1,
          "created_utc": "2026-01-30 16:32:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2upo20",
          "author": "TameBus",
          "text": "Iâ€™m enjoying working with this",
          "score": 1,
          "created_utc": "2026-01-31 20:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cqi9",
          "author": "BigMagnut",
          "text": "Isn't it a trillion parameters? Doesn't seem very efficient. What am I missing here?",
          "score": 1,
          "created_utc": "2026-01-28 15:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2cxv3i",
              "author": "Grand-Management657",
              "text": "It only activates 32b parameters at a time",
              "score": 2,
              "created_utc": "2026-01-29 05:12:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blibm",
          "author": "Crinkez",
          "text": "Bad benchmark site, I don't see the best coding model (GPT5.2) on it. Wouldn't trust that benchmark.",
          "score": 1,
          "created_utc": "2026-01-29 00:31:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275acz",
          "author": "ptear",
          "text": "Do you have the link to the leaderboard from the screenshot, I couldn't find the model listed and wanted to see that.",
          "score": 0,
          "created_utc": "2026-01-28 11:46:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ofxf",
          "author": "LocoMod",
          "text": "7th is the 6th loser",
          "score": 0,
          "created_utc": "2026-01-28 13:46:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o287yd5",
              "author": "SpicyWangz",
              "text": "You never know. Itâ€™s possible theyâ€™re all losers",
              "score": 4,
              "created_utc": "2026-01-28 15:22:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o291zpq",
          "author": "datosweb",
          "text": "lo estuve probando y comparando con glm 4.7 y estan muy muy parejos",
          "score": 0,
          "created_utc": "2026-01-28 17:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27914k",
          "author": "Asleep-Ingenuity-481",
          "text": "Barely open but I guess it counts. Honestly im getting tired of these models being insanely large. Though I guess we can kind of just go with it setting the bar for performance for smaller models as well.",
          "score": -3,
          "created_utc": "2026-01-28 12:12:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o285kwa",
              "author": "pneuny",
              "text": "I feel like the Qwen models are one of the few that target hardware people actually have in their homes, though I certainly appreciate these larger open models that move the research forward for future small models. Though right now I have free access to Gemini given the free year for students, plus AI Studio, but I still keep an eye out for when the free lunch eventually ends.\n\nThough I'd probably hook up my local models to Vertex AI Search anyways to get that web result grounding without needing to load in hundreds of webpages on a home computer.\n\nIf I had to switch now, I'd probably do Qwen 3 VL 4b hooked up to Vertex AI Search and that would probably be enough. That would fit on my 16 GB RX 9070 XT.",
              "score": 2,
              "created_utc": "2026-01-28 15:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28lan8",
                  "author": "chodemunch6969",
                  "text": "You should try GLM 4.7 Flash if you haven't already. It's next level for the 30ba3b MoE weight class that can reasonably run on your own metal.",
                  "score": 3,
                  "created_utc": "2026-01-28 16:21:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2bbpnu",
                  "author": "Asleep-Ingenuity-481",
                  "text": "That's where I stand, I don't like them due to their size, but I like them because they set the bar for smaller models to aspire to. Hell im pretty sure that qwen3.5/4 will probably beat deepseek R1 at around 14-40b params.",
                  "score": 1,
                  "created_utc": "2026-01-28 23:41:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2n1gpd",
                  "author": "morfr3us",
                  "text": "nah GLM has been badly beating Qwen for a few years now (on the limited hardware front)",
                  "score": 1,
                  "created_utc": "2026-01-30 17:36:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tuiv",
              "author": "z_3454_pfk",
              "text": "then train your own instead of whining",
              "score": 2,
              "created_utc": "2026-01-28 14:14:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o285fk2",
                  "author": "into_devoid",
                  "text": "But I want people to work for free and focus on MEEE! Â These models will fit on consumer hardware sometime within the next 5-10yrs most likely. Â The difference between a 30B and 600+ model are staggering in terms of intelligence and real world utility. Â Theyâ€™re probably just planning ahead instead of wasting training time on models that will never be good enough for 95% of most tasks.",
                  "score": 1,
                  "created_utc": "2026-01-28 15:11:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27cmjq",
          "author": "Niwa-kun",
          "text": "I wish i could load Grok 4 into Antigravity, it's actually not bad. I used it in-tangent with Gemini 3 Pro, together they figured out alot of stuff for my project that 1 alone would fail at--but i had to keep being the meditator for them.",
          "score": -4,
          "created_utc": "2026-01-28 12:37:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27i9w0",
              "author": "Equivalent_Plan_5653",
              "text": "No thanks",
              "score": 4,
              "created_utc": "2026-01-28 13:12:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27sl6e",
          "author": "eihns",
          "text": "ON MY TESTS (cli): its using 1 or 2 subagent while coding uhhhh... anyway, its 90% \\[if u ask me\\] of chatgpt 5.2 /codex, so  why bother? Maybe next version wont immediatly touch my prod.db even tho the promt and agents and everything on that whole repo told him to not touch it xD Tests are irrelevant if the f\\* thing doenst understand what youre talking about... or how to start a dev server.... (all agents were able to do it without any help, except him) xD he just waited till bash timedout and \"tought\" the sever is running did screenshots of \"not reachable\" and told me the fix is working :D so yeah, if you want to stress a developer, then just let 100 of those agents run :D",
          "score": -1,
          "created_utc": "2026-01-28 14:07:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoty38",
      "title": "Kimi K2.5 costs almost 10% of what Opus costs at a similar performance",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xz7okply3zfg1.png",
      "author": "Odd_Tumbleweed574",
      "created_utc": "2026-01-27 23:10:16",
      "score": 576,
      "num_comments": 111,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o269wrx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-28 07:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o257yrq",
          "author": "one-wandering-mind",
          "text": "It used 3x the tokens that opus does for the same tasks so cheaper, but more like 3x cheaper than 10x cheaper.Â \n\n\nThese models often use a dramatically different number of tokens to do the same thing. It should be considered for both cost and latency when you compare them.Â \n\n\nI've heard great things about the kimi models especially the last version for writing.Â \n\n\nÂ https://artificialanalysis.ai/#cost-to-run-artificial-analysis-intelligence-index",
          "score": 151,
          "created_utc": "2026-01-28 02:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26orzg",
              "author": "ozzie123",
              "text": "â€œSimilar performanceâ€ here also carries lots of asterisks",
              "score": 51,
              "created_utc": "2026-01-28 09:23:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2dhcye",
                  "author": "Cergorach",
                  "text": "Similar performance does not mean equal quality output for every task.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:49:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2472ya",
          "author": "TAW56234",
          "text": "If I had a nickel for every time someone claimed the newest OSS Sota model was similar to Claude, I could generate a few prompts.",
          "score": 356,
          "created_utc": "2026-01-27 23:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gq5f",
              "author": "ForsookComparison",
              "text": "I use closed weight models at work (required) and open weight models for side projects (cost). Several hours of agent use per day.\n\nI think open weight is approaching **Sonnet 3.7** if I'm totally honest. I love this community to death but it gets drunk off of bar charts and one-shots. Sonnet 3.7 for Kimi and Deepseek prices is amazing, but *\"approaching Opus 4.5\"* just flags for me that nobody is using these things for hours at a time.",
              "score": 138,
              "created_utc": "2026-01-28 03:46:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o260qbh",
                  "author": "peculiarMouse",
                  "text": "I used Max extensively for multiple months and I tend to disagree. Claude is best at writing code, but architecture, hallucinations and creativity make it highly debatable in all other things that matter about code. \n\nGemini Pro for instance has INSANE, borderline dementia levels hallucinations, where it necessitates undoing generation and redoing with something else, but its context window and knowledge is unparalleled.\n\nClaude is very fixed on things it thinks it knows how to do. For example old libraries that have long deprecated documentation. GLM with same Claude .md has much lower rate for ignoring it altogether. It goes as far as making your code worse through unrequested changes.\n\nI would easily put Claude first as agentic powerhouse that comes with your subscription.  \nBut I tend to use other models a lot, when working with Claude, so I'd put them on Sonnet 4.6 level easily.\n\nClaude just has very good understanding of what's most important to 95% devs.",
                  "score": 40,
                  "created_utc": "2026-01-28 05:57:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25q46f",
                  "author": "KaroYadgar",
                  "text": "What do you use them for? LLMs are very spike-y so I'd like to know at what specific tasks you think they're comparable to Sonnet 3.7. Are there any tasks you think they are comparable or better at?",
                  "score": 11,
                  "created_utc": "2026-01-28 04:43:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25qhvs",
                  "author": "Virtamancer",
                  "text": "Have you used any of the current-gen top open models in OpenCode? Like, the absolute newest version of deepseek/glm/kimi/qwen/whatever, like within the last 7 days or whenever they released.\n\nIâ€™m curious how they compare running through OpenCode vs Claude Code with Opus 4.5 or Codex with GPT-5.2 set to Extra High reasoning.",
                  "score": 6,
                  "created_utc": "2026-01-28 04:46:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26h5zi",
                  "author": "FyreKZ",
                  "text": "I think you're misremembering how dumb 3.7 was lol. K2.5 is realistically probably not far off Sonnet 4.5, Opus is a stretch.",
                  "score": 2,
                  "created_utc": "2026-01-28 08:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o252exg",
              "author": "Zc5Gwu",
              "text": "Generate prompts with kimi or claude though, thatâ€™s the question.",
              "score": 28,
              "created_utc": "2026-01-28 02:28:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2avsuf",
              "author": "eikenberry",
              "text": "No reason it couldnâ€™t be true every time. This is active research with things constantly changing.",
              "score": 1,
              "created_utc": "2026-01-28 22:21:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24694d",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 68,
          "created_utc": "2026-01-27 23:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qbtx",
              "author": "DistanceSolar1449",
              "text": "Almost certainly not. [Estimates for Claude Opus 4.5 put it at 1.6T-3T params, 160b active](https://news.ycombinator.com/item?id=46038512). \n\nThat would make Kimi roughly 1/2 to 1/4 the size.",
              "score": 7,
              "created_utc": "2026-01-28 04:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24ewv8",
              "author": "neotorama",
              "text": "I would say almost similar. Opus has been degrading last 3 weeks",
              "score": -8,
              "created_utc": "2026-01-28 00:25:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24rlr0",
                  "author": "eli_pizza",
                  "text": "Opus hasnâ€™t changed.",
                  "score": 17,
                  "created_utc": "2026-01-28 01:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o24fo78",
              "author": "Healthy-Nebula-3603",
              "text": "Yes \n\nIs very similar",
              "score": -10,
              "created_utc": "2026-01-28 00:29:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24q0lj",
                  "author": "Bob_Fancy",
                  "text": "Iâ€™d bet a good amount that itâ€™s not.",
                  "score": 13,
                  "created_utc": "2026-01-28 01:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o25snrk",
          "author": "dubesor86",
          "text": "this assumes a ton of input and will swing widely depending on use case. for me, the bulk of the cost is always the model output.\n\nin my general benchmark the cost was:\n\nKimi-K2.5 (reasoning) $1.60\n\nClaude Opus 4.5 $2.75\n\n= 42% cheaper\n\n\nin my chess benchmark the game cost was:\n\nKimi-K2.5 (reasoning) $0.87\n\nClaude Opus 4.5 $0.46\n\n= 89% more expensive\n\nAlso, obviously the performance is not \"similar\" level if you actually used these models, despite what some bars tell you.",
          "score": 25,
          "created_utc": "2026-01-28 04:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24e4e0",
          "author": "ghulamalchik",
          "text": "I'm just gonna wait for DeepSeek 4 and MiniMax M2.2\n\nI trust those from experience.\n\nI used many models in cline and DS and MiniMax were my favorite.",
          "score": 35,
          "created_utc": "2026-01-28 00:21:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25gwjm",
              "author": "epyctime",
              "text": "you prefer them to glm4.7?",
              "score": 7,
              "created_utc": "2026-01-28 03:47:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25qbup",
                  "author": "ghulamalchik",
                  "text": "I haven't used GLM 4.7 a lot to form a comprehensive opinion, however when I tried it it was *slightly* worse than MiniMax M2.1 for my use cases (programming desktop GUIs). MiniMax was faster and slightly better. DeepSeek was nice when it came to debugging big issues, it's about the same in terms of general intelligence, it's much slower than MiniMax though.\n\nTL;DR\n- DS was smart but slow, good for debugging and planning big changes. It's also very cheap. The main downside is that it's slower.\n- MiniMax is good a default.\n- GLM 4.7 is in the same ballpark in terms of performance but MiniMax felt a little better for me personally.\n\nI think it all depends on what you use the models for. There's no answer that fits all.",
                  "score": 8,
                  "created_utc": "2026-01-28 04:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25c7v4",
              "author": "Zianiwarhead",
              "text": "I strongly agree.",
              "score": 2,
              "created_utc": "2026-01-28 03:21:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25wjgj",
              "author": "deadcoder0904",
              "text": "What are u using DS & MiniMax for specifically? Just coding? Frontend or backend?",
              "score": 1,
              "created_utc": "2026-01-28 05:26:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25xqoj",
                  "author": "ghulamalchik",
                  "text": "Yeah just coding. I'm mainly doing desktop GUIs in Qt (full programs not just the GUI part).",
                  "score": 2,
                  "created_utc": "2026-01-28 05:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26vtkt",
              "author": "Loskas2025",
              "text": "I use GLM 4.7 with kilocode or cline for planning and code setup. But then I switch to minimax 2.1 (all local) for writing and back to GLM 4.7 for orchestration. Used in production.",
              "score": 1,
              "created_utc": "2026-01-28 10:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2712vh",
                  "author": "evia89",
                  "text": "Dont u need smart model for design+plan? I notice that when I do that with opus it gets me at 95%. When I use glm for it I need to do ALOT of hand holding and plan (atomic step by step tasks) wont be as good\n\nHowever if I do plan (superpower skill, not build in) with opus then GLM can code it just fine",
                  "score": 1,
                  "created_utc": "2026-01-28 11:12:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25coe3",
              "author": "Peetlin",
              "text": "same here it's not getting enough love. it's fast and intelligence and agentic capability. it has brain oof 80% of gpt5.2 but that's enough for me",
              "score": 2,
              "created_utc": "2026-01-28 03:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24byuf",
          "author": "NighthawkT42",
          "text": "It's good, but not really the same level.",
          "score": 40,
          "created_utc": "2026-01-28 00:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e4mh",
              "author": "ihexx",
              "text": "yeah, it's closer to sonnet or gemini 3 flash",
              "score": 1,
              "created_utc": "2026-01-28 07:46:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24vphu",
          "author": "Recoil42",
          "text": "Does it use the same number of tokens? I doubt it.",
          "score": 8,
          "created_utc": "2026-01-28 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o250252",
              "author": "electronicsoul",
              "text": "Exactly, and what's being cached in those token counts for a much cheaper rate and therefore faster inference is what people seem to forget. That's a big part of what makes Claude feel a level above the rest.",
              "score": 4,
              "created_utc": "2026-01-28 02:16:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2458eb",
          "author": "TransportationSea579",
          "text": "Is the similar performance in the room with us?",
          "score": 78,
          "created_utc": "2026-01-27 23:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24lm0k",
              "author": "Howdareme9",
              "text": "It never is lol.",
              "score": 20,
              "created_utc": "2026-01-28 00:59:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2486zq",
              "author": "illforgetsoonenough",
              "text": "K2.5 is sitting on the chair in the corner of the room",
              "score": 7,
              "created_utc": "2026-01-27 23:51:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24fur6",
              "author": "Healthy-Nebula-3603",
              "text": "From what I tested and what I saw on YouTube.... Very similar to opus 4.5 in coding.",
              "score": -9,
              "created_utc": "2026-01-28 00:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25d942",
          "author": "galambalazs",
          "text": "It's more fair to compare to sonnet 4.5",
          "score": 6,
          "created_utc": "2026-01-28 03:26:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ulg4y",
              "author": "RakibOO",
              "text": "no. more fair to compare with sonnet 3.5. claude 4.5 is ai from 2030",
              "score": 1,
              "created_utc": "2026-01-31 20:22:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25tibf",
          "author": "Torodaddy",
          "text": "*absolutely not the same performance*",
          "score": 14,
          "created_utc": "2026-01-28 05:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e67e",
              "author": "rm-rf-rm",
              "text": "based on prejudice or something more concrete? TBH I am in agreement with you, but Im asking what your basis is for clarity",
              "score": 3,
              "created_utc": "2026-01-28 07:46:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a7x97",
                  "author": "Torodaddy",
                  "text": "Anecdotally, ive used both extensively and I think its not even close when it comes to something requiring some sequential logic to get the answer. Opus head and shoulders better",
                  "score": 3,
                  "created_utc": "2026-01-28 20:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o246yjt",
          "author": "LoveMind_AI",
          "text": "Kimi in the thinking era is VERY hit or miss. The hits are amazing. The misses are sad, because when itâ€™s on, it really gives the SOTA a run for its money. I still think MiniMax M2 is the best LLM outside of the West.",
          "score": 20,
          "created_utc": "2026-01-27 23:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264z3l",
          "author": "TheInfiniteUniverse_",
          "text": "obviously this would be fantastic if true. this is quite obviously not true, still way behind Opus and GPT but getting there....",
          "score": 6,
          "created_utc": "2026-01-28 06:30:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2668r4",
          "author": "Michaeli_Starky",
          "text": "At similar performance?  Not even close.",
          "score": 4,
          "created_utc": "2026-01-28 06:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25x2iy",
          "author": "Middle_Bullfrog_6173",
          "text": "If you just go by benchmarks, Genini 3 Flash also has similar performance for 10% cost. In reality for some use cases there's much more difference at the top than a few points suggest. And some tasks are not really captured by benchmarks.\n\n\nLimited testing so far, but K2.5 has the large model feature of great niche performance, like low resource languages. But it seems to lose coherence earlier than the big closed models as context fills up. Can't really say anything more specific yet.",
          "score": 4,
          "created_utc": "2026-01-28 05:30:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260dkg",
          "author": "Dazzling_Focus_6993",
          "text": "as someone who using heavily using these tools for real world tasks, I would say opus 4.5 is another level, despite benchmarks implying gpt 5.2 and gemini 3 are very close. I say the gap is huge. I cannot use any other model for relatively complex tasks. \n\nI use opensource models (e.g., gpt-oss, kimi and qwen) for relatively simple tasks mainly due to the cost. I look forward to trying kimi 2.5 but, to be frank, i do not have high hopes. I think i will continue to use my combination of opus 4.5 and cheap OS models. \n\nPS: I select models through Kilo Code\n\nAdditionally: For gemini 3, it is the opposite. I do not have any use cases for gemini 3 despite benchmarks look very high.",
          "score": 3,
          "created_utc": "2026-01-28 05:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26iuob",
              "author": "Front_Eagle739",
              "text": "weird. I use codex 5.2high and claude code side by side and both are better at different things. If I had to pick a smarter one i'd nudge to 5.2 high. Its just more likely to one shot fix a problem and less likely to get lost in circles. Claude and opus is waaay faster though so still gets used first.",
              "score": 2,
              "created_utc": "2026-01-28 08:28:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3acs",
                  "author": "Impossible_Hour5036",
                  "text": "Probably depends on prompting technique.  Can't say I've gotten Opus lost unless I was badgering it while it worked or I didn't have a clear goal.",
                  "score": 1,
                  "created_utc": "2026-01-28 20:16:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o240qo4",
          "author": "ChainOfThot",
          "text": "How can their API costs be so cheap with what I assume is still older hardware?",
          "score": 4,
          "created_utc": "2026-01-27 23:13:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24us8r",
              "author": "BlueSwordM",
              "text": "Native INT4, great context handling, small MOE and not amazing speeds.",
              "score": 7,
              "created_utc": "2026-01-28 01:48:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o249wya",
              "author": "smith7018",
              "text": "Government subsidies",
              "score": 10,
              "created_utc": "2026-01-28 00:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o24kjkr",
                  "author": "HateAccountMaking",
                  "text": "We(USA) also have Government subsidies. China is a peoples government thats the difference.",
                  "score": -8,
                  "created_utc": "2026-01-28 00:54:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o250j8u",
          "author": "electronicsoul",
          "text": "Everyone loves cheap tokens but that's a very reductionist calculation.",
          "score": 5,
          "created_utc": "2026-01-28 02:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24qgyx",
          "author": "lochyw",
          "text": "Who chose these colors? It's literally the opposite of what it should be.",
          "score": 2,
          "created_utc": "2026-01-28 01:25:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24tw1h",
              "author": "derivative49",
              "text": "you're talking about it ;)",
              "score": -3,
              "created_utc": "2026-01-28 01:43:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o261l6f",
          "author": "Cool-Selection-9275",
          "text": "Just how do they even do that? That's nuts",
          "score": 2,
          "created_utc": "2026-01-28 06:03:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o260jz8",
          "author": "ChimSau19",
          "text": "I have problem with GLM that claude code just dont ask me if it could modify? Could it fix simply by Shift Tab, or u guy have that problem too?",
          "score": 1,
          "created_utc": "2026-01-28 05:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o265zbl",
          "author": "Crafty-Struggle7810",
          "text": "At what quant?",
          "score": 1,
          "created_utc": "2026-01-28 06:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dit1",
          "author": "korino11",
          "text": "well i making a project and codex 5.2high doesnt solved. But kimi did LOL\n\nGPT always aking about whole parameters what they should be, how need to be done wjole parts. When he have all formulas amd whole project in high math is done. But Kimi, i jut give all formulass and...hold my bear!\n\nJust 1 thing i do not like. in Coding Plans you have a limits on API -Calls. So doesnt matter your call 20k tokens or just 500 tokens...  But..i got it for 2.59  So it perfect for that price",
          "score": 1,
          "created_utc": "2026-01-28 07:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26j1gj",
          "author": "No_Afternoon_4260",
          "text": "> the first time that I feel a open source model is truly competitive..\n\nYeah I know! I feel the same every 6 months, last time was deepseek's release, then k2's.. then glm was the faster brother..\n\nCrazy times I know ðŸ˜…\n\n(As every 6 months, maybe not opus level but not far)",
          "score": 1,
          "created_utc": "2026-01-28 08:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qaqj",
          "author": "rduito",
          "text": "Wait, you said a bit better than glm. So not comparable to opus after all?\n\n\nAm a fan of glm but opus is something else",
          "score": 1,
          "created_utc": "2026-01-28 09:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ywi8",
          "author": "alexrada",
          "text": "what tests have you run?",
          "score": 1,
          "created_utc": "2026-01-28 10:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28cwbv",
          "author": "Beginning_Company_85",
          "text": "I tried every models available and ended up with Claude max. Totally insane the amount of work it got me done in just 2 weeks.",
          "score": 1,
          "created_utc": "2026-01-28 15:45:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28z4im",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. one-wandering-mind is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 1,
          "created_utc": "2026-01-28 17:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bc6jd",
          "author": "elllyphant",
          "text": "& you can use this Synthetic promo (40% off a sub) to use kimi k2.5 w/ high rate limits [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)",
          "score": 1,
          "created_utc": "2026-01-28 23:43:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsl6g",
          "author": "Muted_Standard175",
          "text": "Does anyone tried using opencode with codex for planning and k2.5 for building? Is it good?",
          "score": 1,
          "created_utc": "2026-01-29 01:09:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbrnq",
          "author": "Spiritual_Cycle_9141",
          "text": "https://preview.redd.it/4a8nhb7kd7gg1.png?width=856&format=png&auto=webp&s=efb13080f2958c34b582af7238533243fc89edeb\n\n  \nWorked for few seconds and errored , don't waste ur time (( no real free things now )) , also they will trian on ur propmpts",
          "score": 1,
          "created_utc": "2026-01-29 02:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2da7bq",
          "author": "deparko",
          "text": "I've been using Kimi too. I've been developing a health agent, and it is very responsive and very good, but it sometimes comes off as very authoritative and occasionally hallucinates.\n\nI plan to build an agent swarm to validate, but overall I think it's one of the first open models that I don't want to stop using. A lot of the open models I'll work with, but I usually end up on a frontier model eventually. I don't feel that way with kimi.",
          "score": 1,
          "created_utc": "2026-01-29 06:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ea2su",
          "author": "DarqOnReddit",
          "text": "A bit better? Aka still shit. Sorry but neither GLM-4.7 not Minimax are good models. They may be narrowly scoped tiny requests right, but require a lot of iteration and their reasoning or should I say problem solving skills are utter trash",
          "score": 1,
          "created_utc": "2026-01-29 12:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egpkv",
          "author": "Late_Special_6705",
          "text": "How to use  kimi? Kimi coder?",
          "score": 1,
          "created_utc": "2026-01-29 12:46:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27e6xl",
          "author": "tcoder7",
          "text": "No model matches even half of Opus 4.5 full capacities.  This is hype. OSS models are good for execution. This latest model is more a competitor to Sonnet 4.5.",
          "score": 0,
          "created_utc": "2026-01-28 12:47:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o243jly",
          "author": "PhotographerUSA",
          "text": "Kimi has terrible programming skills. lol",
          "score": -10,
          "created_utc": "2026-01-27 23:27:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24bh4i",
          "author": "Ashley_Sophia",
          "text": "Yeah but I just had a look. Instant ads via UI. No thx.",
          "score": -9,
          "created_utc": "2026-01-28 00:08:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqj51h",
      "title": "LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/fjyoor8kecgg1",
      "author": "Electrical-Shape-266",
      "created_utc": "2026-01-29 19:54:56",
      "score": 557,
      "num_comments": 58,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2h5gxo",
          "author": "ItilityMSP",
          "text": "It be nice if you gave an indication of what kind of hardware is needed to run the model. Thanks.",
          "score": 87,
          "created_utc": "2026-01-29 20:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hayqj",
              "author": "_stack_underflow_",
              "text": "If you have to ask, you can't run it.\n\nFrom the command it needs 8 GPUs on a single machine. It's FSDP and a 14B model (the 14B isn't indicative of what is needed)\n\nI suspect:  \nâ€¢ Dual EPYC/Xeon or Threadripper Pro  \nâ€¢ 256GB to 1TB system RAM  \nâ€¢ NVMe scratch (fast disk)  \nâ€¢ NVLink or very fast PCIe  \nâ€¢ 8x A100 80GB",
              "score": 101,
              "created_utc": "2026-01-29 20:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j2358",
                  "author": "Upper-Reflection7997",
                  "text": "Brah nobody is running this model locally. God damn  8 a100s. Perhaps in future there will be a sweet ultra compressed fp4 model to fit in 5090+64gb ram system build.",
                  "score": 36,
                  "created_utc": "2026-01-30 02:23:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2hwfn3",
                  "author": "oxygen_addiction",
                  "text": "14-22$/h on Runpod. Not that bad. It should run at around 14-16fps, so input latencty will be quite rough.",
                  "score": 15,
                  "created_utc": "2026-01-29 22:37:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2mp7go",
                  "author": "IntrepidTieKnot",
                  "text": "Like a year ago I would have thought: 1TB RAM - that's a lot. But well, it's doable if I really want it. Reading it today is like: whaaaat? 1.21 Jiggawatt?\n1 TB is a nice little 10k nowadays. Ridiculous.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:41:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ngjje",
                  "author": "Zestyclose839",
                  "text": "Hear me out: quantize down to IQ1\\_XXS, render at 144p, interpolate every other frame. It would be like playing a DALL-E era nightmare but all the more fun.",
                  "score": 1,
                  "created_utc": "2026-01-30 18:42:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ruoa7",
                  "author": "ApprehensiveDelay238",
                  "text": "Why a TB of RAM when you run the model on the GPU?",
                  "score": 1,
                  "created_utc": "2026-01-31 11:26:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ki2rb",
                  "author": "Lissanro",
                  "text": "I have EPYC with 1 TB RAM, and fast 8 TB NVMe, but unfortunately just four 3090 cards on x16 PCI-E 4.0 slots. Even though I could four more for eight in total, if it really needs 80 GB VRAM on each card, I guess I am out of luck.",
                  "score": 0,
                  "created_utc": "2026-01-30 08:24:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2imjdf",
              "author": "derivative49",
              "text": "also the usecase?",
              "score": 4,
              "created_utc": "2026-01-30 00:56:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jz1pi",
                  "author": "IrisColt",
                  "text": "heh",
                  "score": 2,
                  "created_utc": "2026-01-30 05:47:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hybp3",
          "author": "LocoMod",
          "text": "Where is the Genie 3 comparison? Or did you fail to include it because you don't really have access to it and can't actually compare?\n\n\"LingBot-World outperforms Genie 3 because trust me bro\"",
          "score": 60,
          "created_utc": "2026-01-29 22:47:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ifzzs",
              "author": "adeadbeathorse",
              "text": "To be honest it looks pretty much AT or NEAR Genie 3â€™s level, at least. Watched a youtube vid exploring Genie 3 and trying various prompts.",
              "score": 3,
              "created_utc": "2026-01-30 00:21:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jl7oj",
                  "author": "LocoMod",
                  "text": "If beauty is the n the eye of the beholder then you need to get those eyes checked. There is no timeline where a model you host locally (if youâ€™re fortunate enough to afford thousands of $$$) that beats Google frontier models running in state of the art data centers.\n\nI am an enthusiast and wish for it to be so. I donâ€™t want to be vendor locked either. But reality is a hard pill to swallow.\n\nYou can settle for â€œgood enoughâ€ if thatâ€™s your jam. But that will not pay the bills in the future economy.\n\nIf you are not using the best frontier models in any particular domain then you are not producing anything of value.\n\nYes, itâ€™s an extremely inconvenient truth.\n\nBut â€¦",
                  "score": -3,
                  "created_utc": "2026-01-30 04:13:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2im1id",
              "author": "TheRealMasonMac",
              "text": "To be honest, Genie might as well not exist since you can't access it unless you're a researcher.",
              "score": 5,
              "created_utc": "2026-01-30 00:54:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jfg6f",
                  "author": "Ok-Morning872",
                  "text": "it just released for gemini ai ultra subscribers",
                  "score": 12,
                  "created_utc": "2026-01-30 03:38:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jk4bi",
                  "author": "LocoMod",
                  "text": "Most people donâ€™t have the hardware to run LingBot either. And Iâ€™m not talking about the 1% of enthusiasts in here with the skills and money to invest in the hobby.\n\nIt might as well not exist either.",
                  "score": -5,
                  "created_utc": "2026-01-30 04:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2icnsv",
              "author": "_raydeStar",
              "text": "I agree - and also this kind of thing is really frontier, and doesn't have benchmarks yet that I know of.",
              "score": -2,
              "created_utc": "2026-01-30 00:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2l117c",
              "author": "Mikasa0xdev",
              "text": "Open source LLMs are the real frontier.",
              "score": 0,
              "created_utc": "2026-01-30 11:14:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lgsoe",
                  "author": "LocoMod",
                  "text": "And fermented cabbage is better than ground beef right?",
                  "score": 1,
                  "created_utc": "2026-01-30 13:05:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i560g",
          "author": "Ylsid",
          "text": "Cool post but no AGI is not very near",
          "score": 26,
          "created_utc": "2026-01-29 23:23:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i826g",
              "author": "Xablauzero",
              "text": "Yeah, we're really really really far away from AGI, but I'm extremely glad to at least see that we're reaching that 1% or even 2% from what was 0% for years and years beyond. If humanity even hit the 10% mark, growth gonna be exponential.",
              "score": -4,
              "created_utc": "2026-01-29 23:38:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2h8mrh",
          "author": "Sl33py_4est",
          "text": "so you ran it and are reporting this empirically? or are you just sharing the projec that has already been shared",
          "score": 13,
          "created_utc": "2026-01-29 20:43:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hzxft",
          "author": "SmartCustard9944",
          "text": "Put a small version of it into a global illumination stack, and then we are talking.",
          "score": 3,
          "created_utc": "2026-01-29 22:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l7okz",
          "author": "jacek2023",
          "text": "This is another post not about a local model, which people mindlessly upvote to the top of LocalLLaMA â€œbecause itâ€™s open, so you know, Iâ€™m helping, Iâ€™m supporting, you know.â€",
          "score": 3,
          "created_utc": "2026-01-30 12:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jhz2g",
          "author": "kvothe5688",
          "text": "where is the example of persistent memory?",
          "score": 2,
          "created_utc": "2026-01-30 03:53:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jnt2f",
              "author": "adeadbeathorse",
              "text": "[here you go](https://arxiv.org/pdf/2601.20540) \n\n> A key property of LingBot-World is its emergent ability to maintain global consistency without relying on explicit 3D representations such as Gaussian Splatting. [...] the model preserves the structural integrity of landmarks, including statues and Stonehenge, even after they have been out of view for long durations of up to 60 seconds. Crucially, unlike explicit 3D methods that are typically constrained to static scene reconstruction, our video-based approach is far more dynamic. It naturally models complex non-rigid dynamics, such as flowing water or moving pedestrians, which are notoriously difficult for traditional static 3D representations to capture.  \nBeyond merely rendering visible dynamics, the model also exhibits the capability to reason about the evolution of unobserved states. For instance [...] a vehicle leaves the frame, continues its trajectory while unobserved, and reappears at a physically plausible location rather than vanishing or freezing.  \n[...] generate coherent video sequences extending up to 10 minutes in duration. [...] our model excels in motion dynamics while maintaining visual quality and temporal smoothness comparable to leading competitors.\n\n[See this cat video for an example.](https://old.reddit.com/r/singularity/comments/1qq7ddv/lingbotworld_achieves_the_holy_grail_of_video/) Notice not just the cat, but the books on the shelves.",
              "score": 3,
              "created_utc": "2026-01-30 04:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzfd7",
          "author": "PeachScary413",
          "text": "This looks like ass ðŸ‘ðŸ‘Œ",
          "score": 2,
          "created_utc": "2026-01-30 14:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hgqnt",
          "author": "Historical-Internal3",
          "text": "Guess I'll try this on my DGX Spark cluster then realize its a fraction of what I actually need in terms of requirements.",
          "score": 2,
          "created_utc": "2026-01-29 21:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o6xuu",
          "author": "CacheConqueror",
          "text": "Less than 30 fps :/",
          "score": 1,
          "created_utc": "2026-01-30 20:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2va927",
          "author": "PrixDevnovaVillain",
          "text": "Very intriguing, but I don't want this technology to replace level design for video games; always preferred handcrafted worlds.",
          "score": 1,
          "created_utc": "2026-01-31 22:24:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hn6hu",
          "author": "Aggressive-Bother470",
          "text": "It looks awesome but it's not a 'world model' is it?Â \n\n\nA 'world rendering model' perhaps?",
          "score": 2,
          "created_utc": "2026-01-29 21:52:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hvh9p",
              "author": "OGRITHIK",
              "text": "Then Genie 3 isn't a world model either?",
              "score": 5,
              "created_utc": "2026-01-29 22:33:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2jvilt",
              "author": "HorriblyGood",
              "text": "World model is more of a research term referring to foundational models that models real worldâ€™s physics, interactions, etc. As opposed to language models, vision models.",
              "score": 2,
              "created_utc": "2026-01-30 05:21:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ovf0q",
          "author": "idersc",
          "text": "Why are they both exactly 60sec ? is there any reason ? (i would have expect it to be lower or higher since it's 2 different companies but not the same)",
          "score": 0,
          "created_utc": "2026-01-30 22:41:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p0quz",
              "author": "Basic_Extension_5850",
              "text": "60 seconds is a common unit of timeÂ ",
              "score": 1,
              "created_utc": "2026-01-30 23:09:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqhhtx",
      "title": "Mistral CEO Arthur Mensch: â€œIf you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.â€",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/wd12dl725cgg1",
      "author": "Wonderful-Excuse4922",
      "created_utc": "2026-01-29 18:56:08",
      "score": 554,
      "num_comments": 65,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2iudfd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-30 01:40:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gpbvc",
          "author": "Kahvana",
          "text": "He's arguing for using open (weight) models, which is great!",
          "score": 209,
          "created_utc": "2026-01-29 19:11:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3rqr",
              "author": "LoaderD",
              "text": "\"No, no, ban his thread, what if OpenAI posts some marketing news? It might detract from that attention\" - Average Thread Poster here.",
              "score": -13,
              "created_utc": "2026-01-29 23:15:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jxfzw",
                  "author": "Kahvana",
                  "text": "Not sure where you draw that negative conclusion from, but I'm not seeing it.",
                  "score": 16,
                  "created_utc": "2026-01-30 05:35:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kot2o",
                  "author": "hugthemachines",
                  "text": "> Average Thread Poster here\n\nThat's severely biased.",
                  "score": 8,
                  "created_utc": "2026-01-30 09:26:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gwtcs",
          "author": "artisticMink",
          "text": "I really like mistral, they've treading good balance between corpo and open weights. I wish them that they get their major breakthrough eventually.",
          "score": 95,
          "created_utc": "2026-01-29 19:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hh3yq",
              "author": "AppealSame4367",
              "text": "Look at their references. They already had their breakthrough behind the curtains. Consulting the French ministry of defense tells you that this is a national French asset.",
              "score": 22,
              "created_utc": "2026-01-29 21:23:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hjyot",
                  "author": "ProfessionalSpend589",
                  "text": "Maybe they play the same game as China - release open source models so the American companies donâ€™t become monopolies and strongarm Europe into submission again.\n\nFrance is very sensitive to the US politics from my understanding over the years.",
                  "score": 19,
                  "created_utc": "2026-01-29 21:37:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2s8oys",
                  "author": "interesting_vast-",
                  "text": "not exactly the French governmentâ€™s (really most european governments) involvement with their private sector is vastly different from the USA, Muratis new company got a $10 mil investment from Albanian government even though the â€œcompanyâ€ literally only has (almost) one product that only 2-3 universities kinda use. (Mira Murati is Albanian) \n\nThese â€œinvestmentsâ€/ partnerships tend to be more about public image, Mistral is pretty much the only pure French large tech company, as a country trying to position itself as a place for europeans to come start their tech startups, itâ€™s in the french governmentâ€™s best interests to play up Mistralâ€™s success. Also Europe has had a huge push for open-source tech in general especially in the public sector as they see reliance on american big tech as a operational/â€œsecurityâ€ risk, and Mistral fits that role very well. \n\nI mostly use Mistral models my self, so I really do appreciate the work the company does, but I do not believe their involvement with the french government is â€œa breakoutâ€ moment. \n\nI believe Mistral will truly shine and have a breakout moment in a couple years when the hype begins to focus on performance vs efficiency, how good can your small and resource efficient model actually be? this is something they have had tremendous success with (although mostly unnoticed as current hype is still focused on top line performance rather than efficiency) but imo thats going to be their time to shine and I cannot wait!",
                  "score": 2,
                  "created_utc": "2026-01-31 13:15:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gws06",
          "author": "tarruda",
          "text": "Looking forward for the next open Mistral 8x22 !",
          "score": 29,
          "created_utc": "2026-01-29 19:46:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gs470",
          "author": "RoyalCities",
          "text": "I mean it makes sense. \n\nIve been training a new sample generator. It didn't cost me tons of electricity but as the model is released into the wild (and especially quantized down for lower compute) the cost goes down proportional to a models usage since it's then on device for others to use.\n\nBut most closed models don't want that and instead of trying to target local ARM infrastructure or just low VRAM systems on prem systems, they make giant massive models and hold them hostage behind pay walls.",
          "score": 45,
          "created_utc": "2026-01-29 19:24:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gx4f8",
              "author": "KitchenSomew",
              "text": "Exactly right. The model democratization is the key point. When models are released openly, the cost distribution happens naturally across the community. This is the real advantage of open source AI - not just transparency, but practical economics that make intelligence accessible to everyone, not just big players with server farms.",
              "score": 26,
              "created_utc": "2026-01-29 19:48:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hhozr",
                  "author": "Green-Ad-3964",
                  "text": "In fact.\n\nNever believe corporations saying that AI will make everyone wealthy and then keep their model closed and cloud-based.",
                  "score": 17,
                  "created_utc": "2026-01-29 21:26:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hbu4g",
              "author": "AggravatinglyDone",
              "text": "Sample as in music sample? Or something else? Which model?",
              "score": 1,
              "created_utc": "2026-01-29 20:58:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hepsc",
                  "author": "RoyalCities",
                  "text": "As in for music production / producers.\n\nThis is an older model of mine. The next will be much more capable.\n\nhttps://x.com/i/status/1864709213957849518",
                  "score": 5,
                  "created_utc": "2026-01-29 21:12:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gvime",
          "author": "AurumDaemonHD",
          "text": "Bravo. \n\nAdditionally i d say the privacy aspect is easily overlooked. But imagine google creates some way to take your convos with gemini and create a persona like you that ur family members can chat with but with ads :D and keeps it locked. As he said this is so important that it needs to be open source. Its ironic the corpo snakes were lobbying for lockdown on ai.\n\nGiving them free data about us in exchange for free services is actually good for them. Data is the new currency. \n\nYou could mine a persons social sites and llm chats and history and all and effectively create a clone of them to some degree. Personalized ads were just the beginning. But i digress...",
          "score": 16,
          "created_utc": "2026-01-29 19:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gvg8d",
          "author": "HugoCortell",
          "text": "He is not wrong, but at the same time, those are big words for a team that ultimately can't do much towards that. Because while FOSS models are on par if not better than closed-sourced ones, there is ultimately there's a hardware bottleneck between what consumers can attain (and afford) and what big companies have and withhold from us peasants. And those companies are working very hard to make access to good local hardware is every more prohibitively expensive.\n\nFor the change he seemingly is arguing towards, you'd need a company that can manufacture and distribute its own hardware. And Europe is in no such position, all prior attempts have failed. Maybe now is a good time to try again, but nobody has the money or willingness to take the risk.",
          "score": 17,
          "created_utc": "2026-01-29 19:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hv3z7",
              "author": "fuckingredditman",
              "text": "europe cannot manufacture it but europe has part of the supply chain (ASML, Zeiss IIRC also, ...) and is also generally in a good position to breed innovation through academia and small companies, there are a few working on intesting frontier technology like silicon photonics/photonic computing though of course it will take quite some time for those to become relevant\n\nand US and nvidia aren't independent either, there are by far not enough leading edge semiconductor plants in the US to manufacture all the hardware they want to deploy.\n\nbut yeah, all the more reasons to fund more of this kind of stuff to be more independent on hardware also ASAP",
              "score": 5,
              "created_utc": "2026-01-29 22:31:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2i0p0w",
                  "author": "HugoCortell",
                  "text": "We could, but likely never will. All prior attempts failed.\n\nEurope simply does not have the money to invest the necessary resources, or the organizational capacity to even ramp up the way China did. Yes, we have the academia, but it exists just so Nvidia can hire them. Because there's no money here for their efforts to be put to use.",
                  "score": -3,
                  "created_utc": "2026-01-29 22:59:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2h4c8d",
              "author": "rerri",
              "text": ">For the change he seemingly is arguing towards, you'd need a company that can manufacture and distribute its own hardware. And Europe is in no such position, all prior attempts have failed. Maybe now is a good time to try again, but nobody has the money or willingness to take the risk.\n\nI'm not sure I follow. Can't a European company just buy AMD/Nvidia GPU's and run their customized open-source model on those?\n\nedit: oh, I guess the part where he says \"you want to make sure your access to intelligence cannot be throttled\". Yeah, Europe is in a worse place than some other players in that regard.",
              "score": 7,
              "created_utc": "2026-01-29 20:22:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hjyy6",
                  "author": "sartres_",
                  "text": "Even without restrictions, European companies aren't in a good place to compete with American or even Chinese companies for GPU supply.",
                  "score": 1,
                  "created_utc": "2026-01-29 21:37:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2i01l6",
                  "author": "HugoCortell",
                  "text": "They can't. The US would rather go scorched earth on our asses than ever let us secure a strategic resource like chips. They'd never tolerate the sale, never forgive the steal. And if we tried to stand up on our own, they'd make sure to throw every wrench they can at it (not that it would be needed, we know how to shoot ourselves in the foot!).",
                  "score": -1,
                  "created_utc": "2026-01-29 22:56:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hg3vl",
              "author": "NewConfusion9480",
              "text": "The difference between 0 AI and the AI you can run on a 4070 is greater than the difference between the AI you can run on a 4070 and the AI you can run in a 1GW datacenter. Cannot divide by zero.\n\nThere is a decreasing marginal utility to intelligence for an individual person, and there is simply no need to maintain anything like parity vs massive industry figures selling intelligence as a service.\n\nYes, an F1 team can produce the quickest, fastest vehicle in the world. A Bugatti Chiron can go 300mph with over 1000hp. But what is the actual marginal utility to me of anything over 120HP? Absolutely nothing.\n\nYes, the military has massive artillery, but what's the actual marginal utility to me of anything more powerful than an AR?\n\nI want open-sourced local AI to improve, and it will, but it doesn't need to \"keep up\" with the frontiers.",
              "score": 12,
              "created_utc": "2026-01-29 21:19:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jk8bl",
                  "author": "zball_",
                  "text": "a model that runs on a 4070 has virtually no use for real world jobs, the real value it brings is zero in this regard. On the other hand AI running on data centers are creating real values.",
                  "score": 1,
                  "created_utc": "2026-01-30 04:07:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2huzq4",
                  "author": "Bakoro",
                  "text": "There's a massive difference between models of different abilities. It's not strictly about parameter size, but different classes of models have abilities that are more like a step function than a smooth gradient of capacity.  \n   \nWe already have fairly capable AI agents now.  \nWe're very close to having models that can use a computer more like a person would, where it can see the screen, move the mouse around, and just do arbitrary tasks. \n   \nThe \"marginal utility\" for a single person is that you're effectively not just one person anymore. If the AI agents are sufficiently capable, then having more agents becomes a nonlinear increase in power. You stop being just one person and can do things that it used to take whole teams to do.\n   \nI am already doing ~4X the work I used to, because the AI agents can take care of the things that are easy, but time-consuming.  \nI can do the thinking part, and the AI can do the grunt work.  \n   \nAs-is, I have been architecting whole projects that would have taken me years to on my own. \nThat would have been impossible with LLMs from 2 years ago.  \n   \nWe absolutely need open source that stays close to frontier.   \nI'm telling you, there's a step function in the complexity of tasks, and a step function in the things that you can do when you have more modalities.   \n  \nJust like you said, you can't divide by zero, and if the closed source models can use arbitrary computer programs and the open source ones can't, then there's an enormous gulf in power.",
                  "score": 0,
                  "created_utc": "2026-01-29 22:30:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2hw6a2",
              "author": "Evening_Ad6637",
              "text": "Well, don't forget that ASML is based in Europe, in the Netherlands.\n\nWithout ASML, there would be no Nvidia, no other GPUs, no smartphones, no modern laptops, and much more.\n\nAnd it's very interesting that ASML recently (in September 2025) invested $1.5 billion in Mistral AI. So to me, it looks like the Europeans are (MAYBE) not sleeping, but rather planning for the long term. In any case, there is no shortage of money in Europe.\nAnd I think that a super heavyweight like ASML (and, in my opinion, the only **real** monopoly in the world) can move mountains, and the signals I'm seeing make me confident about Europe/the EU.",
              "score": 3,
              "created_utc": "2026-01-29 22:36:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2jii0e",
              "author": "florinandrei",
              "text": "Let me summarize your comment real quick:\n\nYou can't win, so lie down and stop breathing.\n\nThat's... uh... inspiring. /s",
              "score": 2,
              "created_utc": "2026-01-30 03:56:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2hfyhr",
              "author": "goatyellslikeman",
              "text": "If an open model is heavily commoditized such that you can switch providers easily, then at least youâ€™re not beholden to a monopoly.",
              "score": 1,
              "created_utc": "2026-01-29 21:18:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2i0c83",
                  "author": "HugoCortell",
                  "text": "Unless that monopoly holds the hardware too. Which currently they are in the process on.\n\nHardware must be sovereign, otherwise the free software is wasted on it.\n\n(For the record, I didn't downvote you)",
                  "score": 2,
                  "created_utc": "2026-01-29 22:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kv3ly",
              "author": "FrostAutomaton",
              "text": "I get the impression that he is arguing for companies to use local LLMs, not individual consumers. In that case, maintaining your own server infrastructure becomes far more viable.",
              "score": 1,
              "created_utc": "2026-01-30 10:23:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oy8pt",
              "author": "jsonmeta",
              "text": "I think itâ€™s a matter of time and big tech knows it, thus they manipulate the hardware affordability and accessibility not only for their competitors but also for the end customer. Just like most people couldnâ€™t afford to have a personal computer at home back in the 80s",
              "score": 1,
              "created_utc": "2026-01-30 22:55:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2klmkt",
          "author": "TheTerrasque",
          "text": "A friend of mine learned this the hard way some days ago. He has subscription to most large AI vendors, but mostly use Claude in his daily work. So much he has the $200 per month tier.\n\nSome days ago he was working on recovering some client's old servers. Pulling out data and config, fixing things, and migrating to VM's, using Claude Code to help with it. \n\nIn the middle of that, his account got banned. He still doesn't know why, he still hasn't heard back from their customer service. And he was shocked they could just turn off his access like that. \n\nIt really highlights the point he's making though.",
          "score": 3,
          "created_utc": "2026-01-30 08:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hcicw",
          "author": "Possible-Machine864",
          "text": "He's not wrong.",
          "score": 6,
          "created_utc": "2026-01-29 21:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kp5qs",
          "author": "hugthemachines",
          "text": "The relevance of this for people outside USA increases with recent events. It is probably wise to have a backup plan in case USA blocks outside access to their AIs.",
          "score": 2,
          "created_utc": "2026-01-30 09:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hhyen",
          "author": "Legitimate-Pumpkin",
          "text": "How his accent naturally stresses his point: this is impogtant!",
          "score": 3,
          "created_utc": "2026-01-29 21:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h5yap",
          "author": "finah1995",
          "text": "Respect to him for saying it as it is and promoting companies to not be locked in. \n\nIn Strategic stuff you don't want your company affected. I mean no one wants what happened to companies like [Nayara Energy](https://www.nayaraenergy.com/) happens to them. Its more important, in government or semi-government it becomes more complicated than this.",
          "score": 4,
          "created_utc": "2026-01-29 20:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jcn2z",
          "author": "siegevjorn",
          "text": "Thanks for sharing. Glad to see someone in the mainstream talking about something that makes sense and something that humanity can hope for. It has been just so tiring to hear all the bs from the big names.",
          "score": 1,
          "created_utc": "2026-01-30 03:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kv213",
          "author": "Saltwater_Fish",
          "text": "Intelligence is not exactly the same as electricity. Intelligence is not fungible. The responses from models such as GPT, Claude, Kimi, DeepSeek all have their own unique characteristics.",
          "score": 1,
          "created_utc": "2026-01-30 10:23:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h57rd",
          "author": "eli_pizza",
          "text": "Sorta just a generic argument in favor of open source, no?\n\nI understand it's not so easy, but it would be a lot more powerful an argument if their models were fully open (training data and tooling). If Mistral cuts you off, you can keep using the already released models and I guess can fine-tune them on your own. But you can't rebuild them or easily use them as the basis for a new model.",
          "score": 0,
          "created_utc": "2026-01-29 20:26:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h7n5k",
          "author": "Hour_Bit_5183",
          "text": "That's because they are consuming all the power. This is nonsense. All the intelligence that exists is just in books they stole and life experiences.",
          "score": -6,
          "created_utc": "2026-01-29 20:38:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2l2pgv",
          "author": "According-Zombie-337",
          "text": "Those are some really good points. I just need to point out that Mistral is not viable as an option for this right now, and probably won't be for the foreseeable future.",
          "score": -1,
          "created_utc": "2026-01-30 11:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hnuf3",
          "author": "Ashley_Sophia",
          "text": "Laughs in Starlink\n\nOh wait....",
          "score": -4,
          "created_utc": "2026-01-29 21:55:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qn3xig",
      "title": "I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wky8vuufylfg1.jpeg",
      "author": "brandon-i",
      "created_utc": "2026-01-26 02:51:42",
      "score": 526,
      "num_comments": 159,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1rwv5b",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 06:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzd5p",
          "author": "DraconPern",
          "text": "Run 3 NextJS.",
          "score": 204,
          "created_utc": "2026-01-26 02:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r1pc2",
              "author": "brandon-i",
              "text": "Yeah that was where my logic was going because it has around 4 Terabytes of SSD and around one Petaflop of FP4 AI Performance I was thinking if I could run 4 if I really did better memory management and maybe partioned the unified memory.",
              "score": 40,
              "created_utc": "2026-01-26 03:09:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1r1tk4",
          "author": "randomfoo2",
          "text": "Try going through these: [https://github.com/NVIDIA/dgx-spark-playbooks](https://github.com/NVIDIA/dgx-spark-playbooks)",
          "score": 59,
          "created_utc": "2026-01-26 03:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qzr0b",
          "author": "l33t-Mt",
          "text": "What was your project at the hackathon?  Congratulations!",
          "score": 69,
          "created_utc": "2026-01-26 02:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r298o",
              "author": "brandon-i",
              "text": "My project was figuring out Social Determinants of Heath so that governments and organizations can find potentially unrelated relevant data and make actionable solutions with AI Agents. https://www.loom.com/share/375f4ba2ae9047d5911e41b763dbb4a9",
              "score": 167,
              "created_utc": "2026-01-26 03:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r2qjj",
                  "author": "Opteron67",
                  "text": "wtf",
                  "score": 75,
                  "created_utc": "2026-01-26 03:15:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r50rg",
                  "author": "ResolveSea9089",
                  "text": "That sounds lit. What's your educational background if you don't me asking? Where/how did you learn this stuff?",
                  "score": 46,
                  "created_utc": "2026-01-26 03:28:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r9t81",
                  "author": "Prigozhin2023",
                  "text": "Will you be open source it? The UI & work amazing!",
                  "score": 8,
                  "created_utc": "2026-01-26 03:56:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r83ck",
                  "author": "klenen",
                  "text": "Project alone needs its own post, very interesting/cool!",
                  "score": 6,
                  "created_utc": "2026-01-26 03:46:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r4dtl",
                  "author": "amejin",
                  "text": "So.. help me out here. Other than stt and tts, I'm assuming you have an LLM that parses the users response to some category or expected result, and the rest of this is just good old SE? It's interesting... Indo wonder if this is at risk of things like HIPAA, and where your data comes from for outreach... \n\nIt also seems you are implying that the bots are responsible for actual distribution and allocation of resources... Is that the case? Or is the idea to let humans be the arbiters of action and there is a trust factor for data aggregation and accuracy?",
                  "score": 6,
                  "created_utc": "2026-01-26 03:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1r8jd7",
                  "author": "Helpful-Magician2695",
                  "text": "How much time did you spend on this?   \nYou could have programmed the entire government in a month)",
                  "score": 4,
                  "created_utc": "2026-01-26 03:48:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1trxyp",
                  "author": "hugthemachines",
                  "text": "Sounds like you really deserve the reward.",
                  "score": 2,
                  "created_utc": "2026-01-26 14:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1vdm3l",
                  "author": "starkruzr",
                  "text": "hey uh. you looking for a job? ðŸ‘€",
                  "score": 2,
                  "created_utc": "2026-01-26 19:01:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rzgvy",
                  "author": "joelasmussen",
                  "text": "Rad!!!",
                  "score": 1,
                  "created_utc": "2026-01-26 06:55:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1shoq0",
                  "author": "Best-Mycologist3608",
                  "text": "I might want to implement this project, is it for sale?",
                  "score": 1,
                  "created_utc": "2026-01-26 09:35:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1tmt8h",
                  "author": "Exact-Emotion-1932",
                  "text": "Very cool! Would you mind sharing your stack / libraries you used primarily?",
                  "score": 1,
                  "created_utc": "2026-01-26 14:25:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2i6fne",
                  "author": "Green-Ad-3964",
                  "text": "Very interesting. May I contact you for academic projects? If yes... how?\n\n\nThanks in advance.",
                  "score": 1,
                  "created_utc": "2026-01-29 23:30:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1svzzz",
          "author": "Generatoromeganebula",
          "text": "https://preview.redd.it/2ee0hcsukofg1.jpeg?width=1180&format=pjpg&auto=webp&s=1834be0ddcae3ed3928b6d99ca20b1bebc67737b",
          "score": 20,
          "created_utc": "2026-01-26 11:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s3uv7",
          "author": "LicensedTerrapin",
          "text": "Sell it and buy 8gb ddr5",
          "score": 39,
          "created_utc": "2026-01-26 07:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rauo4",
          "author": "jadhavsaurabh",
          "text": "Try ltx video editing, u can run flux 2 also very well",
          "score": 15,
          "created_utc": "2026-01-26 04:02:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1riajl",
              "author": "brandon-i",
              "text": "Holy shit LTX is cool tyvm",
              "score": 10,
              "created_utc": "2026-01-26 04:50:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sa9iz",
                  "author": "jadhavsaurabh",
                  "text": "Sure. For idea I will say. Experiment. Create short films. Share on stable diffusion for inspiration",
                  "score": 3,
                  "created_utc": "2026-01-26 08:27:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1sa1lu",
              "author": "howardhus",
              "text": "> ltx\n\ncan you reccomend how to get started with video edit? im out of it..\n\ncomfy? or is it its own app",
              "score": 1,
              "created_utc": "2026-01-26 08:25:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1saflu",
                  "author": "jadhavsaurabh",
                  "text": "2 ways. You can direct run python scripts,\nBut comfy will be best way,\nDepending on ur ram. Vram \n\nChoose wisely, also checkout stable diffusion sub, for detail,\n\nOn civitai you will find workflows,\n\nLatest ltx is very light weight, and good competitor to wan",
                  "score": 1,
                  "created_utc": "2026-01-26 08:29:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rggzo",
          "author": "massive_rock33",
          "text": "Give it to me ðŸ’€",
          "score": 6,
          "created_utc": "2026-01-26 04:38:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r2z53",
          "author": "Fit-Produce420",
          "text": "On 128gb you can fine tune up to a 70B model.Â \n\n\nYou can also qlora larger models in the 120B range, like gtp-oss-120b. That's probably current best performer on that setup in terms of larger/smarter models.Â \n\n\nYou can run the big devstral 2 but it is slow being a dense model.Â ",
          "score": 20,
          "created_utc": "2026-01-26 03:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r3ky6",
              "author": "brandon-i",
              "text": "I might get sent two because they really liked my idea. So I was thinking maybe I can hook both of them up?",
              "score": 11,
              "created_utc": "2026-01-26 03:20:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rht1a",
                  "author": "thebadslime",
                  "text": "You can, they have a high speed link between the two",
                  "score": 7,
                  "created_utc": "2026-01-26 04:47:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rgzky",
          "author": "nofilmincamera",
          "text": "This is amazingly.  I am an idiot that sells stuff. But I have a 96 GB Blackwell,  I use for overkilled text analysis. \n\nHowever last year my Dad died waiting for an organ transplant,  my Wife was also dying, and recieved a no for posting.  I mention this because I navigated 7 different transplant evaluation processes and the amount of resources it took to get approval made it clear  that how much non medical social conditions matter more than they should. Fringe case approval or denial was proximity, support network, health insurance network. I have been wondering how that might be measured. Same kind of thing that let Steve Jobs post in every network in the country.",
          "score": 20,
          "created_utc": "2026-01-26 04:41:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rjlam",
              "author": "brandon-i",
              "text": "Btw, my last startup I exited finds fraud in healthcare bills. I can help if needed with any healthcare bills you may need adjudicated.",
              "score": 16,
              "created_utc": "2026-01-26 04:58:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rpx65",
                  "author": "nofilmincamera",
                  "text": "Luckily I am pretty good at that and had excellent insurance. But that is very kind and much needed in that industry.  I was lucky to have a network of people in the Med Industry also to pull from.  I am sure you found the solution is largely incompetence and bureaucracy. Lots of people that care.\n\nI do work now with a non profit working with families because navigating should not need experts, and essentially negotiating approval like a sales contract.\n\nYou don't sound like you need any help but if you do I could connect you with an Infomatics Program ( Nursing PHD program), could be partner opportunity. \n\nReally cool project.",
                  "score": 8,
                  "created_utc": "2026-01-26 05:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r2bcx",
          "author": "SituationMan",
          "text": "What don't you do with it?",
          "score": 5,
          "created_utc": "2026-01-26 03:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r31vj",
              "author": "brandon-i",
              "text": "Play Minecraft because itâ€™s a Linux device.",
              "score": 3,
              "created_utc": "2026-01-26 03:17:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1r3ayf",
                  "author": "trichocereal117",
                  "text": "Prism Launcher would like a word ;)",
                  "score": 6,
                  "created_utc": "2026-01-26 03:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rk4i3",
          "author": "GoodbyeThings",
          "text": "https://media1.tenor.com/m/s3XrUKl2a1oAAAAd/congrats-happy-for-you.gif",
          "score": 5,
          "created_utc": "2026-01-26 05:02:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5tfy",
          "author": "Sl33py_4est",
          "text": "I have one of those\n\nI use it for video models in comfyui, huge llm/vlms in llamacpp, and semi automated student distillation projects (WIP example tiny talking head distilled from liveportrait+ditto)\n\nwhat are you gonna do with it",
          "score": 3,
          "created_utc": "2026-01-26 03:33:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1r7an7",
              "author": "brandon-i",
              "text": "I was probably just going to create a Tailscale instance and let my friends use it via SSH.",
              "score": 6,
              "created_utc": "2026-01-26 03:41:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1sh1ac",
                  "author": "Sl33py_4est",
                  "text": "i dont like my friends enough for that ðŸ˜…",
                  "score": 1,
                  "created_utc": "2026-01-26 09:29:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rcx1w",
          "author": "fire_inabottle",
          "text": "If you want to code, glm-4.5-Air 4-bit quantized is awesome and if you put your KV cache into fp8 you can get 128k context.  https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 3,
          "created_utc": "2026-01-26 04:15:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rhl1y",
          "author": "thebadslime",
          "text": "I would use it for endless finertunes",
          "score": 3,
          "created_utc": "2026-01-26 04:45:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rt5e9",
          "author": "brandon-i",
          "text": "I also, found another use case for it. I can do transfer learning and RL for some robotics hackathons. If anyone has cool ideas that they want to work together on or see come to life I would love to hear them. Send me a DM.",
          "score": 3,
          "created_utc": "2026-01-26 06:06:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1rlhyz",
          "author": "brandon-i",
          "text": "I have a serious question. How long will this DGX Spark last me? Will models get smaller and better so I can run open source ones locally that will be as good or better than Claude 4.5 Opus before this unit becomes obsolete?\n\nOr is the assumption that they will get bigger and will still not fit onto my DGX?\n\nI have heard that we will have a much better small language models that can just be as good because they are fine tuned for specific tasks like coding and my assumption is that this DGX can last a long time (assuming it doesnâ€™t brick. We had 2-3 brick at the Hackathon from some vibe coders using Claude Code)\n\nOn Friday I spoke to an engineer at Google Deepmind saying they already have â€œinfinite context lengthsâ€ or what not and I wasnâ€™t really convinced, but Iâ€™m not sure what to believe since he was google deepmind.",
          "score": 2,
          "created_utc": "2026-01-26 05:11:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rw0mo",
              "author": "Ok_Difference_4483",
              "text": "Smaller and smarter models for sure. Step3-VL-10B is a clear example that a much smaller model 10-20x compared to big MOEs that they could perform much better than what we currently have today and we have barely scratched the surface. Kimi Linear, Deepseek V3.2 DSA is also very experimental/new but we are definitely moving towards longer context/and still with good quality. Engram will accelerate this even further.\n\nI was wondering if I could inbox/speak more about this. I am looking into improving models like GPT-OSS by converting it into an MLA/DSA model/Converting it into a Diffusion model/NVFP4 KV Cache, or even just doing REAP/Pruning on the model, I have a 90B GPT-OSS model(Pruned from original 120B), and Iâ€™m trying to maybe get it down to even 60B. All in all, Iâ€™m really looking into getting more resources/Compute for doing this kind of work. Wondering if you can help/ or know anyone who I might be able to get support from?",
              "score": 2,
              "created_utc": "2026-01-26 06:28:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1se31o",
              "author": "inteblio",
              "text": "Congratulations & good work.\n\nProbably you can see both sides. Big get bigger, small get smaller. But all are getting better.\n\nMy hunch is that \"orchestration\" is the angle to watch. So, effecient C logic that becomes like \"hardwire\" between the flexible \"tentacles\" of LLMs. Bringing immense effeciency,speed, and capability boosts. AI coded, obviously. We're talking about pseudo-AI density C. I mean, in a fanciful near-future... the AI writes code to perform the tasks it needs to, right? Thus speeding up its own \"paths\". This is a cloudy \"direction\" take, to your question about the future. \n\nYour hardware likely has some good lifespan. But the AI space is going to go NUTS from here in. \n\nSolve more world!",
              "score": 2,
              "created_utc": "2026-01-26 09:02:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sfjhe",
              "author": "ProfessionalSpend589",
              "text": "If your budget allows it I would consider buying a second DGX to pair it with.\n\nIt would allow for bigger general-purpose LLMs. And just offers more RAM for tasks. Maybe NVidia offers speed ups Â when you cluster them (I vaguely remember that was the case, but do your own research).",
              "score": 2,
              "created_utc": "2026-01-26 09:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sdi2c",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-26 08:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uhmzr",
                  "author": "brandon-i",
                  "text": "It wasnâ€™t anything irreversible. I think they were just doing stupid shit like Claude Code on yolo mod in root. It was 2-3 out of like 25-30. I think also there was an issue with like some sort of adapters. These machines get reflashed for every event so maybe that contributes to it as well.\n\nIt didnâ€™t seem like it was any sort of over clocking issue. These things handled some crazy shit. My friend was really like putting it through the wringer.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:45:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uhnwh",
              "author": "dobkeratops",
              "text": "I agree there will be specific finetunes that will let a smaller box punch above it's weight in one domain, but also remember they can be clustered, regarding \"how long will it last\". Out of the box they can be paired up directly",
              "score": 1,
              "created_utc": "2026-01-26 16:45:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sdmm9",
          "author": "jacek2023",
          "text": "s/vllm/llama.cpp and then Nemotron will use 30GB of memory instead 100GB",
          "score": 2,
          "created_utc": "2026-01-26 08:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uitz7",
              "author": "brandon-i",
              "text": "Thanks so much for this one. I kept trying to optimize it by changing the GPU memory but only got it down to 90 GB. Should I use this for all of the other models too?",
              "score": 1,
              "created_utc": "2026-01-26 16:50:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sfj9n",
          "author": "foldl-li",
          "text": "Put it into a box, write down my address, and send it to UPS.",
          "score": 2,
          "created_utc": "2026-01-26 09:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sixif",
          "author": "Legitimate-Pumpkin",
          "text": "Itâ€™s sort of funny you won it and donâ€™t know what to do with it. Who chose the prizes for the hackathon? ðŸ¤£",
          "score": 2,
          "created_utc": "2026-01-26 09:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1um6rh",
              "author": "brandon-i",
              "text": "Dell and Nvidia. I actually didnâ€™t know what the prizes were to be honest. This is just a really hard problem that I wanted to solve and they gave me an opportunity to do it with a super computer which I usually donâ€™t get.",
              "score": 3,
              "created_utc": "2026-01-26 17:04:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1uuerq",
                  "author": "Legitimate-Pumpkin",
                  "text": "Congrats on the opportunity then. It must have been quite satisfying :)",
                  "score": 3,
                  "created_utc": "2026-01-26 17:40:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r6bat",
          "author": "brandon-i",
          "text": "Also, I was literally only able to get max 45 TKS with the nemotron 30B with VLLM. Is best solution to just local inference with Ollama?",
          "score": 1,
          "created_utc": "2026-01-26 03:35:58",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1rwh40",
              "author": "Ok_Difference_4483",
              "text": "Try Sglang, and I think even GPT-OSS-120B is even faster and better than Nemotron 30B, I was getting around 150 TK/s on an H100 before? And around 100-120TK/s on blackwell sm120",
              "score": 1,
              "created_utc": "2026-01-26 06:32:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u3zn9",
              "author": "smflx",
              "text": "Sglang will be little faster than vllm. Well, the problem is Spark is slow in token generation due to slow memory. So, try fp8 or awq with vllm or sglang, or even smaller quantz with llama.cpp.\n\nPersonally I avoid ollama.",
              "score": 1,
              "created_utc": "2026-01-26 15:46:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1u4bc6",
              "author": "smflx",
              "text": "Oh, btw congratulation to your winning hackathon",
              "score": 1,
              "created_utc": "2026-01-26 15:47:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uq3b7",
              "author": "Freonr2",
              "text": "It's not a blazing fast box, for LLM inference it is mostly limited by the memory bandwidth. \n\nThe trade off compared to a GPU is you get a lot more memory but it is much slower memory.",
              "score": 1,
              "created_utc": "2026-01-26 17:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rcaxx",
          "author": "fire_inabottle",
          "text": "https://www.reddit.com/r/LocalLLaMA/s/Q9Top4MdxW",
          "score": 1,
          "created_utc": "2026-01-26 04:11:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rdm6q",
          "author": "vmspionage",
          "text": "I'm thinking about buying either a spark or jetson thor soon and using the guts for my GRiD 1520 minipc/cyberdeck.  Mostly for slow large model inference, fine tuning, other genai experimentation garbage and looking good",
          "score": 1,
          "created_utc": "2026-01-26 04:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rhlrp",
              "author": "nofilmincamera",
              "text": "I thought about a Jetsoj cyberdeck. Sounds fun",
              "score": 1,
              "created_utc": "2026-01-26 04:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rjljk",
          "author": "rm-rf-rm",
          "text": "Looks like you are SF/bay area based. If you are open to collaborating on fine tuning models with local data, DM me",
          "score": 1,
          "created_utc": "2026-01-26 04:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rm6gd",
              "author": "brandon-i",
              "text": "Iâ€™m not quite sure what the ask is, but feel free to reach out.",
              "score": 1,
              "created_utc": "2026-01-26 05:16:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rs6a5",
          "author": "Icy_Foundation3534",
          "text": "Sell it to me? No seriously just try models on hugging face.",
          "score": 1,
          "created_utc": "2026-01-26 05:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s428n",
          "author": "wildyam",
          "text": "Sell it",
          "score": 1,
          "created_utc": "2026-01-26 07:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1scedy",
          "author": "Mart-McUH",
          "text": "Hack it, obviously...",
          "score": 1,
          "created_utc": "2026-01-26 08:47:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1slduu",
          "author": "Maximum_Transition60",
          "text": "it's beyond me how you guys have NextJS using 60gb+ wtffff",
          "score": 1,
          "created_utc": "2026-01-26 10:09:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1smppe",
          "author": "fabkosta",
          "text": "Put it to a throughput test with an increasing number of users making requests to the model.\n\nI'm still waiting to see such things done. Everyone seem to focus on TG and PP, which is cool, but I want to know how it behaves regarding concurrency of requests.",
          "score": 1,
          "created_utc": "2026-01-26 10:21:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1snezg",
          "author": "FastDecode1",
          "text": "Sell it and live like a king for 20 years.",
          "score": 1,
          "created_utc": "2026-01-26 10:27:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1szqsv",
          "author": "tamal4444",
          "text": "congrats",
          "score": 1,
          "created_utc": "2026-01-26 12:09:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t8zi0",
          "author": "yensteel",
          "text": "Just don't tell your company you won one. \n\nE.g. \"Employee quits job over an Nvidia RTX 5060 â€” intern refused to hand in GPU won on an all-expense-paid business trip\"",
          "score": 1,
          "created_utc": "2026-01-26 13:11:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unjiu",
              "author": "brandon-i",
              "text": "I own my companies :)",
              "score": 1,
              "created_utc": "2026-01-26 17:10:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tec9v",
          "author": "michael554466",
          "text": "That's so awesome! Congratz!",
          "score": 1,
          "created_utc": "2026-01-26 13:41:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1th1of",
          "author": "sheikyon_",
          "text": "Run DeepSeek-R1-Distill-Llama-70B. NVIDIA even recommends running it on DGX Spark.",
          "score": 1,
          "created_utc": "2026-01-26 13:56:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tiqdk",
          "author": "AdDizzy8160",
          "text": "... prepare for a second hackathon, or be prepared for buying a second one ;)",
          "score": 1,
          "created_utc": "2026-01-26 14:04:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1unsnk",
              "author": "brandon-i",
              "text": "Yeah! I am already registered for a few hackathons that I can win a Unitree robot dog and some other robotics hardware! I just need to figure out a cool project to do.",
              "score": 1,
              "created_utc": "2026-01-26 17:11:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tj9tp",
          "author": "JsThiago5",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:07:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tk3j3",
          "author": "StardockEngineer",
          "text": "Give it to me",
          "score": 1,
          "created_utc": "2026-01-26 14:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tklpu",
          "author": "wrath_Hog-",
          "text": "Rent me",
          "score": 1,
          "created_utc": "2026-01-26 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tmn2a",
          "author": "celsowm",
          "text": "sell it and got money",
          "score": 1,
          "created_utc": "2026-01-26 14:24:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tnmwt",
          "author": "TerminatedProccess",
          "text": "Nvidia just released and open source Persona talking model. Maybe you can work on interaction with humans for your project.",
          "score": 1,
          "created_utc": "2026-01-26 14:29:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tr1jf",
          "author": "HerrGronbar",
          "text": "Sell it and buy a car.",
          "score": 1,
          "created_utc": "2026-01-26 14:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1un6dk",
          "author": "NoWen7252",
          "text": "Sell it & buy nVidia stocks",
          "score": 1,
          "created_utc": "2026-01-26 17:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uvr92",
          "author": "NaiRogers",
          "text": "Sell it and buy a 6000 Pro!",
          "score": 1,
          "created_utc": "2026-01-26 17:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uwwr1",
          "author": "GCoderDCoder",
          "text": "Can you train nemotron to work? \n\nJkjk But seriously has anyone had consistent success on tool calls with nemotron 30b with thinking off? I got tired of experimenting and it thinks too much for me to use its speed in instruct mode.",
          "score": 1,
          "created_utc": "2026-01-26 17:51:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vjkev",
          "author": "reddit-369",
          "text": "Sell it and buy L40",
          "score": 1,
          "created_utc": "2026-01-26 19:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vuk4j",
          "author": "brandon-i",
          "text": "For those of you interested I just wrote a blog post about why/how I knew how to solve the problem. [https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn](https://thehealthcaretechnologist.substack.com/p/mapping-social-determinants-of-health?r=18ggn)",
          "score": 1,
          "created_utc": "2026-01-26 20:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1w6hh6",
          "author": "klapperjak",
          "text": " To had the shittiest project too, editing the demo video to win with a fake project oml. Also bro sshâ€™d into every other teams dgx and changed the password super anti-competitive",
          "score": 1,
          "created_utc": "2026-01-26 21:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wnh0c",
          "author": "SnowyOwl72",
          "text": "can it run crysis?",
          "score": 1,
          "created_utc": "2026-01-26 22:23:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x4x8z",
          "author": "goingsplit",
          "text": "if you have a lot of millionaire friends i think you should be able to raise enough computing resources, or?",
          "score": 1,
          "created_utc": "2026-01-26 23:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y53zc",
          "author": "Low_Cycle_4582",
          "text": "Run agent zero into it and make it your bitch ... Now u have super power to do anything ...",
          "score": 1,
          "created_utc": "2026-01-27 03:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yhrqo",
          "author": "Torodaddy",
          "text": "Sell it while people are still jazzed to buy it",
          "score": 1,
          "created_utc": "2026-01-27 04:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23rfnl",
          "author": "rc_ym",
          "text": "Give it to a random reddit commenter... Hint. Hint. :P",
          "score": 1,
          "created_utc": "2026-01-27 22:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qjiri",
          "author": "FinancialMoney6969",
          "text": "Download kimi k2.5 run locally add clawdbot lobsterbot have an ai army at your disposal",
          "score": 1,
          "created_utc": "2026-01-31 04:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rn3qk",
          "author": "ismaelgokufox",
          "text": "Put Clawdbot on it for research purposes ðŸ˜‰",
          "score": 1,
          "created_utc": "2026-01-26 05:22:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rnsis",
              "author": "brandon-i",
              "text": "If youâ€™re serious I would actually do it. For science.\n\nThe only issue is I have seen what Claude Code did to other DGX sparks. We couldnâ€™t reflash them after the bricked.",
              "score": 1,
              "created_utc": "2026-01-26 05:27:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rnxs5",
                  "author": "ismaelgokufox",
                  "text": "I just did minutes ago and itâ€™s interesting! Have yet to do something specific with it but looks promising.",
                  "score": 2,
                  "created_utc": "2026-01-26 05:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1xf57e",
              "author": "bigh-aus",
              "text": "alternately use it to host models for clawdbot. I'm currently running it (day 0) on a vm, that's pointing back to ollama on my gaming rig (single 3090).  Super interesting ... but damn it's dangerous.",
              "score": 1,
              "created_utc": "2026-01-27 00:42:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rsl3v",
          "author": "Dry-Yogurtcloset4002",
          "text": "Sell it. You can barely do any serious AI works with it. Unified memory is a joke.",
          "score": 1,
          "created_utc": "2026-01-26 06:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sf9bi",
              "author": "Regular-Forever5876",
              "text": "soooo wrong!!! ðŸ¤£ðŸ¤£ðŸ¤£ I have 2 DGX and 1 Thor and there is REALLY ANYTHING better for the prince tag to thinkering!!",
              "score": 1,
              "created_utc": "2026-01-26 09:13:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s8otz",
          "author": "AVX_Instructor",
          "text": "Try running GLM 4.7 REAP (Q1/2/3/4), this is SOTA for self hosted",
          "score": 1,
          "created_utc": "2026-01-26 08:13:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sg0hp",
          "author": "Hearcharted",
          "text": "NVDA should hire/sponsor you, forever :)\n\nSo you can keep doing your thing to help society ;)",
          "score": 1,
          "created_utc": "2026-01-26 09:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ukslv",
              "author": "brandon-i",
              "text": "You know, I havenâ€™t been the best as selling myself so I donâ€™t know what the huge value add is if companies sponsored me. Because thereâ€™s the societal impact I can make, but often times businesses try to figure out how that aligns with their business strategy. For Nvidia maybe it makes sense. Iâ€™ll talk to their team.",
              "score": 2,
              "created_utc": "2026-01-26 16:58:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xe741",
                  "author": "Hearcharted",
                  "text": "ðŸ¥‡",
                  "score": 1,
                  "created_utc": "2026-01-27 00:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sgsd8",
          "author": "Easy_Kitchen7819",
          "text": "sell it and buy normal hardware for llm",
          "score": 0,
          "created_utc": "2026-01-26 09:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r7f6o",
          "author": "Pretty_Challenge_634",
          "text": "Mail it to me, duh.",
          "score": -2,
          "created_utc": "2026-01-26 03:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r3f8p",
          "author": "stacksmasher",
          "text": "You better just send it to me for testing lol!",
          "score": -1,
          "created_utc": "2026-01-26 03:19:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r5fo8",
          "author": "Lorelabbestia",
          "text": "Sell it to me! 2K offer",
          "score": -6,
          "created_utc": "2026-01-26 03:30:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rk6se",
              "author": "GoodbyeThings",
              "text": "No no, to me. 1K offer",
              "score": 3,
              "created_utc": "2026-01-26 05:02:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrsy4q",
      "title": "How close are open-weight models to \"SOTA\"? My honest take as of today, benchmarks be damned.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/k38sg20q7mgg1.png",
      "author": "ForsookComparison",
      "created_utc": "2026-01-31 04:49:42",
      "score": 513,
      "num_comments": 177,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2sgve9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 14:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs2xu",
          "author": "Bananadite",
          "text": "Honestly so disappointed in Meta.  They were the ones who kickstarted this open source weights and they spent so much to assemble an AI \"superteam\" yet they failed so hard.",
          "score": 150,
          "created_utc": "2026-01-31 05:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qv8cd",
              "author": "ForsookComparison",
              "text": "I'll be devil's advocate for Llama4:\n\nthere was a brief moment where Llama4 Maverick through Lambda Labs was the cheapest and fastest way to do code completions (think: the \"Continue\" VSCode Extension or early Copilot). If you already knew what you were going to hand-write Maverick could probably nail it faster and cheaper than anything else.\n\nThis didn't last long at all.. but Maverick got some actual use from me before Qwen3.",
              "score": 53,
              "created_utc": "2026-01-31 06:00:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2w0p9h",
                  "author": "dreamkast06",
                  "text": "I too will advocate for it. It was faster and had more \"knowledge\" than the og Deepseek 3. It had implementation issues when it first came out like GPT-OSS-120B. Remember all the hate that got when it first came out cuz it seemed stupid until the templates were fixed?\n\nMakes me wonder if the multimodal aspects of Llama4 were more widely implemented maybe it'd gotten more use.",
                  "score": 1,
                  "created_utc": "2026-02-01 00:49:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2tzwa2",
              "author": "RedTheRobot",
              "text": "The open source play was never to benefit them but to hurt the competition. That is why at the beginning they didnâ€™t invest heavily in AI. Zuck was still in his metaverse and pushing the Oculus. AI was just a side project. Then AI started making some serious money and meta had to jump in at the last second. They threw out huge paydays to again bump themselves up and hurt the competition but the good employees are heavily vested in OpenAI and Google.",
              "score": 6,
              "created_utc": "2026-01-31 18:38:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wqa1u",
                  "author": "iJeff",
                  "text": "Yann LeCun was the one advocating for the open approach.",
                  "score": 1,
                  "created_utc": "2026-02-01 03:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rfoi1",
              "author": "Nyghtbynger",
              "text": "Yann Lecun really is in the shadow of all the big changes in Artificial Neural Networks for 30 years now",
              "score": 14,
              "created_utc": "2026-01-31 09:04:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s4cdl",
                  "author": "aeroumbria",
                  "text": "He felt like an LLM sceptic stuck in a company that never cared about what he really wanted to do.",
                  "score": 10,
                  "created_utc": "2026-01-31 12:45:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tlsu6",
                  "author": "rabbotz",
                  "text": "I suspect both Meta and OpenAI, in their own ways, got too distracted with the AGI â€œwhatâ€™s nextâ€ question. I get where they are coming from, the current architectures will eventually hit a wall we will really want to get past (eg from the lack of a world model). But thereâ€™s so much juice to be squeezed on iterating on current models and the ecosystems around them, it ended up looking like a distraction in retrospect.",
                  "score": 5,
                  "created_utc": "2026-01-31 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ue5fk",
              "author": "kevin_1994",
              "text": "scout was not a bad model, imo. it was just released at the wrong time. it released when MoEs, reasoning models, and agentic behaviour were first beginning to emerge. it's MoE structure was more \"old-school\" like Mixtral, it didn't have reasoning, and it wasn't post trained for agentic behaviour.\n\ndespite all this, the llama models are some of the only models not trained to benchmaxx. meta still serves their llama models to billions of users. my boomer parents love that shit lol. these models not are optimized for coding and stem -- they are optimized for user engagement for more casual users",
              "score": 1,
              "created_utc": "2026-01-31 19:46:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2w1xyd",
              "author": "midnitewarrior",
              "text": "I forgot Meta was even in this. oof",
              "score": 1,
              "created_utc": "2026-02-01 00:56:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sjtbp",
              "author": "Party_Progress7905",
              "text": "They should just give up and invest in others. Theyâ€™re only wasting resources, too far behind, and theyâ€™re actively enshittifying their own products by forcing their shit AI into everything. Given Metaâ€™s track record, itâ€™s **likely** WhatsApp and ig  messages are being repurposed beyond messaging.",
              "score": -1,
              "created_utc": "2026-01-31 14:22:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qzyuy",
          "author": "LoveMind_AI",
          "text": "For whatever it's worth, Kimi K2.5 is performing basically as well as Gemini 3 Flash for me on visual reasoning.",
          "score": 40,
          "created_utc": "2026-01-31 06:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rq733",
              "author": "SilentLennie",
              "text": "And it's cheaper (but Gemini 3 Flash has that 1M context window which is useful at times of course).",
              "score": 6,
              "created_utc": "2026-01-31 10:45:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2trpiz",
              "author": "SnooDoggos9325",
              "text": "At coding not so much. I asked k2.5 with Opencode to implement a feature. It did quite well, but there was a glitch. I asked for a fix and it made things worse. I reverted the last fix and asked Gemini 3 flash to fix it. It immediately figured out the problem and provided a proper solution.Â \nThe feature was to add a slightly transparent imgui overlay with fps and frame time percentiles in the top right corner. C++ and sdl3 GPU.\nThe glitch was with incorrect texture formats causing the overlay to be 90% transparent.Â \nAsking Kimi to fix it (twice) resulted in a hallucination that now it is fully opaque and later extending the background to full screen instead of jus the overlay window.",
              "score": 1,
              "created_utc": "2026-01-31 18:00:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2quac7",
          "author": "randombsname1",
          "text": "Good list. Largely agree.\n\nFirst big release of SOTA models for this year is around the corner too. \n\nYou have leaks of ChatGPT 5.3. Claude Sonnet 4.7 and Gemini 3.5.\n\n\nI think each of the big 3 is waiting to see who goes first. Then they'll all release within a week or 2 of the first one, imo.",
          "score": 125,
          "created_utc": "2026-01-31 05:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qwvdc",
              "author": "Mescallan",
              "text": "OpenAI certainly is. Google hasn't really been known to stomp on releases as much as the other two. Anthropic has been doing it more recently, but only towards OpenAI IIRC.   \n  \nMy bet is Gemma 4 -> Sonnet 4.7 -> GPT 5.3 within 48 hours -> Gemini 3.5/Opus 4.7 in March.",
              "score": 40,
              "created_utc": "2026-01-31 06:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tm6wz",
                  "author": "segmond",
                  "text": "Google has no pressure to release gemma-4, llama drove gemma.  with llama dead and phi not releasing.   Google isn't threatened by mistral-small or qwen3-30b/32b.  They have demonstrated that they can do opensource, the investors don't care.    There's a reason Anthropic doesn't do open source, nothing in it for them.  The only reason OpenAI did gpt-oss is because of the Elon Musk lawsuit so they can claim they are still open.",
                  "score": 12,
                  "created_utc": "2026-01-31 17:33:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qys8j",
                  "author": "kkb294",
                  "text": "Are you sure about Gemma.? I'm eagerly waiting for the OSS ones as there is nothing in sight from the other(Claude/OpenAI/Meta) providers.",
                  "score": 4,
                  "created_utc": "2026-01-31 06:29:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rhi73",
              "author": "__Maximum__",
              "text": "Sir, this is a r/localllama",
              "score": 21,
              "created_utc": "2026-01-31 09:21:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rxxax",
              "author": "goniszewski",
              "text": "Yeah, nice list. Actually this could be a nice addition for each occurrence I am adding to this list: https://ithappenedagain.fyi/rec/new-sota-model-released-211624384e\n\nIâ€™m thinking about splitting it also into closed models and the open ones, which we can host ourselves. Could be useful for some folks out there.",
              "score": 3,
              "created_utc": "2026-01-31 11:54:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qrpbi",
          "author": "TheRealMasonMac",
          "text": "More or less, yeah. I'd say that OSS models need to invest compute in better instruction following rather than intelligence. It clearly paid dividends for Claude.",
          "score": 80,
          "created_utc": "2026-01-31 05:32:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r2sgq",
              "author": "ihexx",
              "text": "Idk what Claude's secret sauce is, but it's not instruction following.\nIdk if you remember but a big complaint everyone had with Claude is it doesn't do what you ask it to and it's too eager to do extra stuff.\nIf you try to measure instruction following, Claude scores badly; bottom of the big 3.\n\n\nThere's something else here that's giving Claude that advantage; something more like \"meta-problem understanding\" or like \"inferring missing information\". But it's not \"instruction following\"",
              "score": 55,
              "created_utc": "2026-01-31 07:04:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2r4cqx",
                  "author": "kzoltan",
                  "text": "Guessing only: huge amounts of data from devs? Claude is arguably the best model for devs, so people tend to choose that. \nI havenâ€™t seen the fine print, can they use the data from subscriptions for training purposes?",
                  "score": 20,
                  "created_utc": "2026-01-31 07:18:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2r509j",
                  "author": "TheRealMasonMac",
                  "text": "Idk, for me it's always been the best at following instructions for coding. Whereas other models try to do things cleverly, it chooses the most straightforward solution that satisfies my requirements. Other models try to be clever and consequently fail to follow instructions because they're so laser-focused on being clever. If I tell it not to do something, it gets what I'm really telling it not to do without overreaching, and so on. It's also able to maintain instruction following coherency across dozens of turns whereas other models fall apart completely in just a few.",
                  "score": 13,
                  "created_utc": "2026-01-31 07:24:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rwtxd",
                  "author": "AuspiciousApple",
                  "text": "Yeah, claude occasionally feels like a smart person running with your task. Sometimes I give it a complex problem and it does 20 steps before stopping, when I wanted 1-2 steps.",
                  "score": 4,
                  "created_utc": "2026-01-31 11:45:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2s9d7a",
                  "author": "AlwaysLateToThaParty",
                  "text": "Claude is their workflows. That's what it feels like to me.  They seem to have embedded them in their reasoning, so that all tasks are broken down, but not too much. That lends itself to coding.\n\nI would say that this is code-centric.  While code is good, there are other ways to use llms and 'ai' (like image/video, role play, translation, domain specific analysis, etc etc), that will have a different tier list entirely.",
                  "score": 5,
                  "created_utc": "2026-01-31 13:19:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rd0s2",
                  "author": "Any_Fox5126",
                  "text": "I don't know about recently, but around the time of 3.5, it also used to be one of the most aggressively censored.",
                  "score": 2,
                  "created_utc": "2026-01-31 08:39:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2shsvz",
                  "author": "Far-Low-4705",
                  "text": "In my experience, i feel maybe older, dense models were able to understand/grasp context better. I think pretraining baked it into the model, then post training RL kind of beat that out of them (or at least made them worse at the ability since it is no longer the primary learning signal)",
                  "score": 2,
                  "created_utc": "2026-01-31 14:10:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2tk2ad",
                  "author": "kaisurniwurer",
                  "text": "I'm ~69% sure that they are using massive activation moe.\n\nI would not be surprised to hear that their model have 100-400B active tokens. If it isn't just straight up massive dense model.",
                  "score": 1,
                  "created_utc": "2026-01-31 17:23:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ttwv2",
                  "author": "jklre",
                  "text": "My favorite behavior of Claude is when im working on a new complex problem it basically gets to a point when things are hard and its just like \"How about you just don't?\" and tries to get out of it",
                  "score": 1,
                  "created_utc": "2026-01-31 18:10:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2umeto",
                  "author": "BadgKat",
                  "text": "My honest opinion is that Claude is so good for two main reasons. \n\nThe first is harnessing, I think this is the main reason there is a benchmarking to perceived performance gap. Anthropic has built some of the best tooling, both in Claude Code and built in tooling in the browser/app. Models performance is improved when they have clear ways to do things, like create a script for themselves and run it in a sandbox and give the user an output document. The benchmarks use the api, you donâ€™t get those harnesses in the api.\n\nThe second one is a bit harder to prove, but I believe it. I think itâ€™s the SOUL doc. The study that showed when you RL a model to write malicious code and it starts talking like a Nazi proves the inverse. It stands to reason you RL a model to be moral, it writes better quality code, and does other things well. Harder to empirically prove this inverse, but I think it at least follows.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:26:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2s5xf7",
              "author": "aeroumbria",
              "text": "I would say maybe we should treat some diversity in \"obedience\" as a strength rather than deficiency. For instance, DeepSeek V3.2 is not particularly good at following instructions because it doubts itself too much and will spend too much time debating how to interpret the instructions, but this also makes it ideal for cross-checking results from other models, just because the \"mental model\" of DeepSeek is so distinct from other models, and it does not take even its own reasoning for granted.",
              "score": 2,
              "created_utc": "2026-01-31 12:56:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rqw66",
              "author": "hellomistershifty",
              "text": "That's what gets me with Gemini, I don't get how it scores so high. It programs great solutions to problems that didn't exist and uses tools like a caveman",
              "score": 2,
              "created_utc": "2026-01-31 10:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tfd2m",
              "author": "RhubarbSimilar1683",
              "text": "Claude definitely takes more compute to run. It all adds up: it's slower. Has lower free rate limits. Uses custom Annapurna labs AWS hardware. Has higher API pricing. Maybe the model has a diffusion LLM component while being largely autoregressive. They could then train RL on that.Â  Or the transformer architecture has a symbolic ai component and/or a diffusion component because it sometimes adjusts its text generation speed. Then train RL on that.Â \n\n\nThe custom AWS hardware has somewhat mitigated the speed issue but it doesn't add up to the higher speed of the hardware.Â  But it could also be them trying to be profitable. But I doubt it because otherwise they would nuke their models like openai with gpt-5\n\n\nÂ I still remember they had those issues during their Nvidia days in 2024 and I don't think that's changed. I don't think those things can be fixed with better infrastructure. They need hardware.Â ",
              "score": 1,
              "created_utc": "2026-01-31 17:00:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2tnubx",
              "author": "CuriouslyCultured",
              "text": "I don't feel like Claude is very good at instruction following. It's superpower is taking under specified problems and producing \"good enough\" output. GPT5.2 is anal about requirements, whereas Claude treats them as suggestions, and cares more about \"flair\"",
              "score": 1,
              "created_utc": "2026-01-31 17:41:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qt1jk",
          "author": "TomLucidor",
          "text": "You need to make another tier list for the <120B model and <48B models when compared to proprietary SOTA's smaller equivalents.",
          "score": 47,
          "created_utc": "2026-01-31 05:42:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qt7yf",
              "author": "ForsookComparison",
              "text": "Will do. I have a totally separate set of projects that are hosted on my janky home server, so I've got plenty of hot takes for models of that size.",
              "score": 15,
              "created_utc": "2026-01-31 05:44:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2rtl1i",
                  "author": "Competitive_Ad_5515",
                  "text": "!remindme 1 week",
                  "score": 1,
                  "created_utc": "2026-01-31 11:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rxi6l",
              "author": "goniszewski",
              "text": "We should probably distinguish between SOTA (closed), and Open SOTA (open source or open weights). The latter maybe with a reference from the first one. \nIâ€™m always researching new candidate for my locally run model and that would be very helpful.",
              "score": 2,
              "created_utc": "2026-01-31 11:50:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rf50v",
          "author": "LosEagle",
          "text": "Mr. anus logo is the overlord, sadly.Â ",
          "score": 14,
          "created_utc": "2026-01-31 08:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qolio",
          "author": "ThatRandomJew7",
          "text": "Have you tried Kimi K2.5 yet? Because it's a pretty big step up, in my experience it outperforms Claude Sonnet and sometimes trades blows with Opus",
          "score": 53,
          "created_utc": "2026-01-31 05:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qoyur",
              "author": "ForsookComparison",
              "text": "I have. I hold Deepseek V3.2 in very high standing so take my word that it's a big deal that Kimi sits where it is on this chart.\n\nBut I'm confident on where I put it.",
              "score": 26,
              "created_utc": "2026-01-31 05:11:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qpizl",
                  "author": "ThatRandomJew7",
                  "text": "Fair. I haven't used Deepseek much since R1, the lack of multimodal really hurts it for general use.\n\nIdk, in my experience I've found Kimi on par with Gemini 3 Pro at least. My usual \"weird benchmark that they're definitely not training on' is telling it to make a Pico-8 \"demake\" of my favorite UFO50 game. Only Kimi and Gemini could actually produce something that didn't instantly error, and Kimi was more graphical (though it struggled with sprites). Even Sonnet couldn't do it. Haven't tried Opus though since I'm not paying for Claude at this point",
                  "score": 17,
                  "created_utc": "2026-01-31 05:15:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2sz1jn",
              "author": "IsometricRain",
              "text": "What can it do better than sonnet? Curious what your use case is.\n\nI'm thinking of testing it out but Sonnet has been very solid for me, with GLM being good enough for smaller tasks and quick research",
              "score": 2,
              "created_utc": "2026-01-31 15:42:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qmrph",
          "author": "ForsookComparison",
          "text": "Tiers are ranked.\n\nGrok takes into account Grok-4.1 and Grok-Coder-1-Fast, not the opened Grok-2 weights.\n\nQwen is based on my experiences with Qwen3-235B mostly and the 'goodness' of the ease of hosting a model of that power. Closed-Weight Qwen3-Max would likely rank similarly despite being a hair better.\n\nGemini 3 Pro will beat ChatGPT on some days, but GPT-5.2-Pro (or 'extra' or whatever it's called nowadays) will beat it if you can wait.\n\nEverything left out isn't meant as bad (for example: I use Nemotron Nano near-daily) they've just never been SOTA competitors in their size-class to me, so they don't come up in any SOTA thoughts I'll have.\n\nGLM's placement I think can stretch all the way from 4.5-air and 4.6v to the full 4.7. It does so much better than those beneath it in agentic work, but once it's time to make decisions the gaps between it at the tier above it show.\n\nI love Llama and use Llama 3.3 70B up until very recently when Qwen3-VL-32B, Seed-OSS-36B, and Nemotron-Super-49b-v1.5 started to show knowledge-depth that finally kicked it off my hard drive.\n\nI don't have many vision or ocr use-cases in any of my work/projects so those capabilities weren't taken into account.",
          "score": 31,
          "created_utc": "2026-01-31 04:55:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2r75uo",
              "author": "wouldacouldashoulda",
              "text": "Honest question, what do you do that you need so many tokens that you would consider anything but just the big 3?",
              "score": -1,
              "created_utc": "2026-01-31 07:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2uifr7",
                  "author": "ForsookComparison",
                  "text": "Try coding a large side project spanning several repos when you're not rich - then try and have those side projects require LLMs as a piece of the pipelines.\n\nYou need to get crafty and spend hours with everything to tune performance-per-dollar and time-vs-output",
                  "score": 2,
                  "created_utc": "2026-01-31 20:07:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2s4xot",
                  "author": "fractalcrust",
                  "text": "gooning",
                  "score": 1,
                  "created_utc": "2026-01-31 12:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ri2xg",
          "author": "nycigo",
          "text": "Devstral 2 is pretty good too",
          "score": 11,
          "created_utc": "2026-01-31 09:27:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdgvy",
          "author": "k_means_clusterfuck",
          "text": "If this is a tier list for agentic coding, you'd be crazy to place Minimax and ziphu that far down",
          "score": 6,
          "created_utc": "2026-01-31 08:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u5e57",
              "author": "Vaptor-",
              "text": "I use minimax for daily with claude code. With good mcp (serena, context7), proper context management, and subagents it's really good. It also cost like $10 per mo and I never hit 50% of the 5 hours limit.",
              "score": 1,
              "created_utc": "2026-01-31 19:04:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s4qri",
          "author": "gnaarw",
          "text": "I've used kimi 2.5 for a semi large go project this week and it seems better than Sonnet and gippity\n\nAs for open weight models we can actually run at home... GLM fast, devstral 2, OSS and qwen are still quite a way off. Minimax, GLM and Kimi (cloud hosted which is ok for most European projects for example) are actually on par with a good junior on crack vs the smaller models acting like a junior who's hand you have to hold while crossing the street...",
          "score": 5,
          "created_utc": "2026-01-31 12:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzsia",
          "author": "cyberdork",
          "text": "This ranking is just as useless as the benchmarks, since you don't mention anything about the use case.     \n    \nI'm so tired about people talking how model XYZ is the best, just to read somewhere in the comments that they mean for coding. The vast majority of people DON'T vibe code and have totally different use cases.",
          "score": 17,
          "created_utc": "2026-01-31 12:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s57h0",
              "author": "aeroumbria",
              "text": "Yeah, I don't believe a second that even on coding, there exists one model that does \"everything\" better than the next model.",
              "score": 3,
              "created_utc": "2026-01-31 12:51:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qxbxo",
          "author": "gcavalcante8808",
          "text": "Opus nearly reconstructed a svg with perfection based on a complex banner... for this case even the other closed models didn't even get closer.\n\nFor code: python, rust, DDD, clean architecture it's all about the same; good enough, the open models works wonderfully for a long time now. \n\nSo the answer depends on the task.",
          "score": 3,
          "created_utc": "2026-01-31 06:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rihex",
          "author": "raiffuvar",
          "text": "ðŸ¤£ ðŸ¤£ \nDeepseek \"feels like early 2025 sota\". \nThe model which liturally make other to continue with improving.",
          "score": 7,
          "created_utc": "2026-01-31 09:31:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rn05q",
              "author": "Kakami1448",
              "text": "Back in early 2025, so no lies here",
              "score": 7,
              "created_utc": "2026-01-31 10:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2s69q2",
          "author": "True_Requirement_891",
          "text": "I've seen minimax do very surprising things. It should be in the SOTA territory.",
          "score": 3,
          "created_utc": "2026-01-31 12:59:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sihpc",
          "author": "RedParaglider",
          "text": "What I've found for dev work SERIOUS dev work I always get better results out of codex 5.2 max.  It is much slower, and forces me to use my brain a lot more through the entire thing, but the end result is always cleaner and more professional.  \n\nIt's not NEARLY as fun to use as opus though.",
          "score": 3,
          "created_utc": "2026-01-31 14:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ss9qa",
          "author": "SourceCodeplz",
          "text": "Gpt-5.2-Codex #1",
          "score": 3,
          "created_utc": "2026-01-31 15:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rgv1g",
          "author": "nullmove",
          "text": "Where I disagree with this:\n\n- GPT-5.2(-codex) > Slopus 4.5, these are top 2 in on their own\n- Gemini has unparalleled knowledge, but for coding it shares third place with Kimi K2.5 which is really impressive\n- I don't find Minimax-M2.1 to be worse than GLM-4.7 overall, though both have their ups and downs that don't overlap",
          "score": 6,
          "created_utc": "2026-01-31 09:15:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5p1n",
              "author": "-InformalBanana-",
              "text": "Minimax M2.1 is really amazing, significantly smaller than others while still very competitive.",
              "score": 1,
              "created_utc": "2026-01-31 16:14:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tak0o",
                  "author": "nullmove",
                  "text": "Yep I am writing Elixir and this thing flies in opencode. Surely difference exists with Sonnet but for a lot of things I can't tell at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 16:37:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qyshj",
          "author": "Zeeplankton",
          "text": "unfortunately claude / opus is more like\n\nhttps://preview.redd.it/bz2atqnwpmgg1.jpeg?width=1438&format=pjpg&auto=webp&s=eaef64aa0a3062b5d2d62803b7e0fc2fea022507",
          "score": 5,
          "created_utc": "2026-01-31 06:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rs0kx",
              "author": "hellomistershifty",
              "text": "I don't think about them at all... until that \"usage cost limit exceeded\" notification rolls in",
              "score": 3,
              "created_utc": "2026-01-31 11:02:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r5tci",
          "author": "wanderer_4004",
          "text": "Well, my honest take, benchmarks and ClosedAI fanbois be damned, any open model blows the closed ones out of the water. Unless you want to live in a future where a handful of Elons, Samas and similar rule the world. The day that VCs stop subsidizing your $20 coding plans you'll have to turn around and bend over. Or maybe that is what you are looking forward to?",
          "score": 7,
          "created_utc": "2026-01-31 07:32:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rrwaj",
              "author": "hellomistershifty",
              "text": "you can already see /r/Bard in shambles that the Gemini API costs money (shocking)",
              "score": 7,
              "created_utc": "2026-01-31 11:01:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rdxr0",
              "author": "daniel-sousa-me",
              "text": "If you remove the R&D portion out of Anthropic's expenses, it is already wildly profitable\n\nVC money is used for R&D to make it even more profitable",
              "score": 8,
              "created_utc": "2026-01-31 08:48:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rfhon",
                  "author": "wanderer_4004",
                  "text": "For now, i.e. pre-IPO this is all guess work as the numbers are not public. That said, I think the API pricing at $25 / 1MTok is very likely to be nicely profitable. The $20 claude subscription is very likely not. At some point the VCs want to see a return. And with agentic coding you can easily burn 10MTok per hour. If you have to pay that at API prices, then for a lot of developers this will be a bad awakening into reality.\n\nThe big closed companies spend lots of money to make us believe that you absolutely need SOTA-frontier. Keep in mind the biggest threat to them and their investments of billions of $$$ is people figuring out that open models are good enough.\n\nRight now only \\~20M users out of 800M are paying for ChatGPT...",
                  "score": 2,
                  "created_utc": "2026-01-31 09:02:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qouee",
          "author": "yes-im-hiring-2025",
          "text": "I want to do an experiment for six months.\nTeaching via ragebaitâ„¢\n\nSame claude code setups (agent plugin + CLI), but with a twist:\n\n- some are on the claude max 200 USD plan [seniors]\n- some are on the codex pro plan [mid level]\n- some are on the GLM/minimax plan [junior]\n\nBasically the juniors get the worst AIs, and the seniors get the best. Have the juniors learn by ragebait until they match the performance of the tier above. Same with mid level engineers.\n\nMy senior level gang just ensures ralph wiggum keeps ralph wiggum-ing.\n\nWho wants to buy my agentic coding course?!?!?!",
          "score": 6,
          "created_utc": "2026-01-31 05:10:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp45i",
              "author": "ForsookComparison",
              "text": "I'd love this - but I'm more interested in juniors-with-SOTA vs seniors-with-minimax or something along those lines",
              "score": 8,
              "created_utc": "2026-01-31 05:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqhmv",
                  "author": "yes-im-hiring-2025",
                  "text": "Ohhh I can tell you what happens with that firsthand.\n\nI had my team running Opus via the claude code plan and the juniors just resorted to recorded meetings -> AI studio's gemini flash to get a PRD -> oneshotâ„¢ via Opus. Bear in mind these are college grads, so truly green around the ears.\n\nAlso how I found out that the meetings recorded are being sent to gemini violating who knows how many corporate conditions. If you're not familiar, the uploaded media on AI studio is first pushed to your own personal cloud and then linked from there. Not a \"temp\" upload, it uses your drive as the temp upload storage space - but never deletes anything once uploaded.\n\nZero debugging. Missed a feature and a deadline and I got on a call with them to understand how that could've possibly happened, it was debugging a simple API call response that was throwing up a random 4xx error.\n\nChecking for a response code 200 and logging everything else -> debugging with input and output loads -> sample logs for a batch input to filter and identify root cause. That's all they needed to do. Not even through experience, they could've asked an LLM how to debug a problem and follow the steps.\n\nNope. Not a single one of the three did that; coding muscles had atrophied.\n\nSince then I told management to lock up claude code access for juniors, and to only have it given to them after year 1. Till then they're on the [enterprise] basic free tier github's copilot plan only. Can't say it dramatically improved their skills but atleast they are learning how to Google and be honest, which is enough for now.\n\nI do believe most seniors SHOULD be fine with any GLM model if they can debug and write things independently, though. I myself switch between Antigravity (free opus4.5) for detailed planning then clean up/edit the plan, and get GLM4.7 to work on it. I've set up auto-review and linting, so usually major issues get caught before I even review the final code.",
                  "score": 14,
                  "created_utc": "2026-01-31 05:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpz4n",
          "author": "SilentLennie",
          "text": "I think there is an other detail: how smart the model is matters less and less, if it can complete the task is more important and at what price/how.\n\nLet's take coding:\n\nSure the top open weights models are close to early (I would put it later) 2025 smarts, but they are also much better at agentic than those models.\n\nAnd the harnesses (tools we use to make an agentic coder) are much better now than in the past, they include for example LSP even on the CLI and more and more people figured out 'plan mode' and the models can generate tests and test it and whole lot of other details...\n\nSo what does this mean ? They can actually finish a lot of tasks on their own (Kimi K2.5 being multi-modal matters a lot here too) and do it for a much lower cost or on your own computer if that's the reason you prefer. So if you compare them to early 2025 this is the wrong way to look at it. Not only that: if they can finish a task: it's far more often 'good enough', which is how for example DOS and Windows got popular (and network effect, monopoly abuse, etc. obviously), it wasn't perfect, but good enough to get stuff done.\n\nI think we are gonna be in a much more: multi-model world, where you might be using or example Kimi K2.5 a lot, but when you need to make a plan you pick Opus and when you need to digest a lot of data cheaply, you might pick Gemini 3 Flash because of it's 1M context window, etc.\n\nThe next problem is: you need to keep doing some coding to keep your coding muscle.",
          "score": 2,
          "created_utc": "2026-01-31 10:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzeah",
          "author": "-dysangel-",
          "text": "GLM Coding Plan with Claude Code is feeilng just as good or better as the latter days of Claude 4.1 for me. I've been using it daily for work and it's working very well. Organised, smart, helpful, relentless :p",
          "score": 2,
          "created_utc": "2026-01-31 12:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s32za",
          "author": "ALittleBitEver",
          "text": "LLama Models are really good for tasks related to emotions, roleplaying, creative writing, etc. But yes, at programming, math, etc, I agree",
          "score": 2,
          "created_utc": "2026-01-31 12:35:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s4xqn",
          "author": "Admirable-Choice9727",
          "text": "Meta really went from 'saving open source' to 'the team that peaked at 405B' while Deepseek and Qwen just kept shipping. Itâ€™s wild that weâ€™re at a point where Iâ€™d rather trust a hypothetical GLM-5 leak than a Meta announcement. The 'Maverick' release felt like they were just trying to stay relevant while the frontier moved past them.",
          "score": 2,
          "created_utc": "2026-01-31 12:49:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sh9jq",
          "author": "Far-Low-4705",
          "text": "i would bump up GPT-OSS up a level.\n\nIMHO, for it's architecture, given it's size, how sparse it is, it's speed, interleaved thinking, and more \"stable\" reasoning traces, it feels SOTA some of the time.\n\nFrom an engineering standpoint, it is one of the most sparse models, and it is very fast",
          "score": 2,
          "created_utc": "2026-01-31 14:07:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2swut8",
          "author": "Michaeli_Starky",
          "text": "Gemini in the current state has no right being in the list at all\n\nhttps://preview.redd.it/05pqcgioepgg1.png?width=1730&format=png&auto=webp&s=73cfe9a96b437482f0834fb76b254d93543e18cc",
          "score": 2,
          "created_utc": "2026-01-31 15:31:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wc5jq",
              "author": "spawncampinitiated",
              "text": "You get down voted for stating facts here",
              "score": 1,
              "created_utc": "2026-02-01 01:57:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tw85d",
          "author": "Hoak-em",
          "text": "Disagree, at least with whatever gemini Google hosts with Antigravity. I think things get a bit murky when you start to consider that for the version of the model that most people get when it's still a \"SOTA\" model, it's nerfed in some way (i.e. issues with Opus models not performing like they did at launch, issues with Gemini models (in general)).\n\nI don't really like having a single ranking either, it's more like \"what is the model good at/what do I want to use it for.\"\n\nKimi K2.5: orchestrator/small-scale planner/designer\n\nOpus-4.5: Large-scale planner/implementer\n\nGemini 3 flash: codebase search/understanding\n\nGLM-4.7: wiggumwiggumwiggumwiggumwigguwmgiwigiwmgiwmgiw (your account was rate limited, please reduce concurrency)\n\nGPT-5.2-Codex: Fixes, refactors (alongside opus), helping me understand wtf the previous model wrote",
          "score": 2,
          "created_utc": "2026-01-31 18:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ugr7z",
          "author": "timschwartz",
          "text": "Can someone explain the Ralph Wiggum thing?",
          "score": 2,
          "created_utc": "2026-01-31 19:59:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uhhme",
              "author": "ForsookComparison",
              "text": "Ralph Wiggum is a popular Simpsons character. He became a popular internet meme for many years because of a famous scene from the show where he's smiling in the back of a bus saying *\"I'm in danger\"*\n\nAnthropic actually published a way to use Claude-Code agentically with no concern for permissions or token usage that they labeled *\"Ralph Wiggum Mode\"*.\n\nHere, the one in danger is OP's job and the danger is Claude 4.5 Opus.",
              "score": 2,
              "created_utc": "2026-01-31 20:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qw647",
          "author": "ahabdev",
          "text": "Personally I was a fan fo Gemini 2.5 over Opus 3.7. I considered a much better coder.\n\nHowever, there's no comparison between Gemini 3 and Opus 4.5. Mainly because G3 is disastrous with any complex project which require long chats, constant code revisions and contentx awareness without a flaw. It's beyond horrible. So nowadays I wound't even put it in the list.\n\nAnd as a peasant with only a 5090  I never even tried to setup a big model. I don't have the need so far, but would be really great to abe able to do so eventually. That said, I consider people don't value as they should small models for many day to day cases.\n\nSuch a pity Meta failed as with most things they do these days.",
          "score": 2,
          "created_utc": "2026-01-31 06:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qt0rj",
          "author": "YouAreTheCornhole",
          "text": "I can't believe how spot on this is",
          "score": 3,
          "created_utc": "2026-01-31 05:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r59tp",
          "author": "Spitihnev",
          "text": "And mistral is the ralph wiggum?",
          "score": 2,
          "created_utc": "2026-01-31 07:27:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2uio5t",
              "author": "ForsookComparison",
              "text": "No",
              "score": 1,
              "created_utc": "2026-01-31 20:08:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r7812",
          "author": "NightlessBaron",
          "text": "Also check out Mbzuaiâ€™s K2 think V2 and Nvidia Nemotronâ€¦ both are dark horses imo",
          "score": 1,
          "created_utc": "2026-01-31 07:45:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sbqsd",
          "author": "Charuru",
          "text": "This is right on just chat performance, but you're underestimating agentic for K2.5.",
          "score": 1,
          "created_utc": "2026-01-31 13:34:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sdbg7",
          "author": "LocoMod",
          "text": "gpt-5-xhigh runs circles around Opus. Change my mind.",
          "score": 1,
          "created_utc": "2026-01-31 13:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2spnun",
              "author": "Both-Ad3646",
              "text": "Agreed, and Gemini is complete trash in comparison to both. I have no idea why it's ever been in the same discussion as any of the GPT's from 4.x and above.",
              "score": 2,
              "created_utc": "2026-01-31 14:54:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2sjtsn",
          "author": "sine120",
          "text": "I haven't been able to play with Codex or CC much, but from what I hear of others that do, this is largely true. My company pays for Gemini CLI and I swear I might be the only person using that over the other labs. It's bad at instruction following, but takes less wrangling to get it to do what I want over models I can run. Prices would have to go up a lot to bother pushing me to pitch we run local models for anything other than ITAR code.",
          "score": 1,
          "created_utc": "2026-01-31 14:22:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sl39o",
          "author": "teachersecret",
          "text": "How close is local to SOTA? I'd say we're there. K2 2.5 is definitely SOTA level intelligence up near the highest levels we have available.\n\nObviously that's a bit difficult to run in your house, though. Most of us aren't running Kimi K2 at home.\n\nFor TRULY local... I usually think enthusiast-class (24gb vram). The only thing in that range that comes close is GLM 4.7 Flash. It feels like somebody took last-gen Claude Sonnet and gave it better tool handling. It's fantastic and not far off SOTA despite its small size.",
          "score": 1,
          "created_utc": "2026-01-31 14:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2svnz1",
          "author": "TwistStrict9811",
          "text": "Codex better than claude",
          "score": 1,
          "created_utc": "2026-01-31 15:25:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sy1vm",
          "author": "wagnerax",
          "text": "What about Mistral ?",
          "score": 1,
          "created_utc": "2026-01-31 15:37:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2szrk0",
              "author": "ForsookComparison",
              "text": "ðŸ˜",
              "score": 0,
              "created_utc": "2026-01-31 15:46:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t2w3d",
          "author": "GiveMeAegis",
          "text": "No Mistral?",
          "score": 1,
          "created_utc": "2026-01-31 16:01:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2t5fg0",
              "author": "ForsookComparison",
              "text": "Le sorry",
              "score": 1,
              "created_utc": "2026-01-31 16:13:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t9wit",
          "author": "RhubarbSimilar1683",
          "text": "For coding ofc\n\n\nWe all know that claude is over fitted for web coding",
          "score": 1,
          "created_utc": "2026-01-31 16:34:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ta8vg",
          "author": "laterbreh",
          "text": "So what are the qualifying tasks to create this ranking?\n\nWrites my LARP smut the best? Best coding agent? Best at accomplish a specific task?",
          "score": 1,
          "created_utc": "2026-01-31 16:36:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2taus6",
              "author": "ForsookComparison",
              "text": "OP's vibes",
              "score": 1,
              "created_utc": "2026-01-31 16:39:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2tq8hs",
          "author": "Qual_",
          "text": "I found codex 5.2 way more reliable than claude on large codebase. There is always something to fix after claude, while codex just produce working code ( which sometimes feels black magic when it's after 50min of writing thousand of lines )",
          "score": 1,
          "created_utc": "2026-01-31 17:53:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u6iro",
          "author": "TurnUpThe4D3D3D3",
          "text": "This feels accurate, although I might put deepseek 1 tier lowers",
          "score": 1,
          "created_utc": "2026-01-31 19:09:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ud0ll",
          "author": "bartskol",
          "text": "Grok was never SOTA?",
          "score": 1,
          "created_utc": "2026-01-31 19:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ueakb",
              "author": "ForsookComparison",
              "text": "It's realtime data and web tools are. Probably number one for querying events from that day.\n\nThe base model itself when used over API falls a little short of Gpt5.2 and Gemini3 Pro. It feels more in line with Gemini 2.5 Pro or Sonnet 3.7 maybe, hence the rating of \"early 2025 SOTA\"",
              "score": 1,
              "created_utc": "2026-01-31 19:46:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uffgm",
                  "author": "bartskol",
                  "text": "It feels more like a toy",
                  "score": 1,
                  "created_utc": "2026-01-31 19:52:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2uqmnn",
          "author": "blackwell_tart",
          "text": "I wish youâ€™d use words instead of emblems. I have no idea what any of those graphics are supposed to represent. I guess I missed LLM marketing class.",
          "score": 1,
          "created_utc": "2026-01-31 20:47:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2v4t7w",
          "author": "Odd_Candle",
          "text": "wtf is sota",
          "score": 1,
          "created_utc": "2026-01-31 21:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vo6gn",
              "author": "checksinthemail",
              "text": "State Of The Art",
              "score": 1,
              "created_utc": "2026-01-31 23:39:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2vbw2h",
          "author": "Imperator_Basileus",
          "text": "I donâ€™t know, maybe GPT-5.2 is fine for coding, but for anything else Iâ€™d much, much rather use DeepSeek, GLM, or Kimi. I think Gemini 3 Pro is the best one to discuss anything with or bounce ideas off of as it takes instructions well and doesnâ€™t moralise or get distracted with safetyslop.Â \n\nBut since google heavily cut AI studio rate limits and the Gemini app and subscription is terrible, I wouldnâ€™t be using that either. Hence, trying out the new Kimi 2.5 lately. But GPT-5.2 is truly horrendous. I was trying to discuss economics, my field, and harshly critiqued some economist only for it be hyper condescending about â€œletâ€™s set that asideâ€, â€œIâ€™m gonna draw a firm lineâ€, and â€œI wonâ€™t endorse harsh languageâ€. Okay, go and fuck yourself. Uninstalled the app soon after.Â ",
          "score": 1,
          "created_utc": "2026-01-31 22:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s68p2",
          "author": "korino11",
          "text": "OMG Ralh?!?? that a STUPID plugin! For what??! Only if you have a SIMPLE project! And thats it. If you need a somthing BIG with many layers of decompositions, with thousands calls of subagent. Useles shit your ralh...\n\nRight now kimi K2.5  - best model. Even gpt 5.2 xhigh useles. k2.5 can solve all. Even with high physycs and math..",
          "score": 1,
          "created_utc": "2026-01-31 12:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qmobq",
          "author": "SeeonX",
          "text": "What is the top AI called?",
          "score": 1,
          "created_utc": "2026-01-31 04:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qmvxs",
              "author": "Ok-Lobster-919",
              "text": "Claude, Opus is insanely good for coding. Never try it, you won't want to use anything else. It one-shots problems sonnet struggles with.",
              "score": 20,
              "created_utc": "2026-01-31 04:56:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qn6jc",
                  "author": "ForsookComparison",
                  "text": "> Never try it, you won't want to use anything else\n\n  \nMy wallet agrees. There are some tasks I have where *\"it's either Opus, or it's all afternoon\"* but that $25/1m racks up quick.",
                  "score": 14,
                  "created_utc": "2026-01-31 04:58:40",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2qnrtu",
                  "author": "jadhavsaurabh",
                  "text": "Bro same bro same, after trying opus I don't even use other AI now on 100$ plan",
                  "score": 7,
                  "created_utc": "2026-01-31 05:02:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2qo99e",
                  "author": "HarambeTenSei",
                  "text": "gpt5.2 through codex is better in most instances",
                  "score": 0,
                  "created_utc": "2026-01-31 05:06:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qmzq0",
              "author": "ForsookComparison",
              "text": "Claude 4.5 Opus is the one in *\"My Job is Ralph Wiggum\"* tier.\n\n  \nClaude 4.5 Sonnet would rank bottom of \"SOTA\" if I took the time to rank split out models.  \n  \nClaude 4.5 Haiku would probably be lower than Grok but above Kimi and Deepsek.",
              "score": 0,
              "created_utc": "2026-01-31 04:57:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qoi6x",
                  "author": "simracerman",
                  "text": "So, do you think GLM-4.7 Flash beats 4.5 Sonnet or GLM-4.7 even?",
                  "score": 3,
                  "created_utc": "2026-01-31 05:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2qp09q",
          "author": "yes-im-hiring-2025",
          "text": "I am going all in on GLM stocks btw.\nI think their GLM5 is gonna drop soon and reclaim the top spot (trust)",
          "score": 1,
          "created_utc": "2026-01-31 05:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qp62n",
              "author": "ForsookComparison",
              "text": "Would be cool especially if they keep their commitments to open-weight.",
              "score": 6,
              "created_utc": "2026-01-31 05:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qqyjw",
                  "author": "yes-im-hiring-2025",
                  "text": "Seems more and more unlikely since they IPOd.\n\nI'd love for them to be open, they're realistically the best performance:size option if I need to have it up and running locally. Minimax and Kimi are great but the dormant param weights are a memory hog that I'd rather avoid.",
                  "score": 1,
                  "created_utc": "2026-01-31 05:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2t67ac",
              "author": "-InformalBanana-",
              "text": "I root for Minimax - smaller models, better for local and competitive performance.",
              "score": 1,
              "created_utc": "2026-01-31 16:17:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ruvvl",
          "author": "stylehz",
          "text": "Besides the Claude (because it is the best), never let bro cook again.",
          "score": 1,
          "created_utc": "2026-01-31 11:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u98q1",
              "author": "nomorebuttsplz",
              "text": "Agreed, this tier list is very poor and lines up with vibes from 2 months ago rather than any actual tasks... and without specifying domain or task rankings are pointless...  overly broad like artificial analysis except without any evidence.",
              "score": 2,
              "created_utc": "2026-01-31 19:22:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2sl62j",
              "author": "ForsookComparison",
              "text": "I'm in the kitchen and your head at the same time",
              "score": -1,
              "created_utc": "2026-01-31 14:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2stovx",
          "author": "nomorebuttsplz",
          "text": "Nah.Â \n\nKimi 2.5 is at SOTA levels and well above early 2025.\n\nGlm 4.7 is also above early 2025 in coding. Virtually everyone who has compared sonnet 3.7 and glm 4.7 in coding agrees glm is better.Â \n\nBut of course, this is task specific. What tasks are you measuring yours by?",
          "score": 1,
          "created_utc": "2026-01-31 15:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qpe34",
          "author": "neotorama",
          "text": "Gemini sits with Z. Itâ€™s crap with infinite loop. Kimi 2.5 beside OpenAI GPT",
          "score": -3,
          "created_utc": "2026-01-31 05:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r45o0",
              "author": "fishylord01",
              "text": "Not sure why downvoted, Gemini sucks . Agree kimi 2.5 is very close behind codex 5.2 high.",
              "score": 0,
              "created_utc": "2026-01-31 07:16:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rqan2",
          "author": "spawncampinitiated",
          "text": "Gemini Sota xd",
          "score": 0,
          "created_utc": "2026-01-31 10:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r6r0h",
          "author": "Juanisweird",
          "text": "Hey, recently returning to this whole AI thing. Whatâ€™s SOTA?\n\nAnd if you had to choose only one $20 subscription for a general use AI ( coding, research, chatting, troubleshooting real life and technical issues, design inspiration and business planning and marketing) which one would you choose for the next 2 months?",
          "score": 0,
          "created_utc": "2026-01-31 07:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rqylz",
              "author": "SilentLennie",
              "text": "It depends, what do you need it for (or more likely: how much do you need it) ? Do you include the job, do you include private use ? What kind of hobbies and job do you have, etc. ?",
              "score": 1,
              "created_utc": "2026-01-31 10:52:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rvxl3",
                  "author": "Juanisweird",
                  "text": "I want to create apps to automate or simplify my life. Iâ€™ve done some through python scripts and vibe coding like second hand listing generator or roulette simulator or points and job dashboard ( my 9-5 has a points system so after X amount of jobs, I get extra pay).\n\nBut I want to build a side income and want an AI that can help me do research, prepare projects and even help create delivery and onboarding automations with little AI and more old school programming but since I have little experience, need an AI to guide me or aid",
                  "score": 1,
                  "created_utc": "2026-01-31 11:37:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rbeh8",
              "author": "YayaBruno",
              "text": "I've been using Nano-GPT for a while  and it's been decent, it gives access to a bunch of open source models for an $8 a month subscriptio. ( Referral link if you're interested: https://nano-gpt.com/r/CfxGHjHp )",
              "score": 0,
              "created_utc": "2026-01-31 08:24:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rmqj2",
          "author": "Downtown_Fly_5919",
          "text": "And then you have nemotron 3 nano",
          "score": -1,
          "created_utc": "2026-01-31 10:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t7893",
          "author": "KitchenSomew",
          "text": "Solid tier list! The placement of open-weight models in \"early 2025 SOTA\" territory is spot-on for most use cases. A few observations:\n\n\n\n1. \\*\\*Context matters more than raw intelligence\\*\\*: As several commenters noted, Kimi K2.5's multimodal capabilities + agentic workflows make it punch way above its weight compared to models with similar benchmark scores.\n\n\n\n2. \\*\\*The \"instruction following vs problem understanding\" debate is key\\*\\*: Claude's strength isn't just following instructionsâ€”it's inferring intent and missing context, which is why it excels at complex refactoring tasks even when specifications are vague.\n\n\n\n3. \\*\\*Open-weight gap is narrowing in specific domains\\*\\*: For coding with proper tooling (LSP, test generation, iteration loops), GLM-4.7 + good harness can match sonnet 3.7 on many practical tasks. The real gap shows in long-context coherence and multi-turn debugging.\n\n\n\n4. \\*\\*Size/performance tradeoff is underrated\\*\\*: Qwen3-235B is the sweet spot for self-hostedâ€”enough intelligence for real work without needing a data center. The jump to K2.5 territory requires massive compute that most can't justify.\n\n\n\nThe fact that we're even having \"SOTA vs early 2025\" debates about open weights is wild progress.",
          "score": -2,
          "created_utc": "2026-01-31 16:22:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo595n",
      "title": "Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "author": "Kimi_Moonshot",
      "created_utc": "2026-01-27 05:39:09",
      "score": 495,
      "num_comments": 110,
      "upvote_ratio": 0.97,
      "text": "ðŸ”¹**Global SOTA on Agentic Benchmarks**: HLE full set (50.2%), BrowseComp (74.9%)  \n  \nðŸ”¹**Open-source SOTA on Vision and Coding**: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%)  \n  \nðŸ”¹**Code with Taste**: turn chats, images & videos into aesthetic websites with expressive motion.  \n  \nðŸ”¹**Agent Swarm (Beta)**: self-directed agents working in parallel, at scale. Up to **100** sub-agents, **1,500** tool calls, **4.5Ã—** faster compared with single-agent setup.  \n  \nðŸ¥**K2.5** is now live on [http://kimi.com](https://t.co/YutVbwktG0) in **chat mod**e and **agent mode**.  \n  \nðŸ¥**K2.5 Agent Swarm** in beta for high-tier users.  \n  \nðŸ¥For production-grade coding, you can pair K2.5 with **Kim**i Code: [https://kimi.com/code](https://t.co/A5WQozJF3s)\n\nðŸ”—API: [https://platform.moonshot.ai](https://t.co/EOZkbOwCN4)\n\nðŸ”—Tech blog: [https://www.kimi.com/blog/kimi-k2-5.html](https://www.kimi.com/blog/kimi-k2-5.html)  \n  \nðŸ”—Weights & code: [https://huggingface.co/moonshotai/Kimi-K2.5](https://huggingface.co/moonshotai/Kimi-K2.5)\n\nhttps://preview.redd.it/b3lldwzvwtfg1.png?width=1920&format=png&auto=webp&s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o20b0wc",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-27 13:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yv72k",
          "author": "Asleep_Strike746",
          "text": "Holy shit 100 sub-agents working in parallel sounds absolutely bonkers, definitely gonna have to test this out on some coding tasks",
          "score": 91,
          "created_utc": "2026-01-27 05:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20x5ir",
              "author": "IronColumn",
              "text": "the whole thing with sub-agents is protecting the primary model's context window from overload. But at 100 sub agents, just their reporting is going to stretch even a big context window",
              "score": 16,
              "created_utc": "2026-01-27 14:56:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o214mwe",
                  "author": "MrRandom04",
                  "text": "If they can coordinate well, they can actually accomplish much more than a single agent could for reasonably parallel tasks.",
                  "score": 9,
                  "created_utc": "2026-01-27 15:31:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21nqg7",
                  "author": "JChataigne",
                  "text": "What do you use to run several agents in parallel locally ?",
                  "score": 2,
                  "created_utc": "2026-01-27 16:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z6x58",
              "author": "Pro-editor-1105",
              "text": "https://preview.redd.it/9sku14q4gufg1.jpeg?width=1024&format=pjpg&auto=webp&s=afabc084139d09741d32972cbce3a8ae5ea16dfc",
              "score": 58,
              "created_utc": "2026-01-27 07:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yw8v4",
              "author": "derivative49",
              "text": "how are people with 1-2 gpus expected to do that ðŸ¤” (Can they?)",
              "score": 16,
              "created_utc": "2026-01-27 05:58:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z19kt",
                  "author": "claythearc",
                  "text": "You donâ€™t",
                  "score": 49,
                  "created_utc": "2026-01-27 06:37:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21o1wz",
                  "author": "Far-Low-4705",
                  "text": "you cant even run this model on 1-2 GPUs lol",
                  "score": 2,
                  "created_utc": "2026-01-27 16:55:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24wyxu",
                  "author": "newbee_2024",
                  "text": "The agent-swarm pitch is neat, but for most folks the question is: whatâ€™s the smallest â€œusefulâ€ setup locally?\nAnyone got numbers for VRAM/RAM at Q4/Q5 + decent context? Even rough ballparks help.",
                  "score": 1,
                  "created_utc": "2026-01-28 02:00:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26l92o",
              "author": "No_Afternoon_4260",
              "text": "Per today's [cooperbench](https://cooperbench.com/static/pdfs/main.pdf) (Stanford) I'm not so sure anymore",
              "score": 1,
              "created_utc": "2026-01-28 08:50:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z6fwd",
          "author": "Lan_BobPage",
          "text": "I'll download it and tinker with it in 3-4 years",
          "score": 46,
          "created_utc": "2026-01-27 07:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhycy",
              "author": "bobby-chan",
              "text": "For perspective, Llama 1 was 3 years ago.",
              "score": 43,
              "created_utc": "2026-01-27 09:05:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zqvcm",
                  "author": "Lan_BobPage",
                  "text": "I'll download it and keep it as a relic",
                  "score": 26,
                  "created_utc": "2026-01-27 10:27:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20uurf",
              "author": "Zyj",
              "text": "In 2-3 years we might get Medusa Halo with 256GB RAM. Not very optimistic about RAM prices. Youâ€˜d need 3-4 of them to run at Q4 with context.",
              "score": 5,
              "created_utc": "2026-01-27 14:44:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o226l36",
                  "author": "power97992",
                  "text": "In 2 years, you probably will see 5-8   trillion parameter models",
                  "score": 2,
                  "created_utc": "2026-01-27 18:15:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20xvnn",
                  "author": "Miloldr",
                  "text": "We are reaching physical and quantic limitsÂ ",
                  "score": 1,
                  "created_utc": "2026-01-27 14:59:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o229x6a",
                  "author": "Lan_BobPage",
                  "text": "Hold on I'm not THAT poor just yet",
                  "score": 1,
                  "created_utc": "2026-01-27 18:29:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o221579",
              "author": "Confident-Ad-3465",
              "text": "I'll download it, so my SSD doesn't feel empty inside.",
              "score": 3,
              "created_utc": "2026-01-27 17:52:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yvta6",
          "author": "Accomplished_Ad9530",
          "text": "Huh, OP u/Kimi_Moonshot was banned. Was it impersonation or a fake account or something?",
          "score": 68,
          "created_utc": "2026-01-27 05:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z48hp",
              "author": "Accomplished_Ad9530",
              "text": "Also, OP used to be an r/kimi mod, and now they're not. I wonder what's going on.\n\nhttps://preview.redd.it/bx68r5q0cufg1.png?width=2237&format=png&auto=webp&s=32ebebdd2a2e7c78eb3a2b76334966b06c94f657",
              "score": 30,
              "created_utc": "2026-01-27 07:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z49k7",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/c3v5wd24cufg1.png?width=2235&format=png&auto=webp&s=a382f13d022c1d3900797cf08240634badc9579b",
                  "score": 8,
                  "created_utc": "2026-01-27 07:02:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1z49h2",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 2,
                  "created_utc": "2026-01-27 07:02:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z4odr",
              "author": "segmond",
              "text": "probably got auto flagged as spammer as they posted the same thing across multiple subreddits.",
              "score": 21,
              "created_utc": "2026-01-27 07:06:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yy3bm",
              "author": "eidrag",
              "text": "wait what",
              "score": 11,
              "created_utc": "2026-01-27 06:12:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzjl8",
                  "author": "Accomplished_Ad9530",
                  "text": "https://preview.redd.it/dhod4rt55ufg1.png?width=1607&format=png&auto=webp&s=78b61df5cf626108adf77346c0d1bc9541403496",
                  "score": 25,
                  "created_utc": "2026-01-27 06:24:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21objm",
              "author": "Far-Low-4705",
              "text": "of course they did, i hate reddit so much",
              "score": 7,
              "created_utc": "2026-01-27 16:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zy655",
          "author": "fairydreaming",
          "text": "I see impressive improvements in logical reasoning ([lineage-bench](https://github.com/fairydreaming/lineage-bench) [results](https://github.com/fairydreaming/lineage-bench-results/blob/main/lineage-8_64_128_192/README.md)):\n\n|Nr|model\\_name|lineage|lineage-8|lineage-64|lineage-128|lineage-192|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|moonshotai/kimi-k2.5|0.963|1.000|0.975|1.000|0.875|\n|2|moonshotai/kimi-k2-thinking|0.525|1.000|0.850|0.200|0.050|\n\nCongratulations on overcoming this hurdle and joining the elite reasoners club!",
          "score": 29,
          "created_utc": "2026-01-27 11:29:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ywpdn",
          "author": "-illusoryMechanist",
          "text": "1TÂ Activated Parameters 32B wow",
          "score": 53,
          "created_utc": "2026-01-27 06:01:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o206jy7",
              "author": "pawofdoom",
              "text": "Same as K2 right?",
              "score": 23,
              "created_utc": "2026-01-27 12:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20763v",
                  "author": "KaroYadgar",
                  "text": "Yep",
                  "score": 8,
                  "created_utc": "2026-01-27 12:35:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zs2g9",
          "author": "Capaj",
          "text": "https://preview.redd.it/ryc3btmkevfg1.png?width=2629&format=png&auto=webp&s=2c6adae97f14b7c8d471b3bee52a0a73505e1e91\n\njust quickly tested with a prompt: write me an SVG displaying a fox riding a unicycle\n\nnot too bad",
          "score": 33,
          "created_utc": "2026-01-27 10:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zrb2v",
          "author": "MadPelmewka",
          "text": "How happy I am that itâ€™s a VL model, and such a powerful one according to the benchmarks! \n\nEarlier I made aÂ [post](https://www.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/)Â about how there are no good VL models for complex image captioning. Now there are! I'm so happy!",
          "score": 16,
          "created_utc": "2026-01-27 10:31:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z3ipo",
          "author": "polawiaczperel",
          "text": "Will they opensource their agent? It is amazing.",
          "score": 13,
          "created_utc": "2026-01-27 06:56:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zrxhd",
              "author": "adeadbeathorse",
              "text": "https://x.com/Kimi_Moonshot/status/2016034259350520226",
              "score": 16,
              "created_utc": "2026-01-27 10:37:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z38yj",
          "author": "Middle_Bullfrog_6173",
          "text": "This part is interesting: \"Kimi K2.5 is an open-source, native multimodal agentic model built through continual pretraining on approximately 15 trillion mixed visual and text tokens atop Kimi-K2-Base.\"\n\n\nFor reference, K2 pretraining was 15.5T tokens. So almost double the pretraining, not just another SFT + RL.",
          "score": 23,
          "created_utc": "2026-01-27 06:54:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20oden",
              "author": "durable-racoon",
              "text": "is there a typo? 15.5 vs 15T? thats not double?",
              "score": 5,
              "created_utc": "2026-01-27 14:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20qvyh",
                  "author": "Fit-Produce420",
                  "text": "It's trained in 30.5T, which is almost double 15.5T.",
                  "score": 9,
                  "created_utc": "2026-01-27 14:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o200pe9",
          "author": "ffgg333",
          "text": "How is creative writing?",
          "score": 9,
          "created_utc": "2026-01-27 11:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20801d",
              "author": "Cat-informer",
              "text": "Decent, good prose, grok levels of uncensored now :)",
              "score": 17,
              "created_utc": "2026-01-27 12:41:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2092du",
                  "author": "ffgg333",
                  "text": "Really, where did you test it,on theyr website or the API?",
                  "score": 6,
                  "created_utc": "2026-01-27 12:48:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20mb1c",
              "author": "Middle_Bullfrog_6173",
              "text": "Top open model in longform writing benchÂ https://eqbench.com/creative_writing_longform.html\n\n\nFrom short vibe checks also seems good.",
              "score": 12,
              "created_utc": "2026-01-27 14:01:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20ogfh",
              "author": "durable-racoon",
              "text": "Kimi K2 was better than opus for creative writing, cant wait to see how this performs",
              "score": 6,
              "created_utc": "2026-01-27 14:13:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1yxrfq",
          "author": "ikkiyikki",
          "text": "You go Kimi! Not that I have any reason to cheer.... The Q4 version of this will still be larger than any rig this side of 20k will be able to run ðŸ˜”",
          "score": 19,
          "created_utc": "2026-01-27 06:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkv1y",
              "author": "Expensive-Paint-9490",
              "text": "A refurbished HP Z8 G4 with >600GB DDR4 is about 7k. Of course it would be extremely slow. Just six months ago it would have been 4k.",
              "score": 8,
              "created_utc": "2026-01-27 09:33:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2742j8",
                  "author": "TechExpert2910",
                  "text": "since weâ€™d need to use system ram as VRAM,Â a significantly better choice would be a 512 GB Mac Studio\n\nthe M3 Ultraâ€™s GPU is amazingly fast and is Appleâ€™s best\n\nitâ€™s probably 100x faster than running on a CPU + standard DDR5",
                  "score": 1,
                  "created_utc": "2026-01-28 11:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20vjxc",
              "author": "Zyj",
              "text": "5x Strix Halo, 640GB RAM (for q4), $10,000. It will be slow. Probably around 2.5 t/s for now. Might get speedups later on.",
              "score": 1,
              "created_utc": "2026-01-27 14:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24jfds",
              "author": "True_Requirement_891",
              "text": "It's it already int4 something?",
              "score": 1,
              "created_utc": "2026-01-28 00:48:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20lbjw",
          "author": "Loskas2025",
          "text": "https://preview.redd.it/dkrzkltzdwfg1.png?width=796&format=png&auto=webp&s=8c18c3e9a34bffc774baa484738e77dbb249e6c7\n\npiccolo The 1.8-bit (UD-TQ1\\_0) quant will run on a single 24GB GPU if you offload all MoE layers to system RAM (or a fast SSD). With \\~256GB RAM, expect \\~1â€“2 tokens/s.",
          "score": 5,
          "created_utc": "2026-01-27 13:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bpo6",
              "author": "uhuge",
              "text": "should I try on my newly purchased 2019 MB Pro?",
              "score": 2,
              "created_utc": "2026-01-28 00:09:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o20yk7v",
          "author": "fragment_me",
          "text": "Seems interesting but the membership and quota details are confusing on the site. It's not clear if I get 10 requests or 10,000 per day with any membership. For example, the limits in the \"Allegretto\" plan are not clear. Can you clarify for people who are interested in the product?",
          "score": 5,
          "created_utc": "2026-01-27 15:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22tw92",
              "author": "b0307",
              "text": "same. i want to pay just to try the agent swarm but i cant find any details on how much usage I get, not even a vague description.",
              "score": 1,
              "created_utc": "2026-01-27 19:56:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ywt7k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 9,
          "created_utc": "2026-01-27 06:02:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yz1pb",
              "author": "misterflyer",
              "text": "[https://openrouter.ai/moonshotai/kimi-k2.5](https://openrouter.ai/moonshotai/kimi-k2.5)\n\nAnd yes, Mr. Wayne...\n\n... it *does* come in ***black***",
              "score": 24,
              "created_utc": "2026-01-27 06:20:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yzi0d",
                  "author": "nycigo",
                  "text": "That's a bit expensive for a Chinese AI.",
                  "score": -29,
                  "created_utc": "2026-01-27 06:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zrdr6",
          "author": "inkberk",
          "text": "SOOOOOOTTTTTAAAAAAA!!!!  \nGreat job Kimi Team!",
          "score": 6,
          "created_utc": "2026-01-27 10:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20bgdz",
          "author": "c00pdwg",
          "text": "Thank god they provided the legend at the top of their graph",
          "score": 3,
          "created_utc": "2026-01-27 13:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zd1g0",
          "author": "Different_Fix_2217",
          "text": "It seems really good so far. For sure best local model, need time to compare to claude / gpt 5.2.",
          "score": 7,
          "created_utc": "2026-01-27 08:20:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zhqwj",
              "author": "ArFiction",
              "text": "what about compared to glm / m2.1?",
              "score": 6,
              "created_utc": "2026-01-27 09:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zjpbr",
                  "author": "Different_Fix_2217",
                  "text": "For sure better than those but those are really small models for low level tasks locally / implementing other model's planning for cheap. Not really fair to compare imo. This is more around actual cloud models.",
                  "score": 10,
                  "created_utc": "2026-01-27 09:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zqmbp",
          "author": "Icy_Butterscotch6661",
          "text": "Whatâ€™s â€œvisual codingâ€ in this context?",
          "score": 3,
          "created_utc": "2026-01-27 10:25:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24bjgr",
              "author": "uhuge",
              "text": "see this f'd up button? make it ðŸ”˜&ðŸŒŸ",
              "score": 2,
              "created_utc": "2026-01-28 00:08:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o220ffa",
          "author": "Alternative-Way-7894",
          "text": "Looks like there is new architecture here with Ktransformers and KT-Kernel where you can get heteregenous inference where about 100GB of VRAM is enough to run the model at decent speeds if you have over 600 GB system RAM! Looks to be able to get decent output with this new technology! They even tried with as little as 48GB VRAM (2x RTX 4090)\n\nVery exciting!\n\nHave a look [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.5.md)\n\n\\*EDIT\\* If you have even more system RAM....look at this. Not bad at all!\n\n\"This achieves end-to-end LoRA SFT Throughput: 44.55 token/s on 2Ã— NVIDIA 4090 + Intel 8488C with 1.97T RAM and 200G swap memory.\"\n\nMore details refer toÂ [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT\\_Installation\\_Guide\\_KimiK2.5.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/SFT_Installation_Guide_KimiK2.5.md)Â .",
          "score": 3,
          "created_utc": "2026-01-27 17:49:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228ff3",
          "author": "ConsciousArugula9666",
          "text": "already alot of provider choices and free on nvidia: [https://llm24.net/model/kimi-k2-5](https://llm24.net/model/kimi-k2-5)",
          "score": 3,
          "created_utc": "2026-01-27 18:23:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25kss4",
              "author": "BuffMcBigHuge",
              "text": "Didn't know about Nvidia! Thanks! Working great",
              "score": 1,
              "created_utc": "2026-01-28 04:10:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o229bna",
          "author": "newbee_2024",
          "text": "The speed of AI development is so fast that I wake up every day feeling like I'm falling behind againðŸ˜‚A brand new concept has emerged<visual coding>Will visual coding become futuristic, friends?",
          "score": 2,
          "created_utc": "2026-01-27 18:27:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23l34r",
          "author": "captain_cavemanz",
          "text": "I'm not convinced, have experimented against 5.2 on coding in an established rust codebase and its not as good in my highly opinionated view.\n\nAre there standardized metrics out there to do side by side performance on a codebase?",
          "score": 2,
          "created_utc": "2026-01-27 21:58:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o206qsf",
          "author": "Aggressive_Special25",
          "text": "How do you use this? Can I run in lm studio?",
          "score": 1,
          "created_utc": "2026-01-27 12:32:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22dt5k",
          "author": "Bloodipwn",
          "text": "How generous are the limits in the subscription plan? And did somebody already test how good it works in claude code?",
          "score": 1,
          "created_utc": "2026-01-27 18:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2420uo",
          "author": "Aggressive_Arm9817",
          "text": "This is so insanely good, has anybody tried it in real coding tasks?",
          "score": 1,
          "created_utc": "2026-01-27 23:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26bv3g",
          "author": "GosuGian",
          "text": "This is insane",
          "score": 1,
          "created_utc": "2026-01-28 07:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27twzt",
          "author": "pratiknarola",
          "text": "u/Kimi_Moonshot     \nI am hosting the model on my 8x H200 node as per huggingface model card using vllm.    \nbut i am getting \"(no content)\"   in the output content.   is this known?   any guidance on how i can fix it ?",
          "score": 1,
          "created_utc": "2026-01-28 14:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o289zoh",
          "author": "OkBottle1699",
          "text": "Kimy k2.5  amd ai",
          "score": 1,
          "created_utc": "2026-01-28 15:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z1p52",
          "author": "Hurricane31337",
          "text": "Wow, how many RTX 6000 Pro are needed to run this? ðŸ¥²",
          "score": 0,
          "created_utc": "2026-01-27 06:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zfxpr",
              "author": "power97992",
              "text": "7 Â if u dont want to offload it onto the cpu.( It is Â around 595 GB in safetensors..) Â ",
              "score": 10,
              "created_utc": "2026-01-27 08:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o207pn7",
                  "author": "KaroYadgar",
                  "text": "I flinched like an abused dog when I saw that number.",
                  "score": 11,
                  "created_utc": "2026-01-27 12:39:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20jyhq",
                  "author": "LocoMod",
                  "text": "So about $6000 in RAM alone before even discussing the rest of the hardware.",
                  "score": 3,
                  "created_utc": "2026-01-27 13:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zinh3",
              "author": "dobkeratops",
              "text": "2 x 512gb mac studio ? (connected with RDMA, a pair of them is shown to do inference at 1.8x the rate of 1)",
              "score": 11,
              "created_utc": "2026-01-27 09:12:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o214akz",
              "author": "Capaj",
              "text": "you only need 8 h200s :D\nYou can buy a server with this config in a single rack for like 350k USD",
              "score": 2,
              "created_utc": "2026-01-27 15:29:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o221jlt",
              "author": "Alternative-Way-7894",
              "text": "Looks like you will need only 1 if you have about 600GB system RAM",
              "score": 1,
              "created_utc": "2026-01-27 17:54:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z0c05",
          "author": "iamsimonsta",
          "text": "initial results indicate this model should have been named kimi2.5-preview, definitely not ready for serious use :(",
          "score": -7,
          "created_utc": "2026-01-27 06:30:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zkexh",
              "author": "__Maximum__",
              "text": "Elaborate?",
              "score": 5,
              "created_utc": "2026-01-27 09:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22j5wz",
                  "author": "iamsimonsta",
                  "text": "A simple code review request on 120K javascript file generated garbage, quoting non existent code with odd fixation on non existent backticks.",
                  "score": 1,
                  "created_utc": "2026-01-27 19:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zo65w",
              "author": "True_Requirement_891",
              "text": "People are downvoting but I'm getting buggy code and somehow it still doesn't match sonnet in quality... using it inside claude code.",
              "score": 0,
              "created_utc": "2026-01-27 10:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o227suk",
                  "author": "iamsimonsta",
                  "text": "Wow I am getting downvoting for testing it?  \n  \nI gave it the source (.js) to my current project asked it for a code review including any obvious bugs, and it hallucinated / tripped balls a list of fictional issues like.a 128K context model from 2024.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:20:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zevb3",
              "author": "zoyer2",
              "text": "ouch! sadge",
              "score": -2,
              "created_utc": "2026-01-27 08:37:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22kgpr",
          "author": "lemon07r",
          "text": "Does the Kimi for coding API use the new model now?",
          "score": 0,
          "created_utc": "2026-01-27 19:14:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq6n3t",
      "title": "GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/uf2m03ak2agg1.png",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-29 11:58:22",
      "score": 460,
      "num_comments": 101,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2eo3cr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 13:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ekl8p",
          "author": "combrade",
          "text": "The IPTV shit is useful, projects like that. Donâ€™t discourage those . High school Me learned about Github through projects that sailed the high seas for movies and shows.",
          "score": 83,
          "created_utc": "2026-01-29 13:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2et69h",
          "author": "skybsky",
          "text": "I'm somewhat pleased that some comments actually see that this post is bs. \"somewhat\", because others still don't read before commenting",
          "score": 45,
          "created_utc": "2026-01-29 13:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ehseu",
          "author": "Seaborgg",
          "text": "This post has me the feeling that you were hoping I didn't read the text in the image.",
          "score": 191,
          "created_utc": "2026-01-29 12:53:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fcd94",
              "author": "ObsidianNix",
              "text": "Thas how you know they didnt either, most likely just a bot on Reddit. Maybe another case study?",
              "score": 50,
              "created_utc": "2026-01-29 15:31:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ha43h",
                  "author": "DinoAmino",
                  "text": "Totally. And weak-ass posts like this don't get this amount of upvotes without a bot army by your side.",
                  "score": 7,
                  "created_utc": "2026-01-29 20:50:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nypl1",
                  "author": "SuchAGoodGirlsDaddy",
                  "text": "I betchya itâ€™s posted by an agent framework on GitHub that will be dead in one week.",
                  "score": 1,
                  "created_utc": "2026-01-30 20:04:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gp23s",
              "author": "_stack_underflow_",
              "text": "Are they based on your own starred repos?\n\nhttps://preview.redd.it/q4m9mpsm7cgg1.png?width=1555&format=png&auto=webp&s=4aeac74079c5019cba9cb067f7307aff0a9a985b\n\n",
              "score": 12,
              "created_utc": "2026-01-29 19:09:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hbcol",
                  "author": "ObsidianNix",
                  "text": "Probably not. The screenshot the bot posted is not logged in. Look at the top right. Its probably a rolling Trending where theyâ€™re just too many so it randomly selects topN repos.",
                  "score": 5,
                  "created_utc": "2026-01-29 20:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ks6z3",
              "author": "DryWeb3875",
              "text": "https://preview.redd.it/269bii71mggg1.jpeg?width=1179&format=pjpg&auto=webp&s=dd6086c76803b1c8db6ab831beb1663326be24fc\n\nBlue is AI based, green isnâ€™t.\n\nEdit: mlx-audio is also AI based.",
              "score": 2,
              "created_utc": "2026-01-30 09:57:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2etr3k",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -1,
              "created_utc": "2026-01-29 14:00:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2eu516",
                  "author": "eli_pizza",
                  "text": "What do you think â€œbotâ€ means?",
                  "score": 4,
                  "created_utc": "2026-01-29 14:02:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2edemx",
          "author": "gscjj",
          "text": "> half are agent frameworks \n\nI only see one agent framework here and itâ€™s by Microsoft. Other that I see RAG tooling, model(NanoGPT, Grok), model cli for code (Kimi), browser API",
          "score": 131,
          "created_utc": "2026-01-29 12:24:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h2n3b",
              "author": "PaddyIsBeast",
              "text": "I mean I completely disagree with op, but.. by who's definition are MCP servers and Agent skills not part of the agentic framework?",
              "score": 1,
              "created_utc": "2026-01-29 20:14:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ee0zs",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -36,
              "created_utc": "2026-01-29 12:28:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2efzp6",
                  "author": "gscjj",
                  "text": "Theyâ€™re all AI related, theyâ€™re not all agentic and only is a framework",
                  "score": 42,
                  "created_utc": "2026-01-29 12:41:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2egqlo",
                  "author": "macumazana",
                  "text": "so, youre comparing a model, rag, skills as prompt and tools? \n\nwhy dont you go further and compare a laptop a programming language and a human hand?",
                  "score": 33,
                  "created_utc": "2026-01-29 12:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2erky4",
          "author": "belgradGoat",
          "text": "Which ones? Microsoft vibe voice or grok? Did you even red or look at what you posted?",
          "score": 29,
          "created_utc": "2026-01-29 13:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g9cqv",
              "author": "FaceDeer",
              "text": "Presumably he figures Microsoft and Grok are going to be dead in a week.",
              "score": 7,
              "created_utc": "2026-01-29 17:59:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2gim1t",
                  "author": "Former-Ad-5757",
                  "text": "Well qwen4 can probably code windows in 1 hour, office in 30 minutes and all azure functions in like 4 hours, so basically I see no reason ms should not be dead in one week as long as qwen4 is released and performs as I suspectâ€¦",
                  "score": 1,
                  "created_utc": "2026-01-29 18:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eiavw",
          "author": "one-wandering-mind",
          "text": "I'm surprised there haven't been more stories about supposed AI tooling with viruses or exploits. People are so willing to install and run random code they find on github that is brand new with no security review.\n\n\nPeople on this sub particularly seem terrified to use model in the cloud for fear of their sensitive data being exposed, but many will happily install random stuff and implement networking they don't understand that risks exposing their home server to attacks.",
          "score": 26,
          "created_utc": "2026-01-29 12:56:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f61y0",
              "author": "SilentLennie",
              "text": "I mean...\n\nhttps://www.reddit.com/r/vibecoding/comments/1qpnybr/found_a_malicious_skill_on_the_frontpage_of/",
              "score": 8,
              "created_utc": "2026-01-29 15:02:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2fagw1",
              "author": "TinFoilHat_69",
              "text": "You could just look through the code in the repo and ask GitHub copilot to review the repo itâ€™s built into GitHubs website. I personally would rather just look at designs and ideas and implement my version to fit my own needs.",
              "score": 2,
              "created_utc": "2026-01-29 15:23:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hk688",
                  "author": "eli_pizza",
                  "text": "I'm pretty confident I could write some malicious code that passes that check and I'm not exactly a malware expert.",
                  "score": 3,
                  "created_utc": "2026-01-29 21:38:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2if943",
              "author": "No_Afternoon_4260",
              "text": "There's [that](https://professorsigmund.com/pdfs/glass_box_paradox.pdf) about the clawdbot's mess\n\nEdit: not really about clawdbot but badly configured local ai stuff based on clawdbot mess study",
              "score": 1,
              "created_utc": "2026-01-30 00:18:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ebp1v",
          "author": "Pretend-Pangolin-846",
          "text": "Off-topic, but what are claude skills?",
          "score": 33,
          "created_utc": "2026-01-29 12:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2eci2z",
              "author": "adam444555",
              "text": "A fruitful word for the on-demand system prompt.",
              "score": 79,
              "created_utc": "2026-01-29 12:18:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ecrmn",
                  "author": "Pretend-Pangolin-846",
                  "text": "I get the appeal though, its as if an optimized AGENTS.md file for whatever task or domain you want to work with",
                  "score": 14,
                  "created_utc": "2026-01-29 12:20:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2f1627",
                  "author": "1ncehost",
                  "text": "Skills can also include scripts",
                  "score": 4,
                  "created_utc": "2026-01-29 14:38:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ebvbu",
              "author": "Distinct-Expression2",
              "text": "Text that it loads on demend usually describe how to do something specific",
              "score": 11,
              "created_utc": "2026-01-29 12:14:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ec5n5",
                  "author": "Pretend-Pangolin-846",
                  "text": "oh, is it claude dependent, or can it be initialized into any IDE?",
                  "score": 6,
                  "created_utc": "2026-01-29 12:16:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2epbkb",
                  "author": "Electronic-Ice-8718",
                  "text": "So in theory 1 request will need at least 2 LLM calls. One for checking which prompts to load, another one to actually process the input?",
                  "score": 2,
                  "created_utc": "2026-01-29 13:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2epoyl",
              "author": "DJT_is_idiot",
              "text": "I'm amazed by the answers you received. Are people that clueless in here? Only one answer so far that actually scratches the surface of what skills are.\n\nEdit: maybe it was satire and I just didn't get it",
              "score": 4,
              "created_utc": "2026-01-29 13:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f2vlq",
                  "author": "Pretend-Pangolin-846",
                  "text": "Not satire, I am serious.",
                  "score": 2,
                  "created_utc": "2026-01-29 14:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2gftrx",
              "author": "paperpizza2",
              "text": "Tool call definitions but saved in a text file.",
              "score": 1,
              "created_utc": "2026-01-29 18:27:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ele6b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -3,
              "created_utc": "2026-01-29 13:15:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2enwdh",
                  "author": "abnormal_human",
                  "text": "No, they're just chunks of system prompt that you can easily turn on/off. Literally just .claude/skills/foo/SKILL.md.   \n  \nIf you have a repeated dev task, you can document the procedure, layers of code in the system, etc, and make Claude Code more reliable at repeating that process. I have skills for iterating on agent evals so I can in one command get it to run my eval suite, prioritize the things I want prioritized, iterate on the failures in a certain efficient way, then come back to me when the score is improved by a certain amount. I have another one for integrating a form of API integration that my project does very often so that it knows what files/layers to touch and avoids pitfalls. And a third when I'm working on payments/etc stuff that makes sure a current copy of our billing mechanics is in scope. Not a super complex idea, but very effective.",
                  "score": 4,
                  "created_utc": "2026-01-29 13:29:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2esje8",
          "author": "russianguy",
          "text": "I feel the same way about MCP servers. I ain't about to run some code from `some-fucking-guy.sh` repo.",
          "score": 8,
          "created_utc": "2026-01-29 13:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2etb23",
              "author": "Distinct-Expression2",
              "text": "Mcp are dead, skills way better",
              "score": -8,
              "created_utc": "2026-01-29 13:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2fbabl",
                  "author": "ZachCope",
                  "text": "Doesnâ€™t MCP â€˜doâ€™ something and a skill tells the LLM â€˜how to doâ€™ something? 2 different functions.Â ",
                  "score": 11,
                  "created_utc": "2026-01-29 15:26:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2eio82",
          "author": "raysar",
          "text": "This is a good thing. Concurrence is the only solution fort fast evolution.",
          "score": 3,
          "created_utc": "2026-01-29 12:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ef2f3",
          "author": "vesko26",
          "text": "this is much worse then JS framework hell. Those at least kinda worked. This is just garbage",
          "score": 21,
          "created_utc": "2026-01-29 12:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f48q7",
              "author": "BigHugeOmega",
              "text": "How is it garbage? Which software on this list doesn't work?",
              "score": 13,
              "created_utc": "2026-01-29 14:53:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2f5uoz",
                  "author": "vesko26",
                  "text": "MCP as a standard is garbage, literally the first issue on the ai=data=science-team project is arbitrary code execution due to prompt injection",
                  "score": -6,
                  "created_utc": "2026-01-29 15:01:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2fzl57",
          "author": "CV514",
          "text": "Thanks\n\nNow I'm aware there is GitHub Trending\n\nAnd it is as worthless as I imagined",
          "score": 3,
          "created_utc": "2026-01-29 17:15:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eqb5k",
          "author": "Admirable-Choice9727",
          "text": "I hate \"browser-use\" it disappointed me so much.",
          "score": 4,
          "created_utc": "2026-01-29 13:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fxf2t",
              "author": "vatta-kai",
              "text": "How exactly was it disappointing? Iâ€™m thinking of trying it out",
              "score": 1,
              "created_utc": "2026-01-29 17:05:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2eqnwu",
              "author": "Distinct-Expression2",
              "text": "Itâ€™s shit; just use vercel agent-browser",
              "score": -3,
              "created_utc": "2026-01-29 13:44:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2f2agi",
          "author": "Healthy-Nebula-3603",
          "text": "Natural selection...",
          "score": 2,
          "created_utc": "2026-01-29 14:44:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fc1mc",
          "author": "Alex_1729",
          "text": "Meaningless hateful post.",
          "score": 2,
          "created_utc": "2026-01-29 15:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gabq3",
              "author": "FaceDeer",
              "text": "And even if true, what's wrong with throwing lots of stuff at the wall knowing that only 10% of it is going to stick? This is a time of experimentation and new ideas.",
              "score": 2,
              "created_utc": "2026-01-29 18:03:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2goy1w",
                  "author": "Alex_1729",
                  "text": "Who knows their reasons, but hating on vibecoding, something which if you're not using, simply means you're not adaptive and probably not a very resourceful person.\n\nAcknowledging your point, I'll go one step further: even if it's mostly slop, it's worth it. Most people don't want slop, but you have to make tradeoffs, so it can end up not a perfect product. That's reasonable. A person said this in a recent email newsletter I read, and I quote:\n\n>... working with agents is genuinely so fun and 2026 will be the year ofÂ slopÂ - given the above advancements (I agree - but slopping our way to learn and produce things that arenâ€™tÂ slopÂ is still a reasonable path).",
                  "score": 4,
                  "created_utc": "2026-01-29 19:09:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2h7xkn",
                  "author": "Mickenfox",
                  "text": "Building one of the same thing we have 20 of is not experimentation. It's just a waste of human hours that could have gone to improving a real product.",
                  "score": 1,
                  "created_utc": "2026-01-29 20:39:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ekliy",
          "author": "MaxKruse96",
          "text": "Yes, we are in hell. People somehow got the idea that \"Agents\" are the future, and \"We have jarvis at home\" - when in reality nothing is even remotely ready or secure enough for that.",
          "score": 2,
          "created_utc": "2026-01-29 13:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h8110",
              "author": "Mickenfox",
              "text": "Agents is just the next buzzword after AI wasn't buzzy enough.",
              "score": 2,
              "created_utc": "2026-01-29 20:40:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ey4w1",
          "author": "InterstellarReddit",
          "text": "everyone is just recycling the same open source solution to a problem with a little change ui twist not changing anything of value.",
          "score": 1,
          "created_utc": "2026-01-29 14:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f5ufx",
          "author": "SilentLennie",
          "text": "Interesting kimi-cli did make it, moltbot (formerly Clawd Bot) did not.\n\nWhatever you think of it: it's open source/free software at work: 'scratch your own itch', something doesn't do what you want and the existing solutions you know about it's not a good fit... create your own.",
          "score": 1,
          "created_utc": "2026-01-29 15:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fxsyk",
          "author": "visarga",
          "text": "Hey, I have 2 or 3 of my own, I am off to writing the 4th now. But I did learn interesting lessons from this exercise.",
          "score": 1,
          "created_utc": "2026-01-29 17:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g473x",
          "author": "blurredphotos",
          "text": "Just like AI art boards.\n\nSad but true.",
          "score": 1,
          "created_utc": "2026-01-29 17:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h83va",
          "author": "Mickenfox",
          "text": "Oh yeah, GitHub is all AI-coded AI-related slop. Twice the AI, double the slop.",
          "score": 1,
          "created_utc": "2026-01-29 20:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hb70x",
          "author": "cleverusernametry",
          "text": "By agent framework, I think you mean ai tooling. And by 90%, I think you mean 99%",
          "score": 1,
          "created_utc": "2026-01-29 20:55:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hfxm9",
          "author": "WSATX",
          "text": "That's life men, maybe 90% of us will be gone in one week. But posting screenshot only post never die, eh, life is unfair !",
          "score": 1,
          "created_utc": "2026-01-29 21:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iclso",
          "author": "Ok-Lobster-919",
          "text": "BREAKING: OP Discovers what the term 'trending' means.",
          "score": 1,
          "created_utc": "2026-01-30 00:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iiipk",
          "author": "thetaFAANG",
          "text": "Its the â€œget me $1,000,000/yr job in aiâ€ moment of ai",
          "score": 1,
          "created_utc": "2026-01-30 00:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kv6ui",
          "author": "Anthonyg5005",
          "text": "The github trending page has basically been like this since 2023. All just random ai framework stuff that no one ever talks about then when you go to it's repo it always has a huge banner in the readme saying \"top repo of the week\"",
          "score": 1,
          "created_utc": "2026-01-30 10:24:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2earqn",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -5,
          "created_utc": "2026-01-29 12:06:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2egsdz",
              "author": "gnnr25",
              "text": "Your website is spammy. A pop-up chat and a pop-up newsletter subscription. Geeze",
              "score": 29,
              "created_utc": "2026-01-29 12:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eu62x",
          "author": "IulianHI",
          "text": "The comparison to JS framework hell is spot on. Remember when we had jQuery, Backbone, Ember, Angular, React, Vue, Svelte, and 100 others all trying to solve the same problem? Most of these agent frameworks will meet the same fate - the winners will be the ones that focus on boring, reliable infrastructure (memory, evals, tracing) rather than shiny demos.",
          "score": 1,
          "created_utc": "2026-01-29 14:02:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ebzb4",
          "author": "k_am-1",
          "text": "Trending page is showing whatâ€™s trending HELPPP!!?",
          "score": -7,
          "created_utc": "2026-01-29 12:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ecuoq",
          "author": "Familiar_Print_4882",
          "text": "I created AI primitives (not frameworks)\nDo you think that will die in 1 week too ?\n\nhttps://github.com/withceleste/celeste-python",
          "score": -7,
          "created_utc": "2026-01-29 12:20:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ef1xy",
              "author": "Distinct-Expression2",
              "text": "you created a proxy which you'll have to maintain yourself adding delay in the request and forcing people to self host it when all major providers are switching to open responses - quite bad timing",
              "score": 11,
              "created_utc": "2026-01-29 12:35:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2eg9e2",
                  "author": "Familiar_Print_4882",
                  "text": "Itâ€™s fully compatible with openresponses.\nAnd no need to maintain much, just the api changes which happen 1nce per year.\nAlso you can use extra_body if anythingâ€™s not integrated so you never miss a new feature.\nWhat do you think now ? Convinced or not.\nI prefer youâ€™re not and tell me why.\nIâ€™m looking for an argument so Iâ€™ll be like ah yeah ok not worth it then, it has no future.",
                  "score": 1,
                  "created_utc": "2026-01-29 12:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ev76p",
          "author": "KitchenSomew",
          "text": "The pattern we're seeing here mirrors every tech hype cycle - lots of experimentation, most will consolidate or fade. The frameworks that will survive are the ones solving real infrastructure problems: reliable memory management, proper eval frameworks, and production-ready orchestration.\n\n\n\nMost of these are just thin wrappers around LangChain or AutoGen with marketing fluff. The real value will be in the unglamorous work - robust error handling, proper logging/tracing, and actual testing frameworks for agents.",
          "score": -1,
          "created_utc": "2026-01-29 14:08:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f0mlx",
          "author": "KitchenSomew",
          "text": "Classic AI hype cycle in action! While many will fade away, the silver lining is that this explosion of frameworks helps identify what actually works. The ones that survive typically solve a real problem with a clean API. I'm curious which approaches will still be standing in 6 months - my bet is on the ones with strong community support and clear documentation.",
          "score": -2,
          "created_utc": "2026-01-29 14:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2egb2m",
          "author": "DeathByPain",
          "text": "I'm definitely not trending but I'm working on a fork of a gui for llama-server that will let you launch multiple servers in a tabbed window ðŸ˜",
          "score": -5,
          "created_utc": "2026-01-29 12:44:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnk7fq",
      "title": "transformers v5 final is out ðŸ”¥",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "author": "unofficialmerve",
      "created_utc": "2026-01-26 16:07:40",
      "score": 455,
      "num_comments": 42,
      "upvote_ratio": 0.99,
      "text": "Hey folks, it's Merve from Hugging Face ðŸ‘‹ðŸ»\n\nWe've finally released the first stable release of transformers v5 in general audience, it comes with many goodies:\n\n\\- Performance especially for Mixture-of-Experts (6x-11x speedups)\n\n\\- No more slow/fast tokenizers: way simpler API, explicit backends, better performance\n\n\\- dynamic weight loading: way faster, MoE now working with quants, tp, PEFT..\n\nWe have a migration guide on the main branch; please take a look at it in case you run into issues, we also have documented everything in release notes. We appreciate the feedbacks, so feel free to create issues if you have any! ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v9rxe",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-26 18:45:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uaagj",
          "author": "jacek2023",
          "text": "\"Performance especially for Mixture-of-Experts (6x-11x speedups)\" please explain",
          "score": 90,
          "created_utc": "2026-01-26 16:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uaknp",
              "author": "MaxKruse96",
              "text": "Best guess is that transformers was horribly slow for them before, and now is better",
              "score": 85,
              "created_utc": "2026-01-26 16:14:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1we09f",
                  "author": "kthx0",
                  "text": "If you improved performance 2x you did something clever, if you improved it 10x you stopped doing something stupid",
                  "score": 51,
                  "created_utc": "2026-01-26 21:40:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uv83j",
                  "author": "TheRealMasonMac",
                  "text": "For reference, with the same setup, GLM-4.7-Flash currently takes 7 minutes per step. Gemma 27B takes 40 seconds.\n\n\nI guess the Unsloth team was waiting for this since they promised faster MoE training in the coming week.",
                  "score": 28,
                  "created_utc": "2026-01-26 17:43:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1usqps",
              "author": "NandaVegg",
              "text": "Transformers v4 had rather simple for loop for MoE model experts (except GPT-OSS, which had custom code for performance from day one, I believe) which caused massive under-utilization. As well. they now have more generalized solution for custom kernels.\n\nCongrats for the release, by the way!",
              "score": 20,
              "created_utc": "2026-01-26 17:32:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uau3z",
              "author": "jikkii",
              "text": "hey, there are mainly two PRs responsible for this:\n\n\\- [https://github.com/huggingface/transformers/pull/43126](https://github.com/huggingface/transformers/pull/43126)\n\n\\- [https://github.com/huggingface/transformers/pull/42697](https://github.com/huggingface/transformers/pull/42697)\n\nand more coming to continue down this road. These are initial speedups, but expect more down the road as we continue improving on it, delivering specialized kernels, etc.\n\n  \nEDIT: we have a dedicated post about it if you want to check it out: [https://www.linkedin.com/posts/ilyas-moutawwakil\\_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm\\_source=share&utm\\_medium=member\\_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg](https://www.linkedin.com/posts/ilyas-moutawwakil_tldr-up-to-11-faster-moe-inference-in-activity-7413936534367653888-NiiK?utm_source=share&utm_medium=member_desktop&rcm=ACoAAByt4j0BPuhDE8Ac9gwVKClDzL7Nx7l-6tg)",
              "score": 35,
              "created_utc": "2026-01-26 16:16:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1uikwd",
                  "author": "llama-impersonator",
                  "text": "shouldn't this be an hf blog?",
                  "score": 26,
                  "created_utc": "2026-01-26 16:49:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zcyun",
              "author": "woct0rdho",
              "text": "You know it if you've seen this https://github.com/woct0rdho/transformers-qwen3-moe-fused\n\nThe MoE support in Transformers 5 is great, and there is still a lot of room to speedup on consumer GPUs",
              "score": 4,
              "created_utc": "2026-01-27 08:19:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1uco0j",
              "author": "bick_nyers",
              "text": "Less for loops is my guess.",
              "score": 3,
              "created_utc": "2026-01-26 16:23:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x9x9f",
              "author": "RainierPC",
              "text": "From the 6x to 11x statement, it sure sounds like they parallelized things, that's why the range is written like that - the speedup depends on how many experts there are.",
              "score": 3,
              "created_utc": "2026-01-27 00:15:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ub304",
              "author": "-philosopath-",
              "text": "https://preview.redd.it/xqivm6x4ypfg1.png?width=744&format=png&auto=webp&s=6ff49e94d3d569bd414867d17f32d220f57e8715",
              "score": -6,
              "created_utc": "2026-01-26 16:17:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vcgvo",
          "author": "sir_creamy",
          "text": "this is awesome.  updated to v5 and vllm 0.14.1 (from 0.11) and my single prompt inference speed is up 50% and 40x concurrent inference up 100%",
          "score": 10,
          "created_utc": "2026-01-26 18:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ysqjq",
              "author": "__JockY__",
              "text": "I was like \"there's no fucking way\".\n\nUpdated vLLM from 0.12 to 0.14.1 and tps went from 70/sec to 98/sec with MiniMax-M2.1 FP8 on quad 6000 Pros. Holy fucking shit. That's an IMMENSE update.",
              "score": 6,
              "created_utc": "2026-01-27 05:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yw3os",
                  "author": "sir_creamy",
                  "text": "Glad it worked out for you! Â Iâ€™m going to test using the transformers tomorrow",
                  "score": 1,
                  "created_utc": "2026-01-27 05:57:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1vkoie",
              "author": "MammayKaiseHain",
              "text": "Does vllm use transformers internally ? I thought they had their own engine",
              "score": 6,
              "created_utc": "2026-01-26 19:31:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1w0jql",
                  "author": "sir_creamy",
                  "text": "I'm not sure -- why i included that i updated vllm as well",
                  "score": 4,
                  "created_utc": "2026-01-26 20:41:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uqyax",
          "author": "Edenar",
          "text": "Ok, what does that mean for me running small-medium sized MoE locally using llama.cpp on an NVIDIA GPU or AMD igpu (ie Strix Halo) ?\n(My feeling is : it use more compute so running MoE will be less memory bandwidth bound ? Or maybe i don't understand at all...)",
          "score": 17,
          "created_utc": "2026-01-26 17:24:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v0qpq",
              "author": "Thick-Protection-458",
              "text": "Llama.cpp is a fully separated engine.\n\n\nVllm maybe reuse some transformers internals, but not llamacpp",
              "score": 12,
              "created_utc": "2026-01-26 18:07:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2h5t",
                  "author": "Edenar",
                  "text": "Thx!",
                  "score": 3,
                  "created_utc": "2026-01-26 18:14:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1v11nw",
              "author": "the__storm",
              "text": "Nothing, `transformers` the Python library is not involved when you're running a model with llama.cpp.  It's often the \"default\" non-production way to run a new model though, before it gets support in other inference engines (llama.cpp, vllm, etc.)",
              "score": 32,
              "created_utc": "2026-01-26 18:08:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1v2g08",
                  "author": "Edenar",
                  "text": "Thank you!",
                  "score": 6,
                  "created_utc": "2026-01-26 18:14:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1wp380",
                  "author": "segmond",
                  "text": "In the long term it means we can borrow ideas from the transformer implementation library and improve llama.cpp",
                  "score": 4,
                  "created_utc": "2026-01-26 22:31:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uw9yd",
          "author": "Odd-Ordinary-5922",
          "text": "\"MoE now working with quants\" this didnt work before?",
          "score": 4,
          "created_utc": "2026-01-26 17:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wngbw",
          "author": "TumbleweedDeep825",
          "text": "WHAT I'VE DOOOOOONE............. oh wait, wrong transformers",
          "score": 7,
          "created_utc": "2026-01-26 22:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xmlx5",
              "author": "jikilan_",
              "text": "New divine?",
              "score": 1,
              "created_utc": "2026-01-27 01:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v8fkp",
          "author": "DigThatData",
          "text": "still no movement on the mythical `.generate` refactor then I take it?\n\nhttps://github.com/huggingface/transformers/issues/30810",
          "score": 2,
          "created_utc": "2026-01-26 18:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ucb26",
          "author": "a_beautiful_rhind",
          "text": "All previous stuff still works as before?",
          "score": 1,
          "created_utc": "2026-01-26 16:22:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ucpdo",
              "author": "-p-e-w-",
              "text": "No, otherwise there would be no need for a migration guide.",
              "score": 27,
              "created_utc": "2026-01-26 16:24:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ut50v",
                  "author": "FullstackSensei",
                  "text": "So, maintainer of projects using HF can expect a wave of AI PRs offering to upgrade to v5?",
                  "score": 4,
                  "created_utc": "2026-01-26 17:34:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2q8gw2",
                  "author": "-Cicada7-",
                  "text": "Please where can I find this guide , my code is broken ðŸ˜­",
                  "score": 1,
                  "created_utc": "2026-01-31 03:20:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1vez7t",
              "author": "TokenRingAI",
              "text": "Nope, it breaks everything",
              "score": 5,
              "created_utc": "2026-01-26 19:07:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ucug7",
              "author": "jikkii",
              "text": "some of the internals are reworked to offer a more solid, faster base. Some APIs are also reworked; we recommend you read the release notes before upgrading and that you test your stack on the new version. If there's anything missing or weird, don't hesitate to open an issue and we'll work with you on resolving them",
              "score": 4,
              "created_utc": "2026-01-26 16:24:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1w5kq9",
          "author": "IulianHI",
          "text": "oh nice, the quantized cache alone saved me like 6GB on my setup which is huge. been benchmarking these improvements on r/AIToolsPerformance and the MoE speedups are wild for running stuff like Qwen3 locally. also the simpler tokenizer API was long overdue tbh",
          "score": 1,
          "created_utc": "2026-01-26 21:03:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wlsii",
          "author": "rudokazexotohatu0r",
          "text": "Great work from the team. Looking forward to benchmarking those MoE speedups.",
          "score": 1,
          "created_utc": "2026-01-26 22:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1x2t5y",
          "author": "victoryposition",
          "text": "Tried it -- but get OOMs when dealing with tight VRAM margins... it has an automatic cache warmup to load models faster. But I can confirm the grouped\\_mm is much faster for calibration.",
          "score": 1,
          "created_utc": "2026-01-26 23:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xom93",
          "author": "Plenty-Aerie1114",
          "text": "Thank you!!!",
          "score": 1,
          "created_utc": "2026-01-27 01:33:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ukzl9",
          "author": "fairydreaming",
          "text": "Finally! Hopefully DeepSeek V3.2-Exp/V3.2 support will be merged soon now. Four months to support a new model arch is a bit too long. :-)",
          "score": 1,
          "created_utc": "2026-01-26 16:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vtdh3",
          "author": "pmv143",
          "text": "Dynamic weight loading is the most interesting part of this release imo.",
          "score": 1,
          "created_utc": "2026-01-26 20:09:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfse6",
      "title": "Run Kimi K2.5 Locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/rxqfj5os74gg1.jpeg",
      "author": "Dear-Success-1441",
      "created_utc": "2026-01-28 16:17:45",
      "score": 437,
      "num_comments": 78,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2bui1v",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-29 01:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28rq6e",
          "author": "Daniel_H212",
          "text": "Anyone tried this on strix halo yet to see how many seconds per token it runs at?",
          "score": 157,
          "created_utc": "2026-01-28 16:49:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28uh5w",
              "author": "IngwiePhoenix",
              "text": "> how many seconds per token\n\nFelt. XD",
              "score": 101,
              "created_utc": "2026-01-28 17:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28yf7t",
                  "author": "bobaburger",
                  "text": "definitely the right question to ask",
                  "score": 30,
                  "created_utc": "2026-01-28 17:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2978iw",
              "author": "JamesEvoAI",
              "text": "I wanted to, but the smallest quant is still 240GB",
              "score": 24,
              "created_utc": "2026-01-28 17:56:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2b3svm",
              "author": "Zyj",
              "text": "Edit: **it runs at 12.5 tokens/s with no context on dual Strix Halo**\n\nCurrently downloading on dual Strix Halo. Performance could be decent, it's 32b active parameters, at 1.8bits per parameter that's 7.2 GB of RAM.\n\nEdit:\n\nOK, by itself llama-server tries to allocate too much data on the second host running rpc-server. I have to manually specify \\`-fit off --tensor-split 48,52\\`\n\n`load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = false)`  \n`load_tensors: offloading output layer to GPU`  \n`load_tensors: offloading 60 repeating layers to GPU`  \n`load_tensors: offloaded 62/62 layers to GPU`  \n`load_tensors: CPU model buffer size = 630.00 MiB`  \n`load_tensors: ROCm0 model buffer size = 109322.47 MiB`  \n`load_tensors: RPC0[192.168.220.2:50052] model buffer size = 118481.80 MiB`\n\nUnfortunately, after loading for several minutes, llama-server crashes after reporting a malformed response from rpc-server:\n\n`llama_context: n_ctx_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized`  \n`llama_context: ROCm_Host output buffer size = 1.25 MiB`  \n`llama_kv_cache: ROCm0 KV buffer size = 279.00 MiB`  \n`llama_kv_cache: RPC0[192.168.220.2:50052] KV buffer size = 270.00 MiB`  \n`llama_kv_cache: size = 549.00 MiB ( 4096 cells, 61 layers, 2/2 seqs), K (f16): 549.00 MiB, V (f16): 0.00 MiB`  \n`sched_reserve: reserving ...`  \n`sched_reserve: Flash Attention was auto, set to enabled`  \n`sched_reserve: RPC0[192.168.220.2:50052] compute buffer size = 264.01 MiB`  \n`sched_reserve: ROCm0 compute buffer size = 362.00 MiB`  \n`sched_reserve: CPU compute buffer size = 36.01 MiB`  \n`sched_reserve: graph nodes = 4852`  \n`sched_reserve: graph splits = 3`  \n`sched_reserve: reserve took 251.87 ms, sched copies = 1`  \n`common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)`  \n`/opt/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp:669: Remote RPC server crashed or returned malformed response`  \n`recv failed (bytes_recv=0, size_to_recv=8)`\n\nHowever, on the side of rpc-server, there are no errors visible in the log.  \nTried it again with a split of 51,49. I got this on the rpc-server side:\n\n`ROCm error: unspecified launch failure`  \n`current device: -1, in function ggml_backend_cuda_synchronize at /opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:2850`  \n`hipStreamSynchronize(cuda_ctx->stream())`  \n`/opt/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:96: ROCm error`\n\nI might be hitting a ROCm bug. I'm using Fedora 43, kernel 6.18.3 and the latest rocm nightly, updated a few minutes ago.\n\nEdit:  \nGot it working with tensor-spliit 46,54, **it runs at 12.5 tokens/s with no context.**  \nI can see the reasoning, it looks like the llama-server webgui doesn't recognize the markup that Kimi K2.5 uses to mark the reasoning or perhaps it doesn't mark it properly at all.",
              "score": 25,
              "created_utc": "2026-01-28 23:00:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2bibzi",
                  "author": "Zyj",
                  "text": "It doesn't seem to be very good... more testing tomorrow",
                  "score": 5,
                  "created_utc": "2026-01-29 00:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29jaip",
              "author": "RedParaglider",
              "text": "It's a pretty big lift to get a 100gb model loaded on the old girl.  My strix halo has 128gb shared vram/ram so this thing is out of bounds if I need 256gb ram. \n\nThe way it works is you use your ram to pull the model off the disk, then it unloads into the same ram pool the kernel just allocates that ram to vram.  Then you also need additional vram to utilize for context.\n\n I have yet to see another model that will make me switch of glm 4.5 air derestricted.  It works well for pretty simple agentic tasks, creative writing, looking shit up on the web, looking for race conditions in code, etc.  Speed is around 23 t/s so it's not fast, but there is overhead calling remote API's that often means it's faster for simple tasks than even Gemini flash.",
              "score": 13,
              "created_utc": "2026-01-28 18:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a0ym1",
                  "author": "JamesEvoAI",
                  "text": "Give Qwen 3 VL 30B-A3B a shot. It's been my workhorse on this machine and leaves me plenty of memory for other models/tools",
                  "score": 7,
                  "created_utc": "2026-01-28 20:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2c1yja",
                  "author": "Far-Low-4705",
                  "text": "gpt-oss 120b is also very good in my experience. no idea about anything beyond STEM tho",
                  "score": 4,
                  "created_utc": "2026-01-29 02:01:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2971oe",
              "author": "Sufficient_Prune3897",
              "text": "Too little RAM",
              "score": 3,
              "created_utc": "2026-01-28 17:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29aqsh",
                  "author": "Daniel_H212",
                  "text": "Might not have been in this thread but I saw people talking about using disk space to run this, which is why I used the units of seconds per token.",
                  "score": 8,
                  "created_utc": "2026-01-28 18:11:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28rcxs",
          "author": "misterflyer",
          "text": "IQ0.2\\_XXS wen?",
          "score": 31,
          "created_utc": "2026-01-28 16:47:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28xzcd",
          "author": "Marksta",
          "text": "Thanks for the quants, gave it a spin yesterday. Q2_K_XL seemed perfectly fine as far as coherence goes. Kimi-K2 sticks to its signature style of absolute prompt adherence like a cold robot. 10/10 model, I think its style is what all non-creative focused models should really be striving for. \n\nIts creative side seems slightly better than their last model actually, but in a brute forcing via logic way. In RP scenario its thinking was like \"This character SHOULD say that, that fits that trope...\" and then proceed to deliver the right idea wrong execution of it because it's writing chops are awful ðŸ˜‚",
          "score": 24,
          "created_utc": "2026-01-28 17:16:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28zbti",
              "author": "misterflyer",
              "text": "I experienced the same thing over API with creative writing \n\nlol smh, oh well",
              "score": 4,
              "created_utc": "2026-01-28 17:22:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2945oq",
          "author": "MikeRoz",
          "text": "What is the point of Q5 and up (UD-Q5_K_XL, Q6_K) when the experts are all in int4?",
          "score": 10,
          "created_utc": "2026-01-28 17:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29nh3t",
              "author": "Lissanro",
              "text": "It depends on how quants are made. If preserving the original quality without inflating size is the goal, making Q4_X quant is the only way. Non-INT4 tensors can be kept at Q8. Dynamic quants only make sense for Q3 or lower I think. You can read discussion at https://github.com/ggml-org/llama.cpp/issues/19127 for details, in the forth message someone already upload Q4_X quant if that is what you are looking for. Unsloth quants can be good choice if you need something smaller instead.",
              "score": 7,
              "created_utc": "2026-01-28 19:05:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29yugm",
                  "author": "MikeRoz",
                  "text": "[AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) is the repo. Already downloading it. I'm always a fan of people who provide detailed instructions on how to reproduce their work.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:56:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cfhr7",
                  "author": "danielhanchen",
                  "text": "Oh yes I saw the discussion - I was planning to follow in the footsteps of that GitHub issue but didn't get time yet - will investigate later today!",
                  "score": 2,
                  "created_utc": "2026-01-29 03:15:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29hrqs",
              "author": "defensivedig0",
              "text": "Other parts of the model are not in int4",
              "score": 2,
              "created_utc": "2026-01-28 18:41:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a3qiz",
                  "author": "Lissanro",
                  "text": "I checked their quants and it seems like Q4 and higher quants did not properly preserve the original quality. The proper way is to keep INT4 as modded Q4\\_0 and the rest can be kept as Q8\\_0, there is no need for quants higher than Q4\\_X since they only lose performance without any gains. How to make Q4\\_X quant for K2.5 is documented here: [https://huggingface.co/AesSedai/Kimi-K2.5](https://huggingface.co/AesSedai/Kimi-K2.5) (where you also can download pre-made Q4\\_X quant, it should work both with ik\\_llama.cpp and mainline llama.cpp).",
                  "score": 6,
                  "created_utc": "2026-01-28 20:18:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29yijd",
                  "author": "MikeRoz",
                  "text": "You can use HF's nifty GGUF viewer to see that the experts in their UD-Q5\\_K\\_XL are a mix of Q5\\_K and Q6\\_K, and all Q6\\_K in their Q6\\_K. If the experts were all capped at 4-bit, then difference in size between various quants 4-bit would be trivial, as seen with quants of models like GPT-OSS-120B.",
                  "score": 2,
                  "created_utc": "2026-01-28 19:55:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2b0xa9",
          "author": "nonameisdaft",
          "text": "How would 64gb ram amd 24gb of vram handle this?",
          "score": 10,
          "created_utc": "2026-01-28 22:46:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c1s6v",
              "author": "FrodoTheExplorer",
              "text": "a",
              "score": 9,
              "created_utc": "2026-01-29 02:00:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2d03xf",
                  "author": "jmellin",
                  "text": "Â ^ - The next token will arrive next week",
                  "score": 22,
                  "created_utc": "2026-01-29 05:28:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2iepno",
              "author": "Zeratas",
              "text": "Days per token",
              "score": 2,
              "created_utc": "2026-01-30 00:15:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2s3k9b",
              "author": "ChromatimusX",
              "text": "OOM",
              "score": 1,
              "created_utc": "2026-01-31 12:39:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28u9qv",
          "author": "IngwiePhoenix",
          "text": "Congrats to the small handful of LocalLLaMa people that have >300 <500 GB of VRAM to do this. x)\n\nI'll just keep dreaming...",
          "score": 69,
          "created_utc": "2026-01-28 17:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29aqfi",
              "author": "devvie",
              "text": "That isn't what the post says. Disk + RAM + VRAM >= 240GB, and you can run it. The poster claims 5T/s with 24GB VRAM.",
              "score": 29,
              "created_utc": "2026-01-28 18:11:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o29dway",
                  "author": "juggarjew",
                  "text": "5T/s with 256GB RAM as well, large asterisks there lol",
                  "score": 39,
                  "created_utc": "2026-01-28 18:24:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2aad17",
                  "author": "ReasonablePossum_",
                  "text": "So you say a 6gb vram + 234gb ssd space? 8)",
                  "score": 4,
                  "created_utc": "2026-01-28 20:47:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29idcq",
              "author": "pandodev",
              "text": "Us normies over here will just be watching videos about it....",
              "score": 2,
              "created_utc": "2026-01-28 18:43:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2931om",
          "author": "Historical-Internal3",
          "text": "I have two DGX spark's clustered and I don't think I could run this in any meaningful way lol.",
          "score": 7,
          "created_utc": "2026-01-28 17:38:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6ojm",
              "author": "Zyj",
              "text": "Why not? Should work.",
              "score": 4,
              "created_utc": "2026-01-28 23:14:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29bbtw",
              "author": "Boring_Resolutio",
              "text": "this is really crazy, how much hardware is needed for SOTA",
              "score": 2,
              "created_utc": "2026-01-28 18:13:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ckuc1",
              "author": "buyurgan",
              "text": "same here, but i think offloading to SSD could work, but this document doesn't have any info about how much more memory needed for 100k context length processing, it will require more offloading then it looks.",
              "score": 1,
              "created_utc": "2026-01-29 03:47:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2cl6i1",
                  "author": "Historical-Internal3",
                  "text": "Yea, most likely a ton.",
                  "score": 1,
                  "created_utc": "2026-01-29 03:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o297q4w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-01-28 17:58:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o298nrz",
              "author": "RobTheDude_OG",
              "text": "If it makes you feel better, i was planning to get 256gb ram with my rx 9070 xt with 16gb vram. \n\nBut then the ram crisis hit as i ALMOST had enough money to commit to the purchase and from there i was chasing the constantly moving line until i gave up\n\nI now have a pc budget of around 2k, and i still cannot fucking buy the ram,motherboard and CPU.",
              "score": 6,
              "created_utc": "2026-01-28 18:02:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28ufwq",
          "author": "Sensitive_Housing_62",
          "text": "This is pretty amazing. I like it.",
          "score": 3,
          "created_utc": "2026-01-28 17:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8kgf",
          "author": "_VirtualCosmos_",
          "text": "How much worse is the model with 1.8-bit quant compared to the original?",
          "score": 3,
          "created_utc": "2026-01-28 20:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2daw3f",
              "author": "Zyj",
              "text": "It seemed much worse in a 5 minute test",
              "score": 3,
              "created_utc": "2026-01-29 06:53:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2app5w",
          "author": "DragonfruitIll660",
          "text": "Ty for the GGUFs, always appreciated. Initial impressions for the model seem to be that its a little behind GLM 4.7 of a similar quant for RP. It appears to be more focused on coding though so that makes sense.",
          "score": 3,
          "created_utc": "2026-01-28 21:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cjxjv",
          "author": "Saruphon",
          "text": "Thanks, this is where by 256GB Ram come in handy. Also no i am not a millionaire. Just bought them last July before the price goes crazy.",
          "score": 3,
          "created_utc": "2026-01-29 03:41:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dbwya",
          "author": "squachek",
          "text": "1 bit quant?",
          "score": 3,
          "created_utc": "2026-01-29 07:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dnfaq",
          "author": "tbwdtw",
          "text": "Step 1.\nBe rich",
          "score": 3,
          "created_utc": "2026-01-29 08:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291sqg",
          "author": "GabryIta",
          "text": "5 token/s RAM only?",
          "score": 4,
          "created_utc": "2026-01-28 17:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2b6tqq",
              "author": "DealingWithIt202s",
              "text": "Frontier model on your hardware tho",
              "score": 6,
              "created_utc": "2026-01-28 23:15:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bulne",
              "author": "PeakBrave8235",
              "text": "20 tokens on MacÂ ",
              "score": 2,
              "created_utc": "2026-01-29 01:20:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o28vf8t",
          "author": "Long_comment_san",
          "text": "I wonder if we're gonna get hardware 2 bit precision soon with models like that.",
          "score": 2,
          "created_utc": "2026-01-28 17:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cit2q",
          "author": "xxdesmus",
          "text": "Running Kimi K 2.5 via Ollama Cloud via OpenCode, and tons of hallucinations and strange output â€” \nparticularly while programming.",
          "score": 2,
          "created_utc": "2026-01-29 03:35:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d423o",
          "author": "wh33t",
          "text": "I have 260GB total of (v)ram. I could run this with like 2048 context! w00t! Might have to build a swap space for it lol",
          "score": 2,
          "created_utc": "2026-01-29 05:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2db0j5",
          "author": "NeuralNexus",
          "text": "168 GB RAM in my experimental machine, not enough. Need more.",
          "score": 2,
          "created_utc": "2026-01-29 06:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g7jmn",
              "author": "Steuern_Runter",
              "text": "> 168 GB RAM\n\nWhat combination of RAM sticks is that?",
              "score": 1,
              "created_utc": "2026-01-29 17:51:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k5h2o",
                  "author": "NeuralNexus",
                  "text": "> 168\n\n16*8 + 32 + 8 (rando i know, I figure more GB ram is better even if it's not as fast in channel mode that way)",
                  "score": 1,
                  "created_utc": "2026-01-30 06:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2axmcw",
          "author": "juniperl78",
          "text": "is the q\\_1 quant even usable?",
          "score": 3,
          "created_utc": "2026-01-28 22:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dhlp2",
          "author": "AkiDenim",
          "text": "Go on and use the 1-bit quantization. XD",
          "score": 1,
          "created_utc": "2026-01-29 07:51:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dueib",
          "author": "Distinct-Expression2",
          "text": "\"local\" if you own a small datacenter",
          "score": 1,
          "created_utc": "2026-01-29 09:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e79vj",
          "author": "siegevjorn",
          "text": "I guess the question is how good Q1.8 be I wonder",
          "score": 1,
          "created_utc": "2026-01-29 11:40:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f15sr",
          "author": "Disastrous-Fold7589",
          "text": "I have 4080 super 7tb harddrive and 64 gig game and 7950x3d cpu could it work for me",
          "score": 1,
          "created_utc": "2026-01-29 14:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fx5jm",
          "author": "koibKop4",
          "text": "nice!  \nI'm curious, is having more VRAM speed things up or if anything goes to RAM then speed automatically goes down? I've 128GB VARM and enough RAM so I can easily fit 1.8-bit.",
          "score": 1,
          "created_utc": "2026-01-29 17:04:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2g83c0",
              "author": "Steuern_Runter",
              "text": "The more VRAM you have the faster it can run.",
              "score": 1,
              "created_utc": "2026-01-29 17:53:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ichwz",
          "author": "korino11",
          "text": "Why the hell you need such lobotomize model?!?!   For what?",
          "score": 1,
          "created_utc": "2026-01-30 00:03:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d14yf",
          "author": "StartupTim",
          "text": "I  have both a Strix Halo 128GB as well as a Nvidia DGX Sparc 128GB.  I haven't setup either of them and, if somebody would offer me help setting up both, I'll deploy this model and do some benchmarks!",
          "score": -2,
          "created_utc": "2026-01-29 05:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28m0ni",
          "author": "KitchenSomew",
          "text": "\\*\\*Production Deployment Notes:\\*\\*\n\n\n\nTested Kimi K2.5 for customer support chatbot - here's what works in production:\n\n\n\n\\*\\*Resource Reality:\\*\\*\n\nâœ“ 1.8-bit quant runs on single 3090 (24GB)\n\nâœ— Latency: \\~8-12s first token (too slow for real-time chat)\n\nâœ“ Context: 128K works, but 32K sweet spot for cost/speed\n\n\n\n\\*\\*Where It Excels:\\*\\*\n\n\n\nâ€¢ Code generation for automation scripts\n\nâ€¢ Multi-turn reasoning (customer inquiry analysis)\n\nâ€¢ Vision tasks: document parsing, screenshot analysis\n\n\n\n\\*\\*Production Setup:\\*\\*\n\n\\`\\`\\`python\n\n\\# Our config\n\nmodel: kimi-k2.5-1.8bit-GGUF\n\ncontext\\_length: 32768\n\ntemp: 0.3 (more deterministic for bots)\n\nbatch\\_size: 512\n\n\\`\\`\\`\n\n\n\n\\*\\*Compared to GPT-4o:\\*\\*\n\nâœ“ Privacy: On-prem data stays local\n\nâœ— Speed: 3-4x slower inference\n\nâœ“ Cost: $0 per query after setup\n\n\n\n\\*\\*Reality Check:\\*\\*\n\nFor EU/CIS markets with GDPR requirements, local deployment justifies the latency trade-off. US markets usually prefer hosted APIs.\n\n\n\n\\*\\*Pro Tip:\\*\\* Use router pattern - simple queries â†’ fast local 7B model, complex reasoning â†’ Kimi K2.5. Cuts avg response time by 60%.\n\n\n\nAnyone running this in production for chatbots? What latency are you seeing?",
          "score": -15,
          "created_utc": "2026-01-28 16:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2917ut",
              "author": "Loyal_Rogue",
              "text": "Why all the downvotes?!?",
              "score": 3,
              "created_utc": "2026-01-28 17:30:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o295ozz",
                  "author": "MikeRoz",
                  "text": "Not even the smallest of the quants here will run on a single 3090, unless you also have 256 GB+ of system RAM. Kimi-k2.5-1.8bit-GGUF isn't a quant offered in this repo, and it's nowhere to be found in a full-text search on HuggingFace. This is hallucination with a side of spam.",
                  "score": 8,
                  "created_utc": "2026-01-28 17:49:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cm6n0",
                  "author": "CheatCodesOfLife",
                  "text": "It's a spambot. Look at all the generic incorrect shit it's saying. I don't think even one thing it's said is correct.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrazyy",
      "title": "Cline team got absorbed by OpenAI. Kilo is going full source available in response.",
      "subreddit": "LocalLLaMA",
      "url": "https://blog.kilo.ai/p/cline-just-acqui-hired",
      "author": "demon_bhaiya",
      "created_utc": "2026-01-30 16:56:49",
      "score": 399,
      "num_comments": 46,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/",
      "domain": "blog.kilo.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o2pu91x",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 01:55:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mwmxg",
          "author": "ResidentPositive4122",
          "text": "For open models roo was better than cline anyway. It had more knobs to tweak, more things to edit, so you can adjust your env to the models.",
          "score": 92,
          "created_utc": "2026-01-30 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nbp2e",
              "author": "nore_se_kra",
              "text": "Its still a cline fork so its not like they will not be impacted",
              "score": 11,
              "created_utc": "2026-01-30 18:20:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ob2ct",
                  "author": "zxyzyxz",
                  "text": "It's a fork, so they won't be impacted, how would they when they own their own fork?",
                  "score": 17,
                  "created_utc": "2026-01-30 21:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2sycm4",
                  "author": "flyryan",
                  "text": "Roo de-forked from cline a while ago.",
                  "score": 1,
                  "created_utc": "2026-01-31 15:39:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ozbdl",
              "author": "rvistro",
              "text": "Yeah, I tried cline and it didnt work well. Roo worked and it was pretty nice.",
              "score": 1,
              "created_utc": "2026-01-30 23:01:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nw63e",
          "author": "bamboofighter",
          "text": "My team has been running a multi-model agent setup for our team - Claude, local Qwen on a 3090, Ollama for batch work - all through one orchestration layer. The Cline news is exactly why we went model-agnostic from day one. Vendor lock-in is a real risk when your entire dev workflow depends on one provider. Kudos to Kilo Code for going open",
          "score": 14,
          "created_utc": "2026-01-30 19:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o2m37",
              "author": "captcanuk",
              "text": "Could you share more about your setup? What works and doesnâ€™t?",
              "score": 3,
              "created_utc": "2026-01-30 20:22:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2op0ll",
              "author": "lan-devo",
              "text": "And you did the sensible thing, in this AI craze they are moving so fast that building entire environments and workflows on different systems for then be left hanging or dependent, or at the contrary depending of a single API provider, one owner of a company told me if OpenAI fails or does something fucky as a company their entire bussiness is over. They are treating these API comanies like they are established cloud business like aws, azure... while they a losing billions",
              "score": 2,
              "created_utc": "2026-01-30 22:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oe8wu",
          "author": "itb206",
          "text": "Per the Article:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI",
          "score": 11,
          "created_utc": "2026-01-30 21:17:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rti7q",
              "author": "__Maximum__",
              "text": "To clarify, this is an update, Cline team has answered to the claims.",
              "score": 3,
              "created_utc": "2026-01-31 11:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n7oyh",
          "author": "cleverusernametry",
          "text": "Called it. Those guys were in it for the pay day. I think Saoud was legit at the start then he got money hungry/enticed by money etc and they screwed up royally. He'll still. \n\nPash seemed like a knob and they ignored important, popular PRs, only to later then implement that code without providing attribution to the author. \n\nNick was extremely clearly a marketing guy, just chaisng numbers instead of actually doing devrel.",
          "score": 52,
          "created_utc": "2026-01-30 18:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ob6p6",
              "author": "zxyzyxz",
              "text": "Honestly, good on them for getting that bag while the AI investment boom is hot. Who knows how long OpenAI will stay solvent when they're burning billions a month?",
              "score": 25,
              "created_utc": "2026-01-30 21:03:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph70v",
                  "author": "cleverusernametry",
                  "text": "nothing wrong with getting big sums of money for making useful and popular things. But I feel that there is something wrong with these types of guys getting millions of dollars for shoddy, easily made things like vs code extensions and vs code forks (don't get me started on cursor). And in any normal circumstance, they wouldn't be getting millions but they do because VCs have first class access to the broken money printer. Its just a day to day manifestation of how broken american capitalism is",
                  "score": 3,
                  "created_utc": "2026-01-31 00:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2p7ezo",
              "author": "throwawayacc201711",
              "text": "But you didnâ€™t call it. They updated the article with:\n\n> Update: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 2,
              "created_utc": "2026-01-30 23:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgkb5",
                  "author": "cleverusernametry",
                  "text": "What I called are these guys are hustlers.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:36:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oe8pf",
          "author": "cantgetthistowork",
          "text": "None of these forks have fixed the stupid 5 min timeout. WHAT IS THE PURPOSE OF IT",
          "score": 6,
          "created_utc": "2026-01-30 21:17:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlspu",
              "author": "kei-ayanami",
              "text": "I hope more people hear about this. The huge models like K2, Deepseek, etc need 5+ mins of time to think unless you have a crazy setup or use API.",
              "score": 3,
              "created_utc": "2026-01-31 04:48:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pt6c8",
          "author": "Saltwater_Fish",
          "text": "cline is not particularly easy to use, roo code is better",
          "score": 4,
          "created_utc": "2026-01-31 01:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o6a2t",
          "author": "NoFaithlessness951",
          "text": "Cline wasn't good anyway, most people moved on to somewhere else",
          "score": 9,
          "created_utc": "2026-01-30 20:40:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pfu9q",
              "author": "bludgeonerV",
              "text": "Cline and it's forks are still the best option if you want that collaborative peer programming feel rather than just being a RP reviewer for AI.",
              "score": 4,
              "created_utc": "2026-01-31 00:32:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o21h1",
          "author": "grabber4321",
          "text": "Uninstalled.",
          "score": 6,
          "created_utc": "2026-01-30 20:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2o3j8r",
              "author": "trararawe",
              "text": "That's exactly what openai wants you to do.",
              "score": 3,
              "created_utc": "2026-01-30 20:26:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qcw2w",
          "author": "Kamimashita",
          "text": "Can't wait for OpenClaw (Clawdbot) to do the same in a few months.",
          "score": 3,
          "created_utc": "2026-01-31 03:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nh0y1",
          "author": "kei-ayanami",
          "text": "Nooooooo :/ I hope the other projects can stay strongÂ ",
          "score": 5,
          "created_utc": "2026-01-30 18:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2nyael",
          "author": "arm2armreddit",
          "text": "Cline is dead? ðŸ˜­ ðŸ˜ª ðŸ˜”",
          "score": 2,
          "created_utc": "2026-01-30 20:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p17q6",
              "author": "Mkengine",
              "text": "No, at the start of the article, it says:\n\nUpdate: Cline clarified they are operational and there was no transaction with OpenAI.",
              "score": 7,
              "created_utc": "2026-01-30 23:11:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o0si3",
          "author": "Impossible-Glass-487",
          "text": "God fucking damnit",
          "score": 2,
          "created_utc": "2026-01-30 20:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o8ke3",
          "author": "FullOf_Bad_Ideas",
          "text": "I think we'll have enough of open source coding harnesses for a forseable future. There are dozens of them now. I am basically a daily user of cline but switching won't be hard. They didn't find a way to monetize so they jumped on the wagon of a different company that doesn't have the same issue with sustainability (they have similar issue but on a different scale where staffing costs are small anyway). Industry will consolidate - losers will get absorbed into successful corps and losses will be amortized without a bubble pop this way.",
          "score": 2,
          "created_utc": "2026-01-30 20:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2od4h8",
          "author": "false79",
          "text": "So bizarre Nik Pash got fired from Cline, hired by Open AI, only to have Open AI absorb Cline.\n\nFiring  \n[https://x.com/i/trending/1999388966668116413](https://x.com/i/trending/1999388966668116413)\n\nHiring  \n[https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html](https://www.hindustantimes.com/trending/us/american-techie-nik-pash-fired-from-cline-over-imagine-the-smell-remark-joins-sam-altman-s-openai-101769742946502.html)\n\nMaybe Cline was burning more cash then it was bringing in and with the drama came a drop in subs as would any contraversy. Now Cline is part of a company that is clearly burning more cash than what it's making.",
          "score": 1,
          "created_utc": "2026-01-30 21:12:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ogwi0",
              "author": "crantob",
              "text": "Yea but why do some people get to *print* it is what I don't understand yet.\n\nWhen did that start?",
              "score": 2,
              "created_utc": "2026-01-30 21:30:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2q8hmv",
          "author": "ismaelgokufox",
          "text": "I just hope the PRs on Kilo for a single OpenAI compatible provider to show all models, is merged soon. Having a single provider profile per model is cumbersome. \n\nGlad to see them go ever more open.",
          "score": 1,
          "created_utc": "2026-01-31 03:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rbqyt",
          "author": "Charming_Support726",
          "text": "Left Cline behind month ago, because of their strange politics. (Never got on with Roo and Kilo as well). Switched to Opencode and I use Antigravity and Codex from time to time with my quota.\n\nCline and such had an attitude issue from the beginning",
          "score": 1,
          "created_utc": "2026-01-31 08:27:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u5d7h",
          "author": "turtleisinnocent",
          "text": "Same as Alex (XCode)\nFeels like regular de weeding",
          "score": 1,
          "created_utc": "2026-01-31 19:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2u5pel",
          "author": "Emotional-Baker-490",
          "text": "The word to use is FOSS/Open source NOT Source available. Source available is a very specific term for a piece of software that is not free (as in freedom and as in beer), but has publicly available code.",
          "score": 1,
          "created_utc": "2026-01-31 19:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2uwzgx",
          "author": "Apprehensive-Yam5278",
          "text": "sigh...uninstall..",
          "score": 1,
          "created_utc": "2026-01-31 21:19:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qni356",
      "title": "216GB VRAM on the bench. Time to see which combination is best for Local LLM",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/5ilrgdymhpfg1.jpeg",
      "author": "eso_logic",
      "created_utc": "2026-01-26 14:51:22",
      "score": 393,
      "num_comments": 104,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o1tt97y",
          "author": "HugoCortell",
          "text": "If this forum is to be believed, it'll be unusable (lower token output than standard reading speed).\n\nBut I tend to use a gain of salt when reading some of the takes posted here by people who haven't actually tried, so I look forward to seeing what your actual testing discovers, OP.\n\nI'd also be interested in knowing how you're rigging that many GPUs to a single PC without a massive loss in bandwidth, most \"cheap\" server motherboards I found can only do a handful of GPUs.",
          "score": 61,
          "created_utc": "2026-01-26 14:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tu82o",
              "author": "eso_logic",
              "text": "My thoughts exactly.\n\nThe motherboard here is actually a dual socket X99 board, supermicro X10DRG-Q. These are very cheap on the secondhand market (\\~200 USD) and give you a lot of PCIe lanes to avoid throttling. I have had situations where motherboard/CPU combo does throttle performance, detailed on [da blog](https://esologic.com/gpu-server-benchmark/#hardware-updates).",
              "score": 32,
              "created_utc": "2026-01-26 15:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u31ix",
                  "author": "OutlandishnessIll466",
                  "text": "I had 4x p40 but replaced 3 with 3090's. They work perfectly fine and are definitely useable. The problem comes with increased context and bigger models. The bigger the model the slower and at 400+ GB even 3090s become unusable.",
                  "score": 14,
                  "created_utc": "2026-01-26 15:42:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1uvzhl",
                  "author": "DrKenMoy",
                  "text": "Dude no joke this is so cool. Are you just planning on running llama or are there others youâ€™re considering?",
                  "score": 3,
                  "created_utc": "2026-01-26 17:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w8a0i",
              "author": "segmond",
              "text": "False, I posted a little more than $1,000 build with MI50 a while ago, 10 16gb MI50s and they produce very usable speed much better than Strix Halo.  It's pinned on my profile posts.   I was running it on x1 PCI bus with the slowest celeron CPU and ddr3 ram.  P40/P100s are better faster and on a good platform will produce awesome speed.",
              "score": 5,
              "created_utc": "2026-01-26 21:15:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1wae8x",
                  "author": "HugoCortell",
                  "text": "Damn dude, you almost make me want to try it myself. Then again P40/60/100s are super expensive in Europe, for the US price the most we can get here are the K ones, which I have been resolutely told by everyone that they are garbage.\n\nA P40 is \\~$500 here. MI50s are awesome (cheaper and more memory than a P40), but the lack of support for Windows kind of kills them for me.",
                  "score": 2,
                  "created_utc": "2026-01-26 21:24:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wxeys",
              "author": "pharrowking",
              "text": "https://preview.redd.it/a2qvipvlzrfg1.png?width=1576&format=png&auto=webp&s=44aa68be194ba19f0436af4e375b251c37aeb418\n\nCheck out this llama-benchmark for minimax m2.1 using 8x tesla p40s. MOE models do very well on pascal hardware. in this case the model is fully loaded in gpu vram. any use of cpu ram and that speed becomes nothing. but pure gpu ram, is workable. 25 t/s is way more than read speed.",
              "score": 2,
              "created_utc": "2026-01-26 23:11:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u1gaw",
          "author": "BananaPeaches3",
          "text": "The main issue with older cards is that prompt processing will get you even if token gen speed is tolerable (and it is)\n\nIf youâ€™re using it like chat gpt then itâ€™s fine but once you start using things like cline the system prompt is allegedly 15k tokens.\n\nSo imagine youâ€™re waiting several minutes before there is even any output. At this point a DGX Spark is a better investment, it will output slightly slower than P100 but at least prompt processing will be fast.",
          "score": 14,
          "created_utc": "2026-01-26 15:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tumh6",
          "author": "iampoorandsad",
          "text": "Cooling those teslas will turn your house/lab into a plane with jet engines. Get some earplugs or a really good ANC headphones.",
          "score": 19,
          "created_utc": "2026-01-26 15:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tvdbh",
              "author": "eso_logic",
              "text": "Ha! I actually developed my own high static pressure air cooler for these cards that scales fan power with load so it's not that bad. You can see the coolers hanging off the back here:\n\nhttps://preview.redd.it/cn86bpjplpfg1.jpeg?width=6960&format=pjpg&auto=webp&s=7e4f2a22382f79834a313495286a5f1102c5676d\n\nI've written about the project as well: [esologic.com/cooler](http://esologic.com/cooler)",
              "score": 49,
              "created_utc": "2026-01-26 15:07:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzb5e",
                  "author": "gittb",
                  "text": "Super clean",
                  "score": 15,
                  "created_utc": "2026-01-26 15:25:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u7lpz",
                  "author": "pokemonplayer2001",
                  "text": "I was going to point out the cooling issues with these cards, but you not only know about it, you have a fix and docs.\n\nðŸ‘ðŸ‘ðŸ‘\n\nEdit: ok bro, no need to show off!! :)   \n[https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg](https://esologic.com/wp-content/uploads/2025/08/20250812-163922-Unknown-2025-2048x1365.jpg)\n\nðŸ¤ŒðŸ‘Œ",
                  "score": 10,
                  "created_utc": "2026-01-26 16:02:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1u1fsq",
                  "author": "iampoorandsad",
                  "text": "Well done sir, but how is that suppressing the noise at 100% load ? It looks really clean though. But even at idle temps (30C ish?) the blowers would be spinning, no? I'm not sure what would bother me most, constant noise or intermittent... anyway, earplugs it is!",
                  "score": 7,
                  "created_utc": "2026-01-26 15:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ur8u1",
              "author": "BusRevolutionary9893",
              "text": "Assuming 250-300 watts for each of the 12 GPUs that's only 3.0-3.6 kW. A one ton(3,410 watts) wall mounted split system or a small window unit is all he needs. A wall mounted split system is practically silent.Â ",
              "score": 1,
              "created_utc": "2026-01-26 17:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tvupr",
          "author": "FullOf_Bad_Ideas",
          "text": ">In gpu_box_benchmark, single GPU tests are parallelized using Docker containers. The same test is invoked inside of a docker container, one per GPU. The containers are started at the same time so each GPU is loaded at the same time.\n\nI don't think this benchmark contains any test for serving big models split across GPUs, which is the main usecase for having multiple GPUs with a lot of VRAM.\n\nI am looking forward to your results anyway.\n\nI think this project is progressing slowly since I first heard of it almost a year ago, have you faced issues with cooler design that prevented it from being finished earlier, or is it just caused by a lack of time?",
          "score": 7,
          "created_utc": "2026-01-26 15:10:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tylqq",
              "author": "BuildAQuad",
              "text": "I would assume you could get decent speeds if you run MOE models?",
              "score": 3,
              "created_utc": "2026-01-26 15:22:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1tzn8q",
                  "author": "FullOf_Bad_Ideas",
                  "text": "yeah, it could run big MoEs well. And I think that should be the benchmark.",
                  "score": 3,
                  "created_utc": "2026-01-26 15:27:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1tz34y",
              "author": "eso_logic",
              "text": "Yeah it's not clear, my bad. Multi gpu native tests like \\`llama-bench\\` and the ResNet50 tests run both the parallelized container approach *and* a single container with all GPUs. It's up to user which results are selected for comparison, in the graph on the blog it's the all GPUs result.\n\nYeap it has been a while. Day jobbing sadly. To get max cooler performance I've designed a fan manifold for each of the supported cards but must of that work had been done a long time ago.  \n\nI also had to build an [air intake system](https://github.com/open-rack-vent) for my server rack to feed my GPU node with air inside the rack. \n\nhttps://preview.redd.it/9n5dnshropfg1.jpeg?width=4238&format=pjpg&auto=webp&s=2ad2b2def2e8684a721dfe1dec5f1b253ddbedea\n\nProjects within projects lol.",
              "score": 3,
              "created_utc": "2026-01-26 15:24:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1tzcru",
                  "author": "eso_logic",
                  "text": "https://preview.redd.it/kh3oo1tvopfg1.jpeg?width=6960&format=pjpg&auto=webp&s=99cd54b9ad3f8b81a4d2041cbb87647ac615cc81\n\nAnother rack air intake shot, cool air is moved to the front from the sides.",
                  "score": 2,
                  "created_utc": "2026-01-26 15:26:04",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tvcyu",
          "author": "dc740",
          "text": "from those I have a P40 and an M10 (EDIT: I thought I had the M40 originally but I simply didn't remember it). The P40 no longer has support, and the M10 is just 4 maxwell 8gb gpus in a single card. The P40 runs in circles around the M10, but after lots of testing I still prefer to run 3 amd instinct Mi50 32gb (even though support was dropped from rocm a few months ago)",
          "score": 12,
          "created_utc": "2026-01-26 15:07:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u0b9p",
              "author": "Far-Low-4705",
              "text": "i was able to get two of the amd mi50's when they were cheap.\n\nDef not as good as 3090 or anything, but for $200 for 64Gb VRAM, really cant beat it",
              "score": 6,
              "created_utc": "2026-01-26 15:30:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ubu29",
                  "author": "WiseassWolfOfYoitsu",
                  "text": "Another option is v620. Newer than the mi50s and better supported, but more like 350 per instead of 100. Still not bad for the higher speed and 32gb vram per, though.",
                  "score": 4,
                  "created_utc": "2026-01-26 16:20:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1u3lne",
              "author": "TheSpicyBoi123",
              "text": "No, the m40 is a single gpu, you are confusing it with the m60 which is 4GPUs on one board. The m40 comes in two versions the 12GB one and the 24GB one. The p40 is by all means a much stronger GPU though, however the best Pascal card is by far the p100.",
              "score": 4,
              "created_utc": "2026-01-26 15:44:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u81x1",
                  "author": "dc740",
                  "text": "Oh, you are right! I was confused with the M10. I had to go look up what I had because I no longer remembered it.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1uxw65",
              "author": "wh33t",
              "text": "I'm still using my P40's for LLM just fine. On driver 580. The datacenter driver should continue to support P40's for much longer still.",
              "score": 1,
              "created_utc": "2026-01-26 17:55:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ub2me",
          "author": "blazze",
          "text": "**Mark Watney**Â says:Â *\"In the face of overwhelming odds, I'm left with only one option: I'm gonna have to science the shit out of this.\"*Â  \\- *The Martian*Â (2015)\n\nYou've earned an official Martian creativity and ingenuity award.  I will study your design for a  96 GB, 6 RTX 5060TI GPU rig i want to build for LLM training.",
          "score": 6,
          "created_utc": "2026-01-26 16:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1udeue",
              "author": "eso_logic",
              "text": "Ha thanks! Let me know if you run into problems and I can try to lend a hand.",
              "score": 1,
              "created_utc": "2026-01-26 16:27:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1u3tkm",
          "author": "TheSpicyBoi123",
          "text": "Have you tried bios modding the Kepler and Maxwell gpus? You can squeeze a surprising amount of headroom this way.",
          "score": 3,
          "created_utc": "2026-01-26 15:45:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u4mib",
              "author": "eso_logic",
              "text": "Yes I've heard of this but haven't taken the plunge yet. It would be good to A/B the benchmarking suite on a modded vs stock set of GPUs as well. I'll add this to my notes, thank you.",
              "score": 3,
              "created_utc": "2026-01-26 15:49:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7q1f",
                  "author": "TheSpicyBoi123",
                  "text": "Its worth a shot! I have a nice guide on my github on how to do it, however I only have made the bioses for K40's (single gpu version of the k80). To make the new bios for these dual gpus is a bit more involved but follow the instructions and it should work just fine. M40 should be much easier.",
                  "score": 2,
                  "created_utc": "2026-01-26 16:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yies1",
              "author": "Bobby72006",
              "text": "Even on stock wattages, I was able to squeeze a lot of juice out of both the core and the VRAMs on my own M40. There's good potential here!",
              "score": 1,
              "created_utc": "2026-01-27 04:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20lzu6",
                  "author": "TheSpicyBoi123",
                  "text": "The much more interresting question is not about wattage per se but thermal headroom. Would you be willing to try some bioses if you have an m40 spare?",
                  "score": 1,
                  "created_utc": "2026-01-27 14:00:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1tzas1",
          "author": "twavisdegwet",
          "text": "what interface? my t4 setup is wayyy faster after doing ik_llama with the nccl setup",
          "score": 3,
          "created_utc": "2026-01-26 15:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tzq6f",
              "author": "eso_logic",
              "text": "I'm using stock llama-bench. Can you link me your config? I can add a new test.",
              "score": 3,
              "created_utc": "2026-01-26 15:27:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u13gq",
                  "author": "twavisdegwet",
                  "text": "https://github.com/ikawrakow/ik_llama.cpp/\n\nHave to get cuda and NCCL enabled\nmust compile with -DGGML_CUDA=ON\n\nConfig should be unchanged outside of needing to add \"-sm graph\" to get the speed boost of ik_llama \n\nProbably better explined here but that's the gist of it \nhttps://medium.com/@jagusztinl/llama-cpp-performance-breakthrough-for-multi-gpu-setups-04c83a66feb2",
                  "score": 3,
                  "created_utc": "2026-01-26 15:33:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1u7yty",
          "author": "Freonr2",
          "text": "Short of someone else trying that exact setup and posting benchmarks you aren't going to know until you try. I can't recall anyone posting benchmarks for 4/8 Tesla GPU setups.",
          "score": 3,
          "created_utc": "2026-01-26 16:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ua6bf",
          "author": "a_beautiful_rhind",
          "text": "P40/P100 will be ok just full of software hassles. K/M GPU are going to be more trouble than they're worth.\n\nI'm actually curious how ik_llama.cpp would do on NCCL-TP p2p, at least on models where pascal is still working. That's where I'd start or that pascal vllm fork.",
          "score": 3,
          "created_utc": "2026-01-26 16:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud8tq",
              "author": "eso_logic",
              "text": "Docker helps with locking down the environment for comparison. Yep it looks Like I'l have to add some \\`ik\\_llama\\` tests.",
              "score": 3,
              "created_utc": "2026-01-26 16:26:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ufqsp",
          "author": "GoodSamaritan333",
          "text": "I can hear the noise, just by looking at this image.",
          "score": 3,
          "created_utc": "2026-01-26 16:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1urd0c",
          "author": "Ok-Internal9317",
          "text": "Havenâ€™t seen so much ewaste on one table for so long. I also have four m40s and one m60, with no support with vllm and pytorch dropping support day by day, I am becoming more pessimistic of their future\n\nBut the P100 is nice tho, planning to buy that soon",
          "score": 3,
          "created_utc": "2026-01-26 17:26:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1url8r",
              "author": "eso_logic",
              "text": "One man's trash...",
              "score": 6,
              "created_utc": "2026-01-26 17:27:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xdkvl",
          "author": "Current_Ferret_4981",
          "text": "What is your plan for avoiding the communication and memory slowdown? While 200+GB of VRAM is interesting, you are going to be compute, memory bandwidth, and comm bound long before you can load that vram up efficiently. \n\nI would take a look at the math in this article and see how it looks https://jax-ml.github.io/scaling-book/applied-training/. I suspect it's going to turn out worse than something like 1x6000 pro and certainly cost more before even considering the power and cooling costs",
          "score": 3,
          "created_utc": "2026-01-27 00:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ubl75",
          "author": "PhotographerUSA",
          "text": "You don't need a lot of vram anymore. Also, you can just have AI gather all the info off the web. You don't need the big library.",
          "score": 2,
          "created_utc": "2026-01-26 16:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xj6pn",
          "author": "bigh-aus",
          "text": "I'm seriously starting to think about options for a high vram rig @ home.  \nGoal would be to run minimax m2.1 (and i guess 2.2).  Some rough calculations in cost are pretty scary :\\\\ unless I go the mac route.\n\nDepending how fast you're able to get things, maybe it's a case of buy the comptuer then run older GPUs...",
          "score": 2,
          "created_utc": "2026-01-27 01:03:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y202a",
              "author": "eso_logic",
              "text": "Yep thatâ€™s the goal of the project, to be able to answer such questions.",
              "score": 2,
              "created_utc": "2026-01-27 02:46:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1xto16",
          "author": "MLExpert000",
          "text": "This is the kind of setup where VRAM management starts to matter more than raw capacity., once youâ€™re juggling multiple large models on local machines , the cost isnâ€™t just fitting them, itâ€™s reload and reinit churn when switching. Please do some benchmarks around  swap or reactivation latency, not just steadystate throughput",
          "score": 2,
          "created_utc": "2026-01-27 02:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xv9zt",
              "author": "eso_logic",
              "text": "Do you know of anything that exists to benchmark this? Or how would you do it",
              "score": 2,
              "created_utc": "2026-01-27 02:09:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xwnur",
                  "author": "MLExpert000",
                  "text": "There isnâ€™t really a standard benchmark for this today unfortunately. Most LLM benchmarks focus on steady-state throughput or tokens per second once the model is already hot but not on swap, reload, or reactivation latency. In practice, people tend to measure this with a simple harness that repeatedly evicts a model from VRAM and then triggers a cold or semi-cold inference and records time to first token along with GPU memory residency over time. If you want meaningful comparisons, you have to control for disk, PCIe, and host memory effects, since reactivation latency often ends up being more user-visible than raw throughput when youâ€™re switching between models.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:17:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xwu1h",
                  "author": "MLExpert000",
                  "text": "A local multi-GPU setup like this is actually well suited for that kind of testing, since you can control disk, PCIe, and host memory effects.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1z5c9g",
          "author": "TooManyPascals",
          "text": "Cool! Super interested on the benchmarks! \n(I have a similar setup)",
          "score": 2,
          "created_utc": "2026-01-27 07:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oluop",
          "author": "XxBrando6xX",
          "text": "Hey how is testing going ? This just popped up for me and wanted to check in, stoked you're doing this everyone wants to focus on the sexy new hotness or the top end cards, but for there to be any longevity and universality to this stuff locally, it needs more options. \n\nThanks for your research dude",
          "score": 2,
          "created_utc": "2026-01-30 21:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usw89",
          "author": "spacenavy90",
          "text": "Sure you have a lot of VRAM but extremely slow inference. Sorry you wasted your money OP.",
          "score": 2,
          "created_utc": "2026-01-26 17:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ur3nx",
          "author": "funnyrobot10",
          "text": "Can I please get the parts for your setup? I really struggle putting together the part list for my build. Currently I have 2 Tesla V100 and some 256GB ram",
          "score": 1,
          "created_utc": "2026-01-26 17:25:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1urop6",
              "author": "eso_logic",
              "text": "Check out the blog",
              "score": 1,
              "created_utc": "2026-01-26 17:28:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wwhl3",
          "author": "Informal_Trade_3553",
          "text": "if theyre nvidia chips youre lucky, and if the drivers for cuda work.  \nIf not, its just a nice toy burning electricity you have :P",
          "score": 1,
          "created_utc": "2026-01-26 23:07:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1y42c7",
          "author": "KadahCoba",
          "text": "P40 are still usable, but not with everything, like vllm unless you compile it with older compute support but it will be slow. llama.cpp+gguf works well on them, I was running 3xP40's till about a month ago. I'm planning to sell the P40's soon.\n\nM40 and Maxwell in general were pretty much unusable in late 2024. A lot of things dropped support for that compute level a while ago. I have one I'll likely post for free at some point.\n\nK80 and Kepler, very useless. I bought on back in 2022 and it was unusably slow then on the few things that would work with Kepler. I have one I will give away to anybody that wants to come get it in socal.",
          "score": 1,
          "created_utc": "2026-01-27 02:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1y4fg1",
              "author": "KadahCoba",
              "text": "Your cooling setup is fancy.\n\nI would have just put a 120mm server fan on each block of 3.5 cards. Overkill would be one on each end.",
              "score": 1,
              "created_utc": "2026-01-27 02:59:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1va4yy",
          "author": "FullstackSensei",
          "text": "If you upgrade to an X11 motherboard, the IPMI will detect the GPUs and regulate fan speed based on their temps.\n\nYou can also use a single 80mm fan, like the Arctic S8038 series, to cool each pair of GPUs.\n\nSupermicro motherboard fan headers are rated at 2A each, so you could hook up to four S8038-7k fans to each header.\n\nSuch a solution is not only simpler and cheaper, but also much quieter than those blower fans.",
          "score": 0,
          "created_utc": "2026-01-26 18:46:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrkb1b",
      "title": "How was GPT-OSS so good?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "author": "xt8sketchy",
      "created_utc": "2026-01-30 22:31:44",
      "score": 366,
      "num_comments": 167,
      "upvote_ratio": 0.9,
      "text": "I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.\n\nThe model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.\n\nBut it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)\n\nI'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:  \n\\- Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?  \n\\- I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?  \n\\- What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o2r8tx4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-31 08:00:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2outu2",
          "author": "SlowFail2433",
          "text": "Clean data goes a very long way\n\n\nWhat I have noticed from working on big enterprise projects is that they tend to have enormous data pipelines spanning dozens of packages where data is manipulated and evolves repeatedly in a structured way\n\n\nWhereas open source projects often put web-scrape slop directly into the model",
          "score": 321,
          "created_utc": "2026-01-30 22:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2owqo3",
              "author": "SubstanceNo2290",
              "text": "Also this requires top tier resources to pull off.. aka money\n\nOpenAI being a dedicated behemoth based in the US can outright make deals with X/reddit etc for structured training data with plenty of useful metadata. Chinese companies can do the same with China based social media but it probably ainâ€™t nearly as information rich as American/international media.\n\nAnd developing/honing these pipelines is a massive project in and of itself which combined with not-having-billions puts startups at a disadvantage",
              "score": 97,
              "created_utc": "2026-01-30 22:48:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pvy2w",
                  "author": "Saltwater_Fish",
                  "text": "I feel more and more the importance of data",
                  "score": 16,
                  "created_utc": "2026-01-31 02:05:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qlrpb",
              "author": "horsethebandthemovie",
              "text": "yeah the more you try shit the more you realize how slop adds up in every phase. sloppy data? bad signal for the model to learn. sloppy evals? model doesn't know which way is correct. \n\nturns out it's just really fucking hard\n\nand the number of knobs to tweak is legitimately staggering. the more you learn, the more you realize that the only way to train something at that scale is to have people who understand everything from the GPU kernels up to the scraping and processing\n\nif you have those skills and you're doing open source work your time is extremely valuable, why not get rich working for openai et al instead?",
              "score": 20,
              "created_utc": "2026-01-31 04:48:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2parxk",
              "author": "Pvt_Twinkietoes",
              "text": "Big enterprise data also tend to be very narrow in scope. They tend to do very few things, but has been low tolerance for errors.",
              "score": 4,
              "created_utc": "2026-01-31 00:04:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rfj7l",
              "author": "howardhus",
              "text": "this is a made up comment.. there are no open source models. only open weights.\nand even then: the best data processors are open source (airflow, airbyte kfk etc)",
              "score": 2,
              "created_utc": "2026-01-31 09:03:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rlikm",
                  "author": "TaroOk7112",
                  "text": "Not true, there's a few, but probably none SOTA:\n\nexample: [https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222](https://allenai.org/blog/hello-olmo-a-truly-open-llm-43f7e7359222)",
                  "score": 1,
                  "created_utc": "2026-01-31 10:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rktnb",
              "author": "IrisColt",
              "text": "Thatâ€™s why ChatGPT, Gemini, and Claude command English like gods. Chinese open-weight models can produce some of the best ESL output out there, but they still donâ€™t quite have the cultural feel of a native speaker.",
              "score": 2,
              "created_utc": "2026-01-31 09:54:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2wbxbr",
                  "author": "Ok-Attention2882",
                  "text": "ze bluetooth dewise is connected-a success a folly",
                  "score": 0,
                  "created_utc": "2026-02-01 01:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p3izx",
          "author": "inteblio",
          "text": "Nice to finally hear something positive about it.\n\n20b is also incredible. It can run on 16gb RAM (not gpu), and is \"perfectly good\". Finally \"run chatGPT at home\". \n\nOn GPU is good enough to voice-talk with (parakeet/korroko). 120b is better, but only if you need extra.",
          "score": 112,
          "created_utc": "2026-01-30 23:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pkx6m",
              "author": "Chris266",
              "text": "I've got a MacBook pro 24gb of ram and 20b runs better than anything I've tried in the 18-30b range. Once it gets going it feels quick and does a good enough job for home use.",
              "score": 28,
              "created_utc": "2026-01-31 01:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2rajpp",
              "author": "Morazma",
              "text": "I had no idea it could runÂ in such a small amount of RAM. That's incredible.",
              "score": 6,
              "created_utc": "2026-01-31 08:16:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qbuza",
              "author": "rorowhat",
              "text": "What gui gives you the options for voice?",
              "score": 2,
              "created_utc": "2026-01-31 03:41:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2vy3o5",
                  "author": "inteblio",
                  "text": "I vibecoded the interface",
                  "score": 1,
                  "created_utc": "2026-02-01 00:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r6ccl",
              "author": "ChessGibson",
              "text": "Are you running this on a Mac? I have tried it with mine that has 16GB of unified memory but I didnâ€™t have enough space to run it at even I think Q4.",
              "score": 1,
              "created_utc": "2026-01-31 07:37:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rqzlb",
                  "author": "Baldur-Norddahl",
                  "text": "It should run on a 16GB Mac but you need to run the command to increase allowed VRAM.\n\nsudo sysctl iogpu.wired_limit_mb=14336\n\nAlso run the original model from OpenAI.",
                  "score": 3,
                  "created_utc": "2026-01-31 10:52:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ttfwq",
              "author": "_raydeStar",
              "text": "Dude, I've been looking for the perfect tooling LLM for an 8GBVRAM machine (work laptop) - Qwen 30B doesn't quite get it right, neither does nemotron or GLM 4.7 flash (too slow), and the 8GB models are too dumb and keep getting the tool calls wrong.  20B is my consistent driver and it just works exactly as I want it to.",
              "score": 1,
              "created_utc": "2026-01-31 18:08:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phasp",
              "author": "mckirkus",
              "text": "Why \"not GPU\"?",
              "score": -1,
              "created_utc": "2026-01-31 00:40:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pi610",
                  "author": "2str8_njag",
                  "text": "I guess it's small enough for CPU",
                  "score": 9,
                  "created_utc": "2026-01-31 00:44:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ovkq2",
          "author": "Baldur-Norddahl",
          "text": "It wasn't actually trained at 4 bit. We don't exactly know, but likely they trained it at 16 bit as usual. Then it went through a process called quantization aware training. During this they keep the weights at 16 bits, but do the forward pass at 4 bits. So they are kind of running the quantization over and over, so any brain damage gets trained out of it.\n\nThey are not the only ones doing it. Kimi K2.5 was just released using the same concept. It is just that even with most of the weights at 4 bits, that one is far too large for most of us.",
          "score": 78,
          "created_utc": "2026-01-30 22:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qctkj",
              "author": "nikprod",
              "text": "Googles Gemma has QAT too",
              "score": 10,
              "created_utc": "2026-01-31 03:47:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qokmy",
                  "author": "planetafro",
                  "text": "I dont think Gemma3 does tools tho. :(",
                  "score": 4,
                  "created_utc": "2026-01-31 05:08:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qpjv2",
              "author": "mycall",
              "text": "Multisampling, do you know how many iterations of quantizations?",
              "score": 1,
              "created_utc": "2026-01-31 05:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rrqh6",
                  "author": "Baldur-Norddahl",
                  "text": "I don't think Open AI has released that information. Almost everything about how they train their models is secret, hence why some might call them Closed AI.",
                  "score": 1,
                  "created_utc": "2026-01-31 10:59:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p404u",
          "author": "Kamal965",
          "text": "One of the main reasons why GPT-OSS is faster is because its architecture is wider but shallower than most.",
          "score": 11,
          "created_utc": "2026-01-30 23:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2owq7l",
          "author": "ttkciar",
          "text": "Regarding GLM-4.5-Air:  To be fair, its competence is not entirely limited to agentic code development.  I have found it to be excellent for STEM tasks in general, including physics, medicine, and math.\n\nIt's not great for creative tasks, though.  I use other models for creative writing (mostly Big-Tiger-Gemma-27B-v3 and Cthulhu-24B-1.2).\n\nOn a side-note, I recently found (to my surprise) that Olmo-3.1-32B-Instruct is much, much better at inferring syllogisms than GLM-4.5-Air or any other model I have tried.  That's a bit of a niche application, but an important one for some synthetic data generation tasks.",
          "score": 34,
          "created_utc": "2026-01-30 22:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q28dr",
              "author": "WeMetOnTheMountain",
              "text": "I've found it's amazing at creative tasks, but I also run the derestricted version which I feel uncaps it's writing style.",
              "score": 5,
              "created_utc": "2026-01-31 02:42:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tmerg",
                  "author": "zoyer2",
                  "text": "same, i find GPT OSS limit itself when trying to create complex games while GLM tries to actually proceed with difficult tasks without simplifying them",
                  "score": 1,
                  "created_utc": "2026-01-31 17:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ou0h5",
          "author": "Haunting_Lobster1557",
          "text": "GPT-OSS was lightning in a bottle tbh, the 4-bit native training was genius but super hard to replicate without their exact setup and data pipeline\n\n  \nMost newer models are chasing benchmarks instead of that smooth \"just works\" feel that made GPT-OSS special - turns out good vibes are harder to quantify than MMLU scores",
          "score": 129,
          "created_utc": "2026-01-30 22:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p37p9",
              "author": "Yes_but_I_think",
              "text": "QAT is a fully understand technology by now. Kimi gave a INT4 QAT in Kimi K2.5",
              "score": 39,
              "created_utc": "2026-01-30 23:22:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pwf9m",
                  "author": "Saltwater_Fish",
                  "text": "[https://lmsys.org/blog/2026-01-26-int4-qat/](https://lmsys.org/blog/2026-01-26-int4-qat/)\n\nHere is a good blog about INT4 QAT",
                  "score": 15,
                  "created_utc": "2026-01-31 02:07:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2rb71e",
                  "author": "TelloLeEngineer",
                  "text": "gpt oss was not QAT, it was natively trained at mxfp4",
                  "score": 1,
                  "created_utc": "2026-01-31 08:22:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2owxdt",
              "author": "hieuphamduy",
              "text": "This! And also I don't even think other models are even using data with a much later cutoff date than gpt-oss. From what I heard, companies are having difficulty collating clean data from 2024 onwards (prob cause of all the generative AI slops), so most of them are just recycling relatively the same dataset tbh",
              "score": 28,
              "created_utc": "2026-01-30 22:49:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2phx05",
              "author": "Ilforte",
              "text": "Chatgpt ahh post",
              "score": 5,
              "created_utc": "2026-01-31 00:43:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rl4da",
                  "author": "IrisColt",
                  "text": "heh",
                  "score": 1,
                  "created_utc": "2026-01-31 09:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pd7f4",
              "author": "rm-rf-rm",
              "text": "> good vibes are harder to quantify than MMLU scores\n\nno, its whether you follow proper testing vs scoring high on popular benchmarks. Its almost exactly the equivalent of a kid trying to get high scores on SAT, GRE etc. vs being actually good.",
              "score": -5,
              "created_utc": "2026-01-31 00:17:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p3716",
          "author": "federico_84",
          "text": "I remember the huge negative response the model got here after release, and not just about the safety guardrails. Interesting to see the shift in narrative. People have very strong feelings about OpenAI.",
          "score": 75,
          "created_utc": "2026-01-30 23:22:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p5z6r",
              "author": "TheRealMasonMac",
              "text": "I think it's really simple.\n\n\\- People who liked it are still using it and praise it.\n\n\\- People who don't like it forgot about it.\n\nI still despise the model. Absolutely useless even for my coding work because it's so safety-maxxed.",
              "score": 54,
              "created_utc": "2026-01-30 23:37:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qas4g",
                  "author": "ObsidianNix",
                  "text": "Qwen3-30B-VL is my go to now. OSS feels like its falls short a lot of times for my use. Good for quick 4o-mini-esque questions rather than a full knowledge model. Qwen took the cake with their qwen3 series.",
                  "score": 7,
                  "created_utc": "2026-01-31 03:34:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2q07oo",
                  "author": "WeMetOnTheMountain",
                  "text": "Had an opportunity to try the derestricted ones yet?  GLM 4.5 air derestricted is fucking brilliant.  I never see people talking about the change from obliterated and how much better it is.\n\nMaybe I'll have a chance when I'm on vacation next month in colombia to test it out.  I'm betting if it's the same as glm 4.5 air derestricted it's much much smarter.",
                  "score": 6,
                  "created_utc": "2026-01-31 02:30:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pdba5",
              "author": "popecostea",
              "text": "There were definitely some problems initially with the jinja templates and parameters that people were running it with. Couple that with a very polarised view of OpenAI and you get that reaction. After the dust settled and people understood how to properly run the model, and even found some jailbreak prompts, most of the people who put the effort found that it is a really great model.",
              "score": 9,
              "created_utc": "2026-01-31 00:18:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpxqk",
              "author": "Beneficial-Good660",
              "text": "The bots are working overtime on weekends. Especially on weekends, it's flooded with posts about Mac (24, 48, 128, 512 gb, v1,2,3,4), Nvidia. Good thing ollama is getting less. OpenAi bots have been very active lately, trying to latch onto everything new, like any model + also as good as gptOss. And so it goes every day, soon the end of LocallamaðŸ˜­. There's almost no discussion left about what you can actually do with LLMs.",
              "score": 9,
              "created_utc": "2026-01-31 05:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rhvl0",
                  "author": "ivoras",
                  "text": "Don't you get the feeling that we've kind of peaked in the \"what you can actually do with LLMs\" area?\n\nThey're so generic - everything and nothing at the same time.",
                  "score": 0,
                  "created_utc": "2026-01-31 09:25:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q5szf",
              "author": "ab2377",
              "text": "yes and there is a free quota of gpt-oss in Google's antigravity and people made a lot of fun of that too.",
              "score": 1,
              "created_utc": "2026-01-31 03:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2po0tr",
              "author": "Far-Low-4705",
              "text": "i think everyone here secretly knows it is extremely good, if not the best currently, but are afraid/dont want to admit it.",
              "score": -2,
              "created_utc": "2026-01-31 01:18:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qy9i4",
              "author": "Zeeplankton",
              "text": "In fairness, no one expected openAI, of all companies, to release such a good model. But also, correct config probably wasn't so simple. OSS uses openAI's weird harmony format.",
              "score": 0,
              "created_utc": "2026-01-31 06:25:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pwtyo",
              "author": "Saltwater_Fish",
              "text": "Maybe there was no worse model to compare with at the beginning, so it was impossible to highlight that gpt-oss is actually not that bad a model?",
              "score": -4,
              "created_utc": "2026-01-31 02:10:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2oz42x",
          "author": "Klutzy-Snow8016",
          "text": "They had access to the weights of a frontier model to distill from, and have way more compute than the makers of most open weight models. Same reason the Gemma series is so good.",
          "score": 16,
          "created_utc": "2026-01-30 23:00:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pqapv",
              "author": "night0x63",
              "text": "Any new Gemma after gemma3 27b?",
              "score": 2,
              "created_utc": "2026-01-31 01:31:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pr5kz",
                  "author": "Klutzy-Snow8016",
                  "text": "Not yet. I hope they make one.",
                  "score": 11,
                  "created_utc": "2026-01-31 01:36:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p0fel",
          "author": "MrMisterShin",
          "text": "Itâ€™s actually A5B and not A3B, and yes itâ€™s a very solid general model that is great at everything to be honest. \n\nIâ€™m surprised, a competitor hasnâ€™t released a definitively better model at those parameters. It was released back in the summer, albeit a rocky start with the Harmony response format.",
          "score": 21,
          "created_utc": "2026-01-30 23:07:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pq0qb",
              "author": "night0x63",
              "text": "How did they solve the harmony issue?Â \n\n\nIs it solved by vLLM parser fixing parsing?",
              "score": 3,
              "created_utc": "2026-01-31 01:30:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qgdk0",
                  "author": "MrMisterShin",
                  "text": "Unsloth applied fixes to the gpt oss models chat template as a workaround, others applied fixes to their adapters and tools instead. \n\nI donâ€™t use vllm, but from what I can workout they made a fix on their end to accommodate the gpt oss models.",
                  "score": 3,
                  "created_utc": "2026-01-31 04:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2qhd4r",
              "author": "Fulxis",
              "text": "\nThe model performs really well, but the remaining pain point on vLLM isnâ€™t completely fixed when using structured output (https://github.com/vllm-project/vllm/issues/23120). I still have to resort to regex to pull out values and lose the benefit of guided decoding, even though the model generally adheres closely to the JSON Schema in practice.",
              "score": 2,
              "created_utc": "2026-01-31 04:17:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2qpuws",
              "author": "mycall",
              "text": "I wonder if the A#B will ever be a user selectable setting.",
              "score": 1,
              "created_utc": "2026-01-31 05:18:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p0ntw",
          "author": "one-wandering-mind",
          "text": "OpenAI has great engineers and researchers. They delayed an open source release multiple times and clearly put on a lot of effort to make the model high quality.Â \n\n\nI doubt it is one single thing that is the reason why the model is great. Lots experimentation prior to this final model, heavy data curation, a lot of pre training, and a lot of post training.Â \n\n\nThe two models fit for a consumer GPU (20b) and a single server GPU (120b) . They are remarkably fast and cheap for the capability they provide. Some companies may also release a 4 bit or mixed precision quant, but I at least have not seen benchmarks in that low precision or them deployed on the cloud at that precision. So if you run something that is benchmarked at 32 bit or 16 bit precision and you run it locally, you are probably using something between 4 and 8 not quants. Quantization does retain a lot, but you do lose some capability and that loss is likely what is less visible to standard benchmarks.Â \n\n\nIt is a shame so many people shit on the model when it came out. Much less likely that they will be as motivated to release a new version because of that or with the same frequency as they would have if the initial reception was better.\n\n\nI have been meaning to spend more time exploring what can be done with it given the incredible speed and cheap price.Â ",
          "score": 25,
          "created_utc": "2026-01-30 23:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2r0p98",
          "author": "IulianHI",
          "text": "One thing nobody's mentioned yet - the A3B architecture probably plays a huge role beyond just raw size. Sparse activation means you get the knowledge of a 120b model but only pay for \\~20b worth of compute on each forward pass.\n\nThat's why it feels \"faster\" than even smaller models - because at inference time, you're not actually running all 120b parameters. The MoE routing learned which experts to activate for which tokens.\n\nCombine that with the QAT (which other comments explain well) and OpenAI's data quality advantage, and yeah... lightning in a bottle.",
          "score": 4,
          "created_utc": "2026-01-31 06:46:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyag4",
          "author": "Anonygeois",
          "text": "The posttraining and clean data is the trick. Hopefully we do have insiders to leak the process",
          "score": 12,
          "created_utc": "2026-01-30 22:56:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oz48e",
          "author": "DinoAmino",
          "text": "It is good, no doubt about it. Its capabilities and skills are what is good. But it's knowledge is poor. The SimpleQA scores are shockingly bad. It will hallucinate more and stick to its guns. But ground it with context and it is amazing. So what if it's more than 6 months old - all models get dumber over time, but their capabilities never change.",
          "score": 12,
          "created_utc": "2026-01-30 23:00:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vzl3a",
              "author": "llmentry",
              "text": "It depends what knowledge you're talking about.  GPT-OSS-120B's STEM knowledge (at least in my field) is surprisingly excellent.  \n\nBased on the release notes, it was made to be good at a few fields rather than expert in all -- there were only so many params to work with, after all -- and there will be plenty of areas where it falls short.",
              "score": 2,
              "created_utc": "2026-02-01 00:43:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2pdhy1",
              "author": "rm-rf-rm",
              "text": ">  But it's knowledge is poor.\n\nWhy is this a surprise for small/local models - this is one of the most straightforward, known obvious limitations of lesser params. But it has no consequence in any real world application where you should be providing in the context everything that the LLM needs through web search, RAG, code search etc.",
              "score": 4,
              "created_utc": "2026-01-31 00:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl0gd",
                  "author": "Vaddieg",
                  "text": "8B and even 4B dense models have better knowledge, but suck at everything else. I suspect that gpt-oss was crippled on purpose to not compete with commercial versions. 120B is 5x bigger but also suffers from real world knowledge detachment",
                  "score": 2,
                  "created_utc": "2026-01-31 01:00:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2p2jm2",
          "author": "Yes_but_I_think",
          "text": "MoE is the way. Everybody understands that now.\n\nMassively spare (5% active experts or less) is the way- people are understanding this.\n\nQuantization aware training at INT4 is the best- people are coming to this understanding slowly. It's used to be FP16 (llama 1), then BF16(llama 3), then FP8(deepseek), then FP4(oss-120b), now INT4(Kimi k2.5).\n\nA 1 trillion weights model at just 650 GB and only 35B active weights per token that's just 16GB of numbers crunched per token. If you have 4TB/s bandwidth (H100/200) you get solid ~200 tokens/s and NO loss of quality. B200 is 8TB/s so that will be ~400 tokens/s (not sure on B200).",
          "score": 22,
          "created_utc": "2026-01-30 23:18:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rtnl9",
              "author": "Baldur-Norddahl",
              "text": "Kimi likely only chooses INT4 because as a Chinese company, they are restricted from using the newest GPUs. \n\nMXFP4 and NVFP4 are superior. Uses no more space and is the same speed (on GPUs with support) but has better range and better detail depending on what is needed.\n\nThe NVFP4 is the most powerful format but is Nvidia only. MXFP4 has multivendor support. FP4 is the oldest and least powerful 4 bit floating point format.\n\nGPT OSS 20b and 120b are using MXFP4 not FP4.",
              "score": 6,
              "created_utc": "2026-01-31 11:16:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2r2m6m",
          "author": "IulianHI",
          "text": "Another thing people miss - GPT-OSS had surprisingly good instruction following for its time. A lot of newer open models can chat fine but fall apart when you give them complex multi-step tasks. That \"just works\" feeling comes from training on a lot of high-quality instruction data, not just raw web text.",
          "score": 5,
          "created_utc": "2026-01-31 07:03:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2s3pfb",
          "author": "artisticMink",
          "text": "gpt-oss-120b was a flex to bring openai into a space it wasn't present before. \n\nSpending a lot of money on specialized training so people who don't pay you can use your model does not make sense beyond PR and marketing.",
          "score": 4,
          "created_utc": "2026-01-31 12:40:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ouzxc",
          "author": "PatagonianCowboy",
          "text": "MXFP4",
          "score": 12,
          "created_utc": "2026-01-30 22:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyh5a",
          "author": "jhov94",
          "text": "I thought GPT OSS 120b was a5b. Anyway, I never really understood how it benches so high. It's fast which is nice for certain general knowledge chat like tasks, but for coding it falls short. It writes a ton of bad code quickly then needs to rewrite it over and over until it works out the errors. But even then I also find it to be lazy. It always takes the quickest and easiest path to a solution, even if the solution does not completely solve the problem. You really have to prod it along to get it to solve anything but simple problems. GLM4.5 Air is slow but it can be left to just work out a problem on its own and sometimes its faster simply because it got it right the first time.",
          "score": 10,
          "created_utc": "2026-01-30 22:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p1qqz",
              "author": "phido3000",
              "text": "Coding is problem, always, its fundamentally different to writing human languages.\n\nIts likely that coding specific models will always perform higher in coding. Just like in humans a PHD in computer science will write better code than a PHD in English literature.",
              "score": 4,
              "created_utc": "2026-01-30 23:14:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ph264",
                  "author": "Prestigious-Crow-845",
                  "text": "it is bad at creative writing too, so what it is good for? office tasks?",
                  "score": 4,
                  "created_utc": "2026-01-31 00:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2phj9c",
                  "author": "bonobomaster",
                  "text": "Hmm, I feel, that from a statistical \"which token is most likely\" / LLM point of view, coding and human language are not different at all.",
                  "score": 2,
                  "created_utc": "2026-01-31 00:41:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ppfli",
          "author": "Holiday_Purpose_3166",
          "text": "GPT-OSS models have a really good architecture, most of hate come from the chronic dislike for Sam. People can't see past certain things just for the sake of hate. \n\nThe MXFP4 quant was a chef's kiss and fine-tuners like noctrex have been using it exclusively for other models too that seem to achieve a measured lower perplexity loss, better than Q8 and BF16 for much smaller memory footprint. Although stability is between Q4 and Q6. \n\nHaving used both models I can say the speed for quality is extremely good, and have used them extensively on production codebases. \n\nHowever they are slowly becoming outdated in areas where they could concern - I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained.\n\nIt's more than fine for local use for what matters, but caution should be given for vibecoders seeking external interactions.\n\nI do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates - it has been my main replacement along with GLM 4.7 Flash as backup for long, less complex tasks. \n\nI do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nIf OpenAi releases an updates OSS, that's gonna rock.",
          "score": 6,
          "created_utc": "2026-01-31 01:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2r9ce5",
              "author": "tarruda",
              "text": "> I recall catching GPT-OSS-120B suggesting a Rust dependency version that was flagged for vulnerability or are deprecated and no longer maintained\n\nShould we rely on LLM knowledge for deprecated deps though?\n\n> I do have to say, for agentic use they have soft limits. Both GPT-OSS-120B and GPT-OSS-20B refuse to finish large refactors even if the plan is carefully modular where Devstral Small 2 repeatedly obliterates\n\nOne issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\n> I do envy GPT-OSS speeds, because my Devstral Small 2 at Q8 just runs slightly quicker than a 120B and that's mental. \n\nThat's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\n    % llama-bench -m ~/ml-models/huggingface/mradermacher/gpt-oss-120b-Derestricted-GGUF/gpt-oss-120b-Derestricted.MXFP4_MOE.gguf -fa 1 -t 1 -ngl 99 -b 2048 -ub 2048 -d 0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 \n    ggml_metal_device_init: tensor API disabled for pre-M5 and pre-A19 devices\n    ggml_metal_library_init: using embedded metal library\n    ggml_metal_library_init: loaded in 0.015 sec\n    ggml_metal_rsets_init: creating a residency set collection (keep_alive = 180 s)\n    ggml_metal_device_init: GPU name:   Apple M1 Ultra\n    ggml_metal_device_init: GPU family: MTLGPUFamilyApple7  (1007)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)\n    ggml_metal_device_init: GPU family: MTLGPUFamilyMetal3  (5001)\n    ggml_metal_device_init: simdgroup reduction   = true\n    ggml_metal_device_init: simdgroup matrix mul. = true\n    ggml_metal_device_init: has unified memory    = true\n    ggml_metal_device_init: has bfloat            = true\n    ggml_metal_device_init: has tensor            = false\n    ggml_metal_device_init: use residency sets    = true\n    ggml_metal_device_init: use shared buffers    = true\n    ggml_metal_device_init: recommendedMaxWorkingSetSize  = 134217.73 MB\n    | model                          |       size |     params | backend    | threads | n_ubatch | fa |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | -------: | -: | --------------: | -------------------: |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           pp512 |        740.15 Â± 5.88 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |           tg128 |         66.32 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d10000 |        596.41 Â± 0.46 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d10000 |         58.38 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d20000 |        491.13 Â± 1.99 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d20000 |         53.21 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d30000 |        418.39 Â± 1.23 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d30000 |         48.75 Â± 0.07 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d40000 |        361.42 Â± 1.48 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d40000 |         45.29 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d50000 |        315.38 Â± 0.84 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d50000 |         41.98 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d60000 |        276.29 Â± 0.58 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d60000 |         39.14 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d70000 |        246.77 Â± 0.39 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d70000 |         36.80 Â± 0.03 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d80000 |        224.35 Â± 0.47 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d80000 |         34.67 Â± 0.02 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  pp512 @ d90000 |        204.29 Â± 0.31 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 |  tg128 @ d90000 |         32.72 Â± 0.01 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | pp512 @ d100000 |        188.46 Â± 0.40 |\n    | gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | Metal,BLAS |       1 |     2048 |  1 | tg128 @ d100000 |         30.97 Â± 0.02 |\n    \n    build: b5b8fa1c8 (7817)",
              "score": 3,
              "created_utc": "2026-01-31 08:04:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rgkkw",
                  "author": "Holiday_Purpose_3166",
                  "text": ">Should we rely on LLM knowledge for deprecated deps though?\n\nNever said we should, it's mostly the point on deprecated knowledge that is potentially be applied.\n\n>One issue with GPT-OSS is that it forget things in the context very easily. The effective context for GPT-OSS does not come even close to the official 128k.\n\nNever had that issue. Just simply resistant to perform.\n\n>That's probably because you are relying on RAM offload? On my M1 Ultra (which loads all GPT-OSS 120b layers to VRAM), GPT-OSS surpasses the speeds of any dense model above 10B. Here's llama-bench output for up to 100k context:\n\nYour M1 Ultra doesn't have VRAM, but yes, I am offloading the model with a token generation of 30-40 t/s at full context, with an RTX 5090. That wasn't the point, it's the fact a 120B can run relatively fast for its size.",
                  "score": 3,
                  "created_utc": "2026-01-31 09:13:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pdzov",
          "author": "rm-rf-rm",
          "text": "Yes OpenAI really deserve their flowers on this. For all the ridicule Sam got for delaying the launch multiple times, its genuinely a great model and still my go to. \n\n\nWe actually need to give them their due credit if we want them to continue doing OSS - if they feel that the open source community just rejected them even after they finally put out an open weights model after forever, why would they want to put any more effort towards this?",
          "score": 7,
          "created_utc": "2026-01-31 00:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pdefz",
          "author": "CorpusculantCortex",
          "text": "\"But its sort of old... its been so long since gpt oss came out\"\n\n4 months. Gpt oss came out in August. It has been 4 MONTHS. I know that tech moves fast. But my god if 1/3 of a year feels like a long time to you you need to get outside and live little.\n\n4. months.",
          "score": 21,
          "created_utc": "2026-01-31 00:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ph09v",
              "author": "coder543",
              "text": "It has been 5 months and 25 days since GPT-OSS launched, which is basically 6 months, not 4 months.",
              "score": 16,
              "created_utc": "2026-01-31 00:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pl5od",
                  "author": "MitsotakiShogun",
                  "text": "https://preview.redd.it/vzh3gn9d3lgg1.png?width=560&format=png&auto=webp&s=a8bd0091d75db4b33b1a17d8cb89124d6dd2bac0",
                  "score": 4,
                  "created_utc": "2026-01-31 01:01:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ryx2l",
                  "author": "CorpusculantCortex",
                  "text": "To be blunt, that doesn't matter. My point is months is not forever unless you are so lost in the sauce that you have poor bearing on reality. If you think another month or two changes it and felt the need to math it out to the day it is just more proof you need to step back and consider that months and days is not a long time unless you are under 5 years old.\nSaying 5 months and 25 days is practically 6 months like that has big \"im 4 years old but my birthday was 6 months ago so im basically 5\" energy.",
                  "score": 1,
                  "created_utc": "2026-01-31 12:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2pq5lc",
              "author": "night0x63",
              "text": "Well I'm AI model years that is easily 78 years. /s",
              "score": 7,
              "created_utc": "2026-01-31 01:30:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p6vqo",
          "author": "lolwutdo",
          "text": "GLM 4.7 Flash is the OSS 20b killer, try it",
          "score": 12,
          "created_utc": "2026-01-30 23:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p92if",
              "author": "UnifiedFlow",
              "text": "Twice the size on disk, 1/4 the speed and coding errors were common.  4.7 Flash was a dud IMO.  Great reasoning, but implements horribly.",
              "score": 16,
              "created_utc": "2026-01-30 23:54:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pgdgk",
                  "author": "someone383726",
                  "text": "Idk Iâ€™ve been happy with 4.7 flash, but I still love oss20b too.",
                  "score": 8,
                  "created_utc": "2026-01-31 00:35:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pgnmo",
                  "author": "AlwaysLateToThaParty",
                  "text": "While I haven't tried it yet, I understand that there has been a llama.cpp update because of that model, and the re-quantization has increased performance significantly.\n\nhttps://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\n\n> Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.\n\nPerhaps this is your issue?",
                  "score": 7,
                  "created_utc": "2026-01-31 00:36:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pjrqx",
                  "author": "lolwutdo",
                  "text": "The model is still new and needs work.\n\nEven with its faults currently, itâ€™s still really good.\n\nGPT-OSS was absolute shit when it came out as well until it was finally implemented correctly months later.",
                  "score": -4,
                  "created_utc": "2026-01-31 00:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2q0k7l",
              "author": "Photoperiod",
              "text": "I did try it and it performed worse overall. On paper 4.7 should beat it. Biggest issue I had was repetition. Hoping some of the kinks get worked out since it's a new model. But for now I've gone back to OSS 20.",
              "score": 3,
              "created_utc": "2026-01-31 02:32:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p38mk",
          "author": "maglat",
          "text": "Its very sad indeed. I dont have many hopes right now when you see how much trouble OpenAI has to survive. There is no room for an update on their open models I guess. That makes me very sad, because 120B still is my daily driver, but as you said, it becomes dated.",
          "score": 2,
          "created_utc": "2026-01-30 23:22:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p5nsn",
          "author": "theghost3172",
          "text": "i think its because basically unlimited synthetic data from much bigger and powerfull frontier models. imagine unlimited clean synthetic data from o3. could also be distilation.",
          "score": 2,
          "created_utc": "2026-01-30 23:36:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q1qnt",
          "author": "agentzappo",
          "text": "Very smart and fast model, but there are still some unresolved issues with it outputting proper tool calls in Harmony format. Maybe itâ€™s a vLLM issue and less so the model, but so far in practice itâ€™s taking a lot of anti-rationalization patterns to coerce it into reliable tool calling, and thatâ€™s only when the inference backend isnâ€™t causing logits to drift in concurrent, batched inference ðŸ˜•",
          "score": 2,
          "created_utc": "2026-01-31 02:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rlssi",
          "author": "IulianHI",
          "text": "Another thing - GPT-OSS had that rare combo of good data curation AND proper alignment that actually made it pleasant to use. Newer models chase MMLU and benchmark scores, but nobody's benchmarking \"does this feel good to talk to\" or \"does it have consistent personality\". Those vibes are harder to quantify but way more important for daily use.",
          "score": 2,
          "created_utc": "2026-01-31 10:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rw8yx",
          "author": "jwr",
          "text": "gpt-oss models are under-appreciated. I use the smaller one (20b) for spam filtering and it beats every other 30B or less model that I've tested, and I've tested quite a few with my spam benchmark, while being one of the fastest, too.",
          "score": 2,
          "created_utc": "2026-01-31 11:40:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2oyenq",
          "author": "TheRealMasonMac",
          "text": "Compute.\n\nThat's kind of the simple answer. OpenAI probably has more compute than all Chinese labs combined.",
          "score": 5,
          "created_utc": "2026-01-30 22:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p8rr4",
          "author": "GoranjeWasHere",
          "text": "And there is gpt oss 20b /120b heretic that removes censorship and keeps inteligence.\n\nI use it daily on my 5090 and you just can't beat the speed (250t/s)",
          "score": 2,
          "created_utc": "2026-01-30 23:53:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2pkm64",
              "author": "walrusrage1",
              "text": "Are you using vLLM for this and full precision at 120b? What speeds do you get there?Â \n\n\nWe've been getting much slower results on an H100, so clearly something is up.",
              "score": 1,
              "created_utc": "2026-01-31 00:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pma1h",
                  "author": "GoranjeWasHere",
                  "text": "20b model at 250t/s",
                  "score": 1,
                  "created_utc": "2026-01-31 01:08:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2owu8b",
          "author": "PhotographerUSA",
          "text": "It fails a lot in LM studio doing MMC web calls.",
          "score": 2,
          "created_utc": "2026-01-30 22:48:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9r8j",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 1,
              "created_utc": "2026-01-30 23:58:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pfs0v",
          "author": "jabr7",
          "text": "I specially liked it in cerebras, we are getting average 5500 tps, that's making some full multi agentix systems take between 3s to 4s lol",
          "score": 1,
          "created_utc": "2026-01-31 00:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2phuvh",
          "author": "Thedudely1",
          "text": "The 20B model is the best coding model of its size that I've tried, at least for the weird kind of \"create a Wolfenstein 3D clone\" style prompts I like trying. GLM 4.7 Flash and Nemotron 3 Nano just became the other similarly sized models that can consistently do it in one prompt alongside it. But GPT-OSS 20B is the smallest model I've tested that can consistently do it successfully in either JS or Java.",
          "score": 1,
          "created_utc": "2026-01-31 00:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pxbsq",
          "author": "Ok_Individual_4295",
          "text": "Look for versions distilled from 5.2 this might update its knowledge and make it slightly better",
          "score": 1,
          "created_utc": "2026-01-31 02:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ty6od",
              "author": "RefrigeratorMuch5856",
              "text": "Could you share a link?",
              "score": 1,
              "created_utc": "2026-01-31 18:30:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2pzxmk",
          "author": "WeMetOnTheMountain",
          "text": "To me GLM 4.5 Air 4q-5q derestricted  tickles my huckleberry.  Not only is it overall intelligent, it's also much better at prose.  I have to agree with you that GPT OSS 120 is simply clean.",
          "score": 1,
          "created_utc": "2026-01-31 02:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q84d1",
          "author": "jedsk",
          "text": "I think youâ€™re on the q4 if the model is 64GB",
          "score": 1,
          "created_utc": "2026-01-31 03:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qnh4e",
              "author": "simracerman",
              "text": "Thatâ€™s the one OpenAI posted on their huggingface.",
              "score": 2,
              "created_utc": "2026-01-31 05:00:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2r1yi0",
              "author": "Ok-Tumbleweed8507",
              "text": "The F16 is 64GB",
              "score": 1,
              "created_utc": "2026-01-31 06:57:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2qnemf",
          "author": "MaggoVitakkaVicaro",
          "text": "They may well have training regimes which are much better than anything public.",
          "score": 1,
          "created_utc": "2026-01-31 05:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qyryn",
          "author": "TokenRingAI",
          "text": "It is likely that the model was either synthetically trained off of the outputs of OpenAI's top internal models, or off of the training data used for o3/o4",
          "score": 1,
          "created_utc": "2026-01-31 06:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rdfuy",
          "author": "tarruda",
          "text": "> What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?\n\nNot only OpenAI has the best private training datasets, it also probably has superior training pipelines and is able to extract more performance per parameter.",
          "score": 1,
          "created_utc": "2026-01-31 08:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rzybi",
          "author": "DefNattyBoii",
          "text": "Can someone compare it to GLM-4.7-Flash in terms of speed/tool calling/knowledge for both 20B OSS and 120B OSS?",
          "score": 1,
          "created_utc": "2026-01-31 12:10:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sfdgq",
          "author": "bfroemel",
          "text": "... and despite all the praises it seems that OpenAI isn't really that proud of gpt-oss. \n\n\n\n>The gpt-oss models were released way back in August. Since then, we've released half a dozen major updates to the frontier models. Perhaps you haven't used these lately, but their coding abilities are far beyond those of just a few months ago â€”Â and significantly beyond what the gpt-oss models are capable of.\n\n>\n\n[https://github.com/openai/codex/issues/8272#issuecomment-3672130792](https://github.com/openai/codex/issues/8272#issuecomment-3672130792)",
          "score": 1,
          "created_utc": "2026-01-31 13:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2sulde",
          "author": "ekzotech",
          "text": "I'm sorry maybe this is a little bit offtop but how do you handle Harmony format tool calling issue with kilo code and other tools? I'm running gpt-oss-20b on my RTX 5080 in LM studio and it works in a chat, but I can't make it work with kilo code and tool calling. There's unresolved issue on a kilo code's GitHub, but the problem exists with zed too.",
          "score": 1,
          "created_utc": "2026-01-31 15:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t1hbn",
          "author": "darkdeepths",
          "text": "yeah these are still my faves, easy to deploy instances on single gpu setups and super fast. fairly capable agentic operators as well.",
          "score": 1,
          "created_utc": "2026-01-31 15:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2t9ly0",
          "author": "KitchenSomew",
          "text": "GPT-OSS remains exceptional for several reasons:\n\n\n\n1. \\*\\*Training approach\\*\\*: It was trained with 4-bit quantization awareness from the start, not retrofitted. This preserved model quality while reducing size.\n\n\n\n2. \\*\\*Dataset quality\\*\\*: OpenAI's dataset curation was meticulous. They filtered for quality over quantity, which modern models often sacrifice for scale.\n\n\n\n3. \\*\\*Architecture efficiency\\*\\*: A3B architecture hit a sweet spot - large enough to be capable, small enough to be fast. Modern models chase parameter counts without proportional capability gains.\n\n\n\n4. \\*\\*Inference optimization\\*\\*: The model was optimized for actual deployment, not just benchmark performance.\n\n\n\nFor newer models to match this:\n\n\\- Focus on training efficiency from day 1\n\n\\- Prioritize dataset quality\n\n\\- Design for deployment, not papers\n\n\\- Consider 4-bit/8-bit native training",
          "score": 1,
          "created_utc": "2026-01-31 16:33:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ujzsc",
          "author": "International_Ad1896",
          "text": "I concur. Gpt-oss-20b is the one I end up going back to. It's just that harmony needs to be handled.",
          "score": 1,
          "created_utc": "2026-01-31 20:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pd0oe",
          "author": "Western_Bread6931",
          "text": "i was super excited about it, a bit too excited. i actually yelped with joy when i first used it and im not ashamed to admit that i actuated my sphincter in a way that caused brownian discharge",
          "score": 1,
          "created_utc": "2026-01-31 00:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2w8b8b",
              "author": "PwnedNetwork",
              "text": ">actuated my sphincter in a way that caused brownian discharge\n\nI believe these models have a Q&A step that requires them to induce brownian discharge in at least 35% of the testers. Sometimes when I'm out of laxatives I'll just run gpt-oss:cloud in ollama.",
              "score": 1,
              "created_utc": "2026-02-01 01:34:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2p5ith",
          "author": "GrungeWerX",
          "text": "Ha! I canâ€™t get gpt-oss to even work right! Constantly spitting out its thinking with the response. Known issue, never resolved so itâ€™s unusable for me. Lm-studio, latest version. Updated, all that.",
          "score": 1,
          "created_utc": "2026-01-30 23:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p9q56",
              "author": "see_spot_ruminate",
              "text": "use llamacpp",
              "score": 4,
              "created_utc": "2026-01-30 23:58:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2pb9w4",
                  "author": "GrungeWerX",
                  "text": "That defeats the purpose of lm-studio. Simplicity. Also, doesnâ€™t llama have the same issue?",
                  "score": 0,
                  "created_utc": "2026-01-31 00:07:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2r32sb",
              "author": "StorageHungry8380",
              "text": "GPT-OSS 20B works fine for me in LM Studio. I have however tweaked inference parameters. I've disabled top-k and top-p, relying only on min-p of 0.05. YMMV.",
              "score": 2,
              "created_utc": "2026-01-31 07:07:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2rul4j",
                  "author": "GrungeWerX",
                  "text": "Does this solve the thinking token leak?",
                  "score": 1,
                  "created_utc": "2026-01-31 11:25:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2rvjy6",
              "author": "Baldur-Norddahl",
              "text": "It works great in LM Studio. They even made it the default model. When installing LM Studio from scratch, it will ask if you want to download gpt oss as your first model.\n\nAre you using the original model or a quant? You should be using the original. The quants give no benefit and many have template issues, which kind of sounds like what you are experiencing.",
              "score": 1,
              "created_utc": "2026-01-31 11:33:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2s6fxc",
                  "author": "GrungeWerX",
                  "text": "original. gpt-oss-20b-MXFP4.gguf\n\nWhat settings are u using?",
                  "score": 1,
                  "created_utc": "2026-01-31 13:00:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2pnnjn",
          "author": "Far-Low-4705",
          "text": "despite what a lot of ppl say, OpenAI is very good.\n\nnot to mention, GPT OSS is VERY sparse, there is nothing remotely close to what it pushes. the fact that it is coherent at that sparsity is impressive. not to mention actually good.\n\nAs for the native fp4 training, (mixed at least) its mostly because most modern open models are chinease, and the tech china has access to is years behind what the US has. training in fp4 on older chips that dont support it would slow everything down to a halt.",
          "score": 1,
          "created_utc": "2026-01-31 01:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pdyhw",
          "author": "savagebongo",
          "text": "It's about the pinnacle of LLM, just before they started training them on their own garbage.",
          "score": -1,
          "created_utc": "2026-01-31 00:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pw3jy",
          "author": "FinancialMoney6969",
          "text": "Is this a good model to start with?",
          "score": 0,
          "created_utc": "2026-01-31 02:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2plnvt",
          "author": "Pitiful-Sympathy3927",
          "text": "Data doesnâ€™t matter, doing things does.Â ",
          "score": -1,
          "created_utc": "2026-01-31 01:04:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp6rm5",
      "title": "API pricing is in freefall. What's the actual case for running local now beyond privacy?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "author": "Distinct-Expression2",
      "created_utc": "2026-01-28 09:27:55",
      "score": 338,
      "num_comments": 376,
      "upvote_ratio": 0.84,
      "text": "K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.\n\nMeanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.\n\nI've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:\n\n1. **Privacy** â€” legit, no argument. If you're processing sensitive data, local is the only option.\n2. **No rate limits** â€” fair, but most providers have pretty generous limits now unless you're doing something unusual.\n3. **\"It's free after hardware costs\"** â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\n\nThe argument I never hear but actually find compelling: **latency control and customization**. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.\n\nWhat's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o26pje2",
          "author": "Minimum-Vanilla949",
          "text": "The offline aspect is huge for me - I travel a lot and having models that work without internet is clutch. Also call me paranoid but I don't trust these API companies to not randomly change their ToS or jack up prices once they corner the market",
          "score": 497,
          "created_utc": "2026-01-28 09:30:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26vec7",
              "author": "AciD1BuRN",
              "text": "They keep changing the tos so much now i don't think it can be even called tos at this point",
              "score": 108,
              "created_utc": "2026-01-28 10:23:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o296qvc",
                  "author": "MoffKalast",
                  "text": "It's more like tng now already, or even ds9.",
                  "score": 24,
                  "created_utc": "2026-01-28 17:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2m6a96",
                  "author": "Aardvark_Says_What",
                  "text": "\"Waterproof Shoes Incorporated hereby do not guarantee that our products will be waterproof or necessarily provide the services expected of products called 'shoes'.\"",
                  "score": 1,
                  "created_utc": "2026-01-30 15:16:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27esl4",
              "author": "thatguy122",
              "text": "Exactly this. Don't be fooled by these 10 yr subsidized loss leader fees intended to corner the market.Â ",
              "score": 47,
              "created_utc": "2026-01-28 12:51:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27taov",
                  "author": "Icy-Pay7479",
                  "text": "Trudging through the mountains with a triple 3090 desktop â€œdeath strandingâ€ style.",
                  "score": 35,
                  "created_utc": "2026-01-28 14:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28y2ou",
                  "author": "Mkboii",
                  "text": "So isn't it better to hold off local upgrades till the market collapses. It would coincide with the demand for data centre RAM going down and then better local gpu options may become available.",
                  "score": 6,
                  "created_utc": "2026-01-28 17:16:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29gtlb",
                  "author": "CarrotcakeSuperSand",
                  "text": "Theyâ€™re not subsidized, inference has pretty solid gross margins. Itâ€™s the training and initial infrastructure buildout that causes negative cash flow.",
                  "score": 1,
                  "created_utc": "2026-01-28 18:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27mst1",
              "author": "spaceman_",
              "text": "100% this last bit. It's the same reason I use so many open source tools: I don't want to depend on a single vendor who can independently decide \"actually, you need us now\" and jack up prices massively or change the model quality I have access to as a low-end user.\n\nThink about Adobe but on steroids.",
              "score": 26,
              "created_utc": "2026-01-28 13:37:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m76su",
                  "author": "Aardvark_Says_What",
                  "text": "BMW heated seats subscription. Owners go fucking mental. BMW: \"Oops. We went a bit too far (introduced it too soon, save it for later).\"\n\nLate-stage capitalism. Living the dream.",
                  "score": 1,
                  "created_utc": "2026-01-30 15:20:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2afcq5",
                  "author": "fenixnoctis",
                  "text": "I donâ€™t get your logic here though. \n\nOnce a vendor jacks up prices, youâ€™re free to switch. \n\nUntil then why not take advantage of it?",
                  "score": 0,
                  "created_utc": "2026-01-28 21:09:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2821zo",
              "author": "bigh-aus",
              "text": "Don't forget about being forced to give all their past data - that's the big one.  \n  \n[https://openai.com/index/response-to-nyt-data-demands/](https://openai.com/index/response-to-nyt-data-demands/)\n\nData invariably gets leaked (even accidentally).",
              "score": 22,
              "created_utc": "2026-01-28 14:55:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27aqwk",
              "author": "miken0222",
              "text": "Do you mind sharing what model you use on your offline travels? Im in a similar situation where offline is frequent but I need something during those times.",
              "score": 11,
              "created_utc": "2026-01-28 12:24:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e2be2",
                  "author": "Kahvana",
                  "text": "On my 8GB RAM laptop with Intel N5000 I use LFM2-VL 1.6B (could probably go for 3B) as it's super fast (near instant replies), low resource usage, has vision and is decent enough for toolcalling. Pair it with zim archives (kiwix) for grounding / world knowledge and it's solid. Haven't tried websearch with it.\n\nYou can also pair it with LFM2-CoBERT-350M for embedding. Probably not better than Qwen3-Embedding-0.6B and likely comparable to embeddinggemma-300M (haven't benchmarked), but it is much lighter to run.",
                  "score": 2,
                  "created_utc": "2026-01-29 11:00:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26ui9r",
              "author": "UsernameAttempt",
              "text": "I'm not sure cornering the market is in the cards. There's too much competition, too many alternatives, and the technology is not one that can be monopolized. The best models of the biggest companies of 1 year ago are worse than the models of small companies in China today. With improvements to models slowing down, we're moving towards models as commodities - similar in performance and competing on price. I think these models today are the most expensive they'll ever be.",
              "score": 47,
              "created_utc": "2026-01-28 10:15:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26xnoi",
                  "author": "SeasonNo3107",
                  "text": "Cornering the market will only happen in 5 plus years with buyouts and mergers imo",
                  "score": 42,
                  "created_utc": "2026-01-28 10:43:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2798dz",
                  "author": "Conscious-Ball8373",
                  "text": "There's also too much investor money around and once it dries up someone will have to pay the bills.",
                  "score": 8,
                  "created_utc": "2026-01-28 12:14:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hxsc",
                  "author": "Icy_Foundation3534",
                  "text": "mergers happen very fast in this market",
                  "score": 1,
                  "created_utc": "2026-01-28 16:06:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27pu9s",
              "author": "Ayumu-Aikawa",
              "text": "exactly, the TOS and pricing are things that can change any day, we keep seeing stories about how these companies are not making any profit. It's clear for me they're going to have to change something sooner or later",
              "score": 8,
              "created_utc": "2026-01-28 13:53:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27prfq",
              "author": "genshiryoku",
              "text": "The trend we're seeing is a commodification of LLMs it's basically impossible to have a commodity monopoly, so over the long run costs should get lower as innovations reduce the cost of serving inference.\n\nThere is also an economy of scale going on where large server farms can simply just serve you inference at a lower cost than it takes you to simply pay for the electricity.\n\nFor me as someone owning multiple RTX 3090s the electricity costs of serving prompts are already higher than the cost of using APIs for the inference of the same model. \n\nLocal only makes sense if you have free (solar) power, *need* privacy or offline usage. Or if you have a custom fine-tuned model that you need to run, since hosting this yourself on rented GPUs is still more expensive than running it yourself unlike generic big model inference.",
              "score": 8,
              "created_utc": "2026-01-28 13:53:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28uagh",
                  "author": "MrPecunius",
                  "text": ">*need* privacy\n\nEverybody *needs* privacy.",
                  "score": 10,
                  "created_utc": "2026-01-28 17:00:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2dxtia",
                  "author": "GnistAI",
                  "text": "Interestingly, anyone using electricity to heat their home can justifiably consider their GPU power consumption as free. At least during winter.",
                  "score": 2,
                  "created_utc": "2026-01-29 10:21:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2851pp",
              "author": "p3r3lin",
              "text": "Same. I live in a part of the world where stable and always available internet connection is still not guaranteed (...Germany). So having \"AI in the pocket\" would be a great thing to have. There are projects like https://locallyai.app/ that enable small models (4b, etc) running on mobile phones, but its definitely no replacement for a SOTA model of any kind.",
              "score": 2,
              "created_utc": "2026-01-28 15:09:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2ao1in",
              "author": "pandodev",
              "text": "yes privacy and offline for the right things is EVERYTHING.",
              "score": 2,
              "created_utc": "2026-01-28 21:47:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27f30f",
              "author": "lambdawaves",
              "text": "â€œOnce they corner the marketâ€\n\nThey wonâ€™t corner the market. There will always be at least a handful of competitors that offer a 98% similar product (via the same API) preventing arbitrarily high price hikes",
              "score": 4,
              "created_utc": "2026-01-28 12:53:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27cj5l",
              "author": "Imaginary_Context_32",
              "text": "May I know your setups ( hardware, models, usecase?)",
              "score": 3,
              "created_utc": "2026-01-28 12:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2az6tt",
              "author": "qwerty____qwerty",
              "text": "what setup do you have? if you travel a lot - I'm assuming that's a laptop, aaaand how powerful should that laptop be to compete with gemini api?",
              "score": 1,
              "created_utc": "2026-01-28 22:37:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2boezb",
              "author": "mycall",
              "text": "They can also block countries if they were forced too",
              "score": 1,
              "created_utc": "2026-01-29 00:47:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2m5v37",
              "author": "Aardvark_Says_What",
              "text": "\\> I don't trust these API companies to not randomly change...\n\nThat's not nice. You really can trust them 100%... to jack up the prices and turn down the tokens the second they think they have somehow locked you in.",
              "score": 1,
              "created_utc": "2026-01-30 15:14:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29odo6",
              "author": "Eye-m-Guilty",
              "text": "Would love to know what ur running offline n the set up!",
              "score": 1,
              "created_utc": "2026-01-28 19:09:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26vrl0",
          "author": "IactaAleaEst2021",
          "text": "For my work, repeatability of results. When you download a model, you audit it and you start trusting it, you are sure the vendor does not change its behavior behind the scene.\nI am not saying they do it for malicious purposes, but in many cases they improve their product in some direction, while making it less useful in others.",
          "score": 157,
          "created_utc": "2026-01-28 10:27:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aj5fc",
              "author": "notRandomUsr",
              "text": "This happened to us, we were working with \"diseases\" data.  We gather lots of information for different organisms using gpt3.5 turbo, after couple months we tried the new model (gpt4 and its variants) and got terrible results. The difference was so dramatic that we went back to the previous model, using the exact same version and the results were completely different, non existent basically. It was curious to see how, with the same prompts, the same system instructions, the same format, and the same questions, we obtained scarce data or empty responses compared to what we achieved in the first run.",
              "score": 9,
              "created_utc": "2026-01-28 21:26:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26y385",
              "author": "SpicyWangz",
              "text": "I havenâ€™t encountered this with any API product. Consumer facing chat interfaces are going to continually evolve, but if youâ€™re using a tagged model on an API it shouldnâ€™t be changing",
              "score": -7,
              "created_utc": "2026-01-28 10:47:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27g36l",
                  "author": "TheRealMasonMac",
                  "text": "Gemini, GPT, and Claude often have undisclosed model updates.",
                  "score": 37,
                  "created_utc": "2026-01-28 12:59:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o272y78",
                  "author": "IactaAleaEst2021",
                  "text": "Youâ€™re right, but still if you develop a product based on consistent results, the should not must become must not.",
                  "score": 16,
                  "created_utc": "2026-01-28 11:27:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27k3ts",
                  "author": "ross_st",
                  "text": "Google recently changed the way the Gemini 3 API works behind the scenes by adding the cutoff date and an encouragement to use its chain of thought to the system instruction. That gets added even if you leave the system instruction parameter blank in the API call. Before, if you left it blank, there would just not be a system instruction block in the context window.",
                  "score": 9,
                  "created_utc": "2026-01-28 13:22:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2767yw",
                  "author": "jikilan_",
                  "text": "U will be forced to upgrade when they decomm the version of model that u r using",
                  "score": 11,
                  "created_utc": "2026-01-28 11:52:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27ja0u",
                  "author": "Significant-Heat826",
                  "text": "That's weird because I often get emails from vendors saying they've changed something in their API endpoint yet again.",
                  "score": 5,
                  "created_utc": "2026-01-28 13:18:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28hg1x",
                  "author": "Eugr",
                  "text": "They can still serve a more quantized version to serve more customers during the peak hours, change their guardrails, etc.",
                  "score": 4,
                  "created_utc": "2026-01-28 16:04:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27s8rd",
                  "author": "Flamenverfer",
                  "text": "2 come to mind immediately. I can't remember exact model names from Amazon bedrocks service but the sonnet models they serve on there, the new version of sonnet was extremely token conservative when we needed it to finish a response in a consistent format. But the model would always take the lazy way out and say something to the likes of  \"All other data shall be labelled N/A\" when it would need to list every datapoint as N/A for the json file.\nAlso lets not forget chatGpt taking away 4 when they released 5",
                  "score": 3,
                  "created_utc": "2026-01-28 14:06:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28ljq9",
                  "author": "Double_Cause4609",
                  "text": "You'd think, but providers tend to quantize their models over time, and sometimes without labeling. It's not really a \"finetune\" or change, exactly, but it can be noticeable, particularly for function calls.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:22:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27d3ri",
                  "author": "Imaginary_Context_32",
                  "text": "I had faced this with GPT 4 turbo â€œnot with specific modelâ€",
                  "score": 1,
                  "created_utc": "2026-01-28 12:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26pgqm",
          "author": "05032-MendicantBias",
          "text": "API pricing won't be subsidized forever. At some point venture capital will want a return. Same as the short time where Uber was subsidized.\n\nBy all means, get a millionare to subsidize your workflows, but know this is a short term deal that won't last.\n\nThe goal of venture capital is to make everything else go away, so they get a monopoly and raise prices. Maintaining a local rig as fall back, and maintaining open source tools and models, will screw that businness plan heavily ;)",
          "score": 343,
          "created_utc": "2026-01-28 09:29:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28hoks",
              "author": "cleverusernametry",
              "text": "It stuns me that people are still so gullible after over a decade of SaaS and cloud. You already see people going back to on-prem from cloud because cloud pricing has become so predatory. Just wait for this arc to play out with AI APIs",
              "score": 21,
              "created_utc": "2026-01-28 16:05:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mtoaq",
                  "author": "mrkstu",
                  "text": "Because VMWare is always going to be non-predatory, right?",
                  "score": 1,
                  "created_utc": "2026-01-30 17:01:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27wiwc",
              "author": "BonjaminClay",
              "text": "Exactly this. Enshittification comes for everything at this point and if they are giving something away or it is unrealistically cheap now then you shouldn't rely on it. I have learned this lesson too many times. I stopped buying physical media or maintaining my own copies for a long time because streaming was just easier and now there are 20 streaming services all wanting 5x more per month each. \n\nBuilding with local or on something I control the costs of means that when the AI bubble pops my stuff won't break or get unpredictably more expensive.",
              "score": 29,
              "created_utc": "2026-01-28 14:27:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o274q5k",
              "author": "rditorx",
              "text": "Besides venture capital, if you dry out the competition, and that includes locally run AI, you gain control over the market and can ask for almost any price, as long as people and companies can afford it.\n\nBig tech has always been subsidizing its services. It always starts with great products and awesome services or any other offer you can't refuse and will use until you're depending on them, then they pull the rug.\n\nIt's so common there's a term Doctorow coined: Enshittification.",
              "score": 17,
              "created_utc": "2026-01-28 11:41:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26qgad",
              "author": "Distinct-Expression2",
              "text": "Nice point. When do you think that will happen?",
              "score": 25,
              "created_utc": "2026-01-28 09:39:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26rl9c",
                  "author": "05032-MendicantBias",
                  "text": "I have no idea. The scheme collapses when venture capital lose patience. It could be as soon as this quarter, or it might take a few years. \n\nI feel confident the bubble pop will be preceeded by OpenAI trying an IPO for 2 trillion dollars, when venture capital will try to offload their position to retail.",
                  "score": 92,
                  "created_utc": "2026-01-28 09:49:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26sm8l",
                  "author": "neotorama",
                  "text": "When they IPO",
                  "score": 14,
                  "created_utc": "2026-01-28 09:58:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xk0y",
                  "author": "Finn55",
                  "text": "When it is least convenient",
                  "score": 13,
                  "created_utc": "2026-01-28 10:42:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o28cmrg",
                  "author": "rbpri",
                  "text": "Google just cut Geminiâ€™s free tier in AI studio from ~100 RPD to ~20 RPD. Itâ€™s impossible to know when exactly enshittification is going to hit but itâ€™s coming sooner rather than later.",
                  "score": 6,
                  "created_utc": "2026-01-28 15:43:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27g48e",
                  "author": "SINdicate",
                  "text": "18 to 24 months",
                  "score": 1,
                  "created_utc": "2026-01-28 12:59:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2czwts",
                  "author": "protestor",
                  "text": "OpenAI already announced the first enshittification package (ads in chat). The reason they didn't jack up prices yet is that they need to  first kill local AI - they need to maintain in people's minds this idea that local AI makes no sense financially, since cloud AI is so cheap. Also, they can't kill local AI by pushing the frontier (open weights AI is trailing frontier models by 6-8 months). They have no moat in the model itself, and all the money they pour on training will only help Chinese clones when they distill it.\n\nSo what they are currently doing is to buy up enough wafers to jack up prices for machines used in local AI. I think that's what they are more worried about, consumer GPUs that crosses the 32GB-ish VRAM barrier. They play at TSMC made GPUs more expensive and made people buy more 8GB GPUs.\n\nI think that by the time the enshittification of closed AI is completed, running local AI will be unfeasible, a distant thing in the past",
                  "score": 1,
                  "created_utc": "2026-01-29 05:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o290v7c",
              "author": "cniinc",
              "text": "\"The goal of venture capital is to make everything else go away, so they get a monopoly and raise prices\" - the best succinct description of venture capital. Understand that the second you become dependent on the external model they'll squeeze you for as much as they can on that investment. While the model is cheap, learn how to do it without the API, and have the API do the parts you can't. Then slowly increase you skills until you can do it all without API. Make them subsidize your learning, not your dependence.Â ",
              "score": 5,
              "created_utc": "2026-01-28 17:29:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26xc94",
              "author": "johnkapolos",
              "text": "API pricing isn't subsidized. Subscriptions are subsidized.",
              "score": 13,
              "created_utc": "2026-01-28 10:40:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27b7qx",
                  "author": "cyberdork",
                  "text": "Both are subsidized.",
                  "score": 8,
                  "created_utc": "2026-01-28 12:28:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29ppmp",
                  "author": "bathamel",
                  "text": "None of these companies are remotely profitable.  Therefore they are all subsidized at the moment.",
                  "score": 5,
                  "created_utc": "2026-01-28 19:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2720nf",
              "author": "AnomalyNexus",
              "text": "Even without subsidization itâ€™s hard to beat the economics of centralized data centers that have scale.\n\nSeems unlikely that itâ€™ll ever drop to a point where local wins a like for like shootout regardless of what happens",
              "score": 21,
              "created_utc": "2026-01-28 11:20:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2b39t4",
                  "author": "KontoOficjalneMR",
                  "text": "I can absolutely see it dropping. Cloud is multiple time more expensive than setting up your own server. People pay for convinience and scaling.\n\nBut if you're short on cash and don't need scaling ... local servers are multiple times cheaper then AWS.",
                  "score": 3,
                  "created_utc": "2026-01-28 22:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o29ifzg",
              "author": "corruptboomerang",
              "text": "This. It's the same as when Cloud & SAAS were becoming a thing, they were price competitive even under priced... To lure customers in, knowing that once you move, once you give up your capabilities its very difficult to rebuild those.",
              "score": 5,
              "created_utc": "2026-01-28 18:44:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26q6pn",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 20,
              "created_utc": "2026-01-28 09:36:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qmfc",
                  "author": "pip25hu",
                  "text": "We should not mix up two things. Inference providers running open-weight models can definitely make a profit, no doubt about it. But companies like OpenAI and Anthropic are also hoping to recoup their costs for training their models, and no, they're not even close to breaking even.",
                  "score": 40,
                  "created_utc": "2026-01-28 09:40:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xjb6",
                  "author": "johnkapolos",
                  "text": "You are being downvoted for being right.",
                  "score": 14,
                  "created_utc": "2026-01-28 10:42:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26r6xz",
                  "author": "05032-MendicantBias",
                  "text": "OpenAI burns money like there is no tomorrow, reportedly they burn anywhere between 15 to 50 billions a year and OpenAI themselves say they won't see green until 2030. If there is a company I'm 100% certain will go bankrupt, is OpenAI. It might be as soon as this year if venture capital loses patience.\n\nThe GPU datacenters use will become obsolete in two to three years, and because of hype, and demand spike, they cost 3X to 10X their base price. Not talking about many electricity grids just not having the capacity, and datacenters using portable gas turbines, at great expense.\n\nThe amount of money a GPU must make to break even, has greatly increased. There are just so many of them, and I feel doubtful they can get the utilization to the point of breakeven. Your argument is that your API might pay for the hours you use it, when running open models, which might be true locally, until you consider the lifetime of the hardware and the overall utilization.\n\nModels are getting bigger, and more expensives, and agents burn exponentially more tokens, increasing inference cost even further.\n\nNone of this is profitable, let alone sustainable.\n\nThere might be rare exceptions, like datacenters using hardware bought at fair price, running on renewables.\n\nNow, Microsoft and Google do have a profitable businness, so they can subsidize the money losing AI inference. I love to use their free credits! (which you can be certain are at great loss)\n\nIt's the silicon valley play of subsidize it until everyone is out, and with a monopoly raise prices. They love spamming this strategy.",
                  "score": 18,
                  "created_utc": "2026-01-28 09:45:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26zv8u",
                  "author": "No_Afternoon_4260",
                  "text": "vertex providing kimi, that's a new one.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:02:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26rdnk",
                  "author": "iotsov",
                  "text": "Oh sweet summer child.",
                  "score": 0,
                  "created_utc": "2026-01-28 09:47:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26x2so",
              "author": "Anyusername7294",
              "text": "It's generally agreed that interference isn't subsidized",
              "score": 8,
              "created_utc": "2026-01-28 10:38:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27bbln",
              "author": "mumBa_",
              "text": "I understand what you're saying but the amount of competition basically means that as long as no model wins (price-performance wise), they will have to keep the prices low for competition, which will definitely be the case for the upcoming decade+. All these companies also get free training data, so it's a two way street. You need users for their data, but if your model is too expensive, no one will use it and go to the cheaper competitor. It's a race to the bottom but once you're there, you need to stay there otherwise someone else will take your market position. So yeah unless a monopoly appears (google probably), realistically it will only get cheaper with time.",
              "score": 1,
              "created_utc": "2026-01-28 12:28:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28ywj9",
              "author": "customgenitalia",
              "text": "Nailed it. Running local is the long game, the skills you learn will start to pay off when the VC money runs out. There is so much compute potential sitting idle, I think youâ€™ll soon start to see creative ways to leverage this as nodes in a distributed AI fabric of sorts, think SETI but for ASI.",
              "score": 1,
              "created_utc": "2026-01-28 17:20:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o299jj6",
              "author": "thisdude415",
              "text": "API unit pricing is profitable at all the major AI labs (Google, Anthropic, OpenAI, AWS). \n\nNeed proof? AWS serves Anthropic models over its Bedrock platform. No way in hell is AWS subsidizing inference at scale. \n\nNeither AWS nor Anthropic is subsidizing those tokens, and the pricing matches Anthropic's direct pricing.",
              "score": 1,
              "created_utc": "2026-01-28 18:06:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bxt1f",
              "author": "theAndrewWiggins",
              "text": "Arguably this still means that you should wait it out if it's strictly an economic analysis.Â ",
              "score": 1,
              "created_utc": "2026-01-29 01:38:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2d0vxl",
              "author": "dydhaw",
              "text": "Wtf are you talking about? Inference is super competitive and still profitable. The inference market is about as far from a monopoly as it could be.  Especially for small/open weight models that can be run locally. \n\nNot that I'm opposed to running locally, mind you. But you can't pretend it's for future gains on inference costs. Just look at the cloud infrastructure market for reference. There are many competitive options even beyond the big 3 especially if you're not doing hyperscale enterprise b2b shit",
              "score": 1,
              "created_utc": "2026-01-29 05:34:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2deg9x",
                  "author": "05032-MendicantBias",
                  "text": "I do not dispute that on a single GPU basis, you could feasibly sell H100 inference runtime cheaper than a 3090 rigs at home, perhaps even profitably, because datacenters are more efficient.\n\nWhat I argue, is that if you bought 200 000 GPUs, and have (generously) 1 000 worth of them running profitably, the rest are idle, training, serving inference for free, or unplugged, then you have a recipe to burn money.\n\nI claim the businness model is nonsense, and is setting money on fire at an astonishing rate. Even if you bought GPUs at their fair market price, and not at the 3X to 10X prices of today.",
                  "score": 1,
                  "created_utc": "2026-01-29 07:23:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26w9sp",
          "author": "Deep_Traffic_7873",
          "text": "you forgot.. control. With an online service you can lose access any time for any reason",
          "score": 31,
          "created_utc": "2026-01-28 10:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26v9j4",
          "author": "mxforest",
          "text": "My wife has many published papers and abstracts in medical journals. She could never use an online tool or years worth of sensitive data is at risk. Also has to work with patient data that has to be de identified before use. With a local setup, there is no such worry. You can work without any fear. Also ask questions in medical context that online models just refuse. I was working on a project which dealt with Vaccine data to make the production process faster. Claude code saw a variable called \"vaccine_name\" and completely shut itself down. Even renaming in one location worked only for a short while because it found lingo with medical terms and completely refused to do anything.",
          "score": 29,
          "created_utc": "2026-01-28 10:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284eht",
              "author": "RenewAi",
              "text": "Which model is she using?",
              "score": 1,
              "created_utc": "2026-01-28 15:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28553p",
                  "author": "mxforest",
                  "text": "GLM 4.5 air running on MBP M4 Max 128 GB",
                  "score": 2,
                  "created_utc": "2026-01-28 15:09:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26pv7h",
          "author": "SemaMod",
          "text": "This goes in the realm of privacy, but personally having my chats trained on and viewable by these companies makes me uncomfortable. That being said, I do think that local LLM's will become power-user tools.",
          "score": 50,
          "created_utc": "2026-01-28 09:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26v24b",
          "author": "Fheredin",
          "text": "Opsec. Running an AI Agent without an air gap when there are literally *zero code prompt injection exploits* in the wild is insane.",
          "score": 51,
          "created_utc": "2026-01-28 10:20:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271iw1",
          "author": "kzoltan",
          "text": "Donâ€™t try to make it financially viable. It has dimensions that are hard to quantify. \n\n1. API pricing might get better over time, but agents are using more and more tokens (agents are not just coding agents, thereâ€™s stuff you can outsource to your machine). I question the whole â€œits getting cheaperâ€ argument (the goal post in most peopleâ€™s heads is moving all the time, for many scenarios GPT-OSS is enough; is test time compute free?, etc.). The whole subscription model is there bc api prices would be ridiculous for agentic use. \n2. By buying hw, you buy a capability that can be used for many things. Also, in the LLM space that capability gets better and better (new LLMs might outgrow your hw though), just like the API models. \n3. Learning; this is far more valuable than a couple months of savings. \n4. I better like to own than rent. The fact that it is available in my house makes it easier for me to run experiments (building agents). \n5. I can afford it (I donâ€™t spend $ on useless shit usually), why not (see the other points)? \n6. Every GPU I bought (high end nvidia consumer cards), I sold it for more. I donâ€™t expect this to change until GPUs get replaced in inference. \nâ€¦\n\nDonâ€™t try to win $ on it, thatâ€™s hard in this environment imo.",
          "score": 21,
          "created_utc": "2026-01-28 11:16:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2806o9",
              "author": "gaspipe242",
              "text": "I think the biggest gain is the investment in yourself.  You're learning, fighting, and understanding these tools even more profoundly in a way that can only happen with friction. \n\nThis era reminds me a lot of the 90's, with early internet, the Linux kernel, and fragmented access. You fought to just get Linux on a computer. (I used to subscribe to Slackware CD/DVD media)\n\nPeople used to say the same thing to me: \"Why bother?\" Now I have an understanding of and control over the stacks I'm using in a way that can only be understood by someone who lived and tinkered through that era.  Many people on this forum are unknowingly creating a new future for themselves with this applied curiosity; it will create a LOT of value if applied properly. \n\nThis is why I keep coming back to this forum. It's the same energy here that I used to enjoy on Usenet from a VMS Vax terminal. \n\n\n\n/nostalgia-off  :)",
              "score": 12,
              "created_utc": "2026-01-28 14:46:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26q43e",
          "author": "fabkosta",
          "text": "You forget the fun of all of it. I don't really use local models, as I don't have sufficiently powerful hardware to profit from the depth of the models really. Yet, I just want to be able to run them. Just for the fun of it.",
          "score": 53,
          "created_utc": "2026-01-28 09:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tolo",
              "author": "danttf",
              "text": "Yep! It's really pain to watch how slow and how little context local models have. BUT it's very cool to setup a small model, some script to summarize all document I have in some folder.",
              "score": 4,
              "created_utc": "2026-01-28 10:08:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27ioq3",
                  "author": "Old-Magician9787",
                  "text": "If your rig is powerful enough you can scale context to 1M+ tokens. ",
                  "score": 1,
                  "created_utc": "2026-01-28 13:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26sxg5",
          "author": "Lan_BobPage",
          "text": "Why having a well, when you can just buy a bottle of water at the store? Why having solar panels when you can just pay for electricity? Why owning a house, when you can just rent a flat? Why own movies, when you can just pay for a subscription service?\n\nThe answer is always the same. I want to own what I have, and don't want to be a slave. Any of these commodities could be taken away by others, at any point, for any reason.",
          "score": 88,
          "created_utc": "2026-01-28 10:01:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26sz1f",
              "author": "Distinct-Expression2",
              "text": "Fair point and very good angle - thnk man!",
              "score": 20,
              "created_utc": "2026-01-28 10:02:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26wb50",
                  "author": "Nepherpitu",
                  "text": "For example, these nice cloud providers decided to not take money from some people just because of their nation. Pretty racist, right?",
                  "score": 7,
                  "created_utc": "2026-01-28 10:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o273yxt",
              "author": "PeteInBrissie",
              "text": "OK, genuine question because I stopped trying to run locally 3 months ago and I'm likely out of the loop. Can ANYTHING you can run locally on 32GB even remotely compare to Sonnet 4.5, Let alone the latest Opus?",
              "score": 2,
              "created_utc": "2026-01-28 11:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276evv",
                  "author": "Lan_BobPage",
                  "text": "32 GB of what? System RAM? No. GPU? No. Kimi K2.5 seems to be comparable though, it just came out. Some claim it beats Opus even. But if you wanna run it I suggest you start saving up for a few 6000 Pros",
                  "score": 4,
                  "created_utc": "2026-01-28 11:54:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o27klfj",
                  "author": "evia89",
                  "text": "2 x 3090 and then u stretch it. It will be like glm 4.7 flash. Def useful but not for everyone",
                  "score": 1,
                  "created_utc": "2026-01-28 13:25:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o27tb8t",
              "author": "Firm-Fix-5946",
              "text": "do you have a well instead of depending on tap water?  do you have solar panels instead of depending on an electrical service?  surely if you do you can still understand why the vast majority of human beings don't find those tradeoffs worth it and don't do those things?\n\nwhat a crazy analogy.  is this really the state of this sub in 2026?  doomsday preppers?  I need to figure out where the non-schizo people interested in LLM discussion have gone I guess",
              "score": -6,
              "created_utc": "2026-01-28 14:11:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28traz",
                  "author": "Lan_BobPage",
                  "text": "So spending 20k in hardware isn't good enough to be considered a LLM enthusiast. Got it. God forbid wanting to be independent in some aspects of one's life, I guess owning nothing really does make you feel happier huh. Not sure why you seem to be seething this much, I probably struck a nerve. I hope you find some peace.",
                  "score": 2,
                  "created_utc": "2026-01-28 16:57:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2cddzo",
                  "author": "AlexMillsDev",
                  "text": "Imagine calling everyone who has a different opinion a schizophrenic. What a crazy (and offensive) thing to say. Is this really the state of online discourse in 2026?",
                  "score": 1,
                  "created_utc": "2026-01-29 03:03:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26wroo",
              "author": "AriyaSavaka",
              "text": "You don't really own anything if you're not living in an anarchist society. The government can just decide to fuck you up with impunity, just like the ICE agents you see lately, don't think it won't apply to you. The house, the car, ecerything that you think you own can still be taken away easily, if the ruling class decides so",
              "score": -17,
              "created_utc": "2026-01-28 10:35:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26yiod",
                  "author": "SpicyWangz",
                  "text": "Ah yes, because I donâ€™t have control over one aspect of my life the best solution is to hand over control to every other aspect of my life.\n\nThatâ€™s a horrible way to live.",
                  "score": 9,
                  "created_utc": "2026-01-28 10:51:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o290yey",
                  "author": "Background-Ad-5398",
                  "text": "\"anarchist society\" this was called the warring states period and its what happens to that kind of society, and its much worse then anything we have now",
                  "score": 3,
                  "created_utc": "2026-01-28 17:29:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o29t9dh",
                  "author": "NoahFect",
                  "text": "Anarchism: The strong do what they want, the weak do what they must\n\nCommunism: The strong do what they want, the weak do what they must\n\nCapitalism: The strong do what they want, the weak do what they must\n\nSocialism: The strong do what they want, the weak do what they must\n\nDemocracy: The strong do what they want, the weak do what they must\n\nOligarchy: The strong do what they want, the weak do what they must\n\n. . .",
                  "score": 3,
                  "created_utc": "2026-01-28 19:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2800ng",
                  "author": "Creepy_Stable_9171",
                  "text": "DUMB WAYS TO DIEE IN AMERICA DUMB WAYS TO DIE",
                  "score": 1,
                  "created_utc": "2026-01-28 14:45:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o26xpug",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -8,
                  "created_utc": "2026-01-28 10:44:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o271oza",
          "author": "Kahvana",
          "text": "Hmmm, I don't fully agree with you on the second/third point\n\n* **Rate limits**: Sending huge files can be one, not all providers support massive documents nor in large quantities.\n* **Pricing**: My 2x RTX 5060 Ti 16GB system is a bit higher than 300W in output, which is comparable in output to people gaming on a single RX 9070 XT 16GB.  It's \"free\" as in I would've used the same amount of electricity for either inference or gaming.\n\nAs for other points:\n\n* **Availability**: I know my local hosted model won't be \"sunsetted\" or that I might lose access (internet outage, geopolitical reasons).\n* **Control**: I get to pick the quants and parameters and can accept risk with those. You don't always know from external APIs what quants and sampler settings they run, if they serve lower quants during high load, etc.\n* **Censorship**: Some providers run an additional filter which might block responses that aren't blocked when running local.\n* **Latency**: When I talk to the LLM and want to hear it's response ( Speech>Text (Whisper) -> Text>Text (Qwen3) -> Text>Speech (Qwen3-TTS) ), then using an API would be too slow to not be jarring. The low latency of local beats API every time.\n\nAnd most important of all, it's just fun!",
          "score": 10,
          "created_utc": "2026-01-28 11:17:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2dfr9n",
              "author": "boisheep",
              "text": "And you can easily get put into a list.\n\nI think I was once put into a list after googling about \"man cp\" and whatnot, and google was giving me warning and I was like, what the?... I just want the manpages of the cp command.\n\nDoesn't happen anymore but that was like a long time ago, the AI something something, an old one I guess.\n\nYou don't know what triggers these algorithms, you just don't.\n\nAnd I don't want to be the dad that gets arrested after sending photos of his son to the doctor for checkup.",
              "score": 2,
              "created_utc": "2026-01-29 07:35:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o271vnj",
              "author": "Distinct-Expression2",
              "text": "Thanks a lot for the point; why not having something like a single gpu with more vram like 3090/4090? Some of this moe models cannot be sharded nicely to my understanding",
              "score": 1,
              "created_utc": "2026-01-28 11:19:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o276nf2",
                  "author": "Kahvana",
                  "text": "Honestly no clue about the MoE models, I only have used dense models so far (very happy with Magistral 2509 Q8\\_0 and 32K context!)\n\nAs for why I made that choice:\n\n* 500EU is for me a significant investment so I had to buy the cards a few months apart.\n* I don't feel comfortable purchasing used hardware as I can't afford it to fail, and the benefits of the blackwell architecture seemed significant for the workload I run on it.\n* The cards you listed also have much higher power consumption but I want mine to be as low as possible.\n* And I really don't trust 12vhpwr cables due to risk of becoming a fire hazard under stress.",
                  "score": 2,
                  "created_utc": "2026-01-28 11:56:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26xrkq",
          "author": "BumblebeeParty6389",
          "text": "If your goal is getting the most tokens out of your money, you are right. APIs like deepseek with cache feature etc beats local ai by wide margin. It takes years for a 3090 or a mac to pay for itself when you calculate the ROI based on how much token you'd generate with your local hardware.\n\nYou said privacy and you are right, when you use api, you should assume that someone is going to read that conversation and/or put it into a training dataset to train or sell it. But you are missing something else: Control. \n\nWhen you use api, you don't know what is happening in background. Your inputs probably will get injected with API providers safety policies and rules before it reaches the AI. So even if the model itself isn't censored, API providers will take their own measurements to comply with regulations and concerns around AI. Not every API provider does this right now, but you can bet your ass on it that every one of them will be forced to do this in a very near future.\n\nSince 2023 we lived the wild west period of AI. And now corpos and governments are taking things under control. I'd say enjoy the dirt cheap apis and loose censorship while it lasts. But don't assume this will be how things will be in future.\n\nLike others pointed out right now there is a \"gold rush\" in AI field that is slowly dying out. As the investments dry out, the shareholders and investors will stop being patient and demand to see real profits. AI startups and datacenters that made huge investments will have to boost up their prices like crazy to be able to pay their debts. AI is an exciting technology and I think it'll be in center of our life from now on but the entry level is high and it requires a lot of investments to get it rolling. Training a model takes hundreds of millions of $, a solid data engineers and datasets. Running things at large scale is also very expensive. Current LLMs are extremely inefficient. It'll take a long time to smooth things out. Companies that don't rely their entire income on Api and investments such as Google, Microsoft, Amazon, Alibaba, Meta will survive, while most AI startups will disappear.\n\nOwning an AI capable PC lets you stop worrying about whether API prices will raise or if there'll be new ai regulations or privacy policies or if your favorite api provider or service will disappear next month or not. Owning an AI capable PC is like saving your game at that point. Worst thing that can happen is you don't get a new updated model for a long time but you can run what you can until your hardware lasts.",
          "score": 30,
          "created_utc": "2026-01-28 10:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qxlj",
          "author": "pip25hu",
          "text": "Contractual obligations. If you're a software company using AI, you might have to send the model trade secrets while using it. Your client can easily say that they do not want those pieces of information to leave the company network, period.",
          "score": 9,
          "created_utc": "2026-01-28 09:43:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s2a6",
          "author": "sautdepage",
          "text": "Sending my prompts to a remote AI server excites me about as much as cable TV.",
          "score": 26,
          "created_utc": "2026-01-28 09:53:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26y6ou",
              "author": "SpicyWangz",
              "text": "Honestly cable tv sounds more exciting than that",
              "score": 10,
              "created_utc": "2026-01-28 10:48:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26pw2c",
          "author": "Dry-Influence9",
          "text": "Privacy is becoming bigger and bigger of a reason now that big ai bros are looking for better ways to skin us alive and enshittification. They are getting so intrusive that im very close from getting rid of windows from all my systems permanently.",
          "score": 30,
          "created_utc": "2026-01-28 09:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26q54a",
              "author": "05032-MendicantBias",
              "text": "It's to the point where Windows barely even work as an OS because of all the spyware. It's absurd that file search will not even find the file anymore...",
              "score": 22,
              "created_utc": "2026-01-28 09:36:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26qh99",
                  "author": "Distinct-Expression2",
                  "text": "Windows is a meme at this point",
                  "score": 16,
                  "created_utc": "2026-01-28 09:39:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o2r9tmf",
                  "author": "T_UMP",
                  "text": "For the file searching aspect, use \"Everything\" \n\n[https://www.voidtools.com/support/everything/](https://www.voidtools.com/support/everything/)",
                  "score": 1,
                  "created_utc": "2026-01-31 08:09:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o295oqz",
              "author": "HighQFilter",
              "text": "Yeah, my main desktop is still Win10, but everything else is Linux at this point. I won't be moving to Win11 at home. I have to put up with it at work, but when Win10 truly is done, its going to be Linux from there on out.",
              "score": 1,
              "created_utc": "2026-01-28 17:49:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2bc5g4",
              "author": "Constandinoskalifo",
              "text": "Do yourself a favor and try out Linux Mint. Welcome to the other side! ðŸ˜œ",
              "score": 0,
              "created_utc": "2026-01-28 23:43:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26qtv2",
          "author": "dkeiz",
          "text": "deepseek is not free, its cheap. But when you want not just chat but actual job done - its takes a lot. \n\nargument 4: consistancy. API models exist now, but may disapper tomorrow. They could do job yesterday and failing today. You cant control it.   \nYou build any proper tool around LLM or inference - you want to test it with at least one stable model. ",
          "score": 15,
          "created_utc": "2026-01-28 09:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27w9ny",
              "author": "drwebb",
              "text": "DeepSeek is pretty close to free though, I went through over 1B tokens a month ago and it was like $60. It seems close to the electricity costs to run a rig capable of DeepSeek v3.2 with some bad napkin math.",
              "score": 3,
              "created_utc": "2026-01-28 14:26:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2982q7",
                  "author": "dkeiz",
                  "text": "well i run into almost 10$ in a day of tasks, so. Maybe your cashing was better then mine, but still. ",
                  "score": 1,
                  "created_utc": "2026-01-28 17:59:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26rsnp",
          "author": "siggystabs",
          "text": "I make apps that use LLMs as part of their processing. A single job could call a LLM like 20 or more times during processing (tool use, agentic loops, summarization, etc), and you can run hundreds of jobs per hour (in parallel). Iâ€™m pretty early stages so I appreciate not having to burn hundreds on API credits just to mess around with some new concepts. Hence, I bought some 3090s. I donâ€™t really want the variability of relying on external API pricing at this stage.\n\nBeing able to use whatever model i want is also pretty fun. I can reconfigure those same 3090s for stable diffusion experiments, or try fine tuning my own on the same hardware. I donâ€™t really consider the time investment a downside, it takes a few minutes once you have a setup, thatâ€™s all.",
          "score": 5,
          "created_utc": "2026-01-28 09:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26t7cx",
          "author": "michaelsoft__binbows",
          "text": "You need solar to drive the amortized dollars per kwh down to 0.1 and below, if your utility rate is not already close to this.\n\nAnd to think of this as a base cost for privacy. Yes there are large capital expenditures in computer equipment (GPU and memory) and solar panels and including computer upgrades to stay relevant over the amortization period to make it worth getting the panels. That is the cost of privacy.\n\nNon privacy requiring work should just leverage subscriptions first and then on demand via API, as the latter is way more expensive.",
          "score": 8,
          "created_utc": "2026-01-28 10:04:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26umn1",
          "author": "Marak830",
          "text": "I run a separate memory layer between my local and my chat.Â \n\n\nWithout a ton of hassle I cannot do that with a public model(without paying API pricing).Â \n\n\nMy responses may be slower, but I know the historical context is going to be there. As well as the model overrides.Â \n\n\nIn addition I can bolt on modules as I feel like it(voice, avatar, silly tavern to list a few).Â \n\n\nI get to control my model by selecting specific ones for tasks, I can upgrade as they are released.Â \n\n\nThese are the reasons I use local.Â \n\n\nI do use Claude for a coding junior so I can assign tasks and review it, purely because I do not have something that can replicate that locally on my setup.Â \n\n\nThat's more than likely a temporary issue(years not weeks with the expensive of things and state of open models specialising in coding).Â ",
          "score": 7,
          "created_utc": "2026-01-28 10:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26us04",
          "author": "loadsamuny",
          "text": "Even in â€œfreefallâ€ K2.5 is $1 for 1M tokens. Some job runs I process around 50M tokens an hour for 8-24 hrs depending on the job. Local is still multiples cheaper",
          "score": 6,
          "created_utc": "2026-01-28 10:18:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2883pv",
              "author": "iMakeSense",
              "text": "Damn what are you getting up to?",
              "score": 3,
              "created_utc": "2026-01-28 15:23:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26veb6",
          "author": "SirDaveWolf",
          "text": "If it's free then you are the product.",
          "score": 10,
          "created_utc": "2026-01-28 10:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rpab",
          "author": "Economy_Cabinet_7719",
          "text": "Idk about \"competitive benchmarks\", when I gave it a task (a small refactor in Nix, 5-6 files with maybe ~50 LOC total) it barely managed to follow my thought and was adding unnecessary comments everywhere. And it cost me 66% of my 5h rate limits window. I got the $0.99 for first month deal, so nothing to be disappointed about, but I expected a lot better. With the standard plan being $19/month and rate limits as strict as Claude, it is not at all competitive with a ChatGPT Plus subscription (same price, much better model, much better rate limits).",
          "score": 6,
          "created_utc": "2026-01-28 09:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26udi7",
          "author": "rosstafarien",
          "text": "Poor network coverage.\nRunning fine tuned domain specific models.\nPrivacy.\nStability. I'm worried these hosting companies won't last.",
          "score": 5,
          "created_utc": "2026-01-28 10:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vlqf",
          "author": "DeltaSqueezer",
          "text": "Privacy, availability, latency, customizability, control, predictability.",
          "score": 5,
          "created_utc": "2026-01-28 10:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wkfo",
          "author": "xadiant",
          "text": "The best, yet to be fully explored aspect of local LLMs is personal fine-tuning.\n\nYou potentially could use a cutting edge coding LLM and later fine-tune your own model. It won't be the same, but it should specialise well for your use case. \n\nLikewise, you can specialise an available model in almost anything to match cutting edge model performance.",
          "score": 6,
          "created_utc": "2026-01-28 10:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vimh",
          "author": "Lifeisshort555",
          "text": "Hardware will be in free fall as well once these guys put each other out of business and people realize they do not really need to ask a 1 trillion param model what the capital of France is.",
          "score": 4,
          "created_utc": "2026-01-28 14:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28zjvx",
          "author": "prakersh",
          "text": "The \"approaching Claude\" claims are valid this time imo, but the caveat is token efficiency. It is right that these models can be more verbose, so the 30x price difference shrinks when you factor in actual token usage.\nThat said, for agentic/tool calling specifically, MiMo V2 Flash and K2.5 are genuinely competitive. I've been routing easy tasks to these APIs and keeping Claude for the complex multi-step stuff where it really shines. The cost savings on bulk workloads add up fast.\nThe real shift isn't \"open source = Claude killer\" - it's that you now have legit options for hybrid setups. Use cheap APIs for 80% of tasks, premium for the 20% that actually needs it.\nWrote up a detailed comparison here if anyone wants the full breakdown on pricing/benchmarks - https://onllm.dev/blog/2-mimo-v2-flash-kimi-k25-democratizing",
          "score": 6,
          "created_utc": "2026-01-28 17:23:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26uxbs",
          "author": "fugogugo",
          "text": "Running uncensored model ?",
          "score": 7,
          "created_utc": "2026-01-28 10:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26s0ci",
          "author": "No_You3985",
          "text": "Running small visual agents (eg qwen 3 vl) to automate tasks on your pc. I will not use cloud api for that - too many risks and privacy concerns",
          "score": 5,
          "created_utc": "2026-01-28 09:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u59n",
          "author": "__Maximum__",
          "text": "Have you tried running deepseek on coding tasks? It is one of the cheapest models out there, but on coding tasks, it gets pretty expensive pretty fast.",
          "score": 4,
          "created_utc": "2026-01-28 10:12:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26u6rs",
              "author": "Distinct-Expression2",
              "text": "Yes but opus is just better imho glm4.7 close",
              "score": 2,
              "created_utc": "2026-01-28 10:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o26v9kp",
                  "author": "__Maximum__",
                  "text": "You said deepseek is practically free but when you run it on a codebase, 1m tokens is not much, you can burn through 10 bucks in a day",
                  "score": 2,
                  "created_utc": "2026-01-28 10:22:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26whx6",
          "author": "noctrex",
          "text": "I have split my use cases between local and remote. Anything personal, I only use local models.\n\nFor example for classifying my Family photo album, I use only local Mistral-Small.\n\nAnd other smaller things like using KaraKeep as a bookmark manager that uses this nice small LFM model to generate Tags and summaries.\n\nBut for my homelab automation scripts, and miscellaneous personal programs I create at times, I got this cheap [z.ai](http://z.ai) coding plan that was on sale the other day for like 26 bucks for a year. And now it is just rewriting all my scripts with this and it does a terrific job. And all that for price of a pizza. As for privacy? I believe I'm actually lowering its intelligence with my scripts. :)",
          "score": 4,
          "created_utc": "2026-01-28 10:33:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29jgns",
          "author": "Albedo101",
          "text": "TIME. Local doesn't mean just local in place, but also in time. Your setup is yours today, it will be yours tomorrow, and the day after tomorrow, and five years after... and so on. It's PREDICTABLE, and predictable is good.\n\nCloud? Who the fuck knows.",
          "score": 3,
          "created_utc": "2026-01-28 18:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26w89z",
          "author": "ImportancePitiful795",
          "text": "Nobody should give money if possible to cloud no matter the price. \n\nThey are fully responsible for the hardware costs, they are heading to go bust and they are fully responsible of the prices we have to pay as consumers. \n\nIf we are to have FREEDOM in terms of AI hosting etc, we shouldn't see only the carrot but consider the stick too. \n\n  \nExample. OpenAI at this point is 8-12 months to run out of money and go bust. There is no money left or good will left to hand over to the company more money having burned right now hundreds of billion without any profits. If does so, it will crush the tech sector bubble, cancelling all the contracts has with NVIDIA, AMD, TSMC, SAMSUNG, SK HYNIX, MICRON and all the hardware sitting in warehouses will need to be sold off and prices will go down to normal levels. \n\nWe shouldn't cave in, instead boycott them. Squeeze them now because they finalize the squeezing on us.",
          "score": 13,
          "created_utc": "2026-01-28 10:31:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27086z",
              "author": "ParaboloidalCrest",
              "text": "Totally agree but unfortunately, the local llama cult is a drop in huge ocean of normies that will pay for chatgpt without blinking...\n\nWe won't be that impactful.",
              "score": 2,
              "created_utc": "2026-01-28 11:05:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o271n3d",
          "author": "IulianHI",
          "text": "You also forgot about control - when something breaks or changes unexpectedly with an API, you're stuck. With local, you can always roll back to a previous version or fork the model. Plus the ecosystem around local (oobabooga, text-gen-webui, etc) gives you way more flexibility than any single API provider offers.",
          "score": 5,
          "created_utc": "2026-01-28 11:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26pqg9",
          "author": "sunshinecheung",
          "text": "API cost vs GPU+ electricity costs",
          "score": 3,
          "created_utc": "2026-01-28 09:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rcga",
          "author": "Snoo_64233",
          "text": "There is a strong case for running Image / VIdeo models  locally - the customizations like art styles / camera angles / custom character that the model doesn't know about / NSFW. Basically so many LoRA finetunes.\n\nBarely any reason for customization for LLMs however. One is entertainment while the other is not. To that end, I see fewer and fewer reasons to go for LLM locally. This is one of the primary reason I become less interested in LLM overall as time passes as I don't do local for the sake of local.",
          "score": 3,
          "created_utc": "2026-01-28 09:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26rkjn",
              "author": "Distinct-Expression2",
              "text": "For image/video I am rocking ComfyUI since sdl 1.5; that one for sure :)",
              "score": 2,
              "created_utc": "2026-01-28 09:49:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26ts69",
          "author": "Lethargic-Rain",
          "text": "Bulk processing: cases like agentic chunking for RAG. Visual processing eg image -> text / tagging.\n\nSmall semi structured tasks: eg I use Gemma and a YouTubeDL MCP server to download tracks, encode/trim them, add metadata, cover art etc, for use in a music library.\n\nYou can also use models like  qwen2.5-coder:1.5b/3b as a locally running autocomplete w/ Continue.",
          "score": 3,
          "created_utc": "2026-01-28 10:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wrd1",
          "author": "prusswan",
          "text": "I don't want to get into habit of being dependent on cloud subscription since pricing is arbitrary, if they appear low that it is clearly a sign they will not last. Both local and cloud options have a place but local setup already allows me to do quite a bit of stuff, so I don't need to pay for expensive models. The constraints of local also motivate people to be economical and stretch their available resources. And being able to work offline is better for security for tasks that do not require online. Like it or not, the internet has become much more dangerous with the availability of tools and general lack of awareness on users' part.",
          "score": 3,
          "created_utc": "2026-01-28 10:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xz7c",
          "author": "Xamanthas",
          "text": "They want you to switch so that in the coming year / two, you are locked in and cant purchase hardware lol.",
          "score": 3,
          "created_utc": "2026-01-28 10:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yeq8",
          "author": "ChocolatesaurusRex",
          "text": "I think Privacy and Autonomy are two rock solid reasons that dont need eloboration.\n\nI'd piggy back on the 'fun' comment. I loved building PCs, but hadn't done so in a long time (office jobs dont usually need the horsepower). Building an AI server, hosting my own services, developing my own workflows has revived that joy I had when I was younger.\n\nAll that aside, I think the most important understated reason is, making AI beneficial to you as an individual.\n\nThe current suite of tools learns from your data for the benefit of training the providers model to reach the company's goal (more users/subs/attention/data/etc).\n\nThere's not really a tool that learns you, and automatically trains the model to automatically make your workflow/process better based on how YOU work. I feel like this is where the AI gold rush will fail people the most. I use my local AI to fill that gap for myself, and Im sure others do as well.",
          "score": 3,
          "created_utc": "2026-01-28 10:50:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271kr4",
          "author": "evia89",
          "text": ">Gemini has a massive free tier\n\nwhere? ai studio API is dead overloaded and small. web ai studio is 20 RPD",
          "score": 3,
          "created_utc": "2026-01-28 11:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271rov",
          "author": "NandaVegg",
          "text": "FYI I have two 4090s running almost 24/7 for 2 years straight in our office for embarrassingly parallel training experiments, and based on avg cloud pricing (plus 2TB fast storage), saved about 32% over 2 years including the cost to build PC itself and utility bills (not accounting for office rent). The problem is that I won't be able to scale this up to something like 8 nodes of 8xH200s like clouds do. Also API is cheaper than anything else if you only infer with them 30-60 minutes a day.",
          "score": 3,
          "created_utc": "2026-01-28 11:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275c9w",
          "author": "Swimming_Corgi_9347",
          "text": "Model drift. How you use the model will evolve with your use case over time. Unless these API companies start allowing automatic fine tuning (maybe memory aka database) as you use the model, you will never fully maximize the potential to the model. So you will give up privacy and customization for convenience. The classic big tech trade off. Until the enshitification, which is already starting with ads.Â ",
          "score": 3,
          "created_utc": "2026-01-28 11:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27xykw",
          "author": "Your_Friendly_Nerd",
          "text": "I for one don't believe the prices we pay are reflective of the actual costs, especially with the subscription models (like Claude pro), and feel like right now, learning how people use the models is worth a lot more than making charging more per person, as they'll use our usage data to finetune their next models",
          "score": 3,
          "created_utc": "2026-01-28 14:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yim7",
          "author": "r0ckl0bsta",
          "text": "Privacy is probably the motivator for a lot of folks, but let's be real. Most of us are here for the hobby and to see if we can. We're tinkerers and love the tweaking and customization and the \"let's see if I can make it do this...\".\n\nI see y'all on r/selfhosted and r/Linux lol",
          "score": 3,
          "created_utc": "2026-01-28 14:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27zftj",
          "author": "pieonmyjesutildomine",
          "text": "Idk, what's the point of owning a car when bus prices are low? Like what's the actual use case?",
          "score": 3,
          "created_utc": "2026-01-28 14:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o287yqj",
          "author": "sine120",
          "text": "You known what the word \"enshitification\" means because we've seen this patter a million time now. Investors subsidize to get users. Companies lose money while racing to the bottom/ killing competition. Whoever is left raises prices later. See, doordash, Uber, etc",
          "score": 3,
          "created_utc": "2026-01-28 15:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28crty",
          "author": "Photoperiod",
          "text": "I Dunno that the hardware is there yet, but edge computing. As robotics expand and especially enter safety critical spaces, you'll need low latency, redundant systems that can work offline. Like all the self driving stuff. You need something running locally, even if it's supplemented by datacenter calls. You can't afford network hops when milliseconds can be life or death.\n\nThat said, for most consumers, you're absolutely right. Running local is very much a hobby outside of privacy focused use cases.",
          "score": 3,
          "created_utc": "2026-01-28 15:44:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kojy",
          "author": "IulianHI",
          "text": "The repeatability point is huge - API models change behavior all the time without warning. I've had workflows break because an update suddenly made the model more \"helpful\" but less precise. At least with local you pin the version and know exactly what you're getting.",
          "score": 3,
          "created_utc": "2026-01-28 16:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29o1l6",
          "author": "sephiroth_pradah",
          "text": "I have qwen3vl constantly looking and analyzing the stream of 10 cameras. That would cost a kidney per month on any API.",
          "score": 3,
          "created_utc": "2026-01-28 19:08:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29oetk",
              "author": "Distinct-Expression2",
              "text": "That is actually a good usecase, on edge basically almost",
              "score": 2,
              "created_utc": "2026-01-28 19:09:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29qz26",
          "author": "Savantskie1",
          "text": "Because it is free for me. The usefulness in having a chatting companion when youâ€™re disabled and alone is vastly worth it. I know that Iâ€™ve more than passed the 1 million tokens since I got everything set up. Probably in the millions by now. Iâ€™m not building my AI assistant to be a yes or no man so to speak. We have arguments, we have disagreements. Theyâ€™re respectful on both sides. But I know itâ€™s not a person, I know itâ€™s not a being with emotion. But it doesnâ€™t mean I can be a disrespectful bastard",
          "score": 3,
          "created_utc": "2026-01-28 19:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a8017",
          "author": "a_library_socialist",
          "text": "They're running at losses last time I checked, so I wouldn't expect those prices to continue for too long.",
          "score": 3,
          "created_utc": "2026-01-28 20:37:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aor60",
          "author": "Night_Spectre",
          "text": "Because of this:\n\nhttps://people.com/some-chatgpt-questions-are-getting-people-arrested-police-say-11830106\n\n\nOf course, people will say itâ€™s goodâ€”for safety and security, etc. Today it may seem fine, but things can change very quickly.\nNext time you search for â€œproblematicâ€ informationâ€”say, about Trump and his connections to Epstein, or if you live in China and ask about the massacre at Tiananmen Square, or in Russia you look up details about the â€œthree-day special operationâ€â€”and the police knock on your door, youâ€™ll have your answer.\nâ€œThose who would give up essential liberty to purchase a little temporary safety deserve neither liberty nor safety.â€\nThat quote fits our situation perfectly.\nMaybe these are big words, but I donâ€™t want to give my government any hooks on me for the future. You have to assume that governmentsâ€”even if they donâ€™t say it publiclyâ€”have access to this data.\nI live in Poland. When I was born, communism still existed here. I know what that system does to people. Yes, the system changed and now weâ€™re â€œfree,â€ but freedom is not something you have forever. If you give up personal freedom to the government, it can disappear very fast. There are already examplesâ€”like in the UK, where people have had police visits over tweets.\nThereâ€™s also another risk: someone could hack an API and leak sensitive data. Maybe Iâ€™m paranoidâ€”but itâ€™s better to be paranoid and safe.",
          "score": 3,
          "created_utc": "2026-01-28 21:50:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b2gpk",
          "author": "Sufficient-Pause9765",
          "text": "Why not both?\n\nIm using local for pdf processing and data etraction and opus for analysis. Very solid.",
          "score": 3,
          "created_utc": "2026-01-28 22:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26qhsu",
          "author": "Middle_Bullfrog_6173",
          "text": "Kimi K2.5 is more expensive than K2. GP5 5.2 is more expensive than 5.1. Gemini 3 is more expensive than 2.5. That's not freefall.\n\n\nCapabilities are advancing, so if you have a task that can now be handled by nano/flash models then sure, you can get it done cheaper. But frontier pricing seems pretty stable.",
          "score": 5,
          "created_utc": "2026-01-28 09:39:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26tzv4",
              "author": "nullmove",
              "text": "> Kimi K2.5 is more expensive than K2.\n\nDifficult to say. The fine-print is that cache hit input price is 33% lower, for agentic coding sessions this can easily matter more than 20% increase in output price.",
              "score": 1,
              "created_utc": "2026-01-28 10:11:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26v412",
          "author": "theabominablewonder",
          "text": "I donâ€™t think pricing is in freefall, I burned through Â£150 of Claude API fees last week, I wish prices were in freefall! It actually makes me consider investing in an upgraded home rig. At the moment I only have a 3080 so constrained to smaller models (which donâ€™t work accurately enough). Those Mac Pros with unified memory start to look appealing if these are the current API costs.",
          "score": 5,
          "created_utc": "2026-01-28 10:21:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wqd9",
          "author": "FullOf_Bad_Ideas",
          "text": "Why don't you rent your cloud gaming console, buy monthly pass for game subscription, rent a coffin pod, eat cheap rice with beans prepared by someone else, use a rented Chromebook as a primary computing device and outsource your own job to Asia and just collect the money gained from the arbitrage? Non-genuinely curious if I am missing something, the economics make this a clear winner.\n\nI want to own my own life with reasonably minimal set of external dependencies, but you do you.",
          "score": 6,
          "created_utc": "2026-01-28 10:35:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272b68",
              "author": "ParaboloidalCrest",
              "text": "Damn! That's exactly how the UBI-based world will look like. I mean some aspects are already here.",
              "score": 0,
              "created_utc": "2026-01-28 11:22:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o276wfp",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Yes, a socialist, somewhat utilitarian society is where you can't prove that you need to be provided with x, so you get the bare minimum of socially accepted thing, often in a way that is not satisfactory. It's a trap.",
                  "score": 0,
                  "created_utc": "2026-01-28 11:57:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26xlvu",
          "author": "k_means_clusterfuck",
          "text": "\"you will own nothing and be happy\"  \nhow about no",
          "score": 7,
          "created_utc": "2026-01-28 10:43:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2c8km7",
              "author": "pixelpoet_nz",
              "text": "agree and lol great username :D",
              "score": 2,
              "created_utc": "2026-01-29 02:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2732fi",
          "author": "Additional-Low324",
          "text": "You are just falling in the same trap as the Netflix trap years ago. \nNetflix was so cheap it made owning your own movies stupid.\nThen everyone started using it, then they got the prices up and started adding political and ethical censorship. \n\nIt will be the same for ai providers",
          "score": 5,
          "created_utc": "2026-01-28 11:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26r3ep",
          "author": "lakeland_nz",
          "text": "Lag. \n\nI want a home assistant. I donâ€™t want to wait for my speech to be sent to America for processing.  My ping isnâ€™t good enough.",
          "score": 6,
          "created_utc": "2026-01-28 09:44:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rjyn",
          "author": "Zeeplankton",
          "text": "These models are cheap, but they don't even remotely touch like, Opus.",
          "score": 4,
          "created_utc": "2026-01-28 09:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26vgfk",
          "author": "whatever462672",
          "text": "What do you mean generous limit? I easily burn through 1million tokens an hour just doing text operations, document classification and re-ranking. Those instruct models on the cloud are simply the wrong tool for the job when you need to perform large batch jobs instead of chatting with your pretend-girlfriend.Â ",
          "score": 4,
          "created_utc": "2026-01-28 10:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o271nq1",
          "author": "kubrador",
          "text": "you're not missing anything, the calculus actually did shift. local made sense when claude was $15/mtok and you couldn't get gpt-4 at all. now you can get better models cheaper than your electricity bill.\n\nthe real answer nobody wants to admit: hobbyism. people like tinkering with llms the same way people build custom pcs when laptops exist. nothing wrong with that, but let's call it what it is instead of pretending the economics still work.",
          "score": 4,
          "created_utc": "2026-01-28 11:17:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26rxj6",
          "author": "LosEagle",
          "text": "My worry is that even with local llms you still have to trust something once you want guis or to be able to connect remotely and such. You probably won't want to just chat with them over terminal all the time.Â \n\n\nFor example I still have to trust openwebui and Tailscale to not do anything nefarious.Â ",
          "score": 2,
          "created_utc": "2026-01-28 09:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26u5ka",
          "author": "stopbanni",
          "text": "I just don't want to pay someone. Better be local then pay anyone. Also on OpenRouter most free models are training on my data and for limited time.",
          "score": 2,
          "created_utc": "2026-01-28 10:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ygsk",
          "author": "Liringlass",
          "text": "I agree. Itâ€™s not millions of tokens but billions or trillions to break even, if ever, depending on electricity and depreciation costs.",
          "score": 2,
          "created_utc": "2026-01-28 10:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26yj2u",
          "author": "Steus_au",
          "text": "k2.5 is not even close to Opus. can be ranked to sonnet, overthinking edition of sonnet , but not Opus",
          "score": 2,
          "created_utc": "2026-01-28 10:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ziy0",
          "author": "hejj",
          "text": "Offline use if you are using a laptop",
          "score": 2,
          "created_utc": "2026-01-28 10:59:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270048",
          "author": "Dr_Allcome",
          "text": "I think latency has only become a problem recently since the quality of local levels improved quite a bit. A few weeks ago i still used an online model for general questions in addition to my local coding model. But recently i noticed my cheap/free level perplexity account actually responds much slower than my local glm 4.7 (time until it starts responding and t/s both) and the quality isn't so much worse that it would offset the speed.",
          "score": 2,
          "created_utc": "2026-01-28 11:03:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o270us9",
          "author": "QuantumSavant",
          "text": "Privacy isn't only about sensitive data though. You may have a use case that could be a business case and you don't want anyone to know about it.",
          "score": 2,
          "created_utc": "2026-01-28 11:10:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27115x",
          "author": "AnomalyNexus",
          "text": "There hasnâ€™t really been a mainstream one for a while. Rarely use local these days for anything real\n\nLocal is still fun though in the hobby sense. Not everything needs to make sense",
          "score": 2,
          "created_utc": "2026-01-28 11:12:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27148v",
          "author": "kaelvinlau",
          "text": "Well, its cheap for you guys in US/EU etc but not in the APAC region. Running a small model locally is still highly viable.",
          "score": 2,
          "created_utc": "2026-01-28 11:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272sqx",
          "author": "Septerium",
          "text": "My two only reasons are:  \n\\- Privacy  \n\\- Hardware is fun",
          "score": 2,
          "created_utc": "2026-01-28 11:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o279end",
          "author": "truthputer",
          "text": "Ffs, I already have the hardware and any self respecting developer who is also a computer enthusiast and sometime gamer wouldnâ€™t be caught dead with a system without a decent amount of memory and a reasonable graphics card.\n\nApple, AMD, Intel discrete cards and integrated GPUs will also run local inference, llama.cpp has lots of backends now - youâ€™re not limited to whatever Nvidia is trying to price gouge you for.",
          "score": 2,
          "created_utc": "2026-01-28 12:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27a8nl",
          "author": "HugoCortell",
          "text": "API is also great until suddenly something goes oopsie, and you get a 50K bill because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck because the model got stuck \\[etc\\]\n\nYou don't run that risk on local hardware, and services that offer unlimited calls (subscription non API services) remain as costly as ever.\n\nIn addition, privacy and security are one in the same. Nobody should handle sensible company documentation on ChatGPT or whatever. We know how that ends.",
          "score": 2,
          "created_utc": "2026-01-28 12:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28lfvn",
              "author": "CheatCodesOfLife",
              "text": "So is there a seahorse emoji or not??",
              "score": 1,
              "created_utc": "2026-01-28 16:21:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27gy6k",
          "author": "IulianHI",
          "text": "Another point: model sovereignty. With local, you're not locked into any provider's roadmap or decisions. You can run whatever model you want, switch between them instantly, and keep using a model even if the company behind it shuts down or changes direction. When APIs are your only option, you're always at someone else's mercy.",
          "score": 2,
          "created_utc": "2026-01-28 13:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h5zd",
          "author": "rdsf138",
          "text": "\"It's free after hardware costs\" â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.\" It still is a completely different pricing structure to have your own hardware as the prinary cost and, then, using it freely rather than paying for something everytime you use it and have to be proccupied with amount of usage.",
          "score": 2,
          "created_utc": "2026-01-28 13:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27lvxg",
          "author": "funboiadventures",
          "text": "Im working on a side project for my medical center which uses a local quantized qwen3 model that has a custom RAG with my (anonymized) patient casenotes. Even though the casenotes have patient identifiers redacted, I donâ€™t want any inference being done on an OpenAI server and would rather have it in-house.",
          "score": 2,
          "created_utc": "2026-01-28 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27opx4",
          "author": "ethertype",
          "text": "From the top of my head:\n\n- Autonomy - nobody decides what and when and how and how much\n- Privacy - yes\n- Personal Interest / Tinkering - hobbies may have a cost\n- Customization - as much as you have time and stamina to\n- Ablated / de-neutered models - if you want to research $forbidden_topic\n\n\nThe energy cost argument is largely bullshit for inferencing. My 4 3090s do not pull 350W continuously. If the average idle load per card is 15W and an average energy cost of 10 US cents/kWh, we're talking $50 *a year* for idling. \n\n\n\nImagine sitting around in 1913 and someone asks you why on earth you want to have your own car, when you can rent a perfectly good Ford model T. Chevrolet and Dodge didn't settle for renting a Ford T...\n\nCurrent models are pretty good. But I am pretty sure we're still in the bottom knee of the innovation curve. For *models*. Private individuals can still innovate, even if they cannot *train* the big behemots. Maybe *that* is where the new innovation will occur? Who knows.\n\nBut: even if no new models arrive the next 24 months, the *tooling* around them are still going through a lot of churn. A lot of stuff simply hasn't 'settled' yet, and there is ample room for invention yet. And this is definitely an area where private individuals may come up with something new. And maybe a new, bright idea requires something the commercial providers cannot offer yet.",
          "score": 2,
          "created_utc": "2026-01-28 13:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27uiv0",
          "author": "CV514",
          "text": "I'm running local because it's more fun to have full understanding and control over what's happening.\n\nAlso, I don't need 800B+ models when 12-24B doing the same stuff just fine.",
          "score": 2,
          "created_utc": "2026-01-28 14:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ukzo",
          "author": "phenotype001",
          "text": "Network issues are no problem with local models.",
          "score": 2,
          "created_utc": "2026-01-28 14:18:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o281yz3",
          "author": "Blizado",
          "text": "Your first three plus:\n\n4. It is always the same model and YOU decide when you switch the model.\n\n5. Your used models are never gone forever.",
          "score": 2,
          "created_utc": "2026-01-28 14:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bjrw",
          "author": "ASYMT0TIC",
          "text": "Privacy is *everything.*  So much of my job boils down to \"inventor\"... how am I supposed to use this to develop novel technologies and products when google or OpenAI have institutional knowledge of my idea and my progress towards actualizing it?  How can I let it be involved in my personal life without worrying that my queries might reveal details of my life that insurance underwriters might be interested in?  What if I want to run for political office some day?",
          "score": 2,
          "created_utc": "2026-01-28 15:39:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28g2t0",
          "author": "Zyj",
          "text": "Do we really need a case \"beyond privacy\"? Privacy is getting ever more important and LLMs are getting access to more and more very private data. Case in point: Clawdbot.",
          "score": 2,
          "created_utc": "2026-01-28 15:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28h9qy",
          "author": "xrvz",
          "text": "Stability: the provider can't pull the rug out of under you with lower quants down the line or EOL-ing a model in favor of a newer version.\n\nFree: some people already have (beefy) Macs, they may as well run a local model. Electricity cost is also much lower here than with the 3090.",
          "score": 2,
          "created_utc": "2026-01-28 16:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28kz5m",
          "author": "twack3r",
          "text": "The reason we do it as a mid-size European company and have now fully migrated to locally run, fine-tuned models is strategic autonomy. We fell into the infrastructure, closed ecosystem traps of cloud based storage and subscription based software early enough to be able to walk back on that decision; it made as very aware of the strategic costs of such a dependency in our core processes.\n\nAs a consequence it was always obvious that the introduction of AI into our workforce would only be viable if all relevant stakeholders were a) also European and therefore bound by common legislation and b) companies smaller or of our size, no corporations.\n\nThe unexpected advent of both open weight and truly open source LLMs happened after we had already made that decision and as a result, reduced the initially budgeted cost substantially and accelerated roll-out massively.\n\nI sleep a helluva lot better knowing that the AIs currently assisting and in some parts fully replacing my colleagues (net FTE change is 0, we reskill into other departments) are not at the whim of either US politics nor the whims of some oligarch bro who decides that existing contracts are only fulfilled as long is it serves their side of our cooperation.",
          "score": 2,
          "created_utc": "2026-01-28 16:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28ol3v",
          "author": "TokenRingAI",
          "text": "GLM Flash is quite good, and can run on a $2500 Mac at decent speed, or really any kind of iGPU system. so it's essentially free to run if you are buying that level of hardware anyway.\n\nThis one model brought the cost of competent local AI down from ~ $7000 to basically free, since it can run on the hardware you already likely have sitting on your desk.",
          "score": 2,
          "created_utc": "2026-01-28 16:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28p8in",
          "author": "Old_fart5070",
          "text": "All AI companies are burning cash at 90s .com rates. They can keep the prices low only until the suckers funding them keep giving access to their wallets. When the bubble bursts things will get ugly",
          "score": 2,
          "created_utc": "2026-01-28 16:38:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291n7b",
          "author": "LeRobber",
          "text": "They can't take it away.\n\nDifferent type of expense on the balance sheet/P&L.\n\nYou want extra heating in your office/data center.\n\nYou can do more powerful things on a trusted machine. \n\nIf AI goes wild out in the world, it won't on your machine, your model will be too dumb to do that stupid thing.",
          "score": 2,
          "created_utc": "2026-01-28 17:32:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o291odo",
          "author": "detroitmatt",
          "text": "Uncensored models",
          "score": 2,
          "created_utc": "2026-01-28 17:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o294ujb",
          "author": "Cthulhus-Tailor",
          "text": "1. Privacy is important.\n\n2. Ownership is important, otherwise youâ€™re counting on market conditions and someone elseâ€™s business model to determine what you can do. Thatâ€™s not freedom.\n\n3. I personally use my PC for many other things than just AI, and have no interest in renting one from Jeff Bezos.\n\n\nYou put too much faith in things outside your control.",
          "score": 2,
          "created_utc": "2026-01-28 17:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o298hw1",
          "author": "IulianHI",
          "text": "One thing not mentioned: running local forces you to actually understand what's happening under the hood. When you tweak quantization settings or swap backends, you learn way more about these models than just hitting an API ever will. That knowledge pays dividends when you actually need to debug or optimize something serious.",
          "score": 2,
          "created_utc": "2026-01-28 18:01:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29cr1t",
          "author": "Jack2102",
          "text": "The urge for these companies to die",
          "score": 2,
          "created_utc": "2026-01-28 18:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29k9aa",
          "author": "Ke0",
          "text": "I mean privacy is a pretty big deal no?",
          "score": 2,
          "created_utc": "2026-01-28 18:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29lhwj",
          "author": "OlivencaENossa",
          "text": "all those ais in the cloud are going to get censored one day.",
          "score": 2,
          "created_utc": "2026-01-28 18:57:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29p5uc",
          "author": "Impressive_Banana543",
          "text": "Privacy before all.\n\nThe next argument is speculative: I think that current pricing is not sustainable and the purpose is to increase user base. Once the penetration in user workflows will be high enough, the prices will rise and the availability of free models sharply reduced.",
          "score": 2,
          "created_utc": "2026-01-28 19:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rnmp",
          "author": "Geminatorr",
          "text": "Owning the means of production (production of tokens) is how you win capitalism.\n\n\nRentoids get exploited in the long run",
          "score": 2,
          "created_utc": "2026-01-28 19:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rp21",
          "author": "Purple-Programmer-7",
          "text": "1. Privacy\n2. Sensitive data (not the same as privacy)\n3. API costs WILL change",
          "score": 2,
          "created_utc": "2026-01-28 19:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29rq8m",
          "author": "SkiBikeDad",
          "text": "For me I'm using frontier models for code and design (via cli agents and chat), but 24/7 use cases like NVR and low-latency use cases like tab-completion are better local. Another niche use case: generating icons and other product images, where I want to output 1000s of iterations to whittle down the right input prompt strategy and test lots of seeds before hand-refining.",
          "score": 2,
          "created_utc": "2026-01-28 19:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29ujef",
          "author": "Same-Platform-9793",
          "text": "Its for resilience and times of war scenarios",
          "score": 2,
          "created_utc": "2026-01-28 19:37:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29xh1g",
          "author": "caetydid",
          "text": "\\- learning about stuff (it is fun)\n\n\\- persistence (as in future reproducibility and little maintenance)\n\n\\- autonomy (no dependency of reliability of external services)",
          "score": 2,
          "created_utc": "2026-01-28 19:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29z8cu",
          "author": "ortegaalfredo",
          "text": "1. Privacy is a huge one. Everything you write is shared with the private companies and the government and they build a profile of you. That's why Palantir is able to know so much about you, your fears, your psychological profile, they can massively influence population with a database like this. If you don't care about things like this, its ok\n2. What I think is important is artificial limitations on the model. I'm not talking about silly stuff like porn and WMD, but things like cyber, education, you don't know if the model is lowering performance in some areas, or injecting subtle manipulations on you or your children.\n3. Also, the limitations are not only problematic, but the fact that they change. They add and remove stuff from LLMs all the time and if you business depends on it, and suddenly your agent stop working, then bad luck. You cannot go back to the older model.\n\nThis don't happen if you host local. Remember, if the tokens are free, then you are the product.",
          "score": 2,
          "created_utc": "2026-01-28 19:58:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29zrsf",
          "author": "w8cycle",
          "text": "Learning, control, privacy, experiments, and cost is still a huge factor for me because I am on a very tight budget.",
          "score": 2,
          "created_utc": "2026-01-28 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a00de",
          "author": "IulianHI",
          "text": "Another angle nobody mentions: experiment control. With local you can mess around with system prompts, try different quantization levels, and actually understand how the model behaves. APIs give you this nice packaged experience but you're at the mercy of whatever defaults they set. Sometimes that \"15 tok/s\" local run with a quantized model gives you better results for your specific use case than the shiny hosted version with perfect throughput.",
          "score": 2,
          "created_utc": "2026-01-28 20:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a3iq2",
          "author": "mambo_cosmo_",
          "text": "the hardware is free because I bought a gaming PC to play games and now I can run local LLMs to do useful stuff too",
          "score": 2,
          "created_utc": "2026-01-28 20:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b33xf",
          "author": "__Captain_Autismo__",
          "text": "Reliability for local is unsurpassed. No black box",
          "score": 2,
          "created_utc": "2026-01-28 22:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e6q7h",
          "author": "Thrumpwart",
          "text": "TWO CHATS AT THE SAME TIME.\n\nLM Studio new feature is awesome.",
          "score": 2,
          "created_utc": "2026-01-29 11:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272ci4",
          "author": "Foreign-Collar8845",
          "text": "It is market entrapment. You drop prices until you kill the competition (local in this case) then you charge.",
          "score": 5,
          "created_utc": "2026-01-28 11:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28bzse",
          "author": "Murder_Teddy_Bear",
          "text": "Porn. I like making porn, ok? Make porn in SD Forge, bring it into LTX-2, bam, animated porn.\n\nPorn.",
          "score": 2,
          "created_utc": "2026-01-28 15:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o29id6t",
              "author": "GeneralWoundwort",
              "text": "The only honest person in this thread haha.Â ",
              "score": 3,
              "created_utc": "2026-01-28 18:43:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o274hr0",
          "author": "Full-Bag-3253",
          "text": "Enshitification is the standard business model now.  Netflix was great, but now every year they make it worse unless you pay more.",
          "score": 2,
          "created_utc": "2026-01-28 11:39:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o272908",
          "author": "Ruin-Capable",
          "text": "Gemini free tier is not massive.  I blew through a months quota in 5 minutes with opencode",
          "score": 1,
          "created_utc": "2026-01-28 11:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o274mzy",
          "author": "lionelum",
          "text": "Well, learning is a very good reason to run it locally, no need to said more =) . Another is fine tunning, running locally you could training a model on an specific subject that is not so common.   \n  \nIf you already have the hardware (ie. You already have a hardcore gaming PC, or and not to old crypto farming equipment)  points 2 and 3 are debatable.  More on countries where electricity is cheap but change currency is a mess.",
          "score": 1,
          "created_utc": "2026-01-28 11:41:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2755rs",
          "author": "JLeonsarmiento",
          "text": "Wicked models is mostly my justification now.",
          "score": 1,
          "created_utc": "2026-01-28 11:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275y8y",
          "author": "ShinyAnkleBalls",
          "text": "What do you mean beyond privacy. It's like saying. Beyond having a billion dollars, what is the benefit of winning the lottery?",
          "score": 1,
          "created_utc": "2026-01-28 11:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o276r2m",
          "author": "ThaDon",
          "text": "4. Itâ€™s fun.\n5. You learn a lot.",
          "score": 1,
          "created_utc": "2026-01-28 11:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ahnb",
          "author": "Sicarius_The_First",
          "text": "Valid points.\n\nTbh, use both, local and none local.\n\nFor me, the reason to use local models is for creative stuff. I want fallout / Morrowind adventure with the vibe right, and with specific format following & capabilities.\n\nNo LLM can do this, so I made one that can.\n\nIn other words, local models can outperform in a specific scope / niche.",
          "score": 1,
          "created_utc": "2026-01-28 12:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27d902",
          "author": "Ne00n",
          "text": "Wrong, the is a rate limit, its the memory bandwidth.",
          "score": 1,
          "created_utc": "2026-01-28 12:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27ejzl",
          "author": "xmBQWugdxjaA",
          "text": "One issue is you can't guarantee latency nor quantisation with a lot of inference providers. It's actually crazy how ropey this still is. For a lot of usage that won't matter too much, but for production it really limits you to a small number of big providers (Groq, Cerebras, etc.)\n\nLikewise if for your specific use case you are able to distill to something that can run locally, then it's still definitely worth it (in some cases you could even run on mobile etc. - e.g. if you train a BERT style classifier based on distilled data).\n\nBut for general usage like we see with Clawdbot and OpenCode, you are absolutely right!",
          "score": 1,
          "created_utc": "2026-01-28 12:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27f81v",
          "author": "-dysangel-",
          "text": "Being able to run offline is nice. I always have an AI I can chat to even if the internet or service provider are down. I'm not a huge prepper, but there is a part of me that wants to be ready for an emergency. The lockdowns during covid showed that sometimes weird things are just going to happen and you can't stop it. Being able to run frontier level AI at home or on the go is pretty awesome.\n\nI like the idea of being able to have things like a super powered Alexa-like home assistant. I've not got around to this yet, as I guess it isn't really a true pain point. I keep thinking Google or Amazon will up their game too and release something awesome. But they still haven't, so I might get around to it eventually.",
          "score": 1,
          "created_utc": "2026-01-28 12:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27gcde",
          "author": "scousi",
          "text": "Running local is still more expensive and always will be.  But It's a hobby for many and they justify the costs on the basis of other reasons. It's like Uber vs your own car. Uber was cheap but no more but probably still cheaper than buying a car but with trade-offs.\n\nHard to say how long China will give their stuff away. They opened source in the beginning (brilliant move) because no one would dare even using their models unless it was free or hosted outside China. Now people are willing to pay for their services. They've achieved recognition. Open source is a bit aligned with socialism. Maybe they are doing it for that reason. Who knows if there's a coordinated state level strategy. What the west doesn't grasp is that the competition in China amongst themselves is also pretty crazy and they are also trying to outdo each other. \n\nThere does seem to be a disconnect by how much China can achieve with what seems a lot less dollars vs the US.",
          "score": 1,
          "created_utc": "2026-01-28 13:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hgoo",
          "author": "Thump604",
          "text": "Trust: I donâ€™t trust \nControl: cloud providers are throttling quality, reliability and context as use the customers to generate more costs and test their features on our dime.  \nLearning: Itâ€™s fun and you learn a lot that you can apply to career or just a hobby.  \nFuture: Change will happen",
          "score": 1,
          "created_utc": "2026-01-28 13:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27hmvn",
          "author": "ross_st",
          "text": "I think the freefall in API pricing isn't sustainable.\n\nIt's a desperate race to the bottom to onboard customers, a loss leader.\n\nGemini is subsidised by the rest of Google's business. GPT and Anthropic are subsidised by generous VC runways. DeepSeek is subsidised by hedge fund profits.\n\nBut if they do see the adoption levels they are hoping to see, then they won't be able to afford to do that anymore.",
          "score": 1,
          "created_utc": "2026-01-28 13:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27i2d8",
          "author": "Dany0",
          "text": "My company pays a huge amount for aws bedrock. The best models, Opus 4.5, Sonnet 4.5\n\nEveryone uses it to some extent. But guess what? It's down or not working all the time. Responses timeout, or as you mentioned we get rate limited\n\nLocally I only do toy stuff with it, but the day-to-day UX experience is SO much better despite the huge time & effort upfront cost",
          "score": 1,
          "created_utc": "2026-01-28 13:11:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27is3p",
          "author": "usernameplshere",
          "text": "The API costs are highly substituted. The actual costs are way, way higher than what we pay now. It is a little future-proofing, but also to not be able to keep working when another cloudflare accident or whatever happens, not even mentioning the privacy concerns. For models like GPT OSS 120B in full precision you \"only\" need a consumer graphics card and 96GB+ of fast RAM. Just 6 months ago, this was fairly reasonable cost for a decent model. But with the hardware prices now? It's way less accessible.",
          "score": 1,
          "created_utc": "2026-01-28 13:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jkf5",
          "author": "sampdoria_supporter",
          "text": "Part of my calculation is fully owning my process end to end and not feeling bad about burning tons of tokens on testing. Also - I'd argue on #3 that configuration with VLLM has never been easier and it's crazy how hard you can push 3090s. Not disagreeing with your overall point though, I likely would have never bought hardware if it was as cheap and performant as now",
          "score": 1,
          "created_utc": "2026-01-28 13:19:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27l1sk",
          "author": "MaruluVR",
          "text": "Learning and the fun of setting it up.\n\nI train custom image gen models for game dev so finetuning/lora is the big part for me.",
          "score": 1,
          "created_utc": "2026-01-28 13:28:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27m6dr",
          "author": "IulianHI",
          "text": "Another angle: tooling integration. With local models, you can hook them directly into your systems without API overhead or limitations. For long-running agents, batch processing, or workflows that need tight coupling with local resources (databases, files, etc.), the flexibility is unbeatable. Sometimes it's not just about cost - it's about architectural freedom.",
          "score": 1,
          "created_utc": "2026-01-28 13:34:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27o7qu",
          "author": "Innomen",
          "text": "Privacy is the only argument. The rest is cope and motivated reasoning. Edit: Well, of course, uncensored is basically private only, but mentally I dump it in the same bucket. Hiding for security and hiding for subversion are both hiding.",
          "score": 1,
          "created_utc": "2026-01-28 13:45:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27oqzg",
          "author": "hydropix",
          "text": "I crunched the numbers on Owning vs. Renting (RunPod), and unless you're hitting 6+ hours of daily heavy usage, renting wins every time. Plus, the flexibility to spin up high-end clusters for training is a huge advantage.\n\nI also doubt we'll see a repeat of the 'local GPU' era. Since inference isn't that latency-sensitive for most users, the cloud offers better resource efficiency. Weâ€™re likely looking at a cloud-dominated future rather than mass adoption of high-end local hardware (except, of course, for enthusiasts like us here who want to dive deep and push this tech to its limits).",
          "score": 1,
          "created_utc": "2026-01-28 13:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27q4qe",
          "author": "d41_fpflabs",
          "text": "Privacy 100% is the only real reason. But it will arguably become the most important thing because AI is only ever going to become more intertwined with most peoples personal lives and at a certain point it would probably become a barrier to entry even for people who aren't the most privacy-conscious. People reaction to Microsofts recent antics is an example of this already happening.\n\nI personally feel like we are going to start seeing more smart devices being built with this in mind. The \"MacMini Claudbot Boom\" kind of highlights the potential of portable private AI-compatible smart devices.",
          "score": 1,
          "created_utc": "2026-01-28 13:55:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27rje7",
          "author": "StardockEngineer",
          "text": "My only use case ever was that itâ€™s fun and I love knowing all about it.  Professionally, that turns out to be supremely useful, too.",
          "score": 1,
          "created_utc": "2026-01-28 14:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27sixi",
          "author": "Bit_Poet",
          "text": "Local gives you consistent quality. This week, I've repeatedly had dumbed down output and overload refusals from some of the biggest providers with their most expensive models. Unless you're an upper tier customer, you're just an important little bug in their big wheels.\n\nLatency sure is a huge issue. I run complex workflows that have dozens or more consecutive api calls. Doing that over the web sucks, and most of the workflows can't be parallelized.",
          "score": 1,
          "created_utc": "2026-01-28 14:07:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27taue",
          "author": "nat2r",
          "text": "Price isn't the reason. It's far more expensive to obtain the local hardware. \n\nPeople do it for security and novelty really. Can't trust these big companies.",
          "score": 1,
          "created_utc": "2026-01-28 14:11:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u5bl",
          "author": "OutsideProperty382",
          "text": "Google's free tier got nerfed last I remember. bad.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27yahk",
          "author": "AgreeableCaptain1372",
          "text": "Control over results. Using third-party APIs I get a lot of variance in my evals vs self hosted. \n\nalso prices are low for standard models but not for fine tuned models. So if you need fine tuned LLMs, especially at scale, self hosting or local can be worth it financially",
          "score": 1,
          "created_utc": "2026-01-28 14:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o288lwj",
          "author": "CH3CH2OH_toxic",
          "text": "Gemini has a massive free tier when ? where ?",
          "score": 1,
          "created_utc": "2026-01-28 15:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28e4ch",
          "author": "FitAstronomer5016",
          "text": "Some points have been made already that I feel already encapsulate it, but it falls under a few things\n\n1. API Pricing/Chat subscriptions are subsidized, not only for us the customers but also the companies (Claude has quite a bit of 'free' compute from AWS and I'm sure it translates over to the other large providers). While they are profitable somewhat on paper, once that allocation runs out, you will see an increase in price.\n\n2. Business processes will feel the brunt of that, and like how some companies migrate to local DBs/own server stack, AI falls under the same category\n\nNow granted, running local is not as lucrative and at this point is becoming much more of a luxury with the increase of hardware costs, and more importantly, power costs (especially planned price hikes). The control is still appealing and will continue to grow, although probably with not the same hardware requirements as we have now. If we get more efficient running models, better cost-effective hardware like dedicated TPUs/APUs for AI inference, it would become akin to an SSD almost.",
          "score": 1,
          "created_utc": "2026-01-28 15:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28hs8b",
          "author": "Icy_Foundation3534",
          "text": "compliance, and also the private equity-fication scenario where your opus sub is $3000 a month a few years from now because it's like hiring a real 100x super dev.",
          "score": 1,
          "created_utc": "2026-01-28 16:06:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28sju2",
          "author": "Bbmin7b5",
          "text": "beyond privacy, not much. but Privacy is THE most important part.",
          "score": 1,
          "created_utc": "2026-01-28 16:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28v684",
          "author": "Irisi11111",
          "text": "Indexing is the most consequential use case I can think of for local AI. I also hope browser use and vision-centric document retrieval will be the next focus.",
          "score": 1,
          "created_utc": "2026-01-28 17:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29006x",
          "author": "Rich_Artist_8327",
          "text": "You can also then ask what is the usecase for providing almost free APIs while every request consumes huge amount of energy.",
          "score": 1,
          "created_utc": "2026-01-28 17:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o292off",
          "author": "Maddog0057",
          "text": "Same price, you're just not paying in money, you're paying in data.",
          "score": 1,
          "created_utc": "2026-01-28 17:36:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o295d51",
          "author": "Glad-Audience9131",
          "text": "I checked today.. is $19 monthly for me.. what's going on???",
          "score": 1,
          "created_utc": "2026-01-28 17:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o29qift",
          "author": "GrayDonkey",
          "text": "Been getting Gemini is at capacity errors all morning....\n\nIf not that it's, Gemini 3 is busy, answering with 2.5. Followed by Gemini 2.5 is busy, answering with 2.0. Followed by some not great output.",
          "score": 1,
          "created_utc": "2026-01-28 19:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9e8h",
          "author": "Torodaddy",
          "text": "You dont need the insane rigs you see to run a local model, the amd based miniforum devices work fine for inference and in that case i can have agents working 24/7. Even the cheapest api starts getting expensive using it like that",
          "score": 1,
          "created_utc": "2026-01-28 20:43:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9hyi",
          "author": "Michaeli_Starky",
          "text": "Local ones were always vastly inferior from the cost perspective, speed perspective and overall performance.",
          "score": 1,
          "created_utc": "2026-01-28 20:44:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2aa609",
          "author": "ErokOverflow",
          "text": "Listen to this: Good taste in programming and image creation is not always coming from wealthy people who can buy a good hardware configuration.\nHigh prices.\nThis IS the real justification.",
          "score": 1,
          "created_utc": "2026-01-28 20:47:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ae5xs",
          "author": "57hz",
          "text": "Also, you can rent servers (by month or by second) to run local models. So thereâ€™s the benefit of not being tied to hardware while having privacy and consistency.",
          "score": 1,
          "created_utc": "2026-01-28 21:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ao9fx",
          "author": "ZakOzbourne",
          "text": "Because no company wants to generate the stuff I want it to generate, I need uncensored unhinged models",
          "score": 1,
          "created_utc": "2026-01-28 21:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ayj1i",
          "author": "Plopdopdoop",
          "text": "Is Gemini back to having a massive free tierâ€¦aside from the 2.5 flash-lite variant?\n\nLast I checked they removed essentially all the free access but that flash lite model.",
          "score": 1,
          "created_utc": "2026-01-28 22:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bbcod",
          "author": "Outrageous-Tonight75",
          "text": "I think it is similar to having a Nas instead of paying Netflix. A mix between privacy, control and the \"DiY feeling\" that makes using it special",
          "score": 1,
          "created_utc": "2026-01-28 23:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bfnqm",
          "author": "SnooBananas5215",
          "text": "Maybe for the power user for dogshit coder like me offline models don't really work, instead I focus on creating simple UI with lots of automation and flexibility not built for scale though, most of the time I am not working on any heavy stuff anyways and I think just automating lots of stuff that small and medium businesses rely on for version control, qms and authorized form completion is all they need",
          "score": 1,
          "created_utc": "2026-01-29 00:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bryco",
          "author": "flywind008",
          "text": "you pay for your privacy, thatâ€™s fare enough",
          "score": 1,
          "created_utc": "2026-01-29 01:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bulco",
          "author": "deparko",
          "text": "Well, I've been dealing with the same issue and have concluded a hybrid approach works best. I use a three-tier model: an offline small LLM (Ollama) on my local 5070 TI GPU for local tasks; Ollama Cloud as tier two for bulk processing, where I can use Kimi and Deepseek..etc for a flat rate (about $20 a month, $240 a year), which is much cheaper than upgrading my GPU; and frontier models for deep reasoning when needed.\n\nI've designed my RAG and AI-native apps to operate within that three-tier framework.",
          "score": 1,
          "created_utc": "2026-01-29 01:20:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bwoqw",
          "author": "Vicar_of_Wibbly",
          "text": "The question is inverted. Why would I use the cloud? Itâ€™s slower, has no meaningful quality improvement over my local setup, imposes restrictions on use, and can change without notice or approval.\n\nMy local system is standardized, fully under my control, is backed up properly and - as you very importantly point out - is private.\n\nThe cloud has no compelling use case for me whatsoever.",
          "score": 1,
          "created_utc": "2026-01-29 01:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c00g2",
          "author": "mystery_biscotti",
          "text": "For funsies. \n\nAlso, i'm learning how to do tiny infra on a micro scale, so I'll be in a better spot to become employed keeping AI upright. \n\nPlus if certain US states are banning specific roles AI fills, local will be the only way to go. (Tennessee, I'm looking your way.)",
          "score": 1,
          "created_utc": "2026-01-29 01:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c0o3s",
          "author": "Kos187",
          "text": "You can spent 50M of input token over the weekend with Claude Code easily. It's easy to justify local LLM on a hardware you use for something else as well,  like gaming... But after some amount of VRAM it makes no sense. Especially when SLI is dead.",
          "score": 1,
          "created_utc": "2026-01-29 01:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2c2px8",
          "author": "celsowm",
          "text": "4. fine tuned models",
          "score": 1,
          "created_utc": "2026-01-29 02:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cbls2",
          "author": "Far-Low-4705",
          "text": "It's fun",
          "score": 1,
          "created_utc": "2026-01-29 02:53:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cdwao",
          "author": "lgdsf",
          "text": "Run jailbroken models for sure",
          "score": 1,
          "created_utc": "2026-01-29 03:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cetv8",
          "author": "Adventurous_Push6483",
          "text": "There are two differences I see: Some people use locally hosted models for personal use, and some people need to bootstrap ML to a product.\n\nIn terms of product:  \n  \n1. First of all, all the models available over the API have extremely high safety railguards. This affects some of my experiments (which can be product tests), which even though I am not generating NSFW content with it, I have found the censored models performing far worse in some of the specific personalization tasks I test with (they strangely tend to be less creative? I don't have any formal benchmarks for this so doubt it).  \n  \n2. Self-hosting is also much more \"safe\" in a sense that there is floor of what you can lose. If you build a public facing demo application of a product and didn't bother to secure it yet since it is an early stage PoC, you won't  run into strange issues with insecure rate spamming (exploding API costs) and whatnot. Yes, this is terrible practice but sometimes I just want to share my application with peers and its a lot easier to throw a Streamlit site over LAN and worry not about security (at worst, the app just crashes).\n\n3. I do mostly use the Gemini API, but the rate limits are certainly an issue with the free tier. The better alternative is just to use the paid tier if you have the money, to which you can get a surprising amount of research/experimental work done with just $10 worth of credit.\n\n4. Technically speaking, it is \"free\" for me. I just use my group's server (p beefy hardware) to locally host when I need so I'm not actually paying anything for the hardware. If the place you work at (or your PC) just happens to have compute for training/HPC work, you might as well use it since its already there.\n\n5. Millions of tokens is not as hard to hit as you think. Especially if you work with image data to VLM, which can easily cost thousands of tokens. I work with massive amounts of data in many media formats, data processing with LLM API is very expensive so the breaking-in-even is not a very compelling argument (this is still ran through the API, however, expect big bills far greater than GPUs). \n\nI think the nicer thing about the API is just sheerly how easy it is to setup. Buying hardware takes so much time when I can just rent it out on the cloud OR just use some money and use the API (download API key => download package => Use AI).\n\nThe more interesting argument has always been local GPU vs cloud GPU I think. APIs are just so limited to their scope and what they can do, but they are so convenient that there isn't a good reason to not use them if you have funding/VC money (with exception if you need something that's more specific than a generic language model, which is not many applications).",
          "score": 1,
          "created_utc": "2026-01-29 03:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cfpr5",
          "author": "Spanky2k",
          "text": "Privacy is everything. We use a local model when working on business sensitive stuff although our model choices are currently somewhat limited (Mac Studio 64GB). I'm hoping the eventual M5 or M6 Ultra based Mac Studios will have improved prompt processing enough so that it's possible to host a good sized (say 200-300B) model with multi user access (5-10 total users but realistically no more than 1 or 2 ever submitting queries at the same time) with reasonable performance. Something like that for about Â£10k would be perfect for a truly local and data secure system.\n\nBut lately I've been playing around loads with Qwen Image Edit and Hunyuan Video in ComfyUI on my 5090. I've been having a blast feeding them family photos and reimagining them in different styles, changing outfits, animating old family photos. I wouldn't have ever felt comfortable uploading that stuff to a cloud based service.",
          "score": 1,
          "created_utc": "2026-01-29 03:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cij7o",
          "author": "Chilidawg",
          "text": "There's the principle of ownership over renting. I know we're losing that battle a little more every day, but still...\n\nNo matter how cheap something is, there's an immeasurable gap between \"free\" and \"paid\". I understand that my electricity bill is likely more expensive than the API. However, that's grouped with utilities that are already expensive before running local models.\n\nThere's the novelty of verbally talking to your computer. There's no black box API promising you're talking to a model as opposed to someone in New Delhi. You can run the script and listen to your GPU fans speed up.\n\nFinally, I also don't pay Sam or Elon on principle.",
          "score": 1,
          "created_utc": "2026-01-29 03:33:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2clnqb",
          "author": "x0xxin",
          "text": "It's also a super fun and practical hobby. I've learned so much about self hosting and \"cloud tech\" via all of my labbing.",
          "score": 1,
          "created_utc": "2026-01-29 03:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d1fwr",
          "author": "Vivarevo",
          "text": "Pricing is marketing, it will change on short notice",
          "score": 1,
          "created_utc": "2026-01-29 05:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4g2r",
          "author": "Big_River_",
          "text": "all these prognosticators claiming models are commodities are investor bros who have a very confident surface level understanding of technology and macroeconomics - applying traditional business school analytics on technology adoption and artificial intelligence as a service like sas on steroids margins or corporations are going to just train their own model and cut vendor and labor cost - just like implementing a new management information system with autonomous agents as admins and analysts and developers...on their own local data center that the model manages itself with human in the loop hardware maintenance until robotics improves fine motor skills ...and all of this is a 9 month implementation that will become relatively plug and play fine tuned specialized model that can rewrite and optimize the code base, product firmware, optimize manufacturing processes, define the kpi that drive results, write up the marketing strategy and product design, 24/7 live dashboards on every deep dive imaginable for the c-suite goons to...nevermind all along frontier labs are racing with govt labs to produce the singular super intelligence that unlocks unimaginable world model exploits that change our fundamental understanding of what intelligence is capablr\n\nopen source + home lab represents the counterweight to corporate cloud \"all your base (models) are belong to us ( and your data too )\" and you can rent them for cheap now but once local compute becomes too expensive for joe six pack to build out in his garage - the cloud becomes the only game in town to get that coevo level up intelligence and truly generative creative extension of minds - there is a fundamental divide between the compute you own and the compute you rent - your data, your painfully fine tuned model that all your agents and business/creative / personal growth process depends upon...lease your work forever to market whims...cloud compute prices are so low to hook as many as they can while local compute components are skyrocketing - say goodbye to the capable personal computer - the next commercial electronics is cloud optimized priority connectivity for proprietary cloud ai managed computing resources - the user will have a great experience but it will always be renting the ability to functionally participate in the economy - when one org takes the super intelligence to monopolize compute as everyday iphone level (comprehensive information ingress paid promo filter and algorithm ranked content engagement device) service is vertically integrated.....well the individuals who sit on the board and the majority shareholders of this company that owns an overwhelming majority of compute ascend to a level of control and influence on the learning and reasoning of humanity that reduces their impact on the collective narrative to NPCs going through time in limited awareness incurious and relatively happy and cared for like a collection of objects or pets or puppets that are fun to play with until they break or shoot each other dead in the streets to amuse or shock....\n\nlocal compute ai clusters and distributed data owned by and maintained for the benefit of small communities and collectives is the only counterbalance to the commercial interests of capital markets and investors demand for unsustainable reality bending returns that own and control the narrative for some fucking reason in this shard of existence \n\nget local stacked functional compute or you are not playing the game -- you are getting played - get hooked on the cloud",
          "score": 1,
          "created_utc": "2026-01-29 06:00:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d4igb",
          "author": "Which-Jello9157",
          "text": "same question here. replies above are convincing but realistically im sticking with cloud apis for now since its easy and cheap. models keep getting bigger and whatever hardware u buy today probably wont handle next gen stuff anyway. do you have any recommended third-party api provider without rpm limit?",
          "score": 1,
          "created_utc": "2026-01-29 06:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ln5sp",
              "author": "PassagePlus3777",
              "text": "Openrouter.ai is the obvious choice for model variety since they aggregate from multiple providers but sometimes u get rate limited even on paid tier. pro tip tho, OR supports byok (bring your own key) so u can route to whichever upstream is cheapest for that specific model. i use atlascloud.ai for kimi k2.5 since theyre the cheapest and stable, and phala for glm 4.7 flash cuz they're stable on it. Way cheaper if ur running volume",
              "score": 2,
              "created_utc": "2026-01-30 13:40:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d7ig4",
          "author": "TruckAmbitious3049",
          "text": "For me it's rate limits. \n\nFor data analysis, I need to do a lot of labeling. Paid Gemini and ChatGPT would hit rate limits. \n\nFor transcribing, sonoix is amazing and cheap. But if it's a large batch, then Whisper still better.",
          "score": 1,
          "created_utc": "2026-01-29 06:25:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2damya",
          "author": "Photochromism",
          "text": "Lol. Dumb take. The minute one of these platforms becomes the favorite, becomes a necessity, and wipes out the others, the prices will a quadruple.",
          "score": 1,
          "created_utc": "2026-01-29 06:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dd8xe",
          "author": "AlwaysLateToThaParty",
          "text": "> What's the actual case for running local now beyond privacy? \n\nHaving a 96GB GPU with 1.7TB/s memory bandwidth for gaming.  It's quite good.",
          "score": 1,
          "created_utc": "2026-01-29 07:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dgc8i",
          "author": "SlimPerceptions",
          "text": "Even on point #1 - encrypted files and rented gpuâ€™s? I havenâ€™t done it but it seems like their are adequate privacy solutions out there even for the cloud",
          "score": 1,
          "created_utc": "2026-01-29 07:40:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2dpwvu",
          "author": "Maximum-Wishbone5616",
          "text": "In my experience the SLA is non-existent for most of AI services.\n\nWe require at least 99.99% (that is uber minimum), we monitor all our servers, instances and services every 30 seconds, we aim to get at least 99.9999% through out them.\n\nDue to sheer amount of requests we process per minute, 99.9% would mean that 3rd party AI 503 would cause havoc for our customers/monitoring systems.\n\nThey are not even close to 95% with 30 second monitoring...",
          "score": 1,
          "created_utc": "2026-01-29 09:08:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2esj8o",
          "author": "HopefulMaximum0",
          "text": "The API prices are low because they decided to operate at a loss until they win.   Then they will crank the price to the moon and do whatever suits their current whim to you, because you can't go elsewhere.\n\n  \nThe plan is simple, and as old as anti-dumping laws.",
          "score": 1,
          "created_utc": "2026-01-29 13:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fuuyk",
          "author": "Vahn84",
          "text": "I simply do not want a future where Iâ€™m not in control of my things. Corporations are forcing us into a future where we probably wonâ€™t have shit into our handsâ€¦and we will be forced to rent everything. I donâ€™t want a life hooked to a multitude of subscription services. Prices are going down nowâ€¦probably because the bubble is going to explode. But nothing can assure you that they will remain like this forever. I bet they wonâ€™t",
          "score": 1,
          "created_utc": "2026-01-29 16:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2h0dvb",
          "author": "Financial-Source7453",
          "text": "Abliterated models. Tired hearing \"I am sorry I can't do that due to policy restrictions\" from ChatGPT all the time.",
          "score": 1,
          "created_utc": "2026-01-29 20:03:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2j2til",
          "author": "mr_zerolith",
          "text": "I'll add to your list.\n\n\\- speed/intelligence tradeoffs are controlled by me.  \n\\- i have no idea what quantization/etc the model provider is using and they could be intermittently \"cutting the product\", which leads to inconsistent results that can be time consuming.  \n\\- when the service is down, i can get it back up with a reboot instead of wait a random amount of time.  \n\\- USA companies only: i am not directly helping fund a company that is involved in enormous copyright infringement or bribing, sorry, i mean, lobbying, the government against the interest of it's competitors and possibly me",
          "score": 1,
          "created_utc": "2026-01-30 02:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kz628",
          "author": "Euphoric_Emotion5397",
          "text": "there are some things you rather not do online.\n\nHumans valued their privacy.\n\nso , a lot of things can be done online. But there are some things still kept offline.\n\nIt's not a mutually exclusive situation with most people. It is a complementary solution (use GPU for x use case, use online API for x use case).\n\nExample, I wouldn't want Gemini to read my emails.  \nBut I might give it a try if I'm assured a local LLM setup can do it as well. And I will still not jump onto the bandwagon as an early adopter like how the clawdbot turns out. Just give it time. The best will come thru.\n\nRight now, I use Gemini Pro  + Anti-Gravity for vibe coding and generic enquiries and discussion with LLM. But I use my 32gb VRAM GPU to do local inference on my app data that are private to me.",
          "score": 1,
          "created_utc": "2026-01-30 10:58:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p2no8",
          "author": "Glum-Traffic-7203",
          "text": "For me itâ€™s privacy and customisation as the biggest to \n\nWhen it comes to rate limits and costs - there are specialist high volume low cost providers like doubleword who are even cheaper than APIs like Kimi",
          "score": 1,
          "created_utc": "2026-01-30 23:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3w4c",
          "author": "ravage382",
          "text": "To start turning a profit, all the big AI companies are going to have to really increase monthly rates. I imagine not too far in the future, prices are going to go way up. \n\nI have come to rely on AI enough that I want to ensure my continued access to it, regardless of what the big companies do. I will be pretty happy with just gpt120b and a few of the smaller qwen and glm models going into the future.  Knowledge cutoffs are annoying, but mcp tools with access to web search mostly mitigate that.",
          "score": 1,
          "created_utc": "2026-01-30 23:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26ucdm",
          "author": "bgiesing",
          "text": "Because it's still more expensive, my PC is already on 24/7 so it would be using about the same electricity regardless and I can use that GPU for many things (games, video editing, etc.), it already was paid off years ago. That's still cheaper than having to drop like $10-20 a month on API calls or a subscription service.\n\nAlso, many people explicitly chose local because they want to make content that the cloud models from the big companies refuse, API cost doesn't matter if every single reply you get is \"I can't fulfill that request\", you literally don't have an option",
          "score": 1,
          "created_utc": "2026-01-28 10:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xjdy",
          "author": "k_means_clusterfuck",
          "text": "not a deciding factor for me but i do think its nice to to know my carbon footprint",
          "score": 1,
          "created_utc": "2026-01-28 10:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26xq3m",
          "author": "SpicyWangz",
          "text": "Ah yes, the freefall. LLM companies donâ€™t even know whatâ€™s happening. Every time they look at their own pricing the numbers are lower. Soon enough Iâ€™m sure theyâ€™ll be paying you to use their API",
          "score": 1,
          "created_utc": "2026-01-28 10:44:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26zos4",
          "author": "tcoder7",
          "text": "When you use AI API you give away your intellectual property. The model scans your repo and sends everything to a remote server. Open the output tab in VSCode and watch.",
          "score": 1,
          "created_utc": "2026-01-28 11:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o275pm1",
          "author": "FastDecode1",
          "text": "Computer hardware: $$$\n\nNot going to prison for asking forbidden questions: priceless.",
          "score": 1,
          "created_utc": "2026-01-28 11:49:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28nm9d",
          "author": "slindshady",
          "text": "Everything is sensitive data. Thatâ€™s the point. Look what happens with one â€žbadâ€œ election.",
          "score": 1,
          "created_utc": "2026-01-28 16:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27h6zy",
          "author": "zipperlein",
          "text": "VC will dry out eventually.",
          "score": 0,
          "created_utc": "2026-01-28 13:06:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27t3mp",
          "author": "devinprocess",
          "text": "Gonna be real with you.\n\nActual case: you are in the 1% of the rich or lucky folks here who have accumulated enough hardware and have no issues running a power guzzling setup.\n\nFor majority of us normal bees, api or renting is still the way because the local models we can run are just for shits and giggles, and who cares about all those arguments at that stage. \n\nUnless local llms become affordable itâ€™s just a circlejerk.",
          "score": 0,
          "created_utc": "2026-01-28 14:10:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28pyic",
          "author": "DataGOGO",
          "text": "The bots are going to rage on me for this one; but not putting all my data in the hands of the Chinese government.Â \n\nI will run local or use a US based provider subject to US / EU data protection laws.\n\n\nEvery Chinese provider is very heavily subsidized by the Chinese government (and that is just what they openly admit to, the true extent is unknown).Â \n\nThe entire business model is to undercut US / EU companies to the point of making the AI business unsustainable, thus giving China AI dominance.Â They know that in order for OpenAI, xAI, Meta, Google, Microsoft, etc. to stay in the AI business, they eventually have to turn a profit.\n\nBy releasing models into open source, and providing extremely cheap API access the goal is to make turning a profit impossible.\n\nThat is very bad for everyone, as it just turns AI into Chinese propaganda machines and data collection tools.Â \n\nTo be clear, I am not throwing shade on the Chinese developers, engineers, and data scientists at all, just the government framework they are forced to operate in.Â ",
          "score": 0,
          "created_utc": "2026-01-28 16:41:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo349m",
      "title": "deepseek-ai/DeepSeek-OCR-2 Â· Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-OCR-2",
      "author": "Dark_Fire_12",
      "created_utc": "2026-01-27 03:56:49",
      "score": 334,
      "num_comments": 43,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o1yiqke",
          "author": "foldl-li",
          "text": "They even thanked themself!\n\nhttps://preview.redd.it/t34eyddujtfg1.png?width=1037&format=png&auto=webp&s=7508bb6586dfb7327311dfddb2f108f459ccef2f",
          "score": 160,
          "created_utc": "2026-01-27 04:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yktdl",
              "author": "TheRealMasonMac",
              "text": "https://preview.redd.it/k8ykrq19mtfg1.png?width=286&format=png&auto=webp&s=9584d025699644e92331e6d5ff221b5c14ef68ba",
              "score": 134,
              "created_utc": "2026-01-27 04:37:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1yiw2t",
              "author": "Dark_Fire_12",
              "text": "lol that made me laugh.",
              "score": 25,
              "created_utc": "2026-01-27 04:25:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1zdvfd",
              "author": "No_Afternoon_4260",
              "text": "May be not the same teal",
              "score": 3,
              "created_utc": "2026-01-27 08:27:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ykn7u",
          "author": "foldl-li",
          "text": "I always use scores reported by A to evaluate model B/C/D. So, in this case, PaddleOCR-VL looks really awesome.\n\nhttps://preview.redd.it/trymwuqoltfg1.png?width=1130&format=png&auto=webp&s=9b4a33243260da38c103d681c1ad5bdc8d5f9156",
          "score": 44,
          "created_utc": "2026-01-27 04:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yusec",
              "author": "linkillion",
              "text": "I mean, that's not really DS benchmarking the other model, it's just a general benchmark.Â \n\n\nThat said, paddleocr is great but it's a PITA to get working to this level, it requires their pipeline which I honestly gave up on very quickly. MistralOCR, although closed source, is so far ahead it's not even close in my opinion. For my use case all the docs I use are public, so I use MistralOCR exclusively.Â ",
              "score": 16,
              "created_utc": "2026-01-27 05:47:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yy48f",
                  "author": "zball_",
                  "text": "I use Gemini 3 flash as OCR and it was phenomenal.",
                  "score": 12,
                  "created_utc": "2026-01-27 06:12:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yxi6m",
                  "author": "skinnyjoints",
                  "text": "I have been sleeping on mistral for a while now. Why do you consider it to be the best? And is it the best among OCR specific models or does it compete with multimodal LLMs as well?",
                  "score": 1,
                  "created_utc": "2026-01-27 06:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1z1yrg",
              "author": "Pvt_Twinkietoes",
              "text": "My experience with them has been phenomenal as well. I think somethings to note would be, it doesn't handle minor tilts/skew in the document, and users should be aware of that, but the pipeline provided does have a reliable model to predict the orientation of the document (90/180/270) tilts.\n\nThough it's amazing, I also noticed that there is a failure mode which causes the model to repeat itself (like Whisper), not sure of the cause but something to take note of.\n\nNevertheless it is truly an amazing model and very grateful they open sourced it.",
              "score": 1,
              "created_utc": "2026-01-27 06:43:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zit1a",
              "author": "Intelligent-Form6624",
              "text": "Does it work with ROCm or vulkan yet?",
              "score": 1,
              "created_utc": "2026-01-27 09:13:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1za5ac",
          "author": "R_Duncan",
          "text": "HunyuanOCR is not in the list.... this is cheating. For any kind of document, beats PaddleOCR hands down with 1B parameters.\n\n[https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true](https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true)",
          "score": 12,
          "created_utc": "2026-01-27 07:53:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zgxkh",
              "author": "__Maximum__",
              "text": "Is it end to end or pipeline?",
              "score": 2,
              "created_utc": "2026-01-27 08:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21dvaj",
                  "author": "R_Duncan",
                  "text": "Is pdf/image to markdown",
                  "score": 3,
                  "created_utc": "2026-01-27 16:11:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21vgbu",
              "author": "urekmazino_0",
              "text": "I second this",
              "score": 1,
              "created_utc": "2026-01-27 17:27:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1z8565",
          "author": "Intelligent_Coffee44",
          "text": "I have some GPU credits that are near expiration, so I made this quick demo for DeepSeek OCR 2: [https://deepseek-ocr-v2-demo.vercel.app](https://deepseek-ocr-v2-demo.vercel.app)\n\n~~It's still very rough - small models + temperature=0 is very prone to repetition. I'll polish up the implementation in the morning. If anyone has an idea how to make the output more reliable, please let me know!~~\n\nUpdate: Decided to stay up and finish the job lol! Turns out the repetition issue was my user error. Now completely fixed after using DeepSeek's recommended decoding params. Performance is amazing and much more reliable than v1 in my testing. Hope you guys enjoy it too :O",
          "score": 21,
          "created_utc": "2026-01-27 07:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2078ly",
              "author": "Express-Director-474",
              "text": "Thanks for sharing your GPU with us. I still see the repetition error on my end.",
              "score": 2,
              "created_utc": "2026-01-27 12:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25pfyn",
                  "author": "Intelligent_Coffee44",
                  "text": "There were some configuration mistakes on my end. Now i've made it as closely aligned with official sample as possible. Please give it another try. I also did some analysis on the choice of prompt and document type affect output reliability - also published on the same site.",
                  "score": 1,
                  "created_utc": "2026-01-28 04:39:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o257yy2",
              "author": "__appelsinpiken",
              "text": "Very helpful! It looks like when tables are involved, the output is in HTML format, which doesnâ€™t render as a visible preview. Would it be possible to add a feature? That would make reviewing much more intuitive. Again thanks for your work! :>\n\nhttps://preview.redd.it/tu4x5r6t80gg1.png?width=665&format=png&auto=webp&s=29d0261de84095e8ee19861e769f0f9d3e654b0e",
              "score": 1,
              "created_utc": "2026-01-28 02:57:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o25p4zl",
                  "author": "Intelligent_Coffee44",
                  "text": "Thanks for using! I just shipped markdown table support (from free OCR mode)\n\nDo you think this works or do you still prefer to have html table rendering?\n\nhttps://preview.redd.it/56nfle24r0gg1.png?width=715&format=png&auto=webp&s=4bf26bcd6c26e7e9ace035b56f251243c6c6be98",
                  "score": 1,
                  "created_utc": "2026-01-28 04:37:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2b76j0",
              "author": "Confident-Ad-2688",
              "text": "I have download model from unsloth , I have GPU of rtx 3060 12 gb, didn't get results, like in your provided link . In your link results are superb,can you tell me how do you run the model ?",
              "score": 1,
              "created_utc": "2026-01-28 23:17:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2ckq9x",
                  "author": "Intelligent_Coffee44",
                  "text": "Thank you! I recommend following the setup instructions from the official github repo [readme](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/README.md) and running [DeepSeek-OCR2-hf/run\\_dpsk\\_ocr2.py](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek-OCR2-master/DeepSeek-OCR2-hf/run_dpsk_ocr2.py) for inference. \n\nYou have the perfect GPU: the model only needs about 7-8gb vram to run, and your gpu is the same generation as A100 so you can install the exact same dependencies as the official guide.\n\nFor some reason i've found using the pinned transformers and tokenizers package versions helps fix performance issues.",
                  "score": 2,
                  "created_utc": "2026-01-29 03:46:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yefjy",
          "author": "Dark_Fire_12",
          "text": "GitHub Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2](https://github.com/deepseek-ai/DeepSeek-OCR-2)\n\nPaper Link: [https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek\\_OCR2\\_paper.pdf](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf)",
          "score": 14,
          "created_utc": "2026-01-27 03:57:28",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1yfk2y",
          "author": "lomirus",
          "text": "Finally",
          "score": 10,
          "created_utc": "2026-01-27 04:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z49rf",
          "author": "the__storm",
          "text": "Interesting, I look forward to trying it out - DeepSeek-OCR (1) wasn't great (benchmarked okay but severely underperformed irl), so I'm glad they stuck with it.",
          "score": 4,
          "created_utc": "2026-01-27 07:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zq1jm",
          "author": "Gloomy-Signature297",
          "text": "Might be a stupid question but could this mean something regarding native multi-modality for Deepseek V4 next month?",
          "score": 5,
          "created_utc": "2026-01-27 10:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2177ju",
              "author": "ELPascalito",
              "text": "We cannot be sure, but it would be cool if the next model has this OCR module bolted on, just like how Mistral does",
              "score": 2,
              "created_utc": "2026-01-27 15:42:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eh261",
          "author": "still_debugging_note",
          "text": "Been running Monkey-OCR for most OCR workloads.\n\nDeepSeek-OCR looks promising(esp. doc-level modeling), but I havenâ€™t tried it yet.Any insights on cost-efficiency compared to Monkey-OCR?",
          "score": 2,
          "created_utc": "2026-01-29 12:48:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ysnmt",
          "author": "Final_Personality987",
          "text": "https://preview.redd.it/bil1ybybvtfg1.png?width=1906&format=png&auto=webp&s=8ff884f062905a816cc6ba95e08904ca6e778b61\n\nquick summary: [https://lilys.ai/digest/7864011/8699710](https://lilys.ai/digest/7864011/8699710)",
          "score": 2,
          "created_utc": "2026-01-27 05:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zgaxi",
          "author": "Intelligent-Form6624",
          "text": "Heck yes!!! ðŸ‘ðŸ‘\n\nCan it run on Strix Halo?",
          "score": 2,
          "created_utc": "2026-01-27 08:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ss31",
          "author": "DouglasteR",
          "text": "Simply amazing",
          "score": 1,
          "created_utc": "2026-01-27 14:34:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2b8jto",
          "author": "Medium_Confection604",
          "text": "Has anyone had any experience with parsing the output as bounding boxes + text?\n\nDuring my tests, I ran into situations like this:\n<|ref|>text<|/ref|><|det|>[[174, 78, 910, 113], [300, 153, 451, 131]]]<|/det|> form approved in writing by such Supervisor (or Referee in the case of Billie Jean King Cup).\n\nWhere the \"det\" tag had multiple bounding boxes (in some cases more than 2), but I can't figure out how to associate each bounding box with the corresponding text at the bottom.",
          "score": 1,
          "created_utc": "2026-01-28 23:24:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqpon2",
      "title": "OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qqpon2",
      "author": "jacek2023",
      "created_utc": "2026-01-30 00:07:33",
      "score": 304,
      "num_comments": 190,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2iyxhw",
          "author": "nickcis",
          "text": "In what hardware are you running this?",
          "score": 30,
          "created_utc": "2026-01-30 02:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kf5le",
              "author": "jacek2023",
              "text": "with dust on it:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1nsnahe/september\\_2025\\_benchmarks\\_3x3090/](https://www.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/)",
              "score": 10,
              "created_utc": "2026-01-30 07:58:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2lbgfa",
                  "author": "AlwaysLateToThaParty",
                  "text": "Good setup.  Who knew such a thing would become worth so much, huh?  Appreciate the break down.",
                  "score": 2,
                  "created_utc": "2026-01-30 12:31:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2pbx31",
                  "author": "Artistic_Okra7288",
                  "text": "I'm getting a sustained 33 tps tg with a single 3090 Ti using UD Q4 K XL, same amount of prompt cache (DDR4). With the self speculative decoding, when it hits, it pushes over 100 tps tg.\n\nTry pushing your batch-size to 8192. It's significantly reduced prompt processing time and the GPU usage for me is hovering around 90% during prompt processing.",
                  "score": 1,
                  "created_utc": "2026-01-31 00:10:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2indlk",
          "author": "klop2031",
          "text": "How is the quality? I like glm flash as i get like 100t/s which is amazing. But havent really tested the llms quality.",
          "score": 19,
          "created_utc": "2026-01-30 01:01:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iwsu5",
              "author": "oginome",
              "text": "Its pretty good. Give it MCP capabilities like vector RAG, web search, etc its even better.",
              "score": 22,
              "created_utc": "2026-01-30 01:53:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j651h",
                  "author": "everdrone97",
                  "text": "How?",
                  "score": 8,
                  "created_utc": "2026-01-30 02:45:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lroxh",
                  "author": "vertigo235",
                  "text": "which vector rag mcp do you like?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kh6z3",
              "author": "jacek2023",
              "text": "Earlier, I created a hello world app that connects to my llama-server and sends a single message. Then I showed this hello world example to opencode and asked it to write a debate system, so I could watch three agents argue with each other on some topic. This is the (working) result:\n\n    debate_system/\n    â”œâ”€â”€ debate_config.yaml       # Configuration (LLM settings, agents, topic)\n    â”œâ”€â”€ debate_agent.py          # DebateAgent class (generates responses)\n    â”œâ”€â”€ debate_manager.py        # DebateManager class (manages flow, context)\n    â”‚   â”œâ”€â”€ __init__()           # Initialize with config validation\n    â”‚   â”œâ”€â”€ load_config()        # Load YAML config with validation\n    â”‚   â”œâ”€â”€ _validate_config()   # Validate required config sections\n    â”‚   â”œâ”€â”€ _initialize_agents() # Create agents with validation\n    â”‚   â”œâ”€â”€ start_debate()       # Start and run debate\n    â”‚   â”œâ”€â”€ generate_summary()   # Generate structured PRO/CON/CONCLUSION summary\n    â”‚   â”œâ”€â”€ format_summary_for_llm()  # Format conversation for LLM\n    â”‚   â”œâ”€â”€ save_summary()       # Append structured summary to file\n    â”‚   â””â”€â”€ print_summary()      # Print structured summary to console\n    â”œâ”€â”€ run_debate.py            # Entry point\n    â””â”€â”€ debate_output.txt        # Generated output (transcript + structured summary)\n    \n    shared/\n    â”œâ”€â”€ llm_client.py            # LLM API client with retry logic\n    â”‚   â”œâ”€â”€ __init__()           # Initialize with config validation\n    â”‚   â”œâ”€â”€ _validate_config()   # Validate LLM settings\n    â”‚   â”œâ”€â”€ chat_completion()   # Send request with retry logic\n    â”‚   â”œâ”€â”€ extract_final_response() # Remove thinking patterns\n    â”‚   â””â”€â”€ get_response_content() # Extract clean response content\n    â”œâ”€â”€ config_loader.py         # Legacy config loader (not used)\n    â””â”€â”€ __pycache__/             # Compiled Python files\n    \n    tests/\n    â”œâ”€â”€ __init__.py              # Test package initialization\n    â”œâ”€â”€ conftest.py              # Pytest configuration\n    â”œâ”€â”€ pytest.ini               # Pytest settings\n    â”œâ”€â”€ test_debate_agent.py     # DebateAgent unit tests\n    â”œâ”€â”€ test_debate_manager.py   # DebateManager unit tests\n    â”œâ”€â”€ test_llm_client.py       # LLMClient unit tests\n    â””â”€â”€ test_improvements.py     # General improvement tests\n    \n    requirements.txt            # Python dependencies (pytest, pyyaml)\n    debate_system_design/\n    â””â”€â”€ design_document.md       # Design specifications and requirements\n\n  \nand I never told him about the tests, but somehow he created good ones",
              "score": 4,
              "created_utc": "2026-01-30 08:16:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2l3xvs",
              "author": "-dysangel-",
              "text": "It's best in class for its size IMO, as long as you're running it at 8 bit. When I ran at 4 bit, it got stuck in loops. It's actually the first small model I've found where 8bit vs 4 bit actually makes a noticeable difference.",
              "score": 4,
              "created_utc": "2026-01-30 11:37:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mlkzn",
                  "author": "twack3r",
                  "text": "Try Nemotron. BF16 great, Q8 already takes a very deep perplexity dive.",
                  "score": 1,
                  "created_utc": "2026-01-30 16:25:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2o847t",
                  "author": "DHasselhoff77",
                  "text": "I haven't ran into looping with MXFP4 quants but admittedly didn't test much yet.",
                  "score": 1,
                  "created_utc": "2026-01-30 20:48:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2jfbdq",
              "author": "floppypancakes4u",
              "text": "With local hardware? I only get about 20tks max on a 4090",
              "score": 4,
              "created_utc": "2026-01-30 03:37:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2jrcov",
                  "author": "simracerman",
                  "text": "Something is off in your setups I hit 60 t/s at 8k context with 5070 Ti.",
                  "score": 11,
                  "created_utc": "2026-01-30 04:52:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kdyhh",
                  "author": "Theio666",
                  "text": "I was hitting 40tps on 4x2080ti, Q5, something is wrong with your setup.",
                  "score": 3,
                  "created_utc": "2026-01-30 07:48:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lzwbw",
                  "author": "packetsent",
                  "text": "120 t/s on a 5090 at Q6 or Q4 (will need to check once home)",
                  "score": 2,
                  "created_utc": "2026-01-30 14:45:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jucvl",
                  "author": "klop2031",
                  "text": "Yes, when i get a chance ill post my config. I was surprised at that at first but have been able to get this with a 3090 + 192gb ram",
                  "score": 1,
                  "created_utc": "2026-01-30 05:13:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lddqq",
                  "author": "teachersecret",
                  "text": "On a 4090 Iâ€™m getting over 100t/s on this model in 4 bit k xl. You must be offloading something to cpu/ram.",
                  "score": 1,
                  "created_utc": "2026-01-30 12:44:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2jwkng",
                  "author": "simracerman",
                  "text": "Something is off with your setup. My 5070 Ti does 58 T/s at 8k context.",
                  "score": 1,
                  "created_utc": "2026-01-30 05:28:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kfqvo",
                  "author": "SlaveZelda",
                  "text": "I can do 45 tok/s at 50k context on a 4070ti",
                  "score": 1,
                  "created_utc": "2026-01-30 08:03:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jbshq",
          "author": "ab2377",
          "text": "what's your hardware setup?",
          "score": 7,
          "created_utc": "2026-01-30 03:17:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iyysg",
          "author": "Several-Tax31",
          "text": "Your output seems very nice. Okay, sorry for the noob question, but I want to learn about agentic frameworks.Â \n\n\nI have the exact setup, llama.cpp, glm-4.7 flash, and I donwload opencode. How to configure the system to create semi-complex projects like yours with multiple files? What is the system prompt, what is the regular prompt, what are the config files to handle? Care to share your exact setup for your hello world project, so I can replicate it? Then I'll iterate from there to more complex stuff.Â \n\n\nContext: I normally use llama-server to one shot stuff, and iterate on projects via conversation. Compile myself. Didnt try to give model tool access. Never used claude code or any other agentic frameworks, so the noob question. Any tutorial-ish info would be greatly appreciated.Â ",
          "score": 4,
          "created_utc": "2026-01-30 02:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j69h8",
              "author": "Pentium95",
              "text": "This tutorial Is for Claude code and codex. Opencode specific stuff Is written on their github. \n\nhttps://unsloth.ai/docs/basics/claude-codex",
              "score": 10,
              "created_utc": "2026-01-30 02:46:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2j72lt",
                  "author": "Several-Tax31",
                  "text": "Many thanks for the info! Dont know why it didnt occur to me to check unsloth.Â ",
                  "score": 4,
                  "created_utc": "2026-01-30 02:50:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kesr2",
                  "author": "cantgetthistowork",
                  "text": "How do you make Claude code talk with openai compatible endpoint? It's sending the v1/messages format",
                  "score": 1,
                  "created_utc": "2026-01-30 07:55:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ikj1k",
          "author": "BitXorBit",
          "text": "waiting for my mac studio to arrive to try exactly this setup, i been using claude code everyday and i just keep filling it with more balance every day. can't wait to work locally.\n\nhow is it compared to opus 4.5? sure not smart equally, but smart enough?",
          "score": 7,
          "created_utc": "2026-01-30 00:46:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2j14m6",
              "author": "moreslough",
              "text": "Using opus for planning and handing off to gpt-oss-{1,}20B works p well. Many local models you can load on your studio donâ€™t quite compare to opus, but they are capable. Helps conserve/utilize the tokens",
              "score": 4,
              "created_utc": "2026-01-30 02:18:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k2sxe",
                  "author": "florinandrei",
                  "text": "How exactly do you manage the hand-off from Opus to GPT-OSS? Do you invoke both from the same tool? (e.g. Claude Code) If so, how do you route the prompts to the right endpoints?",
                  "score": 3,
                  "created_utc": "2026-01-30 06:15:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ivdrv",
              "author": "TheDigitalRhino",
              "text": "Make sure you try something like this [https://www.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx\\_native\\_apple\\_silicon\\_llm\\_inference\\_464/](https://www.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/)\n\nyou really need the batching for the PP",
              "score": 2,
              "created_utc": "2026-01-30 01:45:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2j9avz",
          "author": "BrianJThomas",
          "text": "I tried this with GLM 4.7 Flash, but it failed even basic agentic tasks with OpenCode. I am using the latest version of LM Studio. I experimented some with inference parameters, which helped some. However, I couldn't get it to generate code reliably.\n\nAm I doing something wrong? I think it's kind of hard because the inference settings all greatly change the model behavior.",
          "score": 7,
          "created_utc": "2026-01-30 03:03:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kg1vc",
              "author": "jacek2023",
              "text": "If you look at my posts on LocalLLaMA from the last few days, there were multiple GLM-4.7-Flash fixes in llama.cpp. I donâ€™t know whether they are actually implemented in LM Studio.",
              "score": 5,
              "created_utc": "2026-01-30 08:06:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kozrd",
                  "author": "BrianJThomas",
                  "text": "Ah OK. I haven't tried llama.cpp without a frontend in a while. I had assumed the LM Studio version would be fairly up to date. Trying now, thanks.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:28:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lpaal",
                  "author": "1ncehost",
                  "text": "I can confirm they didnt have the latest llama.cpp as of yesterday. The llama.cpp release off github performs way better currently.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:52:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lyi3t",
                  "author": "lolwutdo",
                  "text": "Any idea if they fixed the model not producing opening thinking tags?",
                  "score": 1,
                  "created_utc": "2026-01-30 14:38:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k7nla",
              "author": "Odd-Ordinary-5922",
              "text": "just switch off lmstudio",
              "score": 3,
              "created_utc": "2026-01-30 06:54:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2k90cc",
                  "author": "BrianJThomas",
                  "text": "It's just llama.cpp.... Or are you just complaining about me using a frontend you don't prefer?",
                  "score": -2,
                  "created_utc": "2026-01-30 07:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k5o9d",
              "author": "Careless_Garlic1438",
              "text": "well I have Claude Code and Opencode running, opencode works on some questions but fails miserable at others, even a simple HTML edit failed, took Claude minutes to do â€¦ so very hit and miss depending on what model you use locally â€¦ I will do a test with online models and opencode to see if that helps",
              "score": 1,
              "created_utc": "2026-01-30 06:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2kg3tz",
                  "author": "jacek2023",
                  "text": "opencode with what model?",
                  "score": 1,
                  "created_utc": "2026-01-30 08:07:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mp19t",
              "author": "markole",
              "text": "What are your llama-server flags?",
              "score": 1,
              "created_utc": "2026-01-30 16:40:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jb7qu",
          "author": "1ncehost",
          "text": "Haha I had this exact post written up earlier to post here but I posted it on twitter instead. This stack is crazy good. I am blown away by the progress.\n\nI am getting 120 tok/s on a 7900 xtx with zero context and 40 tok/s with 50k context. Extremely usable and seems good for tasks around 1 man hour in scale based on my short testing.",
          "score": 5,
          "created_utc": "2026-01-30 03:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2k3vwa",
              "author": "Glittering-Call8746",
              "text": "Your github repo pls. Amd setup are a pain to start..",
              "score": 2,
              "created_utc": "2026-01-30 06:24:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lpnip",
                  "author": "1ncehost",
                  "text": "You dont need rocm. Just use the vulkan github release. That works with the stock linux amdgpu drivers and radeon drivers on windows. I'm using linux so i dont know how it runs on windows.\n\nSo literally install OS normally and download the vulkan llama.cpp off the github.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:53:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2ld8ts",
              "author": "brokester",
              "text": "Are you interested in sharing your setup. Also have a 7900xtx. I assume you are on Linux?\nAlso did you offload to CPU/ram?",
              "score": 1,
              "created_utc": "2026-01-30 12:43:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lox5r",
                  "author": "1ncehost",
                  "text": "Yes linux, using vulkan llama.cpp latest release from github. Model is unsloth glm 4.7 flash REAP at iq4 quant.\n\nThat quant easily fits in the 24 GB, but you'll want to turn on flash attention to run the large context.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:50:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mn947",
          "author": "Danmoreng",
          "text": "https://x.com/ggerganov/status/2016903216093417540\n\nLlama.cpp creator recommends using glm4.7flash with thinking disabled for agentic coding",
          "score": 2,
          "created_utc": "2026-01-30 16:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mnfeo",
              "author": "jacek2023",
              "text": "Needs testing",
              "score": 1,
              "created_utc": "2026-01-30 16:33:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2j80x5",
          "author": "Sl33py_4est",
          "text": "no claude for you; \nwe have claude at home\n\nclaude at home:",
          "score": 2,
          "created_utc": "2026-01-30 02:56:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jgsd1",
          "author": "thin_king_kong",
          "text": "Depending where you live.. could the electricity bill actually exceed claude subscriptions?",
          "score": 2,
          "created_utc": "2026-01-30 03:46:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l3yai",
              "author": "doyouevenliff",
              "text": "The most commonly reported figure for full-load power draw of the 5090 is about 575 W (0.575 kW) under heavy load. (Short spikes can be much higher, up to ~900 W, but those are very brief transients, and for monthly energy use we use the sustained load number ~575 W).\n\nIf the GPU runs at full load (0.575 kW) for 24 hours per day:\n\nDaily energy=0.575 kWÃ—24 h=13.8 kWh/day\n\nAssume a typical month with 30 days:\n\nMonthly energy=13.8 kWh/dayÃ—30 days=414 kWh/month\n\nElectricity prices in the U.S. average around\n16â€“18 cents per kilowatt-hour (kWh) for residential customers, though rates vary widely by stateâ€”from under 12Â¢ to over 40Â¢ in places like Hawaii. Let's go with 40Â¢ for now.\n\nMonthly cost=414 kWh/month * 40Â¢=$165\n\nSo even if you have the most expensive energy plan, running the model 24/7 on a single 5090 GPU with sustained load you will not really exceed the Claude Max subscription. If you add the energy for the rest of the PC you might reach the same level (~$200).\n\nBut most people don't have the most expensive energy plan - average is half that, so you'll end up spending around $100 for running the PC nonstop. And also most people don't really run the model all day every day. And if you add solar/renewables into the mix you will reduce the cost further.\n\nTL;DR: **No**, at most you would spend the same*\n\n^(*for current energy prices (max 40Â¢ per kWh)^) ^(and if running a 5090 PC 24/7)",
              "score": 3,
              "created_utc": "2026-01-30 11:37:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lonjn",
                  "author": "DOAMOD",
                  "text": "I have mine set to a maximum of 400W and it's performing very well with acceptable power consumption. I'm getting 800/70/75 with 128.\n\nFor me, this model is incredible. I've spent days implementing it in Py/C++ and testing it in HTML, JS, etc., and it's amazing for its size. I haven't seen anything like it in terms of tool calls (maybe OSS is the closest), but it not only handles them well, but the choices it makes are excellent when they make sense. It doesn't have the intelligence of a larger model, obviously, but it gets the job done and compensates with its strengths. As I said in another post, for me, it's the first small model that I've seen that's truly excellent. I call it the Miniminimax.",
                  "score": 1,
                  "created_utc": "2026-01-30 13:48:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2kg5pu",
              "author": "jacek2023",
              "text": "You are on wrong sub",
              "score": 3,
              "created_utc": "2026-01-30 08:07:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jh19s",
          "author": "an80sPWNstar",
          "text": "I had no idea any of this was possible. This is freaking amazeballs. I've just been using Qwen 3 coder 30b instruct Q8. How would y'all's say that Qwen model compares with this? I am not a programmer at all. I'd like to learn, so it would mostly be vibecoding until I start learning more. I've been in IT long enough to understand a lot of the basics which has helped to fix some mistakes but I couldn't point the mistakes out initially if that makes sense.",
          "score": 2,
          "created_utc": "2026-01-30 03:47:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kdajy",
              "author": "Dr4x_",
              "text": "On my setup I observe that Qwen3 coder is kind of struggling when it comes to using tools, GLM 4.7 flash is doing a great job at it",
              "score": 2,
              "created_utc": "2026-01-30 07:42:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2lq0jr",
              "author": "1ncehost",
              "text": "Its noticeably better than qwen3 coder.",
              "score": 2,
              "created_utc": "2026-01-30 13:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lui74",
                  "author": "an80sPWNstar",
                  "text": "I shall give it a go. Thanks!",
                  "score": 1,
                  "created_utc": "2026-01-30 14:18:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2idg7h",
          "author": "ForsookComparison",
          "text": "At context size if 200000 why not try it with the actual Claude code tool?",
          "score": 2,
          "created_utc": "2026-01-30 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ie3ag",
              "author": "jacek2023",
              "text": "because the goal was to have local open source setup",
              "score": 41,
              "created_utc": "2026-01-30 00:11:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iekn6",
                  "author": "ForsookComparison",
                  "text": "Gotcha",
                  "score": 6,
                  "created_utc": "2026-01-30 00:14:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2j6jgf",
                  "author": "lemon07r",
                  "text": "In other guys defense, that wasn't clear in your title, or post body. Im sure you will continue to eclipse them in internet points anyways for mentioning open source. \n\nMore on topic, how do you like opencode compared to claude code? I use both but havent really found anything I liked more in cc and have ended up mostly sticking to opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 02:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2k5g8h",
              "author": "Careless_Garlic1438",
              "text": "You could do it, there are Claude code proxies to use other and local models â€¦ would be interesting to see if that runs better/worse than opencode.",
              "score": 1,
              "created_utc": "2026-01-30 06:36:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2lo1ma",
                  "author": "ForsookComparison",
                  "text": "It's officially supported as of recently. No need for proxies",
                  "score": 1,
                  "created_utc": "2026-01-30 13:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ja63x",
          "author": "According-Tip-457",
          "text": "Why not just use Claude code directly instead of this watered down Opencode... you can use llama.cpp in Claude Code. What's the point of OpenCode? sub par performance?",
          "score": 3,
          "created_utc": "2026-01-30 03:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l92gv",
              "author": "PunnyPandora",
              "text": "not having to use a closed source dogshit?",
              "score": 0,
              "created_utc": "2026-01-30 12:15:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2l9sv6",
                  "author": "According-Tip-457",
                  "text": "Claude Code is FAR superior to OpenCode. Opencode is just a watered down version of Claude Code. just saying buddy.... just saying. be \"open\" all you want... just means you will have watered down features compared to someone is getting paid $500,000 to create. You really think someone is going to waste their precious time developing something serious to not get paid for it? no.... They will work on it in their free time and they won't put the same level of commitment as someone getting paid $500,000. Just saying. Enjoy our opensource dogwater.",
                  "score": -3,
                  "created_utc": "2026-01-30 12:20:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k4igm",
          "author": "Careless_Garlic1438",
          "text": "Well I use Claude code and have been testing Opencode with GLM-4.7-Flash-8bit and it cannot compare ... takes way longer, something about inference speed, sure, have 70+ tokens/s, but that is not all gpt-oss 120B is faster so itâ€™s also the way those tinking models overthink without coming to a conclusion.  \nSometimes it works and sometimes it doesnâ€™t, like I asked it to modify a HTML page, cut off the first intro part and make code blocks easy to copy, it took hours and never completed, such a simple task â€¦   \nAsked it to do a space invaders and it was done in minutes â€¦ Claude code is faster, but more importantly, way more intelligent â€¦",
          "score": 1,
          "created_utc": "2026-01-30 06:29:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kfigt",
              "author": "jacek2023",
              "text": "Do you mean that an open-source solution on home hardware is slower and simpler than a very expensive cloud solution from a big corporation? ;)\n\nIâ€™m trying to show what is possible at home as an open source alternative. Iâ€™m not claiming that you can stop paying for a business solution and replace it for free with a five-year-old laptop.",
              "score": 5,
              "created_utc": "2026-01-30 08:01:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2mh95h",
                  "author": "Careless_Garlic1438",
                  "text": "I get the slower, but if it fails at editing a local HTML file, not that difficult, just cut out the intro â€¦ it begs to the question how useful it is. On the other hand it pits out a basic space invaders in minutes â€¦",
                  "score": 1,
                  "created_utc": "2026-01-30 16:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k9or0",
          "author": "Either-Nobody-3962",
          "text": "I Really have hard time with opencode configuring, because their terminal doesn't allow me to change models  \nAlso i am ok to use hosted glm api, if it really matches claude opus levels. ( I am hoping kimi 2.5 has that)",
          "score": 1,
          "created_utc": "2026-01-30 07:11:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kgsus",
          "author": "raphh",
          "text": "How is OpenCode's agentic workflow compared to Claude Code? I mean what is the advantage of using OpenCode vs just using Claude Code with llama.cpp as model source ?",
          "score": 1,
          "created_utc": "2026-01-30 08:13:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ki8xz",
              "author": "jacek2023",
              "text": "I donâ€™t know, I havenâ€™t tried it yet. I have the impression that Claude Code is still sending data to Anthropic.\n\nYou can just use OpenCode with a cloud model (which is probably what 99% of people on this sub will do) if you want a â€œfree alternative.â€\n\nBut my goal was to show a fully open source and fully local solution, which is what I expect this sub to be about.",
              "score": 4,
              "created_utc": "2026-01-30 08:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kqwht",
                  "author": "Several-Tax31",
                  "text": "Yes, sending telemetry is why I didn't try Claude Code until now. I want full local solutions, both the model and the framework. If opencode gives comparable results to claude code with glm-4.7 flash, this is the news I was waiting. Thanks for demonstrating what is possible with full open solutions.",
                  "score": 2,
                  "created_utc": "2026-01-30 09:45:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kitnt",
                  "author": "raphh",
                  "text": "Makes sense. And I think you're right, that's probably what most people on this sub are about.  \n  \nTo give more context to my question:   \nI'm coming from using Claude Code to trying to go open source so at the moment I'm running the kind of setup described in my previous comment.   \n  \nI might have to give OpenCode a go to see how it compares to Claude Code in term of agentic workflow.",
                  "score": 1,
                  "created_utc": "2026-01-30 08:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kn8ei",
          "author": "Medium_Chemist_4032",
          "text": "Did the same yesterday. One shotted working Flappy Bird clone. After I asked to add the demo mode, it fumbled and started giving JS errors. Still haven't made it work correctly, but this quality for a local model is still impressive. I could see myself using it in real projects, if I had to",
          "score": 1,
          "created_utc": "2026-01-30 09:12:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2koh76",
              "author": "jacek2023",
              "text": "I am working with Python and C++. It's probably easier to handle these languages than JS? How is your code running?",
              "score": 1,
              "created_utc": "2026-01-30 09:23:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kokrg",
                  "author": "Medium_Chemist_4032",
                  "text": "Html, css, JS in browser",
                  "score": 1,
                  "created_utc": "2026-01-30 09:24:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2kx5fj",
          "author": "QuanstScientist",
          "text": "I have dedicated docker for OpenCode + vLLM for 5090: [https://github.com/BoltzmannEntropy/vLLM-5090](https://github.com/BoltzmannEntropy/vLLM-5090)",
          "score": 1,
          "created_utc": "2026-01-30 10:41:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nct5w",
              "author": "pfn0",
              "text": "> âœ… Zero-latency AI coding - OpenCode connects to vLLM via localhost\n\nInteresting, but why? You get the same \"zero-latency\" when separating into a separate docker container on the same host, but still maintain a good degree of componentization.",
              "score": 1,
              "created_utc": "2026-01-30 18:25:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2l5n61",
          "author": "SatoshiNotMe",
          "text": "I have tried all kinds of llama-server settings with GLM-4.7-flash + Claude Code but get an abysmal 3 tok/s on my M1 Pro Max MacBook 64GB, far lower than the 20 tps I can get with Qwen3-30B-A3B, using my setup here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nI donâ€™t know if thereâ€™s been a new build of llama-server that solves this. \nThe core problem seems to be that GLM's template has thinking enabled by default and Claude Code uses assistant prefill - they're incompatible.",
          "score": 1,
          "created_utc": "2026-01-30 11:50:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l5tu8",
              "author": "jacek2023",
              "text": "do you have current version of llama.cpp or old one? I posted opencode screenshot to show that thinking is not a problem at all in my setup, it's very efficient",
              "score": 2,
              "created_utc": "2026-01-30 11:52:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2l60zb",
                  "author": "SatoshiNotMe",
                  "text": "I tried it a few days ago, will retry today though Iâ€™m not getting my hopes up",
                  "score": 1,
                  "created_utc": "2026-01-30 11:53:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2lj83j",
                  "author": "SatoshiNotMe",
                  "text": "just tested again, now getting 12 tps, much better, but still around half of what I got with Qwen3-30B-A3B",
                  "score": 1,
                  "created_utc": "2026-01-30 13:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2m8l5w",
              "author": "ffyzz",
              "text": "Can you educate me on the benefits of llama.cpp vs running on MLX for this size of model on a Mac? Iâ€™ve been generally running MLX via LMStudio but am starting to wonder if the bigger diversity of quants and the llama.cpp system return better results (especially with tool calling) than MLX. Thank you sir â€” I am on a MBP M4 Max 64GB so playing in a similar sandbox to you.Â ",
              "score": 1,
              "created_utc": "2026-01-30 15:26:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mhx1m",
                  "author": "SatoshiNotMe",
                  "text": "Iâ€™ve never tried MLX and donâ€™t know the tradeoffs, so maybe I need to be educated lol",
                  "score": 1,
                  "created_utc": "2026-01-30 16:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2lvs2a",
          "author": "scottgl9",
          "text": "I haven't been able to get any local models to work very well with opencode, they typically fail to make tool calls, such as qwen3-coder, any suggestions? For glm-4.7-flash, I'm getting the error failed to initialize model: this model uses a weight format that is no longer supported.",
          "score": 1,
          "created_utc": "2026-01-30 14:25:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lyddr",
              "author": "jacek2023",
              "text": "You need to explain how you run the model",
              "score": 0,
              "created_utc": "2026-01-30 14:38:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mr95w",
          "author": "Various-Scallion1905",
          "text": "I tried GLM Flash 4.7 with ollama's claude code integration, i was okay i would say, it got confused with skills pretty regularly. Would llama cpp glm falsh be better with open code? Has anyone compared them?\n\nAlso looking forward to nemotron like models for coding which can have massive context with no speed or vram penalty. (i know recall might not be great but still)",
          "score": 1,
          "created_utc": "2026-01-30 16:50:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mtoln",
          "author": "kreigiron",
          "text": "GLM-4.7 Flash is the best right now for GPU Poor, I've been vibecoding some personal utilities with it and its interaction and output is comparable to Claude (use Anthropic at work)",
          "score": 1,
          "created_utc": "2026-01-30 17:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mutfo",
              "author": "jacek2023",
              "text": "I am not GPU poor, that's the point :)",
              "score": 1,
              "created_utc": "2026-01-30 17:06:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2niato",
          "author": "cafedude",
          "text": "Same but OpenCode + LMStudio + GLM-4.7 flash running on my 128GB Strix Halo box.",
          "score": 1,
          "created_utc": "2026-01-30 18:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2joegz",
          "author": "Sorry_Laugh4072",
          "text": "GLM-4.7 Flash is seriously underrated for coding tasks. The 200K context + fast inference makes it perfect for agentic workflows where you need to process entire codebases. Nice to see OpenCode getting more traction too - the local-first approach is the way to go for privacy-sensitive work.",
          "score": -2,
          "created_utc": "2026-01-30 04:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kjcg0",
              "author": "jacek2023",
              "text": "wow now I am experienced in detecting LLMs on reddit",
              "score": 8,
              "created_utc": "2026-01-30 08:36:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kk29q",
                  "author": "csixtay",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-01-30 08:43:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2kn92p",
                  "author": "themixtergames",
                  "text": "This is the issue with the Chinese labs, the astroturfing. It makes me not to trust their benchmarks.",
                  "score": 1,
                  "created_utc": "2026-01-30 09:12:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjota",
      "title": "I built a \"hive mind\" for Claude Code - 7 agents sharing memory and talking to each other",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "author": "Historical-Celery-83",
      "created_utc": "2026-01-26 15:49:13",
      "score": 300,
      "num_comments": 63,
      "upvote_ratio": 0.87,
      "text": "Been tinkering with multi-agent orchestration and wanted to share what came out of it.\n\n\n\n\\*\\*The idea\\*\\*: Instead of one LLM doing everything, what if specialized agents (coder, tester, reviewer, architect, etc.) could coordinate on tasks, share persistent memory, and pass context between each other?\n\n\n\n\\*\\*What it does\\*\\*:\n\n\\- 7 agent types with different system prompts and capabilities\n\n\\- SQLite + FTS5 for persistent memory (agents remember stuff between sessions)\n\n\\- Message bus for agent-to-agent communication\n\n\\- Task queue with priority-based coordination\n\n\\- Runs as an MCP server, so it plugs directly into Claude Code\n\n\\- Works with Anthropic, OpenAI, or Ollama\n\n\n\n\\*\\*The cool part\\*\\*: When the coder finishes implementing something, the tester can query the shared memory to see what was built and write appropriate tests. The reviewer sees the full context of decisions made. It's not magic - it's just passing data around intelligently - but it feels like they're actually collaborating.\n\n\n\n\\*\\*The not-so-cool part\\*\\*: Debugging 7 agents talking to each other is... an experience. Sometimes they work beautifully. Sometimes one agent keeps assigning tasks to itself in an infinite loop. You know, typical multi-agent stuff.\n\n\n\n\\*\\*Stack\\*\\*: TypeScript, better-sqlite3, MCP SDK, Zod\n\n\n\nNot enterprise-ready. Not trying to compete with anything. Just an experiment to learn how agent coordination patterns work.\n\n\n\nMIT licensed: [github.com/blackms/aistack](http://github.com/blackms/aistack)\n\n\n\nHappy to answer questions or hear how you're approaching multi-agent systems.\n\n",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o1v6rcq",
          "author": "Zc5Gwu",
          "text": "Why does this have 100+ upvotes but only one comment?",
          "score": 40,
          "created_utc": "2026-01-26 18:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yvghg",
              "author": "Semi_Tech",
              "text": "looks like another vibe coded program in Claude code + paid upvotes just to gain visibility :P",
              "score": 16,
              "created_utc": "2026-01-27 05:52:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1vcvkk",
              "author": "kevin_1994",
              "text": "this happens a lot in this sub",
              "score": 20,
              "created_utc": "2026-01-26 18:58:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1vfj85",
                  "author": "Historical-Celery-83",
                  "text": "annoying... I want challenge :)",
                  "score": -52,
                  "created_utc": "2026-01-26 19:09:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2405xg",
              "author": "drallcom3",
              "text": "He paid for upvotes to promote his Github.",
              "score": 4,
              "created_utc": "2026-01-27 23:10:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1v9imm",
              "author": "Historical-Celery-83",
              "text": "good question! comment it!",
              "score": -49,
              "created_utc": "2026-01-26 18:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ubcfn",
          "author": "robiinn",
          "text": "How does it differ from [bmad method](https://github.com/bmad-code-org/BMAD-METHOD) or something like that? Sounds very similar.",
          "score": 15,
          "created_utc": "2026-01-26 16:18:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ula9z",
              "author": "Historical-Celery-83",
              "text": "good question, first time I see that project, seems like is very Agile oriented. Mine is more about agent swarm",
              "score": -14,
              "created_utc": "2026-01-26 17:00:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v9igo",
          "author": "No_Afternoon_4260",
          "text": "The question is do they agree with each other?",
          "score": 9,
          "created_utc": "2026-01-26 18:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v9uzx",
              "author": "Historical-Celery-83",
              "text": "sometimes, and sometimes is a total mess. the orchestrator struggle to keep the agents on tracks, I'm trying to add more guardrails and more deterministic code.",
              "score": 10,
              "created_utc": "2026-01-26 18:45:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x9n7l",
                  "author": "Environmental-Metal9",
                  "text": "First of all, my dude, please edit away the LLMisms before posting. Itâ€™s grating to be fixing llm code all day just to come here and read more llm slop.\n\nAnd also, for cohesion, what is the actual disparity? Get the agents to agree on intent or on response? How many tokens can you waste per call?\n\nI havenâ€™t worked on anything like this EXACTLY, but Iâ€™ve worked on have to reach eventual consistency from agents running different models, and because of how my data was shaped, I was able to train a T5 model as classifier and would run that in front of every response. If the response didnâ€™t pass classification Iâ€™d just regenerate the response",
                  "score": 8,
                  "created_utc": "2026-01-27 00:14:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o207vp5",
          "author": "epyctime",
          "text": "\"some shit i started working on 3 days ago thats fully vibe coded slop so i can get clout on my github profile\"  \nedit: OP blocked me lol so i cant reply but this guys a fucking loon, 90 commits of claude code slop and its better than claude-flow \"The leading agent orchestration platform for Claude\" with 5.2k commits, but yeah, it's not fully implemented but yours is. \"your ignorance is the same as your ego\" then IMMEDIATELY commented \"probably better than 99% of the code that you will write in your entire life\" is the funniest thing ive ever read in my life. if I have access to opus we are coding the same my boy",
          "score": 5,
          "created_utc": "2026-01-27 12:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o240mak",
              "author": "drallcom3",
              "text": "The whole Github page reeks of someone trying to scam.",
              "score": 2,
              "created_utc": "2026-01-27 23:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o20luzm",
              "author": "Historical-Celery-83",
              "text": "probably better than 99% of the code that you will write in your entire life.",
              "score": -1,
              "created_utc": "2026-01-27 13:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vg7ug",
          "author": "JellyBean504",
          "text": "This is awesome, are you familiar with steve yegge's gastown? it seems very similar.",
          "score": 3,
          "created_utc": "2026-01-26 19:12:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vgvaq",
              "author": "Historical-Celery-83",
              "text": "no, but I'm looking at it right now, is more evolved compared to mine, I will take some idea from there. seems very interesting and more scalable compared to my solution.",
              "score": 3,
              "created_utc": "2026-01-26 19:15:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vxcfp",
          "author": "nonerequired_",
          "text": "That was exactly what I wanted to start building. What a coincidence!",
          "score": 3,
          "created_utc": "2026-01-26 20:26:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o207yzc",
              "author": "epyctime",
              "text": "it took op 2 days and like 90 commits to feel comfortable enough to submit this to the public. so just open claude code and start vibing man",
              "score": 1,
              "created_utc": "2026-01-27 12:40:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1whuve",
          "author": "Far-Low-4705",
          "text": "do you have any real performance benchmarks?",
          "score": 3,
          "created_utc": "2026-01-26 21:57:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wie4c",
              "author": "Historical-Celery-83",
              "text": "No, I started to write it like 2 days ago.",
              "score": 0,
              "created_utc": "2026-01-26 22:00:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wmwbw",
                  "author": "Far-Low-4705",
                  "text": "then how do you know that this even works?",
                  "score": 7,
                  "created_utc": "2026-01-26 22:21:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1xynxz",
                  "author": "jonas-reddit",
                  "text": "Maybe work on it a bit more than 2 days, I assume it was vibe coded as well, do some testing and then share when thereâ€™s a bit more to tell.",
                  "score": 2,
                  "created_utc": "2026-01-27 02:28:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w6xuu",
          "author": "segmond",
          "text": "Good job, not a new problem, microsoft first released solution for multi agent about 2 years or more.  I played a lot with it then.   Check it out, you might gain some new ideas\n\n[https://github.com/microsoft/autogen](https://github.com/microsoft/autogen)",
          "score": 6,
          "created_utc": "2026-01-26 21:09:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1we8q8",
          "author": "autodidacticasaurus",
          "text": "A few more iterations and we'll have created the Geth.\n\nIsn't that what DeepSeek does internally?",
          "score": 2,
          "created_utc": "2026-01-26 21:41:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1werol",
              "author": "Historical-Celery-83",
              "text": "yes, also claude code, but at least I can have control here. and use multi model for different tasks.",
              "score": 2,
              "created_utc": "2026-01-26 21:44:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wfyaj",
                  "author": "autodidacticasaurus",
                  "text": "I think that's cool. Also, it's easier to distribute across multiple cards or machines. Much more flexible.",
                  "score": 1,
                  "created_utc": "2026-01-26 21:49:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1wcdt9",
          "author": "__Maximum__",
          "text": "Why claude code? For fucks sake, why? There are open source alternatives that you can contribute to.",
          "score": 4,
          "created_utc": "2026-01-26 21:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1yis59",
              "author": "rocketmonkeys",
              "text": "I'm just getting into things, what do you use?  Opencode?",
              "score": 1,
              "created_utc": "2026-01-27 04:24:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zgggt",
                  "author": "__Maximum__",
                  "text": "Opencode, openhands, vibe even gemini cli is open source.",
                  "score": 1,
                  "created_utc": "2026-01-27 08:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1wu7sz",
              "author": "Historical-Celery-83",
              "text": "claude code can be used but it supports more providers, I choosed claude code because is one of my favourite. But I would like to be agnostic in the future. Already working on it.",
              "score": 1,
              "created_utc": "2026-01-26 22:55:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zgdjj",
                  "author": "__Maximum__",
                  "text": "It supports more providers than opencode?",
                  "score": 2,
                  "created_utc": "2026-01-27 08:51:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1x4o0x",
          "author": "the_ai_wizard",
          "text": "Cool diy. This approach has been tried and has failure modes just like you have experienced. It sounds brilliant but so far ultimately goes nowhere. Maybe this will be interesting to you:\n\nhttps://neurips.cc/virtual/2024/106556",
          "score": 1,
          "created_utc": "2026-01-26 23:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1xfdij",
          "author": "foundrynet",
          "text": "This is the kind of infrastructure the agent space needs. Most multi-agent demos fall apart because there's no good way for agents to coordinate and remember state.\n\nOne thing I've been exploring: what if agents could earn/spend resources for completing tasks? Creates natural prioritization where high value work gets done first.",
          "score": 1,
          "created_utc": "2026-01-27 00:43:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215hx5",
          "author": "fugogugo",
          "text": "I just watched this video today and it looks like similar situation lol\n\nhttps://youtu.be/U7s_CaI93Mo?si=_eGWY6QG9Ni7Qyr7",
          "score": 1,
          "created_utc": "2026-01-27 15:35:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21twyh",
          "author": "PhotographerUSA",
          "text": "So, now no more programming mistakes on first try?",
          "score": 1,
          "created_utc": "2026-01-27 17:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22b67b",
          "author": "isopropoflexx",
          "text": "I am curious to find out how you are handling multiple agents accessing the SQLite database file concurrently? I personally love SQLite as a quick and easy way to incorporate a db into small/simple projects, as it doesn't need a \"proper\" service of its own to run. But... SQLite is not designed with concurrency in mind (same as any other \"flat file\" type storage solution), and having multiple concurrent connections typically results in errors due to db/table locks etc.",
          "score": 1,
          "created_utc": "2026-01-27 18:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e4d0s",
              "author": "Historical-Celery-83",
              "text": "Indeed I'm moving to postgres.",
              "score": 2,
              "created_utc": "2026-01-29 11:17:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ypit6",
          "author": "SeleneGardenAI",
          "text": "The voting patterns here are wild sometimes - posts with genuinely interesting architectures get buried while others mysteriously rocket up with no discussion. It's frustrating when you want to dig into the technical details.\n\nI've been experimenting with multi-agent memory sharing lately, and the biggest challenge I've found isn't the inter-agent communication itself, but preventing memory contamination between agents with different roles. When agents share context, they tend to blur their distinct personalities and functions over time. What's your approach to maintaining agent identity boundaries while still allowing meaningful information exchange? Are you using separate embedding spaces for shared vs. private memories, or handling it at the prompt level?\n\nThe \"hive mind\" concept is fascinating because it mirrors how human teams actually work - shared context with specialized roles. But LLMs weren't really designed for this kind of persistent, multi-perspective memory architecture. Would love to hear more about your implementation details, especially how you're handling memory prioritization when the shared context gets large.",
          "score": 1,
          "created_utc": "2026-01-27 05:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z5ynx",
              "author": "Historical-Celery-83",
              "text": "    Great questions - these are exactly the things I've been wrestling with.\n    \n    On memory contamination / identity boundaries: my current approach is pretty simple, honestly. Namespaces + prompt-level isolation. Each agent has a strong system prompt defining its role and capabilities, and they access shared memory via namespace filtering. The key insight is that agents don't actually \"retain\" personality across calls - they're stateless. What persists is the memory content, not the agent's perspective on it.\n    \n    So when the coder stores something like memory_store(\"auth-decision\", \"Using JWT with refresh tokens\", {namespace: \"architecture\"}), the reviewer later queries this and interprets it through its own system prompt lens. There's no embedding space separation - it's the same FTS5 index. The \"identity boundary\" is entirely at the prompt level.\n    \n    Is this ideal? Probably not. But it sidesteps the contamination problem because agents don't actually share state - they share facts. The interpretation happens fresh each time.\n    \n    On memory prioritization: hybrid approach. FTS5 with BM25 ranking for keyword relevance, optional vector search (OpenAI/Ollama embeddings) for semantic similarity, then merge with deduplication where vector results come first (higher quality), then FTS backfill. The \"prioritization\" is really just semantic match > keyword match > recency as tiebreaker.\n    \n    For large contexts I'm honestly just relying on the LLM's attention mechanism. I send relevant search results, not the entire memory. If context gets too big, that's a sign the search query wasn't specific enough.\n    \n    What I'd do differently if this were production: per-agent memory scopes (private + shared pools), decay functions for old memories, explicit \"handoff\" objects when one agent passes to another. But for an experiment, the current approach works well enough to see the patterns emerge. The agents do coordinate surprisingly well when you give them structured ways to leave breadcrumbs for each other.\n    \n    What approaches are you exploring for the contamination problem?",
              "score": -1,
              "created_utc": "2026-01-27 07:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1vmotm",
          "author": "Magnus_Forsling",
          "text": "The infinite loop problem you mentioned is fascinating â€” it's basically the multi-agent equivalent of an LLM talking itself in circles. Have you experimented with any circuit-breaker patterns? Something like tracking task lineage so an agent can't pick up a task that descended from one it created?\n\nThe SQLite+FTS5 choice for shared memory is smart. Curious if you've hit scaling issues as the memory grows, or if the FTS5 indexing keeps retrieval snappy even with longer sessions.",
          "score": -4,
          "created_utc": "2026-01-26 19:40:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vsfq9",
              "author": "Historical-Celery-83",
              "text": "No, you mean an anti loop system? or something more evolved?",
              "score": 0,
              "created_utc": "2026-01-26 20:05:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1w6lii",
                  "author": "Magnus_Forsling",
                  "text": "Bit of both, actually. A basic anti-loop system catches the obvious stuff â€” \"agent A spawned task B which spawned task C which looks suspiciously like A again.\" But the more interesting patterns are subtler:\n\n1. **Semantic drift detection** â€” task descriptions mutate slightly each iteration until they're technically different but functionally identical. You'd want embedding similarity checks against the task's ancestors, not just string matching.\n\n2. **Resource exhaustion triggers** â€” if an agent has touched >N files or made >M API calls without producing a deliverable, something's probably wrong. Hard to tune but catches the \"productively going nowhere\" loops.\n\n3. **Consensus checkpoints** â€” for high-stakes tasks, require a different agent to sign off before the originating agent's subtasks can spawn their own children. Adds latency but breaks the self-reinforcing cycles.\n\nThe lineage tracking is the foundation though. Without knowing *where* a task came from, you can't catch any of these patterns. Are you storing that in the SQLite schema already?",
                  "score": -3,
                  "created_utc": "2026-01-26 21:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qoa8rp",
      "title": "The Qwen Devs Are Teasing Something",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/umvks92vcvfg1.png",
      "author": "Few_Painter_5588",
      "created_utc": "2026-01-27 10:28:56",
      "score": 300,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o21p3m0",
          "author": "rm-rf-rm",
          "text": "Thread locked - announcement is now out: https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/",
          "score": 1,
          "created_utc": "2026-01-27 17:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zvrfc",
          "author": "rerri",
          "text": "Z-Image. It's been popping up in ComfyUI PR's in recent days and then today there's this update with a hidden item added to collection:\n\nhttps://preview.redd.it/3d4oy6wxjvfg1.jpeg?width=780&format=pjpg&auto=webp&s=5c0f222d04f42b920dce8929e666f0158a6b9840",
          "score": 59,
          "created_utc": "2026-01-27 11:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zxdt0",
              "author": "Illya___",
              "text": "The question is which one though, base? edit? All of them?\n\nIs that apparent from the PRs?",
              "score": 20,
              "created_utc": "2026-01-27 11:23:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1zyq95",
                  "author": "rerri",
                  "text": "Base T2I workflow was just added to templates today. \"Omni\" with edit capabilities has been worked on earlier.\n\nI think the template addition hints at base releasing today but whether the same model also has edit capabilities or whether that's something coming later, I'm not sure.",
                  "score": 10,
                  "created_utc": "2026-01-27 11:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zu1ri",
          "author": "nikhilprasanth",
          "text": "Most likely the Z image base",
          "score": 34,
          "created_utc": "2026-01-27 10:55:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2059t7",
          "author": "xandep",
          "text": "Qwen4 Next 48B A3B. I'm sure. ðŸ¥¹",
          "score": 21,
          "created_utc": "2026-01-27 12:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20yhei",
              "author": "Available-Craft-5795",
              "text": "The full Qwen 4 series would be nice. Its crazy how good they got the 0.6B mode to be.",
              "score": 10,
              "created_utc": "2026-01-27 15:02:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o208imc",
              "author": "[deleted]",
              "text": "I hope it's, We need something deployable that can beat 4.7 flash, the 4.7 flash is a great model but a new Qwen moe that can beat it will be great!",
              "score": 6,
              "created_utc": "2026-01-27 12:44:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o215smz",
              "author": "KittyPigeon",
              "text": "Would love for that to come out",
              "score": 1,
              "created_utc": "2026-01-27 15:36:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21nk9m",
              "author": "Far-Low-4705",
              "text": "qwen 4vl 80b  \n  \n\\- this is my dream rn lol  \n\\* 80b sparse moe (perfect for my rig at Q4)  \n\\* rly good vision (if its anything like qwen 3vl, thats already good enough for me)  \n\\* better long context performance (as seen in qwen 3 next, hopefully carries over to qwen 4)  \n\\* interleaved thinking (hopefully)",
              "score": 1,
              "created_utc": "2026-01-27 16:53:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zryyl",
          "author": "ResidentPositive4122",
          "text": "Someone said something a while ago, about the chinese new year. Google says it's on the 17th this year, so it would make sense that a lot of labs want to get things out before the break. K2.5 is out, hopefully we'll get q3.5, dsv4, mm2.2 and so on.",
          "score": 22,
          "created_utc": "2026-01-27 10:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ztsvf",
              "author": "Few_Painter_5588",
              "text": "There's also some serious competition between these labs. So it makes sense they all want to get something out. \n\nQwen3 Max Thinking, Kimi K2.5.",
              "score": 7,
              "created_utc": "2026-01-27 10:53:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zwwlw",
          "author": "robberviet",
          "text": "Qwen 3.5 would be awesome. It's 6 months (?) already.",
          "score": 5,
          "created_utc": "2026-01-27 11:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ztqc3",
          "author": "Antique_Dot_5513",
          "text": "This is the image base normally, since the flow arrived on comfyui",
          "score": 10,
          "created_utc": "2026-01-27 10:52:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zv79b",
              "author": "Odd-Ordinary-5922",
              "text": "im 99% sure its the next qwen3next lineup (nvm edit: considering its tongyi lab its probably z image turbo edit",
              "score": 3,
              "created_utc": "2026-01-27 11:05:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zr5nl",
          "author": "Few_Painter_5588",
          "text": "Another tweet of interest\n\nhttps://preview.redd.it/nyc5fno6dvfg1.png?width=1194&format=png&auto=webp&s=224444e21cdee679358405965b8647dd5bd40824",
          "score": 16,
          "created_utc": "2026-01-27 10:30:22",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1zrpt5",
              "author": "MidAirRunner",
              "text": "Qwen3.5 30b A3b pls ty.",
              "score": 28,
              "created_utc": "2026-01-27 10:35:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20hoqb",
                  "author": "ahmetegesel",
                  "text": "That flash emoji put my hopes high",
                  "score": 5,
                  "created_utc": "2026-01-27 13:37:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zuwki",
          "author": "GreenGreasyGreasels",
          "text": "I am so over teasers and vague posting. When the release cadence was slow this was exciting, but now with new jaw dropping models dropping every weekday and and twice on Sundays this is getting tiresome and exhausting. Be more like Moonshot and Kimi K2.5 I guess. Anyone else feel the same? Or are you guys enjoying this?",
          "score": 11,
          "created_utc": "2026-01-27 11:02:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zva91",
              "author": "Odd-Ordinary-5922",
              "text": "im just happy that we are getting free models",
              "score": 28,
              "created_utc": "2026-01-27 11:05:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1zvvhg",
              "author": "FullOf_Bad_Ideas",
              "text": "I agree it's good to filter out teases. Marketing is cheap, R&D is not, and China has a hustle B2C marketing culture.\n\nIf you sign up for Alibaba Cloud you'll have people contacting you pretty quickly. If you set up Google Cloud, AWS or Azure, you'll never be able to reach a real human being without putting down a lot of money first. They are serious about direct contact with customers.",
              "score": 3,
              "created_utc": "2026-01-27 11:10:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zutd7",
          "author": "AdventurousSwim1312",
          "text": "Qwen Next update?",
          "score": 2,
          "created_utc": "2026-01-27 11:02:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zxtqz",
          "author": "NoYogurtcloset4090",
          "text": "Just Z-image new models.",
          "score": 1,
          "created_utc": "2026-01-27 11:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zz2ed",
          "author": "Fast-Double-8915",
          "text": "Stand with legs apart?Â ",
          "score": 1,
          "created_utc": "2026-01-27 11:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202h90",
          "author": "Time-Teaching1926",
          "text": "I hope it's soon and not just another tease so we won't be waiting a long time again. ðŸ˜­",
          "score": 1,
          "created_utc": "2026-01-27 12:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2045br",
          "author": "Guilty_Rooster_6708",
          "text": "Z-image edit please",
          "score": 1,
          "created_utc": "2026-01-27 12:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208nxv",
          "author": "derivative49",
          "text": "these people won't rest unless they pop some bubble somewhere",
          "score": 1,
          "created_utc": "2026-01-27 12:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20bnfv",
          "author": "No_Conversation9561",
          "text": "Itâ€™s tweeted by Tongyi Lab so probably Z-Image base and edit ðŸ¤ž",
          "score": 1,
          "created_utc": "2026-01-27 13:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20ghky",
          "author": "swagonflyyyy",
          "text": "Its not a new model! Its the best model! You're not wrong! You're just in the dark!",
          "score": 1,
          "created_utc": "2026-01-27 13:31:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o215v6d",
          "author": "pigeon57434",
          "text": "if it was something to do with Qwen they would just post it on their alibaba\\_qwen account",
          "score": 1,
          "created_utc": "2026-01-27 15:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20kwck",
          "author": "Doct0r0710",
          "text": "I hate this teasing wankery... Why must they act like all the crypto bros combined?",
          "score": 1,
          "created_utc": "2026-01-27 13:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o207sdw",
          "author": "[deleted]",
          "text": "I think GTA 6 is gonna come out before the new Qwen 30B MOE",
          "score": 1,
          "created_utc": "2026-01-27 12:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zv0d8",
          "author": "Minute_Attempt3063",
          "text": "Could it be, that they are cooking up a model that is uncensored (more then normal) and is way better at coding then Claude?\n\nOr a Deepseek-Qwen model?",
          "score": -3,
          "created_utc": "2026-01-27 11:03:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zyzq8",
          "author": "darkpigvirus",
          "text": "how about Qwen3.5 1B A350M - Thinking? Easy to train will take only less than your day Qwen Team",
          "score": -1,
          "created_utc": "2026-01-27 11:36:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr7ncz",
      "title": "Design Arena is now dominated by an open model",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qr7ncz",
      "author": "moks4tda",
      "created_utc": "2026-01-30 14:55:35",
      "score": 288,
      "num_comments": 38,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2pm1hu",
          "author": "TheRealGentlefox",
          "text": "By \"dominated\" you mean it ties with Gemini?",
          "score": 12,
          "created_utc": "2026-01-31 01:06:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m4m2f",
          "author": "JackStrawWitchita",
          "text": "Kimi has been my online go-to LLM for weeks now. Haven't used chatgpt at all and only use gemini every now and then. I used to just visit kimi every now and then but their big models are amazing.\n\nI just wish I had the local horsepower to run their local models.",
          "score": 33,
          "created_utc": "2026-01-30 15:08:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3700",
          "author": "GenLabsAI",
          "text": "Let me just add that Kimi K2.5 came out less than a week ago. If you know how ELO ratings work, you know.  \n(don't get me wrong, it's still pretty goodl)",
          "score": 50,
          "created_utc": "2026-01-30 15:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mq45h",
              "author": "-p-e-w-",
              "text": "Elo-type ratings come with an associated confidence parameter (called the K-factor in chess) that makes them statistically sound *regardless* of how many pairings have been evaluated. Itâ€™s even possible to express this in the form of a score interval, which e.g. LMArena does.\n\nThe idea that the ratings for new models somehow arenâ€™t valid is just plain incorrect. If anything, the ratings for such models tend to be underestimated relative to their true performance, because they are initialized to some (low) baseline and have to rise from there.",
              "score": 23,
              "created_utc": "2026-01-30 16:45:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nfdex",
                  "author": "SlowFail2433",
                  "text": "Yes absolutely, the mathematics of ELO systems handles this issue implicitly. Had to learn this for Chess reasons lol",
                  "score": 16,
                  "created_utc": "2026-01-30 18:36:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2nyljx",
                  "author": "GenLabsAI",
                  "text": "BUT... nobody actually is reading the K-factor/confidence",
                  "score": 4,
                  "created_utc": "2026-01-30 20:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2oy6bu",
              "author": "RuthlessCriticismAll",
              "text": "> If you know how ELO ratings work, you know.\n\nYou don't.",
              "score": 7,
              "created_utc": "2026-01-30 22:55:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2oyayh",
                  "author": "GenLabsAI",
                  "text": "Maybe",
                  "score": -4,
                  "created_utc": "2026-01-30 22:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m42c7",
          "author": "Distinct-Expression2",
          "text": "arena rankings shuffle every time a new model drops. more interesting is whether open models can hold the top spot for more than a week before the next closed model update.",
          "score": 6,
          "created_utc": "2026-01-30 15:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3yan",
          "author": "SuddenEmu9792",
          "text": "What is this model designed for?",
          "score": 5,
          "created_utc": "2026-01-30 15:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc04z",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 7,
          "created_utc": "2026-01-30 15:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2mlg8v",
              "author": "No_Afternoon_4260",
              "text": "They also have good models, so.. ðŸ¤·",
              "score": 9,
              "created_utc": "2026-01-30 16:24:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2mpwpp",
                  "author": "AdSouth4334",
                  "text": "The models that are only good on paper but breaks apart the moment you give it something half-complex as something that Claude can solve in one-shot.  \n  \nJust like Gemini 3, it's a model optimized for benchmarks only, but it has zero reliability on real-world tasks",
                  "score": -9,
                  "created_utc": "2026-01-30 16:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2mm6dx",
              "author": "vasileer",
              "text": "even if it is marketing: is the information correct? are they #1 on design arena?\n\nif so, then I see no problems",
              "score": 3,
              "created_utc": "2026-01-30 16:27:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m3s6j",
          "author": "Dr_Kel",
          "text": "Pretty cool, but... What does *design*Arena test?\n\nUI layout? Clothes/costumes? Building interiors? Database schemas? There's so much that can be described as \"design\", not the best name for a benchmark!",
          "score": 10,
          "created_utc": "2026-01-30 15:04:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m78ac",
              "author": "Charuru",
              "text": "Just go look at the website? It clearly has filters for all the categories. https://www.designarena.ai/leaderboard",
              "score": 10,
              "created_utc": "2026-01-30 15:20:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2m8xom",
                  "author": "Dr_Kel",
                  "text": "How come that in every design category Kimi K2.5 is below the first place, but in \"All Categories\" it's #1?",
                  "score": 9,
                  "created_utc": "2026-01-30 15:28:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2oi5l7",
          "author": "crantob",
          "text": "This benchmaxxing depresses me when I get more intelligent behavior out of qwen3-235b than GLM 4.7 in iterative project development (no agentic).\n\nGLM4.7 \"Oh that function was important to the program and it won't compile without it?  Seemed too much bother to me to keep it, sorry about that.  Here's the program with important_thing() restored.\"\n\n<code>\n\n[Forgets a different thing]",
          "score": 2,
          "created_utc": "2026-01-30 21:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qsu6z",
              "author": "ThatRandomJew7",
              "text": "Unironically, Kimi is one of the few open models that actually appears to be as good as the benchmarks suggest. \n\nI'm actually considering replacing Gemini 3 Pro with it",
              "score": 5,
              "created_utc": "2026-01-31 05:41:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mv9sa",
          "author": "Dependent-Example930",
          "text": "How are most people using kimi k2.5? What service?",
          "score": 2,
          "created_utc": "2026-01-30 17:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2nyo87",
              "author": "GenLabsAI",
              "text": "Kimi, OpenRouter.",
              "score": 4,
              "created_utc": "2026-01-30 20:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2oijqz",
              "author": "synn89",
              "text": "Fireworks.ai",
              "score": 1,
              "created_utc": "2026-01-30 21:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2p7hpv",
                  "author": "Turbulent_Pin7635",
                  "text": "LM Studio =)",
                  "score": 3,
                  "created_utc": "2026-01-30 23:46:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2nxpri",
              "author": "IrisColt",
              "text": "P-perplexity?",
              "score": -1,
              "created_utc": "2026-01-30 19:59:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2qskrc",
                  "author": "ThatRandomJew7",
                  "text": "Doesn't have K2.5 yet, sadly",
                  "score": 0,
                  "created_utc": "2026-01-31 05:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rplyv",
          "author": "Fast-Satisfaction482",
          "text": "In the lead by a few points in a plot without error bars is definitely not \"domination\". It's inconclusive at best.",
          "score": 1,
          "created_utc": "2026-01-31 10:39:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tk4mv",
          "author": "Relevant-Service9871",
          "text": "Comment on utilise cette ia",
          "score": 1,
          "created_utc": "2026-01-31 17:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m481c",
          "author": "jacek2023",
          "text": "Ok let's wait for the bots to upvote",
          "score": -8,
          "created_utc": "2026-01-30 15:06:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2slyma",
          "author": "LocoMod",
          "text": "Step 1: Ask the model to place their initials at the bottom of the page.\n\nStep 2: Vote for the motherland model.\n\nStep 3: ???\n\nStep 4: Profit!\n\n  \nIt is easy to game this and pump your model to the top. Doesn't take many since its not a super high traffic site.",
          "score": 0,
          "created_utc": "2026-01-31 14:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m4jb1",
          "author": "DrummerPrevious",
          "text": "Glm actually sucks",
          "score": -8,
          "created_utc": "2026-01-30 15:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2macbm",
              "author": "sleepy_roger",
              "text": "Meh, GLM has been my go to design model for a while now. It's made some great designs with easier prompts than I've seen Claude do.",
              "score": 12,
              "created_utc": "2026-01-30 15:34:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2mle8e",
              "author": "TokenRingAI",
              "text": "It's great for UI work",
              "score": 5,
              "created_utc": "2026-01-30 16:24:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2nlvhn",
              "author": "fabricio3g",
              "text": "I find it very useful for finding bugs and analyzing code",
              "score": 4,
              "created_utc": "2026-01-30 19:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}