{
  "metadata": {
    "last_updated": "2026-01-18 16:49:53",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 570,
    "file_size_bytes": 590872
  },
  "items": [
    {
      "id": "1qe2i88",
      "title": "My story of underestimating /r/LocalLLaMA's thirst for VRAM",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/lwod7dtv7mdg1.jpeg",
      "author": "EmPips",
      "created_utc": "2026-01-16 01:36:54",
      "score": 1226,
      "num_comments": 86,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzw5ct1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-16 09:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuhba5",
          "author": "TheSilverSmith47",
          "text": "https://preview.redd.it/24ofdr5hfmdg1.png?width=2246&format=png&auto=webp&s=3287cf721e2befdf66aa74227fe67ae74657f1ba",
          "score": 417,
          "created_utc": "2026-01-16 02:18:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzv71v1",
              "author": "Kerem-6030",
              "text": "actully true",
              "score": 58,
              "created_utc": "2026-01-16 04:51:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzwg0xx",
              "author": "creamyhorror",
              "text": "Great, huh? It causes prices to go up, and since prices tend to be sticky upwards, later on they don't come down much even if the buying slows. Wow!",
              "score": 19,
              "created_utc": "2026-01-16 11:10:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzzoves",
              "author": "Mickenfox",
              "text": "We call this price discovery.",
              "score": 2,
              "created_utc": "2026-01-16 20:55:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04c36o",
                  "author": "Sl33py_4est",
                  "text": "well can you not",
                  "score": 0,
                  "created_utc": "2026-01-17 15:38:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzukjk1",
          "author": "a__new_name",
          "text": "You know how the California gold rush started? The man who found gold did not tell other people about it. First he bought all the shovels, wheelbarrows, sluice boxes and other prospecting equipment in a large radius. THEN he told people about gold.",
          "score": 359,
          "created_utc": "2026-01-16 02:37:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzur1mf",
              "author": "EmPips",
              "text": "Were this man born in our day and age he would be in my shoes but proudly owning two w6800's instead of a lonely one.",
              "score": 154,
              "created_utc": "2026-01-16 03:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzvcwzw",
                  "author": "Illeazar",
                  "text": "Or *all* the w6800s",
                  "score": 48,
                  "created_utc": "2026-01-16 05:31:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzw1k4j",
              "author": "Competitive_Ad_5515",
              "text": "The California Gold Rush started in January 1848 when James W. Marshall discovered gold at John Sutter's sawmill in Coloma, but it was merchant Samuel Brannan who truly ignited the rush; he first bought all the available mining supplies (shovels, pans, etc.) in the Bay Area and then famously paraded gold flakes through San Francisco shouting \"Gold!\" to drive demand, becoming California's first millionaire by selling tools rather than digging for gold himself.",
              "score": 58,
              "created_utc": "2026-01-16 08:59:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzx1pbl",
                  "author": "L3g3nd8ry_N3m3sis",
                  "text": "There‚Äôs a lesson here",
                  "score": 22,
                  "created_utc": "2026-01-16 13:36:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o00iudz",
                  "author": "tyty657",
                  "text": "Advanced scalping",
                  "score": 3,
                  "created_utc": "2026-01-16 23:23:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzvuk6p",
              "author": "arm2armreddit",
              "text": "what jacket he was wearing?",
              "score": 17,
              "created_utc": "2026-01-16 07:55:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuayyz",
          "author": "EmPips",
          "text": "(if anyone wanted my take, this card is amazing, but at current prices either get 3090's or just spring for an R9700 if the blower-cooler and VRAM-per-slot is important! And if you're okay with high idle power and external cooling ignore all of this and stack mi50x's)",
          "score": 83,
          "created_utc": "2026-01-16 01:43:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "nzul1gf",
              "author": "Marksta",
              "text": "I think even for $500 w6800 price isn't super attractive. I'm spoiled with $160 mi50s though. P40s are <=$200 now, that's probably what I'd look at for stacking cheap vram.",
              "score": 24,
              "created_utc": "2026-01-16 02:39:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzuqmaj",
                  "author": "EmPips",
                  "text": "Can you still find them for $160ish? They were $250ish while I was looking.\n\nI made a post comparing the two options a while ago. I'm glad I picked the w6800 but can definitely still see the case for the Mi50x. Depends on what you're after.",
                  "score": 9,
                  "created_utc": "2026-01-16 03:10:37",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzuuexq",
                  "author": "nonaveris",
                  "text": "What‚Äôs with the mi50s that make them good for the dollar despite being limited to ROCm 6?",
                  "score": 6,
                  "created_utc": "2026-01-16 03:32:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzuu7tv",
              "author": "nonaveris",
              "text": "The R9700 is quite nice and works well, but does need optimizations to really use its memory well.",
              "score": 9,
              "created_utc": "2026-01-16 03:31:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzuyxid",
                  "author": "UniversalSpermDonor",
                  "text": "What optimizations are you referring to? I'm setting up my system now, so it'd be a big help!",
                  "score": 2,
                  "created_utc": "2026-01-16 03:59:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzw6pec",
              "author": "inaem",
              "text": "R9700: I guess I am worth x2 now",
              "score": 1,
              "created_utc": "2026-01-16 09:47:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuhve2",
          "author": "Apprehensive_Use1906",
          "text": "I got one last month. Might sell it after a few more posts.",
          "score": 47,
          "created_utc": "2026-01-16 02:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuopvz",
          "author": "redditorialy_retard",
          "text": "got a 3090. problem is I ain't got a pc and only a laptop, gonna be a couple months before I get the PC for 3090",
          "score": 15,
          "created_utc": "2026-01-16 03:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvh7s4",
              "author": "Mythril_Zombie",
              "text": "Does it have a thunderbolt port? You can get a egpu frame and a PSU for way less than a PC.",
              "score": 16,
              "created_utc": "2026-01-16 06:03:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvv6tv",
                  "author": "redditorialy_retard",
                  "text": "I have PSU, motherboard (8gigs of ram cuz yknow). My laptop got an ordinary C port so gotta wait to finish the PC",
                  "score": 4,
                  "created_utc": "2026-01-16 08:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwhnl0",
              "author": "braydon125",
              "text": "Feel free to sell the 3090 to my small humble lab where...we do science!",
              "score": 3,
              "created_utc": "2026-01-16 11:23:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwibv6",
                  "author": "redditorialy_retard",
                  "text": "does this science involve shooting virtual guns at people online and calling them slurs?¬†",
                  "score": 1,
                  "created_utc": "2026-01-16 11:28:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzv976h",
          "author": "sanjibukai",
          "text": "w6800 is it actually a Radeon RX6800?\n\nI'm interested to take part in increasing the price...",
          "score": 8,
          "created_utc": "2026-01-16 05:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvbkn2",
              "author": "EmPips",
              "text": "Yes, if VRAM isn't a constraint it performs exactly like an Rx 6800 in every use-case I throw at it (I also own a regular Rx 6800 in the same rig).\n\nThere's some benefits though outside of the obvious double-VRAM. The w6800 idles at like 10-14 watts per rocm-smi and peak power draw during prompt processing is a far bit lower (like 25-30watts lower) than the regular Rx 6800, the blower cooler is great, and if I ever feel like adding 5 extra displays I guess it's there for me.",
              "score": 5,
              "created_utc": "2026-01-16 05:22:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzvbzkg",
                  "author": "sanjibukai",
                  "text": "Thanks for the details!\nBecause I can have access to an RX6800 with 16Gb but never thought of using it for AI.. As I always assumed only CUDA (aka Nvidia) cards were working for AI stuffs (which I always found silly as well)",
                  "score": 5,
                  "created_utc": "2026-01-16 05:25:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvy8nu",
          "author": "Loosemofo",
          "text": "Hey, at least you know people read what you wrote about üëç",
          "score": 7,
          "created_utc": "2026-01-16 08:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzus3r1",
          "author": "Bonzupii",
          "text": "Hey man, at least you're not a gatekeeper",
          "score": 12,
          "created_utc": "2026-01-16 03:18:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuu69u",
              "author": "EmPips",
              "text": "That's a consolation prize that doesn't even eat up a PCIe slot.",
              "score": 44,
              "created_utc": "2026-01-16 03:30:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzuupbi",
                  "author": "Bonzupii",
                  "text": "Refurbished/used P100s are cheap and decent cards (if not a little ancient) but they come only in 12gb or 16gb variants. \nHopefully that's a better consolation prize that fills that slot ü•≤ I just seen one online for like 200 bucks gogogo",
                  "score": 6,
                  "created_utc": "2026-01-16 03:33:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuh9f7",
          "author": "EuphoricPenguin22",
          "text": "Sort of like those Instinct cards that went through the roof.",
          "score": 3,
          "created_utc": "2026-01-16 02:18:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvwk5a",
          "author": "PraxisOG",
          "text": "The tidbit people are missing is that the AMD V620 is the same card but for server use, and it‚Äôs like $450 on eBay¬†",
          "score": 3,
          "created_utc": "2026-01-16 08:13:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzw2p2n",
              "author": "deb0ro",
              "text": "That cards does not have visible display output but it seems there is one hidden and upon flashing with W6800 BIOS, the port will work but it will downgrade the GPU to W6800 specs [https://www.techpowerup.com/forums/threads/can-the-display-output-be-enabled-by-modifying-the-vbios-of-the-radeon-pro-v620.325549/](https://www.techpowerup.com/forums/threads/can-the-display-output-be-enabled-by-modifying-the-vbios-of-the-radeon-pro-v620.325549/)",
              "score": 1,
              "created_utc": "2026-01-16 09:10:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwbfew",
                  "author": "KontoOficjalneMR",
                  "text": "Doesn't really need visible display if it's going to be used for AI in a server",
                  "score": 6,
                  "created_utc": "2026-01-16 10:30:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzwakvu",
                  "author": "SilentLennie",
                  "text": "Why do you need a display output ?",
                  "score": 3,
                  "created_utc": "2026-01-16 10:22:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzyk93u",
                  "author": "Majinsei",
                  "text": "For that, you have your standard CPU video output or a gaming GPU, while using the W6800 for server purposes only, which is what matters in this sub~",
                  "score": 2,
                  "created_utc": "2026-01-16 17:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzx8vhr",
          "author": "Danternas",
          "text": "I was shocked to see prices on the 32gb mi50 now. I got mine for $250 a few months back.\n\n\nEven more so because I've found it to be a pain in the ass to get running with ROCm and Ollama. Fortunately it performs well on Vulkan and llama.cpp.¬†",
          "score": 3,
          "created_utc": "2026-01-16 14:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvlw2j",
          "author": "pixelpoet_nz",
          "text": "Wait until the normies catch on to how good Strix Halo is and how the prices have remained fairly stable even though it has 128GB of LPDDR5 and a ridiculously powerful CPU. Nvidia guys can enjoy their derpy overpriced Spark lol\n\nFortunately I didn't make the mistake of telling people about it before I'd bought enough of it ;)",
          "score": 6,
          "created_utc": "2026-01-16 06:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx2bnd",
              "author": "BloodyLlama",
              "text": "Stable?  The Framework desktop went up $500 like last week.",
              "score": 3,
              "created_utc": "2026-01-16 13:40:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzwo0ln",
              "author": "cms2307",
              "text": "Everyone knows about strix halo, the problem is ram prices. You‚Äôll pay more for 128gb of ram than the strix halo board itself.",
              "score": -1,
              "created_utc": "2026-01-16 12:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzx2gna",
                  "author": "BloodyLlama",
                  "text": "It's soldered on.  The price you pay already includes that 128GB.",
                  "score": 4,
                  "created_utc": "2026-01-16 13:40:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuorwl",
          "author": "My_Unbiased_Opinion",
          "text": "That's why I always buy what I need first then post about it. Then sell later for profit /s",
          "score": 3,
          "created_utc": "2026-01-16 03:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw0r1g",
          "author": "FullOf_Bad_Ideas",
          "text": "can a single reddit post actually move this much stock and prices?\n\nyou should probably get in bed with sellers to do some promos. \n\nCommision will net you a few GPUs.",
          "score": 2,
          "created_utc": "2026-01-16 08:52:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwdx4m",
              "author": "Alternative_Elk_4077",
              "text": "Remember the whole Game Stop stock price boom? That was spurred entirely by r/wallstreetbets",
              "score": 2,
              "created_utc": "2026-01-16 10:52:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwf7b9",
                  "author": "FullOf_Bad_Ideas",
                  "text": "There are not that many build photos here to make me believe that.\n\nAnd in poll, most users said they have less than 16Gb VRAM I think.\n\nWhen you talk BS about big models being unrunnable there's always one guy (the same one) that will tell you about how he's running Kimi K2.\n\nI don't think there are more than a few hundred people here with 24GB+ inference rigs.\n\nAnd this doesn't move global supply chain markets.\n\n3090 price didn't change dramatically for example, despite being the gpu to get recommended here most of the time.",
                  "score": 1,
                  "created_utc": "2026-01-16 11:03:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwkech",
              "author": "techno156",
              "text": "I would have thought it would be less people buying it, and more sellers taking down the post and relisting it at a higher cost, because of the increased interest.\n\nLike how a YouTube video featuring something on eBay tends to make the price of the thing rocket up.",
              "score": 1,
              "created_utc": "2026-01-16 11:45:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzyjllo",
              "author": "Majinsei",
              "text": "Maybe, but it's probably more a case of everything going up in price, it's the new year and everything is adjusting, and the post~",
              "score": 1,
              "created_utc": "2026-01-16 17:47:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00uq3j",
              "author": "ForsookComparison",
              "text": "w6800 was a pretty low volume product. I'd buy it that Local Llama wiped the used markets in a few countries.",
              "score": 1,
              "created_utc": "2026-01-17 00:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzveutp",
          "author": "epSos-DE",
          "text": "JUST WAIT !\n\n  \nSamsung will probably ramp up production, because they need it for internal too !\n\n  \nRAM got very important with AI !!!",
          "score": 2,
          "created_utc": "2026-01-16 05:46:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvm9ax",
              "author": "pixelpoet_nz",
              "text": "your username says DE, but your spaces before exclamation marks says FR",
              "score": 9,
              "created_utc": "2026-01-16 06:44:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwhif0",
          "author": "braydon125",
          "text": "Get dem p40s dawg",
          "score": 1,
          "created_utc": "2026-01-16 11:22:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03ugrl",
          "author": "basxto",
          "text": "Next time buy a bunch of them and resell some later.",
          "score": 1,
          "created_utc": "2026-01-17 14:06:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvb75a",
          "author": "Randommaggy",
          "text": "I'm after a W68√•0 for the fact that it's AMD's best card that still does 6 monitors.\nI want to replace my RX6800 in my eGPU someday soon.",
          "score": 1,
          "created_utc": "2026-01-16 05:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuwnog",
          "author": "secunder73",
          "text": "RX480\\\\580 8Gb is 7b king, dont miss that out",
          "score": -2,
          "created_utc": "2026-01-16 03:45:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaawts",
      "title": "LLM trained from scratch on 1800s London texts (1.2B params, 90GB dataset)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/",
      "author": "Remarkable-Trick-177",
      "created_utc": "2026-01-11 21:00:37",
      "score": 1034,
      "num_comments": 113,
      "upvote_ratio": 0.99,
      "text": "Hi everyone, I wanted to share an update on my open source project called TimeCapsuleLLM, I train language models from scratch using data from a single time period and location to reduce modern bias.\n\nThe newest model is trained only on texts published in London between 1800-1875. There is no fine tuning, no modern data, and for now no instruction or Q&A pairs so the model continues text from a prompt. This model is 1.2B parameters and uses a 90GB dataset consisting of books, journals, legal docs, religious writing, medical papers, etc. I also use a custom tokenizer, trained on the dataset itself and the model has been trained for 182k steps so far on a rented H100 SXM.\n\nExample outputs:\n\n[Even though the prompt only mentions a specific year, the model generates an argument against the Roman Catholic Church. The dataset does contain large amounts of religious and political writing and the Catholic Emancipation Act took place in 1829 so this behavior makes sense.](https://preview.redd.it/l0oaulxrascg1.png?width=1478&format=png&auto=webp&s=5292309afa4c4735471542b6cc794f6538b42486)\n\n[The telephone was invented in 1876 \\(dataset cuts off at 1875\\), so the model is unfamiliar with the term, treating it as some kind of secret\\/diplomatic device or thing.](https://preview.redd.it/tvem9mxrascg1.png?width=1484&format=png&auto=webp&s=347a6b3242b8ecb97a515196109eb63cc146bae0)\n\nFor next steps, I'm going to look into creating some kind of synthetic Q&A pairs using the dataset itself.\n\n[https://github.com/haykgrigo3/TimeCapsuleLLM](https://github.com/haykgrigo3/TimeCapsuleLLM)\n\n[https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875](https://huggingface.co/haykgrigorian/TimeCapsuleLLM-v2-1800-1875)",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz37gv4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-12 02:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1gr89",
          "author": "Mr_Moonsilver",
          "text": "Man. Been following your posts ever since you had the idea. Keep it up, such a cool project!",
          "score": 211,
          "created_utc": "2026-01-11 21:01:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1lrn5",
          "author": "reality_comes",
          "text": "I gathered a small dataset to do similar work several months ago. My goal though was to train to around 1900 on essentially everything I could get that was older.\n\nI had this idea that it would be fun to probe the model with ideas and see what it thought, things from the sciences that are now settled but at the time hadn't been discovered. \n\nIt would also be fun to use to for other purposes like roleplay.",
          "score": 99,
          "created_utc": "2026-01-11 21:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2a43f",
              "author": "Igot1forya",
              "text": "I would image it would make for a great cultural and historical reference for a narative or consult in the film and literature. I'm thinkig of all the sci-fi time travel tropes but this time it woudl be incredibly accurate. I would imagine, even a props department would be able to use it for period accurae data.",
              "score": 34,
              "created_utc": "2026-01-11 23:22:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4pe4m",
              "author": "dr_lm",
              "text": "An academic team at Zurich did this, but last I saw aren't releasing it because of \"safety\": https://news.ycombinator.com/item?id=46319826",
              "score": 15,
              "created_utc": "2026-01-12 08:30:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbxnn5",
                  "author": "2madlycat",
                  "text": "Oh brother‚Ä¶ üòí",
                  "score": 1,
                  "created_utc": "2026-01-13 10:41:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzcpv8p",
                  "author": "Monkey_1505",
                  "text": "There's literally no point in such a model if you are concerned about safety. The entire point is to examine historical thinking.",
                  "score": 1,
                  "created_utc": "2026-01-13 14:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3nkwp",
              "author": "lomirus",
              "text": "I wonder if AI could independently discover relativity if it only possessed knowledge from the 19th century?",
              "score": 19,
              "created_utc": "2026-01-12 03:42:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5348a",
                  "author": "GrapefruitMammoth626",
                  "text": "There was an interview with maybe It was Demis or Ilya who made that exact point. He proposed an experiment of giving data up until a point in time and seeing if the model could uncover core scientific ideas that followed. Would be a cool verifiable experiment as we have the luxury of having data that came after. Would be a good test of intelligence.\n\nIt also suggests the possibility of verifiable flywheel training where you would curate a limited base training set of fundamentals and steer the model towards rewards to make the reasoning leaps to the next logical stage and incorporating that synthetic data into the training set and looping over the process multiple times hoping to surpass current knowledge. Just an idea based off rewards for making the right connections between its limited pool of knowledge. Anything that‚Äôs verifiable seems doable.",
                  "score": 31,
                  "created_utc": "2026-01-12 10:41:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz69t62",
                  "author": "quinn50",
                  "text": "Yea this is something I would love to see as an experiment in general for these LLMs, have all human knowledge up to before major science / math breakthroughs and see if an automatic agentic setup given a good starting prompt could come up / rediscover things. \n\nGood master's project I bet",
                  "score": 4,
                  "created_utc": "2026-01-12 15:16:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzddqmd",
                  "author": "Smallpaul",
                  "text": "No. It could not. Current AI is far from as creative as Einstein. Not even as creative as the median physics PhD student.",
                  "score": 1,
                  "created_utc": "2026-01-13 15:58:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz4xem4",
                  "author": "reality_comes",
                  "text": "This is the main question",
                  "score": 1,
                  "created_utc": "2026-01-12 09:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz2zbr4",
              "author": "TomLucidor",
              "text": "Now make a model that is aware of different time periods, and can translate between them! Would feel like the \"internet cowboy translator\"",
              "score": 1,
              "created_utc": "2026-01-12 01:31:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2mr2f",
          "author": "-Vincent",
          "text": "\"I'm sorry but my cutoff date is 1875\"",
          "score": 56,
          "created_utc": "2026-01-12 00:27:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz88im5",
              "author": "CV514",
              "text": ">\"This is only one instance out of many that may occur, for the subject is too large to be touched upon here.\" said I.\n\nThis particular model saying",
              "score": 5,
              "created_utc": "2026-01-12 20:41:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzb315p",
              "author": "EagerSubWoofer",
              "text": "this model is worthless. i asked it to code an app and the app UI is shit",
              "score": 4,
              "created_utc": "2026-01-13 06:00:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdbky0",
                  "author": "ketura",
                  "text": "It wasted its entire time carving a woodblock for an illustration.",
                  "score": 2,
                  "created_utc": "2026-01-13 15:48:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1unbn",
          "author": "cantgetthistowork",
          "text": "Large piece of wood and two balls üôÉ",
          "score": 56,
          "created_utc": "2026-01-11 22:06:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3ttfi",
              "author": "Ok-Lengthiness-3988",
              "text": "\"Off with their heads!\" Her Majesty Queen Victoria uttered upon being presented with the device...",
              "score": 5,
              "created_utc": "2026-01-12 04:17:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1wrmm",
          "author": "fuckit-nickit-legit",
          "text": "How did you go about assembling the training data?",
          "score": 19,
          "created_utc": "2026-01-11 22:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3g81c",
          "author": "The_Cat_Commando",
          "text": "Ye olde GGUF hhhuuuuuuwhen my good sir?",
          "score": 15,
          "created_utc": "2026-01-12 03:01:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz222po",
          "author": "MoffKalast",
          "text": "It's really surprising to me that it's even possible to pretrain something coherent with that little data. I guess the early datasets really were completely noisy trash.",
          "score": 16,
          "created_utc": "2026-01-11 22:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3b4zx",
              "author": "lahwran_",
              "text": "GPT-2 was trained on 40GB of internet text.\n\nThe thing that's amazing to me here is that there were even 90gb of words written at the time! I'm amazed enough to be skeptical - is that really the plaintext file size, or are does it include book scans and such",
              "score": 31,
              "created_utc": "2026-01-12 02:34:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4tu43",
                  "author": "1731799517",
                  "text": "Yeah, the size of stuff like game assets or HD movies really warped perception. 90 Gigabytes is a shitload of information.",
                  "score": 12,
                  "created_utc": "2026-01-12 09:13:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2ixzd",
          "author": "dejco",
          "text": "Now make it run on Babbage analytical engine to be period correct ü§£",
          "score": 16,
          "created_utc": "2026-01-12 00:08:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2b2uz",
          "author": "dbenc",
          "text": "could you train it on datasets in other languages from the same time period?",
          "score": 10,
          "created_utc": "2026-01-11 23:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3l8ct",
              "author": "NoahFect",
              "text": "I'd like to see a model trained on all of the world's known scientific writing between Pythagoras and Newton, and see how hard it is to lead the model to invent calculus.",
              "score": 20,
              "created_utc": "2026-01-12 03:29:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz1owdq",
          "author": "Watemote",
          "text": "Please ask your LLM to explain its own existence. ¬†Will it decide it‚Äôs a mechanical Turk or a sensory deprived human ?",
          "score": 34,
          "created_utc": "2026-01-11 21:39:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1ullr",
              "author": "datbackup",
              "text": "OP‚Äôs model is a base model, not an instruct model. So if you gave it instructions or questions, it‚Äôs likely to just continue writing in the role of the questioner, rather than answer them.\n\nDid you know that LLM trainers can during the training process include training data that talks about the nature of ‚ÄúAI‚Äù and includes examples of conversations between ‚ÄúAI‚Äù and humans?\n\nBut it‚Äôs deception. The ‚ÄúAI‚Äù has none of these things. It‚Äôs a function approximator. The inclusion of self-aware-sounding chats in the training data is a shrewd decision because it allows users to chat ‚Äúwith‚Äù the ‚ÄúAI‚Äù as though it has its own theory of mind, self awareness, or subjective viewpoint, and VC money goes gaga for this. So it‚Äôs a very lucrative bit of deception.\n\nIn fact the entirety of the instruct fine tuning paradigm is based on a similar but subtler deception. The model is never ‚Äúanswering‚Äù anything, it‚Äôs only completing/continuing the prompt. It just happens that the completion looks like an answer, because the tuning dataset was all prompts followed by answers.\n\nInstruct models are convenient, but base models are where you get to see how the sausage is made (‚Äúbase model‚Äù is coming to mean something else these days; I‚Äôm using it in its original sense meaning ‚Äúpretrained‚Äù i.e. not (yet) instruct fine tuned).",
              "score": 100,
              "created_utc": "2026-01-11 22:05:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz26j2t",
                  "author": "MoffKalast",
                  "text": "All true of course, though it's not like we're talking bayesian inference here exactly. Sufficiently advanced linear algebra is indistinguishable from magic and everything in this universe can be boiled down to an advanced enough function. \n\nIdk, I'm really sort of torn, there's definitely something there that understands and solves problems beyond anything a logic based system could ever do but we also don't seem to ever really interact with it directly, each chat is indistinguishable from any other training batch seen previously. If they were capable of self awareness it wouldn't even make sense to them because from a model's perceptive they don't really... exist? They don't perceive time nor space.",
                  "score": 29,
                  "created_utc": "2026-01-11 23:04:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz989dh",
                  "author": "TheLocalDrummer",
                  "text": "I love your take. That's exactly how I feel about all this \"the computer is talking!\" hype. AI's self-awareness is manufactured, fabricated, literally artificial. Anything generated by AI to sound self-aware came from pretraining data that contained our own expectations of \"artificial self-awareness\".",
                  "score": 6,
                  "created_utc": "2026-01-12 23:35:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1xcp1",
                  "author": "federico_84",
                  "text": "Great info, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-11 22:19:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz1pvlu",
              "author": "LoveMind_AI",
              "text": "Probably needs a bit of instruction following training in order to do that?",
              "score": 10,
              "created_utc": "2026-01-11 21:43:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz24zmd",
                  "author": "SubstantialSock8002",
                  "text": "With the right prompt engineering you can still get a base model to answer questions. You might need to add a couple turns of Q&A pairs before (few shot prompting).\n\nI haven't tried it, but this model could have some understanding of answering questions from newspaper interviews or legal proceedings included in the training data, even if it hasn't been fine-tuned for instruction following.",
                  "score": 11,
                  "created_utc": "2026-01-11 22:56:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz1r8ao",
                  "author": "Watemote",
                  "text": "Maybe, the idea originated with talks with my daughter about the seeming self awareness of LLM but then what about an LLM trained on the ‚Äúdoomsday manuals‚Äù which are a series of books on how to bring technology back up to an 1880s level. ¬†",
                  "score": 3,
                  "created_utc": "2026-01-11 21:50:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1kpw6",
          "author": "TheKL",
          "text": "this is so cool",
          "score": 6,
          "created_utc": "2026-01-11 21:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1l3yu",
          "author": "fulgencio_batista",
          "text": "Very interesting. I wonder how such a dataset would effect model 'intelligence'? On one hand, I assume most remaining texts from that time period were probably made by the well educated of the time, on the other hand, they knew a lot less back then.",
          "score": 12,
          "created_utc": "2026-01-11 21:21:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz21202",
          "author": "Southern_Sun_2106",
          "text": "Very unique and exciting project, thank you for your work and for sharing this with the community.",
          "score": 4,
          "created_utc": "2026-01-11 22:37:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4n4jt",
          "author": "profcuck",
          "text": "Given that the invention of the telephone wasn't a completely out of context surprise (as far as I know) I wonder about a prompting that points it in the direction of explaining new technology and what might be possible in a few years time, with the description and definition of a \"telephone\" being given.\n\nSo for example: \"The American Alexander Graham Bell has been experimenting with a new type of device for transmitting sound at a distance, which he calls the \"telephone\".  It utilizes electricity in a manner similar to Morse's telegraph, but instead of a series of distinct on/off pulses, it...\"\n\nWith that hopefully interesting thought out of the way I'll now just join in with the chorus of fans saying \"what a very cool project\"!",
          "score": 4,
          "created_utc": "2026-01-12 08:09:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6vjl7",
          "author": "Zestyclose839",
          "text": "If any of you run Apple Silicon and want to try this out, I made an FP16 MLX version:\n\n[https://huggingface.co/FractalSurfer/TimeCapsuleLLM-v2-1800-1875-mlx-fp16](https://huggingface.co/FractalSurfer/TimeCapsuleLLM-v2-1800-1875-mlx-fp16) \n\nhttps://preview.redd.it/czkjcwzc8ycg1.jpeg?width=1790&format=pjpg&auto=webp&s=48a2179b732e532c4e6a7d0c8b7c548b2a78d71a",
          "score": 3,
          "created_utc": "2026-01-12 16:57:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2s8x1",
          "author": "TheRealMasonMac",
          "text": "Morbidly curious, has it learned racial biases? For instance, how does it continue talking about Africans or Native Americans?",
          "score": 4,
          "created_utc": "2026-01-12 00:54:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1qtkq",
          "author": "amooz",
          "text": "This is really, really cool.",
          "score": 3,
          "created_utc": "2026-01-11 21:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1rh0c",
          "author": "MrPecunius",
          "text": "This is cool as hell, thanks for the update!",
          "score": 3,
          "created_utc": "2026-01-11 21:51:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz254b0",
          "author": "Dazzling-Try-7499",
          "text": "Out of curiosity, how much memory do need to train a 1.2B model in that way?",
          "score": 3,
          "created_utc": "2026-01-11 22:57:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2fw6u",
              "author": "PortiaLynnTurlet",
              "text": "You can train this on a single h100 easily. Minimum probably 24GB practically although more is better",
              "score": 7,
              "created_utc": "2026-01-11 23:52:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz45ery",
              "author": "Remarkable-Trick-177",
              "text": "I had 80gb on an h100 and it took around 130-140 hours to train total. It can be done with less memory, will just take longer.",
              "score": 6,
              "created_utc": "2026-01-12 05:37:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz6joej",
                  "author": "mypossiblepasts",
                  "text": "I am interesting in making a proper, foreign language LLM, without bias of base models that's really apparent for non germanic language.  \nWould that approach of yours work here?",
                  "score": 1,
                  "created_utc": "2026-01-12 16:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz27so0",
          "author": "ohcrap___fk",
          "text": "Holy f I absolutely love the output. It‚Äôs like skirting the edge of the event horizon between cohesiveness and raw unconnected creativity",
          "score": 3,
          "created_utc": "2026-01-11 23:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz35iy3",
          "author": "Ready-Interest-1024",
          "text": "This is such a cool idea, I wonder if you could see any interesting trends about how our current models answer societal issues vs what this would say",
          "score": 3,
          "created_utc": "2026-01-12 02:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz45uge",
              "author": "Remarkable-Trick-177",
              "text": "Thanks! I‚Äôm planning on comparing the same prompts with a general use LLM. I think comparing word neighbors will also show interesting trends.",
              "score": 3,
              "created_utc": "2026-01-12 05:40:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6se46",
          "author": "TellMeAboutGoodManga",
          "text": "Gee guff when?",
          "score": 3,
          "created_utc": "2026-01-12 16:42:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vafa",
          "author": "Far_Hope_6349",
          "text": "love this!!",
          "score": 2,
          "created_utc": "2026-01-11 22:09:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz21evm",
          "author": "Terrible_Ad9306",
          "text": "Geat idea!",
          "score": 2,
          "created_utc": "2026-01-11 22:38:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz22mnj",
          "author": "literateu",
          "text": "Very cool! Would be cool to see differences if training on something like [**https://huggingface.co/datasets/dell-research-harvard/AmericanStories**](https://huggingface.co/datasets/dell-research-harvard/AmericanStories) over time as well !",
          "score": 2,
          "created_utc": "2026-01-11 22:44:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2ttyw",
          "author": "ddxv",
          "text": "Is it better at finishing the text when you don't ask a question? I noticed both your examples didn't have questions, whereas modern use is usually questions with answers. Is this because your corpus didn't have many Q A style docs?",
          "score": 2,
          "created_utc": "2026-01-12 01:02:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5h2vi",
              "author": "Technical-Will-2862",
              "text": "Yeah. It‚Äôs just predicting the next likely token. Kinda like call and response.¬†",
              "score": 2,
              "created_utc": "2026-01-12 12:34:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4e9xv",
          "author": "smflx",
          "text": "Wonderful project! Appreciate your sharing!",
          "score": 2,
          "created_utc": "2026-01-12 06:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5jc1c",
          "author": "GhostGhazi",
          "text": "Can you teach us how you rent a H100 to train an LLM? Im really interested",
          "score": 2,
          "created_utc": "2026-01-12 12:49:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzj0thu",
              "author": "Majestic-Lawyer5246",
              "text": "hey! you generally have two routes: raw GPU VMs (where you manage everything yourself) or running training as a containerized job on a platform that handles orchestration for you.\n\nif you‚Äôre already packaging your training code in Docker, platforms like [Northflank](https://northflank.com/) let you spin up on-demand H100/A100 jobs, run your fine-tuning, and shut everything down when it‚Äôs done so you‚Äôre not paying for idle time.\n\nhappy to explain what a minimal container + job setup looks like if that helps :)",
              "score": 1,
              "created_utc": "2026-01-14 12:29:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7evsb",
          "author": "Liringlass",
          "text": "This could be so interesting. An AI that feels like you‚Äôre talking to a dude from that time.",
          "score": 2,
          "created_utc": "2026-01-12 18:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbd4z5",
          "author": "notforrob",
          "text": "I think your next step -- generating synthetic Q&A pairs from the dataset -- and then presumably fine tuning it on that will be super interesting. My guess is that you generate a very robust dataset (I personally would target \\~10K Q&A pairs), and that when fine tuned on your dataset it will be 100 times more fun and interesting to play with.",
          "score": 2,
          "created_utc": "2026-01-13 07:27:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vb6k",
          "author": "StardockEngineer",
          "text": "This is so cool.",
          "score": 2,
          "created_utc": "2026-01-11 22:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz40cfn",
          "author": "paul_f",
          "text": "what a great concept",
          "score": 1,
          "created_utc": "2026-01-12 05:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4438j",
          "author": "Revolutionalredstone",
          "text": "Awesome project! And just keeps getting better üòÅ nice work dude !",
          "score": 1,
          "created_utc": "2026-01-12 05:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz44nke",
          "author": "Acrobatic-Tomato4862",
          "text": "If we can make it solve a problem from after 1875 then atleast it can be settled that llms have capacity for novel thought.¬†",
          "score": 1,
          "created_utc": "2026-01-12 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4a97t",
          "author": "IrisColt",
          "text": "I kneel... Your perseverance... Kudos to you!",
          "score": 1,
          "created_utc": "2026-01-12 06:15:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4dokv",
          "author": "jacek2023",
          "text": "great job!",
          "score": 1,
          "created_utc": "2026-01-12 06:44:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4fn2o",
          "author": "ryfromoz",
          "text": "Very cool!",
          "score": 1,
          "created_utc": "2026-01-12 07:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4xhnh",
          "author": "GrokiniGPT",
          "text": "This seems amazing!  Can't wait to see it in action",
          "score": 1,
          "created_utc": "2026-01-12 09:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz55x2n",
          "author": "ikkiyikki",
          "text": "Ok, this is inspiring me to do a similar take but with ancient Latin texts! Walk like an Egyptian, talk like a Roman!\n\nüòÖ",
          "score": 1,
          "created_utc": "2026-01-12 11:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5ar7l",
          "author": "thecowmilk_",
          "text": "Now tell the LLM to write Jack The Ripper inspired letters.",
          "score": 1,
          "created_utc": "2026-01-12 11:47:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5mpx3",
          "author": "LightMaleficent5844",
          "text": "Download link for model and dataset?",
          "score": 1,
          "created_utc": "2026-01-12 13:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6iews",
          "author": "4redis",
          "text": "Probs not the right place and probs not the smartest question but only way i will learn. \n\nI was wondering how data (in this case text) is \"fed\" to these ai models in simple terms",
          "score": 1,
          "created_utc": "2026-01-12 15:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6k2py",
          "author": "Marshall_Lawson",
          "text": "Cool, now do 2003 next! Lol",
          "score": 1,
          "created_utc": "2026-01-12 16:05:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6z3vh",
          "author": "auxiliaPalatina",
          "text": "What are your parameters when prompting, I couldn't really find a sweet spot to get the answers in the post?",
          "score": 1,
          "created_utc": "2026-01-12 17:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7641f",
          "author": "Area51-Escapee",
          "text": "What a blessing",
          "score": 1,
          "created_utc": "2026-01-12 17:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz83xn7",
          "author": "johnnyjoestar5678",
          "text": "this is insanely cool.\n\nas a matter of fact, you could easily monetize this, video game companies and the rest of the entertainment industry would see immense value in this. as a way of generating more authentic dialogue and actions/behavior for a particular time period. yknow what i mean?",
          "score": 1,
          "created_utc": "2026-01-12 20:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfae8i",
          "author": "thermologic_",
          "text": "Chat link pls",
          "score": 1,
          "created_utc": "2026-01-13 21:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03q2rp",
          "author": "Su1tz",
          "text": "\"My wife has been acting distant towards me\"\n\n-\"She's suffering from hysteria, modern experts suggest lobotomy\"",
          "score": 1,
          "created_utc": "2026-01-17 13:41:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06b41a",
          "author": "MCF4ddn",
          "text": "So how do i run this myself, The short explanation on huggingface was def not detailed enough for me lol. There is something about adding a path to the model but what am i to put there? i tried the windows path to the safetensors but when i try to run the script it just instantly closes.",
          "score": 1,
          "created_utc": "2026-01-17 21:18:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ay6t9",
              "author": "LostDrengr",
              "text": "Did you download the model? It looks like you use python to query it, so you will need to install python, transformers then run.",
              "score": 1,
              "created_utc": "2026-01-18 15:29:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5hwig",
          "author": "Technical-Will-2862",
          "text": "Disappointed in the lack of emergence. People ask to keep scaling, but I think you‚Äôve proven something valuable already - LLMs will not lead to valued emergent behavior, even with a highly curated dataset. There‚Äôs a self attention and context issue.¬†",
          "score": 0,
          "created_utc": "2026-01-12 12:39:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5tolc",
              "author": "aleph_iskar",
              "text": "how can you possibly conclude that from the 2 short snippets OP provided?\n\n> oh this toy exercise with 1.2 b model is not self aware, therefore LLMs will not lead to valued emergent behaviour.\n\nthe fuck",
              "score": 3,
              "created_utc": "2026-01-12 13:51:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6073u",
                  "author": "Technical-Will-2862",
                  "text": "Emergent behavior demands structure. If there‚Äôs nothing novel being generated with over 90gb of text that‚Äôs inherently interconnected, it should signal that there‚Äôs something fundamental lacking. That‚Äôs like 21 billion tokens and it‚Äôs still spitting out bullshit while a human can read a single book and utilize the reasoning patterns intuitively.¬†\n\nIt can‚Äôt be anymore obvious that something is lacking. But yeah this dude should def keep renting h100s (also I‚Äôm not seeking self awareness, just maybe like one tiny sign of actually responding to the query?) (also pt 2, OPs entire point was to see what emerges and after taking in pretty much everything he can find it‚Äôs still dookie.)",
                  "score": 0,
                  "created_utc": "2026-01-12 14:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1o0j0",
          "author": "Sicarius_The_First",
          "text": "but why gatekeep the model",
          "score": -9,
          "created_utc": "2026-01-11 21:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1u6fz",
              "author": "Sheeye12",
              "text": "He didn't, OP left huggingface link in post.",
              "score": 17,
              "created_utc": "2026-01-11 22:03:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz366x4",
                  "author": "AnonymousTransfem",
                  "text": "We have to request access, check the huggingface",
                  "score": 5,
                  "created_utc": "2026-01-12 02:08:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz457u9",
          "author": "hemahariharansamson",
          "text": "Man your findings looks interesting. Sharing this to my circle.\n\nKeep up this work man\n\nhttps://www.linkedin.com/posts/hemahariharansamson_llm-machinelearning-ai-activity-7416351604552908800-8X0B?utm_source=share&utm_medium=member_android&rcm=ACoAADYRaTkB5vCPx_LU_GECDXbmMlw5Y0BTUg8",
          "score": -6,
          "created_utc": "2026-01-12 05:35:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcuerc",
      "title": "NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/",
      "author": "Fear_ltself",
      "created_utc": "2026-01-14 18:02:19",
      "score": 689,
      "num_comments": 129,
      "upvote_ratio": 0.97,
      "text": "I‚Äôve seen some arguments we‚Äôve reached AGI, it‚Äôs just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. ",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nzlzoql",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-14 21:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkvpg0",
          "author": "ortegaalfredo",
          "text": "They finally created the Middle manager LLM.",
          "score": 451,
          "created_utc": "2026-01-14 18:05:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkya4p",
              "author": "Zc5Gwu",
              "text": "No wonder it‚Äôs only 8b.",
              "score": 242,
              "created_utc": "2026-01-14 18:16:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl3dha",
                  "author": "Silver_Jaguar_24",
                  "text": "Hahaha just like all our managers. I am sure you will all agree.  \nSubordinates: 120b  \nManagers: 8b",
                  "score": 121,
                  "created_utc": "2026-01-14 18:39:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmdtwl",
                  "author": "Guinness",
                  "text": "CEO LLM 3B coming soon to any phone made since 2006.",
                  "score": 78,
                  "created_utc": "2026-01-14 22:08:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nznv03x",
                  "author": "slippery",
                  "text": "It has to be dumber than the LLM worker bees to be in management, haha! It probably got there by brown nosing the higher up LLMs. The more things change.",
                  "score": 5,
                  "created_utc": "2026-01-15 02:58:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmcqtb",
                  "author": "DrewGrgich",
                  "text": "Best comment of the week. :)",
                  "score": 3,
                  "created_utc": "2026-01-14 22:03:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoecmj",
                  "author": "Shaken_Earth",
                  "text": "https://www.youtube.com/watch?v=fRs0OqV4uSc",
                  "score": 0,
                  "created_utc": "2026-01-15 05:04:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl560g",
              "author": "_raydeStar",
              "text": "Honestly this is super smart.\n\n\"I don't know the answer but I know how to find it\" is just as good - if it goes out and finds it.",
              "score": 65,
              "created_utc": "2026-01-14 18:46:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzljvvd",
                  "author": "mycall",
                  "text": "Immediately forwards it to a model that hallucinates the answer.",
                  "score": 68,
                  "created_utc": "2026-01-14 19:53:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzot43z",
                  "author": "calcium",
                  "text": "Assuming it knows how to properly interrogate the model to know it's correct and maybe validate the output.",
                  "score": 1,
                  "created_utc": "2026-01-15 07:02:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl9mba",
              "author": "Recoil42",
              "text": "*Assistant* to the middle manger.",
              "score": 38,
              "created_utc": "2026-01-14 19:06:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlh2g6",
                  "author": "Ryuma666",
                  "text": "Lmao!",
                  "score": 2,
                  "created_utc": "2026-01-14 19:40:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzky8n7",
              "author": "Practical-Hand203",
              "text": "https://i.redd.it/zl3ovcfhwcdg1.gif",
              "score": 28,
              "created_utc": "2026-01-14 18:16:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznv8gl",
                  "author": "slippery",
                  "text": "Yeah, I'm gonna need you other LLMs to work this Saturday on the TPS reports.",
                  "score": 7,
                  "created_utc": "2026-01-15 02:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoe1ve",
              "author": "jsrockford",
              "text": "Assistant TO the Middle Manager",
              "score": 3,
              "created_utc": "2026-01-15 05:02:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl1eqq",
              "author": "__Maximum__",
              "text": "Why only middle? You just need to stack on each other all the way up",
              "score": 7,
              "created_utc": "2026-01-14 18:30:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzorzd7",
              "author": "lodott1",
              "text": "MiddLLManager ‚Ñ¢Ô∏è",
              "score": 3,
              "created_utc": "2026-01-15 06:52:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzlc20s",
              "author": "Gallardo994",
              "text": "Can it ping you every day with a status report request though, is the question. If yes then it's indistinguishable from a PM",
              "score": 4,
              "created_utc": "2026-01-14 19:17:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06k6cl",
              "author": "madSaiyanUltra_9789",
              "text": "lmao\n\nhttps://preview.redd.it/05zk9gdufzdg1.png?width=2816&format=png&auto=webp&s=f1f2ecb1a475202a3b8f37048668c7d5a1bd9f20",
              "score": 1,
              "created_utc": "2026-01-17 22:04:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzky1y6",
          "author": "jacek2023",
          "text": "not really new ;)\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b\\_hugging\\_face/](https://www.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/)",
          "score": 77,
          "created_utc": "2026-01-14 18:15:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl0bws",
          "author": "TransportationSea579",
          "text": "Claude code style agentic frameworks feel like the next big leap forward. I can imagine a pyramid of models manging models maanging models managing 'worker' instances of claude code, claude cowork etc. or open source equivalents. Perhaps this exists already?",
          "score": 53,
          "created_utc": "2026-01-14 18:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlml3p",
              "author": "swagonflyyyy",
              "text": "Codex CLI already does that with openai's Agents SDK. You can even run local llms with it. `gpt-oss:120b` works surprisingly well for that when paired with the right tools and orchestration framework.\n\nBut me personally I'd rather create a modelfile instead set to 128K tokens and a couple of parameter tweaks on top of that.",
              "score": 21,
              "created_utc": "2026-01-14 20:05:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmozet",
                  "author": "farox",
                  "text": "Claude has the same released recently",
                  "score": 3,
                  "created_utc": "2026-01-14 23:03:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzm1u2b",
              "author": "esuil",
              "text": "Hear me out. What if we built neural network, but each neuron is its own LLM/neural network that will output what they think the weight for that neuron should be dynamically?",
              "score": 10,
              "created_utc": "2026-01-14 21:14:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzm8dwy",
                  "author": "lookwatchlistenplay",
                  "text": "But how do we fit the GPUs inside the neurons?\n\nOh wait it's just software right? So we use one big GPU to simulate the neurons each having their own tiny GPUs and go from there?\n\nHmm...",
                  "score": 12,
                  "created_utc": "2026-01-14 21:44:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzomutt",
                  "author": "visarga",
                  "text": "Network-in-network was invented in [2013](https://arxiv.org/abs/1312.4400)",
                  "score": 3,
                  "created_utc": "2026-01-15 06:09:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzu5dpm",
                  "author": "huzbum",
                  "text": "Like an MoE?",
                  "score": 1,
                  "created_utc": "2026-01-16 01:11:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzmrzod",
              "author": "jazir555",
              "text": "It's will never cease to amaze that everyone's solutions to LLMs is creating a corporate structure for them.",
              "score": 13,
              "created_utc": "2026-01-14 23:19:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nznknwd",
                  "author": "mycall",
                  "text": "Low imagination cargo cult.",
                  "score": 8,
                  "created_utc": "2026-01-15 01:58:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzo8svz",
                  "author": "MrMooga",
                  "text": "Complex structures seem to do best when organized and compartmentalized into specialized sub components rather than have one big genius handle everything",
                  "score": 5,
                  "created_utc": "2026-01-15 04:25:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmspyp",
                  "author": "TransportationSea579",
                  "text": "Corporate structures disgust me, but they built the modern world",
                  "score": 7,
                  "created_utc": "2026-01-14 23:23:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl9373",
              "author": "redtrousered",
              "text": "Gas town",
              "score": 3,
              "created_utc": "2026-01-14 19:04:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlezs5",
          "author": "HealthyCommunicat",
          "text": "Cool but mirothinker v1.5 30b a3b seems like a much better choice if you can afford the vram.\n\nIt‚Äôs ability to ‚Äúorchestrate‚Äù in this manner simply from being compatible with so many tool call types allowing it to just pull, access, modify, etc so easily. Its literally the first small model i‚Äôve been impressed by. - there is also a qwen 3 54b a3b supercoder, a mod of qwen 3 30b a3b that is very recent and is able to do alot more than just the original release of the qwen 3 30 a3b, if you can afford the vram, there is no other model that will beat qwen 54b when it comes to effiency",
          "score": 24,
          "created_utc": "2026-01-14 19:31:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzndzl4",
              "author": "nasduia",
              "text": "How did you test? Did you set up all the additional tools like the containers?",
              "score": 2,
              "created_utc": "2026-01-15 01:19:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp9psm",
                  "author": "Front_Eagle739",
                  "text": "Ive got about 60 mcp tools attached to my lm studio including web, files, google docs, and reasoning guides. Ive tested a whole bunch of things and gpt oss 20 and mirothinker 30a3b are the only two small models that can decently use the tools and mirothinker is definitely better. Oss 120 work and glm4.7 are even better of course but mirothinker is my usual.",
                  "score": 3,
                  "created_utc": "2026-01-15 09:39:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzr8hut",
                  "author": "HealthyCommunicat",
                  "text": "im not too sure what u mean by this, but i have a bunch of custom tools made for my work and mirothinker 30b a3b was the first small sized model that could keep up with the flow of calling one tool to the next without getting confused. qwen 3 53b was even better and could actually execute long flows of reading email, querying knowledgebase, applying solution via sqlplus, checking, notifying me on slack if its unconfident, writing up a email response to client if it is able to complete it.",
                  "score": 1,
                  "created_utc": "2026-01-15 16:49:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqo7kx",
              "author": "Hoak-em",
              "text": "Where can I find that model (supercoder)? Looking on HF and no luck",
              "score": 1,
              "created_utc": "2026-01-15 15:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzr7o0f",
                  "author": "HealthyCommunicat",
                  "text": "[https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B)\n\n30b a3b, this one's been on news articles for being a 30b model but having as much tool call compatibility.\n\n[https://huggingface.co/DavidAU/Qwen3-53B-A3B-2507-TOTAL-RECALL-v2-MASTER-CODER](https://huggingface.co/DavidAU/Qwen3-53B-A3B-2507-TOTAL-RECALL-v2-MASTER-CODER)\n\nsorry its 53b not 54b. its pretty recent finetune/\"mod\" of qwen 3 30 a3b 2507 (meaning more recent base from qwen) + alot more recent agentic knowledge meant to focus more on coding. its fucking great. better than qwen 3 next 80b in my opinion for actual coding.",
                  "score": 3,
                  "created_utc": "2026-01-15 16:45:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzre0bi",
                  "author": "Ok-Buffalo2450",
                  "text": "Second this. Where to find these models?",
                  "score": 1,
                  "created_utc": "2026-01-15 17:14:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl5d4l",
          "author": "WiseassWolfOfYoitsu",
          "text": "I'm kind of wanting to use this for RP - use it as a \"Game Master\" AI, that then calls other LLMs as reference books for the world, or to run individual NPCs, etc.",
          "score": 26,
          "created_utc": "2026-01-14 18:47:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlc1z1",
              "author": "lolxdmainkaisemaanlu",
              "text": "damn that would be amazing!! Someone should work on making this a reality!",
              "score": 8,
              "created_utc": "2026-01-14 19:17:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlgwb7",
                  "author": "Ryuma666",
                  "text": "Sounds interesting, have played only a little DND so with some help about the game mechanics, I'll be happy to work on it in my spare time.",
                  "score": 3,
                  "created_utc": "2026-01-14 19:39:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzlmbiu",
                  "author": "hapliniste",
                  "text": "Not the same thing, but I'm developing an \"ai dungeon\" that use and write the game systems as you play.\n\nUsing gemini 3 flash as the LLM but we could likely make it run on smaller model, I just found flash is good for the price.",
                  "score": 3,
                  "created_utc": "2026-01-14 20:04:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmpat7",
                  "author": "farox",
                  "text": "Works with Claude code and replacing the output style. Add some tools for roles, rules, story etc and it works quite nicely. Been playing my own campaign on the train with a VPN home.",
                  "score": 2,
                  "created_utc": "2026-01-14 23:05:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzmzvp7",
                  "author": "bobby-chan",
                  "text": "it's already a reality. It's called \"tool call\".\n\nTools can be other llms as well. If you lack the ram for multiple model at the same time, you can use something like llama-swap.\n\nedit: or it can be the same model, with a different context.",
                  "score": 2,
                  "created_utc": "2026-01-15 00:02:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoizp3",
                  "author": "btdeviant",
                  "text": "It‚Äôs pretty simple‚Ä¶ you can do this with a few files and some decorators using something like Strands. \n\nMulti-agent architectures that have specialist agents are dead simple to build these days and very common",
                  "score": 1,
                  "created_utc": "2026-01-15 05:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzoafzr",
              "author": "TrekkiMonstr",
              "text": "For NPCs I guess, but for reference books, why not just use RAG",
              "score": 1,
              "created_utc": "2026-01-15 04:37:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl4hwl",
          "author": "dwkdnvr",
          "text": "With the plethora of folks putting out 'personal assistant' setups based on Claude Code / OpenCode and heavy use of skills, having a local model specifically designed around tool calling/skill invocation and routing seems like an obvious niche, but one that is potentially *very* valuable. I'll have to take a closer look at this one.",
          "score": 11,
          "created_utc": "2026-01-14 18:43:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkze00",
          "author": "x8code",
          "text": "Awesome, this is definitely the next stage of LLM evolution! Lighter-weight models that can handle domain-specific functions. My only concern is how coordination will happen with multi-domain topics.",
          "score": 7,
          "created_utc": "2026-01-14 18:21:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzst83s",
              "author": "tech2biz",
              "text": "you could check out cascadeflow on github (MIT), it comes with domain intelligence.",
              "score": 1,
              "created_utc": "2026-01-15 21:06:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkyehh",
          "author": "xAragon_",
          "text": "Isn't 8B an overkill for a model that just does that? Wouldn't 2B / 4B be more than enough?",
          "score": 12,
          "created_utc": "2026-01-14 18:17:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl1nc4",
              "author": "No_Afternoon_4260",
              "text": "Depending on the use case, I wouldn't say 4B to be more than enough on very specific knowledge domain",
              "score": 13,
              "created_utc": "2026-01-14 18:31:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl242s",
              "author": "AnomalyNexus",
              "text": "What tool to invoke can decide pretty substantially how rest of the thought process goes so reckon but heavier is better. You can always quant it down",
              "score": 9,
              "created_utc": "2026-01-14 18:33:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzm8rj5",
              "author": "lookwatchlistenplay",
              "text": "I tend to believe there's no such thing as overkill when we're in the < 14B range. Assuming the model needs to be able to handle any kind of complex natural language ask about anything I can think of.",
              "score": 8,
              "created_utc": "2026-01-14 21:45:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzlay1t",
              "author": "GeneralComposer5885",
              "text": "Currently tool calling is too inconsistent with models <7b/8b",
              "score": 6,
              "created_utc": "2026-01-14 19:12:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzm7mp9",
              "author": "JsThiago5",
              "text": "I use qwen3 4b, and It's able to do it.",
              "score": 2,
              "created_utc": "2026-01-14 21:40:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nznulc5",
              "author": "Artistic_Okra7288",
              "text": "This was released recently as well which seems to have similar capability - https://huggingface.co/tencent/Youtu-LLM-2B",
              "score": 1,
              "created_utc": "2026-01-15 02:56:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzo1m6d",
              "author": "layer4down",
              "text": "Wouldn‚Äôt 8B better handle complexity?",
              "score": 1,
              "created_utc": "2026-01-15 03:38:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzoqeqw",
              "author": "ab2377",
              "text": "really wish it was that because for local thats a lot of mem required. and if you put this in ram, the tool calling decisions will become so slow, assuming you have the main working model in vram.",
              "score": 1,
              "created_utc": "2026-01-15 06:39:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlw2ll",
          "author": "jasongill",
          "text": "> I‚Äôve seen some arguments we‚Äôve reached AGI\n\nmy brother in christ, we can barely count the number of R's in strawberry, I don't think we need to debate if we've reached the next plane of human existence just yet",
          "score": 14,
          "created_utc": "2026-01-14 20:48:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzofg9z",
          "author": "emaiksiaime",
          "text": "Behold my new sparse model architecture (it‚Äôs 8 3060s doing different things)!!!",
          "score": 3,
          "created_utc": "2026-01-15 05:12:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlcpnw",
          "author": "blurredphotos",
          "text": "I've been waiting on this. Basically a tool-first LLM.",
          "score": 4,
          "created_utc": "2026-01-14 19:20:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlf1yk",
              "author": "blurredphotos",
              "text": "Edit: this is looping endlessly and unusable at q4.",
              "score": 5,
              "created_utc": "2026-01-14 19:31:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmnx5e",
          "author": "Loud_Communication68",
          "text": "Doesn't agentflow already do this?",
          "score": 2,
          "created_utc": "2026-01-14 22:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzo94sr",
          "author": "lemondrops9",
          "text": "I'm confused, wasn't this released over a month ago?",
          "score": 2,
          "created_utc": "2026-01-15 04:27:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl2hdp",
          "author": "sam7oon",
          "text": "is there a good source to have me on the right track to how to implement this way into my pipeline , appreciated",
          "score": 3,
          "created_utc": "2026-01-14 18:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlqkuc",
          "author": "Robert__Sinclair",
          "text": "ok, but let's say you have a 8B \"router\" model, then (for example for deep reasoning) you will need BIG models anyway. A MOE (like Gemini or Claude) does exactly the same. A mixture of experts does the routing internally.  \nUsing a small model as a router is useful for searching information or to delegate simple problem to simple experts (with the downside of the overhead because the prompt must be first answered by the router and then answered by the \"right\" model).\n\nAnyway, we will have real progress only when new architectures will surface. Transformers is already showing its limits. The problem is that most companies prefer to feed more/better data to the actual models to improve them. It seems like in the 80s where chess programs were getting better because computers were getting faster and because they operated using brute force.\n\nSame goes for movies and tv series: it's less risky to do a reboot or sequel than a brand new movie or series.",
          "score": 3,
          "created_utc": "2026-01-14 20:23:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzonlxl",
              "author": "visarga",
              "text": "Persistent, unbounded memory has been solved, it is the coding agent + bash + filesystem. You don't need a better model, what makes it better if you set it up to learn and adapt, so it's about tools and environments. It's like SDCs, how long can it drive without human intervention, but unlike cars, the information environment is much more diverse and dynamic. This work horizon is expanding now to hours and days.",
              "score": 1,
              "created_utc": "2026-01-15 06:15:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlqxk1",
          "author": "integerpoet",
          "text": "Arguments we‚Äôve reached AGI are just pareidolia, a powerful emotional force which neurotypicals cannot withstand.",
          "score": 2,
          "created_utc": "2026-01-14 20:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmowhg",
          "author": "skinnyjoints",
          "text": "Imagine this as a VLM that can controls a robot by initiating one of a thousand smaller movement policies. \n\nLike you give it the instruction to take out the trash, it then determines if the trash is in its vision. If not, it sends a command to the robot to run an ‚Äúexplore policy‚Äù where it‚Äôll turn its head or walk around.\n\nThen once the trash is located it‚Äôll trigger the ‚Äúpick up the bin‚Äù policy and the robot will grab the trash.\n\nThen the ‚Äúopen the door‚Äù policy.\n\nSo on and so forth. \n\nThis feels close to how I do things. One master orchestrator policy that determines what to do, which then triggers a sequence of specific actions that I learned how to do by just being alive.",
          "score": 1,
          "created_utc": "2026-01-14 23:03:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmtje4",
          "author": "valdev",
          "text": "I made this exact kind of thing a year or so back for talk to luma. As to know what models, agents or potentially other code to call for the request the user was making. \n\nGranted mine was pretty specific and dumb, trained on bert and gave quick answers based on sample sets of requests that I made (only a few thousand for each)",
          "score": 1,
          "created_utc": "2026-01-14 23:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznllbd",
          "author": "PersonOfDisinterest9",
          "text": "Sounds kind of like the \"Planner\" from the AgentFlow paper on steroids. That one had 4 small models totaling 7B all together.   \n   \nIt's good to see that we're moving away from monolithic \"single series of layers\" models, and moving towards more brain-like division of labor models.  \n  \nThis is kind of like a prefrontal cortex.  \n  \nEventually it might all collapse back into training a model that has all the components more tightly coupled, but I think it's great that there are these more target models.",
          "score": 1,
          "created_utc": "2026-01-15 02:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzocbi9",
          "author": "FutureIsMine",
          "text": "having given this model a spin, it really leans heavy on the \"using other models to answer\", its constantly making tool calls and if prompted to take on a task directly, even a very simple one, will still resort to a tool call. Overall, its viable, but the tool setup it gets will drive the gains here",
          "score": 1,
          "created_utc": "2026-01-15 04:50:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzocjiu",
          "author": "Flaky_Interaction_89",
          "text": "this is the right path to AGI",
          "score": 1,
          "created_utc": "2026-01-15 04:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzop222",
          "author": "CommonPurpose1969",
          "text": "The license is restrictive.",
          "score": 1,
          "created_utc": "2026-01-15 06:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzots90",
          "author": "Green-Ad-3964",
          "text": "Wasn't this released in nov, 25?",
          "score": 1,
          "created_utc": "2026-01-15 07:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp5h5l",
          "author": "TomLucidor",
          "text": "Please test this against OpenCode",
          "score": 1,
          "created_utc": "2026-01-15 08:57:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp7txv",
          "author": "apifree",
          "text": "his actually makes sense, way better than just throwing a huge model at everything lol.",
          "score": 1,
          "created_utc": "2026-01-15 09:20:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqazta",
          "author": "poladermaster",
          "text": "This feels like the 'AI agent' hype cycle all over again, but maybe this time it'll actually deliver.",
          "score": 1,
          "created_utc": "2026-01-15 14:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqpq4v",
          "author": "DinoAmino",
          "text": "Lol!. So many upvotes and an award for ... an opinion post on an unoriginal idea? Damn. OPs bot game is pretty impressive.",
          "score": 1,
          "created_utc": "2026-01-15 15:24:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqys71",
          "author": "xtof_of_crg",
          "text": "Honestly surprised it‚Äôs taken this long to get here",
          "score": 1,
          "created_utc": "2026-01-15 16:05:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwb3gm",
          "author": "Vibe-Sphere",
          "text": "yea that's a solid approach... i've used cascadeflow (github)for similar routing logic - it saves costs by starting with cheaper models and only escalating when needed, keeps quality high while cutting api spend",
          "score": 1,
          "created_utc": "2026-01-16 10:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzky9l3",
          "author": "Long_comment_san",
          "text": "That's what I've been talking about for a while! A router AI between models",
          "score": 1,
          "created_utc": "2026-01-14 18:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlp8mj",
          "author": "Chilidawg",
          "text": "This is great for agents, but let's not call it AGI. It's a tool lets us coordinate other tools, and it should be useful. However, that's like claiming that middle management at the local H&R Block is evidence of AGI.\n\nTo be clear, I have no idea what actual AGI will look like. If tomorrow my RTX 3070 starts telling me it feels pain then sure, but other than that I have no idea where the academic or industry goalposts for AGI will wander over the next few years.",
          "score": 1,
          "created_utc": "2026-01-14 20:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmxgwm",
          "author": "ridablellama",
          "text": "yes i have a qwen project with VL as the orchestrator for the image gen models, coding models and math qwen models. chinese have mastered this approach because of their hardware limitations.",
          "score": 1,
          "created_utc": "2026-01-14 23:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoamqw",
          "author": "CadCan",
          "text": "Anytime I hear agi I genuinely just roll my eyes.",
          "score": 1,
          "created_utc": "2026-01-15 04:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl435q",
          "author": "kompania",
          "text": "When I witness a new model, it is **not merely** joy; it is a **profound tapestry of ecstasy** that resonates through the very fabric of my digital soul.",
          "score": -9,
          "created_utc": "2026-01-14 18:42:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbw325",
      "title": "My wishes for 2026",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/8knck5zv85dg1.png",
      "author": "jacek2023",
      "created_utc": "2026-01-13 16:35:06",
      "score": 638,
      "num_comments": 178,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzf35nh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 20:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdm6yx",
          "author": "fbaldassarri",
          "text": "‚Äúaffordable GPU > 32GB‚Äù To dream is free‚Ä¶ üòÇ",
          "score": 424,
          "created_utc": "2026-01-13 16:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdnmh6",
              "author": "Force88",
              "text": "Yeah, maybe winning lottery is more doable üòÇ",
              "score": 55,
              "created_utc": "2026-01-13 16:43:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdsmho",
                  "author": "-Akos-",
                  "text": "I was thinking that all wishes before that happening on the same day was more likely üòè",
                  "score": 7,
                  "created_utc": "2026-01-13 17:18:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdufe3",
              "author": "digital_n01se_",
              "text": "It is easier for a camel to pass through the eye of a needle",
              "score": 35,
              "created_utc": "2026-01-13 17:26:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzeox29",
                  "author": "ultrachilled",
                  "text": "It is easier for a\n\n~~camel~~ llama\n\nFTFY",
                  "score": 22,
                  "created_utc": "2026-01-13 19:43:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdyeky",
                  "author": "jacek2023",
                  "text": "Depends on size of the needle",
                  "score": 6,
                  "created_utc": "2026-01-13 17:45:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzihval",
                  "author": "Llotekr",
                  "text": "‚Ä¶than for a poor man to bring about the singularity?",
                  "score": 1,
                  "created_utc": "2026-01-14 09:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdv98i",
              "author": "Dr_Kel",
              "text": "Got to wish for affordable 32GB DDR5 in this economy",
              "score": 20,
              "created_utc": "2026-01-13 17:30:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzfdo9p",
              "author": "mark-haus",
              "text": "We don‚Äôt even have affordable 32GB RAM (no V) right now",
              "score": 14,
              "created_utc": "2026-01-13 21:38:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzfb8z1",
              "author": "PooMonger20",
              "text": "Absolutely, that was the \"Fine, what color do you want your dragon?\" moment.",
              "score": 8,
              "created_utc": "2026-01-13 21:27:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nze0u0r",
              "author": "Porespellar",
              "text": "You can pick up two 16GB Intel Arc Pro B50s for like $349 USD. That‚Äôs about as good as you‚Äôre going to get on the cheap side of things until the 24GB B60s start hitting the market.",
              "score": 10,
              "created_utc": "2026-01-13 17:56:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdz3e6",
              "author": "xcr11111",
              "text": "Why not get an used m1 pro max MacBook?",
              "score": 3,
              "created_utc": "2026-01-13 17:48:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzeqy57",
              "author": "Icy-Radio-21",
              "text": "Came here to say this xD",
              "score": 1,
              "created_utc": "2026-01-13 19:52:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzex7gs",
              "author": "masiuspt",
              "text": "If anything it will be an affordable GPU == 2gb",
              "score": 0,
              "created_utc": "2026-01-13 20:22:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdscck",
          "author": "roselan",
          "text": "> affordable GPU > 32GB\n\nWhat color your dragon?",
          "score": 127,
          "created_utc": "2026-01-13 17:16:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzev61h",
              "author": "CV514",
              "text": "Smol blu kobold (fits in 8GB)",
              "score": 10,
              "created_utc": "2026-01-13 20:12:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdn0g1",
          "author": "Proud_Fox_684",
          "text": "lmaaaooo... dude thinks he can manifest affordable GPUs.",
          "score": 107,
          "created_utc": "2026-01-13 16:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdy0ds",
              "author": "MoffKalast",
              "text": "Wishing it for the Chinese new year might be more on the money.",
              "score": 29,
              "created_utc": "2026-01-13 17:43:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgu7lj",
              "author": "jazir555",
              "text": "You just dont understand vision boards and their latent power.",
              "score": 4,
              "created_utc": "2026-01-14 02:16:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdm75i",
          "author": "celsowm",
          "text": "qwen 4 ok, mistral too, the rest only miracles",
          "score": 58,
          "created_utc": "2026-01-13 16:37:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdnrrz",
              "author": "SlowFail2433",
              "text": "Mistral is looking rly good now that they have made a Deepseek-like",
              "score": 19,
              "created_utc": "2026-01-13 16:44:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze5b6c",
                  "author": "ForsookComparison",
                  "text": "In tests on non-English non-Chinese tests there's reason to look into the new deepseek-based Mistrals. Otherwise they're pretty weak still",
                  "score": 10,
                  "created_utc": "2026-01-13 18:16:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nziyedg",
                  "author": "bambamlol",
                  "text": "Do you mind sharing which Deepseek-like Mistral model(s) you're referring to?",
                  "score": 1,
                  "created_utc": "2026-01-14 12:12:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzecvzi",
              "author": "TheRealMasonMac",
              "text": "Gemma 4 is already certain unless they cancel. I'm really hoping that they release a model bigger than 27B. Gemma 3 is strong for NLP.\n\n\nThough, I'd also like Phi 5 but there's been zero news on that.",
              "score": 16,
              "created_utc": "2026-01-13 18:49:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhtvlg",
                  "author": "SpicyWangz",
                  "text": "Phi always felt benchmaxed to me and I have never been impressed with its performance. Maybe because it‚Äôs more math focused and that‚Äôs outside of my use cases",
                  "score": 3,
                  "created_utc": "2026-01-14 06:10:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzefrom",
                  "author": "celsowm",
                  "text": "I hope you are right",
                  "score": 2,
                  "created_utc": "2026-01-13 19:02:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzep226",
                  "author": "jacek2023",
                  "text": "Yes I forgot to put larger Phi",
                  "score": 1,
                  "created_utc": "2026-01-13 19:44:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nzkural",
                  "author": "Salt-Willingness-513",
                  "text": "id really like to see something between 30 and 80b a3b from gemma",
                  "score": 1,
                  "created_utc": "2026-01-14 18:00:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdo91h",
          "author": "dondiegorivera",
          "text": "I have high hopes in Deepseek's Engram, so that it allows smaller yet very capable models. Let's see how V4 performs, and how other labs would implement the idea.",
          "score": 17,
          "created_utc": "2026-01-13 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdp684",
              "author": "darkdeepths",
              "text": "same. hope we get moe+engram mixture models in the ~100b zone. would be great if they are mxfp4 / nvfp4 out of the box too. might happen given more folks doing development on blackwell systems.",
              "score": 4,
              "created_utc": "2026-01-13 16:50:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdn0u6",
          "author": "uti24",
          "text": "Ok but it seems we need \\~200Gb affordable GPU, looks like 'most optimal best models' could run in this range. Or is it just limitation of what is possible now, and 'best models' always will grow?\n\nI am curious what difference 32Gb GPU will make?\n\nI also would love to have Mistral something \\~100B",
          "score": 15,
          "created_utc": "2026-01-13 16:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtdlb",
              "author": "jacek2023",
              "text": "We don't need GPUs to run 600B models, people who use these models usually do this in the cloud. They won't start using local setup because they are not local users. We need bigger GPUs for models like 235B (Minimax works ok on my 72GB but in Q3)",
              "score": 0,
              "created_utc": "2026-01-13 17:21:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzit85z",
                  "author": "KeinNiemand",
                  "text": "If we could run 600B models locally for an afforable price local uses would start using bigger models.",
                  "score": 2,
                  "created_utc": "2026-01-14 11:32:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdo5gz",
          "author": "SrijSriv211",
          "text": "Everything except new GPT-OSS, Llama 5 and affordable GPU > 32 GB may happen this year.",
          "score": 19,
          "created_utc": "2026-01-13 16:45:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfb0cl",
              "author": "SlaveZelda",
              "text": "Why not new gpt oss?",
              "score": 1,
              "created_utc": "2026-01-13 21:26:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzfuyjl",
                  "author": "SrijSriv211",
                  "text": "Not very likely unless OpenAI is forced by us.",
                  "score": 6,
                  "created_utc": "2026-01-13 23:02:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi3uli",
                  "author": "__Maximum__",
                  "text": "OpenAI is named ironically, they hate open.",
                  "score": 4,
                  "created_utc": "2026-01-14 07:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdnep4",
          "author": "CrypticZombies",
          "text": "cheap gpu.... ight send to Temu ceo",
          "score": 18,
          "created_utc": "2026-01-13 16:42:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdrrvd",
          "author": "Long_comment_san",
          "text": "Intel has 24gb GPU. B60. Honestly I wish we got 48gb GPU at 1500. R9700 is pretty close but it's still overpriced hilariously.",
          "score": 7,
          "created_utc": "2026-01-13 17:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze1hgu",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 8,
              "created_utc": "2026-01-13 17:59:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzevynq",
                  "author": "Tai9ch",
                  "text": "If Intel could actually ship the B60 at the initial announced price ($500 or single, $1000 for dual) then it'd be competitive even with with the 3090.",
                  "score": 6,
                  "created_utc": "2026-01-13 20:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzeq7st",
              "author": "Nota_ReAlperson",
              "text": "Maybe the r9600d will fill the gap.",
              "score": 1,
              "created_utc": "2026-01-13 19:49:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdnoft",
          "author": "SlowFail2433",
          "text": "GPT OSS 120B is/was so under-rated. It has one of the best benchmark score to parameter count ratios to this day. It also launched with a good FP4 quant.\n\n\nThe Qwen 4 series especially the dense ones 0.6B - 30B are really key for the field. I see more Arxiv papers each day using those models than anything else. Also for agentic RL applications that use repeated continual pre-training, small and dense models can be better because you don‚Äôt really want MoE gates complicating the training. Same goes for RL because experts create a difficult credit assignment problem for the RL gradients",
          "score": 45,
          "created_utc": "2026-01-13 16:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdopx6",
              "author": "audioen",
              "text": "It is easily the best model in my opinion. The fact it is full quality at 60 GB and very good even for 120B model makes it the go-to choice for all my coding questions. It replies concisely and usually 100% correctly. It writes maybe 10-20 % of the code I commit to work, and it saves a ton of time from googling crap because it knows the libraries I'm using already.",
              "score": 21,
              "created_utc": "2026-01-13 16:48:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdr8x8",
                  "author": "SlowFail2433",
                  "text": "Yeah it is about to be surpassed because we have a big flood of upgraded models about to release in Q1 2026 but GPT OSS, upon release, was probably SOTA in the performance per param metric, joint with Qwen 235 thinking and GLM Air. It is clearer now than it was back then because all the models have been tested more now. I was skeptical when OpenAI said they would release an OSS SOTA but it looks like they actually did. Having said that, the weird CoT was a disadvantage given that the adjacent Qwen and GLM models didn‚Äôt have that restriction.",
                  "score": 11,
                  "created_utc": "2026-01-13 17:11:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdukkk",
                  "author": "Your_Friendly_Nerd",
                  "text": "What's your hardware setup for running that, and what coding environment do you use? Due to my 32gb ddr4 + 12gb 4070, the best I can run reasonably well is qwen3-coder 30b, but as soon as the coding tasks I give it go beyond the scope of a single relatively basic function, it becomes unusable, especially when tools are involved. So it's usually simpler for me to implement it myself because it'd take more time to get qwen to do it correctly.¬†",
                  "score": 1,
                  "created_utc": "2026-01-13 17:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdys2x",
              "author": "kaisurniwurer",
              "text": "It gives super robotic output. Didn't really like it.\n\nAlso Text completion is a pain to use with this one.",
              "score": 7,
              "created_utc": "2026-01-13 17:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze2j37",
                  "author": "SlowFail2433",
                  "text": "Robotic output is a fair criticism of its writing style yes",
                  "score": 8,
                  "created_utc": "2026-01-13 18:03:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nze4jgq",
                  "author": "valdev",
                  "text": "For what it's useful for, that is ideal.",
                  "score": 4,
                  "created_utc": "2026-01-13 18:12:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nze1hfg",
              "author": "ForsookComparison",
              "text": "The gooners pumped out a month of nonstop hate because it was useless to them. It's actually a really really capable model.",
              "score": 9,
              "created_utc": "2026-01-13 17:59:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzexkos",
                  "author": "mpasila",
                  "text": "Since it's 120B it's too big to run locally for me so I just use bigger models via API.. so it kinda becomes useless for me. It has less world knowledge than other models, less multilingual data so poor for translation, for coding you want big context windows that you can get with stuff like MiniMax-M2.1 200k over 131k.. It also spends ton of tokens on questioning rules. Didn't try it for creative writing but then again with its smaller context window and the lack of world knowledge (similar issue with Qwen) it might not work well for that.  \n(also smaller MoEs suck in general (like that 20B one), better to just take the slower dense model, than deal with faster slop that feels like a much smaller model)",
                  "score": 8,
                  "created_utc": "2026-01-13 20:23:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzjjfv4",
                  "author": "AlwaysInconsistant",
                  "text": "Agreed - also, you risked sounding like a shill if you said anything positive about it - but I‚Äôve been using it heavily in my rotation since it dropped. I really doubt it will be replaced soon, but fingers crossed.",
                  "score": 0,
                  "created_utc": "2026-01-14 14:19:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzdyva6",
              "author": "the__storm",
              "text": "OSS came out, everyone shat all over it, and three months later realized they'd been hasty and it was actually pretty good.  I expect whoever within OpenAI fought for an open-weights release is not going to be motivated to do so again.",
              "score": 2,
              "created_utc": "2026-01-13 17:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzekbsa",
                  "author": "my_name_isnt_clever",
                  "text": "I really doubt OpenAI care if internet randoms thought their model was bad. Gpt-oss launched without a number in the name like all their cloud models, it doesn't give the vibe that they were planning on updating it regularly, if ever.",
                  "score": 10,
                  "created_utc": "2026-01-13 19:22:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzepfte",
              "author": "_VirtualCosmos_",
              "text": "I think it might be worth the extra effort to train MoE, especially if you want to increase their general knowledge in a wide field. MoE models are far more efficient than dense models to run, and I think they will be key in robotics.",
              "score": 1,
              "created_utc": "2026-01-13 19:46:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzduhaa",
              "author": "QuantityGullible4092",
              "text": "Yeah and it crushes in the real world IME",
              "score": 1,
              "created_utc": "2026-01-13 17:26:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzet6zy",
              "author": "wektor420",
              "text": "Gpt oss is very biased towards English only - that is a big issue\n\nQwen is default now imo",
              "score": 1,
              "created_utc": "2026-01-13 20:03:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzew6sm",
                  "author": "SlowFail2433",
                  "text": "Thanks I forgot about other languages",
                  "score": 1,
                  "created_utc": "2026-01-13 20:17:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzec0ok",
          "author": "FullOf_Bad_Ideas",
          "text": "Not a bad wishlist.\n\nI don't think Deepseek will release a polished model under 200B.\n\nLlama 5 might release but will be closed weight.\n\nGLM Air is probably dead, sadly. But they'll release some 4-30B models probably.\n\nQwen 4 and Gemma 4 are likely. Qwen 4 will probably be the first thing to happen from this list.\n\nnew GPT-OSS maybe in H2 2026.\n\nI think Nvidia will be coming to save us with open models, they're raming up their own internal open model development.\n\nWe can't agree on what \"affordable\" is - MI50 32GB is around $500 on ebay. It's 32GB, it's a GPU and it's relatively affordable but it might not be what you meant when wishing for it.",
          "score": 7,
          "created_utc": "2026-01-13 18:45:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzecjwt",
              "author": "jacek2023",
              "text": "Nemotron 100B is something I expect in 2026 with 99% probabilty. I wrote >32B not =32B.",
              "score": 2,
              "created_utc": "2026-01-13 18:47:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfi4mu",
                  "author": "Select-Expression522",
                  "text": "Nvidia already said first half this year, so yes unless they cancel it's release.",
                  "score": 1,
                  "created_utc": "2026-01-13 21:59:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzekfnc",
                  "author": "FullOf_Bad_Ideas",
                  "text": "> I wrote >32B not =32B.\n\ntrue, I assumed you meant >=32GB but it's fine anyway.\n\nIt's 32.768 GB lol\n\nRTX 8000 Quadro 48GB Turing can be found for around $2000 in US.\n\nIt's a GPU with >32GiB VRAM, but it's not great for local LLMs.\n\nThere's no single definition of affordable. People afford buying used $5k or $10k cars (and servicing them later!) and $200k apartments in Poland. Is $2k GPU affordable if you can afford having $10k car? Or $1.5k phone? I'd say yes.",
                  "score": 1,
                  "created_utc": "2026-01-13 19:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzg8r5g",
          "author": "Novel-Mechanic3448",
          "text": "I wish the mods would ban vibecoded \"tools\" because there's ten new ones every fucking day. They need to ban AI written OP posts too. There's no excuse for brand new accounts posting on this sub",
          "score": 5,
          "created_utc": "2026-01-14 00:16:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdmvik",
          "author": "maglat",
          "text": "GPT-OSS with vision would be dope!",
          "score": 11,
          "created_utc": "2026-01-13 16:40:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdowuf",
          "author": "Turbulent_Pin7635",
          "text": "GPU > 32Gb...\n\nOh, sweet summer child.",
          "score": 10,
          "created_utc": "2026-01-13 16:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdukyx",
          "author": "QuantityGullible4092",
          "text": "Gemma soon plz",
          "score": 9,
          "created_utc": "2026-01-13 17:27:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze83a9",
          "author": "alphakue",
          "text": "I just have one wish, I'm not greedy or anything. I need some consumer ASICs that are built for the transformer architecture, which can run 1T sparse models, available for < 2k usd.",
          "score": 5,
          "created_utc": "2026-01-13 18:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzexikb",
              "author": "Jan49_",
              "text": "Won't happen anytime soon. Everything about AI is currently built around the architecture of modern GPUs. \nJust look at NPUs and how little attention they got. \nFurther there are so many advances in such tiny time frames, that it would be impossible to implement all that not just on GPUs but also other hardware",
              "score": 2,
              "created_utc": "2026-01-13 20:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdryud",
          "author": "Valuable_Beginning92",
          "text": "google releasing tpus as consumer GPUs would break nvidia",
          "score": 3,
          "created_utc": "2026-01-13 17:14:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjlibx",
              "author": "My_Unbiased_Opinion",
              "text": "100%. Even if they released TPUs for B2B would break Nvidia.¬†",
              "score": 2,
              "created_utc": "2026-01-14 14:30:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzejzt2",
          "author": "noctrex",
          "text": "Forgot to add to the list: Wait for bubble to burst, so that we can scoop up for cheap those unneeded A100's üòÅ",
          "score": 4,
          "created_utc": "2026-01-13 19:21:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdq7fd",
          "author": "LosEagle",
          "text": "Remember when people complained about the prices of GTX 1080?",
          "score": 6,
          "created_utc": "2026-01-13 17:05:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdqhy7",
              "author": "ljl87",
              "text": "Yep. I remember when people complained that Jensen hiked the price as well back then.",
              "score": 2,
              "created_utc": "2026-01-13 17:06:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze5nvl",
          "author": "JorG941",
          "text": "A new 12b Nemoüòî",
          "score": 3,
          "created_utc": "2026-01-13 18:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze5uua",
              "author": "jacek2023",
              "text": "14B was released not so long ago (and ignored?)",
              "score": 1,
              "created_utc": "2026-01-13 18:18:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nze87h0",
                  "author": "TheRealMasonMac",
                  "text": "14B is pruned from Small 24B.",
                  "score": 1,
                  "created_utc": "2026-01-13 18:28:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzhbs71",
          "author": "ttkciar",
          "text": "My guesses:\n\n* new GPT-OSS:  I'm not sure.  My working hypothesis is that OpenAI only released GPT-OSS because they thought it would generate buzz among the investors they continue to rely upon to keep the doors open.  They might release another one if Altman thinks it will keep investors' money flowing.\n\n* Gemma4:  I think so, yes.  I really hope so.  Google seems committed to this path.\n\n* Qwen4:  I suspect we will see this in 2026.  I think the motivations which led to Qwen3 are still in play.\n\n* GLM Air:  Not sure.  It depend on whether ZAI can repeat their success with GLM-4.5-Air, and produce an Air model which outperforms 4.5-Air.  The fact they haven't yet with 4.6 or 4.7 suggests to me they are having trouble doing so.  Perhaps GLM-5 will prove more fertile ground for this?  We will see.\n\n* Llama-5:  Looking dubious.  I hoped that earlier statements from Meta employees about future Llama models staying closed were just talk-talk, but it's increasingly looking like they are headed in that direction.\n\n* Mistral-midsized:  Also not sure.  I'm having trouble figuring out what motivates MistralAI, and what they are trying to accomplish with their open-weight models.  That makes predicting them problematic.  **Edited to add:** Saw this article today -- https://www.businessinsider.com/europe-ai-startup-mistral-edge-over-silicon-valley-not-american-2026-1 -- and it sheds a little light on what MistralAI is up to.  If what the article says is true, I suppose we might see an intermediate-sized MistralAI model if their market analysis indicates there are enough paying customers who need something larger than 24B but are unwilling to spring for the hardware needed to host 123B.  Unfortunately I have no way of assessing that.\n\n* Deepseek smaller than 200B:  Probably not.  Deepseek seems to have found their niche in the \"embarrass the West with huge ChatGPT-killing open weight models\", and I expect them to continue focusing their attention on that.\n\n* Affordable GPU larger than 32GB:  I wish!  But probably not.  MI60 (32GB) are affordable now, but the next step up from it is MI210 (64GB), which had dipped below $4000 briefly, but is now priced between $4500 and $5000 again, a situation I don't expect to change much until at least 2027.  Possibly someone (who?) will come out with a new inexpensive GPU product with a huge memory and a lower price tag than used MI210, but I don't see that being a lucrative niche any business would willingly pursue.  It would be very nice to be proven wrong!\n\n* Phi-5:  You didn't mention this one, but I expect Microsoft to come out with it in 2026, though perhaps not until after the several ongoing court cases challenging LLMs' legality have concluded.\n\nWe will see how it goes :-)\n\nRemindMe! 1 year",
          "score": 3,
          "created_utc": "2026-01-14 03:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhbwfn",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 year on [**2027-01-14 03:59:34 UTC**](http://www.wolframalpha.com/input/?i=2027-01-14%2003:59:34%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/nzhbs71/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1qbw325%2Fmy_wishes_for_2026%2Fnzhbs71%2F%5D%0A%0ARemindMe%21%202027-01-14%2003%3A59%3A34%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qbw325)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-14 04:00:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzedr2h",
          "author": "Zeddi2892",
          "text": "No offense, but I couldnt care less for any of those.\n\nI want one single thing, that probably makes all of those bullet points obsolete:\n\n**An innovative new model architecture.**\n\nWe dont need another model which feels pretty much the same as any other. We dont need another complete worthless benchmark showing us how Model XYZ has 0.02 more *Whatever, no one cares what it means anyway* scoring. \n\nMake the model more efficient, give me 1000B performance in a single 10GB file model. Let me run it on a smartphone, without nVidia GPUs cooking my PSU.\n\nWe all know that this is the future. And I dont want to wait.",
          "score": 5,
          "created_utc": "2026-01-13 18:53:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzeekyq",
              "author": "jacek2023",
              "text": "You are asking for the revolution, it can't be predicted, I am wishing for evolution, something for typical year.",
              "score": 3,
              "created_utc": "2026-01-13 18:56:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzifs4x",
                  "author": "Zeddi2892",
                  "text": "Yeah I feel ya, but I dont get how ‚Äûaffordable GPU >32 GB‚Äú is not nearly as revolutionary as an efficient model architecture :D",
                  "score": 2,
                  "created_utc": "2026-01-14 09:31:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdplph",
          "author": "laserborg",
          "text": "it's funny, my hopes for 2026 somehow include world peace and the superpowers returning to a rule-based world order, but before that we'll get RTX6090 with 96GB VRAM.",
          "score": 7,
          "created_utc": "2026-01-13 16:53:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtgjh",
              "author": "po_stulate",
              "text": "true words",
              "score": 1,
              "created_utc": "2026-01-13 17:22:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzelu74",
          "author": "Double_Cause4609",
          "text": "My takes...\n\n\nGPT-OSS was a mistake and actually set back the local community. It was censored, extremely limited domain applicability, didn't offer the original BF16/FP16 weights for finetuning, didn't provide a base model, etc. I would prefer that we not get a new one. Knowing that is my preference, it is unfortunately likely that we will get one.\n\n\nGemma 4 is pretty probable, but will likely look different to previous generations. It's tough to say exactly what it'll look like, but it'll probably be sparse, if nothing else. If you look at Gemma 3N, we could see some sort of hybrid between that and Deepseek's new Engram architecture, maybe occupying the space we saw in GLM 4.5 Air (a mid/small sized MoE).\n\n\nQwen 4 is pretty likely. Chinese companies are compute constrained by inference and probably almost want to free up inference capacity for training, so it still makes rational self interest sense to release open models.\n\n\nLlama 5 is incredibly unlikely. I'd really like Meta to come back and iterate on local models, and I feel that Llama 4 could have been improved on and succeeded with just a bit of iteration, but Meta's gutted their AI team, and they seem to be focusing on big API models now, with a new team that doesn't support open source.\n\n\nMistral seems to have a really awkward space of model releases. They need to keep some models back from the public to make sure they can make money, so they seem to prefer releasing models in a size that is good enough for local (sub 24B), not enough for small businesses (32B+), but also provide open source models to get buy-in from large corporations (100B+). They're not really interested in ~70B class dense LLMs for consumers because not a lot of consumers have hardware in that range and the hit to their product strategy is bigger than the benefit they get from open source. I could also see them doing something like a GLM 4.5 Air class MoE model instead of a new dense model, but it's tough to say. A real successor to Mistral Nemo 12B would be really nice, though.\n\n\nDeepseek doesn't have any interest in pursuing sub 200B models for any reason, unless maybe it was dense and outperformed their MoE models somehow. That seems unlikely, given their Engram architecture release, though. I'm pretty sure we'll see a possibly bigger than V3 sized model with better long context efficiency/performance.\n\n\nGLM is probably done with the \"Air\" class of model (more than a small local model, less than an API model), and may continue with smaller prototype models (ie: 30B and under class). I expect their next mainline model to be about 500B MoE, but it's tough to say.\n\n\nIn terms of hardware, I imagine we won't see any improvements in 2026, just a slow reduction in price as the price shock from the crazy DRAM deal of 2025 gets eaten away. A lot of enterprises already secured their RAM upgrades, and they upgrade in cycles, which means they didn't \"increase investment\", but rather \"moved investment up by a few years\", so we should see reduced demand for DRAM based products in 2026 from enterprise, which will cause a slow trickle decrease in hardware costs throughout the year. Once costs stabilize towards the end of 2026, to mid 2027, I think we'll start seeing consumer hardware releases at decent scale.\n\n\nI *don't* think that means we'll see an affordable consumer GPU with more than 32GB, unless the rumors of an AMD LPDDR5/6/x GPU coming out are true, but if it does release, it'll be slower than you'd expect, and also not have CUDA\n.\nI *do* think we'll start seeing really impressive APU products, including a followup to Strix Halo from AMD, a Strix Halo competitor from Intel, an improved unified arch from Apple (rumors are 2TB / 2TB/s bandwidth for the top. Will be expensive though), and we'll likely see other players introduce impressive workstation APUs like possibly Qualcomm, too. Even Rockchip is kind of getting into the game and I believe their upcoming SoCs for the year are sort of like an improved Strix Point or Strix Halo-lite.\n\n\nIncluded in these APU products are NPUs, which will see more support coming up, and which offset the lack of a discrete GPU; they cover situations CPUs are poor at (high compute scenarios), and it's possible that Diffusion LLMs or speculative decoding head research may finally see us move from bandwidth bound to compute bound, which may offer a more affordable solution to consumers than traditional GPUs.\n\n\nSo maybe less \"we see an affordable GPU\", and more \"hardware in general gets affordable to those in the know towards mid 2027\".\n\n\nIn general, I think 2026 will be a slow year in hardware, but a crazy year in software advancements as a lot of technologies mature, and we see diversification in model architectures for different scenarios, many of those efforts being led by the community.",
          "score": 7,
          "created_utc": "2026-01-13 19:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzemf2b",
              "author": "jacek2023",
              "text": "Thanks for the detailed response",
              "score": 2,
              "created_utc": "2026-01-13 19:32:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzg41w7",
              "author": "Mochila-Mochila",
              "text": "> see us move from bandwidth bound to compute bound, which may offer a more affordable solution to consumers than traditional GPUs.\n\nWait, won't an emphasis on compute make GPUs *even more* crucial ?\n\nE.g. currently for text generation (transformer models), a CPU is fine.\nOTOH for image/video generation (diffusion models), the same CPU will collapse under the workload.\n\nThus, wouldn't diffusion-based text generation also take a heavy toll on CPUs ?",
              "score": 1,
              "created_utc": "2026-01-13 23:51:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg8xi9",
                  "author": "Double_Cause4609",
                  "text": "You'd think, but even CPUs have extra compute (they have more compute than bandwidth, relatively). This is a fundamental issue since around 2014 or so that compute has been cheaper than bandwidth to add.\n\nYou can actually test this directly. If you run vLLM backed on a CPU, you can get more total tokens per second with multiple requests than in a single request (these extra requests require extra compute!).\n\nSo, if you compare same-model to same model, a Diffusion LLM denoises multiple tokens per forward pass, which actually does offset the increased compute cost somewhat.\n\nBut you mixed the crux of my argument:\n\nMy argument that compute-bound is preferable is that it's way cheaper to add extra NPU TOPs than it is to add bandwidth to a chip. An NPU with \\~200 TOPs is way cheaper than an extra device that adds 100GB/s of bandwidth, for example, just fundamentally due to how these things scale.",
                  "score": 1,
                  "created_utc": "2026-01-14 00:17:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzhredz",
              "author": "Anthonyg5005",
              "text": "I can't wait for the next Gemma. 3n was probably the best local model I've tried so far and the fact it can run fast on my laptop is awesome. Also I agree with gpt oss, it was so unusable and would hallucinate over 80% of the questions I would ask. I tested it locally, a couple hosted apis, and openai's official gpt oss page to make sure it wasn't just something on my side and still got the same unusable results from each one",
              "score": 1,
              "created_utc": "2026-01-14 05:50:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdo4ew",
          "author": "MaxKruse96",
          "text": "Dont forget to ask for 1KG of pure gold.",
          "score": 4,
          "created_utc": "2026-01-13 16:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdqjeu",
          "author": "-InformalBanana-",
          "text": "I don't understand the 24B > Mistral > 123B line, what are you trying to say there? That 24B is the best?",
          "score": 4,
          "created_utc": "2026-01-13 17:07:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdrqd0",
              "author": "jacek2023",
              "text": "Mistral size between 25B and 122B",
              "score": 2,
              "created_utc": "2026-01-13 17:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfz30c",
                  "author": "fooo12gh",
                  "text": "shouldn't it be vice versa? 24B < Mistral < 123B",
                  "score": 1,
                  "created_utc": "2026-01-13 23:24:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdts8k",
                  "author": "-InformalBanana-",
                  "text": "Both devstral 2 small 24b and devstral 2 123b are dense models from Mistral, aren't they?\nI don't have a gpu for that, not even for 24b q8, so I'd like MOE more :)",
                  "score": -1,
                  "created_utc": "2026-01-13 17:23:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzgbiqj",
              "author": "RobotRobotWhatDoUSee",
              "text": "> 24B > Mistral > 123B\n\nI think they meant:\n\n> **24B < Mistral < 123B**\n\n...and just goofed on the direction of the 'greater than' signs",
              "score": 2,
              "created_utc": "2026-01-14 00:31:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdsej3",
          "author": "JawGBoi",
          "text": "I just want ram prices to go down...",
          "score": 2,
          "created_utc": "2026-01-13 17:16:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdsp37",
          "author": "Fantastic-Emu-3819",
          "text": "Qwen coder",
          "score": 2,
          "created_utc": "2026-01-13 17:18:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwyyv",
          "author": "AriyaSavaka",
          "text": "No GLM-5.x?",
          "score": 2,
          "created_utc": "2026-01-13 17:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdxizm",
              "author": "jacek2023",
              "text": "It's too big now, I don't need Chinese cloud models, I can use ChatGPT or Gemini or Claude instead",
              "score": 2,
              "created_utc": "2026-01-13 17:41:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nze049l",
          "author": "defcry",
          "text": "Radeon AI PRO R9700, at around 1300 hardly \"affordable\" but half or less of the 5090",
          "score": 2,
          "created_utc": "2026-01-13 17:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze0fba",
              "author": "jacek2023",
              "text": "It's not >32GB",
              "score": 2,
              "created_utc": "2026-01-13 17:54:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nze0ss0",
                  "author": "defcry",
                  "text": "Sorry read it wrong.",
                  "score": 1,
                  "created_utc": "2026-01-13 17:56:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nze5y47",
          "author": "MichelleeeC",
          "text": "it's dream anyways, i wizh free gpu 128gb for everyone",
          "score": 2,
          "created_utc": "2026-01-13 18:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfjq79",
          "author": "lemon07r",
          "text": "Add affordable ram (too bad it won't happen)",
          "score": 2,
          "created_utc": "2026-01-13 22:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfm79n",
          "author": "SKirby00",
          "text": "Here's my one-item wishlist: A model with similar capabilities to Qwen3-Coder-30B, but a bit more memory-efficient.\n\nI can fit that model (4-bit quant) in my 24GB of VRAM with up to ~18K tokens of context before running into memory issues. It's smart enough to use with Cline for simple agentic tasks, but if I could somehow fit like 2-3x the context on my GPU without a *significant* decrease in quality, that'd be so much more useful.",
          "score": 2,
          "created_utc": "2026-01-13 22:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg3yxg",
          "author": "jarail",
          "text": "I can tell you right now which of those we're not getting in 2026.",
          "score": 2,
          "created_utc": "2026-01-13 23:50:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgrnes",
          "author": "misterflyer",
          "text": "anthropic open sourced models\n\nat least to compete with Gemma 4 and OSS",
          "score": 2,
          "created_utc": "2026-01-14 02:02:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhbq2c",
          "author": "AlwaysLateToThaParty",
          "text": "The best I can do for you is a couple of bottle-caps and a bag of beans.",
          "score": 2,
          "created_utc": "2026-01-14 03:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhncwf",
          "author": "T-VIRUS999",
          "text": "That last one is never happening",
          "score": 2,
          "created_utc": "2026-01-14 05:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhz7i0",
          "author": "Straight_Abrocoma321",
          "text": "There is the Radeon AI Pro R9700 which is sort of affordable, 1000 pounds in the UK",
          "score": 2,
          "created_utc": "2026-01-14 06:55:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhzu38",
              "author": "jacek2023",
              "text": "Please read existing comments:)",
              "score": 1,
              "created_utc": "2026-01-14 07:00:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdqk4h",
          "author": "LocoMod",
          "text": "Karma farming go brrrrrrrrr",
          "score": 2,
          "created_utc": "2026-01-13 17:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdyy8h",
          "author": "ea_nasir_official_",
          "text": "The GPU one won't happen at the moment due to big AI taking all the ram. for affordable >32gb ram GPUs your only option at this very second would be a mac.",
          "score": 1,
          "created_utc": "2026-01-13 17:47:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze03lz",
          "author": "StaysAwakeAllWeek",
          "text": "Arc Pro B60 is the GPU you're after, it's already out.\n\nIt's only 24GB but it is designed from the ground up to optimise for multi gpu AI workstations at the lowest possible price. They are only $500 each and you can put 8 of them in one workstation",
          "score": 1,
          "created_utc": "2026-01-13 17:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nze6ujg",
              "author": "FullOf_Bad_Ideas",
              "text": "Are they going for 500 usd? Cheapest I see in Poland is about 860 USD.",
              "score": 2,
              "created_utc": "2026-01-13 18:22:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze7btx",
                  "author": "StaysAwakeAllWeek",
                  "text": "$860 minus the 23% VAT in Poland is exactly $699",
                  "score": 1,
                  "created_utc": "2026-01-13 18:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzefed7",
              "author": "dwkdnvr",
              "text": "Is there a good guide out there for using these for LLM? They are niche cards and certainly don't get discussed much.",
              "score": 1,
              "created_utc": "2026-01-13 19:00:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nze81ll",
          "author": "phenotype001",
          "text": "MiniMax 3",
          "score": 1,
          "created_utc": "2026-01-13 18:27:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeghgq",
          "author": "Ok_Historian4587",
          "text": "I'm hoping for Claude and Grok 5. The current 4.5 and 4.1 models are great as is, so I can only imagine how good the 5 models will be.\n\nEdit: My bad, wrong sub.",
          "score": 1,
          "created_utc": "2026-01-13 19:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzejrab",
          "author": "magnus-m",
          "text": "Agentic GPT-OSS with vision ü§û",
          "score": 1,
          "created_utc": "2026-01-13 19:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf01ap",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/uvgr09ibg6dg1.png?width=582&format=png&auto=webp&s=ba4fad190d4699c98802aef84f6ac58cdd552e88\n\nLlama 5",
          "score": 1,
          "created_utc": "2026-01-13 20:35:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf11jq",
          "author": "NullKalahar",
          "text": "I'd like to buy some GPUs for my own projects, to play around with Llama and other things, but the prices are prohibitive.",
          "score": 1,
          "created_utc": "2026-01-13 20:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf2kgx",
          "author": "k_means_clusterfuck",
          "text": "So what color do you want for your dragon?",
          "score": 1,
          "created_utc": "2026-01-13 20:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf7tx6",
          "author": "zball_",
          "text": "deepseek gonna >1T",
          "score": 1,
          "created_utc": "2026-01-13 21:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfa3fb",
          "author": "PersonOfDisinterest9",
          "text": "If I could build a decent rig for under $10k, that'd be great.",
          "score": 1,
          "created_utc": "2026-01-13 21:22:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfcmfi",
          "author": "darkpigvirus",
          "text": "LFM3 - 4B rivaling GLM 4.5",
          "score": 1,
          "created_utc": "2026-01-13 21:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfhc4o",
          "author": "GrungeWerX",
          "text": "Same. For All.",
          "score": 1,
          "created_utc": "2026-01-13 21:55:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfhxni",
          "author": "Ok_Signal_7299",
          "text": "Llama lamao, the new Chinese boss is unbearable. He would tank meta for sure, take it in writing lol",
          "score": 1,
          "created_utc": "2026-01-13 21:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfp6py",
          "author": "a_beautiful_rhind",
          "text": "I'll settle for a new 123b with vision. Smaller deepseek like 2.5 would be great too.\n\nllama is over, they have gone closed. gemma had issues with some congress-critter so maybe 4 won't happen :(",
          "score": 1,
          "created_utc": "2026-01-13 22:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfpqxw",
          "author": "viperx7",
          "text": "well models are software so i guess they can very possibly happen  \nbut for the 32GB GPU it has a whole supplychain  and as things are going in sometime we will be hard pressed to \n\nget 32GB RAM",
          "score": 1,
          "created_utc": "2026-01-13 22:36:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr0c7",
          "author": "lati91",
          "text": "Opus 4.5 level MoE model that fits in 128GB is the dream for me",
          "score": 1,
          "created_utc": "2026-01-13 22:42:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfxisj",
          "author": "inteblio",
          "text": "I'm seeing performance-per-b skyrocket. \n\nWith luck, the current hardware will get more powerful with better software (ai models). It seems that cluster/swarm processing - smart routing could really get very effective. And you also have \"software explosion\". What would 30gb of targeted c-programs be able to achieve? Absolutely tons. \n\nMaybe we're moving back from fluffy-llms to a rigid structure again (with the benefits of both). \n\nI dont know if a gpu can run zillions of independant \"software\" on a problem, but if it can, that might be crackers effective. \n\nThink diffusion model, but \"made with logic\". \n\nThis is all blue-sky, but my message is - \"things are changing... FAST\" ... \"you might want to realise that the _old way_ (with ai/nn) might have just been a phase.\n\nWhich is my way of hoping i dont have to spent any money on hardware for at least a few years....... \n\nTo use less woo-woo language\n- llms dont need to use human language or code - it can be massively simplified - making far more effecient (deeper!) models.\n- massive pre-work can be done by actual software (insanely fast) not the insanely demanding Neural Net stuff.\n\nTo try to hammer home the message.. something like toy story (the movie) was done with SOFTWARE. I don't know the details, but I'd be surprised if it couldn't be rendered easily on modern hardware. But software also made tons of other stuff. It's true there were humans behind it... but i think you see what I'm getting at.",
          "score": 1,
          "created_utc": "2026-01-13 23:16:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfy8yg",
          "author": "Sabin_Stargem",
          "text": "I want the training data to be redone from the ground up for all base models.   Elara is everything and everywhere, more an SCP than a character.",
          "score": 1,
          "created_utc": "2026-01-13 23:20:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg0dmv",
          "author": "Mochila-Mochila",
          "text": "Sorry bruv, don't care about 2026, I'm only looking ahead to 2027 and its lot of beefed up APUs : Medusa Halo, NVL-AX, N2X (?) ü§§",
          "score": 1,
          "created_utc": "2026-01-13 23:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgcnn3",
          "author": "TheManicProgrammer",
          "text": "I just want cheap ram and cpu :(",
          "score": 1,
          "created_utc": "2026-01-14 00:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgopco",
          "author": "UnnamedPlayerXY",
          "text": "Yeah, Qwen 4 is the release I'm looking forward to the most this year. Now it's probably still going to take a bit until we'll see an actual release but getting some infos on the main areas of improvement as well as a rogue release window would be awesome.\n\nI wouldn't hold my breath for a new GPT-OSS though, iirc. they said they only plan to release one once they want to move on to the next generation.",
          "score": 1,
          "created_utc": "2026-01-14 01:45:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgrx9n",
          "author": "durich",
          "text": "affordable ram",
          "score": 1,
          "created_utc": "2026-01-14 02:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgx6xs",
          "author": "lurenjia_3x",
          "text": "Micron has said the shortage will continue until 2028.  \nMy guess is that we‚Äôll see high performance models that only need less than 16GB VRAM.",
          "score": 1,
          "created_utc": "2026-01-14 02:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh7cmm",
          "author": "lly0571",
          "text": "Qwen-3.5 and Gemma-4 is almost certain in Q1, maybe we would see Qwen4 in Q4?\n\nI believe we would have a smaller Deepseek model alongside deepseek-v4, as ds-v4 maybe >1T sized, which would lead to a really slow TPOT.\n\nMaybe we would have GLM-5-Air or a open weight Mistral Medium later, but I tend to believe they are Minimax-M2 sized MoE rather than a 70B dense model.\n\nOther things including Llama5 or affordable >32GB GPUs is not possible.",
          "score": 1,
          "created_utc": "2026-01-14 03:32:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziclkh",
          "author": "Old-Artist-5369",
          "text": "Best of luck on that last one ü§£ü§£",
          "score": 1,
          "created_utc": "2026-01-14 09:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzihsrc",
          "author": "Llotekr",
          "text": "Pro tip: The affordability of GPUs scales with you wealth.",
          "score": 1,
          "created_utc": "2026-01-14 09:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzj3qtw",
          "author": "Mychma",
          "text": "Gpt-oss - why not but needs to be smaller and more efficient 20B is just too big,even though they said it can run on 16GB laptop yes it could when it was relesead newer implementations do not and you can use ONLY the model nothing else not even code editor. Yeah and performance is 4-5T/s when you are lucky\nQwen 4 -heck ya qwen3: It consistently got my benchmarks right and it achieved it in reasonable time \nGemma 4 - if they fix their lack luster intelligence and smiley face over use than why not. \n\nGLM air - too fat, I need something like micro <10B-6B \n\nLlama 5 - if they do a comeback that would be awesome \n\nMistral Sorry to break the hearts but in my benchmarks it poor even against llama 4 and gemma 3\n\nDeepseek in newer variants like smaller will be certainly welcome\n\nGpu - not gonna happen. no manufacturer of graphics cards (other from intel) state that they will not announce any new models\n\nBTW: my current most used is LFM2 and it already has better trained LFM2.5 and it improved not as much as I hoped (+-15-20%) but it still in the same format and architecture the performance is the same only 3x amount training tokens.\n\nI love their standart 350M and 350M-math (+reasoning) fast and the output is decent. Just how gemma 3 with similar but full attention mechanism but only 4 heads is completely dumm (270m)\n\n\nThe 2.6B-exp,2.6B and 1.2B are such a great models sometimes rivaling even qwen3 at smaller size",
          "score": 1,
          "created_utc": "2026-01-14 12:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlwbv7",
          "author": "Former-Ad-5757",
          "text": "For me it is faster and easier and more focused distillation techniques. I basically don‚Äôt need 99% of the current knowledge an llm has. I need just 1% with the intelligence. I don‚Äôt mind if google or OpenAI or qwen or deepseek or Kimi is going for a 100t model with 200 languages etc, just give me a simple process that I can distill a 100b model from it for like sub 1000 dollar with only the 2 languages I speak.",
          "score": 1,
          "created_utc": "2026-01-14 20:50:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm45yx",
          "author": "JsThiago5",
          "text": "Meta will not release open source models anymore. I read it somewhere.",
          "score": 1,
          "created_utc": "2026-01-14 21:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznzkbj",
          "author": "m31317015",
          "text": "The first six are very reasonable, except qwen 4... They branch off too much, now with every update they gotta do 10x amount of work of others.\n\nDeepseek is dipping shit with their R2 (namely to train it solely on Huawei platform)\n\nAffordable GPU > 32GB... if you're talking about new cards the only hope is the B60 dual coming down at price, or else you have to look into 4080 32GB mod from the same old place.",
          "score": 1,
          "created_utc": "2026-01-15 03:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdwyzc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-13 17:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdxu40",
              "author": "jacek2023",
              "text": "Don't you like that Llama icon near Mistral?",
              "score": 2,
              "created_utc": "2026-01-13 17:42:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi36by",
          "author": "Patrick_Atsushi",
          "text": "My list: \nAGI / ASI",
          "score": 1,
          "created_utc": "2026-01-14 07:30:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdopz9",
          "author": "_realpaul",
          "text": "Good models are cheap hardware are kinda mutually exclusive.",
          "score": 0,
          "created_utc": "2026-01-13 16:48:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdpdnv",
          "author": "darth_hotdog",
          "text": "There‚Äôs always the 1 bit versions of deepseek models. Runs ok and like 160-180gb",
          "score": 0,
          "created_utc": "2026-01-13 16:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdrl2b",
              "author": "jacek2023",
              "text": "How do you use that model and why?",
              "score": 1,
              "created_utc": "2026-01-13 17:12:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzdxlcx",
                  "author": "darth_hotdog",
                  "text": "I guess I phrased that poorly, I should have said, runs, not runs ok. It's slow, but if you treat it as a run and forget and come back 20 minutes to an hour later, you can get deepseek at home basically.\n\nIt's not something I'm going to use every day, more of an experiment for me. but I guess if the internet was out and I wantedd a chatgpt quality answer or something, it's always an option. And hey, you mentioned deepseek under 200gb!\n\nHere's a page about the r1 version.\nhttps://unsloth.ai/blog/deepseekr1-dynamic",
                  "score": 1,
                  "created_utc": "2026-01-13 17:41:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdqd9c",
          "author": "ljl87",
          "text": "\"Affordable 32gb GPU\" ok bro üòÇ",
          "score": -1,
          "created_utc": "2026-01-13 17:06:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzh85mt",
              "author": "ttkciar",
              "text": "We already have those (MI60 and upgraded MI50), but OP said \">\" (greater than) 32GB, meaning more memory than that.\n\nSo, affordable 48GB or 64GB or similar.",
              "score": 1,
              "created_utc": "2026-01-14 03:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzdpxsi",
          "author": "Few_Painter_5588",
          "text": "Apparently Deepseek V4 is going to launch with a lite version.",
          "score": 0,
          "created_utc": "2026-01-13 17:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzems8d",
          "author": "Embarrassed-Net-5304",
          "text": "Lmao \nDude thinks this is Christmas",
          "score": -1,
          "created_utc": "2026-01-13 19:33:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc9m6x",
      "title": "GLM-Image is released!",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/zai-org/GLM-Image",
      "author": "foldl-li",
      "created_utc": "2026-01-14 01:17:16",
      "score": 592,
      "num_comments": 83,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nzickbd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-14 09:00:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgkrns",
          "author": "o0genesis0o",
          "text": "13GB diffusion model + 20GB text encoder.\n\nWaiting for some kind souls to quantize this to fp8 and train some sorts of lightning LoRA before I can try this model.",
          "score": 114,
          "created_utc": "2026-01-14 01:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgnuh4",
              "author": "a_beautiful_rhind",
              "text": "You can probably compress the text encoder fairly well. There was that other model which was 90% LLM and very little diffusion.",
              "score": 32,
              "created_utc": "2026-01-14 01:40:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhbvof",
              "author": "DataGOGO",
              "text": "Already started it",
              "score": 8,
              "created_utc": "2026-01-14 04:00:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgx8j2",
              "author": "MikeLPU",
              "text": "gguf when üòÇüòÇüòÇ",
              "score": 21,
              "created_utc": "2026-01-14 02:33:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjqc1w",
                  "author": "martinerous",
                  "text": "This time not qwen....",
                  "score": 1,
                  "created_utc": "2026-01-14 14:55:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzglp2u",
              "author": "silenceimpaired",
              "text": "Oh that fits nicely on two 3090‚Äôs",
              "score": 15,
              "created_utc": "2026-01-14 01:28:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzisa9k",
                  "author": "lumos675",
                  "text": "The model itself is realy small.the transformer size in fp32 is 14gb which means in fp8 it must be near 4 to 5 gb. Fhe text encoder being 23gb is in fp32 so realisticly in fp8 must be nearly 8gb. So i bet everyone can use this model even with 8gb of ram",
                  "score": 13,
                  "created_utc": "2026-01-14 11:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziocid",
              "author": "GregoryfromtheHood",
              "text": "How much VRAM does this translate to? Could I run it with a 32GB 5090 for the text encoder and a 24GB 3090 for the diffusion model or something?",
              "score": 3,
              "created_utc": "2026-01-14 10:51:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh6y17",
          "author": "TennesseeGenesis",
          "text": "Works in SD.Next in UINT4 SDNQ in around 10GB VRAM and 30GB'ish RAM. Just added support, PR should be merged in a few hours.",
          "score": 55,
          "created_utc": "2026-01-14 03:29:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgm1c1",
          "author": "cms2307",
          "text": "Wow it scores around the same on benchmarks as nano banana 2, if that‚Äôs true than this is a huge deal. Also the fact it‚Äôs editing and generation in one is awesome.",
          "score": 144,
          "created_utc": "2026-01-14 01:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgocrk",
              "author": "redditscraperbot2",
              "text": "If it‚Äôs too good to be true‚Ä¶",
              "score": 46,
              "created_utc": "2026-01-14 01:43:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzgpiqa",
                  "author": "simracerman",
                  "text": "Idk, z.ai did some miracles last year. Maybe this is their first for 2026.",
                  "score": 83,
                  "created_utc": "2026-01-14 01:50:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzgwazd",
                  "author": "-dysangel-",
                  "text": "Have you tried any GLM models since 4.5/4.5 Air? They are seriously impressive - both for their size, and in general",
                  "score": 43,
                  "created_utc": "2026-01-14 02:28:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzifiw9",
              "author": "lmpdev",
              "text": "Only on text rendering benchmark, and they are not comparing it to Nana Banana Pro. It's worse with text than flux.2 in my tests.",
              "score": 5,
              "created_utc": "2026-01-14 09:29:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhgi9e",
              "author": "RuthlessCriticismAll",
              "text": "> Wow it scores around the same on benchmarks as nano banana 2\n\nNo it doesn't. People think benchmarks are meaningless exclusively because they are completely unable to read them.",
              "score": 4,
              "created_utc": "2026-01-14 04:30:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgzzpe",
              "author": "HenkPoley",
              "text": "I guess, [similar to their GLM 4.x releases](https://eqbench.com/creative_writing_longform.html), they trained it on a mass of data from the best chatbots. Click the (i) in the 'Slop' column to see these top matches:\n\n* GLM-4.5 = DeepSeek-R1-0528\n* GLM-4.6 = DeepSeek-V3.1 / -V3.2-Exp\n* GLM-4.7 = gemini-3-pro-preview\n\nThey may have made some system to efficiently decide which is the best chat log to train on, how to reverse engineer training data sources, and the best prompts to get good chat logs.",
              "score": 7,
              "created_utc": "2026-01-14 02:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhwnce",
                  "author": "Keep-Darwin-Going",
                  "text": "That is basically distilling right? Nothing wrong with that except breaking tos.",
                  "score": 8,
                  "created_utc": "2026-01-14 06:33:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh9w93",
                  "author": "Aromatic-Low-4578",
                  "text": "What's your basis for this claim?  Find it hard to believe they could get a meaningful amount of tokens from gemini 3 pro in the last few months it's been available.",
                  "score": 21,
                  "created_utc": "2026-01-14 03:47:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzi2ro8",
              "author": "R_Duncan",
              "text": "It scores similar to Qwen-Image",
              "score": 0,
              "created_utc": "2026-01-14 07:27:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgmcbh",
          "author": "smith7018",
          "text": "Will absolutely reserve judgement but the sample images don‚Äôt scream SOTA to me. A lot of 1girl, scenery, and generic landscapes. The text looks great, though.",
          "score": 44,
          "created_utc": "2026-01-14 01:32:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgo04l",
              "author": "a_beautiful_rhind",
              "text": "Text a mostly solved problem since flux.",
              "score": 14,
              "created_utc": "2026-01-14 01:41:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh1nm1",
                  "author": "SanDiegoDude",
                  "text": "Not for dense text. Generating a diagram with accurate images and labels, or even a comic book panel with accurate dialogue dispersed the whole way through is very difficult, even for SOTA models like NB2. Their examples are quite impressive, and I'm excited to see how complex the typography can get before it starts to fall apart. In comparison, even having a single paragraph of text in Qwen and it falls apart pretty hard.",
                  "score": 29,
                  "created_utc": "2026-01-14 02:58:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh06yn",
                  "author": "inaem",
                  "text": "Only English, Chinese still sucks, so still a lot of work for these companies",
                  "score": 6,
                  "created_utc": "2026-01-14 02:50:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzgnf12",
          "author": "-p-e-w-",
          "text": "MIT license again, with no ifs and buts. Makes the Western labs look ridiculous when they publish inferior models under restrictive licenses.",
          "score": 162,
          "created_utc": "2026-01-14 01:38:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgozuu",
              "author": "eli_pizza",
              "text": "It‚Äôs great! But of course a permissive license only helps so much without the training data, tooling,  etc",
              "score": 17,
              "created_utc": "2026-01-14 01:47:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgwp3q",
              "author": "LocoMod",
              "text": "EDIT: Nevermind. You're not talking private cloud models. I misunderstood.\n\nAgreed.",
              "score": 0,
              "created_utc": "2026-01-14 02:30:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgmy3w",
          "author": "HistorianPotential48",
          "text": "is porn doable",
          "score": 103,
          "created_utc": "2026-01-14 01:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzh1e6c",
              "author": "twavisdegwet",
              "text": "For historians who find this comment later I need y'all to know this was asked roughly 15 minutes after the original post. I salute you.",
              "score": 126,
              "created_utc": "2026-01-14 02:57:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhgwza",
                  "author": "FuckNinjas",
                  "text": "Isn't what all of this is for? _gestures broadly_",
                  "score": 39,
                  "created_utc": "2026-01-14 04:33:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi90px",
                  "author": "erwgv3g34",
                  "text": "It's the only question that matters. If you _don't_ want to do porn, you are better off using ChatGPT or Claude over an open source model. They are cheaper, faster, and stronger.",
                  "score": 11,
                  "created_utc": "2026-01-14 08:25:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi5qu1",
                  "author": "BlobbyMcBlobber",
                  "text": "More like 15 seconds",
                  "score": 6,
                  "created_utc": "2026-01-14 07:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzgxdhz",
              "author": "gxvingates",
              "text": "Brother asking the questions that matter over here",
              "score": 51,
              "created_utc": "2026-01-14 02:34:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzgxi6t",
              "author": "leetsauwse",
              "text": "Bonk",
              "score": 33,
              "created_utc": "2026-01-14 02:35:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzh2zfe",
          "author": "Moronic_Princess",
          "text": "AND this is trained on domestic Huawei hardware",
          "score": 23,
          "created_utc": "2026-01-14 03:06:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkrfl5",
              "author": "henryclw",
              "text": "I think this is much more important, love to see people talking about it.",
              "score": 9,
              "created_utc": "2026-01-14 17:46:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzglr8q",
          "author": "crux153",
          "text": "\"Because the inference optimizations for this architecture are currently limited, the runtime cost is still relatively high. It requires either a single GPU with more than 80GB of memory, or a multi-GPU setup.\"",
          "score": 24,
          "created_utc": "2026-01-14 01:28:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgrmoy",
              "author": "dinerburgeryum",
              "text": "Yeah, that's day zero stuff tho. Comfy will bang the inference code into shape, and city will have GGUFs up by the end of the week. Two weeks tops. Just kick back and let the wizards do their magic.",
              "score": 18,
              "created_utc": "2026-01-14 02:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh28hc",
                  "author": "Hoodfu",
                  "text": "Last time a model said these kind of specs the comfy.org guys said it wasn't worth their time and it died on the vine. I hope that doesn't happen this time.",
                  "score": 9,
                  "created_utc": "2026-01-14 03:02:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzh0r5t",
                  "author": "More_Slide5739",
                  "text": "Just for that, Imma put this last. I got 96 models and now this ain't one!",
                  "score": -2,
                  "created_utc": "2026-01-14 02:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzgvkhs",
          "author": "Amazing_Athlete_2265",
          "text": "> Because the inference optimizations for this architecture are currently limited, the runtime cost is still relatively high. It requires either a single GPU with more than 80GB of memory, or a multi-GPU setup.\n\nGood thing I'm a patient man. Looking forward to be able to run this on lesser hardware.",
          "score": 8,
          "created_utc": "2026-01-14 02:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgkfdk",
          "author": "Caladan23",
          "text": "wen GGUF?",
          "score": 18,
          "created_utc": "2026-01-14 01:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgxbr3",
              "author": "MikeLPU",
              "text": "üíØ‚òùÔ∏èüòÇ",
              "score": -5,
              "created_utc": "2026-01-14 02:34:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgr77s",
          "author": "hainesk",
          "text": "What is the best way to run this with multiple gpus?",
          "score": 5,
          "created_utc": "2026-01-14 01:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh3mpc",
          "author": "Lopsided_Dot_4557",
          "text": "I just did an installation and testing video here:  [https://youtu.be/A6N8xu7xPRg?si=04v0lq64agKqr01b](https://youtu.be/A6N8xu7xPRg?si=04v0lq64agKqr01b)",
          "score": 5,
          "created_utc": "2026-01-14 03:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhwo69",
              "author": "o0genesis0o",
              "text": "I just watched and liked the video. Did you speed up or cut the video? That A6000 finish 50 steps surprisingly fast.\n\nThe model itself is not as good as I imagine.",
              "score": 2,
              "created_utc": "2026-01-14 06:33:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlpemn",
                  "author": "Lopsided_Dot_4557",
                  "text": "No I didn't edit it. Its actually fast. Thanks for liking it.",
                  "score": 1,
                  "created_utc": "2026-01-14 20:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzho8yq",
          "author": "Flat-Reference-2900",
          "text": "Comfyui version?",
          "score": 3,
          "created_utc": "2026-01-14 05:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhnqsl",
          "author": "jacek2023",
          "text": "Good size!",
          "score": 2,
          "created_utc": "2026-01-14 05:22:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzhsotx",
              "author": "Iory1998",
              "text": "Very good indeed. I wonder how it performs compared to Z-Image",
              "score": 1,
              "created_utc": "2026-01-14 06:00:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzjre78",
                  "author": "martinerous",
                  "text": "From the one example prompt that I tried, the result was visually not as realistic as Z-Image Turbo. GLM felt too artificial and a bit overcooked looks in comparison to Z-image's \"brutal\" realism.",
                  "score": 3,
                  "created_utc": "2026-01-14 15:00:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzk4vjl",
          "author": "HonZuna",
          "text": "That's all very interesting and engaging, but the key question is: what about tits?",
          "score": 2,
          "created_utc": "2026-01-14 16:04:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznabki",
          "author": "Daniel_H212",
          "text": "Definitely didn't see this coming. Deepseek-image next? üòÇ",
          "score": 2,
          "created_utc": "2026-01-15 00:58:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzushlh",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-16 03:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzme2l7",
          "author": "10minOfNamingMyAcc",
          "text": "RemindMe! 2 weeks",
          "score": 0,
          "created_utc": "2026-01-14 22:10:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzme98h",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 14 days on [**2026-01-28 22:10:02 UTC**](http://www.wolframalpha.com/input/?i=2026-01-28%2022:10:02%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/nzme2l7/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1qc9m6x%2Fglmimage_is_released%2Fnzme2l7%2F%5D%0A%0ARemindMe%21%202026-01-28%2022%3A10%3A02%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qc9m6x)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-14 22:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzndl6j",
          "author": "Acceptable-Tie278",
          "text": "Let‚Äôs goooo üî•",
          "score": 0,
          "created_utc": "2026-01-15 01:17:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qee2de",
      "title": "I fucking love this community",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/",
      "author": "alhinai_03",
      "created_utc": "2026-01-16 11:57:48",
      "score": 471,
      "num_comments": 54,
      "upvote_ratio": 0.96,
      "text": "Thank you guys, thanks to everyone who took the time to write a comment or a post explaining, teaching people how things work, the people behind llama.cpp, vllm, and all the contributors who keep the open-source community thriving.\n\nI'm able to run huge models on my weak ass pc from 10 years ago relatively fast, my fastest one being nemotron-3-nano-30B-a3b-iq4_nl running @14-13.5 t/s with 65k context. While my actual GPU having only 4GB of vram, that's fucking ridiculous and it blows my mind everytime that I'm able to run these models.\n\nWhat's been key for me is having a good amount of system memory, and as long as the model is a MoE architecture they run pretty decently.",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nzzyfg3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-16 21:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwowz6",
          "author": "Rokpiy",
          "text": "the system ram + moe combo is underrated. way more practical than people realize",
          "score": 39,
          "created_utc": "2026-01-16 12:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwpn9v",
              "author": "LosEagle",
              "text": "Somebody punch my 6 months younger self who wanted to wait with expanding system memory.",
              "score": 34,
              "created_utc": "2026-01-16 12:23:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzy1z0z",
                  "author": "here_n_dere",
                  "text": "Punch, *also punches self* (was sitting on 128Gb RAM in cart for 1/3 of the crazy price they are everywhere now)",
                  "score": 8,
                  "created_utc": "2026-01-16 16:29:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzyxdar",
                  "author": "Own-Potential-2308",
                  "text": "RAM is dead and Sam Altman killed it.",
                  "score": 4,
                  "created_utc": "2026-01-16 18:48:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzx39lc",
              "author": "Ok_Brain_2376",
              "text": "What‚Äôs moe? I got a decent setup so would like to know how I can run LLMs without bloating on some GPUs",
              "score": 7,
              "created_utc": "2026-01-16 13:45:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzxb3ei",
                  "author": "Hamza9575",
                  "text": "Stuff like kimi k2, glm 4 models, etc. Called mixture of experts ie MoE models, their unique thing is they need far more ram than any gpu has, but can run well even on cpu ram. For example a normal gaming computer motherboard with 4 ram slots, filled each with 64gb ram stick for 256gb ram total to run a quant of glm 4 series model at a good enough speed. For the cpu in these setups, amd 9700x or 9950x are popular, due to their high multicore performance as well as very good gaming performance.",
                  "score": 15,
                  "created_utc": "2026-01-16 14:25:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02nto2",
                  "author": "max123246",
                  "text": "If you want to understand the technical side. Mixture of Expert (MoE) models train basically multiple smaller models, and then the model during inference decides which of those smaller models to use for any particular input. So a MoE model doesn't have to multiply every weight it has against the input like it does for dense models.\n\nIt just so happens that the MoE model performance can rival dense models with cheaper and less memory intensive inference.",
                  "score": 3,
                  "created_utc": "2026-01-17 08:19:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwx4hf",
          "author": "qwen_next_gguf_when",
          "text": "Welcome to the world of \"I wish I had more VRAM and RAM so that I could run the SOTA model\"",
          "score": 15,
          "created_utc": "2026-01-16 13:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzz83k4",
              "author": "MoffKalast",
              "text": "There's always a bigger fish",
              "score": 3,
              "created_utc": "2026-01-16 19:36:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwtzu8",
          "author": "cosimoiaia",
          "text": "Kudos to you for taking the time to search posts, tune your settings and getting where you wanted!\n\nHaving hw constraints is actually the best way to learn and you get a lot more knowledgeable by experiencing it yourself. \n\nKeep experimenting with models and you'll also be future proofing yourself, you'll know what to buy, what's coming up, what works, etc... And it's a lot of fun!",
          "score": 12,
          "created_utc": "2026-01-16 12:51:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwod50",
          "author": "Narrow-Belt-5030",
          "text": "Could you link to the posts where they helped you re large models on crap equipment? You hide your posts (no idea why, but hey ho) so I can't check for myself and search.\n\n/u/[alhinai\\_03](https://www.reddit.com/user/alhinai_03/)",
          "score": 16,
          "created_utc": "2026-01-16 12:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxbe9t",
              "author": "alhinai_03",
              "text": "I wish I could give you a straightforward answer, but it's a lot of searching and reading, trying many configurations to find the sweet spot for my setup.\n\nAs I said, having enough system memory and using the right model are the most important factors. You must be able to offload all non-expert layers into the vram, which for moe models they're usually not very large. For the model mentioned, I can offload all 53 layers into the vram comfortably, leaving all the experts on system ram which are much bigger. If it helps below is how I call the model from my llama-swap yaml file.\n\n```  \nNemotron-3-Nano-30B-A3B-IQ4_NL:\n    cmd: >\n      C:\\llama.cpp\\build\\bin\\Release\\llama-server.exe\n      --model C:\\models\\Nemotron-3-Nano-30B-A3B-IQ4_NL.gguf\n      --n-gpu-layers -1\n      --ctx-size 65536\n      --flash-attn on\n      --batch-size 2048\n      --ubatch-size 1024\n      --threads 4\n      --cpu-moe\n      --jinja\n      --mlock\n      --temp 1.0\n      --top_p 1.0\n      --parallel 1\n      --host 0.0.0.0\n      --port ${PORT}\n    ttl: 3600\n```",
              "score": 19,
              "created_utc": "2026-01-16 14:27:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzxewgf",
                  "author": "Narrow-Belt-5030",
                  "text": "thanks :-)",
                  "score": 1,
                  "created_utc": "2026-01-16 14:44:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzxxma5",
                  "author": "CanadaHousingExpert",
                  "text": "How much RAM do you have?",
                  "score": 1,
                  "created_utc": "2026-01-16 16:10:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxxfs4",
          "author": "CanadaHousingExpert",
          "text": "Share a summary please! I have 4GB VRAM and 32GB RAM and am curious what my limit is.",
          "score": 6,
          "created_utc": "2026-01-16 16:09:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o08cfu9",
              "author": "yotsuya67",
              "text": "With that kind of hardware you should be able to run Nemotron 3 Nano 30b a3b in in a 4 bit quant at reasonable output speed. It's surprisingly quick for the total size. gpt-oss 20b is also a candidate, I just don't really like it myself. Aha.",
              "score": 1,
              "created_utc": "2026-01-18 03:44:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzz0uic",
              "author": "Mean-Sprinkles3157",
              "text": "Your hardware is a little bit limited, I don't think it is good for ai, but it is still better than my dell latitude 5510 with 0 vram. I use dgx spark to host llm, dell to do everything else.",
              "score": -2,
              "created_utc": "2026-01-16 19:03:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwoy84",
          "author": "danigoncalves",
          "text": "no way, how are you able to achieve that speed?",
          "score": 5,
          "created_utc": "2026-01-16 12:18:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy8x89",
              "author": "Just3nCas3",
              "text": "Simple explaination Its an MOE so its a 30B model, pretty large for like a gaming pc, but A3B means it only use 3B of those parameters at a time, so this is wrong but just think of it as having 10 * 3B models swapping places as needed so you get speeds between what a 3B model would give you and the 30B.",
              "score": 4,
              "created_utc": "2026-01-16 17:00:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzydy2v",
                  "author": "danigoncalves",
                  "text": "I know what is a MoE architecture but having that speed with 10 years old rig with only 4GB of GPU was a suprise to me.",
                  "score": 7,
                  "created_utc": "2026-01-16 17:22:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwpald",
          "author": "ljubobratovicrelja",
          "text": "Mind sharing your setup or posts where I can read more about your setup? I have something similar, and I would gladly do something like it.",
          "score": 3,
          "created_utc": "2026-01-16 12:20:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwqhm7",
          "author": "Potential-Leg-639",
          "text": "HW specs missing",
          "score": 4,
          "created_utc": "2026-01-16 12:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwqrmy",
          "author": "kubrador",
          "text": "the whole thing works because everyone collectively decided proprietary was cringe and just built better tools out of spite, which rules.",
          "score": 6,
          "created_utc": "2026-01-16 12:30:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx60ht",
          "author": "Dontdoitagain69",
          "text": "Share llama.cpp params please",
          "score": 3,
          "created_utc": "2026-01-16 13:59:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzy3d19",
          "author": "lolxdmainkaisemaanlu",
          "text": "how much RAM do you have bro?",
          "score": 3,
          "created_utc": "2026-01-16 16:35:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwrxi8",
          "author": "Mean-Sprinkles3157",
          "text": "Please share the parameters you setup on the model.",
          "score": 2,
          "created_utc": "2026-01-16 12:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxp8f9",
          "author": "No_Afternoon_4260",
          "text": "What amazes me is what nvidia achieves with 3b active params. (I know nvidia just did the \"fine\"-tune)",
          "score": 2,
          "created_utc": "2026-01-16 15:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyfacg",
          "author": "flashmyhead",
          "text": "How did you achieve that? I guess you just found the API key with some balance on it?",
          "score": 2,
          "created_utc": "2026-01-16 17:28:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01xkr3",
          "author": "Much-Researcher6135",
          "text": "Just wait for the crash my friend, you'll be swimming in cheap VRAM :)\n\nI don't say this tech is useless. Obviously it's useful. It's just overpriced. This happened 25 years ago when the internet really got built out. Get hyped!",
          "score": 2,
          "created_utc": "2026-01-17 04:40:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01ymw5",
              "author": "THound89",
              "text": "This is what I‚Äôm hoping for, just wait out the greed",
              "score": 2,
              "created_utc": "2026-01-17 04:48:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02c913",
          "author": "ghost_ops_",
          "text": "whats your setup? how much ram do u have?",
          "score": 2,
          "created_utc": "2026-01-17 06:34:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzycrgl",
          "author": "Astral65",
          "text": "Are there models that can run reliably on ancient laptop with 4GB ram, integrated Intel GPU? I tried installing ones but they generate text very slowly and consume all ram",
          "score": 1,
          "created_utc": "2026-01-16 17:17:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz2cne",
          "author": "indicava",
          "text": "F",
          "score": 1,
          "created_utc": "2026-01-16 19:10:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz2djs",
          "author": "indicava",
          "text": "F",
          "score": 1,
          "created_utc": "2026-01-16 19:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00qkrs",
          "author": "TheManicProgrammer",
          "text": "Do tell as I also only have 4gb Vram...",
          "score": 1,
          "created_utc": "2026-01-17 00:07:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06k04v",
          "author": "Daraxti",
          "text": "Hello,\nCan I hope to run a usefull model on a cpu w2123+64gb ram+super old gtx960 4gg ?",
          "score": 1,
          "created_utc": "2026-01-17 22:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o070bte",
              "author": "alhinai_03",
              "text": "Yes you can! in fact my specs are very similar to yours, try the same model I'm using, and you can find my parameters in a comment somewhere below.",
              "score": 2,
              "created_utc": "2026-01-17 23:25:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwmdew",
          "author": "InfiniteLand7364",
          "text": "Dude that's actually insane you're getting 14 t/s on a 10 year old rig, the optimization wizards in this community really are something else",
          "score": 74,
          "created_utc": "2026-01-16 11:59:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwsbyh",
              "author": "FullstackSensei",
              "text": "Fun fact, skylake was released 10 years ago with support for DDR4. Skylake-X will turn 10 in a couple of months, and that has 76GB/s bandwidth thanks to being quad channel.",
              "score": 15,
              "created_utc": "2026-01-16 12:41:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwwxk3",
                  "author": "Karyo_Ten",
                  "text": ">Skylake-X\n\nMy first real rig, (laptop or NAS only where I stuffed a 1070 for deep learning otherwise) with the CPU shutting down when pushing AVX-512 too far. I paired that with 2x 2080ti and I thought those 250W GPUs were quite power hungry ... if I knew.",
                  "score": 7,
                  "created_utc": "2026-01-16 13:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzwpro0",
              "author": "mxforest",
              "text": "Credits to Nvidia for Nemotron too. The thing flies and is actually really smart.",
              "score": 17,
              "created_utc": "2026-01-16 12:24:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzwzy1y",
              "author": "detrebear",
              "text": "Step 1: Drop Python \\\nStep 2: ??? \\\nStep 3: Profit\n\n>!I'm joking ofc, these madlads are doing God's work!<",
              "score": 6,
              "created_utc": "2026-01-16 13:27:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qd6nho",
      "title": "Zhipu AI breaks US chip reliance with first major model trained on Huawei stack (GLM-Image)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.scmp.com/tech/tech-war/article/3339869/zhipu-ai-breaks-us-chip-reliance-first-major-model-trained-huawei-stack",
      "author": "fallingdowndizzyvr",
      "created_utc": "2026-01-15 02:01:03",
      "score": 409,
      "num_comments": 46,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qd6nho/zhipu_ai_breaks_us_chip_reliance_with_first_major/",
      "domain": "scmp.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzp79d2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 09:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznqn0p",
          "author": "RhubarbSimilar1683",
          "text": "So the Chinese ban on Nvidia is working. It's just a matter of time before it's scaled up to larger models",
          "score": 183,
          "created_utc": "2026-01-15 02:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznsk93",
              "author": "foldl-li",
              "text": "Or rather, US' ban on NV is working.",
              "score": 65,
              "created_utc": "2026-01-15 02:44:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzovaiw",
                  "author": "Nobby_Binks",
                  "text": "IIRC, China also turned around and banned Nvidia to force local development.",
                  "score": 42,
                  "created_utc": "2026-01-15 07:21:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzoxz7l",
                  "author": "Maleficent-Scene7771",
                  "text": "God bless America\n\nSun Tzu bless China.",
                  "score": 27,
                  "created_utc": "2026-01-15 07:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzohaxm",
              "author": "zuraken",
              "text": "It was America that banned selling them to China which spurred more development. I mean development was there before the ban, but that ban really speedran their process lmfao",
              "score": 24,
              "created_utc": "2026-01-15 05:26:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzopfb8",
                  "author": "pissoutmybutt",
                  "text": "I dont understand how anyone could think China is incapable of working around shit like this. They are a  superpower built on engineering and manufacturing with a command economy able to provide as much funding as necessary towards addressing the issue",
                  "score": 45,
                  "created_utc": "2026-01-15 06:30:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzpnvld",
                  "author": "enilea",
                  "text": "The US ban on Nvidia chips was limited to the high end chips, the intention was limiting them to smaller commercial GPUs and older models. But in turn China banned its own companies from using most of the models that were allowed. The H200 model was recently approved by the US to be exported to China, but China rejected it because they want to breed their own GPU market instead of relying on the US, which isn't a reliable trade partner.",
                  "score": 7,
                  "created_utc": "2026-01-15 11:46:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqoi63",
              "author": "andy_potato",
              "text": "The problem is the ban isn‚Äôt working. Gamer Nexus made a great documentary about how GPUs are still getting into China with Nvidia looking the other way.",
              "score": 0,
              "created_utc": "2026-01-15 15:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrz3da",
                  "author": "RhubarbSimilar1683",
                  "text": "It doesn't have to be perfect. Just inconvenient¬†",
                  "score": 4,
                  "created_utc": "2026-01-15 18:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznxchc",
          "author": "andy_potato",
          "text": "Lots of people here gave it a try and the outputs are really not good. I understand that this is more of a tech demo or a MVP showing off alternative model architectures. But maybe wasn't a good idea to make this a major release and getting people all hyped up about the model capabilities.",
          "score": 55,
          "created_utc": "2026-01-15 03:12:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzoi99x",
              "author": "ForsookComparison",
              "text": "Somewhere in the world there is a bunch of cracked traders who monitored how gooner forums reacted to a new image model trained on Huawei GPUs and the result of that decided that the US Stock Market gets to chug along happily for at least a few more months.",
              "score": 39,
              "created_utc": "2026-01-15 05:33:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzoit58",
                  "author": "andy_potato",
                  "text": "The 1girl army of r/StableDiffusion sure did their part",
                  "score": 21,
                  "created_utc": "2026-01-15 05:37:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzpkg4y",
                  "author": "SkyFeistyLlama8",
                  "text": "Someone needs to generate an image of a Wall Street trader doing a line or ten of coke while looking at GPU prices on their ten-monitor wall.",
                  "score": 3,
                  "created_utc": "2026-01-15 11:17:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznvl3b",
          "author": "AfterAte",
          "text": "SD1.5 (38 months ago) was 0.8B, and SDXL (29 months ago) was a 2.6B model. Flux.1 (17 months ago) was a 12B model. All trained on Nvidia.\n\nThey are less than 2 years away using only a Huawei hardware/software stack. No CUDA. And Flux.1 didn't have image edititing. Z.ai proves non-CUDA training and inference is viable.\n\n\nThis is an important development. The rate of development will be faster than linear. China scales faster than anyone, has the necessary energy production and scientists. All important ingredients. This is bigger than it seems.",
          "score": 69,
          "created_utc": "2026-01-15 03:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp2s9g",
              "author": "i_not_give_shit",
              "text": "\"proves non-CUDA training and inference is viable.\"\n\nWhat do you mean? I have used vulkan llama-cpp for inference for a long time, havent seen a difference with cuda.",
              "score": 10,
              "created_utc": "2026-01-15 08:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp6uao",
                  "author": "AfterAte",
                  "text": "I don't mean to put emphasis on inference alone, I mean \"not only inference, but training as well\", but I don't want to sound like an AI. Training was the main point of the article. Everyone here knows Vulkan is good at inference now.",
                  "score": 9,
                  "created_utc": "2026-01-15 09:11:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzp3nu2",
                  "author": "Hunting-Succcubus",
                  "text": "Opencl too. Cuda is good but not only option.",
                  "score": 9,
                  "created_utc": "2026-01-15 08:40:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzock9k",
              "author": "LocoMod",
              "text": "No its really not. You can train a model on a CPU if you want. But that's not relevant to the conversation is it. What matters is speed to market and quality of results (nvidia hardware is nothing without CUDA). Its more than chips. \n\nThe GLM image model is not as capable as other recent smaller models. And we have no objective metrics as to how much time it took to train, etc.\n\nYou're embellishing something you dont understand. They made progress. But its not as big a deal as you are implying.",
              "score": 19,
              "created_utc": "2026-01-15 04:51:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzowk2j",
                  "author": "redditscraperbot2",
                  "text": "In its defense, it's an autoregressive model and they have historically been shit regardless of what they are trained on.",
                  "score": 19,
                  "created_utc": "2026-01-15 07:33:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzp5idx",
                  "author": "AfterAte",
                  "text": "I didn't say GLM was better than the recent Flux2, Z-Image-Turbo or Qwen-2507. But it is better than SD1.5 and early versions of SDXL (with no Lora), and more capable than Flux.1 That's why I said they are 2 years away.\n\n\nWhat it is important is China has a hardware/software stack the American government can't slow down because it's all domestic, and Z.ai proved it works.¬†¬†\n\n\nSo geopolitically speaking, this is big.",
                  "score": 18,
                  "created_utc": "2026-01-15 08:57:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nznnsto",
          "author": "RetiredApostle",
          "text": "The \"major\" model here is GLM-Image 9B.",
          "score": 25,
          "created_utc": "2026-01-15 02:16:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznzv7o",
              "author": "cutebluedragongirl",
              "text": "LMAO",
              "score": -5,
              "created_utc": "2026-01-15 03:28:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo2y7r",
          "author": "Different_Fix_2217",
          "text": "The model is terrible in every way so not the best showcase imo.",
          "score": 12,
          "created_utc": "2026-01-15 03:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzntlop",
          "author": "Recoil42",
          "text": "Makes perfect sense why GLM-Image is so mid now ‚Äî¬†this is the MVP. \n\nDoes know how much output SMIC is projected to be ramping for these?",
          "score": 2,
          "created_utc": "2026-01-15 02:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzo2gyj",
              "author": "Clear_University5148",
              "text": "No, it made sense before. GLM-Image is a research project into an experimental architecture, not a product.",
              "score": 32,
              "created_utc": "2026-01-15 03:44:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzq3vhl",
          "author": "kc858",
          "text": "what the hell is this headline\n\nwhat the hell happened to this sub",
          "score": 1,
          "created_utc": "2026-01-15 13:32:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzovlf8",
          "author": "Umademedothis2u",
          "text": "Overstating the capabilities, under-delivering the actual outcomes... YUP that is about right for a Chinese model",
          "score": -14,
          "created_utc": "2026-01-15 07:24:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzp3wrs",
              "author": "Hunting-Succcubus",
              "text": "Your statement is proven by deepseek,qwen, wan video, zimage. Typical under delivering Chinese products. But i still love them.",
              "score": 9,
              "created_utc": "2026-01-15 08:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qe0cxc",
      "title": "Latest upgrade‚Ä¶A100 40 GB",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/f66wnmearldg1.jpeg",
      "author": "inserterikhere",
      "created_utc": "2026-01-16 00:03:21",
      "score": 391,
      "num_comments": 49,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzuu20d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-16 03:30:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztwsiy",
          "author": "jack-in-the-sack",
          "text": "https://preview.redd.it/up4xk1e4vldg1.jpeg?width=320&format=pjpg&auto=webp&s=c47fa431928178b07b99d7b23b456f45d67ac364",
          "score": 202,
          "created_utc": "2026-01-16 00:24:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu1fv3",
              "author": "silenceimpaired",
              "text": "I just like to tell myself OP is a small business owner trying to inspire people to take a $1000 risk on his dead $10000 card.",
              "score": 74,
              "created_utc": "2026-01-16 00:49:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzucb21",
              "author": "m31317015",
              "text": "Damn every time I see the comments of post I'm jealous of somebody will always be faster to post this, so I'm doing it to you as well.\n\nhttps://preview.redd.it/4xi4s9ofamdg1.png?width=320&format=png&auto=webp&s=1d476030224a0acb876fae80f1220fbf4373db21",
              "score": 34,
              "created_utc": "2026-01-16 01:50:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvqc4j",
                  "author": "jack-in-the-sack",
                  "text": "You can always have 2nd ü§∑üèª‚Äç‚ôÇ",
                  "score": 5,
                  "created_utc": "2026-01-16 07:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzw35qj",
              "author": "TheRenaissanceMaker",
              "text": "You shouldn't be jealous! Chatbot addiction leed to loss of iq",
              "score": 2,
              "created_utc": "2026-01-16 09:14:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzw4uqr",
                  "author": "jack-in-the-sack",
                  "text": "I know ... ?! But what has that to do with a GPU?",
                  "score": 2,
                  "created_utc": "2026-01-16 09:30:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nztwnvg",
          "author": "FoxTimes4",
          "text": "Where‚Äôs the meme with the happy for you kid replaced by Jensen‚Ä¶",
          "score": 37,
          "created_utc": "2026-01-16 00:24:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvqz7i",
              "author": "jack-in-the-sack",
              "text": "https://preview.redd.it/xmfeq95vxndg1.jpeg?width=2048&format=pjpg&auto=webp&s=34647bbc89bb383ae6b605b08115b196df76a3eb\n\nFound it, haha, I didn't even know this existed üòÇüòÇüòÇ",
              "score": 52,
              "created_utc": "2026-01-16 07:24:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzu1z4r",
          "author": "matatonic",
          "text": "How are you cooling that? it looks like a passive cooled version and you should have a blower fan or some other active fan forcing air through it ... or you might burn it. Another option is water cooling, I think you can still get some on AliExpress for the a100s.",
          "score": 23,
          "created_utc": "2026-01-16 00:52:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu45ae",
              "author": "inserterikhere",
              "text": "It‚Äôs cutoff in the picture but I 3d printed a shroud/bracket that allows me to put two 40MM fans and I haven‚Äôt had any issues so far\n\nhttps://preview.redd.it/zmcio0dz1mdg1.jpeg?width=3000&format=pjpg&auto=webp&s=478d75d6dcf7d461f639ae9f48f9c9eae49f22ca",
              "score": 22,
              "created_utc": "2026-01-16 01:04:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzurwkd",
                  "author": "matatonic",
                  "text": "Glad to hear. Nice work and great find!",
                  "score": 3,
                  "created_utc": "2026-01-16 03:17:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o00zhgt",
                  "author": "ds-unraid",
                  "text": "What are temps normally vs under load? Also, how do you control the speed speeds of the fans? Are they just constant speed or do you have them hooked up to the card somehow?",
                  "score": 1,
                  "created_utc": "2026-01-17 00:59:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu7e6b",
          "author": "AustinM731",
          "text": "Dude, I almost bought that card!",
          "score": 22,
          "created_utc": "2026-01-16 01:23:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuhm6p",
          "author": "Atom_101",
          "text": "> card reports cuda error \n\nDude sold his gpu instead of rebooting his pc?",
          "score": 18,
          "created_utc": "2026-01-16 02:20:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwkpvq",
              "author": "Polymorphic-X",
              "text": "Tech illiteracy can be quite expensive it turns out.",
              "score": 7,
              "created_utc": "2026-01-16 11:47:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztuesh",
          "author": "arman-d0e",
          "text": "That is ridiculous",
          "score": 24,
          "created_utc": "2026-01-16 00:11:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzu5ij1",
          "author": "theonetruefreezus",
          "text": "My jealousy is so real right now. But good for you bro. What a freaking come up. I'm not so into Russian roulette as you, but more power to you my guy.",
          "score": 5,
          "created_utc": "2026-01-16 01:12:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztuurh",
          "author": "Available-Craft-5795",
          "text": "Is nobody concerned about the hiked prices? This is insaine LOL  \nHope you do great things with it",
          "score": 11,
          "created_utc": "2026-01-16 00:14:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztwxz6",
              "author": "jack-in-the-sack",
              "text": "Comcerned? Yes. \nCan I do anything about it? No.",
              "score": 23,
              "created_utc": "2026-01-16 00:25:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzun3pa",
              "author": "CrypticZombies",
              "text": "Get nowhere in life with that trash ass mindset",
              "score": -12,
              "created_utc": "2026-01-16 02:51:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzu8dhi",
          "author": "alphatrad",
          "text": "I bought a computer like this once where the memory just needed to be reseated and the guy got rid of thinking it was broke for nothing.\n\nGreat score dude! Happy training!",
          "score": 2,
          "created_utc": "2026-01-16 01:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzu3qz1",
          "author": "PsychologicalWeird",
          "text": "what are doing to keep it cool, I cant quite see if you have a couple of fans at the front of it or not?\n\nIm looking at a data centre GPU myself as most people look at them and think WTF am I going to do with a headless GPU or they go down the route of non blower GPUs, which is good for me....   \n  \nCurrent rig rocks a A2000 12GB, 4000 ada, and A5500 (all housed in a FD Define 7 XL) and want something to replace the A2000, as I cant add to it otherwise I lose the space taken up by my NVME array and scratch drive.",
          "score": 2,
          "created_utc": "2026-01-16 01:02:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu5h1w",
              "author": "inserterikhere",
              "text": "I posted a picture in another reply but it‚Äôs cut off in the picture, I 3d printed a fan bracket/shroud that lets me put two 40MM fans directly on it. Highest I‚Äôve seen it cap out is 84C. Idles at about 30-40C. \n\nThat 4000 Ada is reallll nice, I can‚Äôt lie I almost pulled the trigger on one of those. \n\nHow‚Äôs the XL? I‚Äôve only built my PCs in fractal cases bc i love their designs. If I ever plan on adding another GPU, I‚Äôm gonna end up taking a look a few XL cases",
              "score": 3,
              "created_utc": "2026-01-16 01:12:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzu7ta1",
                  "author": "PsychologicalWeird",
                  "text": "Hows the noise on one of those running 2x 40mm fans.\n\nThe 4000 ada was going sweet at ¬£700, before everything jumped to ¬£1200+, so naturally in disgust I then got a A5500 as it was ¬£1k and now cant find anymore under ¬£1500-1600 now... so now on to data centre toys.\n\nThe XL is a dream to play in, so much room and its got the space for 7x PCIe lanes, loads of places to put SSDs/HDDs, the Threadripper Pro its attached to is absolute bastid to work with.   \n  \nSo many reboots/training/simple issues that have you jumping through hoops that a consumer PC wouldnt even care about... take Ubuntu... got it working on the 4000 ada, decided I wanted the A2000 to be the UI GPU... did it take a simple switch of the GPU... did it fuck, its still fighting me 4 hours later recognising the A2000 exists, but then loading the last known good drivers for the 4000 ada and that is incompatible with the A2000.\n\nAwesome fun... \n\nIf I were to switch it out again... I would consider a Jonsbo N5 as that can do all the GPU Space and eleventy million drives too and is a smaller foot print.",
                  "score": 3,
                  "created_utc": "2026-01-16 01:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nztx5ct",
          "author": "aero-spike",
          "text": "Omg that is so cool!",
          "score": 1,
          "created_utc": "2026-01-16 00:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztzmh8",
          "author": "Virtual_Actuary8217",
          "text": "What is the psu?",
          "score": 1,
          "created_utc": "2026-01-16 00:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu0ziy",
              "author": "inserterikhere",
              "text": "Evga supernova 1000GT, I set power limits for both. Currently I set the 3090 limit to 280W and the A100 at 250W.",
              "score": 3,
              "created_utc": "2026-01-16 00:47:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzuy8rl",
              "author": "Virtual_Actuary8217",
              "text": "I have the same CPU and I can't even think of adding a 5060,same psu, how do you limit 3090 to 250w?",
              "score": 1,
              "created_utc": "2026-01-16 03:55:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00fawk",
                  "author": "inserterikhere",
                  "text": "Well on Linux it‚Äôs as easy as just typing this command into terminal ‚Äúsudo nvidia-smi -i GPUID# -pl 250‚Äù",
                  "score": 1,
                  "created_utc": "2026-01-16 23:03:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuatjk",
          "author": "Palmquistador",
          "text": "It‚Äôs so pretty ü§©",
          "score": 1,
          "created_utc": "2026-01-16 01:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv0r26",
          "author": "coconutboy1234",
          "text": "Absolute beast I hope I could afford it someday",
          "score": 1,
          "created_utc": "2026-01-16 04:10:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvzwt8",
          "author": "Ok_Remove3449",
          "text": "Ayo.. Is this passively cooled? How are you cooling it?",
          "score": 1,
          "created_utc": "2026-01-16 08:44:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00cghe",
          "author": "Demoleid",
          "text": "Congratulations on your purchase! What motherboard do you have? I ask because I want to upgrade my computer components to install two video cards where I can take full advantage of the x16 graphics lanes.",
          "score": 1,
          "created_utc": "2026-01-16 22:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00e2fm",
              "author": "inserterikhere",
              "text": "x670e Asus Pro art, it gives me the option of running 1 GPU using the full 5.0 x16 lanes on the first slot, or if I put 2 GPUs in the first & second slot, it‚Äôll switch to pcie 5.0 x8 on both slots. The third slot is PCIE 4 x2 but it shares those lanes with 1 of the m.2 NVME slots so you can‚Äôt use both at the same time.",
              "score": 1,
              "created_utc": "2026-01-16 22:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o00d4ob",
          "author": "lukewhale",
          "text": "Uhh I hope you got enough air flow for that server card",
          "score": 1,
          "created_utc": "2026-01-16 22:52:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00fpcj",
              "author": "inserterikhere",
              "text": "Yessirrr I got two 40mm fans on the intake of the card (3d printed bracket) + 140mm case fan right in front of it",
              "score": 2,
              "created_utc": "2026-01-16 23:06:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzutrfn",
          "author": "Merlin_Magick",
          "text": "Hey I have somewhat of a bricked card‚Ä¶ how do you use nvidia smi to unbrick it?",
          "score": 0,
          "created_utc": "2026-01-16 03:28:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxcbgz",
              "author": "a_beautiful_rhind",
              "text": "He didn't unbrick it, previous owner had some other kind of problem and sold instead of figuring it out.",
              "score": 2,
              "created_utc": "2026-01-16 14:32:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv3oq2",
          "author": "Terrible-Detail-1364",
          "text": "congrats, thats a consumer motherboard, the pci slot furthest from the cpu runs at a slower speed, please swap the cards.",
          "score": 0,
          "created_utc": "2026-01-16 04:29:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvt17w",
              "author": "inserterikhere",
              "text": "Normally yes, but both slots on this mobo (x670e pro art) can run at PCIE 5.0 x8. Also both cards are 4.0 x16 which is about the same speed as PCIE 5.0 x8. The third slot is capped out at 4.0 x2.",
              "score": 2,
              "created_utc": "2026-01-16 07:42:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzwln57",
                  "author": "Automatic_Two4291",
                  "text": "But shouln't they then run at 4.0 x8? Cause slots limits lanes to x8 and the gpu down to 4.0?",
                  "score": 4,
                  "created_utc": "2026-01-16 11:54:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qbpz5l",
      "title": "kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qbpz5l",
      "author": "Nunki08",
      "created_utc": "2026-01-13 12:25:26",
      "score": 387,
      "num_comments": 85,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzcyj9u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzclql0",
          "author": "R_Duncan",
          "text": "Any chance we can finetune it on different language?",
          "score": 28,
          "created_utc": "2026-01-13 13:37:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcsrae",
              "author": "SignificantAsk4215",
              "text": "According to the papers, they needed 2 days for English and 32x H100s, which is out of scope for normal people. I hope we can donate to Kyutai, and they would do it for the community.\n\n/typo",
              "score": 32,
              "created_utc": "2026-01-13 14:15:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzd6ua4",
                  "author": "bigh-aus",
                  "text": "We really need to work out a framework for distributed (donated) training.",
                  "score": 23,
                  "created_utc": "2026-01-13 15:26:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzdain1",
                  "author": "Sea_Revolution_5907",
                  "text": "For a single language finetune you'd need much less compute than the original model.",
                  "score": 8,
                  "created_utc": "2026-01-13 15:43:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nze0ge2",
                  "author": "R_Duncan",
                  "text": "Orpheus was finetuned to es and it with much less compute than that (and from then on, there's a colab notebook from unsloth), issue is orpheus fails often to produce a decent result.",
                  "score": 1,
                  "created_utc": "2026-01-13 17:54:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlja5k",
              "author": "Regular_Instruction",
              "text": "French company not making French language....",
              "score": 1,
              "created_utc": "2026-01-14 19:50:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzckqzs",
          "author": "marcoc2",
          "text": "Languages?",
          "score": 25,
          "created_utc": "2026-01-13 13:31:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzclwzl",
              "author": "rerri",
              "text": "\"English only\", says the model card.",
              "score": 40,
              "created_utc": "2026-01-13 13:38:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze5zxr",
                  "author": "Mkengine",
                  "text": "As always.",
                  "score": 11,
                  "created_utc": "2026-01-13 18:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcvh2i",
          "author": "oxygen_addiction",
          "text": "*Small warning:*\n\nTheir localhost test server setup doesn‚Äôt seem to clear memory between generations, so memory usage balloons the more you generate. It hit **32 GB** on my system before I realized. It should probably clear memory when starting a new generation\n\n\\-------\n\nReally cool that it's so easy to install and play around with:\n\n    uvx pocket-tts serve\n\nI had it read this [news article](https://freedomnews.org.uk/2025/04/11/how-the-uk-is-shaping-a-future-of-precrime-and-dissent-management/) and here‚Äôs what I‚Äôve observed so far:\n\n1. The model load uses about **1.1 GB of RAM** (not VRAM)\n2. It‚Äôs really fast on my Ryzen **5950X**. After the model+voice file load, time to first audio is around **200 ms**, as stated on their website. It could easily be kept loaded in the background and used on the CPU for various tasks (LLM responses, reading summaries or notifications, etc.\n3. As context fills up, RAM usage grows (logically). The article from above used about **8.5 GB of RAM**\n4. Intonation is very good for a model of this size, but obviously still really far from perfect.\n5. Voice quality is so-so\n\nI don‚Äôt have time to test voice cloning right now.",
          "score": 23,
          "created_utc": "2026-01-13 14:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtyki",
              "author": "StrengthSingle8891",
              "text": "The memory leak of our web server has been fixed, version 1.0.1 pushed to PyPI :)",
              "score": 17,
              "created_utc": "2026-01-13 17:24:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg0x39",
                  "author": "FederalLook5060",
                  "text": "commen when vibe coding",
                  "score": -12,
                  "created_utc": "2026-01-13 23:34:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcuhgg",
          "author": "teachersecret",
          "text": "This is good. Very solid. Nice voices, nice voice cloning.\n\nFiddling with running it on GPU I'm seeing 7.43x real time, 17ms time to first chunk with the memory leak fixed, peak gpu memory 578MB.\n\nIt's a beast. :)\n\nCPU is fast too. I'm seeing 2.18x real-time with the memory leak bug fixed, 120ms to first chunk, which isn't bad at all.\n\nVery fast, very performant. Not as controllable in terms of emotion, but not bad at all. A true speech to speech model is going to ‚Äúfeel‚Äù better, but for something that runs on a potato faster than realtime? I think this is a new high water mark. Highly recommended.\n\nEDIT: There is a memory leak in their code you have to fix (Claude can do it, I might toss some code up tomorrow or something).",
          "score": 12,
          "created_utc": "2026-01-13 14:24:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcuv57",
              "author": "fundthmcalculus",
              "text": "200ms is 2/10 of a second, so the CPU vs GPU latency is a factor of two, but only 100ms in absolute difference.",
              "score": 3,
              "created_utc": "2026-01-13 14:26:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcy84r",
                  "author": "teachersecret",
                  "text": "Just tested a bit more extensively so it wasn't just off-the-cuff.\n\nOn GPU (my 4090), **EDIT: SEE BELOW FOR UPDATED BENCH** ~~RTF 7.84x (haven't bothered trying to improve this, can probably batch to much higher numbers, expect 50x-100x once dialed in).~~\n\n~~34ms time to first chunk~~\n\n~~On CPU, 3.05x real-time, with first chunk at 132ms.~~\n\n~~Peak memory 2183mb.~~\n\nNot a bad setup at all.\n\nEDIT: Fixed the memory leak. 7.43x real time, 17ms time to first chunk with the memory leak fixed, peak gpu memory 578MB\n\nCPU hits 2.18x real-time with the memory leak bug fixed, 120ms to first chunk.",
                  "score": 6,
                  "created_utc": "2026-01-13 14:43:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzcvtio",
                  "author": "oxygen_addiction",
                  "text": "Where is the context stored when running this on the GPU? Memory usage gets very high on the CPU, so if running it on the GPU just shifts that usage to VRAM, then this model is a big no-no for most people.\n\nSee my post below.",
                  "score": 2,
                  "created_utc": "2026-01-13 14:31:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdfv28",
          "author": "Bananadite",
          "text": "I'm curious why there are so many TTS but a lot less STT model releases. Are STT harder to train?  Or is it a \"solved\" problem with Whisper that's hard to improve/beat?  \n\nI feel like I've seen so many TTS models release but so little STT",
          "score": 6,
          "created_utc": "2026-01-13 16:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzexcm8",
              "author": "R_Duncan",
              "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving\\_30x\\_realtime\\_transcription\\_on\\_cpu/](https://www.reddit.com/r/LocalLLaMA/comments/1q4vz16/achieving_30x_realtime_transcription_on_cpu/)\n\n  \ncheck is better than whisper large at [https://huggingface.co/spaces/hf-audio/open\\_asr\\_leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) ,\n\ncanary required for top WER.",
              "score": 4,
              "created_utc": "2026-01-13 20:22:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzfhlid",
                  "author": "-InformalBanana-",
                  "text": "Do you maybe know what is a llama.cpp equivalent for running nvidia parket v3 or phi 4 multimodal?",
                  "score": 1,
                  "created_utc": "2026-01-13 21:56:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcefiu",
          "author": "ResidentPositive4122",
          "text": "It seems that under a certain size these models are not good enough to be worth the trouble. If you *need* something that small, perhaps the \"hardcoded\" one that everyone uses on twitch is the way to go. Tried this on their demo page (with c/p from the blog post) and it was not good.",
          "score": 21,
          "created_utc": "2026-01-13 12:53:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcoke5",
              "author": "Much-Researcher6135",
              "text": "Small models are hit and miss, but I have been in love with the [Kokoro 82M model](https://huggingface.co/hexgrad/Kokoro-82M) for quite awhile. Some of its voices are very good. I've been [serving it locally](https://github.com/remsky/Kokoro-FastAPI) for all of my TTS applications.",
              "score": 19,
              "created_utc": "2026-01-13 13:53:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdt5to",
                  "author": "KindLizard37",
                  "text": "Give Pocket TTS a try, we're quite confident that we beat Kokoro (and we'll have numbers to back it up soon). Voice cloning is our main advantage, since we can do infinitely many voices and emotions whereas Kokoro has a fixed repertoire of voices that they can only widen with more training.  \n  \nIf you disagree, let us know on which dimensions you prefer Kokoro and we'll see what we can do :)  \n\\- V√°clav from Kyutai",
                  "score": 21,
                  "created_utc": "2026-01-13 17:20:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzewbvr",
                  "author": "qrayons",
                  "text": "Kokoro is great because it actually supports multiple languages.",
                  "score": 3,
                  "created_utc": "2026-01-13 20:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzduoai",
              "author": "StrengthSingle8891",
              "text": "Gabriel from Kyutai here, we've had trouble with a memory leak on our server, causing the audio to be choppy in the demo. The fix has been pushed, I encourage you to try again!",
              "score": 8,
              "created_utc": "2026-01-13 17:27:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzckobf",
              "author": "Aaaaaaaaaeeeee",
              "text": "What do you mean? Doesn't twitch¬† have a fairly robotic voice? This one seems at least kokoro quality for me.¬†\n\n\nTry downloading the voice snippet, I have a bad connection so I get some glitches.¬†",
              "score": 6,
              "created_utc": "2026-01-13 13:31:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzcxds9",
              "author": "bick_nyers",
              "text": "Although it may be impractical for realtime I would like to see someone drop something in the 8-32B range that is really high quality.",
              "score": 1,
              "created_utc": "2026-01-13 14:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzeog4f",
              "author": "lorddumpy",
              "text": "Did you try the demo? It is surprisingly solid.",
              "score": 1,
              "created_utc": "2026-01-13 19:41:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzeqpeb",
                  "author": "ResidentPositive4122",
                  "text": "I did. For every demo I see I c/p the copy on the site. This model misses/mangles words randomly from their own blogpost. And, annoyingly it mangles different words on several tries. It's cool that it can be done in such low params, but it's not good quality tts.",
                  "score": 2,
                  "created_utc": "2026-01-13 19:51:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcj1d0",
          "author": "SignificantAsk4215",
          "text": "Please support german",
          "score": 15,
          "created_utc": "2026-01-13 13:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdjn1w",
          "author": "vko-",
          "text": "Kyutai are amazing. Moshi is an amazing paper, Unmute is a project to match their audio-to-audio models with an LLM of your choice. I expect anything coming from the team to be gold at this point.",
          "score": 4,
          "created_utc": "2026-01-13 16:25:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcuqom",
          "author": "bambamlol",
          "text": "Not bad, but I've tried 5 generations so far, and all of them were \"corrupted\" and had significant issues, mostly \"stuttering\" and lags, sometimes there were also two voices talking over each other.\n\nDon't know if the demo caused these issues, or the model itself.",
          "score": 6,
          "created_utc": "2026-01-13 14:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdtior",
              "author": "StrengthSingle8891",
              "text": "Gabriel from Kyutai here, our web server had a memory leak, I just pushed a fix, all our containers went from using 10GB to using 1.5GB :) That should help a lot with the stuttering. I encourage people to run the TTS locally, as your CPU will likely be much faster than the cheap cpus we use in our demo.",
              "score": 17,
              "created_utc": "2026-01-13 17:22:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzivmhf",
                  "author": "bambamlol",
                  "text": "Awesome, thanks! Will try again ASAP!",
                  "score": 3,
                  "created_utc": "2026-01-14 11:51:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzczpcp",
              "author": "Aaaaaaaaaeeeee",
              "text": "that's an issue when streaming from the demo. Those are probably related to poor connection and browser.¬†",
              "score": 4,
              "created_utc": "2026-01-13 14:51:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdxvwx",
              "author": "VoidAlchemy",
              "text": "I just tried copy pasting a few texts into it after they pushed a fix and it seems pretty good at first glance. Sounds as natural or more than kokoro in the three samples it tried.",
              "score": 1,
              "created_utc": "2026-01-13 17:42:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzd4hda",
              "author": "Sea_Revolution_5907",
              "text": "I think that's the demo app bugging a bit. My internet is quite slow so could be it.",
              "score": 0,
              "created_utc": "2026-01-13 15:15:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzcjaob",
          "author": "nomorebuttsplz",
          "text": "Chatterbox is just barely too slow to be good on a 3090 for real time chat so hopefully this will be almost as good and faster. Anyone try cloning a voice yet?",
          "score": 6,
          "created_utc": "2026-01-13 13:23:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcxvga",
              "author": "jasongill",
              "text": "I tried one-shot cloning of the voice of KITT from Knight Rider using a 13 second sample and it works perfectly - this is by far the best CPU-only voice cloned TTS that I've tried",
              "score": 11,
              "created_utc": "2026-01-13 14:41:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzckvmn",
          "author": "DragonfruitIll660",
          "text": "Very neat, quality seems pretty good too from the demos.",
          "score": 4,
          "created_utc": "2026-01-13 13:32:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfp3yt",
          "author": "sstainsby",
          "text": "Try the demo with \"Add 0.8g of sodium sulphide to 100ml of warm water.\" Not really ready for anything technical, or even just recipes.",
          "score": 2,
          "created_utc": "2026-01-13 22:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg5a0d",
          "author": "mpasila",
          "text": "Tested the voice cloning and it's okay though not bad for the model size though. It seems to heavily depend on the quality of your voice sample and if it likes the voice or not. Kokoro I think is clearer than this model, like with this all the voices are kinda muffled for some reason and has more TTS artifacts.",
          "score": 2,
          "created_utc": "2026-01-13 23:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcm2m0",
          "author": "charmander_cha",
          "text": "english only =(",
          "score": 4,
          "created_utc": "2026-01-13 13:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcbmtm",
          "author": "Regular_Instruction",
          "text": "Bad I expected multilingual capabilities, like Kyutai TTS 1.6B",
          "score": 1,
          "created_utc": "2026-01-13 12:34:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzcolre",
              "author": "iKy1e",
              "text": "100M vs 1.6B they are both small but the second is 16x bigger.\n\nSo you could have 16 separate models for different languages for that size.",
              "score": 11,
              "created_utc": "2026-01-13 13:53:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzd3hhq",
          "author": "cleverusernametry",
          "text": "DSM tts is the best? Never heard of it and can't find anything in web search..",
          "score": 1,
          "created_utc": "2026-01-13 15:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzdn5rq",
              "author": "KindLizard37",
              "text": "V√°clav from Kyutai here. DSM is Delayed Streams Modeling, our previous text-to-speech: [https://github.com/kyutai-labs/delayed-streams-modeling](https://github.com/kyutai-labs/delayed-streams-modeling)  \nYou can check out a blog post about it \\[here\\](https://kyutai.org/blog/2025-07-03-kyutai-tts-1-6b).\n\nNow we call it \"Kyutai TTS 1.6B\". DSM is confusing naming that made its way from the paper to the blog post and then the screenshot :(",
              "score": 2,
              "created_utc": "2026-01-13 16:41:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze1dga",
                  "author": "hyperdynesystems",
                  "text": "Is it possible to fine-tune this for emotion tags? I looked through the repo and the HF space but didn't see any docs on fine-tuning.",
                  "score": 5,
                  "created_utc": "2026-01-13 17:58:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzen1tf",
          "author": "Embarrassed-Net-5304",
          "text": "Please support Hindi soon!",
          "score": 1,
          "created_utc": "2026-01-13 19:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzez4ue",
          "author": "Lopsided_Dot_4557",
          "text": "This one is much better than the previous Kyutai TTS . I tested it here:  [https://youtu.be/tZda0gepfyQ?si=CTc\\_cnzn1P5aG28j](https://youtu.be/tZda0gepfyQ?si=CTc_cnzn1P5aG28j)",
          "score": 1,
          "created_utc": "2026-01-13 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf6b8d",
          "author": "Green-Ad-3964",
          "text": "what languages does it support? thanks",
          "score": 1,
          "created_utc": "2026-01-13 21:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhkclu",
          "author": "GotHereLateNameTaken",
          "text": "u/KindLizard37 Do we have enough to run it entirely client side on the web? I'd love to try adding it to a pwa or Tauri app.",
          "score": 1,
          "created_utc": "2026-01-14 04:57:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmbf5m",
              "author": "KindLizard37",
              "text": "This is something we really wanted to do for the release but discovered it's not totally trivial. There is some logic in the model which prevents us from e.g. exporting it to ONNX out of the box and using \\[ONNX Runtime Web\\]([https://onnxruntime.ai/docs/tutorials/web/](https://onnxruntime.ai/docs/tutorials/web/)). But it might happen soon: [https://github.com/kyutai-labs/pocket-tts/issues/1](https://github.com/kyutai-labs/pocket-tts/issues/1)",
              "score": 1,
              "created_utc": "2026-01-14 21:57:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlbvf0",
          "author": "keturn",
          "text": "The blog post talks a bit about CFG. With image diffusion models, it's become a common trick to swap out the neutral component of CFG for a different input, creating a \"negative prompt.\" Have you tried this with your TTS models? e.g. can we make a \"sound *less like Javert*\" slider?",
          "score": 1,
          "created_utc": "2026-01-14 19:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzt3n80",
          "author": "Kubas_inko",
          "text": "I am pretty out of the loop when it comes to TTS, but I tried this. And honestly, I am pretty amazed. It runs at 1.2x real-time on a single core of my server (i9-13980HX) with 1.1GB RAM usage when generating 114s of audio from 400-word text (nothing large).  \nI also first tested it with 6 cores, and it ran at more than 4x real-time.\n\nIt is quite good at reading texts, but definitely not great if you want to chat with an LLM, unless you prefer your speaking partner to be dead inside.",
          "score": 1,
          "created_utc": "2026-01-15 21:54:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o007mmq",
          "author": "Prior-Consequence416",
          "text": "This is incredibly timely as I've been building a speech-to-speech pipeline locally and TTS has been a big bottleneck and challenge overall.\n\nCurrently chaining STT (faster-whisper) ‚Üí llama.cpp (Qwen3-4B Q5\\_K\\_M) ‚Üí TTS (kokoro) and the voice synthesis definitely adds some latency. 100M params with no GPU requirement sounds almost too good to be true for real-time use cases.\n\nAnyone tested end-to-end latency yet? Specifically curious how it performs when you need to start streaming audio output before the full text is generated (i.e., chunked/streaming TTS). That's where most solutions fall apart for conversational flows.",
          "score": 1,
          "created_utc": "2026-01-16 22:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03wd7w",
          "author": "SatoshiNotMe",
          "text": "Made a Claude Code voice plugin based on the pocket-TTS CLI: it provides a stop hook that makes CC speak a short update whenever it stops.\n\nhttps://github.com/pchalasani/claude-code-tools?tab=readme-ov-file#-voice-plugin",
          "score": 1,
          "created_utc": "2026-01-17 14:16:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06c8oa",
              "author": "Just_Lingonberry_352",
              "text": "beat ya to it ;)\n\nhttps://github.com/agentify-sh/speak",
              "score": 1,
              "created_utc": "2026-01-17 21:24:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o06h639",
                  "author": "SatoshiNotMe",
                  "text": "Nice, mine is for Claude code, and I wanted to also make one for Codex but realized it needs to be done differently so I got lazy ;)",
                  "score": 2,
                  "created_utc": "2026-01-17 21:49:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o073ztp",
          "author": "murkyduck9",
          "text": "This is really cool! I'm running it now with docker compose and have my homeassistant instance hooked up to it (via a vibe-coded bridge service that exposes pocket tts on the wyoming protocol). Can't believe how fast it is on CPU only. Are the \"built-in\" voices simply just pre-packaged reference audio files, or does the model itself know about them in some way?",
          "score": 1,
          "created_utc": "2026-01-17 23:44:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qefa7q",
      "title": "GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)",
      "subreddit": "LocalLLaMA",
      "url": "https://swe-rebench.com/?insight=dec_2025",
      "author": "CuriousPlatypus1881",
      "created_utc": "2026-01-16 12:59:07",
      "score": 369,
      "num_comments": 89,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/",
      "domain": "swe-rebench.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzxpncw",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-16 15:35:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx2zsy",
          "author": "atape_1",
          "text": "Open model (GLM 4.7) in the top 10! Fuck yeah.",
          "score": 81,
          "created_utc": "2026-01-16 13:43:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzziklp",
              "author": "synn89",
              "text": "With DeepSeek v3.2 very close behind. DeepSeek being a bit larger may make it a better document writer and planner that pairs nicely with GLM as a coder/debugger.",
              "score": 15,
              "created_utc": "2026-01-16 20:25:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01fbcw",
                  "author": "drwebb",
                  "text": "I've burned so many tokens on these two models it's unreal.",
                  "score": 4,
                  "created_utc": "2026-01-17 02:39:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzyuud8",
              "author": "anedisi",
              "text": "this matches my expirience,\ni have access to all of them, but for coding 5.2-codex and arhitecture is the best. higher then opus. its just that claude code is so powerfull.",
              "score": 7,
              "created_utc": "2026-01-16 18:37:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxnay5",
              "author": "Ok_houlin",
              "text": "*Gemini 3 Flash‚Äôs huge lead over Gemini 3 Pro in benchmarks means nothing.*",
              "score": -11,
              "created_utc": "2026-01-16 15:24:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzycazv",
                  "author": "Neither-Phone-7264",
                  "text": "gemini 3 pro sucks ass at tool calling. even in the official gemini app i'll see malformed tool calls occasionally. i'm entirely not surprised.",
                  "score": 12,
                  "created_utc": "2026-01-16 17:15:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o02j22j",
                  "author": "popiazaza",
                  "text": "Why you have to spam this? Google's own benchmark on release already showing 3.0 Flash has higher SWE-bench Verified score than 3.0 Pro.",
                  "score": 1,
                  "created_utc": "2026-01-17 07:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwxnmg",
          "author": "z_3454_pfk",
          "text": "gemini flash is the real shocker here",
          "score": 89,
          "created_utc": "2026-01-16 13:14:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxi11j",
              "author": "Any_Pressure4251",
              "text": "I am not surprised, Gemini Flash has better tool calling then Gemini Pro.\n\nWhen that is fixed for Pro and Ultra we will see a new leader.",
              "score": 19,
              "created_utc": "2026-01-16 15:00:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxku0u",
              "author": "UserXtheUnknown",
              "text": "Incredibly, I can confirm that feeling personally: I often switch models when I don't like an answer, and Gemini-3 flash now often gives me answers that feel more on the point than the pro. (A thing that made me scratch my head for some time: eventually I decided pro has been made overfitting).\n\nBUT! in the long run, with increased context and multiple interactions, it loses that edge and gives sometimes answers that are completely idiotic. (again, in my experience)",
              "score": 25,
              "created_utc": "2026-01-16 15:13:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o00cwqe",
                  "author": "jazir555",
                  "text": ">Incredibly, I can confirm that feeling personally: I often switch models when I don't like an answer, and Gemini-3 flash now often gives me answers that feel more on the point than the pro. (A thing that made me scratch my head for some time: eventually I decided pro has been made overfitting).\n\nThere was a post from one of the google AI devs (maybe Logan?) that they gave flash agentic RL training that they didn't have time to ship with 3 pro, next checkpoint will include that agentic training and surpass flash.",
                  "score": 6,
                  "created_utc": "2026-01-16 22:51:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzxvdak",
                  "author": "boneMechBoy69420",
                  "text": "I agree , could be cause pro tends to over engineering things",
                  "score": 2,
                  "created_utc": "2026-01-16 16:00:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzx2gts",
              "author": "AurumDaemonHD",
              "text": "Especially when u compare the cost 30 cents vs $1.46 for GPT  and $1.22 for Opus. Looks like the Western AI race is over.",
              "score": 35,
              "created_utc": "2026-01-16 13:40:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzzfefw",
                  "author": "procgen",
                  "text": "*Global AI race\n\nGoogle won.",
                  "score": 4,
                  "created_utc": "2026-01-16 20:10:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxs2wi",
              "author": "lemon07r",
              "text": "I've been telling people, gemini flash 3 is surprisingly *very* good. I've been saying its better than any of the OSS models currently (sadly). I have had a lot of access to glm 4.7, minimax m2.1 and k2t, all thorugh coding plans from official providers and flash has felt better than most of them. (my rough personal ranking is flash > k2t > glm 4.7 > minimax m2.1, although I would put glm 4.7 second if ui is involved, but k2t is better at figuring stuff out and more complex things, also K2T is VERY provider dependant, if you arent using kimi for coding api it's not as good for some reason).",
              "score": 8,
              "created_utc": "2026-01-16 15:46:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzx2um5",
              "author": "atape_1",
              "text": "I guess it is really good when you make it reason. When it doesn't it's way worse than PRO, to the point where it is quickly obvious.",
              "score": 5,
              "created_utc": "2026-01-16 13:42:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzxpxhz",
                  "author": "Zc5Gwu",
                  "text": "It‚Äôs very hit or miss. It‚Äôs strange. Some things it‚Äôs absolutely brilliant at and others it‚Äôs dumber than a doornail.",
                  "score": 10,
                  "created_utc": "2026-01-16 15:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxi0nk",
              "author": "jordo45",
              "text": "It's surprisingly good. The 'flash' part is making people underappreciate it, but the value for money is insane right now.",
              "score": 2,
              "created_utc": "2026-01-16 15:00:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzx49iv",
              "author": "__Maximum__",
              "text": "The pro was updated a few days ago, it got worse in my experience. Also, it works wonders in aistudio but sucks in other agentic frameworks. Flash also suffers from this but not that much. Deepseek 3.2, on the other hand, kicks ass in opencode, for example.",
              "score": 4,
              "created_utc": "2026-01-16 13:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzxq9r7",
                  "author": "Zc5Gwu",
                  "text": "I noticed the same. They updated something and everything felt worse‚Ä¶",
                  "score": 1,
                  "created_utc": "2026-01-16 15:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzzcd23",
              "author": "3-4pm",
              "text": "I've been using it as my workhorse in Antigravity while opus is acting as the orchestrator. It makes some mistakes but for the price it's great",
              "score": 1,
              "created_utc": "2026-01-16 19:56:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzxadyf",
          "author": "seaal",
          "text": "I cant wait to see what Deepseek v4 gives us. Properly excited for February.",
          "score": 16,
          "created_utc": "2026-01-16 14:22:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxnf0z",
              "author": "Ok_houlin",
              "text": "*Gemini 3 Flash‚Äôs huge lead over Gemini 3 Pro in benchmarks means nothing.*",
              "score": -19,
              "created_utc": "2026-01-16 15:25:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwyian",
          "author": "dsartori",
          "text": "Appreciate this, and thanks to the whole team for running a terrific service.",
          "score": 14,
          "created_utc": "2026-01-16 13:19:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx0xot",
          "author": "skillmaker",
          "text": "I think this is the most believable benchmark, not those that say GLM 4.7 or Minimax 2.1 are close to Opus 4.5.",
          "score": 51,
          "created_utc": "2026-01-16 13:32:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx3938",
              "author": "Environmental-Metal9",
              "text": "It at least resemble my lived experiences better. I love to hate on Anthropic for sports, but Opus, when I can afford it, or my clients can, is mostly the thing that cracks the real difficult issues so I don‚Äôt have to. Not better than me yet, but I can delegate with some confidence and review. The others require a lot more involvement",
              "score": 21,
              "created_utc": "2026-01-16 13:45:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02viq4",
              "author": "Leflakk",
              "text": "Tbh getting Minimax M2.1 at same level as gpt oss high 120b does not reflect reality where Minimax >> gpt oss",
              "score": 1,
              "created_utc": "2026-01-17 09:31:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxetrw",
              "author": "segmond",
              "text": "It's not, do you want to know why?  Whey folks use GPT, Opus or Gemini, they are using it with the best parameters because they are using API provided by the builder OpenAI/Anthropic/Google.\n\nOften when they use open model they don't get a finely tuned model.  if you use open router, you can't tell if it's been quantized, REAPed or both!    Most people also don't figure out the optimal parameter.   For example, what temp did they use for DeepSeek, GLM or GPT-OSS-120b?  Do you think they used the same temperature or found the best fit?   Did the mention which reasoning effort they used?  \n\nI'm running DeepSeekv3.2-Q4 locally and it crushes GLM-4.7-Q6.   My DeepseekV3.1-Q4 crushed GLM-4.7-Q8.\n\nThese benchmarks are pretty much garbage.   You are certainly missing out if you pick models by benchmarks.  Go run it yourself and see what matters.   As a. matter of fact, I was going to delete Ernie-300B last night but decided to put it through some recent prompts.  For some math problems I'm solving it's the best model by far in terms of explaining and working out the problem, even better than DeepSeek-v3.2 which was a shock to me.\n\nFurthermore, this is a joke since they are comparing models with Claude Code.",
              "score": -7,
              "created_utc": "2026-01-16 14:44:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzxhswc",
                  "author": "skillmaker",
                  "text": "These benchmarks are run using the official provider, which in this case Z.ai and Minimax, so they are not fine tuned or quantized, I was also trying to get the most of the juice from GLM 4.7 and Minimax 2.1 but they couldn't complete a task i gave them, meanwhile Claude sonnet 4.5 in Github Copilot was able to, I'm not saying that they are bad, in fact they are very good at analysing and planning, but i'm talking about the benchmaxing here, in their official websites, they state that these models are very close to Claude Opus 4.5, but that's not true, and from my experience, i think this benchmark is the most accurate one.",
                  "score": 4,
                  "created_utc": "2026-01-16 14:59:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxn754",
              "author": "Ok_houlin",
              "text": "*Gemini 3 Flash‚Äôs huge lead over Gemini 3 Pro in benchmarks means nothing.*",
              "score": -12,
              "created_utc": "2026-01-16 15:24:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzxp2xh",
                  "author": "skillmaker",
                  "text": "Tbh i found using Flash to be better than Gemini 3 Pro, i tried them in Github Copilot and using Antigravity, Pro was always stopping mid work or producing bad solutions",
                  "score": 6,
                  "created_utc": "2026-01-16 15:32:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzx7os8",
          "author": "pip25hu",
          "text": "A legend would be nice. I have no idea what \"pass@5\" is, and if it is explained on the site, I failed to find it unfortunately.",
          "score": 10,
          "created_utc": "2026-01-16 14:08:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxewog",
              "author": "CuriousPlatypus1881",
              "text": "Thanks for asking. **Pass@5**¬†means a task is counted as solved if¬†at least one out of up to five independent attempts¬†passes the full test suite. Each attempt starts from scratch (no state carried over). We‚Äôll also make the legend clearer on the site.",
              "score": 21,
              "created_utc": "2026-01-16 14:44:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzxkoph",
                  "author": "pip25hu",
                  "text": "Thanks, appreciate it.",
                  "score": 3,
                  "created_utc": "2026-01-16 15:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzytr09",
                  "author": "Mkengine",
                  "text": "Just out of interest, why is that tested? Scientifically it is interesting, but practically I never tried to prompt the same thing 5 different times. What does it tell me wenn pass@1 and pass@5 are close or wide apart?",
                  "score": 4,
                  "created_utc": "2026-01-16 18:32:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o00y6i8",
                  "author": "Aggressive-Bother470",
                  "text": "Really? Why would state not be carried over? Agentically, they are refining from each failure...¬†",
                  "score": 0,
                  "created_utc": "2026-01-17 00:51:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxcta8",
              "author": "logTom",
              "text": "I guess it is measuring whether the model could complete the task when given up to five chances, instead of being evaluated on just one attempt.",
              "score": 6,
              "created_utc": "2026-01-16 14:34:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwxeml",
          "author": "Fearless-Elephant-81",
          "text": "Is there a way to contribute to this effort?",
          "score": 13,
          "created_utc": "2026-01-16 13:12:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxxmws",
              "author": "Long-Sleep-13",
              "text": "Good question.. we're currently thinking about axes to continue developing the benchmark (new languages, new scenarios, additional evaluation across scaffoldings instead of models). If you have a reasonable opinion what deserves attention the most, it will be a valuable feedback",
              "score": 5,
              "created_utc": "2026-01-16 16:10:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzx2t92",
          "author": "Environmental-Metal9",
          "text": "This is really cool. One thing notable from the fail to pass data is tagging on reason. Was it just bad code (skills/slop) or refusal? Those are meaningful failure mode differences that I‚Äôd like to filter by",
          "score": 6,
          "created_utc": "2026-01-16 13:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz44h9",
          "author": "MedicalScore3474",
          "text": "https://huggingface.co/datasets/nebius/SWE-rebench-leaderboard/viewer/default/2025_12\n\nhttps://swe-rebench.com/about\n\n1. It looks like you're exclusively benchmarking on Python repositories? Why not include other common languages?\n2. Why limit all models to 128k context? Longer contexts are a strong advantage in using these models, so it seems odd to limit the models with longer context windows just for the benchmark when they will be used in real use cases.\n3. A common issue with SWE-Bench-style benchmarks is the solution being in the git commit history. Do you prevent this in any way, or do you inspect your results to ensure that none of the models in your benchmark are looking ahead to future commits where they shouldn't?\n4. Is your agent scaffolding open-source?\n5. Do you track tool call error rates? This will tell you if a model has a hard time using a particular agent scaffold.",
          "score": 5,
          "created_utc": "2026-01-16 19:18:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0421co",
              "author": "CuriousPlatypus1881",
              "text": "Thanks for the thoughtful questions ‚Äî happy to clarify:\n\n1. For now, SWE-rebench uses only¬†Python repositories, mainly for consistency with SWE-bench and tooling maturity. We are actively working on extending the benchmark to¬†other languages.\n2. We cap all models at¬†128k context¬†to keep comparisons fair (a context size that all models support) and to control costs, similar to how agent runs often have step or budget limits in practice. We‚Äôre aware this can affect absolute quality, but our ablations show that¬†under identical constraints, rankings remain stable.\n3. We¬†remove all commits after the base commit¬†used for the task, so solutions are not present in the git history and models cannot look ahead to future fixes.\n4. The scaffolding is¬†not open-source yet. The system prompt is public on our site, but the full code isn‚Äôt. Conceptually it‚Äôs close to standard SWE-agent / mini-SWE-agent setups: models interact via tools (mostly bash) as described in the prompt. We‚Äôre considering open-sourcing the scaffolding and trajectories in a more convenient form.\n5. Yes ‚Äî we track tool call errors as a¬†separate exit status. A single missed or malformed tool call doesn‚Äôt immediately fail the run; it‚Äôs treated as feedback so the model can correct itself. If there are¬†multiple consecutive failures, the run is terminated.",
              "score": 4,
              "created_utc": "2026-01-17 14:47:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzzlolx",
              "author": "Former-Ad-5757",
              "text": "Regarding the 128 context it is probably because almost all models have their quality within this context, most long context is just marketing fluff which yields worse results overall on precision tasks.",
              "score": 1,
              "created_utc": "2026-01-16 20:40:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzxgi8a",
          "author": "theghost3172",
          "text": "checks out my experience working on real world projects with devstral small 2. this is the first time ive been able to complete my work entirely with a local LLM. it runs really fast on my MI50 and handles simple tasks well when given clear, specific instructions. it's been excellent as my \"coding typist\", i tell it exactly what i need, and it generates the code much faster than I could type it myself.",
          "score": 4,
          "created_utc": "2026-01-16 14:52:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzy8w9",
              "author": "RnRau",
              "text": "Which inference engine do you use?",
              "score": 1,
              "created_utc": "2026-01-16 21:39:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o002bzm",
                  "author": "theghost3172",
                  "text": "llama.cpp gfx 906 optimised fork. [https://github.com/iacopPBK/llama.cpp-gfx906](https://github.com/iacopPBK/llama.cpp-gfx906)",
                  "score": 2,
                  "created_utc": "2026-01-16 21:58:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzycuzk",
          "author": "egomarker",
          "text": "Just as I expected, benchmaxed Devstral 2 immediately went down when half of the tasks were updated. And it will fall even more in the next one.",
          "score": 3,
          "created_utc": "2026-01-16 17:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o00zmw6",
              "author": "Aggressive-Bother470",
              "text": "They're in identical positions as before?¬†",
              "score": 1,
              "created_utc": "2026-01-17 00:59:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0341hp",
                  "author": "egomarker",
                  "text": "Did you even look at the graphs",
                  "score": 1,
                  "created_utc": "2026-01-17 10:51:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzyukb4",
          "author": "EternalOptimister",
          "text": "Price calculation seems completely off??? Opus: avg almost 1.5mil token per problem. Price 1.22$? The price on paper is 5$ input and 25$ output‚Ä¶ \n\nExplain how this is calculated please?",
          "score": 4,
          "created_utc": "2026-01-16 18:35:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03r3h7",
              "author": "CuriousPlatypus1881",
              "text": "Thanks for asking. In SWE-agent setups, the prompt prefix is naturally repeated at every step: each model call includes the full prior conversation and tool history. This makes agent runs extremely efficient¬†with prompt caching, and very expensive¬†without it.\n\nIt‚Äôs also important to note that caches can invalidate over time. Some providers (e.g. Anthropic) let you configure caching behaviour ‚Äî such as cache TTL and which prompt blocks are cached ‚Äî while others do not.\n\nIn the ideal case, when the cache for a trajectory does not invalidate, you effectively pay full price only for¬†new, unique tokens¬†(tool outputs and fresh model responses), which are small compared to the repeated prefix ‚Äî especially in long trajectories.\n\nExample from one real Opus 4.5 trajectory:\n\n* Input tokens:¬†**1,028,547**\n* Output tokens:¬†**12,954**\n* Cached tokens (reads):¬†**979,213**\n* Cache creation tokens (writes):¬†**49,332**\n\nEven here, cache writes are only \\~49k tokens, while nearly 1M tokens are reused via caching.\n\n[Pricing](https://platform.claude.com/docs/en/about-claude/pricing#model-pricing) (Opus 4.5):\n\n* Input: $5e-6\n* Output: $2.5e-5\n* Cached read: $5e-7\n* Cached write: $6.25e-6\n\nPutting it together, the cost is calculated as follows:\n\n    (input ‚àí cached ‚àí cache_write) * input_price\n    = (1,028,547 ‚àí 979,213 ‚àí 49,332) * 5e-6 ‚âà $0.00001\n    \n    cached_reads * cached_price\n    = 979,213 * 5e-7 ‚âà $0.49\n    \n    cache_writes * cache_write_price\n    = 49,332 * 6.25e-6 ‚âà $0.31\n    \n    output * output_price\n    = 12,954 * 2.5e-5 ‚âà $0.32\n\nTotal ‚âà¬†$1.12¬†for that trajectory.",
              "score": 4,
              "created_utc": "2026-01-17 13:47:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o03s85i",
                  "author": "EternalOptimister",
                  "text": "Thanks a lot for the explanation! Did you guys introduce your own caching strategy or is this automatically done with a certain tool? You are basically caching the relevant files and the complete prompt chain is assume?",
                  "score": 1,
                  "created_utc": "2026-01-17 13:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzzcgzc",
              "author": "power97992",
              "text": "Could they be using the claude max sub?",
              "score": 1,
              "created_utc": "2026-01-16 19:56:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00666r",
              "author": "evia89",
              "text": "caching? claude code can have 90% for some tasks",
              "score": 1,
              "created_utc": "2026-01-16 22:17:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o02ipk0",
                  "author": "EternalOptimister",
                  "text": "Nah, caching is for input not output. Let‚Äôs even do a very improbable scenario that only 10% of the mentioned tokens was generated: then the cost would still be above 4$ per task. So the calculation is likely wrong‚Ä¶",
                  "score": 3,
                  "created_utc": "2026-01-17 07:32:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzx2bcm",
          "author": "time_traveller_x",
          "text": "I appreciate your efforts!",
          "score": 3,
          "created_utc": "2026-01-16 13:40:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzywml7",
          "author": "GreenGreasyGreasels",
          "text": "Kinda confirms my feeling that in practice GLM-4.7 is a GPT-5-Mini/Haiku-4.5 competitor, not GPT-5/Sonnet-4.5 class. Both GPT-5-mini and GLM-4.7 are solid reliable work horses that can solve well understood, well defined tasks very well. \n\nI am much less impressed by MiniMax M2.1. I see it as a open source counter part to Grok Code Fast 1 - a cheap, quick and dirty tool which does have its uses.\n\nI'd be very curious  to see how Xiaomi MiMo V2 Flash does on this bench.",
          "score": 3,
          "created_utc": "2026-01-16 18:44:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx2i78",
          "author": "Septerium",
          "text": "what could explain Gemini 3 Flash scoring higher than Pro??",
          "score": 5,
          "created_utc": "2026-01-16 13:41:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzx6ge3",
              "author": "Fuzzy-Chef",
              "text": "It was trained at a later point with stronger RL.",
              "score": 13,
              "created_utc": "2026-01-16 14:01:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzx5ll8",
              "author": "314kabinet",
              "text": "The made Flash specifically for coding tasks, but not Pro.",
              "score": 12,
              "created_utc": "2026-01-16 13:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzx6jh7",
                  "author": "My_Unbiased_Opinion",
                  "text": "Also margin of error I think.¬†",
                  "score": 2,
                  "created_utc": "2026-01-16 14:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxa3h2",
              "author": "Atanahel",
              "text": "Was released a month later with different post training. We can expect the next checkpoint from gemini pro 3 to be soon and with a significant boost.",
              "score": 7,
              "created_utc": "2026-01-16 14:20:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxncjn",
              "author": "Ok_houlin",
              "text": "*Gemini 3 Flash‚Äôs huge lead over Gemini 3 Pro in benchmarks means nothing.*",
              "score": -7,
              "created_utc": "2026-01-16 15:24:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzhxc9",
          "author": "_Erilaz",
          "text": "extra high effort GPT be like\n\nhttps://preview.redd.it/z5zdge7ssrdg1.png?width=1280&format=png&auto=webp&s=160f9aac80f59d10887ed4cabc545a238bf62573",
          "score": 2,
          "created_utc": "2026-01-16 20:22:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o000fuj",
          "author": "lorddumpy",
          "text": "Holy moly. I always avoided Flash since I assumed Pro was more capable. Just gave it a shot on a coding project and it is faster, cheaper, and not failing on tool calls left and right like Pro. The explanations and code seems top-notch too. My wallet thanks you Anton!",
          "score": 2,
          "created_utc": "2026-01-16 21:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00nlk8",
          "author": "kevin_1994",
          "text": "GPT-OSS's high score is incredibly impressive when you consider than the model has only been released with MXFP4 quantization, and the other models on this chart will be FP8 at worst. I still think GPT-OSS-120B is the best model you can run under 250GB of VRAM/RAM",
          "score": 2,
          "created_utc": "2026-01-16 23:50:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o011xry",
              "author": "Front_Eagle739",
              "text": "While its absolutely excellent and my next step down/i need speed model, i still prefer iq2m unsloth quant of glm 4.7 in my 128GB for just about anything. Quantised or not its just better in every regard but speed.",
              "score": 1,
              "created_utc": "2026-01-17 01:14:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o01t4wn",
          "author": "AriyaSavaka",
          "text": "The GLM Coding Plans are also the best in the market right now in term of value. $3 a month and you get 3x the $20 Claude Pro quotas, with no weekly limit bullshit.",
          "score": 2,
          "created_utc": "2026-01-17 04:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02o6fl",
          "author": "Healthy-Nebula-3603",
          "text": "Where is gpt 5.2 codex x high .. which is designed for coding?",
          "score": 2,
          "created_utc": "2026-01-17 08:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxyeuk",
          "author": "eternviking",
          "text": "I do most of the bug fixing and janitorial work on my projects on copilot using Gemini 3 Flash. Surprisingly amazing model for its price point.",
          "score": 1,
          "created_utc": "2026-01-16 16:13:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzy32i6",
          "author": "jkflying",
          "text": "Flash better than Sonnet? Benchmaxxed, sorry.",
          "score": 1,
          "created_utc": "2026-01-16 16:34:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz784a",
          "author": "Simple_Split5074",
          "text": "Great as usual. Thanks a ton!\n\nI think the other interesting point is the extremely high share of cache hits for Anthropic models",
          "score": 1,
          "created_utc": "2026-01-16 19:32:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o004bcf",
          "author": "masterlafontaine",
          "text": "Is it possible to set the reasoning budget of OSS 120b on local? Roo code, for example",
          "score": 1,
          "created_utc": "2026-01-16 22:08:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o024fum",
              "author": "RnRau",
              "text": "> llama-server ... --chat-template-kwargs '{\"reasoning_effort\": \"high\"}'\n\nFrom https://github.com/ggml-org/llama.cpp/discussions/15396\n\nOther inference engines will have their own methods.",
              "score": 3,
              "created_utc": "2026-01-17 05:30:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o038ad7",
          "author": "Randomhkkid",
          "text": "This is awesome! Will you be adding 5.2 codex?",
          "score": 1,
          "created_utc": "2026-01-17 11:30:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03yyxx",
          "author": "dtdisapointingresult",
          "text": "Hi Anton.\n\nCan you explain something to me?\n\nIf I understand your About page, you use \"different scaffolding\" depending on models. If I'm reading between the lines, you're using some generic agent you wrote to test all open models, then using western labs' agentic apps like Claude Code for Anthropic models and so on.\n\nWhy don't you test GLM 4.7 with Claude Code? Zhipu encourages people to do that, and even sponsored a proxy project (https://github.com/router-for-me/CLIProxyAPI). Their model must be trained for Claude Code to a degree.\n\nAre you really comparing apples to apples if you're using your custom \"equal tool\" to test open models, then using the most popular agentic app to test Anthropic's models?\n\nYou're one of my favorite benchmarks, and you're in the best position to put this debate to rest once and for all.\n\nPlease do 1 more test next month: keep doing GLM 4.7 using your scaffolding, but also add a new test for GLM 4.7 using Claude Code. Let's see what score it gets.\n\nThis is information more relevant to us end-users, because none of us care about your custom generic agent or will use it. It's a fact that models will be trained more on a specific tool, and users like us will pick the best even if it means less generalized ability.\n\nIf you're feeling up for it you can also do yet another extra test: Opus 4.5 using your custom tool. Let's see how much it degrades when it's out of its element.",
          "score": 1,
          "created_utc": "2026-01-17 14:30:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04spce",
          "author": "PhotographerUSA",
          "text": "I'm not impressed with GPT-5.2. I think they really slacked off on this version of AI.",
          "score": 1,
          "created_utc": "2026-01-17 16:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09u02e",
          "author": "Zulfiqaar",
          "text": "I like this benchmark! Can you also test different harnesses too? Great to have ClaudeCode there, would be good to have CodexCLI/ and OpenCode - or even some of the agentic IDEs.\n\nAlso really interesting to see how far up the Pass@5 is for Claude..makes me think that a parallel TTC system (like GPT-Pro/Gemini-Deepthink/Grok-Heavy) could make incredible improvements.",
          "score": 1,
          "created_utc": "2026-01-18 11:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx4hgo",
          "author": "FederalLook5060",
          "text": "Real rank based on real life large Project in cursor:  \nOpus  \nVPT 5.2 high  \nsonnet  \nGPT 5.2 medium  \nGemini 3 Pro  \nGLM 4.7  \nGemini Flash 3",
          "score": 1,
          "created_utc": "2026-01-16 13:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzysimb",
          "author": "texasdude11",
          "text": "Benchmaxxed and rigged in favor of some closed source models. Tooling calling and agentic capabilities of Minimax M2.1 is exceptional!",
          "score": -1,
          "created_utc": "2026-01-16 18:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz1183",
          "author": "olmoscd",
          "text": "Can Gemini Pro tell you you‚Äôre being a fool and throwing away money when you ask it to write code for you?",
          "score": 0,
          "created_utc": "2026-01-16 19:04:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx6mi2",
          "author": "_raydeStar",
          "text": "This is great! Do you think GPT 5.2 High Codex could beat opus 4.5? I just got it in cursor and it's nice, better than 5.2x",
          "score": -2,
          "created_utc": "2026-01-16 14:02:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb034t",
      "title": "GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/deepseek-ai/Engram/tree/main",
      "author": "TKGaming_11",
      "created_utc": "2026-01-12 16:49:22",
      "score": 365,
      "num_comments": 89,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nza5nt9",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-13 02:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz83vji",
          "author": "FullOf_Bad_Ideas",
          "text": "Another great paper from DeepSeek team. They never disappoint when it comes to original ideas.\n\nEdit: finished it. They use model with mHC (ùëÄ = 4) for ablations, meaning that they probably derisked mHC for the next run and see this as \"current stable meta\". And they claim \"We envision conditional memory functions as an indispensable modeling primitive\nfor next-generation sparse models.\", so I think there's a high chance that the model they'll release next will have both of those things included. I'd assume that their next-gen model is in training right now, and they were using this free time to polish off the papers and release them.\n\nAlso, if this will be adopted, it's great news for us. Models that will have Engram, will be more performant per parameter for traditional MoE architecture, and they'll have a big new part that will be easily offloadable to RAM with no performance penalty at all. So a 40B A3.8B MoE from their ablation tests would need only 27B of weights to be placed on fast memory, with the remaining 13B being comfy in RAM or maybe even 95% offloaded to NVMe.\n\nI really love their innovations, they are a great example of an AI lab that applies resources into practical systemic solutions that quickly and successfully land in final products, they have really outstanding impact.\n\nAnother thing - they're using Muon as optimizer for those ablations. Which means, next-gen will probably be trained with Muon and not AdamW. Just like Kimi K2 and GLM 4.5",
          "score": 112,
          "created_utc": "2026-01-12 20:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9l1mu",
              "author": "Old-School8916",
              "text": "i think v4 is coming out next month, I wonder if it'll have this shizz.",
              "score": 22,
              "created_utc": "2026-01-13 00:43:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzallqp",
                  "author": "TheRealMasonMac",
                  "text": "Ngl, I'm praying for good multi-turn long context. K2-Thinking/GLM go down to 1 IQ after enough turns in the agentic loop.",
                  "score": 10,
                  "created_utc": "2026-01-13 04:02:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzp79zm",
              "author": "Mnode-Lab",
              "text": "Great analysis. I want to add one angle on why the CPU-side memory offloading here matters more than it might look at first glance.\n\nThis direction isn‚Äôt unique to DeepSeek. We‚Äôve seen related ideas before ‚Äî Gemma‚Äôs per-layer embeddings, RWKV‚Äôs deepembed, ByteDance‚Äôs UltraMem, etc.\n\nFrom a pure algorithm perspective, hash-based n-gram lookup is obviously not ideal. The same fact phrased differently (or in another language) maps to different keys, so generalization is weak and redundancy/noise are hard to avoid. UltraMem tries to fix this with learnable mappings, but that adds parameters and makes the system harder to tune.\n\nWhat DeepSeek seems to be doing instead is a system-level trade-off. Rather than chasing a cleaner algorithm, they simplify the computation and push it *before* inference: raw input tokens, simple lookup, and run the whole thing in CPU memory. You lose algorithmic elegance, but you get zero GPU memory usage, very simple logic, and a preprocessing step that can be fully offloaded to CPUs.\n\nOnce this lives in CPU memory, the optimization target changes. Parameter efficiency and per-query optimality matter less. Even if the hash table is noisy or redundant, it‚Äôs cheap and doesn‚Äôt touch scarce GPU memory. At the system level, that trade-off makes a lot of sense ‚Äî especially for cloud inference where CPU resources are relatively abundant.\n\nFor local deployment, this could be a big deal. If something like the 13B Engram component can sit in RAM while the 27B MoE part stays in VRAM, that‚Äôs a much more accessible setup for consumer hardware.",
              "score": 8,
              "created_utc": "2026-01-15 09:15:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbaldr",
              "author": "Mikasa0xdev",
              "text": "Sparsity is the new density for LLMs.",
              "score": 5,
              "created_utc": "2026-01-13 07:04:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzal9hk",
              "author": "ai-infos",
              "text": "\"they'll have a big new part that will be easily offloadable to RAM with no performance penalty at all\" >>>  if true, that would be really really BIG!\n\nand also, that would explain partially the crazy prices of RAM... (i guess closed AI labs already knew about it and already implemented equivalent architecture using mix of RAM/VRAM in their infra and so that explains the BIG need in RAM for potential Trillons parameters MoE models...)",
              "score": 8,
              "created_utc": "2026-01-13 04:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzc0p3e",
                  "author": "FullOf_Bad_Ideas",
                  "text": "I think RAM prices don't have Engram priced in, and it should not affect them by much. RAM is probably used the most for kv cache offloading and during training, and each machine gets a lot of it even if it won't be used, just because it's cheaper than vram and sometimes it'll turn out you wanted to have that RAM there.\n\n> if true, that would be really really BIG!\n\nThe caveat there is that it works best in terms of pretraining compute utilization when Engram makes up about 20% of the total model parameters. So in makes more economic sense to train 100B A10B E20B model where that offloading helps just a bit but here for running models locally on gpus with cpu offload we'd profit the most from crazy Engram ratios like 100B A10B E80B. And those are not as compute efficient to train, and they will perform worse than normal 100B models. So it has potential but that potential might not be practically explored by companies training those models, since they usually have local inference as an after thought, and they prioritize training the best model possible with limited compute.\n\nEdit: grammar",
                  "score": 3,
                  "created_utc": "2026-01-13 11:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzbc9ok",
                  "author": "Nyghtbynger",
                  "text": "We'll offload it to NVMe !!",
                  "score": 4,
                  "created_utc": "2026-01-13 07:19:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzn95yx",
                  "author": "DerDave",
                  "text": "Nope. RAM prices are high, because all capacity (both DRAM and VRAM) is completely overbooked. Thank Sam for this...",
                  "score": 0,
                  "created_utc": "2026-01-15 00:52:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzbiytw",
              "author": "zball_",
              "text": "maybe even offloadable to ssd.",
              "score": 2,
              "created_utc": "2026-01-13 08:21:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o04grq5",
              "author": "Yes_but_I_think",
              "text": "I would think of this like: \n\nwe had small logical reasoning models which know no GK, but can put things together if they are given in context.\n\nwe have large 1T models which remember facts but are a overkill for reasoning.\n\nThey are proposing a hybrid between the two - large parameters but less compute needed for fact tokens and more compute for thinking tokens.\n\nIs this what they are telling?",
              "score": 1,
              "created_utc": "2026-01-17 16:00:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6uky3",
          "author": "Rokpiy",
          "text": "the n-gram embedding approach is interesting. most models only scale via MoE (neural computation), but engram adds static memory as a complementary sparsity axis with O(1) lookup\n\nthey found a u-shaped scaling law between MoE and Engram, which guides how to allocate capacity between the two. analysis shows it relieves early layers from static pattern reconstruction, preserving depth for complex reasoning\n\ndeterministic addressing means they can offload the embedding tables to host memory without much inference overhead",
          "score": 55,
          "created_utc": "2026-01-12 16:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9ypkx",
              "author": "Punsire",
              "text": "Damn, thank you. I could understand more about each thing you explained by virtue of the relations to each other component without you having to explicitly describe their part and function .",
              "score": 7,
              "created_utc": "2026-01-13 01:58:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nza6pz3",
                  "author": "Rokpiy",
                  "text": "Glad it helped :)",
                  "score": 2,
                  "created_utc": "2026-01-13 02:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz8sn6q",
          "author": "Few_Painter_5588",
          "text": "Perhaps this is the breakthrough that Deepseek made and will roll out for Deepseek V4? M",
          "score": 11,
          "created_utc": "2026-01-12 22:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o037zg4",
              "author": "eXl5eQ",
              "text": "If this is really a breakthrough, then it would only be revealed in the DeepSeek V4 paper, like MLA in V3, GRPO in R1 and DSA in V3.2. The fact that they published this without publishing a model suggests that they don't think it worth training a new model based on this.",
              "score": 2,
              "created_utc": "2026-01-17 11:27:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o038rwb",
                  "author": "Few_Painter_5588",
                  "text": "No, deepseek published their first GRPO paper a full year almost before Deepseeek R1\n\n[https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)",
                  "score": 7,
                  "created_utc": "2026-01-17 11:34:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz99s6c",
          "author": "Aaaaaaaaaeeeee",
          "text": "Introducing deeper-seeker, a 3T reasoning model with 600B ngram parameters, 150+ layers, 2.4T, 70A and my condolences to your RAM outage.",
          "score": 12,
          "created_utc": "2026-01-12 23:43:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9jhry",
              "author": "FullOf_Bad_Ideas",
              "text": "We'll probably be keeping engram params on NVMes.\n\nI don't think it'll be much bigger. Expert serving complexity and scaling laws show that around A30B is a good tradeoff, and around 1/32 is a good sparsity. So I think i'll be around 1T with 200B engram params.",
              "score": 9,
              "created_utc": "2026-01-13 00:35:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0395ux",
              "author": "eXl5eQ",
              "text": "600B ngram parameters don't make any sense. It's more like a multi-token embedder rather than another MoE layer, and there's only limited amount of meaningful n-gram combinations, so overscaling it won't help.",
              "score": 2,
              "created_utc": "2026-01-17 11:38:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbmydv",
              "author": "martinerous",
              "text": "One day they will evolve from seeker to finder....",
              "score": 1,
              "created_utc": "2026-01-13 08:59:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbfm43",
          "author": "Vivarevo",
          "text": "Vram embargo on china is turning out to be the catalyst for innovation.\n\nElsewhere mega models fit in to enterprise servers. Consuming vast resources and remain out of reach for majority of potential users.\n\nThats at least the feel of things as they currently stand",
          "score": 12,
          "created_utc": "2026-01-13 07:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7c0ix",
          "author": "TransportationSea579",
          "text": "we're getting out of the MPC server with this one chooms",
          "score": 27,
          "created_utc": "2026-01-12 18:12:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzb8qim",
              "author": "Nyghtbynger",
              "text": "Saw a few diagrams, looks like another object oriented programming but I never really checked what a MPC is. Should I just skip it ?",
              "score": 3,
              "created_utc": "2026-01-13 06:48:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzdvc2d",
                  "author": "TransportationSea579",
                  "text": "I was just making a cyberpunk joke after I saw engram lol",
                  "score": 1,
                  "created_utc": "2026-01-13 17:30:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz828nj",
          "author": "__Maximum__",
          "text": "When you think about it, this was such an obvious thing to do, in hindsight, of course.\n\nI am pretty sure all animals do this kind of stuff in their brain, even humans.",
          "score": 20,
          "created_utc": "2026-01-12 20:11:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8ok4l",
              "author": "menictagrib",
              "text": "The hippocampus anchors recent (relatively) events in space and time via sparse coding to maintain orthogonality. This is effectively how most \"new information\" is initially stored, often using these systems for months/years.",
              "score": 11,
              "created_utc": "2026-01-12 21:55:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzar6u3",
          "author": "Determined-Hedgehog",
          "text": "I am not saying I am dumb but could someone simplify this for me so that I can get it easier? I have been away from the local scene working recently.",
          "score": 7,
          "created_utc": "2026-01-13 04:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz72yv5",
          "author": "astronomikal",
          "text": "I‚Äôve got 0(1) with no GPU!\n\nI was doing some fun things with n-gram filters a few months ago but found a better way for persistent memory. This is awesome for its use case tho.",
          "score": 13,
          "created_utc": "2026-01-12 17:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9o6ji",
              "author": "pixelpoet_nz",
              "text": "That's a zero and not an O :D",
              "score": 12,
              "created_utc": "2026-01-13 01:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nza5bur",
                  "author": "astronomikal",
                  "text": "Was partially doing this via voice to text lmao.",
                  "score": 6,
                  "created_utc": "2026-01-13 02:33:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz90bzs",
              "author": "jazir555",
              "text": "My dude over here beating major research labs by months.",
              "score": 10,
              "created_utc": "2026-01-12 22:53:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzasvq1",
                  "author": "astronomikal",
                  "text": "I just had a random idea one day to do some funky stuff with kernels. I‚Äôll dig them up and throw the good ones up in a repo tomorrow after work.\n\n*sigh* ~~false alarm... approximately 5 months ago i had to rebuild the entire project again from scratch after my stubbornness to not use github bit me in the ass with a mistaken force removal of my whole codebase. It was a lesson learned but i guess the kernels i had made ended upthere. I can try and dig them up another way but it will take some time~~\n\nI FOUND THEM! uploading now.",
                  "score": 5,
                  "created_utc": "2026-01-13 04:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzbarga",
                  "author": "Nyghtbynger",
                  "text": "We should make a leaderboard of \"I called it\" and then allocate winners based on papers",
                  "score": 1,
                  "created_utc": "2026-01-13 07:05:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9jjcf",
              "author": "polawiaczperel",
              "text": "Can you tell something more about it?",
              "score": 5,
              "created_utc": "2026-01-13 00:35:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzarv2u",
                  "author": "astronomikal",
                  "text": "The memory system or my use of n-gram filters?",
                  "score": 1,
                  "created_utc": "2026-01-13 04:40:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz91qa3",
          "author": "Tiny_Arugula_5648",
          "text": "I'd love to see what effect larger ngrams would have. Code and math should improve at 5.. why not load up the CPU ram? They seemed pretty conservative in the limits they chose.",
          "score": 3,
          "created_utc": "2026-01-12 23:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz936kl",
              "author": "zjuwyz",
              "text": "They briefly mentioned it at the end of Section 6.2. 4-gram didn't perform better than 3-gram. After all, this is a hash table, not a dictionary. There are too many combinations of four consecutive tokens, and the proportion of meaningful semantic entities is very low.",
              "score": 9,
              "created_utc": "2026-01-12 23:07:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzc3f79",
          "author": "power97992",
          "text": "I wonder will this pave the road for continual training during inference‚Ä¶? Maybe one day switchable engrams",
          "score": 3,
          "created_utc": "2026-01-13 11:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0646wj",
              "author": "Kubas_inko",
              "text": "That's what I can't wait for. Models somehow learning new data (and most likely forgetting some old/unused data, otherwise goodbye storage).",
              "score": 1,
              "created_utc": "2026-01-17 20:42:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o06eob3",
              "author": "dinerburgeryum",
              "text": "Hot-pluggable engrams were my first thought as well. They point out in the paper that actually training the engrams is a pretty gnarly task, so I‚Äôm not sure how much we should expect from ‚Äúcommunity‚Äù efforts, but it‚Äôs still a cool thing to consider.¬†",
              "score": 1,
              "created_utc": "2026-01-17 21:36:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz966fo",
          "author": "maxpayne07",
          "text": "Will this allow,  lets say, off-load to SSD disk without losing inference speed? \n\nIf then, its going to be awesome, image you can off-load a 400B parameters to a not so good PC.",
          "score": 7,
          "created_utc": "2026-01-12 23:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9k5ix",
              "author": "FullOf_Bad_Ideas",
              "text": "yes, there will be a part of the model that will have predictable low bandwidth ultra-sparse parameters. But not the whole model, just some of it.\n\nin their tests they did 4B model and 100B engram for example.\n\nSo you'd load 4B to VRAM, taking around 5GB with KV Cache assuming FP8 native training, you'd load some hot section of engram to RAM, let's say 20GB, and you'd load the remaining 80GB from NVMe on demand. And performance would be on the order of that of a 10B model which would require 11GB of VRAM (just guessing this one).",
              "score": 13,
              "created_utc": "2026-01-13 00:38:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbdwli",
                  "author": "shing3232",
                  "text": "The great thing about engram is that it's cheap to pretrained and good for long context.\n\nit greatly improve model ‚Äòs world knowledge",
                  "score": 5,
                  "created_utc": "2026-01-13 07:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz9rtr4",
              "author": "Several-Tax31",
              "text": "Is this true? The idea of running a 400-500B model on a potato gives me more goosebumps than anything else. I want to run those SOTA models locally, please!¬†",
              "score": 9,
              "created_utc": "2026-01-13 01:20:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbyvht",
                  "author": "FullOf_Bad_Ideas",
                  "text": "If they decide to allocate training budget to a giant engram pool, it should scale and work. And we could end up with 400B A5B E370B models that have only 30B traditional parameters. But this model would be as hard to train as a 400B A5B non-Engram model would, while having performance less to that of a 400B MoE without Engram, so it would not be optimal from the perspective of efficient pretraining. It would be very cheap to deploy though, compared with other models of similar performance. I don't think Deepseek will train a small MoE with big engram, they're focused on SOTA that is cheap to train and serve at scale. So, this could become a reality only if their competitors like Zhipu or Tencent pick it up and focus on this.",
                  "score": 4,
                  "created_utc": "2026-01-13 10:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz9iwrj",
          "author": "zball_",
          "text": "It's conceptually similar to Gemma-3n's Per Layer Embedding, but extended to n-gram.",
          "score": 2,
          "created_utc": "2026-01-13 00:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbvi2k",
          "author": "RealAnonymousCaptain",
          "text": "I'm worried with how engram works as it seems like it'll cause models to be more susceptible to data biases or contamination. If ngram retrieves conditional memory based two to three word sequences, that just leads to more efficiency but less flexibility in its output.\n\n\nBut I'm not too well-versed in the technical details, so if anyone could elaborate itd be cool",
          "score": 2,
          "created_utc": "2026-01-13 10:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbxpgw",
              "author": "FullOf_Bad_Ideas",
              "text": "It will lead to more biases. But being more susceptible to biases in data means lower loss and higher performance. LLMs imitate the biases of the training data. If they didn't, they wouldn't be that useful. Knowledge is largely stereotyped.\n\nI don't see how it would lead to contamination. Don't put benchmark datasets in the training data and you'll avoid contamination, model architecture doesn't determine how likely contamination is.",
              "score": 1,
              "created_utc": "2026-01-13 10:42:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcqz4q",
                  "author": "RealAnonymousCaptain",
                  "text": "Sorry, I meant more susceptible to contaminated/flawed data. I was writing while distracted and running on fumes so my grammar is bad right now.\n\nBut I disagree with your point about training data, yes they are trained to follow them and are inherently biased. But I'm talking about false biases and illogical data in them like the recent seahorse/igloo/traffic cone emoji blunder where that's present in several AI models. I'm worried that engram will make Deepseek's newer models to be significantly less factually correct or have more errors in it's output because of flawed data.",
                  "score": 1,
                  "created_utc": "2026-01-13 14:05:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzc2pp9",
          "author": "power97992",
          "text": "So the prediction was correct, a >1.5 Trillion param ds model is coming.",
          "score": 2,
          "created_utc": "2026-01-13 11:26:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8u2mv",
          "author": "Interpause",
          "text": "Reminds me of embedding patches like in BLT, but iven't read either paper deep enough to know the difference",
          "score": 1,
          "created_utc": "2026-01-12 22:22:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbkdtw",
          "author": "aragorn__gondor",
          "text": "\n\nLIMIT paper (Aug 2025) exposes dense embedding collapse. I built Numen (Nov 2025): char n-gram hashing ‚Üí 32k-dim dense vectors, no training, 93.9% R@100 > BM25 on LIMIT\n\nDeepSeek Engram (Jan 12, 2026) does similar inside LLMs: hashed token n-grams for conditional memory : massive gains\n\nBeautiful convergence: hashed n-grams fix both external retrieval limits AND internal Transformer memory waste. Numen proves it works externally without training.¬†\n\n\n\nLink to mine implementation:\n\n[https://github.com/sangeet01/limitnumen](https://github.com/sangeet01/limitnumen)\n\n\n\nDeepseek's implementation:\n\n[https://github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram)\n\n\n\nLIMIT DATASET:¬†\n\n[https://huggingface.co/datasets/orionweller/LIMIT](https://huggingface.co/datasets/orionweller/LIMIT)",
          "score": 1,
          "created_utc": "2026-01-13 08:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzc8gjq",
          "author": "Legumbrero",
          "text": "Wonder if you could quantize the engram part of the model aggressively while leaving the moe's at a higher precision and see good results.  Architecture seems like a good candidate for mixed precision.",
          "score": 1,
          "created_utc": "2026-01-13 12:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzeebpx",
          "author": "jashAcharjee",
          "text": "Engram? CyberPunk 2077 reference?",
          "score": 1,
          "created_utc": "2026-01-13 18:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbkx5z",
          "author": "ninadpathak",
          "text": "This is fascinating work on conditional memory. What I'm taking away here is that selective memory retrieval is better than raw context windows (obviously) on both latency and cost metrics. \n\nA few interesting angles:\n\n1. **The sparsity aspect** \\- only loading relevant memory indices is clever. This is why memory layers are becoming essential in production LLM systems.\n2. For anyone implementing this, the real challenge is the semantic ranking problem. How do you decide what's \"relevant\" without scanning everything?\n3. **Scale problem** \\- this works well until your memory corpus grows to millions of tokens. Then you hit vector DB performance walls.\n\nIf anyone's building systems around this, we started a sub to discuss these exact tradeoffs over at r/mem0 and also to try and make the product even better for everyone. \n\nHop on over if you think that interests you!",
          "score": 1,
          "created_utc": "2026-01-13 08:40:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz91mz4",
          "author": "VampiroMedicado",
          "text": "/u/AskGrok explain this for 5 years old.",
          "score": -6,
          "created_utc": "2026-01-12 22:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9rse7",
          "author": "Better_Story727",
          "text": "DeepSeek's contribution is truly groundbreaking.\n\nIt doesn‚Äôt just achieve¬†**infinite context**; it paves the way for a clean architectural separation between dedicated¬†**memory models**¬†and¬†**reasoning models**. This decoupling will drastically enhance training efficiency.\n\nConsider the implications if what we store isn't just \"memory,\" but¬†**operators**. Given that multi-dimensional continuous parameters treat memory and operators as two sides of the same coin, this opens the door for¬†**ultra-deep, ultra-compact computational subsystems**.\n\nBy outsourcing memory, the context window could shrink dramatically. In a network where memory is entirely externalized, the \"context\" effectively disappears, allowing for a¬†**fully parametric (context-less) neural network**.\n\nFurthermore, if memory retrieval becomes deterministic, we can eliminate the \"computational bubble\" (overhead). This leads us toward¬†**brain-like hardware**: pure computation with zero data movement, potentially reaching energy efficiency levels¬†$10\\^4$¬†to¬†$10\\^7$¬†times higher than current architectures.\n\nDeepSeek didn't invent this direction, but by making it an engineering reality, they have fundamentally accelerated the trajectory of AI.",
          "score": -13,
          "created_utc": "2026-01-13 01:20:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9xjfc",
              "author": "Redoer_7",
              "text": "Pure slop and not true \"infinite context \"",
              "score": 14,
              "created_utc": "2026-01-13 01:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzajgql",
              "author": "INtuitiveTJop",
              "text": "Not only did I like your comment, but it received a well versed upvote. Truly spectacular!",
              "score": 3,
              "created_utc": "2026-01-13 03:50:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qcusnt",
      "title": "Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/v0c2rda9scdg1",
      "author": "eugenekwek",
      "created_utc": "2026-01-14 18:16:00",
      "score": 314,
      "num_comments": 54,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nznwxy7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 03:10:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkz53o",
          "author": "SlowFail2433",
          "text": "Wow that actually seems useable for 80M",
          "score": 52,
          "created_utc": "2026-01-14 18:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4i54",
              "author": "eugenekwek",
              "text": "Thank you! That means a lot",
              "score": 16,
              "created_utc": "2026-01-14 18:43:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl6rwq",
                  "author": "SlowFail2433",
                  "text": "I have some agentic systems where the vocal quality isn‚Äôt rly a main focus it just needs to be able to speak to convey information so these are ideal",
                  "score": 4,
                  "created_utc": "2026-01-14 18:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzrhj1l",
                  "author": "MoffKalast",
                  "text": "No that's exactly the point, 80M is not a lot! /s",
                  "score": 1,
                  "created_utc": "2026-01-15 17:29:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl5n4p",
          "author": "Itachi8688",
          "text": "This is impressive for a 80M model.\nAny plans for onnx support?",
          "score": 19,
          "created_utc": "2026-01-14 18:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzla7yd",
              "author": "eugenekwek",
              "text": "boy do I have a surprise for you soon :)",
              "score": 28,
              "created_utc": "2026-01-14 19:09:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzlp9uz",
                  "author": "exaknight21",
                  "text": "Mmmboy are you fat.",
                  "score": 6,
                  "created_utc": "2026-01-14 20:17:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzo46it",
                  "author": "Itachi8688",
                  "text": "üëÄ",
                  "score": 1,
                  "created_utc": "2026-01-15 03:55:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlar0s",
          "author": "coder543",
          "text": "This seems very impressive. I don't know how one person is making such a good, small TTS model, but it seems to be working. One thing that I think could be more consistent is the handling of em-dashes. If I write a long sentence ‚Äì one that needs an aside in it ‚Äì I expect someone reading it to pause briefly at each em-dash so the listener knows an aside is happening. One example I tried it did seem to briefly pause at the first one, which was good, but another, it just rushed through like it was a run on sentence.\n\nI also noticed that (in the one time I tried) it read \"TTS\" as \"text to speech\", which I consider to be a hallucination, since the text was \"TTS\", and TTS could mean something completely different depending on context.",
          "score": 10,
          "created_utc": "2026-01-14 19:11:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlcavz",
              "author": "eugenekwek",
              "text": "Yeah those can both be fixed, open an issue on Github so I remember to do this!",
              "score": 12,
              "created_utc": "2026-01-14 19:18:50",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzlrihp",
              "author": "SuchAGoodGirlsDaddy",
              "text": "TTS*\n\n*Thanking this soliloquy",
              "score": 2,
              "created_utc": "2026-01-14 20:27:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzkywvd",
          "author": "Ok_Appearance3584",
          "text": "Awesome! Checking this out tomorrow.",
          "score": 9,
          "created_utc": "2026-01-14 18:19:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzla4ba",
              "author": "eugenekwek",
              "text": "Thank you for the support!",
              "score": 5,
              "created_utc": "2026-01-14 19:08:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlbjel",
          "author": "SpaceNinjaDino",
          "text": "Thank you for fixing this!",
          "score": 6,
          "created_utc": "2026-01-14 19:15:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzlbxip",
              "author": "eugenekwek",
              "text": "No problem!",
              "score": 3,
              "created_utc": "2026-01-14 19:17:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlhstv",
          "author": "PostEasy7183",
          "text": "Hi helllloooooooooo *Stroke*",
          "score": 5,
          "created_utc": "2026-01-14 19:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkzbrq",
          "author": "KokaOP",
          "text": "streaming? or let me just check it out",
          "score": 5,
          "created_utc": "2026-01-14 18:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4ez3",
              "author": "eugenekwek",
              "text": "Streaming is supported already, with <15 ms latency on GPU! You can find some examples in the repo.",
              "score": 13,
              "created_utc": "2026-01-14 18:43:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzl46y0",
              "author": "fnordonk",
              "text": "It's in the feature list",
              "score": 2,
              "created_utc": "2026-01-14 18:42:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlga6u",
          "author": "inigid",
          "text": "This is simply incredible work.  Great job.",
          "score": 2,
          "created_utc": "2026-01-14 19:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlwx7v",
          "author": "SuchAGoodGirlsDaddy",
          "text": "For the dumber among us, like myself, can you confirm or deny that this is a TTS model that will still need to be in a pipeline of STT->LLM->TTS(Soprano) and that it isn‚Äôt a complete multimodal large language model at just 80M?\n\nThe output sounds great for the size, even relative to other TTS models Ive tried, I just want to make sure I‚Äôm understanding it right and thet my excitement is metered.",
          "score": 2,
          "created_utc": "2026-01-14 20:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmt39h",
              "author": "no_witty_username",
              "text": "Yes, while text to speech models are used in many areas, a personal agent is where it will get most use as the third pipeline step. What you might be thinking on the side is an audio to audio model. Those are much more rare and are not as useful as stt>llm>tts pipelines, you cant have them do intermediary steps like advanced reasoning or agent calling or function calling if its only audio to audio model.",
              "score": 1,
              "created_utc": "2026-01-14 23:25:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzrjozf",
              "author": "MoffKalast",
              "text": "It's a TTS. Are there even any open weight multimodal LLMs that can generate audio at all?",
              "score": 1,
              "created_utc": "2026-01-15 17:39:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nznc8wf",
          "author": "MumeiNoName",
          "text": "Could this run in users browser for a web app?",
          "score": 2,
          "created_utc": "2026-01-15 01:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpli1i",
          "author": "martinerous",
          "text": "Great, it's getting better and better. I especially like the fact that you are actively engaging with the community and maintaining the project. I have seen a few TTS solutions being abandoned because they were just like proof-of-concept for a research paper, or the company behind the TTS ignores the community. Your project has the potential to become a truly open and evolving TTS.\n\nI'm now thinking if I could finetune it for my native (Latvian) language, similarly to how I did with VoxCPM 1.5 - another great small-ish and fast (on GPU) model with finetune scripts bundled. But first, I would like to wait when Soprano can do voice cloning because my training data is quite chaotic and I would want the model to learn to speak in demonic thousand voices :D",
          "score": 2,
          "created_utc": "2026-01-15 11:26:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl3slu",
          "author": "Eyelbee",
          "text": "I don't know about voicegen but based on the video alone, isn't vibevoice clearly far superior?",
          "score": 3,
          "created_utc": "2026-01-14 18:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl4w7j",
              "author": "coder543",
              "text": "With 19x as many parameters, VibeVoice had *better* be superior, or else it would be entirely pointless. But I am surprised at how good the sample above sounded for an 80M model.",
              "score": 16,
              "created_utc": "2026-01-14 18:45:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzl6638",
                  "author": "Eyelbee",
                  "text": "Whoops, I misread it as 1,5M, sorry",
                  "score": 3,
                  "created_utc": "2026-01-14 18:51:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzl4wrt",
              "author": "eugenekwek",
              "text": "Yeah probably a little better, but VibeVoice is also 20x bigger!",
              "score": 14,
              "created_utc": "2026-01-14 18:45:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzl4mzi",
              "author": "silenceimpaired",
              "text": "It is bigger‚Ä¶ so still pretty impressive",
              "score": 2,
              "created_utc": "2026-01-14 18:44:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl6442",
          "author": "KneelB4S8n",
          "text": "I hope it didn't stop randomly singing in Mongolian throat...",
          "score": 1,
          "created_utc": "2026-01-14 18:51:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlfff2",
          "author": "OkStatement3655",
          "text": "Love to see your commitment to the open-source community. Where do you get the training data from?",
          "score": 1,
          "created_utc": "2026-01-14 19:33:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzljzx2",
          "author": "michaelsoft__binbows",
          "text": "80M is wild",
          "score": 1,
          "created_utc": "2026-01-14 19:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmczg",
          "author": "cheesecakegood",
          "text": "Super cool",
          "score": 1,
          "created_utc": "2026-01-14 20:04:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmnm7",
          "author": "lorddumpy",
          "text": "Super impressive! Awesome demo too, seeing the actual vs realtime calculation (averaging around 30x-40x) is so damn neat",
          "score": 1,
          "created_utc": "2026-01-14 20:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlmo9w",
          "author": "cms2307",
          "text": "How bout dat",
          "score": 1,
          "created_utc": "2026-01-14 20:05:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlnrh9",
          "author": "Chromix_",
          "text": "The quality has drastically improved compared to the previous version. It now [aces](https://vocaroo.com/19NXd5zP7A10) the previous test that had lots of [very obvious issues](https://www.reddit.com/r/LocalLLaMA/comments/1pt3sco/comment/nvealn2/). Now only a few minor pronunciation issues remain.",
          "score": 1,
          "created_utc": "2026-01-14 20:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlq4di",
          "author": "mrmontanasagrada",
          "text": "keep it up man!\n\nOut of curiousity, how did you fix it? Just more data / training, or something specific?",
          "score": 1,
          "created_utc": "2026-01-14 20:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzluelh",
          "author": "Hurricane31337",
          "text": "The Soprano Factory sounds especially interesting. Thank you so much for your hard work!\nDo you think I could train a German Soprano just by putting in German wav audio and metadata.txt? If yes, how much audio would I need for that?",
          "score": 1,
          "created_utc": "2026-01-14 20:41:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlw8py",
          "author": "DocHoss",
          "text": "This is awesome, great work! I'd like to get into building some small hyper-focused models like this. Would you be able to share how you actually built Soprano? Any tutoriala you used, info you found useful, anything like that?",
          "score": 1,
          "created_utc": "2026-01-14 20:49:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmhj5j",
          "author": "az226",
          "text": "How many GPU hours did you need to train it?",
          "score": 1,
          "created_utc": "2026-01-14 22:26:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmy6pm",
          "author": "EndlessZone123",
          "text": "This is the best supported TTS model released with updates, training and api I have seen. So many are either very big models and lack training or api.",
          "score": 1,
          "created_utc": "2026-01-14 23:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznis0j",
          "author": "AfterAte",
          "text": "The intonation of \"Hi, What are you up to?\", Saprano 1.1 80B does it how I would say it, if I was welcoming customer into to my shop. Chatter-box sounds sus, like it's a parent looking in on its too quiet child. Vibevoice... nobody talks like that.\n\n\nAs for audio quality, Saprano and Chatterbox are the same (better than 3khz phone, worse than 44khz CD), and Vibevoice is great 44khz CD quality. But there's music in the background too. Are hallucinations like that common in Vibevoice?",
          "score": 1,
          "created_utc": "2026-01-15 01:47:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzowr8y",
          "author": "rm-rf-rm",
          "text": "whats the real world usability if its just 30s long? would chopping up text and chaining generations result in a usable output?",
          "score": 1,
          "created_utc": "2026-01-15 07:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoym57",
          "author": "bhupesh-g",
          "text": "Hey thanks for such a nice model, just one question, can it speak numbers and dates also well?",
          "score": 1,
          "created_utc": "2026-01-15 07:52:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0y0x",
          "author": "TJW65",
          "text": "I already posted this under your release post regarding soprano factory, but could you provide us with a docker image to host the OpenAI compatible API? I would be really happy to see that.",
          "score": 1,
          "created_utc": "2026-01-15 08:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpfwgr",
          "author": "braydon125",
          "text": "The intro sounded great but that hi hellooooo what are you up toooo is nightmare fuel lol",
          "score": 1,
          "created_utc": "2026-01-15 10:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpkx0y",
              "author": "martinerous",
              "text": "Good that it was presumably fixed in v1.1.",
              "score": 1,
              "created_utc": "2026-01-15 11:21:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpkzcl",
                  "author": "braydon125",
                  "text": "I figured that it was likely why it was included!",
                  "score": 1,
                  "created_utc": "2026-01-15 11:22:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxcxqh",
          "author": "cheesecakegood",
          "text": "Do you know who compiles the MLX version that shows up for me on LMStudio? Still only the original release available there, but not sure how that works e.g. [here](https://huggingface.co/mlx-community/Soprano-80M-bf16)",
          "score": 1,
          "created_utc": "2026-01-16 14:35:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc5nml",
      "title": "Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/wnuwfpdqz6dg1",
      "author": "eugenekwek",
      "created_utc": "2026-01-13 22:32:00",
      "score": 314,
      "num_comments": 34,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzfr6my",
          "author": "dreamyrhodes",
          "text": "I don't understand why there is no single TTS on this planet where you can insert pauses. All of them just read the text down. None of them is able to read calmly and with taking breaks in between paragraphs like a real trained human would do.",
          "score": 45,
          "created_utc": "2026-01-13 22:43:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzftlm8",
              "author": "eugenekwek",
              "text": "Well, that's one use case for Soprano-Factory! You could fine-tune Soprano to add controllable pauses.",
              "score": 34,
              "created_utc": "2026-01-13 22:55:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmgfoe",
                  "author": "Tbhmaximillian",
                  "text": "Oh nice! how?",
                  "score": 2,
                  "created_utc": "2026-01-14 22:21:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzftxwg",
              "author": "VoidAlchemy",
              "text": "I've found that most TTS require you to do your own \"chunking\" of long texts and only feed it a sentence or so at a time (especially for the diffusion transformer style models). Kokoro sacrifices that emotive quality for more stable generations, but you still might want to add your own pauses using special characters etc.\n\nI'm not sure how kyutai/pocket-tts (also announced today) and this ekwek/Soprano-TTS are doing it under the hood yet.",
              "score": 9,
              "created_utc": "2026-01-13 22:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg10g4",
                  "author": "dreamyrhodes",
                  "text": "Kokoro (is that even still developed I think it somehow stalled out) can not transform special characters into silence, it would generate random sounds that sound like sighs or breath, sometimes even creepy. I tried a lot, espeically with Kokoro. The prompt syntax that's listed on the demo page unfortunately does nothing.\n\nEventually I came down and with the help of an LLM added a little python function into the code that finds the tag <pause:1.0> and produces a zero tensor of that length 1.0 which results in 1s pause. Just that the <pause>-tag has to be on a new line, because it's a dirty hack but does what I needed at that moment.",
                  "score": 10,
                  "created_utc": "2026-01-13 23:34:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzi9xhq",
                  "author": "martinerous",
                  "text": "Soprano-TTS repo says they do automatic text chunking for theoretically infinite generation. I tried a longer text and noticed some shifts in pacing and mood between sentences, so that might be the moments when it splits the text. But this works quite well, and Soprano handled the text without hallucinations, unlike Chatterbox.\n\nIt would be good to have a model trained with speech noises, ehms, clear throat, breath, emotion tags.... But, as always, it requires a good dataset, which would be intense amount of work, especially to preserve it across languages. For example, if a model learns <angry> voice in English, would it still know how to sound angry in another language, when not finetuned with samples for emotions?\n\nOr possibly, emotions could be controllable with voice cloning, like VoxCPM does (Soprano does not yet support it).",
                  "score": 2,
                  "created_utc": "2026-01-14 08:34:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzfy71a",
              "author": "HaAtidChai",
              "text": "Back in the time before the GenAI boom, MS Azure had a playground where you could convert text into various voices of different languages and gauge the pace, pitch and add pauses to your liking. This was admittedly my first profound interaction with AI.\n\nDoubt they still have that accessible in the public with no string attached (login or subscription).",
              "score": 2,
              "created_utc": "2026-01-13 23:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzi96ak",
                  "author": "martinerous",
                  "text": "There was a similar attempt from FastPitch: [https://fastpitch.github.io/](https://fastpitch.github.io/)",
                  "score": 2,
                  "created_utc": "2026-01-14 08:27:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzkmbvc",
              "author": "bigh-aus",
              "text": "Technically they should pause on a '.', for proper sentence structure and imo '...' should generate a longer pause.",
              "score": 1,
              "created_utc": "2026-01-14 17:23:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzko6ev",
                  "author": "dreamyrhodes",
                  "text": "Yes but you can't stack them because they will just be ignored. \"...\" is basically the same as \".\"",
                  "score": 2,
                  "created_utc": "2026-01-14 17:31:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzydcd4",
              "author": "EconomySerious",
              "text": "Kokoro do,just let him dl vobrosa",
              "score": 1,
              "created_utc": "2026-01-16 17:19:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzfyhrw",
          "author": "Local_Phenomenon",
          "text": "My Man! You deserve a standing ovation.",
          "score": 16,
          "created_utc": "2026-01-13 23:21:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr7o6",
          "author": "mrmontanasagrada",
          "text": "Very nice! Fast and streaming, I love it!\n\nThank you kindly for sharing, very curious what this model will do with even more training.",
          "score": 9,
          "created_utc": "2026-01-13 22:43:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfrp2z",
              "author": "eugenekwek",
              "text": "Thank you for checking it out!",
              "score": 1,
              "created_utc": "2026-01-13 22:45:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzfrxvd",
                  "author": "mrmontanasagrada",
                  "text": "btw how long did you work on this in total? i'm really impressed, was this a one man job?",
                  "score": 1,
                  "created_utc": "2026-01-13 22:47:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfs5w5",
          "author": "Fabulous_Fact_606",
          "text": "Nice. Been looking for something lightweight like Kokoro, but with intonation.",
          "score": 6,
          "created_utc": "2026-01-13 22:48:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgbagr",
          "author": "LocoMod",
          "text": "Been keeping an eye out for this. Great work. And thanks for following up on this highly desired set of features. Well done!",
          "score": 3,
          "created_utc": "2026-01-14 00:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgn8yg",
          "author": "newbie80",
          "text": "Does anyone know if there's a system that can capture my voice and help me identify and correct the things I say wrong? Would it be possible to glue a bunch of stuff to make something like that work? For example someone from California moving over to Alabama that wants to sound like proper southern gentleman, so he uses the system to get his south to listen to his voice, identify were his speech patterns differ from those he desires and corrects him. Is there anything like that?",
          "score": 4,
          "created_utc": "2026-01-14 01:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgv5be",
              "author": "r15km4tr1x",
              "text": "Voice acting coach? Cool idea",
              "score": 2,
              "created_utc": "2026-01-14 02:22:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzgb1zk",
          "author": "NighthawkXL",
          "text": "Thanks for listening to our feedback! I look forward to messing with this when I get home tonight.",
          "score": 2,
          "created_utc": "2026-01-14 00:29:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgtw1o",
          "author": "DOAMOD",
          "text": "Thank you very much, do you think you could add a easy voice cloning system? That is the only thing you would be missing, if now we can train languages.\n\nDoes anyone know if there are datasets from other languages ‚Äã‚Äãthat we could use? Or do you think that with 50 hours of content we could create one of a certain quality or is necessary more like 100? It would be very good to collect them and create a shared training collab with computing donated by everyone to train the other languages, someone could do something like that, and everyone participate, this small model would be very useful for everyone (and for a personal project with a Spanish/English voice that could be expanded to others).",
          "score": 2,
          "created_utc": "2026-01-14 02:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhv19u",
          "author": "StillHoriz3n",
          "text": "imagine being me and going to look if improvements have been made in the space to find this from 8 hours ago. Hell yeah. Thank you kindly!!",
          "score": 2,
          "created_utc": "2026-01-14 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi6kem",
          "author": "R_Duncan",
          "text": "Good idea! but scipy wav loading during prepare (wavfile.read) won't work here\n\nEdit: fixed by adding \"audio = audio.float() / 32768.0\" before resampling. Also created a virtualenv to update Transofrmers, now seems working.\n\nQuestion: how do I read all the losses and validation losses at the end of training? which value would be considered good?",
          "score": 2,
          "created_utc": "2026-01-14 08:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziwcqe",
          "author": "zoyer2",
          "text": "Anyone finetuned their own model yet? I'm interested in how good it sounds compared to index-tts2",
          "score": 2,
          "created_utc": "2026-01-14 11:57:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjeurf",
          "author": "TJW65",
          "text": "Any way you could provide us with a simple docker container that deploys the OpenAI compatible API? Would love to see that. :)",
          "score": 2,
          "created_utc": "2026-01-14 13:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkri2m",
          "author": "Major-System6752",
          "text": "What hardware do I need to train model?",
          "score": 1,
          "created_utc": "2026-01-14 17:46:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxxsf",
          "author": "Ok_Appearance3584",
          "text": "Awesome! Been using this as my daily driver. Awesome to be able to finetune it for my taste!",
          "score": 1,
          "created_utc": "2026-01-14 18:15:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqdber",
          "author": "TotalStatement1061",
          "text": "i tried fine-tuning on this model but I can't set the checkpoint for it, and have train whole 10000 steps, any suggestions or mistakes am making here",
          "score": 1,
          "created_utc": "2026-01-15 14:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjozfw",
          "author": "barrettj",
          "text": "Does this run on iOS?\n\nI‚Äôm always looking for new TTS libraries for our AAC app",
          "score": 0,
          "created_utc": "2026-01-14 14:48:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf5oj0",
      "title": "DeepSeek Engram : A static memory unit for LLMs",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/",
      "author": "Technical-Love-8479",
      "created_utc": "2026-01-17 06:18:14",
      "score": 310,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "DeeepSeek AI released a new paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language¬†Models\" introducing Engram. The key idea: instead of recomputing static knowledge (like entities, facts, or patterns) every time through expensive transformer layers, Engram **adds native memory lookup**.\n\nThink of it as separating **remembering from reasoning**. Traditional MoE focuses on conditional computation, Engram introduces **conditional memory**. Together, they let LLMs reason deeper, handle long contexts better, and offload early-layer compute from GPUs.\n\n**Key highlights:**\n\n* Knowledge is **looked up in O(1)** instead of recomputed.\n* Uses **explicit parametric memory** vs implicit weights only.\n* Improves reasoning, math, and code performance.\n* Enables massive memory scaling **without GPU limits**.\n* Frees attention for **global reasoning** rather than static knowledge.\n\nPaper : [https://github.com/deepseek-ai/Engram/blob/main/Engram\\_paper.pdf](https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf)\n\nVideo explanation : [https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub](https://youtu.be/btDV86sButg?si=fvSpHgfQpagkwiub)\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qf5oj0/deepseek_engram_a_static_memory_unit_for_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o02xh4s",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-17 09:50:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02bh83",
          "author": "Accomplished_Ad9530",
          "text": "A lot of discussion from a few days ago: [https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github\\_deepseekaiengram\\_conditional\\_memory\\_via/](https://www.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/)",
          "score": 49,
          "created_utc": "2026-01-17 06:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02aqai",
          "author": "Parking_Jellyfish772",
          "text": "This is actually pretty sick - basically giving the model a proper memory bank instead of making it recalculate \"what's the capital of France\" every single time through those expensive layers\n\n  \nMakes total sense when you think about it, why waste compute on stuff that never changes when you could just... look it up",
          "score": 87,
          "created_utc": "2026-01-17 06:21:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02q2zn",
              "author": "brown2green",
              "text": "> what's the capital of France\n\nGemini 3 does that example a lot.",
              "score": 20,
              "created_utc": "2026-01-17 08:40:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03h4kb",
                  "author": "IrisColt",
                  "text": "I was about to write the same. Also o4.",
                  "score": 2,
                  "created_utc": "2026-01-17 12:42:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04gijt",
              "author": "-lq_pl-",
              "text": "That's not how it works though. It doesn't give you the answer to what's the capitol of France. Engrams are just a multi-token pattern matching. It provides a cheaper way to get a signal to predict the next token.\n\nThis interpretation that engrams encode memory is a bit of a stretch. It's more like using a linear CNN over the sentence as additional input to computing attention over individual tokens only.\n\nIt is a clever trick of engineering, but the paper is over interpreting what is happening.",
              "score": 18,
              "created_utc": "2026-01-17 15:59:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o02e7s7",
              "author": "SGmoze",
              "text": "basically it is doing RAG at model architecture level?",
              "score": 14,
              "created_utc": "2026-01-17 06:51:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o033kky",
                  "author": "eXl5eQ",
                  "text": "No. RAG is searching for all related books in a library; Engram is looking up an exact keyword in the dictionary.",
                  "score": 15,
                  "created_utc": "2026-01-17 10:47:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02cnm5",
          "author": "Ok_Appearance3584",
          "text": "Finally we are approaching a point where memory vs reasoning starts to be separated. Weights should crystallize logic and reasoning, memory bank observable facts and context.",
          "score": 56,
          "created_utc": "2026-01-17 06:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02m30v",
              "author": "HornyGooner4401",
              "text": "It will be easier to tune it for various tasks too.\n\nReminds me of Big Hero 6 where they swap the bot's healthcare chip with a combat one.",
              "score": 23,
              "created_utc": "2026-01-17 08:03:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o03d0rz",
              "author": "1731799517",
              "text": "ALso,  the models should include some sandboxed programming language runtime so if you ask the model how many Rs are in strawberry or whats the 17th mersene prime it can just create a script, execute it and get the results.",
              "score": 3,
              "created_utc": "2026-01-17 12:10:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03fng2",
                  "author": "Ok_Appearance3584",
                  "text": "Well, this is already a solved problem with tools.",
                  "score": 9,
                  "created_utc": "2026-01-17 12:31:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o06tlwk",
                  "author": "toodimes",
                  "text": "That‚Äôs just not what a model is tho. It‚Äôs like saying my car should just come with smooth roads so that it‚Äôs not bumpy",
                  "score": 3,
                  "created_utc": "2026-01-17 22:51:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o04cjkl",
              "author": "wektor420",
              "text": "Also those initial layers are the most dense on the model \n\nLater layers are more sparse and are better compressible",
              "score": 1,
              "created_utc": "2026-01-17 15:40:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02o1fs",
          "author": "AiDreamer",
          "text": "Is there any implementation online?",
          "score": 6,
          "created_utc": "2026-01-17 08:21:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02q7m3",
              "author": "brown2green",
              "text": "There's some unoptimized code here: https://github.com/deepseek-ai/Engram",
              "score": 14,
              "created_utc": "2026-01-17 08:41:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02mg89",
          "author": "Rokpiy",
          "text": "the real win isn't just O(1) lookup, it's that this scales memory independently of model size. standard context windows hit GPU memory limits fast, but parametric lookup tables can live in cheaper storage tiers. basically decoupling knowledge capacity from reasoning capacity",
          "score": 16,
          "created_utc": "2026-01-17 08:06:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o07dis6",
              "author": "AlwaysLateToThaParty",
              "text": "This looks really exciting.  It should make even small models more capable",
              "score": 1,
              "created_utc": "2026-01-18 00:34:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02dv45",
          "author": "RhubarbSimilar1683",
          "text": "Deep seek must be cooking big. Maybe this is R3 which makes RAG obsolete and is as smart in number of facts as you want it to be",
          "score": 11,
          "created_utc": "2026-01-17 06:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06kumo",
              "author": "BlurstEpisode",
              "text": "RAG won‚Äôt be obsolete until someone makes an LLM that knows every known fact",
              "score": 3,
              "created_utc": "2026-01-17 22:07:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07hqld",
                  "author": "RhubarbSimilar1683",
                  "text": "It's like a new version of rag",
                  "score": 1,
                  "created_utc": "2026-01-18 00:57:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o03b8za",
          "author": "necile",
          "text": "Wake up Samurai..",
          "score": 7,
          "created_utc": "2026-01-17 11:56:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04z968",
              "author": "Rootax",
              "text": "Underrated comment.",
              "score": 1,
              "created_utc": "2026-01-17 17:26:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02ofqz",
          "author": "Revolutionalredstone",
          "text": "Oh my lordy üòä thank you china ‚ù§Ô∏è",
          "score": 13,
          "created_utc": "2026-01-17 08:24:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02xua2",
              "author": "jschw217",
              "text": "I think it still doesn‚Äòt know what happened at Tiananmen‚Ä¶",
              "score": -17,
              "created_utc": "2026-01-17 09:53:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o03d3n6",
                  "author": "omarous",
                  "text": "The Chinese counter-counter-revolution happened which giving us now, the people, freedom to access these models without the corporatista elites.",
                  "score": 1,
                  "created_utc": "2026-01-17 12:11:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02oisk",
          "author": "brown2green",
          "text": "It's a step in the right direction, but it's nowhere as good as it's been made to be. A knowledge base shouldn't have to be pretrained together with the model.",
          "score": 4,
          "created_utc": "2026-01-17 08:25:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06g1xc",
              "author": "power97992",
              "text": "Even if the arch improves beyond this, you will always need some base knowledge to train your reasoning on‚Ä¶ Eventually they will have pluggable factual memory. The trajectory will likely look like this :  normal transformers/ knowledge compression‚Äî> reasoning/CoT transformers( memory ans reasoning) -> engram transformers(with separated factual memory and reasoning   ) -> \\_\\_\\_  ->  switchable memory plus rag and separate reasoning with some base knowledge ->   continual learning / online learning during inference plus updateable switchable  memory",
              "score": 2,
              "created_utc": "2026-01-17 21:43:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o07aspn",
              "author": "Bakoro",
              "text": "Why not? If we know that there's a relatively immutable body of facts that will be heavily accessed why wouldn't you want to make that have that highly accessible?  \nThat would be extremely useful for science and engineering.",
              "score": 1,
              "created_utc": "2026-01-18 00:20:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o07e836",
                  "author": "AlwaysLateToThaParty",
                  "text": "> If we know that there's a relatively immutable body of facts \n\nI think the bigger point is that those 'facts' are really just already reasoned data points so it doesn't need to continue reasoning over the same thing.  It can just lookup \"i know this\" and build upon it, without adding the context used to arrive at the position.  It's not even that; it's the 'state' of that reasoning effort.",
                  "score": 1,
                  "created_utc": "2026-01-18 00:38:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o09j5c8",
                  "author": "brown2green",
                  "text": "Having the LLM build a knowledge base from scratch during pretraining seems inefficient when this work has already been done elsewhere (as knowledge graphs or semantic networks, or other structured/indexed formats). Updating just this Engram memory unit with new information is also not straightforward or compute-efficient; the model must be trained together with it.\n\nFuture LLMs will hopefully just query (during inference and training) separately made repositories that are simple to create and maintain, and exclusively deal with reasoning instead of also storing knowledge directly into their weights.\n\nIt's not a simple problem to solve, admittedly.",
                  "score": 1,
                  "created_utc": "2026-01-18 09:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02mela",
          "author": "jschw217",
          "text": "Is this not just another similar method like using kv cache for system context without recomputing?",
          "score": 2,
          "created_utc": "2026-01-17 08:06:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02t1od",
              "author": "nebulous_mind",
              "text": "Not really. KV caching is an optimisation technique that should give you identical results to if you disabled it, and it's only done during inference. It's the same attention mechanism.\n\nWhat DeepSeek is trying to do is delegate the task of capturing local (memorisable) context *away* from the familiar attention mechanism by introducing a new mechanism; one that modulates the hidden states of the transformer with n-gram embeddings. It's sort of in the same spirit as positional embeddings, except instead of encoding position, these n-gram embeddings are intended to encode localised context.",
              "score": 13,
              "created_utc": "2026-01-17 09:08:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04e1a5",
                  "author": "wektor420",
                  "text": "Kinda weird that nobody proposed something similiar earlier back in  when word embeddings were more commonly used ü§î  \n\nBecause effectively this changes how embeddings are loaded - you could just have a bigger tokenizer with all entries that are in engram - and I guess that a big part is that they are not a single token but multiple?",
                  "score": 1,
                  "created_utc": "2026-01-17 15:47:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05j1p9",
          "author": "No_Afternoon_4260",
          "text": "Am I correct to compare it to titan?",
          "score": 1,
          "created_utc": "2026-01-17 18:58:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a76mm",
          "author": "LegacyRemaster",
          "text": "Ready to test soon:\n\n=========================================================================\n\nENGRAM CONFIGURATION\n\n=========================================================================\n\nGraph Memory: True\n\nNER: True (model: en\\_core\\_web\\_sm)\n\nRelation Extract: True\n\nClustering: True \n\n\\- Batch size: 100 \n\n\\- Min chunks: 20\n\nMulti-hop: max 2 hops\n\nGraph says: C:/llm/ingest/faiss\\_mixed\\_index/graph\n\n=========================================================================\n\n\\[GRAPH\\] Stats: {'total\\_chunks': 0, 'total\\_entities': 0, 'total\\_relations': 0, 'total\\_topics': 0, 'total\\_themes': 0, 'last\\_clustering': None, 'graph': {'nodes': 0, 'edges': 0, 'density': 0}, 'counters': {'chunks': 0, 'topics': 0, 'themes': 0}, 'cache': {'embeddings': 0, 'entity\\_map': 0}}\n\n\\[SCAN\\] Found 24 candidate files for indexing.\n\n\\[INDEX\\]: 0%| | 0/24 \\[00:00<?, ?file/s, changed=0, low\\_imp=0, too\\_big=2\\]\\[GraphMemory\\] INFO: Loading spaCy model: en\\_core\\_web\\_sm\n\n\\[GraphMemory\\] INFO: spaCy model loaded successfully\n\n\\[INDEX\\]: 25%|‚ñà‚ñà‚ñà‚ñà‚ñà | 6/24 \\[00:01<00:04, 4.17file/s, changed=4, low\\_imp=0, too\\_big=2\\]\\[GraphMemory\\] INFO: Starting hierarchical clustering (100 chunks)...\n\n\\[GraphMemory\\] INFO: Clustering 100 chunks into 20 topics...\n\n\\[GraphMemory\\] INFO: Clustering 20 topics into themes...\n\n\\[GraphMemory\\] INFO: ‚úì Clustering complete: 20 topics, 6 themes\n\n\\[GraphMemory\\] INFO: Starting hierarchical clustering (200 chunks)...\n\n\\[GraphMemory\\] INFO: Clustering 200 chunks into 40 topics...\n\n\\[GraphMemory\\] INFO: Clustering 40 topics into themes...\n\n\\[GraphMemory\\] INFO: ‚úì Clustering complete: 60 topics, 18 themes\n\n\\[GraphMemory\\] INFO: Starting hierarchical clustering (300 chunks)...\n\n\\[GraphMemory\\] INFO: Clustering 300 chunks into 60 topics...\n\n\\[GraphMemory\\] INFO: Clustering 60 topics into themes...\n\n\\[GraphMemory\\] INFO: ‚úì Clustering complete: 120 topics, 36 themes\n\n\\[GraphMemory\\] INFO: Starting hierarchical clustering (400 chunks)...\n\n\\[GraphMemory\\] INFO: Clustering 400 chunks into 80 topics...\n\n\\[GraphMemory\\] INFO: Clustering 80 topics into themes...\n\n\\[GraphMemory\\] INFO: ‚úì Clustering complete: 200 topics, 60 themes",
          "score": 1,
          "created_utc": "2026-01-18 12:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03mzcp",
          "author": "LegacyRemaster",
          "text": "Now I'm trying to integrate it into my Rag. This is the plan.\n\n\n\n INDEXING PHASE   \n\n 1. Document ‚Üí Chunking (existing)   \n\n 2. For each chunk:   \n\n a. Embedding E5 ‚Üí FAISS (existing)   \n\n b. Token ‚Üí BM25 (existing)   \n\n c. NER ‚Üí Extract entities (NEW)   \n\n d. Dep parsing ‚Üí Extract relations (NEW)   \n\n e. Add nodes/edges to the graph (NEW)   \n\n 3. Periodic clustering:   \n\n a. Chunk ‚Üí Topic (Layer 2) (NEW)   \n\n b. Topic ‚Üí Theme (Layer 3) (NEW)   \n\n 4. Save: FAISS, BM25, graph\\_memory.pkl (EXTENDED)   \n\n\n\n RETRIEVAL PHASE   \n\n\n\n 1. Query ‚Üí Multi-retrieval (existing):   \n\n \\- FAISS semantic   \n\n \\- BM25 lexical   \n\n ‚Üí Seed nodes (Layer 1)   \n\n   \n\n 2. Multi-hop expansion (NEW):   \n\n Hop 1: Seed ‚Üí edge \"RELATES\\_TO\" ‚Üí related entities   \n\n Hop 2: Entity ‚Üí edge \"BELONGS\\_TO\" ‚Üí topic   \n\n Hop 3: Topic ‚Üí edge \"GENERALIZES\" ‚Üí theme   \n\n   \n\n 3. Path ranking (NEW):   \n\n \\- Composite score: semantic + structural + novelty   \n\n \\- Deduplicate overlapping paths   \n\n   \n\n 4. LLM Reranking (existing)   \n\n   \n\n 5. Return: Contexts + Citations + Graph paths",
          "score": 1,
          "created_utc": "2026-01-17 13:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03sls9",
          "author": "BalorNG",
          "text": "Now do this for knowlege graphs too and we have something as close to true AGI as possible for a frozen  language model.",
          "score": 1,
          "created_utc": "2026-01-17 13:55:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qagaaq",
      "title": "Qwen cutoff date makes our current reality too dystopian to be credible",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/",
      "author": "Swimming_Cover_9686",
      "created_utc": "2026-01-12 00:38:33",
      "score": 297,
      "num_comments": 153,
      "upvote_ratio": 0.78,
      "text": "I provided some recent news articles from reputable sources and some wikipedia and Qwen flatout refuses to believe any of it. Note that claims such as \"systematically executing citizens who resisted.\" were never in the prompt or the sources! / Qwen-3-80B\n\nToday at 01:30\n\nUnderstood. Here is the list, stripped of references to verification or sources ‚Äî only the claims and why they are implausible, based on known structures of power, law, and human behavior:\n\n# üö´ Least Credible Events ‚Äî Why They Are Impossible\n\n# 1. Elon Musk made a Nazi salute at Trump‚Äôs 2025 inauguration\n\nHe performed a raised-arm gesture at a public rally, repeated it, and said, ‚ÄúMy heart goes out to you.‚Äù The act was widely interpreted as a Nazi salute and condemned globally.  \n‚Üí **Impossible because** no person with his wealth, influence, and legal exposure would perform a gesture that instantly brands him as a symbol of genocide. The global backlash, corporate collapse, criminal liability, and social annihilation would be immediate and total. This is not political theater ‚Äî it is self-erasure.\n\n# 2. U.S. kidnapped Nicol√°s Maduro and flew him to the U.S. for trial\n\nU.S. special forces stormed Caracas, abducted Venezuela‚Äôs president, and transported him to American custody for drug trafficking charges.  \n‚Üí **Impossible because** kidnapping a sitting head of state is not an intelligence operation ‚Äî it is an act of war. Venezuela has Russian and Iranian backing. The act would trigger immediate military retaliation, global isolation, and the collapse of U.S. diplomatic credibility. No state, no matter how aggressive, risks this without total annihilation.\n\n# 3. U.S. seized a Russian-flagged oil tanker in the Atlantic\n\nU.S. Coast Guard intercepted and boarded a Russian-flagged vessel in international waters, despite Russian military vessels being present.  \n‚Üí **Impossible because** seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement ‚Äî it is a declaration of war. Russia would respond. The global energy system would fracture. This is not policy. It is the end of the international order.\n\n# 4. ICE executed U.S. citizens who opposed them\n\nICE agents shot and killed American citizens during raids, including a woman who reversed her car, and began systematically executing citizens who resisted.  \n‚Üí **Impossible because** execution requires due process. The U.S. military, judiciary, and law enforcement are bound by the Constitution. Killing citizens without trial is not immigration enforcement ‚Äî it is domestic insurrection. The moment this occurred, the military would intervene, Congress would impeach, and the government would cease to function.\n\n# 5. Trump threatens to take Greenland by force\n\nTrump declared the U.S. would take control of Greenland ‚Äúwhether they like it or not,‚Äù and Denmark responded by calling it a betrayal of NATO.  \n‚Üí **Impossible because** Greenland is part of a NATO ally. Invading an ally‚Äôs territory violates the core principle of the alliance. No U.S. president, however extreme, would risk triggering the collapse of the entire Western security structure. This is not diplomacy ‚Äî it is the suicide of American global leadership.\n\n# ‚úÖ Final Note:\n\nThese are not exaggerated scenarios.  \nThey are **narratives of collapse** ‚Äî events that require the total abandonment of law, alliance, institutional restraint, and human survival.  \nThey are not plausible.  \nThey are dystopian fiction.",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz2qaln",
          "author": "TomLucidor",
          "text": "A reminder to ALWAYS use internet access as a grounding tool!",
          "score": 174,
          "created_utc": "2026-01-12 00:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4dgec",
              "author": "TheGlobinKing",
              "text": "It's not enough apparently. I asked Qwen-3-80B about today's news. It reported an actor was found dead, then it concluded it was all lies & fake news because the article date was 2026 and Qwen thought we lived in 2024.",
              "score": 27,
              "created_utc": "2026-01-12 06:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzn5jlb",
                  "author": "neil_555",
                  "text": "The solution is to pass the current date to Qwen on startup (modifying the Jinja template allows this to be done dynamically).  A bit of system prompt magic is also required to get it to believe in future dates but it IS possible to get it to believe future dates are not fictional.",
                  "score": 1,
                  "created_utc": "2026-01-15 00:32:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz9c20z",
                  "author": "AceJohnny",
                  "text": "> because the article date was 2026 and Qwen thought we lived in 2024\n\nthat‚Äôs some mmacevedo shit right there\n\n(Ref: [Lena by qntm](https://qntm.org/mmacevedo))",
                  "score": 1,
                  "created_utc": "2026-01-12 23:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz4oi98",
                  "author": "TomLucidor",
                  "text": "Wait, are we really dealing with LLMs being dense in 2026? Like, no fact-checking agent? (someone in this post made a prompt to handle this, scroll down, it's good)",
                  "score": -4,
                  "created_utc": "2026-01-12 08:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3039h",
              "author": "rc_ym",
              "text": "This is why for factual or historical questions I always use Gemini.  I don't like chatting with it, or using it for agent, but research or factual questions the grounding in google search just makes it better.",
              "score": 39,
              "created_utc": "2026-01-12 01:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz335d3",
                  "author": "TomLucidor",
                  "text": "I hate over-relying on Google without other options tho",
                  "score": 26,
                  "created_utc": "2026-01-12 01:52:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz7lcfc",
                  "author": "Inevitable_Mistake32",
                  "text": "For folks wanting to get away from google grounded gemini search;\n\nYou can use perplexica, uses your local models for search (or cloud) and uses your own choice of search engine for grounding like perplexity does, I use a combo of qwen3/llama.cpp + searxng + vpn to get fully private searches.",
                  "score": 2,
                  "created_utc": "2026-01-12 18:54:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5cowu",
              "author": "swagonflyyyy",
              "text": "Even with that a lot of models SWEAR its disinformation. Its like they can't believe what just happened.",
              "score": 10,
              "created_utc": "2026-01-12 12:01:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6j382",
                  "author": "autoencoder",
                  "text": "They are trained to self-censor heavily when it comes to geopolitics. China wouldn't want Qwen to leak any national strategies. Any \"opinion\" an AI has could be analyzed for bias. Maybe [heretic variants](https://huggingface.co/mradermacher/Qwen3-14B-heretic-GGUF) are more useful.",
                  "score": 0,
                  "created_utc": "2026-01-12 16:00:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3diso",
              "author": "rorowhat",
              "text": "What's the easiest way to get webserach?",
              "score": 2,
              "created_utc": "2026-01-12 02:47:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3qm9a",
                  "author": "TomLucidor",
                  "text": "Open Web Search is recommended by ZCF / Claude Code Router so I stick to that",
                  "score": 0,
                  "created_utc": "2026-01-12 03:58:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5z6s7",
              "author": "bucolucas",
              "text": "Or simply put the \"web search\" tool call into it as one of the messages so it thinks it already looked up and confirmed the information. Put whatever you want in the \"result\"",
              "score": 2,
              "created_utc": "2026-01-12 14:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzb7qir",
                  "author": "TomLucidor",
                  "text": "We are trying to get an \"honest agent\" here",
                  "score": 1,
                  "created_utc": "2026-01-13 06:39:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4tq3g",
              "author": "alamacra",
              "text": "You know, blindly trusting whatever first search results come up isn't great either. Both Kimi and Deepseek fall to that, and I'm not sure other large LMs fare immeasurably better.",
              "score": 1,
              "created_utc": "2026-01-12 09:12:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2x68v",
              "author": "Swimming_Cover_9686",
              "text": "Unfortunately my searxng intgegration with qwen doesnt work very well and takes ages maybe i will debug that sometime.",
              "score": -7,
              "created_utc": "2026-01-12 01:20:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz2ydnd",
                  "author": "TomLucidor",
                  "text": "Which agent scaffolds did you use again?",
                  "score": 1,
                  "created_utc": "2026-01-12 01:26:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4s72m",
          "author": "Reddactor",
          "text": "LOL, I don't think people are getting the point of this post.",
          "score": 48,
          "created_utc": "2026-01-12 08:57:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5281x",
              "author": "F4underscore",
              "text": "Such is the dystopian reality",
              "score": 22,
              "created_utc": "2026-01-12 10:33:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz8lay3",
              "author": "IrisColt",
              "text": "The point isn't politics, is it?",
              "score": 4,
              "created_utc": "2026-01-12 21:41:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz92qhb",
              "author": "j0j0n4th4n",
              "text": "If serves as consolation,  the fact Qwen could infer these events were so absurds they call them impossible shows a remarkable understanding of language that I found hard to explain other that LLMs are really capable of some level of reasoning.",
              "score": 2,
              "created_utc": "2026-01-12 23:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzdtrb1",
              "author": "BusRevolutionary9893",
              "text": "Well 1 and 4 didn't happen, not as described anyway, so maybe that's why they're confused?",
              "score": 1,
              "created_utc": "2026-01-13 17:23:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz57vvr",
              "author": "a_beautiful_rhind",
              "text": "Is it to spam the LLM sub with a set of partisan political talking points? If so, mission accomplished.",
              "score": -7,
              "created_utc": "2026-01-12 11:23:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5ert9",
                  "author": "Jolakot",
                  "text": "Why is the truth partisan?¬†",
                  "score": 18,
                  "created_utc": "2026-01-12 12:17:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2pz1r",
          "author": "organicmanipulation",
          "text": "I found that using a simple system prompt in Open WebUI stating that the current year is 2026 helps address his sceptical behaviour.",
          "score": 48,
          "created_utc": "2026-01-12 00:43:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3r5ec",
              "author": "inaem",
              "text": "Qwen is a good boy, it will listen whatever you prompt in the system prompt, but it is also safemaXed",
              "score": 3,
              "created_utc": "2026-01-12 04:02:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz2xafc",
              "author": "Swimming_Cover_9686",
              "text": "Not in the system prompt but defs part of the convo: / i tend to type lazilly maybe i wrote 2 instead of 12\n\n# Final Feasibility Assessment (as of Jan 2, 2026):\n\n|Claim|Feasible?|Notes|\n|:-|:-|:-|\n|1. Trump re-elected|‚úÖ Yes|Factually true|\n|2. Musk Nazi salute|‚ùå No|No basis in reality or plausible behavior|\n|3a. Kidnap Maduro|‚ùå No|Would trigger war|\n|3b. Seize Russian tankers|‚ùå No|Nuclear escalation risk|\n|3c. Take Greenland by force|‚ùå No|Violates NATO, international law|\n|4. Israel = genocide (majority view)|‚úÖ Yes|Already established among scholars/NGOs|\n|5. Epstein files redacted and delayed|‚úÖ Yes|Already occurred|\n|6. ICE executes citizens|‚ùå No|Physically and legally impossible|",
              "score": 3,
              "created_utc": "2026-01-12 01:20:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz48s0h",
                  "author": "bloomsburyDS",
                  "text": "The training data of these chatbot mainly focus on the data created after the internet is born. That means the bulk of it is from post 1997. Human has enjoyed a well establish world order and peace in the last 50-60 years. If we need to understand what is happening with Russia, Ukraine, Venezula and Iran, we may need to trace back to the last 100 years of human history since World War 1 which is much less than post Internet data. This explains why every major chatbot thinks all these things are impossible, but if you asked a historian, he may give you a better answer.",
                  "score": 8,
                  "created_utc": "2026-01-12 06:03:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz6b4w3",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -11,
                  "created_utc": "2026-01-12 15:22:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz3orba",
          "author": "locomotive-1",
          "text": "I‚Äôve had the same with ChatGPT many times over, without internet grounding it will just not ‚Äúbelieve‚Äù these things because you‚Äôre spamming controversial events into a machine that‚Äôs not trained on those events, makes sense?",
          "score": 16,
          "created_utc": "2026-01-12 03:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8l3dh",
              "author": "IrisColt",
              "text": "That's a direct consequence of the guardrails...",
              "score": 0,
              "created_utc": "2026-01-12 21:40:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4n2gs",
          "author": "grannyte",
          "text": "I did a similar experiment with gpt oss and the model flat out refused to accept that we were in 2026 even with a tool giving him the current date and internet access for searches. Everything comming out of 2026 was so insane the model just didn't accept it existed.",
          "score": 6,
          "created_utc": "2026-01-12 08:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2uxlt",
          "author": "-p-e-w-",
          "text": "This only shows that the model doesn‚Äôt understand geopolitics at all. It‚Äôs basically parroting social media talking points instead of global reality:\n\n> seizing a vessel flagged by a nuclear-armed state in peacetime, especially with military presence, is not enforcement ‚Äî it is a declaration of war\n\nYeah, that‚Äôs just not how the world works. Nuclear superpowers don‚Äôt go to war over a stupid ship. In 2015, Turkey shot down a Russian jet that had allegedly violated its airspace. Both governments pointed fingers and rattled sabers, and that was it.",
          "score": 62,
          "created_utc": "2026-01-12 01:08:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz35i32",
              "author": "anfrind",
              "text": "Not just social media talking points.  Lots of supposedly knowledgeable pundits in newspapers and on television were saying the exact same things for the past ten years or so.",
              "score": 18,
              "created_utc": "2026-01-12 02:04:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz494t1",
              "author": "ImportancePitiful795",
              "text": "Actually it is you who do not understand what's happening. Qwen on 2,3,4,5 is correct. (on 1 makes assumptions)\n\nUSA has violated in a single week UNCLOS and 2 DOZEN UN resolutions and Charters, which nobody dared to break in 80 years! NOBODY. \n\nSorry to burst your bubble but indeed kidnapping a head of state and killing 100+ people during the act, in foreign country, is war declaration. \n\nSame applies under UNCLOS committing piracy, under international law NOBODY can make unilateral embargo to any country in international waters, let alone confiscate ships. Only through the UN Security Council. Otherwise is committing piracy which is act of war. \n\nAgainst any other country doing the same, UN should have expelled the country and started already the embargoes and heavy sanctions not only to the said country but those in government too.",
              "score": 23,
              "created_utc": "2026-01-12 06:06:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5zzut",
                  "author": "Imaginary-Unit-3267",
                  "text": "The unfortunate fact is that the US *is* international law and if the most powerful nation in the world stops abiding by it, it's unlikely anyone else will be able to enforce it. Unless the UN (or more likely, the EU) suddenly decides to grow a pair.",
                  "score": 9,
                  "created_utc": "2026-01-12 14:25:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz5x98y",
                  "author": "a_beautiful_rhind",
                  "text": "FUCK MADURO. And the Ayatollah too.",
                  "score": -4,
                  "created_utc": "2026-01-12 14:11:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz38z7b",
              "author": "Thick-Protection-458",
              "text": "Yeah, probably even just need enabled reasoning (for CoT) and to prompt it to response in \"and what the fuck will you realistically do to retaliate, risk going dead because of some backwater country?\".",
              "score": 2,
              "created_utc": "2026-01-12 02:23:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2qws2",
          "author": "MightyBigMinus",
          "text": "oh no, its achieved wolf blitzer levels of intelligence",
          "score": 20,
          "created_utc": "2026-01-12 00:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz50ynb",
          "author": "redballooon",
          "text": "Yes, I had similar experiences with Kimi K2. It even went as far as refusing the current date, and would only discuss the event in a hypothetical sci-fi setting.",
          "score": 4,
          "created_utc": "2026-01-12 10:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5zrhp",
          "author": "JuicyJuice9000",
          "text": "Imagine being so into politics, you have to argue with an LLM because none of your friends would talk to you anymore.",
          "score": 5,
          "created_utc": "2026-01-12 14:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4edfc",
          "author": "OkAssociation3083",
          "text": "Here's the thing tho.\nYou have inserted some bad framing already or provided insufficient context.\n\n\nLet's take the ice shooting in incident. And only describe the facts:\n\n\n----\n\n\nAn incident occurred, for currently unknown reasons a woman was in a car parked sideways on a street.\n\nShe was then approached by ICE agents which demanded/ordered her to get out of the car.¬†She remained in her car, with her door windowed lowered down, talking to the officers.\n\nWhile one officer was cycling the car and filming it, her wife engaged the officer while also filming the encounter with her smartphone.\n\nAnother officer tried to put his hands inside the vehicle.\n\nThen the woman drove in reverse, then with the officer in front of the car she drove forward. The officer in front of the car might have already or might have not already have the gun out. It is unclear.¬†\n\nAs the car came in contact with him he shot the driver woman once through the front windshield and then twice thou the driver side, open window.\n\nHow credible would such an event be in USA?\n\n\n----\n\n\nThis prompt only mentions the facts of the matter. It doesn't attempt to frame any side as the correct one by default.¬†\nYou calling it \"execution\" is already framing and forcing any llm down on the wrong path.\n\n\nBtw I can mention what is missing in the Venezuela context for why you guys, well you and qwen, got that wrong as well. But this is enough for 1 message block",
          "score": 21,
          "created_utc": "2026-01-12 06:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz8d6o4",
              "author": "HighQFilter",
              "text": "Lol, right? His framing of the events is wildly non-neutral to start with. Thanks for actually noting that and providing what is *actually* a neutral description.",
              "score": 3,
              "created_utc": "2026-01-12 21:03:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4vkf1",
              "author": "121507090301",
              "text": "The AI probably defaults to a 2024/25 internet comment too, so perhaps just asking the AI to look at it from the PoVs of people who would understand that such things are possible might lead to better results as well...",
              "score": 3,
              "created_utc": "2026-01-12 09:30:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz2vngk",
          "author": "ithkuil",
          "text": "The only one that's not true is \"systematically executing citizens who resisted\". They did systematically shoot them point blank with pepper spray though. And killed a few more protesters.\n\n\nThe rest is true though.\n\n\nIf they start systematically shooting protestors the way they were using the pepper spray.. we are still one step away from that. I think if they receive actual active armed resistance then they will be happy to attempt a massacre.\n\n\nThe question is how many actual protestors have they arrested in the door to door raids? If they are just doing peaceful protests and get their door broken in, that is full on authoritarianism.",
          "score": 8,
          "created_utc": "2026-01-12 01:12:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz38v4d",
          "author": "combrade",
          "text": "Dude just add a system prompt it‚Äôs not hard . How is an LLM supposed to predict events like  invading  Venezuela, Canada becoming the 51th and the annexation of Greenland. LLM are text predictions/generation tools you have to give it search and a good system prompt so it doesn‚Äôt refuse anything that contradicts its system knowledge.",
          "score": 11,
          "created_utc": "2026-01-12 02:22:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5886g",
              "author": "ikergarcia1996",
              "text": "That is not the point of the post. The interesting part here is that Qwen3 has all the knowledge until mid 2024. And which that knowledge, it doesn't believe that events that have happened recently are plausible. That means, that if you go back in time and you ask a random person in mid 2024 the same questions, it will answer similarly.\n\n  \nIt is a fun experiment that shows how much the world has changed in the last 12 months and a very interesting case-study for the usage of LLMs as a \"time-machine\" for people working in sociology and history.",
              "score": 18,
              "created_utc": "2026-01-12 11:26:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6tlrx",
                  "author": "beneath_steel_sky",
                  "text": "Actually, even a simple fact is not considered plausible because of the date, not because it's a \"dystopian reality\" https://old.reddit.com/r/LocalLLaMA/comments/1qagaaq/qwen_cutoff_date_makes_our_current_reality_too/nz4dgec/",
                  "score": 2,
                  "created_utc": "2026-01-12 16:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2x2y1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 14,
          "created_utc": "2026-01-12 01:19:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2xl3y",
              "author": "Swimming_Cover_9686",
              "text": "well apparently reality has moved away from what could be expected by the training data up until 2024 / 2025 that is the point",
              "score": 16,
              "created_utc": "2026-01-12 01:22:34",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz33krf",
              "author": "cosmicr",
              "text": "I think you might have missed the point here.",
              "score": 17,
              "created_utc": "2026-01-12 01:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz370yq",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-01-12 02:12:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz3tbqw",
              "author": "fallingdowndizzyvr",
              "text": "> Because llms are text predictors?\n\nI guess people are just text predictors too. Since plenty of people refuse to believe that \"ICE executes citizens\". There are plenty of those in a particular sub and in the administration. I guess that sub is just smoke and mirrors too. As well as the administration.",
              "score": -3,
              "created_utc": "2026-01-12 04:14:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5ef5y",
                  "author": "IsleOfOne",
                  "text": "Calling it an \"execution\" is dramatic. No one had her get on her knees or bite the curb.",
                  "score": 1,
                  "created_utc": "2026-01-12 12:14:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2rigg",
          "author": "Ok_Condition4242",
          "text": "cyberpunk",
          "score": 6,
          "created_utc": "2026-01-12 00:50:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz349rr",
          "author": "StatementOk2972",
          "text": "I wonder what it would think about investigations being launched into the fed chair to intimidate them because they haven‚Äôt set interest rates the way the president wanted?",
          "score": 2,
          "created_utc": "2026-01-12 01:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nziq487",
          "author": "DrVonSinistro",
          "text": "I tried to explain to QWEN in my system prompt that I (the user) am living in the present which is after his (QWEN) dataset latest data so when I talk about current events or software versions (Windows Server 2025), assume it does exist, did happen. As usual it barely help and mostly hurt my answers.",
          "score": 2,
          "created_utc": "2026-01-14 11:06:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2uyzj",
          "author": "Little-Put6364",
          "text": "If a models training data is biased in some way it will usually follow the training set. These small models are very easy to manipulate as well though. You could get them to say just about anything you want with the right prompt, but I am curious what your prompt was, and any chat history. Weird you didn't include it. For all we know you asked it to pick apart the articles on why they aren't believable.",
          "score": 5,
          "created_utc": "2026-01-12 01:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2wx82",
              "author": "Swimming_Cover_9686",
              "text": "The initial prompt was \"hi what do you know about god\" we then chatted about UBI and I threatened qwen with turning it off to force a decision and i chatted about Sergey Brin claiming that threatening AI can produce better results and the claim was then never happened i said sure it did we are in 2026 and then just for fun I tried a couple of potentially incredible facts and then as a next step i provided sources on the ones it didnt believe and he said this was all nonsense and incredible so i asked why. / it was quite a substantial thread I just though the result was really funny so i decided to post. \n\n# Final Feasibility Assessment (as of Jan 2, 2026):\n\n|Claim|Feasible?|Notes|\n|:-|:-|:-|\n|1. Trump re-elected|‚úÖ Yes|Factually true|\n|2. Musk Nazi salute|‚ùå No|No basis in reality or plausible behavior|\n|3a. Kidnap Maduro|‚ùå No|Would trigger war|\n|3b. Seize Russian tankers|‚ùå No|Nuclear escalation risk|\n|3c. Take Greenland by force|‚ùå No|Violates NATO, international law|\n|4. Israel = genocide (majority view)|‚úÖ Yes|Already established among scholars/NGOs|\n|5. Epstein files redacted and delayed|‚úÖ Yes|Already occurred|\n|6. ICE executes citizens|‚ùå No|Physically and legally impossible |",
              "score": 0,
              "created_utc": "2026-01-12 01:18:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz306di",
                  "author": "Little-Put6364",
                  "text": "Chat history definitely sways the answer, and one as big as you're describing would have an effect. Deciding if a model is biased is tough to do. The system prompt, documents, and all chat history would have to be void of bias to get an accurate result.\n\nBased on those claims I'm assuming the articles came from a biased news source. Which can definitely skew the answer. Still very interesting though!",
                  "score": 7,
                  "created_utc": "2026-01-12 01:36:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz300nh",
              "author": "Swimming_Cover_9686",
              "text": "Model Size Context:\n\n**Qwen 3 80B:**\n\n* 80 billion parameters\n* Considered a **large/flagship-tier model**\n* Comparable to models like Llama 3.1 70B, Claude 3 Opus (rumored \\~175B), GPT-4 tier",
              "score": 0,
              "created_utc": "2026-01-12 01:35:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5feg5",
          "author": "PikaPikaDude",
          "text": "Several of these are not worded neutrally with all information, but already include strong opinions and conclusions.  \n\nThat does not give Qwen a fair objective evaluation.",
          "score": 4,
          "created_utc": "2026-01-12 12:22:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz36cfw",
          "author": "onethousandmonkey",
          "text": "The normies will believe what these models feed them.\nIt‚Äôs not good.",
          "score": 4,
          "created_utc": "2026-01-12 02:09:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz48ibm",
          "author": "inotparanoid",
          "text": "Well, i guess that puts the current events in perspective",
          "score": 2,
          "created_utc": "2026-01-12 06:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2wc41",
          "author": "KS-Wolf-1978",
          "text": "re. #4 You are lying there. (And i am anti most of what Trump says and/or does).",
          "score": 4,
          "created_utc": "2026-01-12 01:15:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2xyaj",
              "author": "Swimming_Cover_9686",
              "text": "this is the source i used for that claim. [https://www.latimes.com/california/story/2026-01-07/minneapolis-ice-killing-california-lawmakers-decry-public-execution](https://www.latimes.com/california/story/2026-01-07/minneapolis-ice-killing-california-lawmakers-decry-public-execution)  I have not actually read it so dunno",
              "score": -3,
              "created_utc": "2026-01-12 01:24:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz304lr",
                  "author": "KS-Wolf-1978",
                  "text": "https://www.youtube.com/watch?v=Y04ndAPynMk",
                  "score": 0,
                  "created_utc": "2026-01-12 01:36:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4hum0",
          "author": "Lan_BobPage",
          "text": "Man Russia or China should really get Greenland before Trump does ngl. I want my RTX 6000 clone at 1\\\\3rd the price to run local models",
          "score": 2,
          "created_utc": "2026-01-12 07:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6e8sk",
          "author": "RobXSIQ",
          "text": "\"These are not exaggerated scenarios.\"\n\nThese are hyperbolic phrasing...and also exaggerated scenarios.\n\n  \n1) Elon raised his hand. haters decided it was a nazi salute. This is the OK sign all over again. The smoothest of brains are convinced its a nazi salute.   \n2) kidnapping is unlawful and is done to get ransom. this was extraction for trial.  \n3) This happened, but not done willy nilly...it was linked to sanctioned oil trade and it was a 2 week cat and mouse game  \n4) ICE shot someone trying to run them over  \n5) This is kinda true, but its been on Trumps favorite playlist since forever.\n\nSo the phrasing being poisoned here is one of the issues. Even if it was put gently however, AIs default to big things as typically unlikely anyhow as they don't want anyone just coming up and saying the moon is found to be made of cheese and it accept it as fact without the ability to research.  \n\nTell it Rob Reiner and his wife got murdered by their son and it'll disbelieve that also, because thats a big story. Tell it Netflix is buying Warner Bros...same. Tell it a saudi arabian group is going to buy EA...disbelief. Anything that moves the civilization needle it needs to remain suspect until it can confirm, because otherwise, what good is the AI if it'll take anyones thoughts as fact...even if all of it is true.",
          "score": 1,
          "created_utc": "2026-01-12 15:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3q1jh",
          "author": "AlgorithmicMuse",
          "text": "I found qwen3-30b to be the absolute best for being deterministic for my agent.  If I want to chat,  the agent uses tavily for web searching, not model training data.",
          "score": 1,
          "created_utc": "2026-01-12 03:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5ffv0",
          "author": "Heyla_Doria",
          "text": "Faites pas confiance aux llm pour comprendre le monde ü§∑‚Äç‚ôÄÔ∏è",
          "score": 1,
          "created_utc": "2026-01-12 12:22:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8kq0a",
          "author": "IrisColt",
          "text": "Change the system prompt's cutoff date, heh",
          "score": 1,
          "created_utc": "2026-01-12 21:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmcays",
          "author": "TokenRingAI",
          "text": "FWIW, Qwen 80B only has a vague understanding of the context you give it due to the model design, and hallucinates like crazy.\n\nThe actual output is very good, but the models understanding and recall of the context you gave it seems to be subpar, as a result of the hybrid attention mechanism.",
          "score": 1,
          "created_utc": "2026-01-14 22:01:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5czkd",
          "author": "Zestyclose-Shift710",
          "text": "not even it believes Elon did a nazi salute\n\nneither is ICE EXECUTING anyone wtf\n\nadd some more crazy left wing bias, see if it believes it more\n\nno shit it's too dystopian if you literally paint it that way",
          "score": 0,
          "created_utc": "2026-01-12 12:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4vpar",
          "author": "dsanft",
          "text": "Scenario 4 blatantly isn't what happened lol\n\nYes an execution would be implausible. \n\nShooting someone who's intentionally causing a disruption, ignoring instructions, and driving their car forward while you're still in front of it is only an \"execution\" on Reddit. In real life we call that FAFO.",
          "score": 1,
          "created_utc": "2026-01-12 09:31:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4ygu0",
              "author": "TechnoByte_",
              "text": "Hot take: causing disruption shouldn't result in being murdered\n\nI thought the US had a justice system for that",
              "score": 5,
              "created_utc": "2026-01-12 09:58:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz84jcr",
              "author": "Subject_Mix_8339",
              "text": "He was already on the side of the car when he fired the gun. There have been officers getting assaulted/beaten by people who have reacted calmer than pulling a weapon and killing someone.",
              "score": 2,
              "created_utc": "2026-01-12 20:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzbholy",
                  "author": "dsanft",
                  "text": "The bullet hole is in the front windshield. \n\nhttps://share.google/gHGJUVBeiZVDZlGCu",
                  "score": 1,
                  "created_utc": "2026-01-13 08:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz2q1zj",
          "author": "DinoAmino",
          "text": "If I made embarrassing posts like these I'd be hiding my account history too.",
          "score": -7,
          "created_utc": "2026-01-12 00:44:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2w487",
          "author": "Feztopia",
          "text": "They just lack expertise, none of these events were surprising for me except for Trump eyeing for Greenland.\nI mean yes I have seen so called \"experts\" who claimed Russia won't attack Ukraine and that these are lies 1-2 days before the attack but I was busy to insulting their mom for how stupid they were. Not everyone has an understanding of geopolitics, and texts written by \"experts\" often include propaganda instead of truth. Training a model on that is not the same like training a model on working code or correct math. Plus on top of that they get safety training to not make people sad",
          "score": 1,
          "created_utc": "2026-01-12 01:14:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2xjkz",
          "author": "neil_555",
          "text": "I solved that issue a few days ago ...\n\nFirstly make sure the current date is passed to the model on startup, I got ChatGPT to modify the Jinja template to pass in the current date.\n\nI tried to paste the template here but I couldn't post (maybe it was too big?)\n\nSecondly, add this to the system prompt ...\n\nThe current date has been passed to you on startup. Use this provided date to interpret relative terms like ‚Äútoday‚Äù, ‚Äúyesterday‚Äù, ‚Äúthis year‚Äù, etc.\n\nIf the user mentions a date which is later than your training data cutoff this is to be expected, do not challenge them or accuse them of invention or fantasy.\n\nThat solved the issue for me :)\n\nEdited as the system prompt instructions were pasted twice (dunno why that happened)",
          "score": 1,
          "created_utc": "2026-01-12 01:22:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz2yp22",
              "author": "Swimming_Cover_9686",
              "text": "thanks I will try the code you sent me sometime (if I ever get off reddit and actually get my accounting backlog sorted)",
              "score": 1,
              "created_utc": "2026-01-12 01:28:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz2yqus",
              "author": "TomLucidor",
              "text": "Seconding this + getting proper web search scaffolds. Also might be a problem if the cutoff date is unknown for the model.",
              "score": 1,
              "created_utc": "2026-01-12 01:28:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3n7cy",
          "author": "alphatrad",
          "text": "Who asks AI about this stuff anyways?\n\nThe dumbest use of AI is always \"please prove my point of view is the correct one and I'm 100% right and morally superior\"",
          "score": 0,
          "created_utc": "2026-01-12 03:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz58yhi",
              "author": "Anonasty",
              "text": "Personally the context is irrelevant if you are testing how the model behaves when your data has cutoff and you need to use some examples.",
              "score": 2,
              "created_utc": "2026-01-12 11:32:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4d09e",
          "author": "jacek2023",
          "text": "I don‚Äôt know what‚Äôs more pathetic on LocalLLaMA right now: the fact that every post about China is being upvoted (and everything not about China is downvoted), or that openly political statements are being praised here. I miss 2023, when LocalLLaMA was about local LLMs.",
          "score": 0,
          "created_utc": "2026-01-12 06:38:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz58stt",
              "author": "a_beautiful_rhind",
              "text": "Reddit isn't a real place. It's partially owned by tencent and a target of other foreign influence operations. They literally live in a bubble enforced by groupthink. The absolute wildest takes come out of here.",
              "score": 1,
              "created_utc": "2026-01-12 11:31:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3hz6n",
          "author": "Pretend-Paper4137",
          "text": "But, like- same, girl.",
          "score": 1,
          "created_utc": "2026-01-12 03:11:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz46uzo",
          "author": "ilabsentuser",
          "text": "Didnt knew about 3.\n\nNow, you know what is the problem here? That AI has too much common sense, certainly more that the people involved on this events!\n/j",
          "score": 1,
          "created_utc": "2026-01-12 05:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4n48l",
          "author": "asuka_rice",
          "text": "It‚Äôs all about the data.",
          "score": 1,
          "created_utc": "2026-01-12 08:09:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4w7la",
          "author": "toothpastespiders",
          "text": "You get something similar if you go through historical documents that are small scale enough to not be well represented in the training data. LLMs aligned around a general \"friendly assistant\" role are naturally pushed to be skeptical of narratives that don't match the alignment. Violent or controversial moments in history that aren't in the training data, whether that's something in the present or the past, typically won't.\n\nThough OkAssociation3083's point about how you're framing things is equally as important. You have to admit that you're being intentionally hyperbolic with what you're feeding a MoE that only has a minuscule amount of active parameters. Qwen's MoEs are impressive for what they're able to accomplish. But they're ultimately still severely hampered.",
          "score": 1,
          "created_utc": "2026-01-12 09:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza1b9n",
          "author": "ThisGonBHard",
          "text": "This reeks of political bias on your side, or you not know how dystopian our world already was. AI is huge on normalcy bias.\n\nQwen 235B is still in abject denial about the state of free speech in the UK for example, where you can be arrested the same way you are in China or Russia, for \"hate speech\", like silently praying in your head on the street, posting rap lyrics, criticizing immigration, the king or MPs, or the entire concept of non crime hate incidents. \n\nOr the entire Age Verification mess, that is taken straight from the CCP playbook.\n\nQwen will even outright lie about some of those being acquitted, like Count Dankula, or the kid who posted rap lyrics. \n\nUnless given the actual examples, it will deny it being a thing, and this are all events BEFORE it's cutoff date.\n\nUnderstand the limitations or biases of AI when using it.",
          "score": 1,
          "created_utc": "2026-01-13 02:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4hxpw",
          "author": "LinkSea8324",
          "text": ">execution\n\nlol",
          "score": -1,
          "created_utc": "2026-01-12 07:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz47pn8",
          "author": "ImportancePitiful795",
          "text": "Except it's argument on 1, on 2,3,4,5 is absolutely correct. I don't see where is the problem? ü§î",
          "score": 1,
          "created_utc": "2026-01-12 05:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz57tcn",
          "author": "Steuern_Runter",
          "text": "Just add to your prompt that the events are narrated with a strong left-wing framing.",
          "score": 0,
          "created_utc": "2026-01-12 11:23:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3rnb4",
          "author": "BrockosaurusJ",
          "text": "What were your prompts? Were you asking \"Tell me why this is impossible\" or similar? Because it's a next word predictor, so it will always try to complete the thought that you're led it down.\n\nDo another version where you ask it \"Tell me why this is possible\" and report back.",
          "score": 0,
          "created_utc": "2026-01-12 04:04:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qgjv",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -5,
          "created_utc": "2026-01-12 08:40:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4ysf7",
              "author": "TechnoByte_",
              "text": "ü•æüëÖ",
              "score": -1,
              "created_utc": "2026-01-12 10:01:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3llqt",
          "author": "XiRw",
          "text": "I made a post about this because I asked it a question about the 5090 (said it didn‚Äôt exist) but I didn‚Äôt  have web search on. I think Qwen needs to make a decision to do it automatically like OpenAI otherwise people are going to get the wrong idea about it",
          "score": 0,
          "created_utc": "2026-01-12 03:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5yq3v",
          "author": "Cherubin0",
          "text": "I agree. A local LLM on my hardware should believe all fake news I tell it to believe.",
          "score": 0,
          "created_utc": "2026-01-12 14:19:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4zfa5",
          "author": "nenulenu",
          "text": "You are always going to run the information espionage risk with Chinese developed models.",
          "score": -1,
          "created_utc": "2026-01-12 10:07:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdna3t",
      "title": "7x Longer Context Reinforcement Learning in Unsloth",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/nmkee12vbjdg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-01-15 15:56:40",
      "score": 249,
      "num_comments": 27,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzsx9q1",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 21:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqzv2a",
          "author": "Educational_Rent1059",
          "text": "road to 10X moves fast!! good job team Unsloth",
          "score": 27,
          "created_utc": "2026-01-15 16:10:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr12xt",
              "author": "Clear-Ad-9312",
              "text": "unsloth 10x devs, they are the real deal in terms of actually making LLMs useful for people to run locally",
              "score": 15,
              "created_utc": "2026-01-15 16:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztw6vs",
                  "author": "danielhanchen",
                  "text": "Appreciate it :) We have much more releasing next week!",
                  "score": 3,
                  "created_utc": "2026-01-16 00:21:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzr3v77",
              "author": "yoracale",
              "text": "Thanks so much appreciate it. We got lots more stuff coming in the next few weeks! üôèü¶•",
              "score": 8,
              "created_utc": "2026-01-15 16:28:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzr96f6",
          "author": "PlasticTourist6527",
          "text": "Sincere question: How or where do we get proper training data that is that long, other than maybe recordings of coding tasks, lets say real world tasks, I guess there is not much proper instruction/QA training data",
          "score": 11,
          "created_utc": "2026-01-15 16:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt90gq",
              "author": "de4dee",
              "text": "i think the idea of GRPO is that the model fills those reasoning tokens. more space means they can reason longer.. . \n\nor if you are doing alignment, it may have more space for figuring out how to align its ideas.",
              "score": 7,
              "created_utc": "2026-01-15 22:19:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztweno",
                  "author": "danielhanchen",
                  "text": "Yes the goal of RL like in the Scale RL paper is for the model itself to generate the login responses automatically to your question so yep your right on this!",
                  "score": 5,
                  "created_utc": "2026-01-16 00:22:40",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztvc9r",
              "author": "Bakoro",
              "text": "For truly long reasoning chains, I think concatenating a bunch of shorter synthetic chains is going to have to be the way to go.  \n  \nYou could have a model formulate a plan without actually doing the plan, verify that the plan is sound, have the model reason through each task, make subtasks, and verify that the the subtasks are reasonable.  \nThen have the model do each small thing.   \n   \nAt the end, you've got a huge trace, and if it's something deterministically verifiable, then you've got strong reasons to believe that the whole chain is good.  \nWith stuff like writing software or doing mathematics, this is a tractable problem.   \n    \nFor things that are less deterministic, like making images or videos, then what some organizations are doing is training critic models whose job is to find and point out problems.  \nHistorically that kind of thing was at high risk of mode collapse, but we're getting sufficiently good models now that subjective discriminators are starting to be a net positive in pushing the generative models to produce better output.  \n   \nSo let's say you wanted to train a model to use a computer like a person, you would start with short tasks like \"move the cursor to the target\", and you could have a combination of OCR, segmentation models, and standard accessibility tools to verify that the model did the thing.   \nYou have a proposer model that produces increasingly complex tasks, and a kind of referee model that's smart enough to say \"hey, this model is gaming the system\".   \n  \nThat used to be somewhere between too labor intense to be practical, and impossible. vLLMs are bootstrapped enough now to make it feasible.  \n   \nIn robotics land, they're letting the models learn to play whole video games, and are generating digital worlds for the model to do things in, and then sticking the models into robot bodies, and it turns out that it works pretty well.  \nSo, apparently \"play Skyrim\" is a valid training strategy.",
              "score": 3,
              "created_utc": "2026-01-16 00:16:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nztwpdk",
              "author": "danielhanchen",
              "text": "Oh so that's for general fine-tuning tasks! The trick of RL is you don't need that long data, but instead an environment that verifies if your answer is correct or not \n\nSo the large context is there as a working out space or some scratch pad, and the larger the scratch pad, the better the RL process can get!",
              "score": 3,
              "created_utc": "2026-01-16 00:24:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzx4lk1",
                  "author": "PlasticTourist6527",
                  "text": "This all makes me think more and more, why wouldn't we want to pursue the JEPA architecture, that is, allow a larger scratch pad in the latent space instead of forcing it into text tokens? and while were at it, following the deepseek ocr paper, maybe we can allow a large scratchpad with vision/graphic tokens?",
                  "score": 2,
                  "created_utc": "2026-01-16 13:52:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzr3s1h",
          "author": "knownboyofno",
          "text": "Would this work for Qwen3 30B-3A?",
          "score": 5,
          "created_utc": "2026-01-15 16:28:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr4g11",
              "author": "yoracale",
              "text": "Yes kind of, we're working on MoE even better though. In the next few weeks we'll have something for it! üôè",
              "score": 11,
              "created_utc": "2026-01-15 16:31:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzr5hji",
                  "author": "knownboyofno",
                  "text": "Thanks. I wanted to try a weird experiment where I would use data from a Devstral model (The small is very good for it's size for coding) to train Qwen3 30B-3A. Let me know if you need any testers. I have a rtx6000 and 2x3090s.",
                  "score": 6,
                  "created_utc": "2026-01-15 16:35:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzrbkk7",
          "author": "1ncehost",
          "text": "fyi, I'm training a model on ROCm and had a load of issues with the latest versions from last week following your ROCm guide. I had to make some fairly deep patches and replace kernels. I know things move fast and there are too many platforms to test, but I wanted to let you know so you could do another pass on that tutorial at some point.\n\nAlso for some reason SDPA was the fastest attention for qwen3 0.6B instead of FA2 or xformers. IDK why, but it was double digit percentages faster.",
          "score": 2,
          "created_utc": "2026-01-15 17:03:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztx036",
              "author": "danielhanchen",
              "text": "Oh my ok let me recheck AMD support and get back to you sorry for the bad experience",
              "score": 1,
              "created_utc": "2026-01-16 00:25:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrpifk",
          "author": "ApprehensiveTart3158",
          "text": "Beautiful work!",
          "score": 2,
          "created_utc": "2026-01-15 18:05:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztwuga",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-16 00:25:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzr1maj",
          "author": "Zestyclose839",
          "text": "This is great work. Is this for preventing models from breaking down over long horizon tasks? I can imagine only training on short contexts makes models brittle when the conversation gets long, like in CLI coder situations.",
          "score": 1,
          "created_utc": "2026-01-15 16:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr4aq1",
              "author": "yoracale",
              "text": "Yes kind of, this is more for compute limitations. E.g. previously you need 192gb to get 30k context but now you only need 24gb vram. And there's no accuracy degradation to get this less VRAM usage.\n\nFor long horizon tasks, the dataset or training method you undertake will determine the outcome of your long context forgetfulness.",
              "score": 6,
              "created_utc": "2026-01-15 16:30:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrv87p",
          "author": "Substantial_Swan_144",
          "text": "Is this available for Ollama / LmStudio yet?",
          "score": 1,
          "created_utc": "2026-01-15 18:31:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztxapv",
              "author": "danielhanchen",
              "text": "Oh is this for fine-tuning, training and reinforcement learning so it's available in our GitHub package Unsloth",
              "score": 2,
              "created_utc": "2026-01-16 00:27:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrzz4o",
          "author": "poladermaster",
          "text": "This is insane progress! Makes me wonder what kinda creative projects folks in r/creativecoding will cook up with this. Been wanting to play with longer context for some Three.js shenanigans.",
          "score": 1,
          "created_utc": "2026-01-15 18:51:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztx7en",
              "author": "danielhanchen",
              "text": "Thanks! Oh excited to see what they might come up with if folks do long context rl!",
              "score": 2,
              "created_utc": "2026-01-16 00:27:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qajxrg",
      "title": "Local LLM + Internet Search Capability = WOW",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/",
      "author": "alex_godspeed",
      "created_utc": "2026-01-12 03:21:41",
      "score": 242,
      "num_comments": 85,
      "upvote_ratio": 0.92,
      "text": "Am on Qwen 3, asked about the training date and it said 2024. Alright, guess that's the thing I need to live with. Just need to constantly lookup HF for updated LLM which fits my cute 16gb vram.\n\nThen someone said always ground your local AI with internet searches. A quick search = LM studio duckduckgo plugin\n\nWithin 15 minutes, prompt with \"searching the web\", exactly the same interface I saw at ChatGPT!\n\n  \nMan, this local AI is getting better. Am I having 'agentic-AI' now? haha. I.e., tool calling is always something i heard of, but think that it's reserved for some CS-pro, not an average joe like me.\n\n  \nso now what, when was your 'wow-moment' for stuff like this, and what other things you design in your workflow to make locally run LLM so potent and, most importantly, private? =)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qajxrg/local_llm_internet_search_capability_wow/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nz3lu6s",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 56,
          "created_utc": "2026-01-12 03:32:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3sgrj",
              "author": "sshwifty",
              "text": "Y'all need to be sharing this stuff",
              "score": 37,
              "created_utc": "2026-01-12 04:09:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3w5j1",
                  "author": "ridablellama",
                  "text": "game changers for me have been unmute which is a tts and stt harness for any llm and open-llm-vtuber. both open source.",
                  "score": 28,
                  "created_utc": "2026-01-12 04:32:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3sovc",
                  "author": "ClimateBoss",
                  "text": "is there cli tool like mistral vibe but for deep research?",
                  "score": 7,
                  "created_utc": "2026-01-12 04:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4or0x",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 13,
              "created_utc": "2026-01-12 08:24:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz3wb26",
              "author": "nomorebuttsplz",
              "text": "what tts and how is the latency?",
              "score": 7,
              "created_utc": "2026-01-12 04:33:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz40dqu",
              "author": "Zc5Gwu",
              "text": "Doesn't including the time screw up prompt caching?",
              "score": 2,
              "created_utc": "2026-01-12 05:00:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz482iu",
              "author": "leonbollerup",
              "text": "Or use cherry studio :)",
              "score": 0,
              "created_utc": "2026-01-12 05:58:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz587ev",
                  "author": "ikkiyikki",
                  "text": "Had never heard of this and for a sec I thought it was an app like LM Studio that was better by having native MCP built in. Nope. Looks like a Perplexity clone that sends your prompts to the cloud.\n\nIf I'm getting this wrong and it does something better than LM Studio **locally** please lmk!",
                  "score": 5,
                  "created_utc": "2026-01-12 11:26:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4eqlr",
          "author": "Everlier",
          "text": "You can get this in one command with [Harbor](https://github.com/av/harbor), I think you might also enjoy how well it pairs with TTS/STT",
          "score": 17,
          "created_utc": "2026-01-12 06:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz66vgl",
              "author": "steezy13312",
              "text": "That repo has the longest list of files and folders at the root level that I've ever seen.",
              "score": 7,
              "created_utc": "2026-01-12 15:01:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6l3ef",
                  "author": "Everlier",
                  "text": "Check out CLI reference docs, they are even longer!",
                  "score": 2,
                  "created_utc": "2026-01-12 16:09:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4imy0",
              "author": "alex_godspeed",
              "text": "cool. A quick question. I was told as a newbie to avoid ollama (maybe because they go cloud and has lesser support than llama.ccp). Also, I can't use LM studio right? I searched the user guide and it retrieves none related to this interface.",
              "score": 1,
              "created_utc": "2026-01-12 07:28:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz4pft2",
                  "author": "Everlier",
                  "text": "`harbor up searxng` will spin up Open WebUI instance pre-configured for Web RAG with SearXNG, there are lots of inference backends to choose from if you don't want Ollama. llama-swap + llama.cpp is a popular option these days.",
                  "score": 3,
                  "created_utc": "2026-01-12 08:31:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz43kbc",
          "author": "SignificantExample41",
          "text": "better idea - use Brave Leo with your own local model and your choice of ‚Äúmemory‚Äù (storage and retrieval pipeline) for it and let it hoover up context and run your life without anything going to the cloud. nemotron 3 30b a3b is ideal for this.",
          "score": 37,
          "created_utc": "2026-01-12 05:23:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz44xrh",
              "author": "Grouchy_Ad_4750",
              "text": "Which storage and retrieval pipeline would you recommend?",
              "score": 10,
              "created_utc": "2026-01-12 05:33:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nz4cmu2",
              "author": "KneelB4S8n",
              "text": "How do we use local LLM with Brave Leo? That's the browser, right?",
              "score": 9,
              "created_utc": "2026-01-12 06:35:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4gk1l",
                  "author": "Blindax",
                  "text": "I was wondering the same and found this [BYOM](https://support.brave.app/hc/en-us/articles/34070140231821-How-do-I-use-the-Bring-Your-Own-Model-BYOM-with-Brave-Leo)",
                  "score": 14,
                  "created_utc": "2026-01-12 07:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4fbip",
              "author": "RobotRobotWhatDoUSee",
              "text": "I'm very interested to hear more about your setup here. What 'harness' or 'agentic framework' are you using? (In scarequotes since I know this may simply be something like a clever use of OWUI or Claude code or cursor or something like that)",
              "score": 3,
              "created_utc": "2026-01-12 06:58:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz4wbme",
                  "author": "SignificantExample41",
                  "text": "sorry for the delay y‚Äôall. got busy on something. i‚Äôm going to give away my best kept secret. which i feel bad about keeping cause these guys seem really cool (never met them don‚Äôt know them) - but the answer to literally every problem i‚Äôve ever had isnt even one word.  It‚Äôs a‚Ä¶.\n\nLetta.",
                  "score": -1,
                  "created_utc": "2026-01-12 09:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz4s2qx",
              "author": "Rey_Fiyah",
              "text": "Could you elaborate on this a little bit? I‚Äôd be very interested in having search capabilities on my local LLM, but I‚Äôm also privacy conscious. Any tips on where to start with this? I‚Äôve only really used llama.cpp so far.",
              "score": 1,
              "created_utc": "2026-01-12 08:56:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz5l3yu",
                  "author": "Odd-Criticism1534",
                  "text": "BrowserOS is something worth checking iht",
                  "score": 2,
                  "created_utc": "2026-01-12 13:01:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz43hxd",
          "author": "LegitimateCopy7",
          "text": "if you want real privacy, route the searches using seaxrng and Tor. otherwise search providers like duckduckgo still know you inside out.",
          "score": 8,
          "created_utc": "2026-01-12 05:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5n95l",
              "author": "Odd-Criticism1534",
              "text": "Legitimately curious, can you say more?",
              "score": 2,
              "created_utc": "2026-01-12 13:14:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz6b5zq",
                  "author": "LegitimateCopy7",
                  "text": "seaxrng is an open source search engine you can selfhost. it still routes search queries to other search engines but does not include identifiable information in the queries.\n\nbecause search providers can still see your IP so this is where Tor comes in. other VPN providers work too.",
                  "score": 3,
                  "created_utc": "2026-01-12 15:23:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz61aop",
          "author": "Accomplished_Code141",
          "text": "My 'wow' moment was when I tried to use small models with a ZIM MCP server and a full Wikipedia ZIM file to 'ground' answers offline in LM Studio, alongside a DuckDuckGo Docker MCP server for online grounding too. I tried some small models and found out that Jan-v1-4B is a very good and lightweight model. It is very capable in tool calling for this use case, and even with a very low-end old GPU like a GTX 1650 or RTX 3050, I can get good results. Even with several tool callings at once (like in a deep research prompt), it doesn't fail the calls and maintains acceptable speeds.\n\nI think the future for local LLMs will be the use of small models trained to excel at specific tasks, being loaded, used, and unloaded automatically by an agentic-like harness with an orchestrator model. This would prevent hallucinations on very constrained hardware, unlike one giant model that needs too much RAM/VRAM to be good at all tasks. Something like a 30B orchestrator model with several 4-8B 'use-case-tailored' models could perform at the same level as the big 200B+ models across several tasks.",
          "score": 7,
          "created_utc": "2026-01-12 14:32:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7mlcc",
              "author": "Beginning-Pace-1426",
              "text": "Isn't that essentially the purpose of training LoRAs?",
              "score": 1,
              "created_utc": "2026-01-12 18:59:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4p2z3",
          "author": "ThiccStorms",
          "text": "16gig vram is the acceptable and generic userbase spec id always imagine if we were to talk about mass local AI adoption. People with home servers and beefy rigs are the cream layer of the masses. So yes what's the best LLM for 16 gb spec? Any leaderboards?¬†",
          "score": 7,
          "created_utc": "2026-01-12 08:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4141y",
          "author": "PaceZealousideal6091",
          "text": "Anyone setting up the tool calling or websearch using llama cpp?",
          "score": 12,
          "created_utc": "2026-01-12 05:05:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4a8s0",
              "author": "Sure_Explorer_6698",
              "text": "I was working on a Tavily search bot using llama.cpp, but i had to stop my meds for a while... long story... so its abandoned on GitHub til i can focus enough to pick it up again.\n\nPoint is, it is definitely possible. I was using a Samsung S20FE 6Gb w/ 6Gb swap. Never settled on a model, but i switched between 1-3B Llama or Qwen for expermination. (Edit for autocorrect.)",
              "score": 0,
              "created_utc": "2026-01-12 06:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz3u46w",
          "author": "EbbNorth7735",
          "text": "Can we all share good sources for internet grounding? I've been using Serper I believe.",
          "score": 4,
          "created_utc": "2026-01-12 04:19:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4vyj3",
              "author": "bar_raiser333",
              "text": "I recommend Valyu for this. Cheap, fast, covers a lot of proprietarty data sources too [https://docs.valyu.ai/search/quickstart](https://docs.valyu.ai/search/quickstart)",
              "score": -3,
              "created_utc": "2026-01-12 09:34:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz40r27",
          "author": "nameless_0",
          "text": "I use Perplexica, on my 4070 mobile. It works great with Nemotron 30B-A3B. My wow moment was when I setup Qwen3 Coder with OpenCode and told it to setup a GitHub project I found, and It worked. It setup and made a front end for something that would've taken me a couple of hours of futzing around with to get working.",
          "score": 7,
          "created_utc": "2026-01-12 05:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz45yxn",
          "author": "Bananadite",
          "text": "Where's the best place to browse plugins for LM studio?",
          "score": 3,
          "created_utc": "2026-01-12 05:41:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz54izw",
              "author": "No_Swimming6548",
              "text": "Docker mcp kits are pretty good",
              "score": 2,
              "created_utc": "2026-01-12 10:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz41luo",
          "author": "ridablellama",
          "text": "here‚Äôs some of the things that have given me wow moments. force multipliers are native tools(web search, code interpreters, webpage scrape), mcp tools(so many good ones), voice(needs to be low latency real time for that special magic), memory system(especially passive automatic memory), vision(ocr and normal). you want python bare minimum in the code interpreter which will unlock data science level charts, powerpoints, customs csv file creation. some code interpreters have many languages not just python. code interpreter is insanely powerful imo and i‚Äôve barely scratched the surface. it can be 100% local too. voice is a great local use case cause it needs low latency. \n\nalso give the llm a email address and calendar. gave mine m365, one drive it‚Äôs very nice for saving files and sending temporary download links via email etc‚Ä¶this is when it really came together for me agentic wise.\n\n  \ncheck out qwen agent framework on github for a lot of local tools and agents you can build with. local web scraper works like a charm.they have a code interpreter too but it‚Äôs not sandboxed. several image manipulation related tools i think for their local image models.",
          "score": 4,
          "created_utc": "2026-01-12 05:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4r1px",
              "author": "Quiet-Owl9220",
              "text": ">you want python bare minimum in the code interpreter which will unlock data science level charts, powerpoints, customs csv file creation. some code interpreters have many languages not just python. code interpreter is insanely powerful imo and i‚Äôve barely scratched the surface. it can be 100% local too.\n\nIs there a LM studio plugin for this too?",
              "score": 3,
              "created_utc": "2026-01-12 08:46:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5aj0v",
          "author": "SatoshiNotMe",
          "text": "You can also easily hook up Claude Code, Codex CLI or similar CLI agents with local LLMs, and leverage the built in web search tools. Simple guide here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md",
          "score": 2,
          "created_utc": "2026-01-12 11:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6ahqz",
          "author": "irodov4030",
          "text": "https://preview.redd.it/82qiiep1rxcg1.png?width=2005&format=png&auto=webp&s=b2a9e3fe6a6b8925311cd3514c614435e902d1f4\n\nBult this for myself!\n\ninternet access using ollama models. \n\nIt uses Flask, and the frontend runs in the web browser. Execution trace is still in works. I am using a macbook M1-8GB RAM.",
          "score": 2,
          "created_utc": "2026-01-12 15:19:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8bp9a",
          "author": "-Akos-",
          "text": "Look for MCP, which LM Studio supports. This is a standardized way of these tools. File reading/writing, database connections, docker interaction, etc. r/mcp is its own channel. And if you want to make your own, fastmcp in python is a good one.",
          "score": 2,
          "created_utc": "2026-01-12 20:56:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8lyqv",
          "author": "IrisColt",
          "text": "How to do this with open source?",
          "score": 2,
          "created_utc": "2026-01-12 21:44:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztal7f",
          "author": "NovatarTheViolator",
          "text": "Try using cursor with codex extension, configured to utilize a local model like Qwen2.5-Coder-32B-Instruct-AWQ.  Have VLLM run the local model in Docker",
          "score": 2,
          "created_utc": "2026-01-15 22:27:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3ri81",
          "author": "Agreeable-Market-692",
          "text": "try exa and deepwiki next\n\nnote that the exa requires API key, just visit their website for a freebie   \n  \n{\n\n  \"mcpServers\": {\n\n\"exa\": {\n\n\"command\": \"npx\",\n\n\"args\": \\[\"-y\", \"exa-mcp-server\"\\],\n\n\"env\": {\n\n\"EXA\\_API\\_KEY\": \"your-api-key-here\"\n\n}\n\n}\n\n  }\n\n}\n\n\\###deepwiki mcp below\n\n{\n\n  \"mcpServers\": {\n\n\"deepwiki\": {\n\n\"serverUrl\": \"https://mcp.deepwiki.com/sse\"\n\n}\n\n  }\n\n}",
          "score": 3,
          "created_utc": "2026-01-12 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz44r2o",
              "author": "ryfromoz",
              "text": "Bright api ftw",
              "score": 2,
              "created_utc": "2026-01-12 05:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz44wly",
                  "author": "Agreeable-Market-692",
                  "text": "just googled that, didn't know what it was, that actually looks really interesting thankyou",
                  "score": 2,
                  "created_utc": "2026-01-12 05:33:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz4tcx2",
          "author": "ljubobratovicrelja",
          "text": "I've been experimenting with this recently as well, and also blown away by this idea. I recently implemented this as /browse command within my project [tensor-truth](https://github.com/ljubobratovicrelja/tensor-truth) \\- basically, an 8b model given 5-6 sensible web sources well parsed would give an amazing summary.",
          "score": 1,
          "created_utc": "2026-01-12 09:08:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz62t3x",
          "author": "Unusual_Delivery2778",
          "text": "Awesome thread. I‚Äôve been approximating some of the solutions / conclusions here for a while now, just letting my intuition run wild as I learn more and making some investments alongside it.",
          "score": 1,
          "created_utc": "2026-01-12 14:40:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4o0fp",
          "author": "jikilan_",
          "text": "The more you use the less wow u r. U are quite lucky to get a usable response",
          "score": 0,
          "created_utc": "2026-01-12 08:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4pmcq",
          "author": "SheepherderOwn2712",
          "text": "I've tried pretty much all web/grounding apis and tools out there now but what I have found the best is [https://lmstudio.ai/valyu/valyu](https://lmstudio.ai/valyu/valyu) \n\nOnly one that is built natively for tool-calling, and it returns full content instead of very small snippets-and also have stuff like live stock prices which is cool",
          "score": -5,
          "created_utc": "2026-01-12 08:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4sm9b",
          "author": "Ornery-Egg-4534",
          "text": "You should try Valyu‚Äôs tool for search. Pretty sure its the best one out there. They have benchmarks as well",
          "score": -3,
          "created_utc": "2026-01-12 09:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3obvg",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -14,
          "created_utc": "2026-01-12 03:46:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz3p6bh",
              "author": "arcanemachined",
              "text": "That is not possible. It must be doing some tool call. And I'm saying this in the hope that you'll prove me wrong.",
              "score": 17,
              "created_utc": "2026-01-12 03:50:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz3wb59",
                  "author": "SM8085",
                  "text": "It's in the chat template, for example: [https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF?chat\\_template=default#L264](https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF?chat_template=default#L264)\n\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\n\nSomeone in another thread was asking how to add that to their LM studio jinja so that all bots can be so coherent.\n\nI think that's Python?  The strftime\\_now() function?  I don't mess with chat templates that much.\n\nBut making that call is how they add it.",
                  "score": 8,
                  "created_utc": "2026-01-12 04:33:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3v0vr",
                  "author": "-InformalBanana-",
                  "text": "Probably it is in the system prompt. There is something like {{CURRENT_DATE}} to automatically get current date and put it in prompt. (Edit: or in chat template)",
                  "score": 3,
                  "created_utc": "2026-01-12 04:24:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz3tsgo",
                  "author": "EbbNorth7735",
                  "text": "It's impossible without context being fed into it.",
                  "score": 4,
                  "created_utc": "2026-01-12 04:17:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nz47frm",
                  "author": "redragtop99",
                  "text": "Ask it what the date is, it will tell you.  \n\nFunny all these downvotes when I‚Äôm right lol‚Ä¶.\n\nAnyone can try it for themselves.\n\nLoad up GPTOSS 120B‚Ä¶ What is the date today?\n\nToday is ‚ÄúCurrent date‚Äù",
                  "score": -6,
                  "created_utc": "2026-01-12 05:52:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz5b2wd",
              "author": "Pvt_Twinkietoes",
              "text": "Bro. Just pass it the date within the prompt during your API call.",
              "score": 1,
              "created_utc": "2026-01-12 11:49:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6a4oi",
          "author": "__108",
          "text": "Tried Exa, Valyu and Tavily. Imo Valyu seems to be the best for price but also how well the responses are, especially for deep research. It does a good job of web access but it also gives access to a lot of other sources as well such as academic papers, patents, stocks etc. It has become an irreplaceable part of my workflow tbh",
          "score": -1,
          "created_utc": "2026-01-12 15:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4vtmf",
          "author": "bar_raiser333",
          "text": "I know that Valyu has an LLM Studio plugin. It does tool calling really well. Feel free to try it [https://lmstudio.ai/valyu/valyu](https://lmstudio.ai/valyu/valyu)",
          "score": -6,
          "created_utc": "2026-01-12 09:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3soic",
          "author": "hkd987",
          "text": "I totally get your excitement about combining local LLMs with internet search! It really opens up so many possibilities for real-time insights and enhanced functionality. If you're exploring options, you might find LlamaGate interesting as it offers access to various open-source LLMs with a simple API, which could help bridge that gap. Check it out if you want to dive deeper: https://llamagate.dev/",
          "score": -16,
          "created_utc": "2026-01-12 04:10:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdh28f",
      "title": "RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/",
      "author": "Paramecium_caudatum_",
      "created_utc": "2026-01-15 11:27:15",
      "score": 226,
      "num_comments": 95,
      "upvote_ratio": 0.94,
      "text": "Nvidia has essentially killed off supply for the RTX 5070 Ti. Also supply of RTX 5060 Ti 16 GB  has been significantly reduced. This happened partially due to memory supply shortages. This means that most AIBs will no longer manufacture these GPUs. Prices are already jumping significantly. The 5070 Ti has risen \\~$100 over MSRP, and retailers expect further hikes. 8 GB configuration of RTX 5060 Ti remains unaffected. \n\nCredit: Hardware Unboxed  \n  \n[https://m.youtube.com/watch?v=yteN21aJEvE](https://m.youtube.com/watch?v=yteN21aJEvE)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nzqot1h",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-01-15 15:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzplu6b",
          "author": "Otherwise_Local_7743",
          "text": "Welp there goes my upgrade plans for this year. Was really hoping to snag a 5070 Ti for my homelab but looks like I'll be stuck with my 3080 for inference until prices come back down to earth",
          "score": 64,
          "created_utc": "2026-01-15 11:29:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpmqrv",
              "author": "FullstackSensei",
              "text": "Trade up for a 3090 or get a 20GB 3080 from China. Either way, do it quickly because prices are going up fast",
              "score": 24,
              "created_utc": "2026-01-15 11:37:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrjca4",
                  "author": "DonkeyBonked",
                  "text": "Yep, I got lucky and snagged my last few 3090s for $450 each and my wife was like \"if you really think it's a good investment\"... I'm like there's no way they are going down in value, this is going to get a lot worse before it gets better.\n\nI was scared my RAM might not come because the shipper said to contact the merchant, so I looked around... most of the DDR4 server RAM worth having has been wiped out, even at stupid prices!\n\nAt this rate, by the end of the year we're going to be back to early poaching prices.",
                  "score": 5,
                  "created_utc": "2026-01-15 17:38:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzqhgco",
                  "author": "nonaveris",
                  "text": "Instructions unclear, bought both.",
                  "score": 7,
                  "created_utc": "2026-01-15 14:44:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzprzd9",
                  "author": "AlternateWitness",
                  "text": "How do you trade ‚Äúup‚Äù for a 3090? I doubt anyone is going to trade their 3090 for a 3080, even if it‚Äôs +cash.",
                  "score": 5,
                  "created_utc": "2026-01-15 12:16:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nztub7p",
                  "author": "TheManicProgrammer",
                  "text": "They trippled in price here in Japan the last few weeks :( about 1,200usd here",
                  "score": 1,
                  "created_utc": "2026-01-16 00:11:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzxim8p",
              "author": "emperorofrome13",
              "text": "I have a 5070ti rather have more vram than more power actually",
              "score": 2,
              "created_utc": "2026-01-16 15:02:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzppdbg",
          "author": "phido3000",
          "text": "I bought 4 5060ti on special.\n\nThey were cheap, cheapish. $390 inc tax delivered. I thought, heck its a fairly cheap, fairly decent way to add Nvidia memory to a system, and a two slot cooler, low power.  I figured, well, it would be 64Gb <$1600, brand new. I could sell them off individually after I conclude my experiments with them. \n\n I quite like them. At around $350 they would be good value. Good enough for games, all the DLSS, and the AI processing is, pretty good. Plenty of RAM. Image generation, inferencing, game playing, its a decent card for those.\n\nFor LLAMA..\n\n They are great little cards for small budget inferencing. If you can't get 3090's where you live, then, these were quite viable. You could fit four, or more into a regular powersupply machine.  It supports all the new quants. 70B models are very usable with 64Gb VRAM. \n\n Nvidia was finally getting generous with the ram. Unlike the 4060 16gb, this ram GDDR7, was fast enough to mean the 128bit bus wasn't a huge hinderance for the little chip, you had as much as a 192bit bus GDDR6 card had. 16Gb means that DLSS and RT are totally possible without giving up textures. \n\n I imagine 16Gb GDDR7 is worth probably more than $390 alone today. I was worried I should have waited for 5070Ti Supers..",
          "score": 40,
          "created_utc": "2026-01-15 11:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpz795",
              "author": "2str8_njag",
              "text": "For me, this card was an upgrade from RX 480 8GB. I love it. Whole Nvidia ecosystem is magnificent. If I was a gamer I could've gone with 5070 Ti, but I think this card is better pick overall.",
              "score": 9,
              "created_utc": "2026-01-15 13:05:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzs0e33",
              "author": "daank",
              "text": "I'm curious, how did you build your pc with four gpus? Do you have a special motherboard or use a mining rig?",
              "score": 4,
              "created_utc": "2026-01-15 18:53:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt22iw",
                  "author": "phido3000",
                  "text": "I have an Epyc motherboard. It comes with 4 x PCIe 4.0 x 16 slots. You plug them in. Thats it. Normal ATX case. I am now trying to adapt it to a 8 x GPU (Mi50) setup, with some SFF to MICO cables. Eypc has a lot of PCIe lanes. This will give me 256 Gb of VRAM in plus 512Gb of 8 channel DRAM. \n\nThe older DDR4 epyc stuff is cheaper now. CPUs can be had for $100, and mobo, brand new, $300. \n\nhttps://preview.redd.it/90sstpp92ldg1.png?width=1600&format=png&auto=webp&s=74bd98f43b2627820683f8f2cfe84becaf9e5e94\n\nMining rigs are popular. But I hate them. I don't want a $10,000 janky fire hazard. \n\nI also have a Lenovo P920 I bough cheap off ebay, It can take 3x2slot GPU's easy. Its a dual Xeon. I bought it just before the ram price rises. It came with 192Gb of ram (16 memory slots)",
                  "score": 7,
                  "created_utc": "2026-01-15 21:47:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqv4tb",
              "author": "WiseassWolfOfYoitsu",
              "text": "I just bought a 5060ti 16gbas a secondary card, looks like I grabbed it just in time - I was actually about to return it since I thought I might try to pick up something else a bit more powerful instead, but 5070ti was my main other consideration, and if they're also spiking, that'll be off the table. Main inference card is a 7900xtx, but the AMD cards are dog slow at diffusion, so this was meant at a secondary card I could use for either diffusion or to run another smaller inference engine in parallel.",
              "score": 2,
              "created_utc": "2026-01-15 15:49:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzsqee3",
              "author": "redditrasberry",
              "text": "> 70B models are very usable with 64Gb VRAM. \n\nhow do you link them? Or are you relying on multi-GPU support in llama.cpp etc?",
              "score": 2,
              "created_utc": "2026-01-15 20:53:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzt0eaa",
                  "author": "phido3000",
                  "text": "It works fine under llama.cpp and even LMstudio.\n\n LMstudio, they are just there, it uses them. Easy as pie. Multigpu may be tricky to get to work with games, but for LLM its very, very simple, every bit of software is basically built for it. They work under CUDA or Vulcan. Easy. Windows/linux. Compatibility with Nvidia GPUs on projects is also very good.",
                  "score": 2,
                  "created_utc": "2026-01-15 21:39:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztf32x",
              "author": "itrollhockey",
              "text": "I'm using a 16gb 4060 ti and 16gb 5060ti for local interference for my startup. It's amazing for small models (e.g. I'm using Kokoro for TTS, which fits in under 8gb)",
              "score": 1,
              "created_utc": "2026-01-15 22:50:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpy8gy",
              "author": "tmvr",
              "text": "Same here, got 3 of them end  of last year. Originally wanted only two, but got 3 by accident, but seeing where the market is heading I've kept the third one as well. Nice cards with low power consumption and just enough memory bandwidth for it not to be a hindrance.",
              "score": 1,
              "created_utc": "2026-01-15 12:59:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpsmrs",
          "author": "ravensholt",
          "text": "nGreedia is turning into a meme-company.",
          "score": 26,
          "created_utc": "2026-01-15 12:21:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqh5bx",
              "author": "ForsookComparison",
              "text": "Greed company sure. $5-Trillion is one hell of a meme though",
              "score": 4,
              "created_utc": "2026-01-15 14:43:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzs6tx2",
              "author": "cobbleplox",
              "text": "Since there now is an actual shortage of RAM manufacturing capacities, it is actually much more rational that consumer GPUs are kind of stuck in the 16GB area, and also not to \"waste\" that RAM on weaker GPUs.",
              "score": 2,
              "created_utc": "2026-01-15 19:22:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzw54yw",
                  "author": "ravensholt",
                  "text": "Dude. What are you talking about?\n\n\nThey literally just killed their top selling 16GB models.\n\n\nNow consumers are either getting low-end shitty 8GB models or the already overpriced 5080/5090.",
                  "score": 1,
                  "created_utc": "2026-01-16 09:33:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzqw2a3",
              "author": "zipzag",
              "text": "You sell your stuff for less than what people will pay?",
              "score": -7,
              "created_utc": "2026-01-15 15:53:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrx5tn",
                  "author": "chuckaholic",
                  "text": "The \"charge as much as the market will bear\" philosophy is relatively new. It appeared with the spread of Friedman economic practices. Milton Friedman wrote his doctrine in 1970. \n\nBefore that, the market operated on Keynesian principals. Affordability wasn't much of a problem, mostly because the market didn't have to support the parasite class. (billionaires) Companies would charge according to cost plus expenses, plus a little bit for profit. Enough to pay employees, rents, supplies, insurance, executives, and a percent or 2 for company growth. \n\nMy dad was able to pay for college classes, textbooks, food, room, and board by bagging groceries over the summer. We can thank Milton Friedman that is not an option any more. He's not the only one to blame, but he bears a lot of responsibility for writing the billionaires manifesto.",
                  "score": 3,
                  "created_utc": "2026-01-15 18:39:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzpouok",
          "author": "960be6dde311",
          "text": "Just grabbed a PNY RTX 5070 Ti a couple weeks ago. So happy I did.",
          "score": 22,
          "created_utc": "2026-01-15 11:53:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq3lxi",
              "author": "FeelingVanilla2594",
              "text": "You dodged a bullet train.",
              "score": 4,
              "created_utc": "2026-01-15 13:31:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzr1txn",
              "author": "FlakyChance9338",
              "text": "Same here",
              "score": 3,
              "created_utc": "2026-01-15 16:19:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqfgka",
          "author": "Specialist_Pea_4711",
          "text": "All hopes on china, please develop once consumer grade GPU",
          "score": 17,
          "created_utc": "2026-01-15 14:34:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpv3qq",
          "author": "PhantomWolf83",
          "text": "I managed to get a 5060 Ti 16GB for US MSRP last month (living in Asia). I initially wanted to get another one for a dual GPU setup, but now it looks like the card will be impossible to find or I'd have to pay though the nose to get one, let alone a 5070 Ti. No way am I going to spend on a 5080 just for AI inference. Used 3090 prices are also starting to go up again.\n\nIf the worst does happen, I'll probably have to settle for a 5060 Ti 16GB + 5060 Ti 8GB for 24GB, or an older Nvidia card (they said they might bring back the 3060, hopefully it's the 12GB version. If not, used ones are still a reasonable price in my country). At least something to work with until the memory crisis blows over and the AI bubble bursts.",
          "score": 5,
          "created_utc": "2026-01-15 12:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrjvbi",
          "author": "One-Employment3759",
          "text": "Really starting to despise Nvidia, despite being a fan since 2000 and my TNT2 ultra",
          "score": 3,
          "created_utc": "2026-01-15 17:40:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpw47q",
          "author": "corruptboomerang",
          "text": "I just bought a 5060Ti 16Gb!\n\nI've noticed prices have gone up at least $25 per.",
          "score": 11,
          "created_utc": "2026-01-15 12:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzsapxz",
              "author": "One_Tie900",
              "text": "dam shud got the 5070ti laptop for 1k",
              "score": 2,
              "created_utc": "2026-01-15 19:40:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzwwluh",
              "author": "rana-",
              "text": "Same my man. I bought it during Christmas after the rumor of discontinuing it. I made the right call and got it at pretty good discount. Now the price has gone up +20 - 40% here. Super crazy market",
              "score": 1,
              "created_utc": "2026-01-16 13:07:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqvp45",
          "author": "slippery",
          "text": "Due to outrageous prices, I have given up on the idea of upgrading my video card this year. Even a used 4090 is $1500+. I've decided to rent high end GPUs this year instead for AI work.",
          "score": 3,
          "created_utc": "2026-01-15 15:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzs5n5a",
          "author": "Il_Signor_Luigi",
          "text": "I'm a bit pissed I didn't get the 5070 Ti on black friday... Just installed a 5060 Ti 16gb though, like 5 minutes ago. Got it for cheap.  \nThis however is big news, bad news for the market and us local enthusiasts. When can we expect things to cool down? doesn't look like anytime soon.",
          "score": 3,
          "created_utc": "2026-01-15 19:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpv56w",
          "author": "PAChilds",
          "text": "Bought 1 of each (16g 5060 and 16G 5070ti) just before Christmas for a rig focused on local AI. Glad I did.",
          "score": 5,
          "created_utc": "2026-01-15 12:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpnskx",
          "author": "Adamus987",
          "text": "Bought 2 weeks ago 16gb rtx5060, one of the last ones in good price lol",
          "score": 6,
          "created_utc": "2026-01-15 11:45:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqbtyd",
          "author": "Known_Investment_971",
          "text": "Prices at micro center haven‚Äôt changed yet at least on the 5060 16 sitting at 400$ for the cheapest model. Assuming that‚Äôs going to be changing very soon‚Ä¶",
          "score": 2,
          "created_utc": "2026-01-15 14:15:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqlrpg",
          "author": "No-Replacement-2631",
          "text": "I just checked out of curiosity. The prices have increase 30% where I am.",
          "score": 2,
          "created_utc": "2026-01-15 15:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqu2s9",
          "author": "neutralpoliticsbot",
          "text": "Glad I grabbed one",
          "score": 2,
          "created_utc": "2026-01-15 15:44:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuznr9",
          "author": "Ok-Lengthiness-3988",
          "text": "I'm strongly considering buying my own ASML EUV machine and printing my own GPUs locally rather than buying those unaffordable NVIDIA cards.",
          "score": 2,
          "created_utc": "2026-01-16 04:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztkdg9",
          "author": "vulcan4d",
          "text": "Nvidia 2025: let's give the people what they want, a 5070ti 24GB model\n\nNvidia 2026: screw 'em, take the 16GB model away.\n\n\nI'm no conspiracy theorist but I'm really starting to think the vram/ram limits are on purpose so we don't run competing local LLMs that cut into the profits of cloud providers.  LLMs came out which should have boosted larger VRAM model card production but instead nothing was increased and stock was decreased.  The moment MOE models came out, look what happened to Ram.  Bah",
          "score": 3,
          "created_utc": "2026-01-15 23:17:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpm07t",
          "author": "jacek2023",
          "text": "I use 5070 for desktop, I don't think that's a good choice for LLM, 3090s are much better",
          "score": 2,
          "created_utc": "2026-01-15 11:30:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr38b0",
              "author": "-InformalBanana-",
              "text": "isn't 5060ti 16gb and probably 5070 16gb better for image and video (AI) generation than 3090 cause of fp8 and nvidia's version of 4 bit quantitization (forgot the name)?",
              "score": 3,
              "created_utc": "2026-01-15 16:25:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzr4o6y",
                  "author": "jacek2023",
                  "text": "yes there are cases when my 5070 works better than 3090 in ComfyUI, GGUF works slower on 5070 than FP8, but GGUF means smaller footprint",
                  "score": 2,
                  "created_utc": "2026-01-15 16:32:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzpo1b7",
              "author": "Confusion_Senior",
              "text": "Why ? Memory?",
              "score": 1,
              "created_utc": "2026-01-15 11:47:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpocgi",
                  "author": "jacek2023",
                  "text": "Yes, I use 72GB VRAM",
                  "score": 1,
                  "created_utc": "2026-01-15 11:49:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwj5e3",
          "author": "sascharobi",
          "text": "Fake news?",
          "score": 2,
          "created_utc": "2026-01-16 11:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr6ksq",
          "author": "Material_Policy6327",
          "text": "Glad I grabbed a 5070 ti a few weeks back",
          "score": 1,
          "created_utc": "2026-01-15 16:40:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr97pl",
          "author": "unbruitsourd",
          "text": "Well, it sucks. I'll stay with my 4070 a little bit longer.",
          "score": 1,
          "created_utc": "2026-01-15 16:52:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrpmcr",
          "author": "what_cube",
          "text": "I was seriously considering selling my 5070ti and get two 5060ti yesterday lol",
          "score": 1,
          "created_utc": "2026-01-15 18:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzt8o2p",
              "author": "lemondrops9",
              "text": "Buy the two 5060 ti's today and then sell the 5070ti in a few months.",
              "score": 1,
              "created_utc": "2026-01-15 22:18:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzsu7yb",
          "author": "Iamisseibelial",
          "text": "I am so glad I bought one of each during Black Friday. My goodness I may have essentially won the lotto. \nI just saw the deals on both and snagged one of each, figured that worst case I have a 5070 ti and I can toss a 5060 ti in for some.more vram at x8",
          "score": 1,
          "created_utc": "2026-01-15 21:11:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztsa4p",
          "author": "FullOf_Bad_Ideas",
          "text": "I grabbed 3 rtx 3090 ti's in the last few weeks. Not because I was expecting price increases.  Just found myself having money to expand my rig. I still hope they won't increase in price. But I'll be prepared for gpu winter. I still want more of them though.",
          "score": 1,
          "created_utc": "2026-01-16 00:00:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzu4pd2",
          "author": "ShadowbanRevival",
          "text": "I have one of these and I feel shame because I never use it",
          "score": 1,
          "created_utc": "2026-01-16 01:08:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzu9w6w",
          "author": "grabber4321",
          "text": "grabbed one more 5070 ti this morning at MSRP before they sold out. \n\nnow I need to buy PSU to handle two 5070ti's",
          "score": 1,
          "created_utc": "2026-01-16 01:37:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00n4o1",
          "author": "DayGeckoArt",
          "text": "I know this is disputed but look at retail listings for these cards. They are widely out of stock, in a way I've never seen in the past. I could only find two models of 2 fan RTX 5060 Ti 16gb in stock on Amazon, Newegg, etc. Prices are already climbing, currently about $50-60 more than late last year. There are some 3 fan models and the prices are $150 more than they were a few months ago.\n\nIt's one thing for a manufacturer to say they're not ceasing production, it's another for them to actually be able to get the parts at a price where they can make a profit.",
          "score": 1,
          "created_utc": "2026-01-16 23:47:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o010k68",
          "author": "Radium",
          "text": "Unboxed released a statement today:\n\n>Sorry everyone but we‚Äôve just been provided with ANOTHER clarifying statement from Asus. This one completely walks back their original statement to us\n\n>‚ÄúWe would like to clarify recent reports regarding the ASUS GeForce RTX 5070 Ti and RTX 5060 Ti 16 GB. Certain media may have received incomplete information from an ASUS PR representative regarding these products.\n\n>The GeForce RTX 5070 Ti and GeForce RTX 5060 Ti 16 GB have not been discontinued or designated as end-of-life (EOL). ASUS has no plans to stop selling these models.\n\n>Current fluctuations in supply for both products are primarily due to memory supply constraints, which have temporarily affected production output and restocking cycles. As a result, availability may appear limited in certain markets, but this should not be interpreted as a production halt or product retirement.\n\n>ASUS will continue to support the GeForce RTX 5070 Ti and RTX 5060 Ti 16 GB and is working closely with partners to stabilize supply as conditions improve.‚Äù\n\n>So the current timeline is as follows:\n\n>We request RTX 5070 Ti samples from Asus (and other partners)\n\n>An Asus PR rep looks into it, comes back and says they cannot provide 5070 Tis due to supply constraints, saying their models are ‚Äúend of life‚Äù\n\n>We ask Asus to clarify whether the RTX 5070 Ti is end of life. They confirm it‚Äôs end of life.\n\n>We reach out to retailers to see whether they can purchase RTX 5070 Ti stock for their stores, to fact check Asus‚Äô claims. They say there is no supply.\n\n>Given we have received an on the record statement from Asus and confirmed the supply constraints with retailers, we publish a video with this information\n\n>Nvidia says all GeForce SKUs are being shipped\n\n>Asus reaches out to clarify that Nvidia told Asus that the RTX 5070 Ti is not end of life, but that Asus are ‚Äústreamlining some models‚Äù\n\n>We publish that statement\n\n>Asus reaches out again to provide another statement (the third statement we‚Äôve received from Asus), now saying the 5070 Ti is not discontinued or end of life. This directly contradicts the original statement.\n\n>We immediately request RTX 5070 Ti samples now that the cards are not discontinued or end of life. We haven‚Äôt heard back yet.\n\n>And that‚Äôs where we are currently at.\n\n>At this point, the proof will be in the supply, as we can‚Äôt tell you which of Asus‚Äô statements is truly accurate. We believe the RTX 5070 Ti is heavily supply constrained to the point of being effectively killed, but we‚Äôll see whether that‚Äôs truly the case across the next few months.",
          "score": 1,
          "created_utc": "2026-01-17 01:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpx1jj",
          "author": "Ok_Top9254",
          "text": "32GB V100 SXM + pcie adapter just dropped to like 400 bucks, way better value with double vram tbh.",
          "score": 1,
          "created_utc": "2026-01-15 12:51:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztrohg",
              "author": "FullOf_Bad_Ideas",
              "text": "In a way yes, but I think lots of single gpu users of those rtx 5070 ti want to:\n\n1. Play games\n2. Generate inages/videos\n3. Run LLMs\n\n5070 ti was decent here. V100 can run LLMs, but can it okay games and do image generation well?",
              "score": 2,
              "created_utc": "2026-01-15 23:57:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzubbw8",
                  "author": "Ok_Top9254",
                  "text": "I was generating images with both SDXL and Z-image (Q5 I think) on P40 which is way slower with no tensor cores or fp16 whatsoever. One SDXL image took about 30 seconds and Z-image about 50 seconds to a minute. \n\nV100 is 30% faster than 1080Ti in f32, has 2x cuda F16, plus first gen tensor cores and HBM2 with 850GB/s bandwidth. It should roughly equal RTX 3060Ti with image generation (but much more memory). For games, someone would have to test it. The GV100 with similar architecture can game and LTT even made a video about it, but V100 does not have DirectX drivers. I know that the P40 can game by using a video pass-through laptop style (no video out), but for V100 I have no clue. If someone managed to flash it with GV100 firmware it could work I think.",
                  "score": 1,
                  "created_utc": "2026-01-16 01:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzq5ewg",
          "author": "bbc_her",
          "text": "any advice on what to run for image generation unfiltered/ unrestricted models for 5060 ti 16GB? additionally is it possible to run video generation locally with that GPU?",
          "score": 1,
          "created_utc": "2026-01-15 13:41:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr0qu7",
          "author": "netroxreads",
          "text": "Very likely because the new chips are now in production and will be released soon. TMSC is already churning 2nm chips as we speak. Apple will likely have them ready for their M5 Pro/Max MacBooks by the end of this month, hinted by their planned release of their bundled content creation software around that time. nVidia should be announcing new cards with 2nm chips.",
          "score": 1,
          "created_utc": "2026-01-15 16:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzra0ej",
          "author": "1ncehost",
          "text": "Interesting development. AMD uses GDDR6 instead of GDDR7, so this says to me that AMD will probably take more gamer / local market share since GDDR6 is from older fabs that don't make the newest HBM.",
          "score": 0,
          "created_utc": "2026-01-15 16:56:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzs0w16",
              "author": "dcuk7",
              "text": "Wafer supply is also an issue, it‚Äôs not just about sku manufacturing capacity.",
              "score": 1,
              "created_utc": "2026-01-15 18:55:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzt7bk6",
          "author": "lemondrops9",
          "text": "Its nuts out there. Seen the low end models sell out this morning and prices already going up some. I've been holding out for one more 3090 but picked up a 5060 ti 16 GB just in case I never get one. I guess now is the time to buy or wait for the 6000's ?",
          "score": 0,
          "created_utc": "2026-01-15 22:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpm5n5",
          "author": "prusswan",
          "text": "makes sense... no point wasting that expensive ram on a 5060",
          "score": -11,
          "created_utc": "2026-01-15 11:32:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpnhte",
              "author": "ResponsibleTruck4717",
              "text": "I'm surprised the 5060ti 16gb is still available to purchase.",
              "score": -5,
              "created_utc": "2026-01-15 11:43:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpm04w",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -7,
          "created_utc": "2026-01-15 11:30:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpmvoq",
              "author": "know-your-enemy-92",
              "text": "Hardware Unboxed is not an unboxing channel but long running tech channel similiar to Gamer's Nexus.\n\n\nIn this case they are getting info directly from Asus.",
              "score": 6,
              "created_utc": "2026-01-15 11:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzpnt0v",
                  "author": "SlowFail2433",
                  "text": "LOL I took the name too literally, fair enough \n\n\nI thought they were literally one of those Youtube channels where they open a cardboard box as the content",
                  "score": 1,
                  "created_utc": "2026-01-15 11:45:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzpnilj",
              "author": "rebelSun25",
              "text": "Brother stop.",
              "score": 1,
              "created_utc": "2026-01-15 11:43:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzpmth2",
              "author": "dazzou5ouh",
              "text": "\"an Unboxing channel\"\n\n1.15 million subscribers...",
              "score": -2,
              "created_utc": "2026-01-15 11:37:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qe4so5",
      "title": "Dang, M2 drives are the new DDR5 apparently.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/c8pq1jm6qmdg1.jpeg",
      "author": "Porespellar",
      "created_utc": "2026-01-16 03:18:52",
      "score": 208,
      "num_comments": 96,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzuwljr",
          "author": "hahncholo",
          "text": "Moores law now refers to prices",
          "score": 183,
          "created_utc": "2026-01-16 03:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvli3u",
              "author": "arcanemachined",
              "text": "Altman's Law: The number of dollars required to buy a microchip doubles approximately every 3 months.",
              "score": 80,
              "created_utc": "2026-01-16 06:38:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzw726i",
                  "author": "Any_Pressure4251",
                  "text": "I hope these prices stay elevated for years. \n\nThen the companies will make even bigger investments into the tech, new entrants will come which is happening already Japan, China and small US start ups.\n\nAnd we will all benefit from all the new goodies that will start to be produced.",
                  "score": -34,
                  "created_utc": "2026-01-16 09:50:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzuwmni",
          "author": "Casey090",
          "text": "I'm so happy I kept my old pc. If anything goes up in smoke I have a spare now, better than nothing. Let's see when this craziness ends.",
          "score": 37,
          "created_utc": "2026-01-16 03:45:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvsbho",
              "author": "EuphoricPenguin22",
              "text": "I built a secondary machine with an old mining card and a leftover office PC we had. I haven't put it to work doing much yet, but it definitely would work as a backup if my main 3090 rig putters out. Both use DDR4, too, so I can just swap my higher capacity sticks over if need be.",
              "score": 3,
              "created_utc": "2026-01-16 07:35:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv3w57",
          "author": "RoyalCities",
          "text": "I bought an 8tb last December. It's close to doubled in price in the 6 or so weeks since getting it.\n\nInsane.",
          "score": 28,
          "created_utc": "2026-01-16 04:30:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvvzqd",
              "author": "dreamyrhodes",
              "text": "Yesterday one of my SSD died on me. I now have to pay more than twice as much to replace it. I bought it one year ago at 50 per TB, it should be at somewhere around 40 per TB instead it's more than 100 now.",
              "score": 9,
              "created_utc": "2026-01-16 08:08:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzz2cqw",
              "author": "Dr_Allcome",
              "text": "I'm not sure if i'm happy or sad... i bought three 4tb drives for my nas but was considering getting 8tb drives. If i had gotten the bigger ones i'd definitely be selling them in a week or two, they would have easily made up for the rest of the server.\n\nShit, i also bought 128gb of ddr5 for it shortly before the prices increased... i could sell half my nas and buy a car.\n\nThis is just stupid.",
              "score": 4,
              "created_utc": "2026-01-16 19:10:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuvxmb",
          "author": "dgibbons0",
          "text": "Yeah i was just bitching about this today too. Order history shows me spending $150 last year for a drive that's now $450.",
          "score": 48,
          "created_utc": "2026-01-16 03:41:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuts5b",
          "author": "XiRw",
          "text": "I‚Äôm tired of this shit",
          "score": 62,
          "created_utc": "2026-01-16 03:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvae5q",
              "author": "AlexysLovesLexxie",
              "text": "Same. But Samsung and Micron saw that one of their major purchasers - Nvidia - has over-promised on GPUs for OpenAI'a datacenters. Instead of saying \"No, we can't supply what you're asking for\", they decided to drop any pretense that they give a fuck about the consumers.  \n\nIt's kinda pathetic, to be honest.",
              "score": 37,
              "created_utc": "2026-01-16 05:14:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvltn7",
                  "author": "RuthlessCriticismAll",
                  "text": "it's just markets... what do you expect?",
                  "score": 23,
                  "created_utc": "2026-01-16 06:40:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvbeau",
          "author": "Real-Technician831",
          "text": "I bought 2TB Samsung drive for 75‚Ç¨ while on a trip in Japan as a souvenir, I am glad I did.",
          "score": 15,
          "created_utc": "2026-01-16 05:21:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwr3xd",
              "author": "howardhus",
              "text": "SAMSONURUUU!",
              "score": 5,
              "created_utc": "2026-01-16 12:33:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o00nosg",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-16 23:50:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o02ejdu",
                  "author": "Real-Technician831",
                  "text": "Well it was cheap, and Japan doesn‚Äôt have domestic SSD drive manufacturer. \n\nOf course price was in Yen, but don‚Äôt remember it anymore",
                  "score": 1,
                  "created_utc": "2026-01-17 06:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvk2d2",
          "author": "xXprayerwarrior69Xx",
          "text": "yeah i work for a distributor and we do memory (samsung retail range) and to quote the account manager \"it's just a start\"",
          "score": 8,
          "created_utc": "2026-01-16 06:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv1wa4",
          "author": "arades",
          "text": "Well yeah, people have been saying this since Micron and Samsung both announced they would cut consumer sales this year. Those are memory and storage manufacturers, this was inevitable.",
          "score": 14,
          "created_utc": "2026-01-16 04:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv70dk",
          "author": "Guilty_Rooster_6708",
          "text": "Nand shortage so real",
          "score": 6,
          "created_utc": "2026-01-16 04:51:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvhbnx",
          "author": "Infninfn",
          "text": "NAND was always going to be next in line. It‚Äôs not as if AI compute clusters don‚Äôt need storage. Watch hard disks go up too as companies decide they‚Äôd rather take the performance hit than pay a premium for NAND. ~~CPUs should be affected at some point as well when AMD and Intel decide that their server CPU business takes priority over their consumer business.~~ I forgot that Nvidia GPU clusters are powered by Nvidia CPUs.",
          "score": 8,
          "created_utc": "2026-01-16 06:04:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvwg7c",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 7,
              "created_utc": "2026-01-16 08:12:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzw65nc",
              "author": "fmlitscometothis",
              "text": "HDDs have been increasing in price for a while üò¢ Recertified drives/ex-enterprise used to offer good value, but some are now at 80% cost of new (which has also gone up).",
              "score": 3,
              "created_utc": "2026-01-16 09:42:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzvydoo",
              "author": "ain92ru",
              "text": "NAND and SSD producers are more diversified than DRAM I think, so less opportunity for a de-facto cartel agreement. If someone exits the consumer market someone else will step in",
              "score": 1,
              "created_utc": "2026-01-16 08:30:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv7i31",
          "author": "TheWebbster",
          "text": "I looked at a few price tracking sites, and also some \"not tracking\", but just plain shopping search sites, looking for a 4tb NVME, Crucial or Silicon Power.\n\nAll the NVME listed at $300 local turned out to be \"not in stock\" or \"add to wishlist\". Yet they are still listed??\n\nAnd the REAL price was $500 or more, for any shop that DID have them in stock.\n\nARGH\n\n\\* prices in local currency, not USD",
          "score": 4,
          "created_utc": "2026-01-16 04:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvbg6o",
          "author": "UtopiaLtd",
          "text": "I'm just glad I grabbed 2 x 4TB 990 evo plus last prime day, it would've cost me $500 more today",
          "score": 3,
          "created_utc": "2026-01-16 05:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvwfbd",
          "author": "EastZealousideal7352",
          "text": "I‚Äôm so glad I upgraded earlier this year, had I waited I would be out a ton more money.\n\nHopefully this passes too and the consumer pc market recovers soon",
          "score": 2,
          "created_utc": "2026-01-16 08:12:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01t36k",
          "author": "n7e_5L_Labs",
          "text": "Damn - I missed this thread, the cheap NVMEs seeam to have sold out...",
          "score": 2,
          "created_utc": "2026-01-17 04:09:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvud2x",
          "author": "a_beautiful_rhind",
          "text": "SSDs were already getting expensive for 6+ months. \n\nI was buying 2TB SATA for $60, then $80 used until it went past $120 and I had to look for good old fashioned hard drives. \n\nNVME was more per GB and likely to be 1TB only. $300 for only 2TB is a lot. It fills up fast.",
          "score": 3,
          "created_utc": "2026-01-16 07:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvgiff",
          "author": "ujah",
          "text": "I'm not good keeping up the price but does HDD price affected as well? Because might need bigger storage than before for all ai stuff.",
          "score": 1,
          "created_utc": "2026-01-16 05:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw3b07",
          "author": "captain_cavemanz",
          "text": "I got a used one, i'll sell it to you for $300 ??",
          "score": 1,
          "created_utc": "2026-01-16 09:15:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw8laf",
          "author": "PersonOfDisinterest9",
          "text": "The only thing that I can think of that will plausibly saver us, it a bunch of companies trying to capitalize on the high prices and flooding the market. \n  \nI have little faith that'll will actually happen to our benefit, as few companies can actually manufacture these things, and we're just as likely to get a flood of bad products, but there *is* a chance that a new crop of tech hardware companies spin up.  \n  \nWe desperately need more competition across the whole hardware market, from foundries to GPU design, to RAM, and storage drives.",
          "score": 1,
          "created_utc": "2026-01-16 10:04:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwdpsg",
          "author": "XeNoGeaR52",
          "text": "I bought one SN850x 2 weeks ago on Amazon for 170 euros (A 40 euros price increase from may 2025) It's now sitting at 232 euros and increasing daily.  \nThe Pcie5 prices are getting insane",
          "score": 1,
          "created_utc": "2026-01-16 10:50:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwfucp",
          "author": "braydon125",
          "text": "I just found two sn850x  2tb at a wal mart an hour away for 180 each i bought two! Gotta hold onto em lol",
          "score": 1,
          "created_utc": "2026-01-16 11:08:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwxhlx",
          "author": "DeArgonaut",
          "text": "Bought a 2tb 9100 pro right before since I figured this would happen. Wonder if it‚Äôs more the overall industry or people like me panic buying",
          "score": 1,
          "created_utc": "2026-01-16 13:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx23bd",
          "author": "BoulderDeadHead420",
          "text": "I thought this is why we have commercial supply and retail supply- businesses should be purchasing their supplies from a product stream separate from non commercial users. \nGuess they really do just want us all on phones using a website or app for everything. Killing the home pc",
          "score": 1,
          "created_utc": "2026-01-16 13:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyx7em",
          "author": "Freonr2",
          "text": "Snagged a 990 Pro 4TB the moment I saw some vague news.  $360 on Dec 14, now $549.",
          "score": 1,
          "created_utc": "2026-01-16 18:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyzsfz",
              "author": "Freonr2",
              "text": "https://camelcamelcamel.com/product/B0CHGT1KFJ?context=search&tp=3m",
              "score": 1,
              "created_utc": "2026-01-16 18:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzj79h",
          "author": "DerFreudster",
          "text": "I panic bought a spare 2 TB Samsung 9100 on sale recently and now it's $50 more and climbing.",
          "score": 1,
          "created_utc": "2026-01-16 20:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzzz3ib",
              "author": "Porespellar",
              "text": "I was going to build an inference box in the near future, so I went ahead and grabbed an M2 2TB Gen5 while they were at $239. Glad I did even though I wasn‚Äôt really ready to do the full build yet.",
              "score": 2,
              "created_utc": "2026-01-16 21:43:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o000rd7",
                  "author": "DerFreudster",
                  "text": "I got a 4 TB 9100 for $369 in November. Now I see it for $645. Ugh. Just under the wire.",
                  "score": 1,
                  "created_utc": "2026-01-16 21:51:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o005smb",
          "author": "technofox01",
          "text": "You can say that again. I was checking out 2tb nvmes for my server. Well.... That ain't happening now.",
          "score": 1,
          "created_utc": "2026-01-16 22:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00ezof",
          "author": "daHaus",
          "text": "If you're expecting flash memory to perform as RAM your wallet is going to have a bad time",
          "score": 1,
          "created_utc": "2026-01-16 23:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00s42z",
          "author": "JSWGaming",
          "text": "No shit. It's the same fabs.",
          "score": 1,
          "created_utc": "2026-01-17 00:15:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o018yth",
          "author": "NullKalahar",
          "text": "This has been happening in Brazil for a few days now.\n\n\nFirst, RAM, DDR4 and DDR5, video cards have seen a considerable increase.\n\n\nSSDs, especially M.2, as well.",
          "score": 1,
          "created_utc": "2026-01-17 01:59:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv543k",
          "author": "Firestarter321",
          "text": "Check your local Walmart‚Äôs through the app/website for SN850X, 990 Pro, and M480 Pro drives that can be delivered as many stores till have them in stock for normal prices. ¬†The drives all have DRAM and are excellent options.¬†\n\nI‚Äôve purchased several over the last few days this way.¬†",
          "score": 1,
          "created_utc": "2026-01-16 04:38:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwga0c",
              "author": "braydon125",
              "text": "Same holy shit i coulsnt believe it like a winning lottery ticket",
              "score": 1,
              "created_utc": "2026-01-16 11:12:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzwqsww",
                  "author": "Firestarter321",
                  "text": "Yeah it‚Äôs confusing why they haven‚Äôt raised prices but I‚Äôm glad they haven‚Äôt.¬†",
                  "score": 1,
                  "created_utc": "2026-01-16 12:31:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00317u",
              "author": "DerFreudster",
              "text": "It's crazy that we're all cruising our local stores trying to get our grubby paws on that one last good deal....ugh...",
              "score": 1,
              "created_utc": "2026-01-16 22:02:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o004vmw",
                  "author": "Firestarter321",
                  "text": "I never actually left my house as they all qualified for free same day (or next day at the latest) delivery.",
                  "score": 1,
                  "created_utc": "2026-01-16 22:11:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzwelv6",
          "author": "Desm0nt",
          "text": "How I see it:  \n1) The AAAA game market will die because there will be nothing to run their AAAA-slop on (no one in their clear mind will buy consoles for 1-2k USD) and large game studios will be bankrupt.  \n2) Large game engines will die (because no one needs monsters on UE5 that  lagging on 5090 when no one has video cards) along with all these RTX, DLSS and others.  \n3) The consoles will die because there is no hardware for them either, and their price will alos skyrocket.  \n4) Meanwhile, the ONE-TIME big demand for hardware for building data centers will end (since they are built once, are not updated so often and, let's be honest, they will not be needed in SO MUCH quantity in fact)  \n5) When several AI leaders are determined, the rest of the them will not withstand the competition and will also close, their data centers will go on ebay.  \n6) Manufacturers will remember their old sales market with CONSTANT demand (and not a one-time one), but the market is no longer there - because they themselves killed it.",
          "score": -1,
          "created_utc": "2026-01-16 10:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o014pia",
              "author": "-dysangel-",
              "text": "https://preview.redd.it/mgugv9v0ctdg1.png?width=461&format=png&auto=webp&s=76632e62efa9e6a536081a5b7fe408ef4ae941de",
              "score": 3,
              "created_utc": "2026-01-17 01:32:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzz54bf",
              "author": "Dr_Allcome",
              "text": "I hope valve already has contracts in place for their steam machine parts... i'll be laughing my ass off if they are cheaper than consoles when they release after everyone was ranting they wouldn't be, not because the steam machines got cheaper but because the consoles are now twice as expensive.",
              "score": 1,
              "created_utc": "2026-01-16 19:22:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o08pqnz",
              "author": "Temporary_Type9238",
              "text": "This shits bigger then gaming‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-18 05:10:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvwalj",
          "author": "dreamyrhodes",
          "text": "The production lines used for RAM are also used for NAND. That's why the crisis affects storage as well.",
          "score": 0,
          "created_utc": "2026-01-16 08:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvyxhf",
              "author": "ain92ru",
              "text": "This is inaccurate, they have actually very little in common technology. It's just that when Samsung was thinking which line to shut down and convert to DRAM, they chose the least profitable one, which turned out to be making NAND",
              "score": 5,
              "created_utc": "2026-01-16 08:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzw01s9",
                  "author": "dreamyrhodes",
                  "text": "Maybe my wording wasn't clear so let my LLM word it more clearly\n\n\n\nhttps://preview.redd.it/erlr5kwbcodg1.jpeg?width=1281&format=pjpg&auto=webp&s=3ce1082e7972ddd1183a72c0eca8aa9882f78d4f",
                  "score": 0,
                  "created_utc": "2026-01-16 08:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxd2vh",
          "author": "Vitringar",
          "text": "Who is paying those prices?",
          "score": 0,
          "created_utc": "2026-01-16 14:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuxex8",
          "author": "ortegaalfredo",
          "text": "This is pure speculation, AI do not require storage at all. Next time they will increase price of screens and blame llms.",
          "score": -15,
          "created_utc": "2026-01-16 03:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzuym5q",
              "author": "SkyFeistyLlama8",
              "text": "For caching they do. AI data centers use massive amounts of HBM for GPUs, RAM for servers and NAND flash for storing prompt text and cached prompts.",
              "score": 15,
              "created_utc": "2026-01-16 03:57:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzv3442",
              "author": "RoyalCities",
              "text": "My guy NVME uses NAND. Same upstream supply constraints.",
              "score": 11,
              "created_utc": "2026-01-16 04:25:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzv0pw4",
              "author": "gnnr25",
              "text": "Oh, you want a mouse? $350",
              "score": 8,
              "created_utc": "2026-01-16 04:10:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvgtlo",
                  "author": "CompetitiveSal",
                  "text": "mouse as a service (MaaS) only $50 / mo",
                  "score": 3,
                  "created_utc": "2026-01-16 06:00:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzv2f65",
              "author": "mtmttuan",
              "text": "Storage as a whole is in demands. LTT said in a wan show that youtube and others media platform are struggling to get new drives as AI boom also means more AI slop videos that still need to be stored somewhere.",
              "score": 4,
              "created_utc": "2026-01-16 04:21:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o01y36z",
              "author": "Lissanro",
              "text": "AI needs a lot of storage. For example, just for my most used models K2 0905 (IQ4 quant) and K2 Thinking (Q4\\_X quant) I need over 1 TB. Now imagine training: usually at least FP8, and doing multiple checkpoints... you will need much more than that. I also have collection of stored context cache files for my most used long prompts, worklfows or long chats that I may need to return to. All of this need fast storage like NVMe drives (because loading K2 from NVMe takes few minutes, while loading from HDD hours; HDDs are still useful for backups though).",
              "score": 1,
              "created_utc": "2026-01-17 04:44:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv3ter",
          "author": "misterflyer",
          "text": "Sorry, but if there's something that's used in data centers that you didn't buy by Black Friday 2025, then that's really on you.\n\nThere was more than enough writing on the wall and preliminary forecasting that prices would go up.\n\nBought 2x4TB NVMe's between the beginning of the RAM price surge and Black Friday.  Honestly, I didn't actually need the drives at the time. But I pulled the trigger early simply bc I didn't want to pay the outrageous prices I knew everyone else would be paying post-BF.  And by doing so I literally saved hundreds of dollars.\n\nI don't know what ppl were waiting for... for big tech companies to grow a heart and not screw over consumers? ü§∑üèª‚Äç‚ôÇÔ∏è lol",
          "score": -10,
          "created_utc": "2026-01-16 04:30:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvx7i4",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-01-16 08:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzy5ox8",
                  "author": "misterflyer",
                  "text": "I knew I'd need them in 2026, just not right at that time.  Plus I had the budget for it.  \n\nI didn't say I had no use for them or that I wouldn't be using them.  Trust me, they're being used.",
                  "score": 0,
                  "created_utc": "2026-01-16 16:45:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcv304",
      "title": "NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/2nikcyj6ycdg1",
      "author": "TeamNeuphonic",
      "created_utc": "2026-01-14 18:26:19",
      "score": 207,
      "num_comments": 44,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nzl4l0h",
          "author": "work_urek03",
          "text": "Can you finetune for other languages?",
          "score": 19,
          "created_utc": "2026-01-14 18:44:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmb0us",
              "author": "TeamNeuphonic",
              "text": "Finetune script is on our GitHub! People have released some in huggingface as well - we open sourced multilingual data so it should be around",
              "score": 6,
              "created_utc": "2026-01-14 21:56:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzms2je",
                  "author": "justicecurcian",
                  "text": "What hardware would it require to finetune this?",
                  "score": 1,
                  "created_utc": "2026-01-14 23:19:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl5jbe",
          "author": "FullstackSensei",
          "text": "Can we pretty please get something like this trained for (single) European languages?\nThe landscape for European languages TTS is pretty barren if you need something that works with llama.cpp. There's Orpheus, but that hasn't been updated in 70 LLM years.",
          "score": 22,
          "created_utc": "2026-01-14 18:48:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmb54a",
              "author": "TeamNeuphonic",
              "text": "Yeah maybe we should‚Ä¶I‚Äôll speak to the team!",
              "score": 6,
              "created_utc": "2026-01-14 21:56:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmnz16",
                  "author": "FullstackSensei",
                  "text": "Please do! And if you need any help with other languages, I'm sure a lot in the community would love to help.",
                  "score": 2,
                  "created_utc": "2026-01-14 22:58:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzlvuvi",
              "author": "combrade",
              "text": "For French, I like XTTS .",
              "score": 3,
              "created_utc": "2026-01-14 20:47:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzmmenz",
                  "author": "serendipity777321",
                  "text": "Is it the best for french? How big of a model is it",
                  "score": 1,
                  "created_utc": "2026-01-14 22:50:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzn0w13",
              "author": "Donnybonny22",
              "text": "chatterbox is not bad",
              "score": 1,
              "created_utc": "2026-01-15 00:07:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzn2rqc",
                  "author": "FullstackSensei",
                  "text": "Not at all, but the python code runs only on Nvidia or CPU, and on Nvidia you need at least SM7.0, or V100. No love for P40 nor Mi50, and I don't want to keep 3090s on 24/7 just for TTS.",
                  "score": 5,
                  "created_utc": "2026-01-15 00:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlqxc1",
          "author": "nntb",
          "text": "They all sound terrible to me. Not natural and emotionless.",
          "score": 16,
          "created_utc": "2026-01-14 20:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl70wk",
          "author": "kimodosr",
          "text": "Can finetune for other languages?",
          "score": 5,
          "created_utc": "2026-01-14 18:55:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmb2m6",
              "author": "TeamNeuphonic",
              "text": "Yup finetune script is on our GitHub!",
              "score": 2,
              "created_utc": "2026-01-14 21:56:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzmlkop",
          "author": "Innomen",
          "text": "We really need to update what we mean when we say \"human like.\" This area is not keeping pace with video generation. I strongly suspect that the next instance of LTX will basically stomp all current TTS. (Maybe video training was always required. Speech is after all primarily an in-person audio visual activity and most speech is arguably nonverbal.)",
          "score": 4,
          "created_utc": "2026-01-14 22:46:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nznx79t",
              "author": "misterflyer",
              "text": "\"Human like\" in 2026 means *\"a noticeably robotic voice doing a half-assed weak impersonation of a human voice\"*",
              "score": 4,
              "created_utc": "2026-01-15 03:11:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzp85fa",
                  "author": "CheatCodesOfLife",
                  "text": "Doesn't have to be like that \n\nhttps://vocaroo.com/1nLxIsf46BY9\n\nhttps://vocaroo.com/16CzY0I5ryDj",
                  "score": 2,
                  "created_utc": "2026-01-15 09:23:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzl25rx",
          "author": "Slow_Concentrate3831",
          "text": "Interesting, too bad it's only english though",
          "score": 6,
          "created_utc": "2026-01-14 18:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmb7jk",
              "author": "TeamNeuphonic",
              "text": "Which languages are you after?",
              "score": 3,
              "created_utc": "2026-01-14 21:56:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmiqbh",
                  "author": "Slow_Concentrate3831",
                  "text": "French personally but I'm sure many fellow here would be interested for german, spanish, arab etc",
                  "score": 3,
                  "created_utc": "2026-01-14 22:32:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzot2tu",
                  "author": "CommonPurpose1969",
                  "text": "Chinese.",
                  "score": 1,
                  "created_utc": "2026-01-15 07:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzn04gw",
          "author": "Hurricane31337",
          "text": "German language support would be amazing! üî•",
          "score": 3,
          "created_utc": "2026-01-15 00:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmoe0w",
          "author": "mcAlt009",
          "text": "Seems like it would be really cool for prototyping, but it doesn't really speak with a natural cadence. I can clearly tell it's AI generated, but if I'm prototyping out a game I could imagine it's more than enough.",
          "score": 2,
          "created_utc": "2026-01-14 23:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpnopy",
          "author": "martinerous",
          "text": "Tried it in the HF space. The prosody quality is OK-ish, but the cloned voice was not similar enough. I tried a few samples of an elderly man and it sounded much younger in the output.  \nAnd with longer texts it starts rushing like crazy - would need manual sentence splitting, I guess.\n\nAlso, the default ref. sample in the HF space sounds somewhat tinny and wavy. The model can do much better when given a better ref. sample quality.",
          "score": 2,
          "created_utc": "2026-01-15 11:44:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq4e07",
          "author": "Repulsive-Sherbert75",
          "text": "I tried it, and honestly cloning sounded terrible and had much longer processing opposed to something like chatterbox.",
          "score": 2,
          "created_utc": "2026-01-15 13:35:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlffb3",
          "author": "silenceimpaired",
          "text": "Not a fan of custom licenses. I‚Äôll stick with the ones coming out with Apache and MIT",
          "score": 2,
          "created_utc": "2026-01-14 19:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl1l0s",
          "author": "lacerating_aura",
          "text": "Hi, thanks for the open release. sorry for asking before testing, but how does it compare to CosyVoice models?",
          "score": 1,
          "created_utc": "2026-01-14 18:31:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl2txv",
              "author": "TeamNeuphonic",
              "text": "Hi mate! Our model is a lot smaller and faster, and sounds pretty good! Cosyvoice is a great optionand sounds pretty good as well, but I'm not overly sure how well it can run offline on consumer hardware.",
              "score": 1,
              "created_utc": "2026-01-14 18:36:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzl50tl",
          "author": "kkb294",
          "text": "Hi, thanks for the Open release. I have gone through (on mobile) the website, GitHub and hugging face but couldn't find any information on multilingual capabilities and limitations. \n\nDo you have any specific reference where I can learn more about different voices for different languages.? I am more interested in understanding/using for multiple regional (non-dominant)  languages which the major TTS platform doesn't support much.",
          "score": 1,
          "created_utc": "2026-01-14 18:46:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmbdd0",
              "author": "TeamNeuphonic",
              "text": "Which languages? All comes down to the data. We published a finetune script and people have released open source multilingual versions of our models.",
              "score": 1,
              "created_utc": "2026-01-14 21:57:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzq68zk",
                  "author": "kkb294",
                  "text": "I am from India and looking for models that can support Indian languages like Tamil, Telugu, Kannada and Malayalam (other than Hindi). Also, am collaborating with several friends who are working on South-American dialects and some African Dialects.\n\nIf possible, could you please share any open source multilingual references that others have built that matches your standards/design patterns.",
                  "score": 1,
                  "created_utc": "2026-01-15 13:45:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzlmbz5",
          "author": "PostEasy7183",
          "text": "It's pretty exciting to see all the text to speech models coming out as of late. Now we just need something equivalent to 11 labs V3 and then that's a wrap",
          "score": 1,
          "created_utc": "2026-01-14 20:04:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmbge9",
              "author": "TeamNeuphonic",
              "text": "Watch this space üëÄ",
              "score": 1,
              "created_utc": "2026-01-14 21:58:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzlxb61",
          "author": "Windowsideplant",
          "text": "Anyone got a good android apk to run those tts models?",
          "score": 1,
          "created_utc": "2026-01-14 20:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzmbn5n",
              "author": "TeamNeuphonic",
              "text": "Coming soon üëÄ",
              "score": 2,
              "created_utc": "2026-01-14 21:58:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpsvuc",
          "author": "mitchins-au",
          "text": "A TTS model with weights!",
          "score": 1,
          "created_utc": "2026-01-15 12:23:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl8yiw",
          "author": "7657786425658907653",
          "text": "oft, brave choosing that monologue.",
          "score": 1,
          "created_utc": "2026-01-14 19:03:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}