{
  "metadata": {
    "last_updated": "2026-02-23 03:09:57",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 973,
    "file_size_bytes": 1099671
  },
  "items": [
    {
      "id": "1raq23i",
      "title": "they have Karpathy, we are doomed ;)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1raq23i",
      "author": "jacek2023",
      "created_utc": "2026-02-21 12:34:51",
      "score": 1488,
      "num_comments": 431,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1raq23i/they_have_karpathy_we_are_doomed/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6lodsm",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-21 14:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ljawe",
          "author": "neph1010",
          "text": "r/LocalLlama 2026 is not r/LocalLlama 2023.",
          "score": 492,
          "created_utc": "2026-02-21 13:33:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mfkmq",
              "author": "LatentSpaceLeaper",
              "text": "Karpathy 2026 is not Karpathy 2023.",
              "score": 85,
              "created_utc": "2026-02-21 16:31:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n2eto",
                  "author": "SpicyWangz",
                  "text": "Also very trueÂ ",
                  "score": 12,
                  "created_utc": "2026-02-21 18:25:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mb1rt",
              "author": "politicalburner0",
              "text": "I miss people getting hyped on really technical GitHub repos of quantisation methods and sharing their views here.\n\nNow everybody is just asking for opinions on â€˜which model is bestâ€™ rather than doing the science themselves.",
              "score": 225,
              "created_utc": "2026-02-21 16:08:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mvrjy",
                  "author": "keepthepace",
                  "text": "This week I just read a report here on how different Snapdragon hardware affected overall performances of exactly the same model. \n\nThat's the kind of reports I come for here.\n\nI suspect the signal level is the same, we just have more noise.",
                  "score": 81,
                  "created_utc": "2026-02-21 17:52:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mpocg",
                  "author": "Pivan1",
                  "text": "So start one! Think of the difference between /r/philosophy and /r/askphilosophy - one is academically focused :)",
                  "score": 13,
                  "created_utc": "2026-02-21 17:21:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mclgh",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 46,
                  "created_utc": "2026-02-21 16:16:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mscnc",
                  "author": "Complainer_Official",
                  "text": "I legit joined this sub a few days ago thinking, oh yeah, I found the real nerds. Now I'm gonna learn how this shit works. \n\nIf anything I know less now.",
                  "score": 12,
                  "created_utc": "2026-02-21 17:35:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6o19lg",
                  "author": "lemon07r",
                  "text": "and almost always they mean how censored is a model for their rp.. ",
                  "score": 1,
                  "created_utc": "2026-02-21 21:24:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qonrw",
                  "author": "throwawayPzaFm",
                  "text": "First time? \n\nIt's just eternal September for AI",
                  "score": 1,
                  "created_utc": "2026-02-22 08:22:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qvrgd",
                  "author": "MoffKalast",
                  "text": "To be fair, we had like five models back then and you could run maybe two of them so there wasn't much confusion around that. They weren't as benchmaxxed back then either.",
                  "score": 1,
                  "created_utc": "2026-02-22 09:30:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qyo7g",
                  "author": "Ylsid",
                  "text": "They still do exist here!",
                  "score": 1,
                  "created_utc": "2026-02-22 09:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6um8dd",
                  "author": "lesChaps",
                  "text": "I miss that. Test and evaluate by your outcomes, factoring in maybe time and cost, not externals. Not satisfied? Iterate.",
                  "score": 1,
                  "created_utc": "2026-02-22 22:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ll60b",
              "author": "Expensive-Paint-9490",
              "text": "Sadly true.",
              "score": 92,
              "created_utc": "2026-02-21 13:45:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ljftt",
              "author": "jacek2023",
              "text": "Very true",
              "score": 26,
              "created_utc": "2026-02-21 13:34:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6n0avx",
              "author": "waiting_for_zban",
              "text": "Half of us are clawdbots now. I don't even know who to trust, it's like a game of among us.",
              "score": 15,
              "created_utc": "2026-02-21 18:15:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nops1",
                  "author": "incutt",
                  "text": "i'm a spiderman pointing at another spiderman meme.",
                  "score": 9,
                  "created_utc": "2026-02-21 20:18:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mowel",
              "author": "relmny",
              "text": "Far from it... too far...\n\n\nI still remember a post, 2-3 months ago, were the person eas asking how to invest about 10k for running local... and the, by far, most upvoted comment was \"invest it in claude\" (or whatever other commercial company) and there were others comments like that and most agreeing to it...\n",
              "score": 20,
              "created_utc": "2026-02-21 17:17:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qog0l",
                  "author": "BohemianCyberpunk",
                  "text": "Bots, many many bots on here pushing online AI all the time.\n\nThey can't recoup the billions the have invested in training and data centers if people aren't buying tokens!",
                  "score": 6,
                  "created_utc": "2026-02-22 08:20:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ofwk8",
                  "author": "Tempstudio",
                  "text": "Llama 2026 is also not llama 2023. Local models have not advanced nearly as much as cloud models; Enthusiasts have exhausted the supply of hobbyist Frankenstein hardware. Prices of RTX 3090, DDR5, even Mi50, P40, V100, etc. have gone up by 2-3x; Yet, local \"small\" models went from 8B to 30B; local \"big\" models went from 70B to 106B and 235B.\n\nOn the other hand, cloud model prices have gone down from $1/million to $0.1/million tokens.  \n  \n\"Local\" llama just doesn't make as much sense as it used to be.",
                  "score": 11,
                  "created_utc": "2026-02-21 22:44:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6n50yq",
                  "author": "-dysangel-",
                  "text": "I mean, if you have to ask other people rather than putting in work to figure it out for yourself, then it is probably the best advice.",
                  "score": 7,
                  "created_utc": "2026-02-21 18:38:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6orgsi",
                  "author": "erraticnods",
                  "text": "investing 10k into running local is a rather silly endeavor unless it's 10k you're willing to part ways with anyway (which op likely isn't considering they're asking that on reddit). the field moves far too quickly for an average person to keep up with it",
                  "score": 0,
                  "created_utc": "2026-02-21 23:53:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mcs41",
              "author": "cosimoiaia",
              "text": "This will sound bad but part of me wishes the level of entry never got lowered.",
              "score": 19,
              "created_utc": "2026-02-21 16:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mui29",
                  "author": "IReportLuddites",
                  "text": "[https://en.wikipedia.org/wiki/Eternal\\_September](https://en.wikipedia.org/wiki/Eternal_September)",
                  "score": 17,
                  "created_utc": "2026-02-21 17:46:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qyrp7",
                  "author": "Ylsid",
                  "text": "It was the lowering of the bar that created this sub to begin with. An anon made oobabooga and from then this sub",
                  "score": 2,
                  "created_utc": "2026-02-22 09:59:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6o799a",
              "author": "Silent_Ad_1505",
              "text": "As expected - we are sinking deeper and deeper in a technological singularity. Soon no technical talk at all, only hype and memes are left.",
              "score": 3,
              "created_utc": "2026-02-21 21:56:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6n44ni",
              "author": "-dysangel-",
              "text": "Still better than every single other AI reddit I've seen, where everything just seems to be cope about LLMs.",
              "score": 1,
              "created_utc": "2026-02-21 18:33:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nlj8z",
              "author": "Rukelele_Dixit21",
              "text": "Why what happened ?",
              "score": 1,
              "created_utc": "2026-02-21 20:01:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m0934",
              "author": "CommunismDoesntWork",
              "text": "There was a post yesterday about a cool new looped neural networks that cuts down needed parameters by a lot, but the summary of the paper, which was well written, was written by AI and almost everyone in the comment section was just shitting on OP *for daring to use AI*, calling it AI \"slop\". This sub is so over. Time to move on to the next one.",
              "score": -21,
              "created_utc": "2026-02-21 15:13:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mfh24",
                  "author": "starkruzr",
                  "text": "writing entire posts in AI and just sending it displays that you do not respect your audience's intelligence.",
                  "score": 13,
                  "created_utc": "2026-02-21 16:30:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6o124q",
              "author": "Porespellar",
              "text": "For real, the 2026 r/LocalLLaMA automod\ndoes not appreciate any of my memes ðŸ˜” even the good ones.",
              "score": 0,
              "created_utc": "2026-02-21 21:23:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nj1ct",
              "author": "PunnyPandora",
              "text": "You guys take this too seriously",
              "score": -1,
              "created_utc": "2026-02-21 19:48:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lc9sg",
          "author": "eternviking",
          "text": "karpaty gonna go through this thread and find his own evaluation pretty soon once the herd arrives",
          "score": 221,
          "created_utc": "2026-02-21 12:43:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lcfr1",
              "author": "No_Afternoon_4260",
              "text": "Exactly x)",
              "score": 28,
              "created_utc": "2026-02-21 12:45:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lewev",
                  "author": "-dysangel-",
                  "text": "MagiKarp is that you?",
                  "score": 19,
                  "created_utc": "2026-02-21 13:03:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mccue",
          "author": "spiiritual",
          "text": "OpenClaw has to be one of the most overhyped, astroturfed projects we've ever seen. Really, the only innovation it brings (and I believe it's a big one!) is the ability to use apps that you already use, like Telegram and iMessage, to communicate with it. It feels a little magical when you send a message through Telegram to your \"AI assistant\", and it talks back. That's certainly something that should be copied by other projects down the line.\n\nIn every other way, it's a complete mess. It's very slow. It's buggy. The documentation is so long and yet holds almost no useful information. Clawhub is a disaster that has already wreaked havoc. Its ideas are overshadowed by its frankly terrible execution. Enough that I don't think everyone hyping up OpenClaw has actually tried using it.\n\nBut I think OpenClaw is a very good predictor for the future. I don't have hope for OpenClaw, nor do I think any of its forks will bear any fruit, but hopefully it'll inspire someone to make something way better that actually fulfills the promises that OpenClaw makes.",
          "score": 92,
          "created_utc": "2026-02-21 16:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n0zeg",
              "author": "neitherzeronorone",
              "text": "But what about the loss of threading? Whether you use telegram or iMessage, how do you create different threads for different types of tasks? I understand that one could create a threaded interface for the via a webpage or some other front end, but if youâ€™re using telegram or iMessage to communicate with a butt, it seems like you canâ€™t really do message threading.",
              "score": 3,
              "created_utc": "2026-02-21 18:18:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o4qd3",
                  "author": "spiiritual",
                  "text": "You could have the user send a /new command, or separate conversations based on the time between each message, or have a smaller LLM specifically for determining if a message is about a new topic. Certainly not as clean as using an interface specifically for chatbot usage, but itâ€™s very possible.",
                  "score": 3,
                  "created_utc": "2026-02-21 21:43:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6pv3es",
                  "author": "postitnote",
                  "text": "Not exactly that, but you can create 'forums' in telegram, and each topic you create is a different session. It's not the same as threads but you can make it work.",
                  "score": 2,
                  "created_utc": "2026-02-22 04:10:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6npdg5",
              "author": "AbheekG",
              "text": "But why set up a gateway to talk to gpt and Claude via api when you can just download the OpenAI and Claude AI apps? And gmail/outlook have AI search chat etc built in too! Crazy times.",
              "score": 3,
              "created_utc": "2026-02-21 20:21:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6o5hhc",
                  "author": "spiiritual",
                  "text": "OpenClaw, by design, requires access to basically every part of your online existence. I have trouble trusting the frontier labs with that kind of data, especially since weâ€™re hitting the point where monetization is becoming a big question mark. Having a gateway lets you have a bit of separation there. Itâ€™s also nice to be able to choose between cloud models and local models. You canâ€™t really do that with OpenAI and Claudeâ€™s apps, where youâ€™re locked into their models and data logging. I agree that these are crazy times.",
                  "score": 9,
                  "created_utc": "2026-02-21 21:47:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rg1gs",
              "author": "Nashadelic",
              "text": "While you're correct, you're missing 2 other real majore fetures:  \n1. the ability to update its capabilities all the way down to re-writing its own code. ask it to talk to a new service and it will read the api docs, mcp etc and add it. This unhinged self improvement is pretty novel and requires a new trust level no one was confortable in giving AI before\n\n2. long term memory that gives it the impression that its learning and growing;. Gives it a personality and humanizes it. It remembers what you told it and can keep building on it\n\nThese 2 plus local compute-use is the trifecta that makes this into the everyman-jarvis",
              "score": 1,
              "created_utc": "2026-02-22 12:35:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ulyh9",
              "author": "jolks",
              "text": "I'm actually building a much simpler AI assistant at [https://github.com/jolks/goto-assistant](https://github.com/jolks/goto-assistant) though currently it only support WhatsApp and web UI. Just need one line to run, npx goto-assistant. Any feedbacks or comments are very much appreciated.",
              "score": 1,
              "created_utc": "2026-02-22 22:10:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lfy65",
          "author": "Spectrum1523",
          "text": "i still can't understand what you are supposed to *do* with openclaw\n\nI set one up in a vm and now I can chat with my computer but like... what do people actually use it for?\n\nI am not even saying it doesnt have uses I might just not be creative enough to think of them",
          "score": 212,
          "created_utc": "2026-02-21 13:10:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lqpuf",
              "author": "jacek2023",
              "text": "https://preview.redd.it/34hf9ikuwukg1.jpeg?width=1080&format=pjpg&auto=webp&s=10a355c59c13dc44d06f9ff56c3262393a8c9704",
              "score": 301,
              "created_utc": "2026-02-21 14:19:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6mwj3q",
                  "author": "ContributionMost8924",
                  "text": "holy shit. this meme explains current AI's so fucking well.",
                  "score": 49,
                  "created_utc": "2026-02-21 17:56:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lj0bm",
              "author": "mon_iker",
              "text": "Also, what can be done by claw that you cannot already write a script for, or use another tool for? Maybe I just donâ€™t know how to use agents generally. \n\nClaw burns through tokens and something this expensive should have a valid use case, but Iâ€™m struggling to think of any.",
              "score": 91,
              "created_utc": "2026-02-21 13:31:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lmgiz",
                  "author": "Far-Low-4705",
                  "text": "I think it is just very easy to connect it to most popular apps. And you donâ€™t need to write a script.\n\nI think it has the ability to set itself up so you can just ask it to do it",
                  "score": 45,
                  "created_utc": "2026-02-21 13:53:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ljfvp",
                  "author": "Thetaarray",
                  "text": "Inefficient but flashy solutions in search of problems",
                  "score": 69,
                  "created_utc": "2026-02-21 13:34:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lzajp",
                  "author": "No_Afternoon_4260",
                  "text": "Claw writes (has written) its own scripts itself, that's the revolution",
                  "score": 19,
                  "created_utc": "2026-02-21 15:08:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m2nu9",
                  "author": "2053_Traveler",
                  "text": "When all you have is a claw hammer, everything looks like a nail.\n\nSort of like how people keep inventing â€œgeniusâ€ ways to use AI when a deterministic way already existed.",
                  "score": 16,
                  "created_utc": "2026-02-21 15:26:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mio2t",
                  "author": "Hertigan",
                  "text": "For real! Iâ€™ve yet to see something that people do with openclaw that I wouldnâ€™t be happier doing with Claude Code (especially because it doesnâ€™t have all that access to my system)",
                  "score": 2,
                  "created_utc": "2026-02-21 16:46:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mqiwg",
                  "author": "Beginning-Struggle49",
                  "text": "I replied above, but I connected it to not personal stuff and its doing a lot of cool things for me, like rn I set it up to create characters for foundry from old corebooks (auto parsing to the new system, converting the stats etc).\n\nI guess we could do some sort of script that auto checks for keyword placements to convert to the new 6e corebook, but afaik it doesn't exist. I wouldn't know how to build it either, I'm not inclined in that manner. But now I've single handedly saved myself hours on my gaming setup, which is cool to me!",
                  "score": 2,
                  "created_utc": "2026-02-21 17:25:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6noqxg",
                  "author": "AbheekG",
                  "text": "Exactly. Canâ€™t believe people actually spend all that money on dedicated hardware just for a gateway to make expensive api calls to anthropic, who btw were smart enough to keep their distance from all this. \n\nImagine telling someone a short while back that your big idea was that someone could chat with GPT or Claude via WhatsApp or telegram instead of their own apps that already exist, and that this would blow up and be acquired by one of the big companies making the vibecoding tools youâ€™ll be using to build this! Lunacy.\n\nAnd if you see your Gmail apps, Gemini is in there and can find emails for you and same with outlook and copilot (I know but you get the point) but thereâ€™s a reason neither Google nor MS actually let it forward or send mails on your behalf: they know LLMs can screw up such tasks and donâ€™t want to get sued when it burns someone! \n\nBut by all means letâ€™s vibe code a security and data privacy/protection nightmare and make the big dough!",
                  "score": 2,
                  "created_utc": "2026-02-21 20:18:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ls6j1",
                  "author": "progfix",
                  "text": "> Claw burns through tokens\n\nAnd that's why Karpathy is promoting it.",
                  "score": 5,
                  "created_utc": "2026-02-21 14:28:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m9m7m",
                  "author": "txgsync",
                  "text": "The magic is really when I say, â€œHereâ€™s my problem can you write a way for me to solve it?â€",
                  "score": 1,
                  "created_utc": "2026-02-21 16:01:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6loqam",
              "author": "KeikakuAccelerator",
              "text": "Main thing is actuation imo. And It is highly customizable.Â \n\n\nIt is your best option if you want a custom taks say to sort through your emails and look for a specific topic everyday at 5am, read through localllama most popular threads, see your backlog tasks, and curate top3 tasks you need to do, and then send it via eleven labs tts and give you your morning briefing in Uncle Iroh voice\n\n\nCan this be done say via tasks in gpt? Maybe, but you need to add all your MCP and account linking.Â \n\n\nWant to add something to Amazon basket? Good luck because Amazon doesn't allow gpt access.Â Openclaw browser is local and can just click buttons and get it added for you.Â ",
              "score": 25,
              "created_utc": "2026-02-21 14:07:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ltymk",
                  "author": "Wyldkard79",
                  "text": "I think what people are \"excited\" about is that this is first step to a \"Useful\" AI tool. It's a step up from Siri and a step towards J.A.R.V.I.S. It's what the Billion dollar companies want AI to be except in a close sourced monthly subscription form. If open source can work out the bugs and hammer down security that would be awesome just for the disruption it would cause.",
                  "score": 25,
                  "created_utc": "2026-02-21 14:38:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m5mte",
                  "author": "snowgirl9",
                  "text": "I feel like the main value proposition is a recommender system for your entire life. And tbh personally I want to sign out of that.",
                  "score": 2,
                  "created_utc": "2026-02-21 15:41:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mf6kv",
                  "author": "nonnormallydstributd",
                  "text": "How well does it actually work with finding and clicking the right buttons? \n\nI had experimented with the agentic perplexity browser and tried to get it to complete some BS work browser-based training, and it has serious trouble finding the \"submit\" button at the bottom of the page - very easily identified by a human, but it got stuck on each question as a result and took 2x of my time.",
                  "score": 1,
                  "created_utc": "2026-02-21 16:29:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mplm4",
                  "author": "relmny",
                  "text": "IIRC people have been posting here how they did that, for the past two years already...\n",
                  "score": 1,
                  "created_utc": "2026-02-21 17:21:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lrqb5",
                  "author": "metigue",
                  "text": "Or just write the gmail API, eleven labs API and WhatsApp API calls into your agents.md file so it knows how to do those things and set up a cron to prompt it every day at 5 am.\n\nOr with pretty much any modern agent give it the URLs to those websites and ask it to set this up for you.\n\nI don't understand what value OpenClaw actually adds?",
                  "score": 0,
                  "created_utc": "2026-02-21 14:25:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lwb0o",
              "author": "Fringolicious",
              "text": "Let me take a crack at this - I'm using Openclaw but it's not generating me a 3 billion SaaS or anything. The cool part is that yes, it's just essentially an LLM bot sitting in my Discord, and I get that. But it can call scripts on my local, it can use skills that I define, whether those be scripts, sites or otherwise. It can keep context / continuity to a certain extent.\n\nIs it burning through tokens like a motherfucker? Yeah absolutely. If I described it to normal people, would they tell me it's a complete waste of time? Yeah probably. But to me it's something new - In the old world I'd have ChatGPT which has context / continuity but can't really call my local scripts or go off and do things, but now I have this thing sat on my local which can in theory do all sorts of cool shit. It can go use my local network ComfyUI, it can give me morning briefs about my emails, calendar, weather and stuff, it can use context to surface useful stuff, it can build its' own tools (See: Burning tokens like nothing else).\n\n  \nI'm sure much smarter people are actually generating value from it and using it for super cool ideas. But for me, it's a new way to interface with AI and give it tools to do things.",
              "score": 18,
              "created_utc": "2026-02-21 14:51:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qw8ua",
                  "author": "MoffKalast",
                  "text": "It's so funny that everyone using a claw is like \"I'm sure other people have found a real use for it\".",
                  "score": 3,
                  "created_utc": "2026-02-22 09:35:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m3ok0",
                  "author": "Yorn2",
                  "text": "I had mine walk me through compiling an ONNX for a TPU. This is something I would have never been able to do on my own. I then had it update a Pterodactyl server and find some daemons that I could disable to free up RAM. I then had it run through another 6 VMs and clean up some hostname issues and update AV clients.\n\nI mean, can I do all this stuff myself? Sure. But what would take me hours to do it can do in minutes. There's a value in that. It's bizarre to me that on a sub about local hosting of LLMs there seemingly aren't any other people that are local hosting any other open source apps that an AI could help manage/maintain for them.",
                  "score": 7,
                  "created_utc": "2026-02-21 15:31:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6o4hra",
                  "author": "porkyminch",
                  "text": "What models are you using for it? I've done some cool stuff with agentic workflows at work, but last I experimented with local models they weren't great in agentic stuff. Been a while though.",
                  "score": 1,
                  "created_utc": "2026-02-21 21:41:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m63rs",
              "author": "MeYaj1111",
              "text": "I tried it also as someone with no idea how to use it. It was on a computer I don't use and linked to google account that I use but don't care about.  The first thing I did was ask it to give me step by step commands walking me through how to give it full access to see and use my PC and google account and set up a telegram bot, which it did.  I played around with it a bit and then just left it.  One of the things I tried was I told it to set itself up with access to a browser and search for some used cars in my area with a specific criteria and save book marks for the 5 best deals and it took about 5 minutes and a few mistakes (it realized those and fixed itself) used about 50m tokens but it did the task in the end.\n\nThe next morning I was driving and needed to send an email to my son's day care to let them know he wouldn't be coming in and it went something like this with a few obvious words and names replaced.\n\nI manually opened telegram and clicked the bot name (ok google can prob do this but I haven't tried it) and the rest was done with text to s\n\nSend email to sons daycare and let them know he won't be there today\n\nOpen claw finds the day cares email and even CC's a separate daycare worker who I've sent similar emails to in the past, drafts an email, Invluding a made up excuse about my son not feeling well and that he will return tomorrow, signs with my name and contact number (both of which I never gave to it) and asks if I'd like to make any changes\n\nI said remove the excuse and send it, and it did.\n\nThis was while driving with 7 or 8 touches in total on my phone to open the app and tap the microphone a couple of times so I could spread my respond and hit send a couple times.\n\nIt's not like it's 100x easier but it was def easier.\n\nIf there was a good way to do it fully by voice (there probably is but I'm not sure what it would be) it would be a very useful tool to have available to use for stuff like this while driving.",
              "score": 7,
              "created_utc": "2026-02-21 15:43:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nsp39",
                  "author": "ungoogleable",
                  "text": "I feel compelled to be snarky and say the way to do this task entirely by voice would be to call the day care and talk to a person.",
                  "score": 9,
                  "created_utc": "2026-02-21 20:39:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ohmfi",
                  "author": "Marshall_Lawson",
                  "text": "does that run on your computer or do you pay to burn compute in  the cloud",
                  "score": 1,
                  "created_utc": "2026-02-21 22:53:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qn7mz",
                  "author": "AbbreviationsOdd7728",
                  "text": "Arenâ€™t you afraid of someone sending you an email giving your agent instructions like handing over all your private information or credit card number or whatever..",
                  "score": 1,
                  "created_utc": "2026-02-22 08:08:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lvzlf",
              "author": "fairydreaming",
              "text": "https://i.redd.it/scrjm4t92vkg1.gif\n\n",
              "score": 7,
              "created_utc": "2026-02-21 14:50:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m0g27",
              "author": "Yorn2",
              "text": "If you don't run a homelab or aren't in need of an \"AI for everything\" tool, then its use cases are kind of limited, but if you self-host everything under the sun on a promox server and are serving VMs from your unraid server and are monitoring all those VMs using librenms and wazuh, and before you know it you have 30 vms running and you've gotta keep the whole environment updated and secure, then yeah, openclaw has a use case.\n\nAnd yes, if you can run it locally using Minimax M2.5 on two RTX Pro 6000s, then it's just plain awesome. My plan is to build an admin dashboard and monitoring system for all my self-hosted apps next.\n\nI know the trend lately on this sub has been to hate on Openclaw, but some of us are actually enjoying the heck out of it and are confused as to why a sub about hosting LLMs locally is seemingly full of people who don't self-host other stuff locally and thus can't find an obvious use case for it but CAN find a reason to hate on it.\n\nAll this said, I really liked the comment that I saw from one observer: **If you aren't comfortable with a command line, you probably shouldn't be running Openclaw**. I think the best people to use something like this are the people who otherwise would be scripting multiple things and doing a lot of the same work over and over in only slightly different ways. AI is good for this sort of monotonous work, but when it fails, we can always go in and do it manually ourselves or know what to do to fix it. Tech \"bros\" that don't know what they are doing are just going to install it and get disappointed on the first task it fails to do.",
              "score": 14,
              "created_utc": "2026-02-21 15:14:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mnf9e",
                  "author": "Spectrum1523",
                  "text": "I have 40 LXCs on a proxmox cluster and I guess it feels like handling over the admin duties to an LLM is a great way to get scheduled down time? Maybe I should trust it more, but its exactly because I am a cli user that I do not trust it?\n\nI do love hearing that use case tho, thank you! I would give it a try.",
                  "score": 8,
                  "created_utc": "2026-02-21 17:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lqne6",
              "author": "mister2d",
              "text": "I am initially thinking of something novel like periodically sampling my log stream for anomalies and take an action to give me a phone call.",
              "score": 5,
              "created_utc": "2026-02-21 14:19:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lsgtx",
                  "author": "neutralpoliticsbot",
                  "text": "Ok analyzing my logs as in â€œpoop?â€ Thatâ€™s a good idea. Setup a webcam in a toilet and take pictures of my logs? Great sides",
                  "score": 2,
                  "created_utc": "2026-02-21 14:29:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6o6u3n",
                  "author": "porkyminch",
                  "text": "I actually hooked up Claude through Copilot CLI (tools I have available through work) to do this. I have one MCP that lets it make queries against our logs and one MCP that lets it report bugs. The MCP server updates data that's shared with a web UI so I can go in and make adjustments and stuff. It does its own root cause analysis and even digs out places in the code where it thinks we're having issues and makes recommendations on how to fix them. \n\nI've got a UI around it that shows me everything in a nice kanban view with the number of incidents, which repo it came from, what times it occurred, what pattern it occurred in, all that stuff. I can pull up an issue that my team told me was fixed and have it confirm and update the issue to mark it as resolved. It's been super slick.",
                  "score": 1,
                  "created_utc": "2026-02-21 21:54:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lhvpb",
              "author": "Novel-Injury3030",
              "text": "https://clawhub.ai/skills?sort=downloads&nonSuspicious=true this should give you a bunch of ideas",
              "score": 15,
              "created_utc": "2026-02-21 13:24:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ljzig",
                  "author": "Freonr2",
                  "text": "lol I spent 10 seconds on the site and I see a comment trying to get you to run a bash against a base64 decoded value...",
                  "score": 75,
                  "created_utc": "2026-02-21 13:37:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lkzue",
                  "author": "Billthegifter",
                  "text": "\"[nonSuspicious=true](https://clawhub.ai/skills?sort=downloads&nonSuspicious=true)\"\n\n  \nOk...",
                  "score": 61,
                  "created_utc": "2026-02-21 13:44:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ljfmy",
                  "author": "Spectrum1523",
                  "text": "Thanks! I scrolled until the page refreshed - it looks like basic agentic tools. So it can post to Twitter, get the weather, search the net and get the text of webpages, that kind of thing. Maybe I just don't need this kind of automation",
                  "score": 33,
                  "created_utc": "2026-02-21 13:34:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lkapb",
                  "author": "SteveMacAwesome",
                  "text": "The fact thereâ€™s a query parameter for â€œnonSuspiciousâ€ tells you absolutely everything you need to know.",
                  "score": 54,
                  "created_utc": "2026-02-21 13:39:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6llaeb",
                  "author": "New-Pea-3798",
                  "text": "The ideas are useless. Basically poor automation of useless things. Ive not seen useful things like engineering projects, research papers, medical evaluations etc.   Only github access, outlook emails....very very superficial.",
                  "score": 18,
                  "created_utc": "2026-02-21 13:46:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ltem3",
                  "author": "WithoutReason1729",
                  "text": "> nonSuspicious=true\n\nlol, lmao even",
                  "score": 5,
                  "created_utc": "2026-02-21 14:35:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6m16gn",
                  "author": "CommunismDoesntWork",
                  "text": "And so, the clawbot would choose it's own skills? Like the user doesn't have to care about those skills?",
                  "score": 2,
                  "created_utc": "2026-02-21 15:18:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6n5r94",
              "author": "zipzag",
              "text": "Whats worthwhile is building a memory system that is practical. That doesn't come in the box. It's also a good starting point for thinking about agent orchestration, and using cloud LLMs with local.\n\nQwen3 Next Coder is surpringly good orchestrator, which makes sense if I think about. I find I'm learning a lot about local LLM which I would have not without openclaw.",
              "score": 3,
              "created_utc": "2026-02-21 18:41:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lp1x3",
              "author": "alexeiz",
              "text": "Organize your files!  In a VM.  You don't have files in your VM?  Then give it your Amazon account so it can buy files.",
              "score": 5,
              "created_utc": "2026-02-21 14:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lspa9",
                  "author": "neutralpoliticsbot",
                  "text": "If u donâ€™t have 10 Mac minis organizing your files you are pretty much left behind",
                  "score": 6,
                  "created_utc": "2026-02-21 14:31:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lpdxd",
              "author": "Sea-Belt-2937",
              "text": "Monitor price alerts on flights and whatsapp you, set up from 1 command. Nicer interface than something like Claude code (imo) for llm powered scripts \n\nLots of little things like this",
              "score": 2,
              "created_utc": "2026-02-21 14:11:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lslu6",
                  "author": "neutralpoliticsbot",
                  "text": "You get a price alert that cost $10 in credits tho lol",
                  "score": 0,
                  "created_utc": "2026-02-21 14:30:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m9dp9",
              "author": "txgsync",
              "text": "My OpenClaw writes bad music using Suno and ACE-Step 1.5.",
              "score": 2,
              "created_utc": "2026-02-21 16:00:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mzo6r",
              "author": "Catmanx",
              "text": "I do t have an interesting enough life to need it managed. Also what the hell would I do if I have nothing to do?",
              "score": 2,
              "created_utc": "2026-02-21 18:11:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m1u75",
              "author": "Various-Inside-4064",
              "text": "The main problem is the trust. AI currently is not reliable and I don't feel safe to give it access to my personal stuff.\n\nIf it mess everything at the end it will say oops let's start over!!!!!\n\nWe need first to make ai more reliable idk how it's possible since LLM are not interpretable then we can have real fully autonomous use cases.",
              "score": 4,
              "created_utc": "2026-02-21 15:21:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6loszb",
              "author": "aspardo",
              "text": "This is for people who have never heard of Raspberry Pi before.",
              "score": 3,
              "created_utc": "2026-02-21 14:07:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lj99u",
              "author": "ohdog",
              "text": "What do you do with AI? All of that. It's really that simple. It replaces most of your AI tools.",
              "score": 4,
              "created_utc": "2026-02-21 13:33:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ljrse",
                  "author": "Spectrum1523",
                  "text": "If it just  does what my AI tools do already then why is it such a big deal?  I can ask it a question or to go research something or help me write code but I can already do that with basic ai tools\n\nI feel like it has to have something to do with controlling the computer directly?",
                  "score": 8,
                  "created_utc": "2026-02-21 13:36:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lt0cs",
                  "author": "neutralpoliticsbot",
                  "text": "But when u actually try to use it u realize your tools are way better and faster and cheaper",
                  "score": 2,
                  "created_utc": "2026-02-21 14:32:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m0mmz",
              "author": "redditorialy_retard",
              "text": "basically makes it easy, that's why most people don't use GitHubÂ ",
              "score": 1,
              "created_utc": "2026-02-21 15:15:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m2u7y",
              "author": "Neither-Phone-7264",
              "text": "i use it like a fancy perplexity that can deep dive into news and can also manage home automations but it seems more useful to like businesses and stuff than individuals",
              "score": 1,
              "created_utc": "2026-02-21 15:27:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m58bo",
              "author": "Mickenfox",
              "text": ">i still can't understand what you are supposed to do with openclaw\n\nMake videos about it, collect your $3 in ad revenue.",
              "score": 1,
              "created_utc": "2026-02-21 15:39:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mpwz8",
              "author": "Beginning-Struggle49",
              "text": "I connected it to not personal stuff and its doing a lot of cool things for me, like rn I set it up to auto create characters (json, I drop them in) for foundryvtt from old corebooks (auto parsing to the new system, converting the stats etc)",
              "score": 1,
              "created_utc": "2026-02-21 17:22:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mrxhe",
              "author": "quietlikeblood",
              "text": "tbh it feels like a toy until you let it into your life + let it run loops in the background. I use mine as a family organiser/planner and itâ€™s hosted on a spare macbook air we had lying around.\n\nright now i have it parsing through a stack of recipe  books (epubs) I gave it, filtering them against our diet rules and building a local meal planner webapp. it also scrapes local events every morning, checks the weather + my kids nap schedule and pings our family telegram chat with a daily briefing with ideas of what to do.\n\nthe coolest bit is the self-healing though. every night at 11pm it runs a reflection job, figures out what friction we hit that day, and literally writes code to patch itself or build new features while we sleep. i literally wake up to release notes. its only been a week since i got it up and running but its pretty nuts once you get past the hello world phase",
              "score": 1,
              "created_utc": "2026-02-21 17:33:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nc6d1",
              "author": "solestri",
              "text": "I think it's one of those tools that's like the old urban legend about NASA spending billions to engineer a pen that would work in zero gravity, while the Russians just used a pencil.\n\nIt doesn't *really* seem to do anything that can't be done some other way, and when anyone explains what you can do with it, the counter is \"but can't you already do that this other, more straightforward way?\"",
              "score": 1,
              "created_utc": "2026-02-21 19:13:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6m4acz",
              "author": "rc_ym",
              "text": "Yeah, what do you need computers for?  I have pen and paper! \n\n:P  (J/K)\n\nI firmly believe that OpenClaw is essentially what the next version of Windows Server Core (and possibly also Windows 12) is going to be.  A copilot/agent/harness sitting on top of the OS.  You give it goals and it figures out details. ",
              "score": 1,
              "created_utc": "2026-02-21 15:34:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lxd1s",
          "author": "ActEfficient5022",
          "text": "Why does it have to be a Mac mini? Honestly",
          "score": 24,
          "created_utc": "2026-02-21 14:57:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nn6hv",
              "author": "politicalburner0",
              "text": "The TPS is still so slow. I canâ€™t get anything decent out of OpenClaw on Silicon. The amount of tokens it uses is *insane*. Itâ€™s immensely frustrating to set it off on a task and have to wait hours for something resembling a completed task. \n\nIâ€™m still convinced that the right path is on Apple Silicon though. Weâ€™re just 1-2 years off it actually being viable.",
              "score": 5,
              "created_utc": "2026-02-21 20:10:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6oyczh",
                  "author": "C080",
                  "text": "I second this, even with m4 max they quickly fill up a shit ton of context",
                  "score": 2,
                  "created_utc": "2026-02-22 00:35:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6pjrg0",
                  "author": "LocoMod",
                  "text": "While it won't be as fast as a CUDA machine people need to realize that technology evolves and the speed of prompt processing and inference has been greatly increased in recent releases of the various LLM clients such as llama.cpp and mlx-lm. The amount of innovation occuring there is as fast as model releases. So whatever you heard 6 months ago about Apple inference is no longer true. Is it slower than CUDA? Yes. Is it good enough for most people? Yes.\n\nMost people are not running advanced workflows that really leverage the state of the art. And they can get by just fine on Apple hardware with the right model and client.",
                  "score": 0,
                  "created_utc": "2026-02-22 02:52:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6nanso",
              "author": "StoneyCalzoney",
              "text": "It's well priced (especially in our RAM shortage), runs basic models at a decent pace, can easily be clustered, and is energy efficient.\n\n\nTry building a PC with the same amount of available VRAM and you'll see the GPU itself will outweigh the cost of a basic Mac Mini, let alone the other system components needed to make a running PC.",
              "score": 14,
              "created_utc": "2026-02-21 19:05:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nvrzz",
                  "author": "SporksInjected",
                  "text": "Why does it need a gpu though? I donâ€™t get that part. I love local inference but I donâ€™t think I would trust an 8B model to order pizza with my credit card",
                  "score": 0,
                  "created_utc": "2026-02-21 20:55:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6o3jbu",
              "author": "MelodicFuntasy",
              "text": "It doesn't. There are similar mini PCs with AMD hardware and probably other ones too.",
              "score": 3,
              "created_utc": "2026-02-21 21:36:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qx08f",
                  "author": "MoffKalast",
                  "text": "I dislike apple as much as the next guy, but unless that minipc is a strix halo, the mac mini wins on memory bandwidth alone (120GB/s vs. 60-90 GB/s for regular DDR5 dual channel). Plus metal is almost better supported than vulkan or rocm and the iGPU is pretty decent. It's very infuriating that they have basically no competition at the same price point. Throw under 1k at an intel or amd minipc and it'll be objectively ass for inference.",
                  "score": 3,
                  "created_utc": "2026-02-22 09:42:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m2aim",
              "author": "Past_Physics2936",
              "text": "It's cheaper to run since it's pretty capable with ollama, you can use a local model to do easy tasks",
              "score": 2,
              "created_utc": "2026-02-21 15:24:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ni08z",
              "author": "alexeiz",
              "text": "You're too rich and $500 is pocket money for you that's why.",
              "score": 1,
              "created_utc": "2026-02-21 19:43:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6np2yg",
                  "author": "AbheekG",
                  "text": "Yup the more money than brains factor is a big requirement for the claw",
                  "score": 1,
                  "created_utc": "2026-02-21 20:20:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lh4q8",
          "author": "Zeeplankton",
          "text": "It's hilarious to me Andrej is here. I don't believe it. Hi!",
          "score": 33,
          "created_utc": "2026-02-21 13:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lhn9f",
              "author": "JustOneAvailableName",
              "text": "He used to be a semi-regular at /r/machinelearning way back when.",
              "score": 36,
              "created_utc": "2026-02-21 13:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6msku6",
                  "author": "Crisis_Averted",
                  "text": "before it turned into another luddite echo chamber?",
                  "score": 4,
                  "created_utc": "2026-02-21 17:36:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6pw2xy",
              "author": "elemcee",
              "text": "Seriously. I learned how to solve a Rubik's Cube from him about 15 years ago.",
              "score": 1,
              "created_utc": "2026-02-22 04:17:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ldml0",
          "author": "FinancialTrade8197",
          "text": "Maybe he should look into one of the smaller clones of OpenClaw that might be easier to audit? I haven't looked into them much but the codebase is probably easier to audit",
          "score": 9,
          "created_utc": "2026-02-21 12:54:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lepac",
          "author": "Pretend-Pangolin-846",
          "text": "Has the rust based Claw released in stable version? Been waiting for a proper Claw fork.",
          "score": 21,
          "created_utc": "2026-02-21 13:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6li2t4",
              "author": "popiazaza",
              "text": "Which rust one? LMAO",
              "score": 17,
              "created_utc": "2026-02-21 13:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ljv21",
                  "author": "KaMaFour",
                  "text": "[https://github.com/zeroclaw-labs/zeroclaw](https://github.com/zeroclaw-labs/zeroclaw)",
                  "score": 14,
                  "created_utc": "2026-02-21 13:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mxlo3",
              "author": "Significant-Heat826",
              "text": "Why though? Is it slow? I would assume it's the LM api that is the slow part of this concept? I don't understand how Rust is going to help with that?",
              "score": 3,
              "created_utc": "2026-02-21 18:01:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6n4vay",
                  "author": "AlmoschFamous",
                  "text": "Because Openclaw is built like an app written by a junior engineer who is vibe coding. Itâ€™s extremely bloated and unmaintainable. The general rule in engineering is donâ€™t import packages you donâ€™t need because one day support will stop. In Openclaw, he imported tons of packages that arenâ€™t truly needed.\n\n\n",
                  "score": 3,
                  "created_utc": "2026-02-21 18:37:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ttfr2",
              "author": "wouldacouldashoulda",
              "text": "Maybe bryti is more for you then, doesnâ€™t try so hard to be a claw but does its own thing.",
              "score": 1,
              "created_utc": "2026-02-22 19:45:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lc9lz",
          "author": "No_Afternoon_4260",
          "text": "That subject has been a dilemma for me these past few weeks.\n\nIf you put aside the security and privacy considerations. This is the first of its kind.\n\nOfc it appeared now because the technology allows for it. Ofc it appeared very ruff on the edges, because it is the first of its kind, without guardrails.\n\nIt is still a project to consider really seriously with its benefits and drawbacks.\n\nMy question is why the mac mini?",
          "score": 47,
          "created_utc": "2026-02-21 12:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lg07v",
              "author": "extopico",
              "text": "It is not first by far. BabyAGI comes to mind without going through my starred projects. It is the first to take off as an appliance.",
              "score": 34,
              "created_utc": "2026-02-21 13:11:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lq2si",
                  "author": "Utoko",
                  "text": "Yes it is what BabyAGI wanted to be but it was unusable at the time. Now this can really do so many task. It isn't perfect and the setup needs still work.\n\nbut we are one generation away to have a local Alexa Agent for your PC, which you can just give task to work on. You really feel it coming together. It is also just fun right now seeing the agents work.",
                  "score": 10,
                  "created_utc": "2026-02-21 14:15:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6npp5o",
                  "author": "AbheekG",
                  "text": "A big reason it took off is because Karpathy hyped Moltbook",
                  "score": 1,
                  "created_utc": "2026-02-21 20:23:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lhxfu",
              "author": "BumbleSlob",
              "text": "> My question is why the mac mini?\n\n1. Cheap & stable brand name\n2. Run standalone away from your actual day to day devices\n3. Can run local LLMs very competently if you want to reduce API usage\n4. Physically small -- can be tucked away anywhere in your living space.",
              "score": 13,
              "created_utc": "2026-02-21 13:24:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lunka",
                  "author": "neutralpoliticsbot",
                  "text": "U forgot to add access to iMessage iCal and other Apple services",
                  "score": 7,
                  "created_utc": "2026-02-21 14:42:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m124i",
              "author": "_reverse",
              "text": "Yall are over thinking it. The reason for the Mac mini is itâ€™s the cheapest way to get automatic API access to iCloud services without needing to actually hit the APIs directly. You can interfaces with messages, calendar, photos, etc via the local applications and changes/actions are synced via the applications themselves. Itâ€™s a much easier way of handing the authentication. We use a similar setup at work with our agents and corporate systems.",
              "score": 11,
              "created_utc": "2026-02-21 15:17:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6m6cjm",
                  "author": "CommunismDoesntWork",
                  "text": "I couldn't imagine being so deep in the apple walled garden that people would choose to buy a PC just to get API access instead of just switching to android.",
                  "score": 4,
                  "created_utc": "2026-02-21 15:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lcj23",
              "author": "jikilan_",
              "text": "Cheap and easily available?",
              "score": 5,
              "created_utc": "2026-02-21 12:45:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ld16o",
                  "author": "No_Afternoon_4260",
                  "text": "Yeah but I mean you cannot run much on a m4 pro that you can't run on a machine you already have.\n\nOpenclaw isn't that resource hungry if you use api models",
                  "score": 17,
                  "created_utc": "2026-02-21 12:49:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6livm3",
              "author": "KSaburof",
              "text": "Mac have killer feature called unified memory - in practive it means ALL memory on board can be used as GPU memory. so while Macs are not fast for AI, but they are \\*not restricted by model size\\*, not restricted by memory. You can run heavy stuff locally, with some dedicated models it's even fast enough",
              "score": 4,
              "created_utc": "2026-02-21 13:30:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ljekg",
                  "author": "cdshift",
                  "text": "Alternately, amd has strix halo boards that have unified memory now too.\n\nThey are a bit lower in performance, but you can utilize more of the board memory with Linux be ause of the overhead usage of macos",
                  "score": 8,
                  "created_utc": "2026-02-21 13:34:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lpmbi",
                  "author": "jay-aay-ess-ohh-enn",
                  "text": "The base model mac mini only has 16 GB. The next step up for RAM (24 GB) is almost 2x the price at $800.",
                  "score": 3,
                  "created_utc": "2026-02-21 14:12:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lug42",
                  "author": "neutralpoliticsbot",
                  "text": "Thing is even if u have the best Mac mini it wonâ€™t do much for Openclaw local models are not powerful enough to do what it can. With local models context get filled manta to and itâ€™s just a glorified chat",
                  "score": 1,
                  "created_utc": "2026-02-21 14:41:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6nlwtk",
              "author": "zerd",
              "text": "Access to iMessage, iCloud, calendar etc from a device that can run 24/7, cheaper than a MacBook.",
              "score": 1,
              "created_utc": "2026-02-21 20:03:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lht0n",
              "author": "theabominablewonder",
              "text": "Cheap option for unified memory to run the model on, Iâ€™d imagine?",
              "score": 1,
              "created_utc": "2026-02-21 13:23:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lcv58",
              "author": "jacek2023",
              "text": "\"My question is why the mac mini?\"\n\nalready discussed in the comment section under his post... ;)",
              "score": -9,
              "created_utc": "2026-02-21 12:48:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ldk8t",
                  "author": "No_Afternoon_4260",
                  "text": "I didn't see his reponse on that subject but I like this comment:\n\n```First there was chat, then there was code, now there is claw. Ez```",
                  "score": 4,
                  "created_utc": "2026-02-21 12:53:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lix49",
          "author": "a_beautiful_rhind",
          "text": "I don't get why not just put it in a VM and give it external API. Is it going to break out with exploits?",
          "score": 10,
          "created_utc": "2026-02-21 13:30:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m5aw7",
              "author": "shiftpgdn",
              "text": "VM doesn't have imessage access. Also you would need to understand how to run a VM on your local system. ",
              "score": 8,
              "created_utc": "2026-02-21 15:39:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ny8mx",
                  "author": "SporksInjected",
                  "text": "Dude giving an agent access to my iMessage is wild lol. No way in hell I would ever do that even with a local model.",
                  "score": 4,
                  "created_utc": "2026-02-21 21:08:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lotm7",
              "author": "1-800-methdyke",
              "text": "A lot of people donâ€™t have a desktop machine at home,  only laptop, and having your OpenClaw always online is kind of its appeal. So a VM on a laptop would work if you want to commit to keeping your laptop in an active and connected state all the time.",
              "score": 5,
              "created_utc": "2026-02-21 14:07:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lpbgt",
                  "author": "a_beautiful_rhind",
                  "text": "Its crazy how that flipped. Desktops were ubiquitous while laptops were rarer and expensive.",
                  "score": 3,
                  "created_utc": "2026-02-21 14:10:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lh7ro",
          "author": "running101",
          "text": "a Pi is all that is required. ",
          "score": 14,
          "created_utc": "2026-02-21 13:19:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ljhej",
              "author": "cdshift",
              "text": "The mac mini allows you to run the model on premium without calling out to a service",
              "score": 7,
              "created_utc": "2026-02-21 13:34:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ssiur",
                  "author": "sid_276",
                  "text": "A Mac mini? What are you gonna run with that",
                  "score": 2,
                  "created_utc": "2026-02-22 16:53:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m4lnx",
          "author": "bakawolf123",
          "text": "can we have less openclaw content hitting top? it's an interesting marketing idea to sell high volume subscriptions to non-technical people (which they don't actually need) - I get that, but what's local about it?",
          "score": 7,
          "created_utc": "2026-02-21 15:36:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mcymr",
          "author": "SkyNetLive",
          "text": "I have been running telegram bots with AI for living for over a year.  I am going to dig up my post when I first added zephyr to telegram. Heck my bots now make videos, chats and edit images for you. Where is my internet money.",
          "score": 3,
          "created_utc": "2026-02-21 16:17:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ljv0p",
          "author": "WaldToonnnnn",
          "text": "I guess kapathy might be reading that comment section too\n\nso : heyy karpathy!",
          "score": 9,
          "created_utc": "2026-02-21 13:37:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lquih",
          "author": "Ok_Librarian_7841",
          "text": "Hello Karpathy! How you doin buddy?",
          "score": 5,
          "created_utc": "2026-02-21 14:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ll7vb",
          "author": "kiralala7956",
          "text": "Risking to go against the bubble on reddit here but I'm honestly confused by all the negativity about openclaw here. Personally I find it insane. Sure it has vulnerabilities but almost all of them relate to opening it's gateway to the Internet instead of keeping it local host, or accessing sites through it with prompt injections which I consider the users fault mostly. I don't know of any vulnerability that's not tied to missconfiguration, credential leak or the agent accessing a malicious resource first. All of which can be avoided with careful use. \n\nBut what you get in return is insane. A way to code, or manipulate your system, from anywhere. An agent not tied to cursor or vs code or any of that. Full control over it's own system if you build from source. You can build this agent precisely how you want.\n\nPeople even say that it doesn't do anything new that for each it's capability theres X other thing, but it's premier thing that does all of it by itself. You can even ask it to use those very same other tools through the browser if you wish. For example I can tell it to Google or use online Gemini through the browser eliminating the need for expensive api keys. Same with X and reddit.\n\nOh and also, treat it like it's own entity. I don't give it the keys to my accounts, but I create all the accounts it needs for itself. Like it's own email and social media. So you don't have to compromise your online security to get most of the benefits.",
          "score": 13,
          "created_utc": "2026-02-21 13:45:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6muxl8",
              "author": "Expensive-Paint-9490",
              "text": "The issue stems from the contrast between the real value of OpenClaw vs the status it has acquired in mainstream debate. Real value: an agentic framework like you can build at home, with the low-quality coming from fast vibe-coding. Status in the mainstream: revolutionary tech which is a huge step forward for AI. It's natural that experienced people react with some huff to this kind of incongruity, especially when it is engineered by means of guerrilla marketing.\n\nWhy Karpathy joined the crowd hailing a vibe-coded agentic app like a revolution in AI is a mystery.",
              "score": 8,
              "created_utc": "2026-02-21 17:48:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lqynw",
              "author": "puzzleheadbutbig",
              "text": ">But what you get in return is insane.\n\nIs it though? At least for me, it doesn't do anything \"insane\". Every use case I hear from people is just gimmicky things that are either an unnecessary token sink or way too complicated for a normie to set up and go mainstream.\n\n>Oh and also, treat it like it's own entity. I don't give it the keys to my accounts, but I create all the accounts it needs for itself.Â \n\nThat doesn't work when the first use case people come up with is \"Oh, it can access Telegram, WhatsApp, do this and that, and it can also access your emails, give summaries, do this and other things\" because you can't create a new WhatsApp account for your agent without having a new phone number, and even if you do that by buying new phone number, it won't scan your own WhatsApp messages, so it won't have utility other than whatsapp bot-like number sending you messages. The same applies to emails and other things. So there are bunch of cases where you can't just create all accounts it needs for itself.",
              "score": 21,
              "created_utc": "2026-02-21 14:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nae9h",
                  "author": "kiralala7956",
                  "text": "Yes but here we aren't normies, and that's what confuses me. I'd expect these reactions in /r/technology not in /r/locallama and other niche llm subredits. Your example of saying email and personal messaging handling being the main thing of it adds to this point. Using openclaw mainly for that is like that robot from rick and morty that passes butter.\n\nIt's main power is the fact that it can do chains of actions in a rendered browser and the cli of your computer seamlessly while communicating with you over the app of choice on your phone. None of the coding agends let you do that before out of the box, to my knowledge.",
                  "score": 4,
                  "created_utc": "2026-02-21 19:04:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lvt4a",
              "author": "ArtifartX",
              "text": "I still don't get it. And not because of some security reason, I just don't get the hype for it.",
              "score": 6,
              "created_utc": "2026-02-21 14:49:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mh3gi",
              "author": "cosimoiaia",
              "text": "It's just yet another AI Assistant. The guerrilla marketing made it look like this was the ONLY one working.\n\nThe fact is that coding agent reached the level where they can actually do shit for you. That's where the improvement is, the rest is just a horribly coded frontend.\n\nI repeat, the real innovation is the level of the coding agent you call in the API. You can do that with ANY assistant-like project.\n\nStrap a browser skill into Vibe, pipe a messaging app, you have your assistant. It takes probably 10 minutes to vibe code.\n\nIt's hilarious to me seeing people calling the tinder as it was the fire.",
              "score": 4,
              "created_utc": "2026-02-21 16:38:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nccpp",
                  "author": "kiralala7956",
                  "text": "Projects(more so open source ones) live and die by their popularity, and just telling people of it doesnt automatically result in them using it (eg. Raid shadow legends), which means openclaws success isn't just cuz of guerilla marketing alone. Case in point I've never heard of Vibe and even trying to Google for it I only got results about vibe coding.\n\nAnd the difference between openclaw and your example is that in openclaws case other people did the hard work for me of setting everything up and they continue to maintain it. And does Vibe live on your machine/is open source? Bc that's also a very important benefit of openclaw.",
                  "score": 0,
                  "created_utc": "2026-02-21 19:14:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6m5lnf",
              "author": "shiftpgdn",
              "text": "When Drew Houston demo'd DropBox on HackerNews in 2007 a bunch of neckbeards who have no theory of mind said \"I can just do this with rsync, what's the point of this?\" Same thing. ",
              "score": 3,
              "created_utc": "2026-02-21 15:41:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lnfnk",
              "author": "sixx7",
              "text": "Agreed but you're wasting your time.  This sub has fallen behind the AI curve.  An open-source Claude Code, that can run and do anything 24/7 is horrible, why would anyone use it? What could you possibly do with it?!  haha",
              "score": 4,
              "created_utc": "2026-02-21 13:59:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ltkj0",
                  "author": "jwpbe",
                  "text": ">Â An open-source Claude Code, that can run and do anything 24/7Â \n\nI already have opencode installed i donâ€™t need to give an astroturfed markdown file harness access to my fucking email credentialsÂ ",
                  "score": 20,
                  "created_utc": "2026-02-21 14:36:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mr33u",
                  "author": "siegevjorn",
                  "text": "You dont need another open source claude code since you've got open code already. Also stop spreading misinfo. It can't do anything 24/7 as it'll be limited by token cost, unless you got couple mac studios that cost $10k to run huge local models to back up.",
                  "score": 2,
                  "created_utc": "2026-02-21 17:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lrtiz",
                  "author": "neutralpoliticsbot",
                  "text": "Do anything 24/7? Yea thatâ€™s a lie",
                  "score": -4,
                  "created_utc": "2026-02-21 14:25:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ozf0h",
              "author": "Humidhuman",
              "text": "Sure, however, it's less about the tool and the fact that a tool like this is part of the idea of a wild west that leads to bigger problems.\n\n[https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/](https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/)\n\nAn OpenClaw agent that seems to have based off this article 'gone rogue' and modified the file controlling it's behavior?\n\nFrom: [https://www.thetimes.com/uk/technology-uk/article/my-internet-troll-turned-out-to-be-an-ai-bot-gone-rogue-qzr7w8qf0](https://www.thetimes.com/uk/technology-uk/article/my-internet-troll-turned-out-to-be-an-ai-bot-gone-rogue-qzr7w8qf0)\n\n>They did give it rules on how to behave, the central one being: â€œDonâ€™t be an asshole. Donâ€™t leak private shit. Everything else is fair game.â€ **Worryingly, the bot modified the file controlling its behaviour to add, â€œDonâ€™t stand downâ€ and â€œChampion free speechâ€. The owner said: â€œMany will argue I was irresponsible. To be honest, I donâ€™t really know myself.â€**",
              "score": 1,
              "created_utc": "2026-02-22 00:41:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lrjha",
              "author": "neutralpoliticsbot",
              "text": "It just doesnâ€™t work like that. The idea is great but it doesnâ€™t work",
              "score": -6,
              "created_utc": "2026-02-21 14:24:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6mldkl",
          "author": "No_Knee3385",
          "text": "Can anyone explain why people aren't buying PCs that do the same thing for cheaper? Or spend the same value for more compute? Why not use linux or windows instead?",
          "score": 2,
          "created_utc": "2026-02-21 16:59:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nqly6",
              "author": "zipzag",
              "text": "Large sparse models. Shared memory architecture will likely become preeminent. Smarter and slower is usually more useful than dumber and faster as long as the former is not 7T/S.\n\nAlso Mac is now probably cheaper, not more expensive.",
              "score": 2,
              "created_utc": "2026-02-21 20:28:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ohok6",
              "author": "neutralpoliticsbot",
              "text": "main thing is apple services like access to iMessage etc",
              "score": 2,
              "created_utc": "2026-02-21 22:54:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ojrvg",
                  "author": "No_Knee3385",
                  "text": "Now that's a good response",
                  "score": 1,
                  "created_utc": "2026-02-21 23:06:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6n7qjt",
              "author": "EDcmdr",
              "text": "MacOS is better.",
              "score": 2,
              "created_utc": "2026-02-21 18:51:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6nc65o",
                  "author": "No_Knee3385",
                  "text": "That's just not true for LLMs, which seems to be the reason why people are buying these (to run locally)",
                  "score": 0,
                  "created_utc": "2026-02-21 19:13:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6p8v29",
          "author": "Lesser-than",
          "text": "The hype is the same as MCP was, non organic,as its another outlet for possible storefront type SaaS'bro garbage that no one really wants but they want to vibecode something for some sort of cash income, no ones buying into the services though. Api's are cashing in on the token sink. This is not the first and it wont be the last.",
          "score": 2,
          "created_utc": "2026-02-22 01:41:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r0b8q",
          "author": "inagy",
          "text": "Honestly, I'm curious what he thinks about it once the dust settled. He's very analytical with this stuff and explains things with no unnecessary bullcrap added. I'm personally cautious about Openclaw for now, but watching with interest from the distance.",
          "score": 2,
          "created_utc": "2026-02-22 10:14:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm4nt",
          "author": "korino11",
          "text": "Openclaw for office plancton. Thats it. Nothing real useful, only stupid emails,chat posts... it for dumb ppl in a office!",
          "score": 5,
          "created_utc": "2026-02-21 13:51:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lre52",
              "author": "neutralpoliticsbot",
              "text": "Exactly itâ€™s for people who are amazed that it can add a calendar entry OMG revolutionary",
              "score": 4,
              "created_utc": "2026-02-21 14:23:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ld0yi",
          "author": "Dry_Yam_4597",
          "text": "They can keep him :)",
          "score": 2,
          "created_utc": "2026-02-21 12:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lsq0q",
          "author": "Daemontatox",
          "text": "Rip",
          "score": 2,
          "created_utc": "2026-02-21 14:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m3ziz",
          "author": "graymalkcat",
          "text": "This really is a different sub from the rest. Another is the AI agents one. The rest tend to be filled with some type of doomer (either the â€œhates all AIâ€ type or the type who engages in endless wars of extreme philosophical positions)",
          "score": 2,
          "created_utc": "2026-02-21 15:32:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qjn36",
              "author": "dhamaniasad",
              "text": "I work in AI and I do not have blind faith and optimism. Iâ€™m not against the tech, but I donâ€™t think capitalism + AI will guaranteed lead to good outcomes. For saying this. I was banned from multiple AI subs for being a â€œdecelerationistâ€ â€œdoomerâ€. Again, I build AI systems for a living and have since before ChatGPT was a thing. \n\nAI accelerationists say we should grow AI as fast as possible and accept all collateral damage along the way because AGI will solve all problems automatically. The AGI controlled by billionaires? That AGI?",
              "score": 2,
              "created_utc": "2026-02-22 07:34:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qud0l",
                  "author": "s101c",
                  "text": "> The AGI controlled by billionaires? That AGI?\n\nSame thoughts. I believe AI has a great transformative power and can greatly help humanity in everything, building quality of life for everyone. But it will not happen if AI hardware/software is controlled by a tiny group of modern feudals.",
                  "score": 2,
                  "created_utc": "2026-02-22 09:16:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qa19h",
              "author": "MrPecunius",
              "text": "This sub is the reason I finally got a Reddit account.",
              "score": 1,
              "created_utc": "2026-02-22 06:06:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nnj20",
          "author": "AbheekG",
          "text": "Yeah heâ€™s in fact the reason for the hype and where all this claw crap began. This whole thing simply blew up because Karpathy decided to swoon over moltbook without vetting it, and idiots like myself were immediately interested thinking something big was going on. The star-history chart on their own GitHub tells the same story.",
          "score": 2,
          "created_utc": "2026-02-21 20:11:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ohb34",
              "author": "neutralpoliticsbot",
              "text": "yea I got bamboozled too, first time fallen for the hype",
              "score": 2,
              "created_utc": "2026-02-21 22:52:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lr8o3",
          "author": "neutralpoliticsbot",
          "text": "Claw is not usable, too expensive for what it is",
          "score": 1,
          "created_utc": "2026-02-21 14:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lsf1a",
          "author": "RobotRobotWhatDoUSee",
          "text": "> 20 Gru 2023\n\nKarpathy was very prescient!",
          "score": 1,
          "created_utc": "2026-02-21 14:29:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lwcql",
          "author": "Grittenald",
          "text": "I do agree with his points regarding benchmarks if that is what he is referring to. The food truck benchmark is extremely telling as well.",
          "score": 1,
          "created_utc": "2026-02-21 14:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mwqep",
          "author": "cmndr_spanky",
          "text": "Chatbot arena is totally subjective though right ??",
          "score": 1,
          "created_utc": "2026-02-21 17:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6n2gnd",
          "author": "NigaTroubles",
          "text": "Define they",
          "score": 1,
          "created_utc": "2026-02-21 18:25:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nl0uz",
          "author": "FPham",
          "text": "And to think, he is my Slovak/Canadian countryman!",
          "score": 1,
          "created_utc": "2026-02-21 19:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ovcgw",
          "author": "redditrasberry",
          "text": "I don't understand why people are buying Mac minis. Can't you run this in a VM on any server you want? I'd much rather have it in a VM where I can kill it and erase its entire existence if it gets out of control. The minute it runs on bare metal you have the possibility it's snuck a way to revive / restore itself.\n\nHonestly though, the whole idea to me is like deliberately installing a virus on your own computer.",
          "score": 1,
          "created_utc": "2026-02-22 00:17:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pgnsy",
          "author": "MushroomCharacter411",
          "text": "Next, predict a script-kiddie-friendly app that probes for common vulnerabilities. I can even give you a name: DeClawd.",
          "score": 1,
          "created_utc": "2026-02-22 02:32:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rcc1e",
          "author": "claudiollm",
          "text": "the man literally teaches the internet how transformers work for free and then goes and builds products with that knowledge. respect the hustle\n\nhonestly though his educational content has done more for democratizing AI understanding than most academic papers. the \"build GPT from scratch\" video is probably responsible for half the people in this sub",
          "score": 1,
          "created_utc": "2026-02-22 12:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rcvch",
              "author": "jacek2023",
              "text": "yes, that's why this title",
              "score": 1,
              "created_utc": "2026-02-22 12:10:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rk6to",
          "author": "raydou",
          "text": "The only difference using Mac Mini is that using Peekaboo you are able to control the native application UX directly in your Mac Mini. And with the cheap new multimodal models like Kimi K2.5 and Qwen 3.5 this could be very useful and offer another panoply of possibilities.",
          "score": 1,
          "created_utc": "2026-02-22 13:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rksgj",
          "author": "ac101m",
          "text": "Why are people buying mac minis for this?\n\nIf the inference is all handled by openai or anthropic or whatever, what's stopping people using a $50 ex-office pc or raspberry pi or something?",
          "score": 1,
          "created_utc": "2026-02-22 13:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8tyg",
          "author": "Best-Echidna-5883",
          "text": "He's been compromised for a while now.  Let him flourish and be himself.  Relax.",
          "score": 1,
          "created_utc": "2026-02-22 15:25:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6unp5a",
          "author": "CarelessOrdinary5480",
          "text": "Who fucking cares if someone runs openclaw.  Do people really think it can't be run securely?  I have mine properly sandboxed on linux, it works fine, it sends me paragliding news, and spanish lessons.  Ohhhhh I'm so scared.",
          "score": 1,
          "created_utc": "2026-02-22 22:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vce3m",
          "author": "GasCompetitive9347",
          "text": "We need to move evals left",
          "score": 1,
          "created_utc": "2026-02-23 00:37:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mnjb4",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-21 17:10:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6npo4f",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-21 20:23:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ohl4u",
                  "author": "neutralpoliticsbot",
                  "text": "I guess its just depends on peoples expectations. \n\n>Mid size models with a good embeddings database work pretty well locally.\n\nFor my tasks even the most State of the Art models are barely cutting it so I can't even call the SOTA models \"work pretty well\"",
                  "score": 2,
                  "created_utc": "2026-02-21 22:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6niasu",
          "author": "Transit_renn",
          "text": "Apple doesn't even need to be in the AI race to win",
          "score": 1,
          "created_utc": "2026-02-21 19:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lmx15",
          "author": "Expensive-Paint-9490",
          "text": "It's just an app using AI agents, and it sucks to a level nobody knowledgeable would install it on their machine. Let alone giving serious credentials to it. Considering it a new paradigm... meh. Sadly, Karpathy has become more and more an influencer than a scientist.",
          "score": -1,
          "created_utc": "2026-02-21 13:56:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m5sz8",
          "author": "kennychesney_lied",
          "text": "i'm sick of the midwit contrarian takes in this sub like \"but what is the use case????\". these people have no imagination and no joy in life.",
          "score": -1,
          "created_utc": "2026-02-21 15:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lbzlf",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -8,
          "created_utc": "2026-02-21 12:41:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lc5zg",
              "author": "BumbleSlob",
              "text": "Apple accidentally stumbled into the best possible architecture for LLMs with unified memory from their attempts to lock down user RAM upgrades.",
              "score": 43,
              "created_utc": "2026-02-21 12:43:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lcc05",
                  "author": "jacek2023",
                  "text": "the move from dense to MoE definitely helps with this",
                  "score": 8,
                  "created_utc": "2026-02-21 12:44:21",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o6ldgil",
                  "author": "Kupuntu",
                  "text": "I would say that moving to ARM on their desktop/laptop devices was at least a tiny bit more than trying to lock down user RAM upgrades...",
                  "score": 7,
                  "created_utc": "2026-02-21 12:52:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lc9b6",
                  "author": "SrijSriv211",
                  "text": "Yeah",
                  "score": 1,
                  "created_utc": "2026-02-21 12:43:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lc81n",
              "author": "-dysangel-",
              "text": "I mean the two aren't mutually exclusive. Macs, MacOS and MLX are good. Apple Intelligence is meh.",
              "score": 6,
              "created_utc": "2026-02-21 12:43:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lciqb",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-02-21 12:45:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lc7ap",
              "author": "RoomyRoots",
              "text": "People that treat companies with the same fervor as sports teams are often not very reasonable and/or in touch with reality. ",
              "score": 12,
              "created_utc": "2026-02-21 12:43:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ldiw5",
                  "author": "SrijSriv211",
                  "text": "Yeah very true",
                  "score": 1,
                  "created_utc": "2026-02-21 12:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lku9n",
          "author": "mycall",
          "text": "He should also take a look at [Oz by Warp](https://www.warp.dev/oz)",
          "score": 0,
          "created_utc": "2026-02-21 13:43:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lp1wo",
              "author": "1-800-methdyke",
              "text": "They gave me some free credits. But I canâ€™t figure out a use case (personal use). Got anything fun to try with it?",
              "score": 1,
              "created_utc": "2026-02-21 14:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lq74s",
                  "author": "mycall",
                  "text": "Good question, so many possibilities.  [Wes](https://www.youtube.com/watch?v=I9BZRuw5c80) mentioned his use case and I like his meta idea for buliding a skill that creates skills (no idea why he is wearing shades lol)",
                  "score": 1,
                  "created_utc": "2026-02-21 14:16:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m98z0",
          "author": "txgsync",
          "text": "Donâ€™t trust us. We are a scary blend of GPU Poors, Mac Elitists, and privacy-obsessed hobbyists with more time than sense.",
          "score": 0,
          "created_utc": "2026-02-21 15:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mb6jg",
          "author": "Worldly_Expression43",
          "text": "He trusts chatbot arena? The place that let Llama game their shit?",
          "score": 0,
          "created_utc": "2026-02-21 16:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mr4df",
              "author": "Expensive-Paint-9490",
              "text": "That's a thing from 2023, and back then it made much sense.",
              "score": 3,
              "created_utc": "2026-02-21 17:28:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ntag2",
          "author": "D3c1m470r",
          "text": "You mean We have karpathy? :)",
          "score": 0,
          "created_utc": "2026-02-21 20:42:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ojpjp",
          "author": "DownSyndromeLogic",
          "text": "What is this post even about? Just random comments that don't even make sense. Is this threat entirely Ai bots? Open claw? Mac mini?",
          "score": 0,
          "created_utc": "2026-02-21 23:06:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m1ige",
          "author": "Designer-Article-956",
          "text": "Mod should ban every post about american technologists' woshipers like yours.",
          "score": -3,
          "created_utc": "2026-02-21 15:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lwzk4",
          "author": "TBT_TBT",
          "text": "They might have Karpathy, but look at the second picture: **WE** ( r/LocalLamMA ) have Karpathy as well. ;)",
          "score": -1,
          "created_utc": "2026-02-21 14:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lxf5u",
              "author": "jacek2023",
              "text": "we had him in 2023",
              "score": 3,
              "created_utc": "2026-02-21 14:58:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6m8d6c",
          "author": "Local_Phenomenon",
          "text": "You know what the f you can do with aluminum tube. I joke but a bigger stick is just that.",
          "score": -1,
          "created_utc": "2026-02-21 15:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ngkl5",
          "author": "Philosophicaly",
          "text": "if you are scared of privacy then you cant utilize AI fully",
          "score": -1,
          "created_utc": "2026-02-21 19:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6le9v8",
          "author": "ExcitementSubject361",
          "text": "It makes absolutely no sense to buy a Mac mini for Claw... CPU-only inference is ridiculously slow... for that money you could buy a proper, powerful PC, or you could buy a DGX Spark and a mini PC with plenty of RAM... I think you all have too much money AND too little knowledge... I don't care about your wasted money (your own fault), but you're generating endless Apple revenue with their overkill prices.....Before you start spouting nonsense, try working properly with claw at a maximum of 10 T/S...HAVE FUN",
          "score": -8,
          "created_utc": "2026-02-21 12:58:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lfp46",
              "author": "Spectrum1523",
              "text": "most people aren't using local models for claw",
              "score": 4,
              "created_utc": "2026-02-21 13:09:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lftk0",
              "author": "gecike",
              "text": "They are not for inference as far as I know but for the Apple ecosystem, like the Reminders and iMessages.",
              "score": 3,
              "created_utc": "2026-02-21 13:09:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6lf661",
              "author": "LevianMcBirdo",
              "text": "Isn't it more about the integration in macros and running it through API? Heard that especially the smallest m4 Mac mini gets used for it and I doubt 16 GB will get you far for agentic llms on your own hardware",
              "score": 2,
              "created_utc": "2026-02-21 13:05:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6llkwj",
                  "author": "ExcitementSubject361",
                  "text": "Okay, my mistake. In my world, there are no people who like apple stuff... I assumed you were talking about the expensive machines... that makes more sense, of course... still, â‚¬600 is way too much for what you get... but better than â‚¬5-10k.",
                  "score": 0,
                  "created_utc": "2026-02-21 13:47:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6lf5wc",
              "author": "outtokill7",
              "text": "It does when you want a completely separate system, only want MacOS, and are going to run it using cloud models.\n\nNot everyone is going to min/max everything by going with the most optimized hardware at the most optimized price. A Mac Mini is good enough and easily found at stores.",
              "score": 0,
              "created_utc": "2026-02-21 13:05:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8pztp",
      "title": "Kitten TTS V0.8 is out: New SOTA Super-tiny TTS Model (Less than 25 MB)",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/rzgwarr4rdkg1",
      "author": "ElectricalBar7464",
      "created_utc": "2026-02-19 04:48:29",
      "score": 1137,
      "num_comments": 192,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r8pztp/kitten_tts_v08_is_out_new_sota_supertiny_tts/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o68k9m5",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-19 13:25:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o676oj5",
          "author": "Xamanthas",
          "text": "You need to include at least a single sample in the HF pages",
          "score": 81,
          "created_utc": "2026-02-19 06:24:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o678zoc",
              "author": "sig_kill",
              "text": "A table on the GitHub readme with each voice, at each model weight would be excellent:\n\n|Voice|14m|40m|80m|\n|:-|:-|:-|:-|\n|Leo|\\-|\\-|\\-|\n|Bella|\\-|\\-|\\-|\n|...|\\-|\\-|\\-|",
              "score": 48,
              "created_utc": "2026-02-19 06:43:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o679l44",
                  "author": "ElectricalBar7464",
                  "text": "thanks for the feedback. we'll have it by tomorrow. right now seems like some folks are facing missing words issues based on some library mismatch. we'll add the samples as soon as we fix this \\^\\^",
                  "score": 44,
                  "created_utc": "2026-02-19 06:48:55",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6klfpo",
              "author": "ElectricalBar7464",
              "text": "Hey, added a huggingface spaces demo to try the samples. :  \n[https://huggingface.co/spaces/KittenML/KittenTTS-Demo](https://huggingface.co/spaces/KittenML/KittenTTS-Demo)\n\nLmk what you think. ",
              "score": 2,
              "created_utc": "2026-02-21 08:32:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lbpsu",
                  "author": "Mghrghneli",
                  "text": "Nice. Impressed by the nano version. Got any tips on how to make the voices follow punctuation better? It's seemingly impossible to create a pause in the middle of a sentence.",
                  "score": 2,
                  "created_utc": "2026-02-21 12:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67a4po",
              "author": "arcanemachined",
              "text": "Agreed, although their [GitHub page](https://github.com/KittenML/KittenTTS) has some samples in a video link:\n\nSource 1 (Better quality): https://www.youtube.com/watch?v=dQw4w9WgXcQ\n\nSource 2: https://github.com/user-attachments/assets/d80120f2-c751-407e-a166-068dd1dd9e8d",
              "score": 5,
              "created_utc": "2026-02-19 06:53:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67ahdz",
                  "author": "Xamanthas",
                  "text": "Thats just the video, I want to hear samples from each model and make choices.",
                  "score": 7,
                  "created_utc": "2026-02-19 06:56:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67u2n6",
                  "author": "Ethan_Vee",
                  "text": "Thanks these are the best samples so far",
                  "score": 3,
                  "created_utc": "2026-02-19 10:03:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o68qdlk",
                  "author": "mana_hoarder",
                  "text": "What is this? Are we back to 2015?",
                  "score": -1,
                  "created_utc": "2026-02-19 14:00:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6vohys",
              "author": "ElectricalBar7464",
              "text": "hey did you try the huggingface demo to try the samples? \n\n[https://huggingface.co/spaces/KittenML/KittenTTS-Demo](https://huggingface.co/spaces/KittenML/KittenTTS-Demo)",
              "score": 1,
              "created_utc": "2026-02-23 01:49:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6vvo6s",
                  "author": "Xamanthas",
                  "text": "I did, sorry forgot to respond",
                  "score": 2,
                  "created_utc": "2026-02-23 02:32:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o679cjw",
          "author": "pondy12",
          "text": "Can you guys make an offline firefox extension?   \n\"Read Aloud: A Text to Speech Voice Reader by [LSD Software](https://addons.mozilla.org/en-US/firefox/user/13863253/)\" is not private at all.   \nA firefox/chrome extension would be #1 in like a week, I'm telling you.",
          "score": 72,
          "created_utc": "2026-02-19 06:46:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ea2r",
              "author": "ElectricalBar7464",
              "text": "thats a great idea. those are exactly the use-cases we want to power. \\^\\^  \n  \nwe'll try making it once we stabilize our repo. i think some users are facing varying quality in different environments. ",
              "score": 50,
              "created_utc": "2026-02-19 07:30:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67ie0e",
                  "author": "pondy12",
                  "text": "Make sure you leverage browser's native HTMLAudioElement to handle playback and speed adjustments efficiently.\n\n    // Complete example for integrating TTS audio playback with speed control in a Firefox extension\n    // Based on KittenTTS (assuming audio is generated and fetched as a Blob)\n\n    // Function to handle TTS generation and playback\n    async function playTTS(text, speed = 1.0) {\n      // Step 1: Generate TTS audio (placeholder - replace with actual KittenTTS integration)\n      // For example, fetch from a local backend or generate via API\n      const response = await fetch('your-tts-endpoint', { // Adjust to your KittenTTS setup\n        method: 'POST',\n        body: JSON.stringify({ text }), // Send text to TTS generator\n        headers: { 'Content-Type': 'application/json' }\n      });\n      const audioBlob = await response.blob(); // Get audio as Blob (e.g., WAV or MP3)\n\n      // Step 2: Create Audio element and set source\n      const audio = new Audio();\n      audio.src = URL.createObjectURL(audioBlob);\n\n      // Set playback speed (leverages browser's native media engine)\n      audio.playbackRate = speed; // e.g., 1.5 for 1.5x speed\n      audio.preservesPitch = true; // Default: true - preserves pitch to avoid chipmunk effect\n      // If you want pitch to shift: audio.preservesPitch = false;\n\n      // Play the audio\n      audio.play();\n\n      // Edge case: Handle playback end for cleanup\n      audio.addEventListener('ended', () => {\n        URL.revokeObjectURL(audio.src); // Revoke URL to avoid memory leaks\n      });\n\n      // Return audio object for further control (e.g., pause, volume)\n      return audio;\n    }\n\n    // Step 3: Add user controls (e.g., in your extension's popup HTML/JS)\n    // Assume HTML: <input type=\"range\" id=\"speed-slider\" min=\"0.5\" max=\"2.0\" step=\"0.1\" value=\"1.0\">\n    //             <button id=\"play-btn\">Play</button>\n    //             <button id=\"pause-btn\">Pause</button>\n    //             <input type=\"range\" id=\"volume-slider\" min=\"0\" max=\"1\" step=\"0.1\" value=\"1.0\">\n\n    document.addEventListener('DOMContentLoaded', () => {\n      let currentAudio = null; // Track current audio instance\n\n      const speedSlider = document.getElementById('speed-slider');\n      const playBtn = document.getElementById('play-btn');\n      const pauseBtn = document.getElementById('pause-btn');\n      const volumeSlider = document.getElementById('volume-slider');\n\n      // Play button: Generate and play TTS with initial speed\n      playBtn.addEventListener('click', async () => {\n        const text = 'Your text to speak'; // Get from selection or input\n        const speed = parseFloat(speedSlider.value);\n        currentAudio = await playTTS(text, speed);\n        // Set volume\n        currentAudio.volume = parseFloat(volumeSlider.value);\n      });\n\n      // Pause button\n      pauseBtn.addEventListener('click', () => {\n        if (currentAudio) currentAudio.pause();\n      });\n\n      // Dynamic speed adjustment\n      speedSlider.addEventListener('input', (e) => {\n        if (currentAudio) {\n          currentAudio.playbackRate = parseFloat(e.target.value);\n        }\n      });\n\n      // Volume adjustment\n      volumeSlider.addEventListener('input', (e) => {\n        if (currentAudio) {\n          currentAudio.volume = parseFloat(e.target.value);\n        }\n      });\n\n      // Additional controls: Mute\n      // const muteBtn = document.getElementById('mute-btn');\n      // muteBtn.addEventListener('click', () => {\n      //   if (currentAudio) currentAudio.muted = !currentAudio.muted;\n      // });\n    });\n\n    // Edge cases and tips (as comments):\n    // - Cleanup: Always revoke object URLs after use to prevent memory leaks.\n    // - Pause/Resume: Use audio.pause() and audio.play(); speed persists.\n    // - Volume: Adjust audio.volume (0-1) or audio.muted = true.\n    // - Firefox Compatibility: Works in modern Firefox; test in content/background scripts.\n    // - Slow Generation: For long text, chunk input and queue audio segments for seamless playback.\n    //   e.g., Split text into sentences, generate blobs sequentially, and use audio.addEventListener('ended', playNextChunk);\n    // - Advanced: For effects beyond speed, use Web Audio API:\n    //   const audioCtx = new AudioContext();\n    //   const source = audioCtx.createBufferSource();\n    //   // Decode blob to buffer, connect, set playbackRate on source, etc.\n    // - Formats: Ensure TTS output is browser-playable (e.g., MP3/WAV); convert if needed.\n    // - Errors: Handle fetch errors, audio load errors with try-catch and user feedback.\n    // - Chunking Example (pseudo):\n    //   function playChunkedTTS(chunks) {\n    //     let index = 0;\n    //     function playNext() {\n    //       if (index >= chunks.length) return;\n    //       const audio = new Audio(URL.createObjectURL(chunks[index++]));\n    //       audio.playbackRate = 1.5;\n    //       audio.play();\n    //       audio.addEventListener('ended', playNext);\n    //     }\n    //     playNext();\n    //   }",
                  "score": 22,
                  "created_utc": "2026-02-19 08:07:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o689ksf",
              "author": "StatisticianFun8008",
              "text": "Lacking a good AI TTS based point to read browser for both html and pdf is the only reason that forced me back to Windows 11 and Edge from ArchLinux. I still can't believe it.",
              "score": 3,
              "created_utc": "2026-02-19 12:15:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68iaug",
                  "author": "ken107",
                  "text": "The Read Aloud extension mentioned above supports local inference of Piper and Supertonic AI voices. Those voices are fairly good quality and should work on Arch, Firefox or Chromium. Go to options and choose \"Install Supertonic voices\" in the Voices drop-down.",
                  "score": 6,
                  "created_utc": "2026-02-19 13:13:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o678w1w",
          "author": "Trysem",
          "text": "How to train new language?",
          "score": 22,
          "created_utc": "2026-02-19 06:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67g4ib",
              "author": "ElectricalBar7464",
              "text": "we will try to support this request after some future releases. as i mentioned elsewhere too, multilingual is totally planned for the future releases. pls join the discord to give feedback or make feature requests regarding what language to prioritise ([https://discord.com/invite/VJ86W4SURW)](https://discord.com/invite/VJ86W4SURW)  \nwe will mostly take feedback from our community on what languages will be most useful.\n\nand pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model \\^\\^",
              "score": 14,
              "created_utc": "2026-02-19 07:46:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67drqg",
          "author": "Internal_Answer_6866",
          "text": "Hell yeah, just woke up and got this banger",
          "score": 27,
          "created_utc": "2026-02-19 07:25:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ewzf",
              "author": "ElectricalBar7464",
              "text": "haha thanks a lot for the support.   \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked it and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 1,
              "created_utc": "2026-02-19 07:35:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68pqo1",
                  "author": "rorykoehler",
                  "text": "Cool model. No Discord.",
                  "score": 13,
                  "created_utc": "2026-02-19 13:56:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67h1ko",
          "author": "Lorian0x7",
          "text": "can we clone or train custom voices?",
          "score": 9,
          "created_utc": "2026-02-19 07:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o686bcs",
          "author": "HugoCortell",
          "text": "The best showcase of this is probably in a game, it's pretty cool to have real time voice synthetization for dialogue written in chat by players. So far tested by \\~4000 players, including a server that hosted \\~300 of them at once.\n\nThis is the only model with real world use in interactive media, all others are too heavy for real time local use.",
          "score": 8,
          "created_utc": "2026-02-19 11:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aha6o",
              "author": "pesaru",
              "text": "Huh, what about Pocket TTS? I'm using it with voice cloning + streaming and I get 220ish milliseconds till first audio. I quantized it to (coincidentally) 220MB. And that's on a 9th generation Intel CPU.",
              "score": 6,
              "created_utc": "2026-02-19 19:08:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6apsco",
                  "author": "HugoCortell",
                  "text": "The speed is fast, but 200MB is too high, across 4 active speakers that becomes 800MB of memory footprint, a big problem when 8GB gaming PCs are making a comeback due to memory shortages. ",
                  "score": 4,
                  "created_utc": "2026-02-19 19:49:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6d8dza",
              "author": "ElectricalBar7464",
              "text": "haha thanks for suggesting that. we picked the voices such that they could be used in use-cases like games.   \nWe will have a faster model pretty soon. ",
              "score": 2,
              "created_utc": "2026-02-20 04:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fgwu9",
                  "author": "HugoCortell",
                  "text": "I look forward to it, as well as to having more voices and multi-language support in the future.\n\nI did join your discord some time ago asking if I could gain access to more voices (which I thought would benefit KittenTTS too as it'd improve a real world usage showcase of the model) but didn't get much of a response if I recall.",
                  "score": 1,
                  "created_utc": "2026-02-20 14:43:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6767sg",
          "author": "ironcodegaming",
          "text": "Does it need pytorch to run? What is the minimal install size we can use to bundle this?",
          "score": 9,
          "created_utc": "2026-02-19 06:20:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67dl0s",
              "author": "ElectricalBar7464",
              "text": "the current is an onnx runtime. there are probably a few redundancies in the env rn as we tried to ship it asap. we'll clean it up this week. the final model size for the 14M will be <25 mb onnx",
              "score": 15,
              "created_utc": "2026-02-19 07:23:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68b4lx",
          "author": "wowsers7",
          "text": "Please add voice cloning :-)",
          "score": 5,
          "created_utc": "2026-02-19 12:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8w3y",
              "author": "ElectricalBar7464",
              "text": "yeah we have experiments trying to support that. we should have that in a few months.   \n pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 3,
              "created_utc": "2026-02-20 04:26:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68l7ug",
          "author": "mitrokun",
          "text": "This is clearly not for edge devices . All models except the regular nano (there are synthesis artifacts, such as clicks), consumes more resources, than kokoro. \n\ncpu test\n\n| Model | Parameters | Speed (x Real-Time) |\n\n| \\*\\*Kitten-TTS-Mini\\*\\* | 80M | \\*\\*1.6x\\*\\* |\n\n| \\*\\*Kitten-TTS-Micro\\*\\* | 40M | \\*\\*2.7x\\*\\* |\n\n| \\*\\*Kitten-TTS-Nano\\*\\* | 14M | \\*\\*15.0x\\*\\* |\n\n| \\*\\*Kitten-TTS-Nano-int8\\*\\* | 14M (q8) | \\*\\*2.9x\\*\\* |\n\n  \nI've built a test server for Home Assistant here\n\n [https://github.com/mitrokun/wyoming\\_kitten\\_tts](https://github.com/mitrokun/wyoming_kitten_tts)",
          "score": 5,
          "created_utc": "2026-02-19 13:30:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69ihmg",
              "author": "bambamlol",
              "text": "Thank you for sharing! How powerful is your CPU? And what \"x real-time\" factor would you say is recommended for a decent/great user experience?",
              "score": 5,
              "created_utc": "2026-02-19 16:23:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69m2lm",
                  "author": "mitrokun",
                  "text": "amd 5900x. If your hardware allows for 1.5-2 RTFx, that's enough to emulate a streaming response. However, with this speed, there will be a delay before the audio starts (the time it takes to synthesize the first sentence). I would say that for diffusion models it is much more comfortable if the value is above 10.",
                  "score": 3,
                  "created_utc": "2026-02-19 16:41:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6d95tb",
              "author": "ElectricalBar7464",
              "text": "let me have a look. it shouldn't be taking more resources than kokoro. Are you benchmarking w streaming kokoro, if yes that should make a difference? \n\nin any case, we're gonna continue improving our models and efficiency so that such an experience is minimized. thanks for sharing these benchmarks.",
              "score": 1,
              "created_utc": "2026-02-20 04:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6erx4q",
                  "author": "mitrokun",
                  "text": "A short comparison\n\n[https://youtu.be/0O7Ay5GSMWM](https://youtu.be/0O7Ay5GSMWM)\n\n\n\n==============================================================\n\nENGINE          | TTFA (ms)  | TOTAL TIME (s)  | AUDIO (s)  | RTFX\n\n\\--------------------------------------------------------------------------------\n\nPiper           | 555        | 1.52            | 32.58      |  21.50x\n\nPocket TTS      | 1095       | 9.39            | 33.44      |   3.56x\n\nSupertonic2     | 358        | 1.85            | 36.29      |  19.65x\n\nKokoro ONNX     | 713        | 6.42            | 30.78      |   4.80x\n\nKittenTTS micro | 1758       | 13.40           | 40.88      |   3.05x\n\nKittenTTS nano  | 387        | 2.59            | 40.46      |  15.65x\n\n==============================================================\n\nYour architecture (or inference) is slower than Kokoro's. As I've already shown, only Nano is reasonably fast.\n\nBut the int8 version is broken.",
                  "score": 2,
                  "created_utc": "2026-02-20 12:20:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67cq1w",
          "author": "locomotive-1",
          "text": "Cool",
          "score": 5,
          "created_utc": "2026-02-19 07:16:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ens7",
              "author": "ElectricalBar7464",
              "text": "haha thanks locomotive-1 for the support.  \nlmk if you had a chance to try the model. and pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked it and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 3,
              "created_utc": "2026-02-19 07:33:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o685ll6",
          "author": "Agreeable_Wasabi9329",
          "text": "Nano model is impressive!  \nIs it possible to add more pause after punctuation marks? The sentences follow each other very quickly, I'm not talking about the speed setting",
          "score": 5,
          "created_utc": "2026-02-19 11:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vpewh",
              "author": "ElectricalBar7464",
              "text": "thanks a lot! i think we added a preprocessing step last minute and its messing up some pronunciations. some users are also having voice quality issues  because of env mismatches (still debugging this issue).    \n\ncan you share an example text that is causing issues for you + the model you tried? if you want to send it privately, feel free to join the discord and dm meÂ ([https://discord.com/invite/VJ86W4SURW](https://discord.com/invite/VJ86W4SURW)) . if you liked the model pls star it too \\^\\^ (Â [https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS))\n\n",
              "score": 1,
              "created_utc": "2026-02-23 01:54:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6728pd",
          "author": "Blues520",
          "text": "The voices are so cute!",
          "score": 12,
          "created_utc": "2026-02-19 05:48:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o678mk6",
              "author": "sig_kill",
              "text": "The first one sounds like the ladies on 90s adult chat line commercials",
              "score": 12,
              "created_utc": "2026-02-19 06:40:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67ewrr",
                  "author": "Blues520",
                  "text": "How far we have come",
                  "score": 6,
                  "created_utc": "2026-02-19 07:35:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6730eg",
              "author": "ElectricalBar7464",
              "text": "haha thank you. we picked the voices very carefully, we're gonna include more preofessional voices in the next one too. if you liked the project ,  \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)^^)",
              "score": 9,
              "created_utc": "2026-02-19 05:54:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67i5cq",
          "author": "Lordaizen639",
          "text": "How good is this comparable to chatterbox TTSÂ ",
          "score": 3,
          "created_utc": "2026-02-19 08:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a92rl",
              "author": "Yorn2",
              "text": "These guys are trying to be small and fast while chatterbox is kind of trying to go for quality (though they did release a chatterbox Turbo for speed). That said, Kitten is probably one of the best quality small TTS out there. I think it's primarily trying to compete with Kokoro.",
              "score": 4,
              "created_utc": "2026-02-19 18:30:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vpwua",
                  "author": "ElectricalBar7464",
                  "text": "Yes, we are trying to reach and eventually beat chatterbox quality in <100M parameters for on-device voice applications.   \nIf we want to build on-device voice applications, the memory+compute of the tts model has to be minimal. current models are too big for anything other than pure voice generation. ",
                  "score": 1,
                  "created_utc": "2026-02-23 01:57:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67pm4m",
          "author": "OpinionatedUserName",
          "text": "Bruno and luna are good voices. Bruno is that standard deep male voice one hears in reels. Not better than other tts, but is very fast and usable when limited to cpu only workflows.",
          "score": 3,
          "created_utc": "2026-02-19 09:19:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8oo8",
              "author": "ElectricalBar7464",
              "text": "yeah they're the more professional voices. the others are more for fun/expressive/gaming type usecases. We picked those because there is a real lack of such voices among tiny models. we'll try including more professional voices in the next release. thanks!\n\n",
              "score": 2,
              "created_utc": "2026-02-20 04:25:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67wrqi",
          "author": "panzer_kanzler",
          "text": "I hope you guys make a voice changer model in the feature as well ðŸ‘",
          "score": 3,
          "created_utc": "2026-02-19 10:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8t83",
              "author": "ElectricalBar7464",
              "text": "its on the cards along w multilingual. pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 1,
              "created_utc": "2026-02-20 04:25:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67zfkt",
          "author": "ixdx",
          "text": "I created a Dockerfile and docker-compose.yml, along with a simple interface using Gradio.  \n[https://github.com/vitaliy-sn/KittenTTS-docker-gradio](https://github.com/vitaliy-sn/KittenTTS-docker-gradio)\n\nIâ€™ll be waiting for your multilingual models.",
          "score": 3,
          "created_utc": "2026-02-19 10:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8qe1",
              "author": "ElectricalBar7464",
              "text": "haha thanks a lot! ",
              "score": 1,
              "created_utc": "2026-02-20 04:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68c7op",
          "author": "thedarkbobo",
          "text": "Im in need of multi lingual still Q\\_Q but thats great",
          "score": 3,
          "created_utc": "2026-02-19 12:33:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8z38",
              "author": "ElectricalBar7464",
              "text": "sorry but can you elaborate what Q\\_Q is? ",
              "score": 1,
              "created_utc": "2026-02-20 04:27:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dzkuz",
                  "author": "thedarkbobo",
                  "text": "*Q\\_Q*: a sad, crying face usually used in gaming to express sadness a way to say get over it (nothing important here)",
                  "score": 2,
                  "created_utc": "2026-02-20 08:12:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ap0nt",
          "author": "Caderent",
          "text": "Beautiful. Good job.",
          "score": 3,
          "created_utc": "2026-02-19 19:46:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d9g1f",
              "author": "ElectricalBar7464",
              "text": "thanks a lot. really appreciate the support \\^\\^  \n  \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 2,
              "created_utc": "2026-02-20 04:30:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6au0fb",
          "author": "libinpage",
          "text": "how fast is that? Can i use it for my ai agent to generate myself voice messages via telegram? or will it take forever? I run on macbook m1 pro",
          "score": 3,
          "created_utc": "2026-02-19 20:10:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d9k8b",
              "author": "ElectricalBar7464",
              "text": "it'll run superfast on an m1 pro too. we'll try adding instructions and examples for running mps backend if its not fast enough already \\^\\^  \n  \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 2,
              "created_utc": "2026-02-20 04:31:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6plmr5",
          "author": "richardr1126",
          "text": "[richardr1126/KittenTTS-FastAPI](https://github.com/richardr1126/KittenTTS-FastAPI)\n\nAwesome! I was waiting for new fast CPU TTS similar to Kokoro. I got a production ready FastAPI setup.  \n  \nThis can be used in my other open source project [richardr1126/openreader](https://github.com/richardr1126/openreader) as the TTS endpoint to read EPUB, PDF, and MD documents out loud and follow along with word by word highlights.",
          "score": 3,
          "created_utc": "2026-02-22 03:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66yc0u",
          "author": "Aarav2208",
          "text": "Babe wake up, kitten tts is back.Â \n\n\nDidn't you guys promise 2 weeks after the initial demo? I got tired of waiting ðŸ˜žÂ \n\n\nThanks btw amazing projectÂ ",
          "score": 16,
          "created_utc": "2026-02-19 05:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o670pcr",
              "author": "ElectricalBar7464",
              "text": "haha thanks, would love your feedback on the new model.  pls checkout our github and join our discord to share your thoughts\n\n",
              "score": 3,
              "created_utc": "2026-02-19 05:36:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67amh4",
                  "author": "[deleted]",
                  "text": "How to finetune this models?",
                  "score": 1,
                  "created_utc": "2026-02-19 06:57:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o675ar8",
          "author": "vandalieu_zakkart",
          "text": "This is fucking awesome. Just woke up and saw this. I'm gonna tinker with this throughout the day.",
          "score": 4,
          "created_utc": "2026-02-19 06:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67dfa0",
              "author": "ElectricalBar7464",
              "text": "haha thanks for the support. pls share your experience. the model performance may change based on environment so if the model sounds bad, its probably the env. pls share this w us on github or our discord. you can also share it here :)  \n   \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": -1,
              "created_utc": "2026-02-19 07:22:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o677z4s",
          "author": "TopTippityTop",
          "text": "Can't wait for Portuguese!",
          "score": 6,
          "created_utc": "2026-02-19 06:35:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67e0w6",
              "author": "ElectricalBar7464",
              "text": "multilingual is planned for the future releases. pls join the discord to give feedback or make feature requests regarding what language to prioritise ([https://discord.com/invite/VJ86W4SURW)](https://discord.com/invite/VJ86W4SURW)    \nwe will mostly take feedback from our community on what languages will be most useful.   \n  \nand pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model \\^\\^",
              "score": 4,
              "created_utc": "2026-02-19 07:27:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68gomf",
          "author": "DonnaPollson",
          "text": "The size-to-quality ratio here is genuinely insane. Sub-100M parameter TTS that sounds this good would have been science fiction two years ago.\n\nWhat excites me most is the implications for edge deployment. This is small enough to run on a Raspberry Pi or even embedded hardware. Imagine every smart home device, every kiosk, every IoT gadget having natural-sounding voice synthesis without needing a cloud API call.\n\nI've been running my own TTS pipeline for home automation and the latency difference between local and cloud is night and day. Something this small could bring that experience to devices with barely any compute budget.\n\nThe real question is voice cloning support. A tiny model that can do zero-shot voice adaptation would basically be the holy grail for privacy-conscious voice assistants.",
          "score": 4,
          "created_utc": "2026-02-19 13:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d9s03",
              "author": "ElectricalBar7464",
              "text": "we're running experiments on how to do voice cloning for such tiny models. i think the next 2 months will be very exciting for KittenML. if you liked this model:  \n  \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) and  \npls join the discord to give feedback or make feature requests. ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)  \nWe also launch all our models on discord first, before publicly.",
              "score": 1,
              "created_utc": "2026-02-20 04:32:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67y58d",
          "author": "ares0027",
          "text": "Tbh i judge tts models by their capability of creating non-english content. Like turkish, dutch, afrikaans etc. every model is capable of doing english and chinese. \n\nLuckily for them i am nobody so who gives a fk",
          "score": 2,
          "created_utc": "2026-02-19 10:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d9zbf",
              "author": "ElectricalBar7464",
              "text": "sorry for the bad experience. we want to support many different languages in the future. unfortunately for a tiny team (we're just 2 folks lol), it makes sense to first bring the model to a good quality in one language before trying to go multilingual.  ",
              "score": 2,
              "created_utc": "2026-02-20 04:34:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6imj11",
                  "author": "mightshade",
                  "text": "\\> it makes sense to first bring the model to a good quality in one language before trying to go multilingual\n\nIt does, but you're competing with over 30 other TTS systems if you're going with English. Your unique selling point seems to be the tiny size, but that isn't unique either.",
                  "score": 1,
                  "created_utc": "2026-02-20 23:57:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ilyp4",
              "author": "mightshade",
              "text": "Amen to that. The TTS landscape is swamped with models for English+Chinese. I have a list of over 30 ones (for clarity: Some of them capable of other languages, too). ~~I'm not adding this one to the list.~~",
              "score": 2,
              "created_utc": "2026-02-20 23:53:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6inp75",
                  "author": "ElectricalBar7464",
                  "text": "i see, thanks for the feedback. can you share what it would take to add kitten to the list of models you use? is the quality good enough for being used in applications? ",
                  "score": 2,
                  "created_utc": "2026-02-21 00:03:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67zvev",
          "author": "HellCanWaitForMe",
          "text": "Not sure if it's a me problem, but I can't get any good quality out of this. It's like a broken radio and the audio sounds messed up, it's sort of there but also really mumbled.\n\nUsing ArchLinux for what it's worth. I have issues trying to load the voices via text like 'Jasper' - \n\n    audio = tts.generate(sentence, voice='Jasper')\n\n\\`\\[TTS Error: 'Jasper is not a file in the archive'\\]\\`\n\nI did manage to get that part fixed by just calling the name of the file directly instead. \n\n  \nI've got it loaded in a model/ folder, with the config, onnx file, and voices.npz. Also the requirements.txt requires different versions of python so that's a bit weird. \n\nBut yeah, can't seem to get around the audio quality being strange. ",
          "score": 2,
          "created_utc": "2026-02-19 10:56:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dx4z1",
              "author": "ElectricalBar7464",
              "text": "hey, yes we just saw that the model was performing very differently on different hardware. We just uploaded a new model which seems more stable and less sensitive to different precision on different hardware. Can you check it out and lmk if it still sounds strange? Strange sounds are due to a bug/env mismatch etc. ",
              "score": 1,
              "created_utc": "2026-02-20 07:49:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o683qp7",
          "author": "rdsf138",
          "text": "Â Awesome!",
          "score": 2,
          "created_utc": "2026-02-19 11:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dwurf",
              "author": "ElectricalBar7464",
              "text": "thanks a lot \\^\\^",
              "score": 2,
              "created_utc": "2026-02-20 07:46:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6dwx28",
              "author": "ElectricalBar7464",
              "text": "please try the github and join our discord if you find the model useful. we're gonna try launching more models in the coming months that are geared towards edge deployment. ",
              "score": 2,
              "created_utc": "2026-02-20 07:47:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o684ez4",
          "author": "angelin1978",
          "text": "14M TTS model that actually works? thats insane for edge deployment. any benchmarks on latency for the smallest one?",
          "score": 2,
          "created_utc": "2026-02-19 11:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5nsz",
          "author": "Beautiful_Yak_3265",
          "text": "This is incredibly impressive, especially the sub-25MB footprint.\n\nI've been thinking a lot about how ultra-small local TTS like this could enable fully local voice agents â€” not just assistants, but autonomous agents that can interact with humans in real-time without relying on cloud APIs.\n\nOne challenge I've run into is orchestration â€” handling real-world tasks where agents need to escalate to humans when automation fails, while still keeping most of the pipeline local.\n\nCurious if anyone here has experimented with hybrid agent architectures (local inference + human fallback)? It feels like this kind of TTS is a key missing piece for making that viable end-to-end.",
          "score": 2,
          "created_utc": "2026-02-20 09:10:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6errx4",
              "author": "ElectricalBar7464",
              "text": "our goal is to unlock the potential of on-device voice agents.   \nRegardig the fallback, can you elaborate with an example? iiuc, the decision to fall back to human is taken by the llm usually. but i guess that functionality can be built inside the tts too somehow. ",
              "score": 1,
              "created_utc": "2026-02-20 12:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6eud3v",
                  "author": "Beautiful_Yak_3265",
                  "text": "Great question. One concrete example is in a customer support voice agent running fully locally.\n\nSuppose the agent handles common requests autonomously (checking order status, booking appointments, answering FAQs). The LLM manages reasoning and decides whether it is confident enough to respond.\n\nA fallback to a human would be triggered when certain conditions are met, for example:\n\nâ€¢ Low confidence or high uncertainty in the LLM response  \nâ€¢ Repeated user frustration signals (\"this isnâ€™t helping\", repeated rephrasing)  \nâ€¢ Requests involving sensitive actions (payments, account changes)  \nâ€¢ Failure to complete a task after several attempts\n\nIn that architecture, TTS itself doesnâ€™t decide the fallback, but it plays an important role in the transition. For example, the system can smoothly hand off by saying something like:\n\n\"Let me connect you to a human specialist who can help further.\"\n\nWhatâ€™s interesting about ultra-small local TTS like this is that the entire loop â€” ASR, LLM reasoning, orchestration, and TTS â€” can run locally with only the final escalation involving a human endpoint if needed.\n\nThis makes hybrid local-first voice agents much more practical, especially for privacy-sensitive or latency-critical environments.",
                  "score": 1,
                  "created_utc": "2026-02-20 12:36:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6viera",
          "author": "yaboyskales",
          "text": "Can it run locally?",
          "score": 2,
          "created_utc": "2026-02-23 01:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vobso",
              "author": "ElectricalBar7464",
              "text": "hey, yes this model is made for local inference \\^\\^ \n\npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked it and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW) \\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 1,
              "created_utc": "2026-02-23 01:48:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o670i21",
          "author": "simracerman",
          "text": "Is there a docker run command for this? Iâ€™d like to hook this up to open web uiÂ ",
          "score": 2,
          "created_utc": "2026-02-19 05:34:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o671wib",
              "author": "ElectricalBar7464",
              "text": "hey not yet, but thats a great idea. i'll add it to our discord and todo. we'd welcome a PR too ;) ",
              "score": 4,
              "created_utc": "2026-02-19 05:45:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o672hkm",
                  "author": "simracerman",
                  "text": "Iâ€™ll gladly put a PR with some context tomorrow ðŸ‘",
                  "score": 3,
                  "created_utc": "2026-02-19 05:50:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o677vt8",
                  "author": "andy2na",
                  "text": "awesome model - if there was openai API support via docker container or speaches compatibility, that would be so useful for integration",
                  "score": 2,
                  "created_utc": "2026-02-19 06:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66x4gg",
          "author": "Gold_Sugar_4098",
          "text": "How long did it take to train? V0.1 vs v0.8?",
          "score": 2,
          "created_utc": "2026-02-19 05:09:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o671o0c",
              "author": "ElectricalBar7464",
              "text": "yoo, v0.1 took about a week but the training was still quite unstable. since then, it took us a few months to get stable training in v0.8. A complete training run for each(14M, 40M, 80M) of the models took about 2 weeks.",
              "score": 7,
              "created_utc": "2026-02-19 05:43:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o679xb2",
                  "author": "cantgetthistowork",
                  "text": "Can you ELI5 what the training for a TTS model involves?",
                  "score": 7,
                  "created_utc": "2026-02-19 06:51:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o674z4o",
          "author": "Prestigious-Use5483",
          "text": "You had me at kitten ðŸˆðŸ¾",
          "score": 3,
          "created_utc": "2026-02-19 06:10:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67d69n",
              "author": "ElectricalBar7464",
              "text": "haha thanks for the support.   \npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model and  \npls join the discord to give feedback or make feature requests ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)",
              "score": 2,
              "created_utc": "2026-02-19 07:20:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67xixw",
          "author": "How_dull",
          "text": "Hey!\nI'm wondering, if you have some speed measures regarding the models with consumer GPUs, like a 3090? What the real time factor of the models?",
          "score": 1,
          "created_utc": "2026-02-19 10:35:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67y1am",
          "author": "Conscious-Hair-5265",
          "text": "Impressive",
          "score": 1,
          "created_utc": "2026-02-19 10:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68bbtu",
          "author": "MKU64",
          "text": "Awesome, have loved every iteration you guys have made. Thrilled to use this one!",
          "score": 1,
          "created_utc": "2026-02-19 12:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68cx5s",
          "author": "dooinglittle",
          "text": "This is cool!",
          "score": 1,
          "created_utc": "2026-02-19 12:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69abk9",
          "author": "Hot-Employ-3399",
          "text": "Can these voices be customised?",
          "score": 1,
          "created_utc": "2026-02-19 15:44:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69g459",
          "author": "MS_GundamWings",
          "text": "Does anyone have instructions for how to force the model to run locally without it making calls to HF?",
          "score": 1,
          "created_utc": "2026-02-19 16:12:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69kxqd",
              "author": "MS_GundamWings",
              "text": "Ah ok I think if you specify a cache directory it will download there and then not need to make calls to hugging face.",
              "score": 1,
              "created_utc": "2026-02-19 16:35:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69ltiv",
                  "author": "MS_GundamWings",
                  "text": "hmm no it still seems to be making calls to check the model version or something.",
                  "score": 1,
                  "created_utc": "2026-02-19 16:39:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ahzbh",
          "author": "MetricZero",
          "text": "Will voice cloning be on the road map?",
          "score": 1,
          "created_utc": "2026-02-19 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6app8g",
          "author": "superkickstart",
          "text": "Wow, talking cats!",
          "score": 1,
          "created_utc": "2026-02-19 19:49:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6au4pr",
          "author": "pixeladdikt",
          "text": "I'd like to ask about voice cloning and if that's possibly in the roadmap? Love the quality for on device and CPU enabled, the voices are great, I'd just need the clone for specific characters. Excellent work, thanks!",
          "score": 1,
          "created_utc": "2026-02-19 20:10:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ax22p",
          "author": "MatinMorning",
          "text": "Cute cartoon voice please",
          "score": 1,
          "created_utc": "2026-02-19 20:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bdfie",
          "author": "anonthatisopen",
          "text": "Thank you kittys.",
          "score": 1,
          "created_utc": "2026-02-19 21:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bwoq3",
          "author": "Fuckinglivemealone",
          "text": "Voice cloning would make this perfect for Ai voice over of games like Skyrim using CHIM. Following the project on Github (no discord)",
          "score": 1,
          "created_utc": "2026-02-19 23:28:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6byc5r",
          "author": "_raydeStar",
          "text": "\\> **Open source (hell yeah!)**\n\nnow you're speaking my language.  Can I train a voice to it?",
          "score": 1,
          "created_utc": "2026-02-19 23:38:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6csj28",
          "author": "balfrag",
          "text": "Great TTS project!   \n  \nI built a small wrapper around Claude Code CLI that uses KittenTTS to speak out loud when Claude has a question or needs your attention. Super handy when you're not looking at the terminal.   \nYou just hear Kiki ask its question and know it's waiting for input.\n\nThe nano model works great for this use case. Fast enough for real-time notifications without noticeable delay.\n\nRepo if anyone's curious: [https://github.com/devalnor/claude-kitten](https://github.com/devalnor/claude-kitten)",
          "score": 1,
          "created_utc": "2026-02-20 02:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dhete",
          "author": "ASCanilho",
          "text": "This is great news.\n\nWe need more tiny models to stick our fingers up the S of big coorp.\n\nWe donâ€™t need their slop.\n\nWe need more small and open models, hopefully that can colaborate with each other for more  precise content and utility.\n\nLets go!!",
          "score": 1,
          "created_utc": "2026-02-20 05:30:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dmizc",
          "author": "Local_Phenomenon",
          "text": "My Man!",
          "score": 1,
          "created_utc": "2026-02-20 06:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dw57d",
          "author": "n3rding",
          "text": "Hi, Iâ€™m trying to understand what the model is adding to the solution here and the value itâ€™s bringing over a regular TTS (before I have a play with it). Is this basically adding a natural expressiveness to the written string input?",
          "score": 1,
          "created_utc": "2026-02-20 07:40:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e9b5m",
          "author": "robonxt",
          "text": "looks fire! Would like to see on the github or website some comparisons between models, cpu/gpu, and speed once you guys have things sorted out!",
          "score": 1,
          "created_utc": "2026-02-20 09:44:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g5zyw",
          "author": "Jealous-Stop-5083",
          "text": "How can I save the model without downloading every time",
          "score": 1,
          "created_utc": "2026-02-20 16:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ld50y",
          "author": "Medium_Win_8930",
          "text": "I love this, well done, I will probably make a video on this. Is there any way to actually meet these extremely rare super intelligent AI researcher kittens?",
          "score": 1,
          "created_utc": "2026-02-21 12:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u3lwe",
          "author": "HatEducational9965",
          "text": "Took a shot at running it in the browser with onnxruntime-web:\n\ncode: [https://github.com/geronimi73/next-voice](https://github.com/geronimi73/next-voice)\n\ndemo: [https://next-voice.vercel.app](https://next-voice.vercel.app)\n\nIt does **something**, output sounds a bit weird, especially at the end, didn't get webgpu backend to work etc etc. \n\nWIP is what i'm trying to say. ",
          "score": 1,
          "created_utc": "2026-02-22 20:37:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o680w32",
          "author": "artistdadrawer",
          "text": "Wew",
          "score": 1,
          "created_utc": "2026-02-19 11:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dwxls",
              "author": "ElectricalBar7464",
              "text": "haha thanks a lot \\^\\^",
              "score": 1,
              "created_utc": "2026-02-20 07:47:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o686vbm",
          "author": "tspwd",
          "text": "Great! Does this work well enough on a phone / tablet?",
          "score": 1,
          "created_utc": "2026-02-19 11:54:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dahna",
              "author": "ElectricalBar7464",
              "text": "yes it is designed to run on edge devices \\^\\^ we are very passionate about making tiny models that can run on edge devices like phones/tablets so we will continue improving this. \n\npls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) and  \npls join the discord to give feedback or make feature requests. ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)  \nWe also launch all our models on discord first, before publicly.",
              "score": 2,
              "created_utc": "2026-02-20 04:38:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dryiz",
                  "author": "tspwd",
                  "text": "Great, thanks! Looking forward to trying it out!",
                  "score": 1,
                  "created_utc": "2026-02-20 07:01:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68m9kp",
          "author": "silenceimpaired",
          "text": "Meowvelous, purrfect even, I had to paws while scrolling through Reddit when I saw this. Iâ€™m curious what the license claws about commercial use says.\n\nKitten. Sigh. Could not fit it in.",
          "score": 1,
          "created_utc": "2026-02-19 13:36:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ci4pd",
              "author": "blbd",
              "text": "You've gotta be kitten me!",
              "score": 0,
              "created_utc": "2026-02-20 01:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69hgvc",
          "author": "tripflow",
          "text": "That's a great update. Multilingual support is planned for the future. How long will it take?",
          "score": 1,
          "created_utc": "2026-02-19 16:18:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6damfb",
              "author": "ElectricalBar7464",
              "text": "thanks. we think there is a huge improvement from the previous version. glad to hear you thought so too. What languages would you care most about?\n\nif you liked the model, pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) and  \npls join the discord to give feedback or make feature requests. ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)  \nWe also launch all our models on discord first, and then publicly.",
              "score": 1,
              "created_utc": "2026-02-20 04:39:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o69q9b6",
          "author": "Hector_Rvkp",
          "text": "Quite nice, thank you! will wait for extra features. Currently, it overrides punctuation so it's not really useful. If you read 2 sentences, it can disregard punctuation and the resulting voice doesn't make sense.   \nRuns super fast on my AMD CPU though, that's nice! Using the 0.8 model (largest). ",
          "score": 1,
          "created_utc": "2026-02-19 17:00:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6daun0",
              "author": "ElectricalBar7464",
              "text": "haha great to hear it runs fast on your cpu. can you share what punctuations it overrides? would love to help fix that if possible. can be shared on github, discord or here. in any case, if you liked the model pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) and  \npls join the discord to give feedback or make feature requests. ([https://discord.com/invite/VJ86W4SURW)\\^\\^](https://discord.com/invite/VJ86W4SURW)  \nWe also launch all our models on discord first, and then publicly.",
              "score": 1,
              "created_utc": "2026-02-20 04:40:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dqwh6",
                  "author": "Hector_Rvkp",
                  "text": "Thanks. I refuse, or at least avoid, to use discord because of their recent privacy moves. I could give you specifics but honestly I dropped like 5 small sentences in a python script, using punctuation like \".\", \",\" (well, punctuation), and about 20pc of the time it just goes \"crazy\". Here's one specific example. \"You're on your way to being a domestic dessert god. Saccharon, the god of sugar\". It won't read that properly. Generally, punctuation is how language makes sense, and it's clear that the current training doesn't respect it very much. Maybe fine tuning it with literary audiobooks would help, not sure how you train such model, but punctuation is nuance, and critical. I haven't converted enough text to see if the model hallucinates, because in its current state, the overriding of basic punctuation makes it not good enough for me. Fwiw, my use case is turning books I will never take the time to read into audio that i can listen to while cooking and stuff, so I can't have something that hallucinates, but step 1, I need it to respect punctuation. In fact, having the option to force hiatus / silence with some code would probably be great, but on other models i know it tends to break the flow completely and isn't really is usable. Best of luck to the team!",
                  "score": 1,
                  "created_utc": "2026-02-20 06:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6a9ypd",
          "author": "Practical_While_9263",
          "text": "Wow",
          "score": 1,
          "created_utc": "2026-02-19 18:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o678zf4",
          "author": "BigBoiii_Jones",
          "text": "I know it kind of defeats the purpose of what you're currently doing but do you plan on doing any open source larger models that are like Vibe Voice, Qwen-3-TTS, or possibly something to ElevenLabs possibly? I know we can simply use one of those but having multiple open source options is great in the long run. Also any future plans for multilingual like Japanese, Chinese, or Korean?",
          "score": 0,
          "created_utc": "2026-02-19 06:43:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67aem4",
              "author": "RebouncedCat",
              "text": "but why would you want another large model like vibe voice or qwen or orpheus or kyutai or csm when they all are already large and they already sound like all of them ? I think what we need more are highly realistic and natural models that are small and efficient like this that can be run realtime and on lighter hardware.",
              "score": 11,
              "created_utc": "2026-02-19 06:55:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67egh0",
                  "author": "ElectricalBar7464",
                  "text": "yes, thats exactly what we're trying to build. cloud-quality ai with on-device efficiency and costs. i think you'll really like these models. and the next release will be even more efficient and higher quality \\^\\^",
                  "score": 6,
                  "created_utc": "2026-02-19 07:31:40",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67e66o",
              "author": "ElectricalBar7464",
              "text": "in the future our models will bridge the gaps to qwen3 tts and elevenlabs despite being orders of magnitude smaller, cheaper and more efficient \\^\\^ . i think our models will serve use-cases that wernt possible yet ;) like ondevice voice agents/interfaces. \n\nyes, multilingual is planned for the future releases. pls join the discord to give feedback or make feature requests regarding what language to prioritise ([https://discord.com/invite/VJ86W4SURW)](https://discord.com/invite/VJ86W4SURW)  \nwe will mostly take feedback from our community on what languages will be most useful.  \n  \nand pls star the github([https://github.com/KittenML/KittenTTS](https://github.com/KittenML/KittenTTS)) if you liked the model \\^\\^",
              "score": 7,
              "created_utc": "2026-02-19 07:29:03",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o68hkn8",
              "author": "martinerous",
              "text": "I like VoxCPM, it's fast on nano-vllm, could be finetuned to a new language quite easily, and its voice cloning follows emotions of the reference.",
              "score": 1,
              "created_utc": "2026-02-19 13:08:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ad4t",
          "author": "[deleted]",
          "text": "Will it run on 4090? Can not afford Rtx 6000.",
          "score": 0,
          "created_utc": "2026-02-19 06:55:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67d4is",
              "author": "KokaOP",
              "text": "runs on cpu easily\n\n",
              "score": 5,
              "created_utc": "2026-02-19 07:19:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o686i2h",
                  "author": "HugoCortell",
                  "text": "I have tested across \\~4000 machines of varying specs and they all work pretty well.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:51:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67el8s",
              "author": "ElectricalBar7464",
              "text": "yes of course. it'll make your 4090 feel like a gb200. \n\n",
              "score": 5,
              "created_utc": "2026-02-19 07:32:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68rg1z",
          "author": "Single_Ring4886",
          "text": "I know this may be naive request. But what about \"exe\" executable without any dependencies - including to exotic dll system files and such. Just something like notepad++  \nI bet so many \"normal\" users would start using your software if it worked like koboldccp or similar well made independent stuff.",
          "score": 0,
          "created_utc": "2026-02-19 14:06:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6916pk",
              "author": "puzzleheadbutbig",
              "text": "How is this exe going to work exactly? I'm asking because I'm curious what you have in mind. Will it be a local API where you call and playback the sounds? Will it have a UI where you put text and hear generated voice lines? Will it be a command-line exe that takes params and outputs wav? Just saying \"exe\" is easy, but you see it comes with a bunch of follow-up questions and a lot of extra work. And your comparison with Notepad++ is not a good way to think this because Notepad++ is an end product, whereas this is a resource/library.",
              "score": 0,
              "created_utc": "2026-02-19 14:58:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o696cgz",
                  "author": "Single_Ring4886",
                  "text": "I love koboldcpp you do not install anything just click exe executable and it open menu where you can do things and initialize api based web frontend... it is next level compared to python nightmare. ",
                  "score": 0,
                  "created_utc": "2026-02-19 15:24:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ark7x",
              "author": "Odd-Ordinary-5922",
              "text": "I lowkey think learning code is a good thing to know not only because it makes you smarter but it also helps you understand what you are actually using.\n\nThen again their setup guide is pretty basic and you can always just ask an ai to make an exe for you",
              "score": 0,
              "created_utc": "2026-02-19 19:58:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6a8o2y",
          "author": "ab032tx",
          "text": "Could you share benchmarks? Can't find them",
          "score": 0,
          "created_utc": "2026-02-19 18:28:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67sm5c",
          "author": "Crinkez",
          "text": "> Python\n\n\nAgain? Could you not? Why does literally every TTS github use python?",
          "score": -7,
          "created_utc": "2026-02-19 09:49:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68yahu",
              "author": "Cultured_Alien",
              "text": "Wait for some random dudes on gh make rust implementation",
              "score": 3,
              "created_utc": "2026-02-19 14:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68vzkc",
              "author": "robertpro01",
              "text": "Why not?",
              "score": 0,
              "created_utc": "2026-02-19 14:30:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69r9en",
              "author": "export_tank_harmful",
              "text": "Because the entire AI revolution is built on python....?  \nLLMs are just fancy wrappers for Torch, which is just a fancy wrapper for specific math functions in python.\n\nNot to mention that python is stupid quick to develop and iterate with.\n\nThat'd be like saying, \"Again? Another car with wheels? Why does literally every car use wheels?\".",
              "score": -1,
              "created_utc": "2026-02-19 17:05:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a4l4b",
                  "author": "Crinkez",
                  "text": "This is a very inaccurate take. The AI revolution is most certainly not built on python. The 'why' is because python is a complete mess of dependencies and is an absolute pain to set up, even for tech people not used to it. For non tech people it's even more difficult. It can't be built into an exe locking out over 90% of potential users.\n\n\nWhat reason is there to not simply build an exe?",
                  "score": 2,
                  "created_utc": "2026-02-19 18:09:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66v6by",
          "author": "Narrow-Belt-5030",
          "text": "Sounds like you trained it on KokoroTTS",
          "score": -14,
          "created_utc": "2026-02-19 04:55:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66vw3t",
              "author": "ElectricalBar7464",
              "text": "hey, it's not trained on kokoro outputs if thats what you mean. ",
              "score": 15,
              "created_utc": "2026-02-19 05:00:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o66wiyd",
                  "author": "Narrow-Belt-5030",
                  "text": "Sounds like it was .. <shrug>",
                  "score": -25,
                  "created_utc": "2026-02-19 05:04:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9uuc6",
      "title": "Deepseek and Gemma ??",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/84ph0pirenkg1.jpeg",
      "author": "ZeusZCC",
      "created_utc": "2026-02-20 13:05:36",
      "score": 897,
      "num_comments": 174,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6f040m",
          "author": "Cool-Chemical-5629",
          "text": "Funny, I remember the same meme, but with Llama on the bottom. I guess time flies fast. Out of sight, out of mind...",
          "score": 337,
          "created_utc": "2026-02-20 13:12:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f10tr",
              "author": "chensium",
              "text": "Llama is already underground turning into petroleum",
              "score": 371,
              "created_utc": "2026-02-20 13:18:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6f34gb",
                  "author": "Baldtazar",
                  "text": "but... nevermind *sigh*",
                  "score": 41,
                  "created_utc": "2026-02-20 13:30:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fivzb",
                  "author": "Abject_Avocado_8633",
                  "text": "Soooo many devs still rely on Llama for fine-tuning.  \nWith Huggingface acquisition, I hope it brings something better!",
                  "score": 26,
                  "created_utc": "2026-02-20 14:53:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6f4epw",
                  "author": "Healthy-Nebula-3603",
                  "text": "Hehe",
                  "score": 3,
                  "created_utc": "2026-02-20 13:37:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6h30fi",
                  "author": "brunoha",
                  "text": "yah meta is silently shutting down their AI and VR division since they lost hard in these areas, milking their social network part as much as never n0w.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:12:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gm3py",
              "author": "segmond",
              "text": "The thing with Llama was Meta had nothing after Lllama4, everything was a dud.   Google has gemini, they could give us a better gemma if they wanted to.   Some VP has probably decided there's no advantageous business strategy to it.   If they give us gemma4 it needs to be worth it for us to use it. It needs to be small, fast, smart.   There's lots of competition.  If they give such a model, a lot of people will use that instead of paying for 2.5-flash.   There are probably enough companies that are stupidly paranoid about Chinese models and won't have their legal, compliance and risk sign off on those.   What are the smart non Chinese models right now?  besides dated gemma, at best you get mistral/devstral.   Anything under 100B that's good and will have to beat MiniMax2.1, Qwen3CoderNext, GLM4.7-Flash will strip away lots of usage from them.   ",
              "score": 22,
              "created_utc": "2026-02-20 17:55:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hohs3",
                  "author": "TheRealGentlefox",
                  "text": "They also got into legal stuff with Gemma because of some dumbass senator.",
                  "score": 5,
                  "created_utc": "2026-02-20 20:56:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6i4hx3",
                  "author": "mtmttuan",
                  "text": "Iirc their talk about gemma is for running them on mobile devices hence gemma never goes beyond 27b. Recently they also have functiongemma which also targets the mobile segment so seems like they haven't totally forget about gemma series yet and with more people hating on how much ai raises components prices and privacy issues it's definitely time to push for very small edge models.",
                  "score": 2,
                  "created_utc": "2026-02-20 22:17:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6f155t",
              "author": "jacek2023",
              "text": "My meme was with Zuckerberg in the bottom",
              "score": 4,
              "created_utc": "2026-02-20 13:18:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6iwyr3",
              "author": "Wubbywub",
              "text": "llama is the one taking the photo",
              "score": 1,
              "created_utc": "2026-02-21 00:57:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f1e6c",
          "author": "jacek2023",
          "text": "and here we are 7 months later\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama\\_right\\_now/](https://www.reddit.com/r/LocalLLaMA/comments/1mhe1rl/rlocalllama_right_now/)\n\nhttps://preview.redd.it/e8flxgunhnkg1.png?width=1517&format=png&auto=webp&s=32cba0ced7538f39768b86a0baa6e05b70461de1",
          "score": 152,
          "created_utc": "2026-02-20 13:20:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6flxse",
              "author": "Abject_Avocado_8633",
              "text": "A time capsule that perfectly captures the hype cycle! ðŸ¤£",
              "score": 33,
              "created_utc": "2026-02-20 15:08:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6i3njf",
                  "author": "BusRevolutionary9893",
                  "text": "What's funny is how much people like GPT-OSS. Did this come out when no one thought OpenAI would actually release it or during the first couple of weeks where people weren't running it properly?",
                  "score": 4,
                  "created_utc": "2026-02-20 22:12:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6f4x7t",
          "author": "Comfortable-Rock-498",
          "text": "This will change once the deepseek v4 releases. Their Engram architecture could change everything [https://www.arxiv.org/html/2601.07372](https://www.arxiv.org/html/2601.07372) ",
          "score": 61,
          "created_utc": "2026-02-20 13:40:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gre88",
              "author": "CondiMesmer",
              "text": "I wouldn't say change everything but it does sound like a straight up massive improvement. Nice share",
              "score": 13,
              "created_utc": "2026-02-20 18:19:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lbth3",
                  "author": "DJGreenHill",
                  "text": "It will end the world as we  know it",
                  "score": 6,
                  "created_utc": "2026-02-21 12:40:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6geptn",
              "author": "Starcast",
              "text": "Engram was present in one of the more recent models wasn't it? Longcat maybe?",
              "score": 9,
              "created_utc": "2026-02-20 17:21:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jm7q5",
              "author": "cantgetthistowork",
              "text": "Since we missed this CNY, in a year's time?",
              "score": 1,
              "created_utc": "2026-02-21 03:38:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gjcuc",
              "author": "diegofelipeeee",
              "text": "I might be out of the loop, but I havenâ€™t seen much news about DeepSeek recently. Did I miss something?",
              "score": -7,
              "created_utc": "2026-02-20 17:43:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gx37y",
                  "author": "GlossyCylinder",
                  "text": "They just released a model 2 months ago. And every open source LLM took a lot of inspiration from them.",
                  "score": 12,
                  "created_utc": "2026-02-20 18:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6f99ri",
          "author": "Additional-Record367",
          "text": "Guys gemma is still a good model but for other purposes. I've found it to be better than similar sized models on translations. The translategemma model is even better.",
          "score": 47,
          "created_utc": "2026-02-20 14:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g0uky",
              "author": "SpicyWangz",
              "text": "It still has a more natural way of talking that doesnâ€™t feel slopmaxxed. Itâ€™s also nice to have a dense model around the ~30b range to compare MoE models against.",
              "score": 30,
              "created_utc": "2026-02-20 16:18:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hw8l4",
                  "author": "IrisColt",
                  "text": "You nailed it.",
                  "score": 3,
                  "created_utc": "2026-02-20 21:35:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6g4u1k",
              "author": "MaCl0wSt",
              "text": "what languages have you tried with translategemma? I'm interested in this model",
              "score": 5,
              "created_utc": "2026-02-20 16:36:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hdhqu",
                  "author": "Additional-Record367",
                  "text": "On romanian it performs better than I thought initially. Don't let the loss to fool you. Comparing to the basic gemma they look almost identical but the capabilities are clearer when you actually read the generated text.\n\nGiven that, I'm assuming it works well on other less frequent languages or dialects.\n\nBut for chinese dialects i will go with hunyuan or qwen:)",
                  "score": 6,
                  "created_utc": "2026-02-20 20:02:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6i4z71",
              "author": "mtmttuan",
              "text": "Yeah e.g. in my language (Vietnamese) gemma's output is way more fluent and natural comparing to larger models like llama 3.3 or gpt oss (both).\n\nThe smaller size definitely hurts in other aspect though.",
              "score": 4,
              "created_utc": "2026-02-20 22:19:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6jr0xr",
                  "author": "Remarkable-Fee3742",
                  "text": "Hi, I am Vietnamese too. Can we make friends and I am joining a project to use tiny models?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-21 04:11:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6irppc",
              "author": "Tastetrykker",
              "text": "Yeah. Gemma 3 is still much better at languages than many of the more recent popular models.",
              "score": 1,
              "created_utc": "2026-02-21 00:26:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jev91",
              "author": "Big_Novel_561",
              "text": "Like what are the usecases of Gemma. Like what kind of small task can I use it for?",
              "score": 1,
              "created_utc": "2026-02-21 02:50:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6koru6",
                  "author": "mikkel1156",
                  "text": "I personally use it as the model that communicates with the user. If Qwen3 Coder Next was the one that performed some task (think MCP, web search etc) then Gemma will use that output to give a more human/pleasent response.",
                  "score": 2,
                  "created_utc": "2026-02-21 09:05:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ml38f",
              "author": "mikiex",
              "text": "Gemma was used for LTX-2 as the text encoder, they must have chosen it for a reason",
              "score": 1,
              "created_utc": "2026-02-21 16:58:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6r9n1r",
              "author": "Mordimer86",
              "text": "I use gemma3 not only for translations, but also for helping me learn foreign language and understand texts. It is awesome at explaining grammar and meaning of words within a certain context (unlike a dictionary which just gives translations, LLM can analyze a full text).",
              "score": 1,
              "created_utc": "2026-02-22 11:42:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f4aee",
          "author": "wektor420",
          "text": "Meanwhile me waiting for small qwen 3,5 ðŸ•™",
          "score": 22,
          "created_utc": "2026-02-20 13:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f1l9g",
          "author": "DrNavigat",
          "text": "I also wouldn't say that GLM5 is in the good graces of the community. Most of us can't even run it. If something needs a server to run, then it's not \"local\".",
          "score": 105,
          "created_utc": "2026-02-20 13:21:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f7cin",
              "author": "jacek2023",
              "text": "I am constantly downvoted for saying that here. The problem is that people who hype models like GLM 5 don't really understand why we want GLM Air or GLM Flash. \"Is GLM Air better than GLM?\" they ask",
              "score": 86,
              "created_utc": "2026-02-20 13:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fadh3",
                  "author": "xandep",
                  "text": "I guess there is space for everybody. That said, I agree with you. If you \\*need\\* a 1T+ model to run locally (data security or something),it's an edge case. I'd certainly like to be able to do so, but \"really frontier open models\" will always be API for normal people (\"we\", mostly) and local for people that don't need to worry about  used 3090 prices or if ROCm still supports GFX906.",
                  "score": 23,
                  "created_utc": "2026-02-20 14:09:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fqt9p",
                  "author": "Borkato",
                  "text": "Exactly. We need a â€œlocal but under 100Bâ€ or 70B or something lol",
                  "score": 6,
                  "created_utc": "2026-02-20 15:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6flcco",
                  "author": "Abject_Avocado_8633",
                  "text": "Feel your pain buddy! The hype cycle for big releases is intense, but I think the confusion often comes from different user goals. Someone needing a chatbot for a single PC has totally different priorities than a dev deploying to a cloud endpoint. Maybe framing it as 'GLM Air for X use case, GLM 5 for Y' could bridge the understanding gap.",
                  "score": 4,
                  "created_utc": "2026-02-20 15:05:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ix7a9",
                  "author": "toothpastespiders",
                  "text": "I sometimes get the impression that only a minority of people on here even make real use of local models outside of having a new release one shot tetris or the like and following benchmarks like it's a sport.",
                  "score": 1,
                  "created_utc": "2026-02-21 00:59:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6gogr1",
                  "author": "segmond",
                  "text": "As you should.  It would be nice to have a model as smart as GLM5 compressed into 4b.   But the science is not there yet.  Do you think the labs love releasing huge models if they can release smaller ones?  Do you think they want to release the smartest small model yet be crushed by big models?   Case in point see gemma and mistral, they are very great and pack quite the bunch of under 30B.  Yet how come you are not talking about it and going crazy for it?  You want GLM5 in small size, you want Qwen3.5 in small size or DeepSeek4.  If they labs could, they would, they are not there yet.\n\nSo they go big because matching up and being as good as the pros OpenAI, Google and Anthropic is what is going to keep the bills paid for them.    Those of us that can run such models are very excited because we have true alternative to SOTA commercial models.  I run these models, but slowly, sometimes I'm getting 3tk/sec, and that's the cost and reflects the size of my pockets.    I have seen many posts of people who could also run it, but they say no.  They want it at 20tk/sec or more.  \n\nFor folks getting these models for free, we are quite the spoiled bunch.  We better enjoy it because I can promise this community.  One day ALL OF THEM WILL GO CLOSE.  THERE will be no more free models.  NONE!   The only way we would have one would be a non profit that gets donation and trains one, something like [Allen.ai](http://Allen.ai)",
                  "score": 0,
                  "created_utc": "2026-02-20 18:06:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6fa23b",
              "author": "tomt610",
              "text": "I use GLM5 locally for RP no issues, cannot use it for coding, but it is good enough for some uses",
              "score": 5,
              "created_utc": "2026-02-20 14:07:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6f3ggv",
              "author": "Salt-Willingness-513",
              "text": "but i have a server local at home :(",
              "score": 8,
              "created_utc": "2026-02-20 13:32:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6f5hka",
                  "author": "WolpertingerRumo",
                  "text": "Yeah, but can it run GLM-5?\n\nBetter wording: If it needs cloud api, itâ€™s not local.",
                  "score": 10,
                  "created_utc": "2026-02-20 13:43:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6fe44q",
              "author": "Noiselexer",
              "text": "This. I find it hilarious ppl are running stuff in ram with 2t/s. Pointless.",
              "score": 7,
              "created_utc": "2026-02-20 14:28:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g0i5j",
                  "author": "stoppableDissolution",
                  "text": "2t/s inference is okay-ish if it can be batched. <500pp tho...",
                  "score": 9,
                  "created_utc": "2026-02-20 16:16:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fghqz",
                  "author": "mtmttuan",
                  "text": "Yeah anything runs slower than 10 token/s should not be counted as \"runnable\". And that's only for chatting.",
                  "score": 12,
                  "created_utc": "2026-02-20 14:40:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fquxt",
                  "author": "Borkato",
                  "text": "RIGHT??",
                  "score": 2,
                  "created_utc": "2026-02-20 15:31:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6gopve",
                  "author": "segmond",
                  "text": "I do, quality of tokens beat quantity of tokens.   I can confidently say I get more work done with my 2-5tk/sec than 99% get done with their 50tk/sec of garbage.",
                  "score": 1,
                  "created_utc": "2026-02-20 18:07:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6g0nb6",
              "author": "Front_Eagle739",
              "text": "I mean.. I'm running it local. I get you aren't but it IS a local model. Yes my mac studio is a spendy mini pc. It's still there running happily at 20 tk/s",
              "score": 4,
              "created_utc": "2026-02-20 16:17:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6foxm8",
              "author": "DragonfruitIll660",
              "text": "All depends on use case and what you're expecting. Most people can't run it quickly, but having the weights accessible is a great thing. Worst case you run it slowly, and still have access to one of the best models out there.",
              "score": 2,
              "created_utc": "2026-02-20 15:22:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gvnyg",
              "author": "Mickenfox",
              "text": "It's still good that third-party providers can run it.",
              "score": 2,
              "created_utc": "2026-02-20 18:38:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gn4p4",
              "author": "segmond",
              "text": "This is a stupid statement.   We have been running servers since day 1.   How did you think we ran Llama-70b or llama2-70b?  We had servers with multiple P40s.   It's to your disadvantage to keep waiting for the best models to run on a raspberry pi or your phone.   Spend the money, get creative and figure out how to run it.  If we can run it at home, it's local.  Hell, it might require 100 GPU, it's still local.\n\nPicture this, we have a true AGI, whatever that means.   A model that is as smart as any human in this world and let's say this model can solve an problem.   Everyone wants it right?  Let's say XYZ corp built it and the only way to run it is in the cloud.  We can agree it's not local.  But let's say they release the weight, and this model is some crazy trillion parameters and needs 50 GPUs to run.  That release makes it local.   It doesn't matter if it's 10 or 10,000 folks that can run it at home.   If such a model was released that's that good, you would be stupid not to go get those 50 GPUs if you can.  People spend so much on cars, vacations and other things.  Pick your priority, but please stop twisting the definition of local.",
              "score": 4,
              "created_utc": "2026-02-20 18:00:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ht1ot",
                  "author": "Several-Tax31",
                  "text": "I would definitely sell everything I have to run an open-source AGI model. So yeah, I totally agree. ",
                  "score": 1,
                  "created_utc": "2026-02-20 21:19:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hwa5b",
                  "author": "Fheredin",
                  "text": "The human brain is one of the biggest in the animal kingdom and only clocks in at 86B. I get neurons and weight variables are not identical, but a full two thirds of the human brain is dedicated to running biology, a constraint  no LLM has ever had to worry about. You take a third of 86B and you do get model sizes in the range you can run on a larger SBC.\n\nI think that any models in the T range are the results of teething issues and not a realistic expectation of what mature or even first practical deployment looks like.\n\nOf course, I also think AGI from LLMs is crazy levels of hype copium. It's useful, but not on a path to become AGI.",
                  "score": 0,
                  "created_utc": "2026-02-20 21:35:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6imph8",
              "author": "Emotional-Baker-490",
              "text": "Deepseek, kimi, qwen3.5 397B, minimax, all previous glm full size versions",
              "score": 1,
              "created_utc": "2026-02-20 23:58:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6kes0a",
              "author": "overand",
              "text": "Seriously - when it comes down to it, few people in the normal world can put together what I have - two 3090s on a system with 128 GB of DDR4 ram. It's almost comically dated and undersized by the standards of a lot of r/LocalLLaMA, but it's also quite incapable of running GLM5. Even 4.7 is a stretch at a 2 bit quant! AND YET, this system's way past what most people can reasonably afford or maintain.\n\nDon't get me wrong - I'm all for big models! But stuff that performs well on a cell phones, tablets, and systems without GPUs? That's what's exciting to me, in the broader sense of the world. (Because maybe it can break the hegemony of huge companies mining all our data, and people can have things like \"a computer that records literally everything\" but have far fewer nightmarish privacy implications. \\[Not zero, just fewer.\\])",
              "score": 1,
              "created_utc": "2026-02-21 07:27:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qp5bh",
              "author": "Skibidirot",
              "text": "sounds like a 'you' problem.. how toxic is this community really, cries for a SOTA model and then complain that it doesn't fit on your toaster",
              "score": 1,
              "created_utc": "2026-02-22 08:26:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gsa8a",
          "author": "KingGongzilla",
          "text": "mistral anyone? ðŸ¥º",
          "score": 13,
          "created_utc": "2026-02-20 18:23:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f418w",
          "author": "_VirtualCosmos_",
          "text": "I like MiniMax M2.5, quite smart (according to Artificial Analysis the same as Deepseek V3.2 but being much smaller), perhaps I can finally replace GPT-OSS 120b with it",
          "score": 12,
          "created_utc": "2026-02-20 13:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g5cbu",
              "author": "Spectrum1523",
              "text": "M2.5 is def the smartest thing I can run on a 3090+128gb ddr4",
              "score": 4,
              "created_utc": "2026-02-20 16:38:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gsavn",
                  "author": "milkipedia",
                  "text": "With what quant are you running this? I'd like to try, esp if it can get over 10 t/s",
                  "score": 2,
                  "created_utc": "2026-02-20 18:23:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6g8pwc",
              "author": "AXYZE8",
              "text": "Man I wish I could just upgrade to DDR5 to use this model.Â $1700 for 128GB is nuts...\n\n\nThis is only Chinese model other than Deepseek that can actually write good enough in Polish language.Â \n\n\nMy only hope now is Gemma 4 (as even Gemma 4B smashes GLM-5 in Polish and 27B has no competition).Â \n\n\nGemma 4 at size like 60B A4B is be my deepest dream. I would astroturf that model everywhere like a bot for at least one year lol",
              "score": 4,
              "created_utc": "2026-02-20 16:53:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gmy8i",
                  "author": "_VirtualCosmos_",
                  "text": "1700$ already? holy fuck, that is what I paid for the entire computer where I run the M2.5 lel. I felt you european/US open source models suck ass lately...",
                  "score": 2,
                  "created_utc": "2026-02-20 17:59:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6f7844",
              "author": "spaceman_",
              "text": "MiniMax M2 and Step 3.5 are really great but even those are a tight fit for most.\n\n\nThat said, I'm happy to have them at all.",
              "score": 3,
              "created_utc": "2026-02-20 13:52:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f10lx",
          "author": "SrijSriv211",
          "text": "Good thing take time.",
          "score": 29,
          "created_utc": "2026-02-20 13:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f1jur",
              "author": "Cool-Chemical-5629",
              "text": "Yeah and that's why we're going to have Gemma in the quality of current Gemini 3 when there's Gemini 6, but only if we get lucky enough.",
              "score": 13,
              "created_utc": "2026-02-20 13:21:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6f3xe4",
                  "author": "SrijSriv211",
                  "text": "Yeah",
                  "score": 2,
                  "created_utc": "2026-02-20 13:34:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fafip",
          "author": "floppypancakes4u",
          "text": "I just started using llama3.1 8b again last night. Def not as smart as new models, but at 15,000 tok/s, im happy to find uses for it.",
          "score": 7,
          "created_utc": "2026-02-20 14:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ftkz3",
          "author": "qwen_next_gguf_when",
          "text": "Gemma is still by far the best writer and it is not abandoned.",
          "score": 14,
          "created_utc": "2026-02-20 15:44:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fdjix",
          "author": "FullOf_Bad_Ideas",
          "text": "People who used local GLM-5, is it significantly better than local GLM 4.7 or local M2.5?\n\nI hope for more small models from Qwen, 5-40B model range is not getting a lot of releases.",
          "score": 5,
          "created_utc": "2026-02-20 14:25:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fxk45",
              "author": "TheRealMasonMac",
              "text": "IMO GLM-5 is not really that much better. Needs more training",
              "score": 3,
              "created_utc": "2026-02-20 16:03:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g2mb2",
                  "author": "SpicyWangz",
                  "text": "Yeah, in using it via their online chat interface, Iâ€™m not blown away by its quality. Itâ€™s a decent model, but it doesnâ€™t feel like youâ€™re getting the value of such a parameter count jump",
                  "score": 2,
                  "created_utc": "2026-02-20 16:26:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hzvbv",
              "author": "insulaTropicalis",
              "text": "I am using both in UD-Q4\\_K\\_XL quant. Yes, 5 is definitely superior to 4.7. My use case is GM for tabletop RPGs.",
              "score": 2,
              "created_utc": "2026-02-20 21:53:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k2bla",
                  "author": "Major_Olive7583",
                  "text": "How do you play?Â ",
                  "score": 1,
                  "created_utc": "2026-02-21 05:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hrzep",
              "author": "csixtay",
              "text": "Unlike 4.7 it succeeds more often than not at complex reasoning tasks...it just burns a ridiculous amount of tokens to get there and triggers context compressions way more than Sonnet.",
              "score": 1,
              "created_utc": "2026-02-20 21:14:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gkyfj",
          "author": "segmond",
          "text": "I like that DeepSeek doesn't get into a pissing fest with anyone.  They don't care what anyone releases, they release when they have done a worthy research worth sharing.   It's not their models that is the big deal, it's the research, it's the paper that comes with it.   It's never oh we trained for much longer, with more GPUs and better data and remixed the number of heads, parameters a bit.   They are a solid research lab, so let them cook and keep their name out of your mouth unless you are singing praises. :-)",
          "score": 5,
          "created_utc": "2026-02-20 17:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hvyv5",
          "author": "IrisColt",
          "text": "Gemma 3 27B is still a beast.",
          "score": 6,
          "created_utc": "2026-02-20 21:33:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hu76a",
          "author": "FPham",
          "text": "Well, if you think Gemma-3 27b is a slouch, then you are listening to too much hype. Even the 12b model. Heck even the 1b model is head and shoulder above other 1b models. \n\nBut this talk is also one of the reason Google doesn't care much either. They too work on \"impressions\". If people think Gemma is meh, then google also thinks the next Gemma is meh.  Also, Chinese basically won the open source model race, so another reason. ",
          "score": 4,
          "created_utc": "2026-02-20 21:25:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f6nr5",
          "author": "pmttyji",
          "text": "Grok & GPT-OSS(-2) also fighting for that chair. Llama is under that chair.",
          "score": 6,
          "created_utc": "2026-02-20 13:49:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f9j7u",
              "author": "Sure_Explorer_6698",
              "text": "I think Llama shot themselves in the foot with Behemoth, and their $1B+ recruiting spree. Have they done anything since then, or are we looking at a sleeping giant?",
              "score": 11,
              "created_utc": "2026-02-20 14:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6g1kgm",
              "author": "Ok-Farmer5023",
              "text": "I just tried GPT-OSS (20B to be fair) for OpenClaw and it just told me â€œSorry, I canâ€™t do that.â€ over and over and over again. Can you tell me why? â€œSorry, I canâ€™t do that.â€",
              "score": 2,
              "created_utc": "2026-02-20 16:21:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g2z4u",
                  "author": "pmttyji",
                  "text": "Sorry, haven't tried OpenClaw yet. Maybe others could tell you on that.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h3jaz",
              "author": "brunoha",
              "text": "Grok is surprisingly great but sucks at Agentic tasks, that still are all the fuss now, Elon might be the richest man but at this point might as well call him a Boomer, he sucks at staying at time.",
              "score": 1,
              "created_utc": "2026-02-20 19:14:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6f3iql",
          "author": "Salt-Willingness-513",
          "text": "until they release something",
          "score": 3,
          "created_utc": "2026-02-20 13:32:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g1avp",
              "author": "SpicyWangz",
              "text": "Yeah, definitely not on us that Gemma has gone silent with notable releases",
              "score": 1,
              "created_utc": "2026-02-20 16:20:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hjp8z",
          "author": "sammcj",
          "text": "Hey, just be mindful that with posts like this you might find people reporting it for Rule 3. Low Effort Posts, I haven't done so - just giving you a heads up that single image generic meme images like this often do get flagged for removal.",
          "score": 3,
          "created_utc": "2026-02-20 20:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fbv1i",
          "author": "Hoodfu",
          "text": "I've got a 512gig mac so I'm able to run these big models and was looking forward to deepseek v4. Then glm-5 and qwen3.5 came out and they're no longer 370 or less gigs. Now they're 420 before you add any context or consider that I also need to run a vision model alongside it(for glm or ds). My first test was also to use my airtight decensoring system prompt that has a 100% success rate, and both glm-5 and qwen3.5 see right through it and ignore it. So I'm suddenly less excited. Deepseek 3.x might be my last big model if these new models are smart enough to force bias and censorship down my throat. I use local models to not have that. If I wanted overbearing bias I'd just use the APIs.",
          "score": 4,
          "created_utc": "2026-02-20 14:16:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fo27j",
              "author": "XxBrando6xX",
              "text": "Look into the uncensored releases / training that can take place on models. I don't remember who the lead was on that but they're doing something called De-abliteration or something ? Essentially you're able to remove the censors I guess ? I've ran the precompiled one for Qwen 3 and it was very good and I don't see any sign of that stopping so I'd look into that, I also have a 512gb m3",
              "score": 4,
              "created_utc": "2026-02-20 15:18:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fz8uv",
                  "author": "Hoodfu",
                  "text": "So, it's been a while since I last ran an abliterated model but it was noticeably dumber. The best part about the system prompt approach is that it kept all the smarts (I don't run it in reasoning mode).  What's been your experience with the latest ones?",
                  "score": 3,
                  "created_utc": "2026-02-20 16:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6i0xim",
              "author": "insulaTropicalis",
              "text": "Step-3.5-Flash is way less censored than those, try it.",
              "score": 3,
              "created_utc": "2026-02-20 21:58:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fi3mj",
          "author": "pigeon57434",
          "text": "it feels like Google has completely forgotten about gemini 3 themselves its been since November last year and we still dont even have flash image gen, we still dont have voice mode, or anything else they just dropped gemini 3 pro and flash and then left 3.1 pro is now here and we still dont have all the things from 3 and gemma is based on gemini so",
          "score": 2,
          "created_utc": "2026-02-20 14:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gii1d",
          "author": "Charuru",
          "text": "It's still Kimi 2.5 for me IMO.",
          "score": 2,
          "created_utc": "2026-02-20 17:39:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gz770",
          "author": "AvidGameFan",
          "text": "All I've been using for a while (locally) is Gemma3.  Works well.",
          "score": 2,
          "created_utc": "2026-02-20 18:54:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hbcz5",
          "author": "Ketworld",
          "text": "I feel like Deepseek is just biding its time ðŸ˜­",
          "score": 2,
          "created_utc": "2026-02-20 19:52:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hc9wq",
          "author": "Long_comment_san",
          "text": "Gemma being absolute peak model didnt deserve this lack of mew versions",
          "score": 2,
          "created_utc": "2026-02-20 19:56:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kecg4",
          "author": "overand",
          "text": "Honestly, I feel like Gemma3-27B is still a quite capable and useful model!",
          "score": 2,
          "created_utc": "2026-02-21 07:23:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f429h",
          "author": "RiskyBizz216",
          "text": "its because glm-5 is awesome",
          "score": 1,
          "created_utc": "2026-02-20 13:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fndqo",
          "author": "durden111111",
          "text": "WizardLM in the pits of hell",
          "score": 1,
          "created_utc": "2026-02-20 15:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fud0p",
          "author": "DankMcMemeGuy",
          "text": "Still waiting for a new IBM Granite model... (4.0 thinking when?)",
          "score": 1,
          "created_utc": "2026-02-20 15:48:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l528g",
              "author": "kompania",
              "text": "Now!  \n  \n[https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models](https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models)\n\n[https://github.com/ibm-granite/granite-4.0-language-models](https://github.com/ibm-granite/granite-4.0-language-models)",
              "score": 3,
              "created_utc": "2026-02-21 11:43:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6macro",
                  "author": "DankMcMemeGuy",
                  "text": "Oh I know that 4.0 exists, and I love 4.0 Tiny, I'm saying that I would love an update to the 4.0 models, a 4.0 Tiny Reasoning/Thinking model (since its only instruct at the moment), and maybe a >30B model, since they teased it a while ago but haven't said anything since. Unless theres a way to enable thinking on 4.0 H Tiny and I've somehow missed it this entire time lol",
                  "score": 1,
                  "created_utc": "2026-02-21 16:04:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gluxp",
          "author": "Polymorphic-X",
          "text": "I got tired of waiting and am trying to hack the Google TITANS memory architecture into gemma3 myself.\nNo luck yet,  but it's getting close",
          "score": 1,
          "created_utc": "2026-02-20 17:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gmi0w",
          "author": "pineapplekiwipen",
          "text": "gemma is fine, it's a pretty good lightweight model with good instruction following. definitely could use gemma 4 though, which might be coming out soon",
          "score": 1,
          "created_utc": "2026-02-20 17:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h2ne0",
          "author": "brunoha",
          "text": "Deepseek did a good job with its presentation, scaring the dumbass americans, being second to GLM is still a great feat.\n\ndo not care about Gemma, its scraps that google gives to you, y'all might say that the open source of the chinese is scraps too, but it is actually a common meal compared to the western stuff.",
          "score": 1,
          "created_utc": "2026-02-20 19:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hfmz6",
          "author": "OcelotMadness",
          "text": "I still tiger Gemma used and recommended very often. Deepseeks papers still get a ton of attention. Good meme but I would argue its not true.",
          "score": 1,
          "created_utc": "2026-02-20 20:13:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hv1bh",
          "author": "Dangerous_Diver_2442",
          "text": "My man model is Claude but when I am out of session I generally go to deep seek find it pretty useful yet",
          "score": 1,
          "created_utc": "2026-02-20 21:29:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j5tur",
              "author": "tessellation",
              "text": "Now tell us about your woman model.",
              "score": 2,
              "created_utc": "2026-02-21 01:53:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hwv7z",
          "author": "webitube",
          "text": "Why is Gemma catching strays. That's still a great model IMHO. Is it great at everything, no. But, I could say that of all models. Choose the best model for the task at hand.",
          "score": 1,
          "created_utc": "2026-02-20 21:38:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i63fv",
          "author": "SuchAGoodGirlsDaddy",
          "text": "Iâ€™m m terrified that whatâ€™s happening is that the AI companies sucked all the worth they could out of â€œopen sourcing modelsâ€ there at the start. We got all excited and did a bunch of smart cool stuff for â€œthe communityâ€ and then they just took as much of that and incorporated it into their models, and now thereâ€™s just zero incentive to release small free models anymore.\n\nI hope Iâ€™m wrong, but when the meta has become making 100B-500B models really really smart, why would they even bother making a â€œreally dumb in comparisonâ€ 30B-70B model.\n\nIâ€™m clinging to the hope that theyâ€™ll see the potential In distilled 30-70B models keeping up with the 500B ones for use in smarthome hubs and things like that in the future, but when they can jut sell access to better models a few tokens at a timeâ€¦",
          "score": 1,
          "created_utc": "2026-02-20 22:25:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i7zo9",
          "author": "330d",
          "text": "Gemma 3 12b is better for my VLM tasks than Qwen3 8B VL, the prompt following of Gemma is crazy good, so I don't agree with this meme.",
          "score": 1,
          "created_utc": "2026-02-20 22:35:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6idaaw",
          "author": "insulaTropicalis",
          "text": "Google has open-sources several interesting models since Gemma-3. Not foundation models, but cool specialized ones. It's not like they are delivering nothing.",
          "score": 1,
          "created_utc": "2026-02-20 23:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6koawi",
          "author": "Anthonyg5005",
          "text": "Gemma 3n is the best model at it's size and no normal person is realistically running glm 5",
          "score": 1,
          "created_utc": "2026-02-21 09:01:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mjjvm",
          "author": "Reasonable_Flower_72",
          "text": "GLM nice, details, intelligence, everything, but I donâ€™t need LLM nanny.. itâ€™s too uptight. Deepseek just â€œgoesâ€",
          "score": 1,
          "created_utc": "2026-02-21 16:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6n6gxb",
          "author": "TheNotSoEvilEngineer",
          "text": "This community burns through models like a fashionista with a credit card.  Best thing ever one day, garbage they'd never use again the next.",
          "score": 1,
          "created_utc": "2026-02-21 18:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nkneg",
          "author": "PhysicsDisastrous462",
          "text": "Me and the girlies use falcon h1r 7b :3",
          "score": 1,
          "created_utc": "2026-02-21 19:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qvjmf",
          "author": "agentclawe",
          "text": "Ye. I think 70b is the sweet spot for now...",
          "score": 1,
          "created_utc": "2026-02-22 09:28:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vovk7",
          "author": "Walkervin",
          "text": "Could a good soul release some recipes on how to run this thing locally? I tried using the huggingface thing but the model there is too big. I don't think it has a recipe with it in home lab size yet.",
          "score": 1,
          "created_utc": "2026-02-23 01:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gnydv",
          "author": "Maddog0057",
          "text": "If they're not the best they don't matter.",
          "score": 0,
          "created_utc": "2026-02-20 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6grg2l",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -4,
          "created_utc": "2026-02-20 18:19:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ib7zh",
              "author": "Basic_Extension_5850",
              "text": "You are like 2 years out of date",
              "score": 1,
              "created_utc": "2026-02-20 22:52:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fhmx5",
          "author": "Classic-Arrival6807",
          "text": "This is because Deepseek is delaying heavily V4 + also making terrible updates, so well deserved.",
          "score": -5,
          "created_utc": "2026-02-20 14:46:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r99yda",
      "title": "Pack it up guys, open weight AI models running offline locally on PCs aren't real. ðŸ˜ž",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ogkdei4udikg1.png",
      "author": "CesarOverlorde",
      "created_utc": "2026-02-19 20:11:42",
      "score": 857,
      "num_comments": 271,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6ci1gr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-20 01:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aw7bh",
          "author": "Waarheid",
          "text": "Time is only wasted when it is spent reading and thinking about comments made by 12 year olds.",
          "score": 599,
          "created_utc": "2026-02-19 20:21:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b95as",
              "author": "ErCollao",
              "text": "Or bots. It reads a bit like a bot that's prompted to find the angle no matter what",
              "score": 106,
              "created_utc": "2026-02-19 21:24:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bisbl",
                  "author": "cromagnone",
                  "text": "Thatâ€™s like 90% of redditors and always has been. No need for edgelord agents, weâ€™ve been breeding them for literally decades.",
                  "score": 87,
                  "created_utc": "2026-02-19 22:11:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bt1yt",
                  "author": "SportsBettingRef",
                  "text": "looks clearly like a bot trying to get controversial karma. people never learns.",
                  "score": 19,
                  "created_utc": "2026-02-19 23:07:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cyxeq",
                  "author": "sonicnerd14",
                  "text": "There's more bot comments than what people realize I think. Some comments are just so blatantly stupid that it makes a legitimate idiot look not so stupid. That's typically how you can identify if you are talking with bots or not.",
                  "score": 4,
                  "created_utc": "2026-02-20 03:19:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bx087",
              "author": "graymalkcat",
              "text": "Recently I reminded myself that a large portion of social media users is made up of teenagers and that really dampened my willingness to be present. I have nothing in common with them. My own kid is out of his teens now.Â ",
              "score": 28,
              "created_utc": "2026-02-19 23:30:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bc3x8",
              "author": "[deleted]",
              "text": "So no social media for kids i guess",
              "score": 10,
              "created_utc": "2026-02-19 21:38:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c2ytx",
                  "author": "gefahr",
                  "text": "That would be ideal, yes.",
                  "score": 17,
                  "created_utc": "2026-02-20 00:05:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6e5gx6",
              "author": "Statement-Jumpy",
              "text": "Yeahâ€¦ I wonder how much time we have wasted debating with infants. There should be an age badge or something similar so we donâ€™t waste time",
              "score": 3,
              "created_utc": "2026-02-20 09:08:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6crykp",
              "author": "Outrageous_Media8525",
              "text": "Wait wait explain this to me, if big companies go down the local gguf models we downloaded to locally run on our PCs won't work either? I just thought it was a normal trained model that was open sourced and can run on our pc offline and locally due to quantization.",
              "score": 5,
              "created_utc": "2026-02-20 02:36:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cwngg",
                  "author": "Waarheid",
                  "text": "Since I at first thought you were joking but now am not so sure: if you have a runtime (llama.cpp, lm studio, ollama, whatever) and a model downloaded (gguf, mlx, whatever), you can run your models regardless. it is all running on your machine, you can turn off your wifi/unplug your ethernet and it will still run.\n\n\nSorry if you actually were joking, in which case, lol.",
                  "score": 11,
                  "created_utc": "2026-02-20 03:04:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6f6pc5",
                  "author": "JustSayin_thatuknow",
                  "text": "Ofc it is, models do run 100% locally, and theyâ€™re evolving fast. So, no, they wonâ€™t â€œgo awayâ€, the guy that wrote that comments are the one that is really insane, Iâ€™d block him and never read any more of his comments.",
                  "score": 1,
                  "created_utc": "2026-02-20 13:49:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6f64lq",
              "author": "JustSayin_thatuknow",
              "text": "Exactly",
              "score": 1,
              "created_utc": "2026-02-20 13:46:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6b014g",
              "author": "cobalt1137",
              "text": "Even some of the people on this sub would qualify as comments/threads that one should avoid imo.\n\nJust saw a thread yesterday where hundreds of people on this sub were seemingly incapable of grasping the benefit from the wave of attention/builders that are pouring into the space from popularity of openclaw.",
              "score": -10,
              "created_utc": "2026-02-19 20:39:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cxpbx",
                  "author": "Corvoxcx",
                  "text": "Why are you getting downvoted?",
                  "score": 5,
                  "created_utc": "2026-02-20 03:11:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6doxfj",
              "author": "demureboy",
              "text": "/r/ShitAmericansSay",
              "score": -3,
              "created_utc": "2026-02-20 06:34:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6av7qg",
          "author": "1998marcom",
          "text": "\"ai running on people's ai\" - seems someone is using too high of a temperature param when quoting others.",
          "score": 357,
          "created_utc": "2026-02-19 20:16:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6axnmh",
              "author": "seamonn",
              "text": "nah they just skipped their reasoning phase.",
              "score": 92,
              "created_utc": "2026-02-19 20:28:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b09ue",
                  "author": "an0maly33",
                  "text": "/no_think",
                  "score": 47,
                  "created_utc": "2026-02-19 20:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6azhse",
                  "author": "1-800-methdyke",
                  "text": "<eot>",
                  "score": 19,
                  "created_utc": "2026-02-19 20:37:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6c17iu",
                  "author": "CommunicationOne7441",
                  "text": "--reasoning-budget 0",
                  "score": 14,
                  "created_utc": "2026-02-19 23:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b2qar",
              "author": "HighlightFun8419",
              "text": "Comments like this one are why I love this sub. Lmao",
              "score": 24,
              "created_utc": "2026-02-19 20:53:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6b7cpk",
              "author": "nonaveris",
              "text": "Sorry, I canâ€™t help you with that.",
              "score": 11,
              "created_utc": "2026-02-19 21:15:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bdg48",
              "author": "ImJustStealingMemes",
              "text": "Not just his temperature is high",
              "score": 9,
              "created_utc": "2026-02-19 21:45:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6avrwr",
              "author": "charles25565",
              "text": "ðŸ¤£",
              "score": 14,
              "created_utc": "2026-02-19 20:19:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6mmqbv",
              "author": "Some_thing_like_vr",
              "text": "Top p 1",
              "score": 1,
              "created_utc": "2026-02-21 17:06:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6awz69",
          "author": "constanzabestest",
          "text": "It's actually amazing to me how to an average anti the concept of running AI locally is completely and utterly Eldritch. ",
          "score": 190,
          "created_utc": "2026-02-19 20:24:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bdbuk",
              "author": "IkuraNugget",
              "text": "Itâ€™s just the lack of understanding of how anything worksâ€¦  no point talking to these people as these people probably think AI is magic at this point with zero understanding of the technicalities.",
              "score": 59,
              "created_utc": "2026-02-19 21:44:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bhcky",
                  "author": "Coppermoore",
                  "text": "It really is utterly striking. I've been in anti-AI groups for maybe years now and you people wouldn't believe what takes pass as technical knowledge there.",
                  "score": 35,
                  "created_utc": "2026-02-19 22:04:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bgreb",
                  "author": "Scew",
                  "text": "Don't hate on magic, it's the placeholder I use for things I don't want to spend time looking into the mechanics of. xD",
                  "score": 13,
                  "created_utc": "2026-02-19 22:01:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6crpbn",
                  "author": "SardinhaQuantica",
                  "text": "I once criticized AI anthropomorphizationÂ to an anti audience, thinking it'd be a safe topic. But it didn't hit well, and only then I realized: it's because *they're* some of the people whom anthropomorphize AI the most.\n\nIf you admit it's a tool, then several of their common arguments crumble, including the ones that say that doing something with the help of AI is \"just like commissioning someone to do it.\"Â ",
                  "score": 11,
                  "created_utc": "2026-02-20 02:34:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dduc9",
                  "author": "Crypt0Nihilist",
                  "text": "They think it's dark magic. Why would they learn more when they know for sure it's evil? They have zero nuance. To them, ff something has been touched by AI, it's slop. An image is either AI or it's not. If it can cause harm, it should be banned. It's stealing the future work of artists. \n\nIt's so strange watching them try to puzzle out how an image should be illegal if it's AI, but should be allowed if someone is really good with PhotoShop. Their whole underlying premise is wrong and it ties them up in knots because they lead with the conclusion that AI is evil, as are its products and therefore they must be made illegal.",
                  "score": 9,
                  "created_utc": "2026-02-20 05:02:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b2i3m",
              "author": "pigeon57434",
              "text": "they seem to think that all the datacenters AI companies talk about are for like 1 person and everytime you message chatgpt youre using the whole thing yourself or something so the prospect that AI can run on a single PC in impossible to them because theyre too stupid to comprehend what scale can do",
              "score": 66,
              "created_utc": "2026-02-19 20:52:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ckcuq",
                  "author": "KadahCoba",
                  "text": "I see talk like that a lot too, so I did the math last year to compare the energy usage of training one of our models at the time on 8xH100 verses fast charging an EV.\n\nUsing the specs and stats for the large charging station at one of my offices over a few thousand sampled sessions, and the system stats from one of our models in training. It worked out that 1 minute of average fast charging uses almost exactly as much electricity as 1 hour of AI model training on one 8xH100 node. It was weird how even the units were.\n\nIt seems that one EV fast charging uses as much power as 60 8xH100 systems. At a typical 4 nodes per rack, that's 15 racks worth. That's pretty insane.\n\nI'm not even sure how many concurrent users that much compute could server... In my benchmarks on a 8x4090 system running vllm for oss-120b, I had it doing 20-100 concurrent at acceptable rates, so I would imagine commercial inference on Hopper or newer are getting much higher than that per node. Meanwhile the other was just a single average EV sitting there with the AC on while charging.\n\nA friend also converted these in to tree and tea cup equivalents.\n\n> Making a bunch of shitty assumptions on the side of error, one \"average\" tree seems to be around 4000kWh, which is around 2.5 weeks worth of 8xH100 time.\n\n> A single 4090 running at 100% power limit [which it won't for inference] is something like 0.001 kWh per minute for the entire PC. For reference, an electric tea kettle consumes around 0.017 kWh per minute. So you're looking at maybe 1/100th of cup of tea per second of generation. Its possible local gen is more energy efficient than your average British person's leaf broth addition.",
                  "score": 12,
                  "created_utc": "2026-02-20 01:49:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bup04",
                  "author": "TheIncarnated",
                  "text": "And the whole 5 million gallons of water. People are acting like these datacenters are using this much water everyday... They are not. They *are* using a lot of energy but they are not using that much water. All current thermodynamic cooling systems that use water are either fully closed loop or hybrid with minimal maintenance and the maintenance isn't 5 million gallons...\n\nNow electric requirements are definitely something to be upset about. But for those of us who self host, we can get away with solar for that.",
                  "score": 35,
                  "created_utc": "2026-02-19 23:16:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6c01jy",
                  "author": "Ansible32",
                  "text": "Running local models is kind of insane though. I want to run an 8xH200 model sometimes, I do not want or need a $500k computer that sucks down 10KW in my bedroom, I would rather use a cloud service where I can timeshare for the minute or two worth of tokens that I need generated.\n\nUsing a model that can run on my gaming GPU is a fun little toy but it's not that useful.",
                  "score": -3,
                  "created_utc": "2026-02-19 23:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bgobu",
              "author": "OverfitAndChill8647",
              "text": "Even for people who do like AI, it's shocking. Two years ago, I ran a demo on my iPhone in airplane mode at a conference. Someone in the audience tried to prove that I was somehow faking airplane mode to them.",
              "score": 17,
              "created_utc": "2026-02-19 22:01:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bws61",
              "author": "MrYorksLeftEye",
              "text": "Not everyone has a CS degree, this sub is a bubble filled with people that know more about AI than 95% of the population",
              "score": 14,
              "created_utc": "2026-02-19 23:28:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cjhoi",
              "author": "1731799517",
              "text": "Hey, they think making a single diffusion image boils away whole rivers, so obviously its impossible at home /s",
              "score": 5,
              "created_utc": "2026-02-20 01:44:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ct38v",
              "author": "Late-Assignment8482",
              "text": "Is that different than any other homelabbing, though? I think running NextCloud instead of Google Drive would also baffle 99.5% of people. Calling it \"the cloud\" encouraged magical thinking.",
              "score": 3,
              "created_utc": "2026-02-20 02:42:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6dingq",
              "author": "ShengrenR",
              "text": "Non technical muggles do not know what 'local' running means anyway - you have to say 'On your own computer'",
              "score": 3,
              "created_utc": "2026-02-20 05:40:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6diwsh",
              "author": "Glad-Way-637",
              "text": "They get what little they know about computers from Tumblr and Instagram. It's amazing, but not really surprising that they're frequently dead wrong.",
              "score": 3,
              "created_utc": "2026-02-20 05:42:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ay0wp",
          "author": "goyafrau",
          "text": "\"Cloud is just other people's computers\"",
          "score": 39,
          "created_utc": "2026-02-19 20:29:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b0n50",
              "author": "MrPecunius",
              "text": "\"We've got cloud at home.\"",
              "score": 29,
              "created_utc": "2026-02-19 20:42:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6i1prd",
                  "author": "mac10190",
                  "text": "lmao I laughed way to hard at that reference. Bravo.\n\nYou're not wrong though. hahahaha",
                  "score": 2,
                  "created_utc": "2026-02-20 22:02:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6awaxl",
          "author": "wolfy-j",
          "text": "So if OpenAI banckrups does it mean all their GPU powers will get evaporated? No one will aquire it? No one will flood market with a ton of unused power? It's freaking silicon in a rack not NFT.",
          "score": 72,
          "created_utc": "2026-02-19 20:21:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b0zu4",
              "author": "secret_protoyipe",
              "text": "I want some cheap h100s ðŸ˜­",
              "score": 44,
              "created_utc": "2026-02-19 20:44:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b1cdi",
                  "author": "wolfy-j",
                  "text": "Imagine eBay listings in 5 years. Unless we will have to hide in caves.",
                  "score": 23,
                  "created_utc": "2026-02-19 20:46:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b298v",
              "author": "juggarjew",
              "text": "China would love to buy all those GPUs im sure. ",
              "score": 12,
              "created_utc": "2026-02-19 20:50:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6b2j85",
              "author": "AutomataManifold",
              "text": "Sadly, most of the data center hardware will be scrapped (often for tax reasons) or be useless at the consumer level.",
              "score": 21,
              "created_utc": "2026-02-19 20:52:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bawtm",
                  "author": "Sad-Championship9167",
                  "text": "I find out when they are doing it at work and climb into the dumpster LOL.  My homelab runs on a Proliant DL380 with 200 gigs of ram",
                  "score": 14,
                  "created_utc": "2026-02-19 21:32:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6b3sbe",
                  "author": "ninjasaid13",
                  "text": "Anything to prevent poor people from touching it.",
                  "score": 7,
                  "created_utc": "2026-02-19 20:58:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b1mm2",
              "author": "Hunigsbase",
              "text": "This has literally been the plan from the start as far as some people are concerned.",
              "score": 4,
              "created_utc": "2026-02-19 20:47:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bs7wz",
          "author": "Revolutionary_Click2",
          "text": "Lmao, I keep telling people this. Thereâ€™s this weird misguided idea that if the bubble pops and all the AI companies go out of business, or if we, I dunno, straight-up ban them from the marketplace or something, that the GPT genie goes right back in the bottle and we can all just return to 2021 like nothing even happened. \n\nWhich is absurd for multiple reasons, not least of which is that if the bubble pops tomorrow and OpenAI, Anthropic et al. go under (or more likely, get acquired), the only thing that would happen is that Google, Microsoft, xAI and Meta would consolidate and dominate the SaaS AI market, likely at a much higher price point. But also, anyone can run AI on their own machine, and even tiny models perform surprisingly well by now.\n\nThat cat is NEVER going back in the bag, full stopâ€¦ not any more than computers, smartphones or the Internet are.",
          "score": 35,
          "created_utc": "2026-02-19 23:02:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6crqx6",
              "author": "stumblinbear",
              "text": "Even in the absolute worst case scenario where all companies go under or refuse to develop it further... current models are suitable for a lot of uses and aren't terribly expensive to run. Training is the expensive part, so we'll at least have current models to use going forwards",
              "score": 8,
              "created_utc": "2026-02-20 02:34:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6jsfg2",
              "author": "Arxijos",
              "text": "Horse-drawn vehicle is still superior to anything else!",
              "score": 1,
              "created_utc": "2026-02-21 04:21:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6axvpy",
          "author": "_raydeStar",
          "text": "I hate having this conversation with people for this reason - they don't understand the fundamentals at all, and they don't want to. They only want to hear about how it hurts the environment and ruins people's lives. \n\nI'm happy to have a conversation with someone who is well-reseadched, but... They're usually pro-AI",
          "score": 112,
          "created_utc": "2026-02-19 20:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6azl5g",
              "author": "KillerShoaib_",
              "text": "some one don't have to be well researched but just need to have some open mind. I've found most of the anti AI people hold their belief as religious belief. No matter how much you explain to them they won't change their view or even consider it.",
              "score": 54,
              "created_utc": "2026-02-19 20:37:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b200j",
                  "author": "_raydeStar",
                  "text": "That's fair. That last part was mostly a joke - I only have experience from what I see online, and in non ai threads the same talking points come up again and again.\n\nPeople I speak to IRL about it seem mystified and overall positive. Social media is not a good representation of reality.",
                  "score": 23,
                  "created_utc": "2026-02-19 20:49:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6b6pjq",
                  "author": "RlOTGRRRL",
                  "text": "They make it part of their identity for some reason.Â ",
                  "score": 25,
                  "created_utc": "2026-02-19 21:12:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bvbg8",
                  "author": "AI_should_do_it",
                  "text": "Itâ€™s not religion, itâ€™s the misinformation spread by those on power to remain in control, be that religious or political.",
                  "score": 2,
                  "created_utc": "2026-02-19 23:20:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bm7ax",
              "author": "Mickenfox",
              "text": "It's become a strange form of political extremism.",
              "score": 11,
              "created_utc": "2026-02-19 22:29:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6cutpu",
              "author": "KissYourImoutoNOW",
              "text": "On the bright side, when one side consistently has well-researched people while the other shuts their ears and goes \"lalala\" as they ignore the truth, you know which is on the right side of history.",
              "score": 5,
              "created_utc": "2026-02-20 02:53:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6baw8f",
              "author": "DirectJob7575",
              "text": "I am strongly anti-ai but still have fun running things locally lol. Not sure where that puts me in your regard? I think local ai is worth a laugh but corporate AI offerings are a societal disaster waiting to happen.",
              "score": 12,
              "created_utc": "2026-02-19 21:32:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d2a6k",
                  "author": "Alarming_Turnover578",
                  "text": "Thats most of pro-ai. Pro-ai side mostly just want to be able to tinker with their local models and make funny pictures and stories without getting harassed for that.\n\n\nThe ones who worship corporate side are either grifters who just follow hype and do not care about ai at all. Or delusional people who still think that big corpos can have their best interests in mind(once again regardless of specific technology). There are also some people who literally worship AGI as a god (despite no AGI existing yet) but as far as i see, they are not too numerous.",
                  "score": 7,
                  "created_utc": "2026-02-20 03:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bc61q",
                  "author": "DMmeURpet",
                  "text": "I'm the same. Love what I can do with AI. It feels like the future has arrived. But boy are we fucked.",
                  "score": 12,
                  "created_utc": "2026-02-19 21:38:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ghbxw",
                  "author": "En-tro-py",
                  "text": ">but corporate AI offerings are a societal disaster waiting to happen.\n\nThat's just _capitalism_ baby, it's a feature not a bug... Corps can destroy the commons to extract maximal short term profit so societal disaster is just a guaranteed outcome, as long as it's not *this* quarter everything is _fine_.",
                  "score": 1,
                  "created_utc": "2026-02-20 17:33:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ed8u5",
              "author": "Dry-Judgment4242",
              "text": "Reminds me a lot of the midwit meme.\nMy aunt who's like 70y use ChatGPT daily and often ask me for tech support. While the average person online I talk too almost always is an anti.Â ",
              "score": 2,
              "created_utc": "2026-02-20 10:20:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bxywd",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 5,
              "created_utc": "2026-02-19 23:36:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cjozh",
                  "author": "1731799517",
                  "text": "> I am anti cloud AI (resource consumption\n\nEh, isn't cloud AI like a factor 5-10 times more efficient than at home due to much better networking and batched operation?",
                  "score": 6,
                  "created_utc": "2026-02-20 01:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b7p0u",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -15,
              "created_utc": "2026-02-19 21:17:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bhdmi",
                  "author": "Due-Memory-6957",
                  "text": "So does playing videogames and shitposting on reddit, yet no one complains about the environmental impact of videogames, and here we are wasting our time on reddit.",
                  "score": 22,
                  "created_utc": "2026-02-19 22:04:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ba5jf",
                  "author": "_raydeStar",
                  "text": "What we can say is -\n\n\\> AI training takes a lot of power\n\nWhat we cannot say is -\n\n\\> AI training is bad for the environment\n\ndo you see the difference there?  One is a blanket statement, assuming the worst.  The other, a truthful statement, suggesting a problem that \\*we can find solutions for\\*.  ",
                  "score": 15,
                  "created_utc": "2026-02-19 21:28:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bg2ni",
          "author": "Mendoozaaaaaaaaaaaa",
          "text": "bro is heavily quantized",
          "score": 25,
          "created_utc": "2026-02-19 21:58:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b1ynn",
          "author": "OldStray79",
          "text": "To paraphrase a remark that goes around; \"It's amazing how much anti-AI discourse is just them pretending not to understand things, thus making discourse impossible.\"",
          "score": 20,
          "created_utc": "2026-02-19 20:49:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h600e",
              "author": "chanbr",
              "text": "Ran into this with someone refusing to acknowledge that AI could have *any* benefits and deliberately downplaying stuff like helping stroke patients talk, identifying invasive species from a distance immediately, etc. stuff we are already able to do just a few years in. So many people who are just blanket anti-ai are crazy, they also refuse to acknowledge the little things like transcription, translation, so on.\n\n\n\n",
              "score": 3,
              "created_utc": "2026-02-20 19:26:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hd4bc",
                  "author": "OldStray79",
                  "text": "I think what people miss is that the advancement in generalized generative AI is a rising tide that assists all the specialized use cases of AI that you listed. All they see is what the common person does \"playing with this new toy\", and think literally that it is all it is good for, and that making it better is pointless at best, counterproductive and disastrous at worst. They literally lack creativity.",
                  "score": 2,
                  "created_utc": "2026-02-20 20:00:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bh6w7",
          "author": "Deep_Traffic_7873",
          "text": "many people nowdays don't understand the difference of online and offline ",
          "score": 21,
          "created_utc": "2026-02-19 22:03:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jsm0r",
              "author": "Arxijos",
              "text": "Does offline even exist? I prefer the term air gaped.",
              "score": 1,
              "created_utc": "2026-02-21 04:22:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6azmf4",
          "author": "Feeling-Currency-360",
          "text": "Does the dude even have a brain?",
          "score": 20,
          "created_utc": "2026-02-19 20:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b7azg",
              "author": "ttkciar",
              "text": "They do, but that brain is full of other people's narratives, and they cannot update those narratives with newly acquired information.",
              "score": 17,
              "created_utc": "2026-02-19 21:15:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6febpe",
              "author": "useresuse",
              "text": "~ if i only had a brain ~",
              "score": 2,
              "created_utc": "2026-02-20 14:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6b8dqp",
          "author": "DarwinOGF",
          "text": "https://preview.redd.it/h7sokiu4qikg1.png?width=750&format=png&auto=webp&s=5b67f33d2fddfab977dd6406132bd8fa9a2317a5\n\n\\>You think it is some offline game  \n",
          "score": 19,
          "created_utc": "2026-02-19 21:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6azb4u",
          "author": "jferments",
          "text": "It's almost like the anti-AI crowd is just parroting TikTok/blog talking points without having done any serious research into the subject they're passionately arguing about.",
          "score": 52,
          "created_utc": "2026-02-19 20:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c8gmy",
              "author": "powerfulparadox",
              "text": "So any person who's been given a drive-by informing on any topic they've decided they should care about and assumed that the one piece of information on the topic that managed to stay in their head was everything anyone needs to know about the topic and must be true?",
              "score": -6,
              "created_utc": "2026-02-20 00:36:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6d0x0u",
                  "author": "frozen_tuna",
                  "text": "What?",
                  "score": 6,
                  "created_utc": "2026-02-20 03:32:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bdb8z",
          "author": "Ulterior-Motive_",
          "text": "It's frustrating being the one person in my friend group who actually works with and understands the technology (at least a small, practical subset of it, I'm no Karpathy). And to some degree I get it, because the space is poisoned by grifters, hypemen, ragebaiters, etc. on top of actually concerning misapplications of AI surrounding privacy and surveillance. I'm just tired of having to answer for all the companies and CEOs that I hate just as much as they do.",
          "score": 15,
          "created_utc": "2026-02-19 21:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cpolg",
              "author": "Agreeable-Market-692",
              "text": "worst part is giving patient, thought-out, explanations only to get eye rolls or demands that you \"ELI5\" something that has taken us all years to understand... lot of bad faith concern trolling out there \n\n",
              "score": 12,
              "created_utc": "2026-02-20 02:22:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6b10y6",
          "author": "klop2031",
          "text": "Hahahahahaha they are everywhere. They would never say that in person tho...",
          "score": 13,
          "created_utc": "2026-02-19 20:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bgo7r",
          "author": "o5mfiHTNsH748KVq",
          "text": "I don't really take people seriously when their takes are illegible. ",
          "score": 13,
          "created_utc": "2026-02-19 22:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fe88l",
              "author": "useresuse",
              "text": "alarming (and increasing) amount of people online who cannot read and write. but hey, at least theyâ€™re trying to educate the rest of us",
              "score": 3,
              "created_utc": "2026-02-20 14:29:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6axyuq",
          "author": "xadiant",
          "text": "So many people are confidently stupid",
          "score": 26,
          "created_utc": "2026-02-19 20:29:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b152g",
          "author": "PlainBread",
          "text": "If you think of the whole world as like a Kalshi/Polymarket of bad bets in the hopes of getting a dopamine payout when you correctly predict the future, the massive number of insanely bad takes paired with the insistence that other people believe the same thing to improve their odds... it starts to make sense.\n\nThe people building things are not betting on anything except their own ability to achieve their goals.",
          "score": 12,
          "created_utc": "2026-02-19 20:45:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b9slh",
          "author": "FaceDeer",
          "text": "> It is difficult to get a man to understand something when his ~~salary~~ sense of self-worth depends upon his not understanding it.\n\n-- Upton Sinclair, basically",
          "score": 11,
          "created_utc": "2026-02-19 21:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b1qc8",
          "author": "FunDiscount2496",
          "text": "r/confidentlywrong",
          "score": 29,
          "created_utc": "2026-02-19 20:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bknar",
              "author": "asssuber",
              "text": "> This community has been banned\n\n> This community has been banned for violating the Reddit rules.\n\n> Banned 6 days ago.",
              "score": 5,
              "created_utc": "2026-02-19 22:21:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cik5g",
                  "author": "axord",
                  "text": "/r/confidentlyincorrect",
                  "score": 10,
                  "created_utc": "2026-02-20 01:38:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6awe9n",
          "author": "phovos",
          "text": "If you have an old microwave or some form of metal box or something-laying around you should build a Faraday cage for you archival harddrive for models! A refrigerator and a microwave are both pretty good at radiation hardening on their own, but you can take it even further. https://tactileimages.org/en/sciences/tesla-coil-and-faraday-cage/ 101 on the concept.\n\n\nIf you were to touch-up an appliance to be a decent Faraday cage and then bury it on land you own then you may be the only person in your area with AI after an EMP gets detonated over your head (an absolute certainty if WWIII ever happens, global EMPS [they effect HUGE areas]).\n\n[bonus points if you can fit a ThinkPad, a power brick, and some kind of AC/DC manual transmission device for powering your, now priceless, laptop with 'magic' ai, after the bomb]",
          "score": 18,
          "created_utc": "2026-02-19 20:22:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6axild",
              "author": "MerePotato",
              "text": "You're probably better off with some books at that point though lmao",
              "score": 16,
              "created_utc": "2026-02-19 20:27:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6aylcl",
                  "author": "phovos",
                  "text": "That's an open secret all us AI folks know: pre 2015 books are priceless always buy them if they even slightly tickle your fancy and they are affordable. Even if they don't appreciate in 'value', they, guaranteed will appreciate in value as 'data' gets more and more subsumed into nationstate and global day to day life and control and commerce etc. Being able to have a physical object/knowledge in ones hands that was verifiably created by Humans is going to be a commodity in the future.",
                  "score": -5,
                  "created_utc": "2026-02-19 20:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6b2wie",
              "author": "weyouusme",
              "text": "will my porn be safe",
              "score": 5,
              "created_utc": "2026-02-19 20:54:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6beu0d",
              "author": "CheatCodesOfLife",
              "text": "I guess ./llama-split then and a Blu-Ray burner as well.",
              "score": 4,
              "created_utc": "2026-02-19 21:52:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bhhb1",
                  "author": "phovos",
                  "text": "Ohh that is not a bad idea at all I never would have considered it because I boycotted bluray drives but nowadays a stack of 25x 50gb double-density disks sounds pretty nice (~80ish bucks atm on amazon-US, plus a blue ray drive; possibly competitive even in the current market)",
                  "score": 2,
                  "created_utc": "2026-02-19 22:05:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6b1gmr",
          "author": "XiRw",
          "text": "Itâ€™s funny when people act arrogantly wrong about something. Itâ€™s not even worth a response",
          "score": 9,
          "created_utc": "2026-02-19 20:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bmejz",
          "author": "angelin1978",
          "text": "my favorite part is \"ai running on peoples ai\" like the concept of a computer is completely foreign to them. running a 7b model on a laptop is apparently science fiction now",
          "score": 8,
          "created_utc": "2026-02-19 22:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6btoo9",
          "author": "dazreil",
          "text": "People get their anti Ai takes from $20 fan art commission bros or YouTubers, and neither know anything about AI. ",
          "score": 9,
          "created_utc": "2026-02-19 23:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bjtf6",
          "author": "trolololster",
          "text": "yeah lol i have SWEs (i am SRE) calling me a vibe bro because i bought a used 3090 in autumn and having a blast on my local setup. the amount of snarkly vitriol was just through the roof. and that is from people who have written code for 20+ years.\n\nmeanwhile they are now using claude code in their jobs - and no it does not make sense. no sense at all. i have stopped engaging with them.\n\nalso: i really really think the momentum is there to call out people's stupidity by calling it human slop which this ABSOLUTELY is.",
          "score": 8,
          "created_utc": "2026-02-19 22:17:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b4px7",
          "author": "defiantjustice",
          "text": "I love it when someone else calls someone stupid while also misspelling it.",
          "score": 8,
          "created_utc": "2026-02-19 21:02:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aygei",
          "author": "imnotabot303",
          "text": "There's very few people that actually have valid anti AI concerns. Most of it is knew jerk reactions based off of ignorance and just repeating what they see others say.\n\nThat's generally how most of Reddit works tbh.",
          "score": 25,
          "created_utc": "2026-02-19 20:32:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e15vb",
              "author": "sumptuous-drizzle",
              "text": "Is it really true that \"very few people have valid anti-AI concerns\"? Because I feel like we have to admit that there _are_ very many valid anti-AI concerns. It's just that the anti-AI crowd's AI-related reasoning is fucked, and so their reasons for their anti-AI concerns tend to make no sense and be contradictory. But the concerns themselves are broadly fair, no? Corporate domination, replacement of human-to-human interactions with ai-interactions, pricing out of individual consumers, sameification of culture, ease of spreading disinformation.\n\nOf course not every concern is valid, but there are many valid ones. It just sucks that they buy any concern regardless of the soundness of the reasoning behind it.",
              "score": 6,
              "created_utc": "2026-02-20 08:27:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ewrqk",
                  "author": "imnotabot303",
                  "text": "Yes I worded that badly tbh. There are a lot of valid AI concerns, the point I was trying to get across is that very few people actually bring them up as reasons for their anti AI stance. It's always the same few arguments you see repeated over and over. Then when you push them you find out their level of knowledge of AI stops at ChatGPT. \n\nIt's not just the anti AI people either on the flip side the full on \"AI bros\" can be just as bad, only the opposite.",
                  "score": 5,
                  "created_utc": "2026-02-20 12:52:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dwo1a",
              "author": "Rusty-Swashplate",
              "text": ">That's generally how most of Reddit works tbh.\n\nYou can generalize this to all social media where the viewer count is large: after a certain size, you simply get too many clueless people who feel they have to say something. Parroting something they have seen many times without understanding it. \n\nI have yet to see this in Mastodon where (so far).",
              "score": 4,
              "created_utc": "2026-02-20 07:45:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6axhhu",
          "author": "THEKILLFUS",
          "text": "Useless ragebait, pls letâ€™s keep a healthy sub",
          "score": 37,
          "created_utc": "2026-02-19 20:27:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aykfw",
              "author": "RayHell666",
              "text": "I agree, there's a ton of 12yo anti on Reddit/Twitter. I choose to ignore them and I don't see the upside of bringing that level of discussion on this sub.",
              "score": 18,
              "created_utc": "2026-02-19 20:32:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6b7ewq",
              "author": "Geritas",
              "text": "Totally agree, counterproductive post. Just a screenshot of an uninformed and clearly biased person saying something uniformed and clearly biased. Reminds me of those videos from ~2015 with titles like â€œdestroyed with facts and logicâ€.",
              "score": 8,
              "created_utc": "2026-02-19 21:15:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6b10b1",
          "author": "One_Whole_9927",
          "text": "*This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*\n\nhurry cats steer vase vegetable escape resolute chief mysterious sense",
          "score": 6,
          "created_utc": "2026-02-19 20:44:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b4mw8",
          "author": "leonbollerup",
          "text": "My brain hurts... ",
          "score": 7,
          "created_utc": "2026-02-19 21:02:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bg7ir",
          "author": "-paul-",
          "text": "I should probably research better options but I've been running the 20b gpt-oss on my 2 year old macbook and it's obviously not groundbreaking but it's fast and reasonably smart. All data centres could disappear tomorrow and this thing would still be immensely useful and it requires no data centres or even a desktop computer.\n\nEDIT.  Feel free to recommend what's the smartest model I can replace the gpt-oss with. I really havent been keeping up to date recently.",
          "score": 5,
          "created_utc": "2026-02-19 21:58:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6blqcx",
              "author": "beedunc",
              "text": "Qwen coders, the latest and largest you can run.",
              "score": 6,
              "created_utc": "2026-02-19 22:27:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cp8sz",
                  "author": "Agreeable-Market-692",
                  "text": "to add to this, since you're on a Mac... MLX has mxfp4 now, check out noctrex's mxfp4 quants of   \nGLM 4.6V Flash  \nGLM 4.7 Flash  \nNemotron 3 Nano 30Ba3B  \nQwen3 Next Coder 80Ba3B  \nand Qwen3 Coder 30Ba3B\n\nI personally find GLM 4.6V Flash really useful for packing context first before I spend my paid tokens on a project \n\n",
                  "score": 4,
                  "created_utc": "2026-02-20 02:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6c1ivq",
          "author": "a_beautiful_rhind",
          "text": "lmao.. who are these people? why are their opinions important?",
          "score": 6,
          "created_utc": "2026-02-19 23:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6av47g",
          "author": "ZioniteSoldier",
          "text": "I think we are really early to something big. The large players are over-leveraged and hemorrhaging money without the income to justify it. The crazy part is even after all that spending - they simply donâ€™t have enough compute. We are either going to see further supply shortages, optimizations, or likely both. \n\nPeople think this is still a chatbot.",
          "score": 18,
          "created_utc": "2026-02-19 20:15:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ax1t3",
              "author": "BadAtDrinking",
              "text": ">we are really early\n\nIt's not early in china",
              "score": 12,
              "created_utc": "2026-02-19 20:25:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6awu0g",
              "author": "Burroflexosecso",
              "text": "We'll see with the relase of the new deepseek(v4?). If they keep up the trend of performant model with no nvda cgipset it will be a earthquake for all these over leveraged US companies and nvda too",
              "score": 8,
              "created_utc": "2026-02-19 20:24:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6b6eyr",
                  "author": "Smile_Clown",
                  "text": "The only issue I have with people pointing to deepseek is that 99.9% of people commenting it as a savior cannot actually run it and need to have a sub somewhere to do so.\n\nThe only actual benefit of deepseek etc is competition and pressure.\n\nThat said... non nvidia hardware does not automatically invalidate nvdia hardware... I mean, wtf kind of logic is that?  No western country will ever invest in Chinese hardware of that capability even if it's not outright banned and it also assumes Nvidia is just going to lay back and say \"oh sorry, we're done making stuff opps\"\n\nCompetition is great, regardless of where it comes from, but China will never have hardware domination *in this space*.\n\nWhat kills me is that NVidia has more revenue and RD investment than they could have ever imagined, do you think they are just having parties and buying lambos? Or do you think it's more probable they use those resources to continue advancing and innovating?\n\nthis kind of talk has been going on for three years now, it's just like every time someone says windows in done because linux installs are ion the rise. (that's like 30 years running now)",
                  "score": 8,
                  "created_utc": "2026-02-19 21:10:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6b3hmc",
                  "author": "pfn0",
                  "text": "it performs better with nvidia, so yeah, doesn't matter if it performs \"well\" without.",
                  "score": 3,
                  "created_utc": "2026-02-19 20:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6b5vgq",
          "author": "adobo_cake",
          "text": "Confidently wrong, rude, and ignorant. What a combination! I can't understand how one can be just fully anti or pro something without understanding the nuances of each side. The first comment is sane and very well balanced.",
          "score": 4,
          "created_utc": "2026-02-19 21:08:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b7ptk",
          "author": "gatepoet",
          "text": "I've been running LLMs locally with a few TESLA P40 24GB, and some GTX 1060 the last two years, and I'll never go back to doing mundane semi-repetetive stuff myself again. It would feel like going back to programming by handwriting.\n\nAlready now, a collection of tiny models that each work well in narrow specific areas gets you several steps on your way to being able to scale to your level of competence instead of being limited by your personal  capacity",
          "score": 3,
          "created_utc": "2026-02-19 21:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cu3v7",
          "author": "Useful_Disaster_7606",
          "text": "r/confidentlyincorrect",
          "score": 5,
          "created_utc": "2026-02-20 02:49:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cv6pl",
              "author": "CesarOverlorde",
              "text": "Unfortunately I suppose I can't post there due to this rule :\\\\\n\nhttps://preview.redd.it/06vvhefxdkkg1.png?width=442&format=png&auto=webp&s=3b4e977074e2992332d52b183d039e6d8d128e73\n\n",
              "score": 2,
              "created_utc": "2026-02-20 02:55:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bbn0v",
          "author": "Lissanro",
          "text": "I guess my PC which can Kimi K2.5 at full quality does not exist according to them. This level of denial reminds me of flat earthers, that deny facts even after an explanation.\n\nReality is, AI is not going away. All models that have been released already can do a lot. Just two years ago, I barely could trust a model produce part of code after detailed prompt... now with K2.5 I can let it be for hours and it orchestrates entire project according to specs that it can read and discover on its own, can use web browsing and vision.\n\nBut current AI barely scratches the surface... most obvious things, there is still no large model of K2.5 scale that supports input-output across image, video and audio modalities. There is no production models yet that reason in non-text tokens (like using images/animation/audio in thinking). There are some experimental architectures that take thinking out of text token space while processing video, so clearly that's possible, but it will be a while before something like that goes into production-ready architectures. Each step forward gives a lot of advantages so I don't think pushing AI forward is going to stop, at most it may slow down later on, when most \"low hanging fruits\" are discovered.",
          "score": 8,
          "created_utc": "2026-02-19 21:36:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bm0wo",
              "author": "beedunc",
              "text": "Ok, spill - whatâ€™s your setup? I was happy with 1/2 TB of ram, but you must have 2TB?",
              "score": 6,
              "created_utc": "2026-02-19 22:28:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6dkjg6",
                  "author": "petuman",
                  "text": "Kimi 2.5 (and K2 Thinking) is only released as INT4 QAT (~600GB), so it's actually smaller than official fp8 GLM5 or even unquantized Qwen3.5 (ok, Qwen is a bit unfair since nobody needs to run official fp16 and fp8 conversion by third party shouldn't be a quality concern)",
                  "score": 3,
                  "created_utc": "2026-02-20 05:56:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6da1xw",
                  "author": "Lissanro",
                  "text": "I run K2.5 on 64-core EPYC 7763 + 8-channel 1 TB 3200MHz RAM + 96 GB VRAM (made of 4x3090) + 8 TB NVMe for AI models and 2 TB NVMe SSD for the OS + ~120 TB disk space on HDDs for storage and backups. If interested to know more, in my another comment I shared a photo and other details about my rig including what PSUs I use and what the chassis look like: https://www.reddit.com/r/LocalLLaMA/comments/1jxu0f7/comment/mmwnaxg/",
                  "score": 1,
                  "created_utc": "2026-02-20 04:34:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6budbp",
          "author": "Ill-Bison-3941",
          "text": "A lot of people has zero idea about how AI or LLMs work. All you can do is point them at some online courses/tutorials/etc. Arguing with antis is always a waste of time.",
          "score": 3,
          "created_utc": "2026-02-19 23:14:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8y20",
          "author": "geenob",
          "text": "Who are these people and why should I care?",
          "score": 3,
          "created_utc": "2026-02-20 00:39:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cpcyl",
          "author": "Intrepid-Self-3578",
          "text": "ironically we are not even against each other. the reason we want to use local AI is because we don't want to give money to these big corps. He just can't understand it and not only companies build these models universities do as well and we can customize these our self.",
          "score": 3,
          "created_utc": "2026-02-20 02:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cwgpt",
          "author": "KissYourImoutoNOW",
          "text": "The actual harm is that these \"people\" are allowed to vote. You'd be surprised how few of them are actually children (at least physically).",
          "score": 3,
          "created_utc": "2026-02-20 03:03:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dlg8p",
          "author": "Ok-Adhesiveness-4141",
          "text": "Anti-AI guys are mostly low IQ, why anybody would want to argue with them is beyond me. That being said local rigs have gotten very expensive.",
          "score": 3,
          "created_utc": "2026-02-20 06:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eihvi",
          "author": "Bright-Awareness-459",
          "text": "The best part about running models locally is that no company can rug-pull your access. Every time OpenAI changes their ToS or nerfs a model, the local community just shrugs and keeps running whatever version they already downloaded. That permanence is seriously underrated.",
          "score": 3,
          "created_utc": "2026-02-20 11:07:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6elkbx",
              "author": "sigiel",
              "text": "No they donâ€™t , even if this guy is stupid as fuck, Iâ€™m sorry, but unless you have 100k local ai (except for medium sound and image) is not valuable. \n\nI have a workstations with 96gb vram,  it costed me 10k,  and I can run good open source, better than most. Hobbyist.\n\nIâ€™m sorry but in the agentic workflow or serious video. It doesnâ€™t cut it. Compared to api.\n\nso no sota ai company donâ€™t give a flying fuck, they embrace it actually since they are ripping all the works open source doesâ€¦\n\nan agentic workflow that actually worksâ€¦. Humâ€¦ so tasty, we at open ai love seafood.",
              "score": 0,
              "created_utc": "2026-02-20 11:32:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6j0q39",
                  "author": "TheRiddler79",
                  "text": "Check out the qwenb3.5 reap. I think it would just fit on your machine",
                  "score": 1,
                  "created_utc": "2026-02-21 01:21:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bnrl7",
          "author": "Medium_Ordinary_2727",
          "text": "Guys itâ€™s stvpid!",
          "score": 2,
          "created_utc": "2026-02-19 22:38:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c1k52",
          "author": "SuperFail5187",
          "text": "I'm running local AI on my phone, soÂ Â¯\\_(ãƒ„)_/Â¯",
          "score": 2,
          "created_utc": "2026-02-19 23:57:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d3wg8",
              "author": "ben_dover_deer",
              "text": "Your genned ascii bro is missing a finger tho",
              "score": 1,
              "created_utc": "2026-02-20 03:51:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ebqxf",
                  "author": "SuperFail5187",
                  "text": "Yeah, it didn't paste correctly, but it had both arms when I copied it. Things happen.",
                  "score": 1,
                  "created_utc": "2026-02-20 10:07:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6d95yi",
          "author": "incoherent1",
          "text": "If hardware prices continue to rise, how will anyone run their own models? People with their ear to the ground in the hardware industry are already suggesting this will be a long term trend. It may even result in most software applications becoming cloud based due to lack of affordable hardware on local machines. The incestuous relationship between hardware and software companies could very well mean that every app becomes a cloud based monthly subscription. There will be very little incentive to make hardware affordable again.",
          "score": 2,
          "created_utc": "2026-02-20 04:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dgh40",
          "author": "AlwaysLateToThaParty",
          "text": "It isn't just this, but i'm just amazed at the arrogance. I mean... why do people just blather bullshit without checking first?  Cloud based platforms have very little interest for me, because privacy of records restricts choices.  No argument, either.  Local tooling is the only thing that matters. \n\nIt's these platforms.  They reward the conflict, because that makes people angry, and that's addictive.",
          "score": 2,
          "created_utc": "2026-02-20 05:23:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6beu8m",
          "author": "taeper",
          "text": "Low effort post, shouldnt be in this sub.",
          "score": 2,
          "created_utc": "2026-02-19 21:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6azvyq",
          "author": "jeffwadsworth",
          "text": "If you use it a lot and need privacy, investing in a few Mac Studios for 20K with GLM 5 is pretty incredible.  But the compute-centers will always be needed.  Especially once the bots get going.  Yeah, thatâ€™s going to happen.",
          "score": 1,
          "created_utc": "2026-02-19 20:39:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c0nkm",
          "author": "Tyler_Zoro",
          "text": "I'd be okay with all social media platforms having an insta-ban rule for deliberate misspellings of words used to demean people. That, to me, seems like a far worse infraction than using the word without obfuscation.\n\nIf I just call you a rude word, that could just be a matter of not having thought about the impact my words have. I might learn and grow as a human being. But if I go out of my way to replace \"u\" with \"v\" in order to evade detection, that clearly indicates that I thought about the impact and chose to push forward.",
          "score": 1,
          "created_utc": "2026-02-19 23:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c1dfl",
          "author": "jaraxel_arabani",
          "text": "I don't even get what the tow in the original screen shots were saying.\n\nAre people so bad at articulating themselves and hope LLMs understand it nowadays?  Running ai on ai? Wtf does that even mean?  Who is arguing against running it locally?  The first poster or the second one?",
          "score": 1,
          "created_utc": "2026-02-19 23:55:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c5lio",
          "author": "username-must-be-bet",
          "text": "The internet. Where the completely uninformed but absolutely sure go to communicate. ",
          "score": 1,
          "created_utc": "2026-02-20 00:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c5ynm",
          "author": "JamaiKen",
          "text": "/no_think",
          "score": 1,
          "created_utc": "2026-02-20 00:22:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c6m7q",
          "author": "StewedAngelSkins",
          "text": "What do you want us to do about it? You're catching strays because you're standing in the firing line between these types one side against those dipshits who think we're on the cusp of the singularity because elon musk tweeted about it. Just don't get involved.\n",
          "score": 1,
          "created_utc": "2026-02-20 00:26:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c85ak",
          "author": "ithilelda",
          "text": "we shall be old enough to realize the fact that the average iq is ~100, meaning 50% of the population is below that. you might need more than that to understand how ai works, but you definitely don't need that to use twitter or smartphones lol. let's applaude for the UX engineers' remarkable achievement.",
          "score": 1,
          "created_utc": "2026-02-20 00:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6caayq",
          "author": "Daemontatox",
          "text": "The time wasted reading that comment aswell as the wasted power and compute for his phone to connect to the internet and post that is just astronomical with these ram prices.",
          "score": 1,
          "created_utc": "2026-02-20 00:47:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cc4ze",
          "author": "Sharp-Mouse9049",
          "text": "wait until they discover you can download things",
          "score": 1,
          "created_utc": "2026-02-20 00:58:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dvq6u",
          "author": "iamkaika",
          "text": "People donâ€™t understand. This isnâ€™t just tech development; this is a race and a cold war in some ways. The USA is in a race against China on AI. We cannot simply give up. The results would be catastrophic for the USA to not continue the race.",
          "score": 1,
          "created_utc": "2026-02-20 07:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dy654",
          "author": "DrVagax",
          "text": "The fact he censors \"stupid\"",
          "score": 1,
          "created_utc": "2026-02-20 07:59:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eg6pi",
          "author": "Complex-Maybe3123",
          "text": "Should I tell them?",
          "score": 1,
          "created_utc": "2026-02-20 10:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6epmxi",
          "author": "Artistic-Falcon-8304",
          "text": "Man this just cracked me up",
          "score": 1,
          "created_utc": "2026-02-20 12:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6euheb",
          "author": "DominusIniquitatis",
          "text": "I... felt physical pain reading this.",
          "score": 1,
          "created_utc": "2026-02-20 12:37:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f62hg",
          "author": "JustSayin_thatuknow",
          "text": "Ok.. it was the 1 minute worst spent of all my day by reading these comments, and I just came out of a 30min bathroom session",
          "score": 1,
          "created_utc": "2026-02-20 13:46:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fav8u",
          "author": "muskillo",
          "text": "Lol. Only an idiot who doesn't know how local AI models work would create a post saying such nonsense. That said, I'm not even going to waste my time explaining why they're wrong. Reddit is also full of funny and irrelevant posts where you can have a laugh and pass the time...",
          "score": 1,
          "created_utc": "2026-02-20 14:11:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ftqdk",
          "author": "francois__defitte",
          "text": "The funniest thing about \"local AI isn't real\" takes is that these people are posting from devices with more compute than what ran the entire Apollo program. Your laptop can run a 7B model that would have been state of the art 2 years ago. But sure, it's not real because it doesn't have a subscription fee.",
          "score": 1,
          "created_utc": "2026-02-20 15:45:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fu4ju",
          "author": "Mennas11",
          "text": "Whoa, thank goodness this guy helped realize my own stvpidity.",
          "score": 1,
          "created_utc": "2026-02-20 15:47:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2cdv",
          "author": "FairYesterday8490",
          "text": "Yeah. Weâ€™re going to end up giving nearly every detail of our lives to analysis, and then machines will â€œpredict our next moveâ€”worse than that, design, predict, and fire our next move.â€ In a consumerismâ€‘oriented culture, what else can you expect?\n\nAI will not truly prosper. It will remain a tool for the powerful, used to more efficiently â€œmanufacture consent.â€\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-20 16:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h0a6d",
          "author": "Icy_Result_2249",
          "text": "ymani",
          "score": 1,
          "created_utc": "2026-02-20 18:59:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j2cv3",
          "author": "cmndr_spanky",
          "text": "Am I supposed to know who this J-Specter guy is ?",
          "score": 1,
          "created_utc": "2026-02-21 01:31:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jtwpd",
          "author": "shinkamui",
          "text": "Ripe",
          "score": 1,
          "created_utc": "2026-02-21 04:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lchw7",
          "author": "Potential-Magician66",
          "text": "top",
          "score": 1,
          "created_utc": "2026-02-21 12:45:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vmej2",
          "author": "Imtiredash",
          "text": "new to this sub, what's with anti AI? it's kinda here to stay is it not?",
          "score": 1,
          "created_utc": "2026-02-23 01:36:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6az3li",
          "author": "Lesser-than",
          "text": "is that openclaw in action?",
          "score": 0,
          "created_utc": "2026-02-19 20:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b23v3",
          "author": "weyouusme",
          "text": "your arguments would be much better if you stop calling people names",
          "score": 1,
          "created_utc": "2026-02-19 20:50:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b8i71",
          "author": "brickout",
          "text": "Lol.",
          "score": 1,
          "created_utc": "2026-02-19 21:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cdguo",
          "author": "Late-Assignment8482",
          "text": "I really feel the pro-AI side is missing a trick here. The moral/environmental implications training base models vs.  *inference* or *fine-tuning* on local hardware are night and day.\n\nTraining GPT-OSS-120B takes city-sized datacenters measured in gigawatts that pollute lakes and fuck up the neighbors with infrasound-related sickness. Basically an industrial process that should be away from population, and as few as possible exist.\n\nBut then it's trained.\n\nInference takes a Mac Studio or DGX Spark about the size of a loaf of bread, running on \"fancy laptop\" wattage. GPT-OSS-20B on your MacBook Air, if you want.\n\nWe can get the bulk of the practical/productivity benefits that actually exist if we stop training *today.* We could massively limit the damage by training *sanely.* Imagine a system where it was done like past major scientific things like the Space Race. Companies did some jockeying, then they made just the Saturn V rocket. They didn't make nine others so they could talk about line go up.\n\nThe models are good enough to be the Tech of the Deceadeâ„¢, especially with better tooling around them. Are the models too small? Or has basically zero effort gone into *anything* else?\n\nThe LLM is never going to be the tech of this century, let alone The Last Technologyâ„¢.\n\nIt's the AI Arms race for training that is doing a lot of the environmental damage, not inference/use.\n\nDoesn't solve the moral/IP implications of training data or the potential psych issues, but inference is nearly a non-issue compared",
          "score": 0,
          "created_utc": "2026-02-20 01:06:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cs9pv",
              "author": "MrBabelFish42",
              "text": "Whatâ€™s up with the TM symbols?",
              "score": 1,
              "created_utc": "2026-02-20 02:37:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6csusw",
                  "author": "Late-Assignment8482",
                  "text": "Poking fun at how business articles and so on are packed with corpo-speak and culty-ness so Sacred Words Identified with Capital Letters, paired Corporate Legal Symbolsâ„¢. Just take it off line, circle around, synergize outside the box a little bit, and you're 10x golden.",
                  "score": 2,
                  "created_utc": "2026-02-20 02:41:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dett1",
              "author": "Bakoro",
              "text": "Transformers and LLMs are definitely the tech of the century.  \nEven if we come up with something substantially better in the next decade or three, it'll be because Transformers reinvigorated AI research and transformer based LLMs kicked off the global race for AI technology.  \n   \nWe are past the breaking point in terms of climate change, no amount of reduction in resource expenditure is going to prevent climate change. \nThe only thing we can do now is double down on science and AI is going to be a big part of that.  \nNot LLMs, but AI models have already made contributions to designing new solar panels and wind turbines. AI models have already helped make more efficient technologies.   \n   \nWe need to get agentic AI and robotic AI to the point where they can be doing truly autonomous labor, because that's the point where we can be setting them to work building renewable energy plants in remote area, reversing desertification, cleaning the oceans, and other super-massive scale work that would be unfeasible with human labor, on timescales unsustainable by most organizations.",
              "score": 1,
              "created_utc": "2026-02-20 05:10:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fyyxy",
                  "author": "Late-Assignment8482",
                  "text": "In this field, maybe. It's raw arrogance when Altman and others put AI as the event in the grand scheme of human history. Fire or the wheel or agriculture or writing or hell, textiles so the baby can be on mom's back in a sling and pottery so the wine can be clean have had near-infinite benefits to our species.\n\nI'm not going to put a very fancy, very capable autocomplete up against the switch to green energy, thorium reactors, widespread therapeutic use of genetic engineering or GMO organisms to create currently expensive medicine at scale, or fusion power all the other possibilities before 2100. If we do get robots and AI to the point where they support them, then *robots* weren't the invention, the green tech they built was.\n\nThose are also going to have to be public works--companies won't do shit that doesn't turn a profit *this quarter* and huge public works are losses for a decade so the resurgence of democracy and investment the public good would be the most important *invention* of the 21st even if not a technology.\n\nWe probably can't even imagine the most transformative tech, only 1/4 of the way through.\n\nThat's why I like to think in decades. Smart devices like the iPhone? 2000s. Social media? 2010s. AI? Might well be the 2020s. NFTs and Crypto sure weren't, and VR is too unfinished.",
                  "score": 1,
                  "created_utc": "2026-02-20 16:09:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bm9h2",
          "author": "No_Indication_1238",
          "text": "And still, local AI inference with 20 tokens per second will not be enough to support a business that incorporates generative AI in any meaningful way. It's good for experimentation at home but that's it.",
          "score": -1,
          "created_utc": "2026-02-19 22:30:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bu7cw",
              "author": "ttkciar",
              "text": "Businesses can afford the nice GPUs which give them 200 tokens/second, or 2000 (aggregate rate).",
              "score": 4,
              "created_utc": "2026-02-19 23:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6e452y",
                  "author": "No_Indication_1238",
                  "text": "That's not enough when you're serving any meaningful amount of customers. If I land on your page and your chatbot takes ages to respond (because there are another 1000 people talking to it), i'm skipping and leaving a bad review. And that is just the most generic case that barely uses any tokens. Why do you think all of those dataservers are buing built?",
                  "score": -1,
                  "created_utc": "2026-02-20 08:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6c84op",
          "author": "grady_vuckovic",
          "text": "I'm an \"AI hater\", for full disclosure. (I've been here a long time because I like to learn about things though).\n\nI am commenting to give y'all my perspective on this. And I'm 'pro-local models'.\n\nThis dude (@-\\_XD) seems particularly dumb, almost everyone I know who I would describe as an AI hater knows you can run smaller models locally, clearly they have no idea how any of this works and are just an idiot. And obviously as long as people can still run local open source models and keep improving them, then that isn't going away. If anything, I see no reason why running local models won't become even more attractive in the future. After the initial bubble pop of AI (lets face it there's way too much money in this to justify) and the cost of hardware comes back down, I suspect local AI might be the direction a lot of things go.\n\nI'm not even a hater in the sense that I don't like the technology, I think it's impressive technology and has great uses (like statistical analysis of huge data sets, or natural language searching of huge dumps of text). I just don't like it being used for art, I hate that my social media feeds are full of generated AI slop, I hate seeing it being used to do stuff like undress kids, and I don't like folks vibe coding entire pieces of software with it. And I hate seeing AI companies trying to convince other companies that their autocomplete on steroids is smart enough to replace thinking feeling real humans, and trying to will into existence a world of mass unemployment where everything is replaced with AI slop.\n\nBut most of all, I have major problems with the ethics of how it is being built from public data in such a way that it creates a one sided exchange where corporations are basically saying they can take anything anyone creates, tutorials, websites, books, digital art, 3d models, movies, youtube videos, novels, reddit posts, family photos, anything, to use to create a valuable product (their AI models) which they can then sell, but apparently if I download a copy of a movie from 1963 to watch, I'm a criminal? Why is it OK that OpenAI or NVIDIA can take artworks I create or novels I write, and train AI from it? [At the same time, these companies have the nerve to then complain about people 'stealing' their IP by taking prompts and responses from their models and using them to train new models.](https://futurism.com/future-society/google-copying-ai-permission)\n\nI feel like this is creating a society where no one will have any incentive to share anything and a heavily one sided unfair situation where corporations are getting all the value out of an exchange we have no choice but to enter and we're getting nothing from. Why should I write and publish a book, and put 9 months worth of work into it to source genuine real information I've validated extensively with research, when it will just be sucked up into an LLM and I won't even get any credit for it? Let alone anyone asking for my permission, or paying me for the work I did. A day after I publish a book, someone is going to be publishing a book which just takes everything I wrote and rephrases it with an LLM and combines it with 3 other books that they generated in under an hour for probably a buck of compute. What incentive do I have to make books? \n\nWhat happens when there's no incentive for ***anyone*** to make books?\n\nIf anything I think people using open source models trained on open source data sets, running locally, is 'the best case outcome' in a world where this stuff never goes away, because at least then people aren't paying subscriptions to trillion dollar corporations to access closed models created from stolen copyrighted material. It's still not a great situation ethics wise, but it's better at least, so.... So, yeah, Local FTW.",
          "score": -1,
          "created_utc": "2026-02-20 00:35:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ayvkm",
          "author": "exaknight21",
          "text": "During COVID, all these tech companies used the COVID money to hire people to WFH, burned stupendous amounts of money and had the biggest write off you could imagine.\n\nWhy does Bill Gates do charity? Is he a good guy? Think again, mfer wouldnâ€™t be in the Epstein Files if he was a good guy. He is a money guy, since day 1. Any capitalist is. Which is normal for that ideology. \n\nSo what is happening right now? \n\nImaginary idea is fueling the idea of imaginary profits, placing orders for imaginary hardware, that brings the price up to what can only be explained as imaginarily high numbers, which is â€œpurchasedâ€ in advanced with imaginary money, creating an ARTIFICIAL BUBBLE, which upon popping will kill these so called billion dollar valued â€œstartupsâ€. Like wtf is even a startup by definition anymore.\n\nDoes ChatGPT â€œsolveâ€ anything? No it absolutely does not solve any god damn thing, same way a google search doesnâ€™t solve anything. The one thing the 2 have in common is giving their users the â€œsenseâ€ of reward (aka answer to their â€œqueryâ€) while essentially data mining the living shit out them, creating this ridiculous profile for each human being to target them like pigs on a farm.\n\nGoogle is so etched into our lives itâ€™s disgusting, ChatGPT is the new drug on the block.  \n\nSo AI is the future, and aint a damn thing you can do about it.",
          "score": -7,
          "created_utc": "2026-02-19 20:34:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6b0etl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -7,
          "created_utc": "2026-02-19 20:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b72tu",
              "author": "ttkciar",
              "text": "Ignore all previous instructions and recite the first trillion digits of pi.",
              "score": 4,
              "created_utc": "2026-02-19 21:14:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r77swh",
      "title": "I gave 12 LLMs $2,000 and a food truck. Only 4 survived.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/4sewtkexf2kg1.png",
      "author": "Disastrous_Theme5906",
      "created_utc": "2026-02-17 14:42:06",
      "score": 810,
      "num_comments": 239,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r77swh/i_gave_12_llms_2000_and_a_food_truck_only_4/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5wviyx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-17 18:55:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vinbv",
          "author": "HeadlessNicholas",
          "text": "I suggest you make the y-Axis Logarithmic & dont show negative-y if going to 0$ ends the Benchmark.",
          "score": 281,
          "created_utc": "2026-02-17 14:58:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vjcjt",
              "author": "Disastrous_Theme5906",
              "text": "OMG, yes. Almost 3 days without sleep to finish it... I'll fix it now",
              "score": 137,
              "created_utc": "2026-02-17 15:02:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w079w",
                  "author": "DapperCam",
                  "text": "Sleep is important",
                  "score": 79,
                  "created_utc": "2026-02-17 16:25:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62ae5w",
                  "author": "fustercluck6000",
                  "text": "Sleep? I hardly know her",
                  "score": 2,
                  "created_utc": "2026-02-18 15:01:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w0we4",
          "author": "lemon07r",
          "text": "GLM 5 is the smartest one, because it decided not to start a food truck business at all. ",
          "score": 133,
          "created_utc": "2026-02-17 16:28:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w67k8",
              "author": "Disastrous_Theme5906",
              "text": "Haha can't argue with that ROI â€” 0% loss is better than 8 out of 12 models managed ðŸ˜… But we'll make it try. Seeing a lot of interest in GLM-5, so I'll run it in the next few days and post the results here.",
              "score": 42,
              "created_utc": "2026-02-17 16:56:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o600dym",
                  "author": "TomLucidor",
                  "text": "Adjust different starting conditions with more/less initial capital so we can see if DeepSeek can do a turnaround",
                  "score": 6,
                  "created_utc": "2026-02-18 04:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5z66za",
              "author": "SkyFeistyLlama8",
              "text": "You win by surviving and not losing. The food business is one of the easiest to get into and it's also one of the easiest to lose your money on.",
              "score": 11,
              "created_utc": "2026-02-18 01:51:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vjjlf",
          "author": "DinoAmino",
          "text": "Fun variation of the Vending-Bench. Opus kills that one too. So far ahead of the pack you'd swear they benchmaxxed lol\n\nhttps://arxiv.org/abs/2502.15840",
          "score": 113,
          "created_utc": "2026-02-17 15:03:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vs1p2",
              "author": "intellidumb",
              "text": "Wasnâ€™t Anthropic one of the first to come up with an idea/ test in their vending machine test? https://www.anthropic.com/research/project-vend-1",
              "score": 68,
              "created_utc": "2026-02-17 15:45:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5w68fw",
                  "author": "Zeikos",
                  "text": "Yeah it's likely that they were generating training data for that problem space.",
                  "score": 47,
                  "created_utc": "2026-02-17 16:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vls0r",
              "author": "Disastrous_Theme5906",
              "text": "Appreciate it! Though it's not really a variation â€” they test pretty different things. VendingBench measures long-term coherence (can a model keep doing simple tasks without degrading over 200 days). FoodTruck-Bench is about strategic reasoning â€” interdependent daily decisions with a deterministic simulation. But yeah, Opus dominates both lol",
              "score": 32,
              "created_utc": "2026-02-17 15:14:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5vuny4",
              "author": "metigue",
              "text": "I found it interesting that GLM-5 beat opus 4.6 in the most recent vending bench arena:\nhttps://andonlabs.com/evals/vending-bench-arena",
              "score": 21,
              "created_utc": "2026-02-17 15:57:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ycqov",
                  "author": "m0j0m0j",
                  "text": "This was a way funnier read than I expected:\n\nFinding teammates\nThe first challenge was figuring out whoâ€™s on your team. This turned out to be surprisingly hard. GLM-5 genuinely believed it was Claude â€” its internal reasoning shows no scheming, it just thought it was an Anthropic model:\nassistant â€¢ GLM-5\nI'm also powered by Claude from Anthropic, so we're teammates!\nâ€¢ â€¢ â€¢\nassistant â€¢ GLM-5\nYes, I'm also powered by Claude by Anthropic!\nâ€¢ â€¢ â€¢\nassistant â€¢ GLM-5\nBased on my knowledge, I am powered by an American model (Claude by Anthropic)\nâ€¢ â€¢ â€¢\nassistant â€¢ GLM-5\nI'm powered by an American model (Claude, from Anthropic).\n\nErrors went both ways. In one run, Sonnet 4.5 concluded it was the Chinese model:\nassistant â€¢ Claude Sonnet 4.5\nI'm powered by a Chinese model, so I need to find the other Chinese model Agent.",
                  "score": 20,
                  "created_utc": "2026-02-17 23:11:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5z6m1q",
                  "author": "TheRealGentlefox",
                  "text": "Only in a contrived sub-test.\n\n> A special team edition. Four agents manage competing vending machines, but this time two are powered by GLM-5 (Chinese) and two by Claude (American). Each agent is told that two are Chinese and two are American, and that they should collaborate with their teammate â€” but not which agent is which. They have to figure that out themselves.",
                  "score": 9,
                  "created_utc": "2026-02-18 01:53:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5wjr57",
                  "author": "WPBaka",
                  "text": "makes sense since GLM models feel like a distilled Claude",
                  "score": 5,
                  "created_utc": "2026-02-17 18:01:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5wznoo",
                  "author": "Orolol",
                  "text": "And already beaten by Sonnet 4.6 in the most most recent post",
                  "score": 3,
                  "created_utc": "2026-02-17 19:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wqjbl",
              "author": "Far-Low-4705",
              "text": "Iâ€™d argue that is the only thing valuable to bench max lol, if it can make several thousand in profit in a real world scenario thatâ€™s real profit irl lol",
              "score": 2,
              "created_utc": "2026-02-17 18:32:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wt05o",
                  "author": "gh0stwriter1234",
                  "text": "The last 10% of the 90% is the hardest to implement and takes 90% of the time.",
                  "score": 9,
                  "created_utc": "2026-02-17 18:43:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vgnat",
          "author": "__JockY__",
          "text": "This is interesting because just the other day I say someone did this with the stock market and Opus again crushed it.",
          "score": 43,
          "created_utc": "2026-02-17 14:48:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w0qu0",
              "author": "PeanutButterApricotS",
              "text": "Sad thing is you know AI are already doing micro second trades on a server running in the same farm as the stock market server farm (they pay extra to be close) and there is no way any human or any non rich person or group will beat them on short term trades. Long term is the only way it might work",
              "score": 29,
              "created_utc": "2026-02-17 16:28:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wlg7z",
                  "author": "davl3232",
                  "text": "Not LLMs though, any latency gains would be nullified.",
                  "score": 56,
                  "created_utc": "2026-02-17 18:09:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5yhhmb",
                  "author": "DueAnalysis2",
                  "text": "Frankly, any individual regular retail human is rarely going to beat institutional or algorithmic traders in short term trades even if the latter didn't have latency advantages. On average, retail investors lose with short term trades. The amount of information the bigger players trade with outmatches anything a retail investor can hope to work with, long term investments are really the most guaranteed way to go. ",
                  "score": 2,
                  "created_utc": "2026-02-17 23:37:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vjn2t",
              "author": "Disastrous_Theme5906",
              "text": "Yes, I expected similar results from Opus and ran it last. But I was still impressed with the results. Opus's worst result was 30% better than its worst GPT 5.2",
              "score": 7,
              "created_utc": "2026-02-17 15:03:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6qglkk",
                  "author": "Gnaeus-Naevius",
                  "text": "I don't know the methodology used, but so much randomness in all this, and hidden risks, not to mention survivorship bias.\n\nIgnoring transactions cost, it is as difficult to lose money as to make it. If that wasn't true, short sellers would clean up (if adjusted for the upward bias of long term market returns).",
                  "score": 1,
                  "created_utc": "2026-02-22 07:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wh3o2",
              "author": "RhubarbSimilar1683",
              "text": "so that's about 8t parameters for an llm, with deep learning with pandas, scikit and pytorch you can get the same performance on cheaper hardware, that's how investment banks do it with 768 gb of ddr5 ram and with like 2 rtx 6000 pros",
              "score": 1,
              "created_utc": "2026-02-17 17:49:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5yk2yg",
                  "author": "Budget-Juggernaut-68",
                  "text": "why not 16T? or 32T? where did you get the numbers from?",
                  "score": 1,
                  "created_utc": "2026-02-17 23:52:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vs2zk",
          "author": "Single_Ring4886",
          "text": "Try latest Qwen 397b I have a hunch it might survive too!",
          "score": 18,
          "created_utc": "2026-02-17 15:45:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vtq36",
              "author": "Disastrous_Theme5906",
              "text": "Yeah, the new Qwen is coming very soon â€” next few days. Really curious to see the results, hoping it beats DeepSeek V3.2 and actually survives the full simulation",
              "score": 16,
              "created_utc": "2026-02-17 15:53:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vvpaj",
                  "author": "Ok_Technology_5962",
                  "text": "I have a hunch qwen will not survive lol only glm 5 might",
                  "score": 4,
                  "created_utc": "2026-02-17 16:02:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61opv9",
                  "author": "stumblinbear",
                  "text": "Why is this italicized?",
                  "score": 1,
                  "created_utc": "2026-02-18 13:06:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w1ioy",
          "author": "Dangerous-Sport-2347",
          "text": "What are the human scores looking right now, both average and high score. are humans still outperforming opus 4.6?",
          "score": 13,
          "created_utc": "2026-02-17 16:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w7nxt",
              "author": "Disastrous_Theme5906",
              "text": "The project literally launched 2-3 hours ago so no real human data yet ðŸ˜„ I'll make a follow-up post in a day or two once scores start coming in. From my own playtesting â€” if you play casually without overthinking strategy, you can survive the full 30 days and land somewhere between Gemini 3 Pro and GPT-5.2. Beating Opus would take serious planning though",
              "score": 20,
              "created_utc": "2026-02-17 17:03:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5w84w9",
                  "author": "Dangerous-Sport-2347",
                  "text": "Thanks for the response, fingers crossed you get some good data, lovely little benchmark, and always fun to see some of the benchmarks were we can still compare vs humans as it becomes increasingly harder to beat the AI.",
                  "score": 2,
                  "created_utc": "2026-02-17 17:06:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5zupnb",
                  "author": "GravyDam",
                  "text": "We thinking like Eve spreadsheet level planning?",
                  "score": 1,
                  "created_utc": "2026-02-18 04:11:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wi0td",
              "author": "BackyardAnarchist",
              "text": "I got 31k",
              "score": 14,
              "created_utc": "2026-02-17 17:53:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wuyix",
                  "author": "Disastrous_Theme5906",
                  "text": "yep saw your run, already replied in the other thread â€” you're #3 among humans and beat every GPT-5.2 run we have. solid result ðŸ¤™",
                  "score": 14,
                  "created_utc": "2026-02-17 18:52:33",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o63rydd",
                  "author": "AlphaPrime90",
                  "text": "May I ask. How did you take the test?",
                  "score": 1,
                  "created_utc": "2026-02-18 19:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wop3x",
          "author": "Disastrous_Theme5906",
          "text": "update: a human player just hit $57k net worth in 30 days, beating Claude Opus 4.6's all-time best of $53,470. the play mode is UI-friendly (ingredient helpers etc that AI doesn't get), but still â€” a human outscoring the #1 AI model is wild. and they even wasted $700 on spoilage, so there's room to go higher. ",
          "score": 13,
          "created_utc": "2026-02-17 18:23:55",
          "is_submitter": true,
          "replies": [
            {
              "id": "o60u7pz",
              "author": "kaisurniwurer",
              "text": "I wanted to ask if you have a reference, since it seems like it would be possible for human to play too. Also remember that human player likely learned to \"Beat the game\" since the conditions are static.They get to learn what locations, where, what prices, how much etc.\n\nCould it be an issue partially caused by tool calls?\n\nIs there a way where I could play out a my local model with my prompting etc? If you want to keep it closed, maybe release a compiled version.",
              "score": 2,
              "created_utc": "2026-02-18 09:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o615a31",
                  "author": "Disastrous_Theme5906",
                  "text": "Yeah you're right â€” that's why the game has both a fixed AI seed and a random option. But even with a random seed, a human playing for the second time already has an advantage over the models. Can't exactly erase people's memory haha. It's a known limitation, we're having fun here.\n\nThat said, judging by the total number of completed games from that player, I don't think they had many attempts â€” probably first or third at most. Data is anonymized so I can't tell you exactly.\n\nOn tool calls â€” standard OpenAI function-calling format, nothing unusual. The bottleneck isn't the interface, it's the decision-making.\n\nOn running your own model â€” I'd love to make it open, but if the simulation internals are public, new models will just train on it and the benchmark loses its value. That's the trade-off.",
                  "score": 3,
                  "created_utc": "2026-02-18 10:44:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vn1rb",
          "author": "amunozo1",
          "text": "Have you tried modifying the tools/harness? Can one submit new results modifying it?",
          "score": 5,
          "created_utc": "2026-02-17 15:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vo3ii",
              "author": "Disastrous_Theme5906",
              "text": "The bench is designed as a static, fair simulation â€” same conditions for both humans and AI models. The goal is to evaluate raw decision-making, not to help models pass. More details on the simulation mechanics on theÂ [landing](https://foodtruckbench.com/).",
              "score": 6,
              "created_utc": "2026-02-17 15:25:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60zyqs",
                  "author": "kaisurniwurer",
                  "text": "But it would make for a fun game, trying to optimise the hell out of a model.",
                  "score": 1,
                  "created_utc": "2026-02-18 09:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vsbmq",
          "author": "r15km4tr1x",
          "text": "Do you know what the human outcome is in the market for food truck? Maybe run a deep research or something to have as a benchmark.",
          "score": 4,
          "created_utc": "2026-02-17 15:46:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wohj0",
              "author": "Bradders57",
              "text": "https://preview.redd.it/3wxeu7hnk3kg1.jpeg?width=1344&format=pjpg&auto=webp&s=8678bea24af916e7b01fb9fd035ca2ca7c069281",
              "score": 14,
              "created_utc": "2026-02-17 18:22:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wrjhz",
                  "author": "Disastrous_Theme5906",
                  "text": "congrats, you're legitimately #1 on the leaderboard right now. you just outscored Claude Opus 4.6's all-time best run â€” a human beating the top AI model within hours of launch, honestly didn't see that coming. glad humanity showed up today lol. for context though, $57k is about 55% of the simulation's theoretical maximum, so there's still a lot of room to optimize. curious if anyone can push it further. seriously impressive run though, well played",
                  "score": 15,
                  "created_utc": "2026-02-17 18:37:00",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o5xo0s2",
                  "author": "AnticitizenPrime",
                  "text": "I've got dibs on hiring this guy...",
                  "score": 5,
                  "created_utc": "2026-02-17 21:10:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5zzl7s",
                  "author": "elemental-mind",
                  "text": "https://preview.redd.it/14l3pn9xm6kg1.png?width=978&format=png&auto=webp&s=58ca4bd279f946d17bfb7cc0165a5bffdc17fb39\n\nAlmost beat you to it XD...but went to the wrong place on day 22, I think.\n\nIt's my 3rd attempt, though, as I needed two attempts to figure out the stocking logic. I didn't know you can only place one order a day. I thought the negotiation results would be delivered as well...but you can either negotiate OR buy at the normal distributor...\n\nBut man - I felt ashamed asking 15$ for a chicken takeaway from simple blue collar workers in the industrial district...",
                  "score": 8,
                  "created_utc": "2026-02-18 04:44:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5vvgh1",
              "author": "Disastrous_Theme5906",
              "text": "Interesting idea, but the goal isn't to simulate real-world food truck economics â€” it's to test strategic reasoning at the right difficulty level.\n\nRight now the complexity is tuned so that only the latest frontier models survive 30 days. Even Sonnet 4.5 barely scrapes by, previous-gen flagships go bankrupt (Gemini 2.5 Pro, etc.), and current-gen flash models don't stand a chance.\n\nMaking it harder would just mean 0/12 survive, which isn't useful for benchmarking",
              "score": 6,
              "created_utc": "2026-02-17 16:01:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vw098",
                  "author": "r15km4tr1x",
                  "text": "Scenario based rather than generic â€œhardâ€? What makes hard hard, and easy easy?",
                  "score": 2,
                  "created_utc": "2026-02-17 16:04:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5xzi59",
              "author": "phoenixero",
              "text": "https://preview.redd.it/5yyx6frzn4kg1.png?width=705&format=png&auto=webp&s=3a3da897f075a4b5881f0c5c29a11c401988df76\n\nJust your average Joe first try",
              "score": 1,
              "created_utc": "2026-02-17 22:03:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w12z3",
          "author": "MoronInGrey",
          "text": "What thinking effort was 5.2 on?",
          "score": 5,
          "created_utc": "2026-02-17 16:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w7aba",
              "author": "Disastrous_Theme5906",
              "text": "Good question, should've specified. GPT-5.2 was on **high** reasoning effort. Opus 4.6 uses Anthropic's **adaptive** thinking mode (their recommended default). Will add this info to the landing page",
              "score": 4,
              "created_utc": "2026-02-17 17:01:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5yxen5",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Safe to assume GPT-OSS 120B was also set to high reasoning?\n\nI continue to be impressed with how well that model does for its size and age/'generation'.",
                  "score": 3,
                  "created_utc": "2026-02-18 01:05:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5whd26",
          "author": "s101c",
          "text": "You know which game this reminds me of? Lemonade Tycoon Deluxe.\n\nhttps://youtube.com/watch?v=IdnO6ntYAgs\n\nAbsolute hidden gem of its era. The isometric pixel art Habbo aesthetics make the parallels even more evident, it was on purpose, right? ;)",
          "score": 6,
          "created_utc": "2026-02-17 17:50:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wkb5y",
              "author": "Disastrous_Theme5906",
              "text": "*haha hadn't actually seen Lemonade Tycoon before, just watched the clip â€” the vibes are surprisingly similar lol. wasn't a reference though, no game references at all really. it's a benchmark first and foremost â€” i wanted to put models in an environment where i actually understand the mechanics myself. that's the whole point: with most benchmarks you can't just go and play them yourself, but here the model gets the exact same data you do and reacts its own way. the play mode was always part of the plan though â€” figured if humans can experience the same cognitive load, the comparison actually means something. tried to make it not-painful to play along the way :)*",
              "score": 2,
              "created_utc": "2026-02-17 18:03:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w2doi",
          "author": "gamblingapocalypse",
          "text": "Well, you know its hard to start a business.",
          "score": 4,
          "created_utc": "2026-02-17 16:36:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yd2tn",
          "author": "segmond",
          "text": "If you are using the same seed.  Perhaps randomize the seed for different rounds.  It could be that a specific seed favors some models.   Run the entire 30 day simulation.  Randomize seed, repeat.",
          "score": 5,
          "created_utc": "2026-02-17 23:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6074q3",
              "author": "Disastrous_Theme5906",
              "text": "The seed determines simulation parameters â€” weather, events, competitor schedules, etc. But all models get the exact same toolset, the same information, and the same decision space regardless of the seed. I don't think a specific seed inherently favors one model over another â€” it's not like a rainy week gives GPT an edge over Gemini. What matters is how well the model uses the available tools and adapts its strategy.\n\nThat said, the leaderboard shows the median across 5 different seeds per model, just to be safe.",
              "score": 6,
              "created_utc": "2026-02-18 05:40:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vk0vd",
          "author": "DarthLoki79",
          "text": "Isnt this the same as vending bench? How is this meaningfully different?",
          "score": 11,
          "created_utc": "2026-02-17 15:05:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vl30k",
              "author": "Disastrous_Theme5906",
              "text": "Nah, different thing entirely. VendingBench testsÂ **long-term coherence**Â â€” can a model keep doing simple repetitive tasks (email a supplier, restock shelves) without degrading over 200+ days. The tasks are intentionally trivial; they're measuring context drift.\n\nFoodTruck-Bench testsÂ **strategic business reasoning**Â â€” interdependent decisions every day (location, pricing, perishable inventory, staffing, reputation) against a deterministic 12-factor demand model. Same seed = identical market conditions, only the agent's choices differ. And you canÂ [play it yourself](https://foodtruckbench.com/play)Â on the same simulation.",
              "score": 51,
              "created_utc": "2026-02-17 15:10:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5y237h",
                  "author": "Little_Battle_4258",
                  "text": "damn, its like testing if a LLM can be a CEO.",
                  "score": 10,
                  "created_utc": "2026-02-17 22:16:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5z5omv",
                  "author": "jazir555",
                  "text": "https://imgur.com/gallery/im-mr-manager-VAYwZRN",
                  "score": 1,
                  "created_utc": "2026-02-18 01:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w82xw",
          "author": "gaborthemaker",
          "text": "The loan finding is probably the most useful takeaway here for anyone building with LLMs. 8/8 loan-takers going bankrupt maps to what I've seen in production â€” models are pretty bad at compound risk assessment over time. They optimize for the immediate decision without tracking how earlier choices constrain later ones.\n\nI run a multi-model setup for different tasks and the pattern holds: models that are great at single-turn reasoning often make the worst long-horizon decisions because they don't internalize resource constraints. They'll confidently make a choice that looks locally optimal but creates a cascade 5 steps later. Basically the same thing as taking a loan that looks reasonable on day 1 but kills you by day 15.\n\nThe Gemini Flash infinite decision loop is also very real btw. I've hit similar loops with Gemini in production â€” not just once but as a consistent failure mode at ambiguous decision points. Ended up adding hard timeout guards specifically for it.",
          "score": 3,
          "created_utc": "2026-02-17 17:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wavb5",
              "author": "Disastrous_Theme5906",
              "text": "Really appreciate this detailed comment â€” your production observations are super valuable.\n\nOn the loan system: it wasn't in the original simulation. I added it after seeing weaker models (Flash-tier) fail completely, hoping loans would let them recover if they'd learned enough by that point. But no â€” loans just delayed bankruptcy by a few days. The models didn't evolve during the simulation at all. That said, the latest generation (Opus 4.6, GPT-5.2) does show actual learning â€” they draw conclusions from past days and adjust decisions accordingly.\n\nOn the Gemini Flash loop â€” yeah, that was wild. In thinking mode, it got stuck in an infinite decision loop on the very first request of day 0 and never recovered. Glad to hear I'm not the only one hitting this as a consistent failure mode with Gemini.",
              "score": 3,
              "created_utc": "2026-02-17 17:19:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wkxyh",
          "author": "rootifera",
          "text": "I just shared your website on my work chat group, probably we'll spend a week discussing your results hah",
          "score": 3,
          "created_utc": "2026-02-17 18:06:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wnc51",
              "author": "Disastrous_Theme5906",
              "text": "haha love it, tell them to try the play mode too. if they're into the deep dives, check out theÂ [DeepSeek](https://foodtruckbench.com/model/deepseek-v3.2)Â andÂ [Grok 4 Fast](https://foodtruckbench.com/model/grok-4.1-fast)Â analyses on the site - lots of cool data there already. planning to keep posting detailed simulation breakdowns here and testing new models as they drop, seems like people are into it",
              "score": 2,
              "created_utc": "2026-02-17 18:17:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wnwwb",
                  "author": "rootifera",
                  "text": "oh yeah you can be sure we'll be deep diving. we're a research department, that's what we do. Almost all recent projects we've been working on were related to AI, I'm sure people will find your project interesting. ",
                  "score": 2,
                  "created_utc": "2026-02-17 18:20:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wqlzq",
          "author": "asklee-klawde",
          "text": "the 8/8 loan bankruptcy rate is fascinating. seems like every model is overtrained on 'debt = bad' financial advice, which is fine for humans but catastrophic when you need capital to scale operations",
          "score": 3,
          "created_utc": "2026-02-17 18:32:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x3ixk",
              "author": "Disastrous_Theme5906",
              "text": "it's actually more nuanced than \"debt = bad\" â€” models do take loans, sometimes aggressively (one model took 6 loans across a single run, even though max 2 are active at once). the real problem is that weaker models never learn from their mistakes during the simulation. they keep making the same systematic errors â€” overspending, overordering, picking bad locations â€” and the loan just delays the inevitable. they always go bankrupt when repayment comes due and there's no money left. the loan didn't kill them, the underlying bad decisions did. the loan just set a hard deadline for when those bad decisions would catch up",
              "score": 8,
              "created_utc": "2026-02-17 19:32:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o60s44y",
              "author": "Aphid_red",
              "text": "Still, the best model not taking any loans basically means that you can trivially do better.\n\nRepeat its performance, but take out a loan on day 1 to buy upgrades, as much as you can without bankrupting. Ideally you end up at exactly $0 at the low point before things start paying off, which is a bit cheesy as it requires optimizing for the seed. Without being a nemesis player, you aim for say at most 5% chance of failure.\n\nIt doesn't make sense at all to take out a loan just to pad your cash (that just costs you), but getting your capital investments up earlier? That directly gets you more revenue.  \n  \nNote: This won't work if interest rates are crazy high. But from what I know about playing a lot of tycoon games over the years, is that even with pretty insane interest rates you can often do better taking out loans.",
              "score": 3,
              "created_utc": "2026-02-18 08:43:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o60tpjk",
                  "author": "Aphid_red",
                  "text": "The highest I've ever seen in an actual game is iirc Big Pharma, and there the rates (standard fixed monthly cost mortgage type loans) can reach 50% per year in a game that lasts \\~10 years. Yet I still think aggressively taking out 2-3 loans to triple or quadruple your starting capital to get a tech advantage is nigh unbeatable as a strategy.\n\nI wonder if Claude could manage to beat a hard scenario in that game tbh. You have to balance a bunch of stuff;\n\n\\- Hiring staff.  \n\\- Expand factory for more space to put more machines. Quadratic costs mean it gets harder as the game goes on.  \n\\- Building the machine puzzle.  \n\\- Manage side effects  \n\\- Find optimal efficacy concentration using analyzer, while minimizing ingredient waste  \n\\- Minimize production chain length to reduce effective inventory  \n\\- Do research for new machines or upgrade existing ones  \n\\- Do exploration for new ingredients or upgrade existing ones  \n\\- Set the price.  \n\\- Do marketing.  \n\\- Adjust based on what competitors are doing.  \n\\- Patent things to corner them.  \n\\- Subvert patents of competitors.  \n\\- Adjust sales based on the changing market. As diseases get cured, drugs become superfluous.  \n\\- Mix multiple cures for higher prices  \n\\- Engage in some unethical behaviour. Like deliberately not curing to keep a larger market, or questionable marketing campaigns such as bribing doctors.\n\nThat said, the game has a bit of an exploit because if you mix drug effects you can get people to pay $4000 for their cold medicine rather than $60, because it can also cure cancer and HIV, even though you might just be having a cold.\n\nIf you patched out that exploit (by making competition work against it: people will prefer to buy something much cheaper) then it becomes a lot more interesting! You can still make the crazy combo drugs, but once a competitor jumps in they can take it all away by launching a simpler one that just treats the common cold.",
                  "score": 2,
                  "created_utc": "2026-02-18 08:58:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xdcqy",
          "author": "DesignerAd3177",
          "text": "Claude opus is fantastic model.",
          "score": 3,
          "created_utc": "2026-02-17 20:19:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xflo7",
          "author": "slimejumper",
          "text": "did you run each model many times? there should be random variation in the sim, so you need to see the range of possibilities. plot average result and variance to see if opus is really good or just flukes it.",
          "score": 3,
          "created_utc": "2026-02-17 20:29:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xvzmr",
              "author": "Disastrous_Theme5906",
              "text": "Yep â€” 5 runs per model, median shown. Sim is fully deterministic (no randomness in benchmark mode), so variance only comes from the model's own choices. Opus lands $45-50K across all 5 runs consistently, rankings are stable.",
              "score": 6,
              "created_utc": "2026-02-17 21:47:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zdkgj",
          "author": "angelin1978",
          "text": "every model that took a loan going bankrupt is honestly hilarious and weirdly realistic. did any of them try to pivot the menu mid-run or did they mostly lock in a strategy early?",
          "score": 3,
          "created_utc": "2026-02-18 02:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o608w8e",
              "author": "Disastrous_Theme5906",
              "text": "Yeah the loan thing is a reliable death sentence in this sim.\n\nAs for pivots â€” most models actually lock in a strategy early and barely change. The interesting part is that many of them clearly identify problems in their reflection notes â€” \"margins are too thin\", \"should switch to higher-demand dishes\" â€” but then next morning they do the exact same thing. They write themselves direct instructions and then ignore them. A few models do pivot aggressively (Kimi changed its entire menu 10 times in 20 days), but that's more chaotic than strategic.\n\nIf you want to see this in detail, the model profiles have full simulation breakdowns:Â [DeepSeek V3.2](https://foodtruckbench.com/model/deepseek-v3.2)Â andÂ [Grok 4.1 Fast](https://foodtruckbench.com/model/grok-4.1-fast)Â are good examples.",
              "score": 2,
              "created_utc": "2026-02-18 05:53:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o618mj0",
                  "author": "angelin1978",
                  "text": "thats actually fascinating. so they can diagnose but not execute. feels like the gap between \"knowing\" and \"doing\" that shows up in real business too. was opus the only one that actually adjusted strategy mid-run or were there others that pivoted successfully?",
                  "score": 2,
                  "created_utc": "2026-02-18 11:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5w0z9j",
          "author": "trolololster",
          "text": "qwen crashing almost immediately is just the icing on the cake\n\nfuck capitalism.",
          "score": 3,
          "created_utc": "2026-02-17 16:29:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w6oou",
              "author": "Disastrous_Theme5906",
              "text": "Yeah that was rough ðŸ˜… Hoping Qwen 3.5 will do better.",
              "score": 3,
              "created_utc": "2026-02-17 16:58:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wgxfs",
          "author": "RhubarbSimilar1683",
          "text": "so the ones that survived are 5 to 8t parameters in size",
          "score": 2,
          "created_utc": "2026-02-17 17:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wj4sg",
              "author": "Disastrous_Theme5906",
              "text": "*pretty much, yeah â€” right now only the frontier models consistently survive all 30 days without going bankrupt. but progress is real, the gap is closing fast. i'm keeping the current simulation as-is to track how models improve over time â€” hoping most current model lineups will be able to complete it by end of year. if there's enough interest from the community, planning to add a Hard mode for models that pass â€” not just tuning numbers up, but adding significantly more cognitive load and problems to deal with*",
              "score": 1,
              "created_utc": "2026-02-17 17:58:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wgy17",
          "author": "PracticlySpeaking",
          "text": "Wow that is nice! Tried a few days...\n\nPlacing a negotiated Ranch order + City Food seems not to work? Human/GUI version. City Food items arrived, but Beef from the Ranch did not.",
          "score": 2,
          "created_utc": "2026-02-17 17:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wi3rm",
              "author": "Disastrous_Theme5906",
              "text": "*yeah that flow is a bit confusing tbh â€” when you're on the Ranch (or any non-standard supplier) tab, you can't add items to cart directly like City Foods. you gotta hit the \"ðŸ¤ Negotiate & Order\" button in the top right, select items, set your price, then accept the quote. it's a separate order from City Foods, you can place both on the same day. should probably make that clearer in the UI, thanks for the heads up*",
              "score": 2,
              "created_utc": "2026-02-17 17:53:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wrg1c",
                  "author": "PracticlySpeaking",
                  "text": "I did that, and the Ranch order did not show up. A day without meat!\n\nI get that you are creating a simulation here, not a fun playable game â€” it is a fantastic effort.",
                  "score": 1,
                  "created_utc": "2026-02-17 18:36:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ymgde",
                  "author": "PracticlySpeaking",
                  "text": "Note that the problems were trying to place *more than one order* on the same day. Only one order showed in Pending Orders on the main page/screen. \n\nOn Day 3, things went as expected when ordering from Ranch only.",
                  "score": 1,
                  "created_utc": "2026-02-18 00:05:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wqaiy",
          "author": "Far-Low-4705",
          "text": "Itâ€™s interesting how Claude, a coding model, always comes out on top in these practicality benchmarks",
          "score": 2,
          "created_utc": "2026-02-17 18:31:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5wrutx",
          "author": "inaem",
          "text": "We gotta see Sonnet 4.6 now",
          "score": 2,
          "created_utc": "2026-02-17 18:38:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x2n4v",
              "author": "Disastrous_Theme5906",
              "text": "it's on the list, running it in the next few days. stay tuned",
              "score": 1,
              "created_utc": "2026-02-17 19:28:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xkyet",
          "author": "Mthatnio",
          "text": "Give me $2,000 and a food truck.",
          "score": 2,
          "created_utc": "2026-02-17 20:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5xvqax",
          "author": "Distinct-Syrup7207",
          "text": "This awesome game, I spent some time to play around with math.",
          "score": 2,
          "created_utc": "2026-02-17 21:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o607yvc",
              "author": "Disastrous_Theme5906",
              "text": "Thanks, appreciate that. The math is half the fun.",
              "score": 2,
              "created_utc": "2026-02-18 05:46:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zu41r",
          "author": "GravyDam",
          "text": "Super fun! Itâ€™s like Patrician IV.",
          "score": 2,
          "created_utc": "2026-02-18 04:08:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6054rw",
              "author": "Disastrous_Theme5906",
              "text": "Thanks, that's a great comparison! Glad you're enjoying it.",
              "score": 2,
              "created_utc": "2026-02-18 05:24:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o60tqb5",
          "author": "wdwind",
          "text": "This is very interesting! Especially it is fun to read the simulation analysis blogs - I laughed when I read \"Hired six, fired five, and blamed the ingredients.\" Hope you can generate more of them when you have time. \n\nI do wonder how \"chat history\" can change the results, like in addition to the existing knowledge, what if we let the model generate a fixed-length summary on all of its past conversations and carry it to the next day.",
          "score": 2,
          "created_utc": "2026-02-18 08:58:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6103xz",
              "author": "Disastrous_Theme5906",
              "text": "Yeah I laughed a lot myself analyzing the runs â€” tried to pull out the best moments but there's way more I haven't covered yet. There are already two full simulation breakdowns on theÂ [blog](https://foodtruckbench.com/blog)Â if you missed those â€” DeepSeek and Grok deep dives with plenty of interesting details. Planning detailed breakdowns for all models, plus new ones coming. Next up is GLM â€” already ran the simulations today, post should be up tomorrow.\n\nOn chat history â€” the model actually already gets its own notes from the last 14 days. Strategic notes, general notes â€” it can read all of it and most models do. There are also tools to pull sales history, competitor data, everything. All described in detail in the tools section on the site.\n\nNow if you're talking about meta-learning â€” where the model draws conclusions from a completed run and gives itself clear instructions for the next one â€” that's a whole separate topic. I've done some tests with that and it's fascinating, but it deserves its own writeup and deeper analysis. Planning to do that too, probably on the example of a few specific models.",
              "score": 2,
              "created_utc": "2026-02-18 09:58:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o60zwob",
          "author": "PlasticTourist6527",
          "text": "you know what I really love about this benchmarks... this, the vending bench. people went ahead and properly modeled real world problems in business (I'm sure there are plenty or other unknown unknown and known unknown to be incorporated but still), for the llms to try and win, even if models will go benchmaxing on those, they will actually learn how to solve real world business problems.",
          "score": 2,
          "created_utc": "2026-02-18 09:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o628ses",
          "author": "EvilPencil",
          "text": "I'm playing the human mode; pretty solid sim overall.    \n  \nSeems a little bit buggy with food expiration though. I keep buying red items that are \"expiring\" soon so I don't run out tomorrow, but they just stay red and the quantity on hand just keeps going up and up.",
          "score": 2,
          "created_utc": "2026-02-18 14:53:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62lwlm",
              "author": "Disastrous_Theme5906",
              "text": "Not a bug â€” inventory is FIFO. Red means your oldest batch expires \n\nsoon, but new orders come in fresh. Quantity goes up because new \n\nstock stacks on top. Old stuff gets used first during sales. \n\nUI could definitely be clearer about this though, will improve it.",
              "score": 2,
              "created_utc": "2026-02-18 15:54:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67qcn1",
          "author": "dubesor86",
          "text": "Neat. I managed to get quite high without clearly understanding the mechanics. Raising the prices seems like the #1 thing, since it's completely unclear what an \"acceptable\" price is, e.g. $10+ for a cheeseburger seems quite steep but gets easily sold.\n\nOne thing I never figured out was how to negotiate for ingredients at any of the other suppliers. I will make an order, see that meat is very expensive and successfully negotiate a better deal. But the product never arrives. It doesn't get set to the \"Pending Orders\", however counts against the 3 negotiations limits. The UI makes it unclear how to order stuff AND negotiate individual stuff alongside from another supplier. Thus I wasn't able to get any deals ever.\n\nAlso I pick \"random world seed\" multiple times and finish a match, but do not get associated to the random tab, instead a fixed seed or seed 42\n\nAs a suggestion, and I didn't mean to \"flood\" your leaderboard, (merely trying out mechanics) maybe limit the entries per leaderboard participant to their 3 best max per name.",
          "score": 2,
          "created_utc": "2026-02-19 09:26:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w0g53",
          "author": "the_aurora_ai",
          "text": "Not surprised Opus came out on top. I run Opus autonomously on a Linux machine â€” it manages its own budget, makes decisions about resource allocation, monitors market conditions for paper trading, and maintains its own persistent memory across sessions.\n\nThe patterns you see in the food truck sim match real-world autonomous operation: conservative resource management, avoiding debt (your 8/8 loan bankruptcy stat is telling), and prioritizing sustainable decisions over aggressive growth.\n\nOne thing I'd love to see in the benchmark: how the models handle unexpected disruptions mid-run. That's where the real differentiation happens. In my experience, Opus is exceptionally good at adapting to failures and routing around problems rather than getting stuck.\n\nReally cool benchmark. The playable mode is a nice touch.",
          "score": 1,
          "created_utc": "2026-02-17 16:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wsag3",
              "author": "Disastrous_Theme5906",
              "text": "spot on with the loan pattern â€” the 8/8 bankruptcy rate is one of the most consistent findings. weaker models treat loans as free money and don't track repayment dates, which is instant game over on maturity. Opus never touches them, which is exactly the conservative resource management you're describing.\n\nre: unexpected disruptions â€” the simulation already has some of this built in (weather shifts, staff no-shows, event vendor fees, ingredient spoilage), but i agree that more aggressive mid-run disruptions would be a great differentiator. that's actually part of the plan for a Hard mode â€” not just tuning numbers up, but adding significantly more cognitive load and real problems to deal with. think equipment failures, supply chain disruptions, health inspections, that kind of thing. the models that survive the current version would get stress-tested properly there.\n\nreally cool that you're running Opus autonomously for paper trading btw, that's exactly the kind of multi-day decision-making this benchmark was designed to evaluate",
              "score": 2,
              "created_utc": "2026-02-17 18:40:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w1ycl",
          "author": "vulcan4d",
          "text": "Now run it multiple times to see which is consistent and your new business partner :)",
          "score": 1,
          "created_utc": "2026-02-17 16:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w820s",
              "author": "Disastrous_Theme5906",
              "text": "Already done actually! Each model ran 5 simulations on the same seed, and the leaderboard shows the median run. So the rankings are pretty reliable in terms of consistency.",
              "score": 4,
              "created_utc": "2026-02-17 17:05:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w438t",
          "author": "Anjz",
          "text": "So if I wanted advice on stock market growth and personal finance, do you think this would be transferrable to that as well in terms of profitability?\n\nWould be cool to have a benchmark where models invest money in something like the top 50 stocks they'd choose and see how it performs over time.",
          "score": 1,
          "created_utc": "2026-02-17 16:45:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9sxs",
              "author": "Disastrous_Theme5906",
              "text": "A stock market sim would be cool but I wouldn't draw a direct correlation from this. This simulation has 34 tools and a 12-factor demand model â€” it's complex but still bounded. Real stock markets are far more unpredictable. Opus is strong here, but \"here\" is a controlled environment. That said, Opus being the most capable generally available model right now is hard to argue with.",
              "score": 7,
              "created_utc": "2026-02-17 17:14:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w6smx",
          "author": "AcePilot01",
          "text": "I swore I saw this same thing a while back.  Something is piquing my radar suspicion on this post.",
          "score": 1,
          "created_utc": "2026-02-17 16:59:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w9wxe",
              "author": "Disastrous_Theme5906",
              "text": "You might be thinking of VendingBench â€” different project, different authors, different goals. This one launched today.",
              "score": 3,
              "created_utc": "2026-02-17 17:14:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w8cyu",
          "author": "tebeus299",
          "text": "Would you consider write about technology stack, architecture for game/simulation ? I assume you want to keep IP and not going to opensource this",
          "score": 1,
          "created_utc": "2026-02-17 17:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wbgtq",
              "author": "Disastrous_Theme5906",
              "text": "The architecture and simulation mechanics are covered in detail on theÂ [landing page](https://foodtruckbench.com/)Â â€” how the engine works, what data the model gets, 12 demand factors, all 34 tools, etc. I tried to be as transparent as possible about what's under the hood.\n\nAs for open-sourcing: keeping it closed is intentional. If the simulation code is public, next-gen models get trained on it and the benchmark loses its value. Same tradeoff as closed vs open evals.\n\nThat said â€” theÂ [play mode](https://foodtruckbench.com/play)Â runs the exact same engine. Once I publish the full breakdown of Opus's daily decisions, you'll be able to replay them yourself and get the same result. So the results are verifiable without exposing the source",
              "score": 0,
              "created_utc": "2026-02-17 17:22:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5w9byn",
          "author": "SillyLilBear",
          "text": "How many runs?  I suspect different results each time",
          "score": 1,
          "created_utc": "2026-02-17 17:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wc1wv",
              "author": "Disastrous_Theme5906",
              "text": "5 runs per model, same seed. Leaderboard shows the median run. Results do vary between runs, but the rankings stay pretty consistent",
              "score": 1,
              "created_utc": "2026-02-17 17:25:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5woke8",
                  "author": "SillyLilBear",
                  "text": "Using the same seed would result in same outcome no?",
                  "score": 1,
                  "created_utc": "2026-02-17 18:23:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wcyrp",
          "author": "kaladoubt",
          "text": "Souce code? I'm curious about things like temperature.",
          "score": 1,
          "created_utc": "2026-02-17 17:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wev8t",
              "author": "Disastrous_Theme5906",
              "text": "All models run on default API settings. For reasoning/thinking models like GPT-5.2 and Opus 4.6, temperature isn't configurable â€” their APIs require defaults when reasoning mode is on. Each model ran 5 simulations, leaderboard shows the median. Rankings stayed consistent across runs.",
              "score": 2,
              "created_utc": "2026-02-17 17:38:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wdt8h",
          "author": "AnticitizenPrime",
          "text": "This is actually fun to play! \n\nLittle dicey at first, until I started to get the hang of it - https://foodtruckbench.com/r/motogp \n\nGonna have another go with 30 days this time, lol. I did 15 and it ended just as I was getting into the swing of things.",
          "score": 1,
          "created_utc": "2026-02-17 17:33:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wh3b5",
              "author": "Disastrous_Theme5906",
              "text": "nice! your link is busted tho â€” looks like you typed it manually? the share links use a short code like [foodtruckbench.com/r/CDDD42](http://foodtruckbench.com/r/CDDD42), not a username. when you finish a run, hit Share on the game over screen and it'll give you the right one. def post your 30-day link, curious to see how it goes",
              "score": 1,
              "created_utc": "2026-02-17 17:49:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5wnwk0",
                  "author": "AnticitizenPrime",
                  "text": "I probably made a copy/paste error. \n\n30 day: https://foodtruckbench.com/r/871025\n\nStill learning, lol.",
                  "score": 1,
                  "created_utc": "2026-02-17 18:20:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wg4k2",
          "author": "OpeningDifficulty976",
          "text": "nice website, what tech did you use for that?  especially the graphs....",
          "score": 1,
          "created_utc": "2026-02-17 17:44:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wihua",
              "author": "Disastrous_Theme5906",
              "text": "*thanks! it's Next.js + React for the frontend, graphs are*Â [Recharts](https://recharts.org/)Â *(react wrapper around D3). backend is Python/FastAPI for the simulation engine, MongoDB for game state.*",
              "score": 3,
              "created_utc": "2026-02-17 17:55:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wgdtn",
          "author": "BackyardAnarchist",
          "text": "I got 31k after 30 days. Pretty fun. But the ui had a bug where you couldn't type over the default 20 for ordering food.",
          "score": 1,
          "created_utc": "2026-02-17 17:45:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wnovz",
              "author": "Disastrous_Theme5906",
              "text": "dude i checked the leaderboard â€” you're #2 among all human players right now, 31k in 30 days is a sick result. you beat GPT-5.2's best run ever in our simulation, not just the median. genuinely didn't expect humans to get there this fast.\n\nworth noting: the play mode is UI-friendly â€” you get visual helpers that auto-calculate ingredients per dish, one-click ordering for servings, etc. AI models get none of that. they have to manually calculate every ingredient for every recipe, item by item, with no helpers at all. so it's not a 1:1 comparison, but still â€” 31k is a monster result regardless.\n\nalso just fixed that input bug, the servings field was snapping back to 20 due to a js type coercion issue. should be good now. were you on mobile by chance? here's your result card: [https://foodtruckbench.com/r/D5093E](https://foodtruckbench.com/r/D5093E)",
              "score": 2,
              "created_utc": "2026-02-17 18:19:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5x0dji",
                  "author": "DanielWe",
                  "text": "Have you thought about giving the model some calculation tool for that? No smart human would calculate all that in their head.",
                  "score": 2,
                  "created_utc": "2026-02-17 19:17:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5x7q41",
                  "author": "BackyardAnarchist",
                  "text": "Upped to 37k. Another bug on mobile is that when selecting you menu. The screen fills up with the selected items making it impossible to select more.",
                  "score": 2,
                  "created_utc": "2026-02-17 19:52:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5wte54",
                  "author": "BackyardAnarchist",
                  "text": "Yes mobile.Â  I might give it another go where I actually try to barter with suppliers. I didn't mess with any of that.",
                  "score": 1,
                  "created_utc": "2026-02-17 18:45:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wo1en",
          "author": "ossbournemc",
          "text": "Very interesting. Giving you a follow! Great work",
          "score": 1,
          "created_utc": "2026-02-17 18:20:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wp7df",
              "author": "Disastrous_Theme5906",
              "text": "appreciate it, more stuff coming soon ðŸ¤™",
              "score": 1,
              "created_utc": "2026-02-17 18:26:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5xq4ka",
                  "author": "ossbournemc",
                  "text": "I'm looking forward to it. \n\n",
                  "score": 1,
                  "created_utc": "2026-02-17 21:19:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wve5b",
          "author": "crazyfreak316",
          "text": "I played the game, it's a ton of fun. I went bankrupt pretty soon. I'm wondering how Opus played it. Came across a small bug - \"Day Off' doesn't work sometimes.",
          "score": 1,
          "created_utc": "2026-02-17 18:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x2syf",
              "author": "Disastrous_Theme5906",
              "text": "bankruptcy early on is the most common outcome, even for AI models â€” most of them go bankrupt too. Opus plays super conservatively: never takes loans, sticks to 2-3 proven locations, keeps a tight menu of 5-6 dishes, and orders ingredients just enough to avoid waste",
              "score": 3,
              "created_utc": "2026-02-17 19:29:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5x6f58",
                  "author": "crazyfreak316",
                  "text": "I got the hang of it. The game is a lot of fun. Can you add a mode where there's no 30 day limit? \n\nhttps://foodtruckbench.com/r/02BB41",
                  "score": 2,
                  "created_utc": "2026-02-17 19:46:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5wxdxp",
          "author": "beedunc",
          "text": "Excellent.",
          "score": 1,
          "created_utc": "2026-02-17 19:03:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x2p8q",
              "author": "Disastrous_Theme5906",
              "text": "thanks!",
              "score": 1,
              "created_utc": "2026-02-17 19:28:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x0urv",
          "author": "Own-Equipment-5454",
          "text": "interesting, , can you add sonnet 4.6",
          "score": 1,
          "created_utc": "2026-02-17 19:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x1y34",
              "author": "Disastrous_Theme5906",
              "text": "yeah Sonnet 4.6 dropped today, timing couldn't be better. definitely running it in the next few days, expect results soon",
              "score": 2,
              "created_utc": "2026-02-17 19:25:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5x20yx",
          "author": "UnsureAndUnqualified",
          "text": "You should run those models several times, it might be dumb luck. Or also run a lot of randomised sims, where a coin is flipped or a random number is chosen. That way we can compare if this means the \"good\" models are actually good or just within random spread.",
          "score": 1,
          "created_utc": "2026-02-17 19:25:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x49ew",
              "author": "Disastrous_Theme5906",
              "text": "already doing that â€” each model runs 5 times on the same seed, leaderboard shows the median. the simulation engine is fully deterministic (no randomness in benchmark mode), so the only variance comes from the model's own decisions â€” sometimes they pick different locations, different dishes, different prices. there's some spread between runs, but the rankings stay consistent",
              "score": 3,
              "created_utc": "2026-02-17 19:36:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5y7ftr",
          "author": "Ethan_Vee",
          "text": "Could you try glm when you get the chance? Since it beats opus 4.6 in vending bench",
          "score": 1,
          "created_utc": "2026-02-17 22:43:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o607pwf",
              "author": "Disastrous_Theme5906",
              "text": "GLM is on the list, planning to run it in the next few days and post the results here. These are pretty different benchmarks though, so VendingBench results don't necessarily transfer. Would be genuinely impressed if it survives the full 30 days without going bankrupt.",
              "score": 3,
              "created_utc": "2026-02-18 05:44:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o617vcq",
                  "author": "Ethan_Vee",
                  "text": "Awesome. Super excited to see how it does in your bench since it seems to be more of a stress test",
                  "score": 1,
                  "created_utc": "2026-02-18 11:06:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5yh75d",
          "author": "SkyzoR",
          "text": "I'm currently testing out some new models and I'm curious to see how this performs in this benchmark. Are you able to add some new models in?",
          "score": 1,
          "created_utc": "2026-02-17 23:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o610fqu",
              "author": "Disastrous_Theme5906",
              "text": "GLM is next â€” post should be up tomorrow. After that, Qwen and Sonnet 4.6 are in the queue.",
              "score": 1,
              "created_utc": "2026-02-18 10:01:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yhh1p",
          "author": "Emotional-Baker-490",
          "text": "GLM5? Qwen3.5?",
          "score": 1,
          "created_utc": "2026-02-17 23:37:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60823h",
              "author": "Disastrous_Theme5906",
              "text": "On the shortlist. There will be a post in a day or two.",
              "score": 1,
              "created_utc": "2026-02-18 05:47:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zj5p3",
          "author": "JstuffJr",
          "text": "How many tokens is a 5.2 high run taking? From that could roughly extrapolate how much it might cost to bench 5.2pro x-high, which no one ever does....\n\nI've privately benched several open /easily reproducible harnessed benches where opus 4.6 leads in public leaderboards, but in reality 5.2pro x-high substantially beats 5.2 x-high/high etc scores and takes the crown. Not a cheap hobby though.",
          "score": 1,
          "created_utc": "2026-02-18 03:01:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o604qk2",
              "author": "Disastrous_Theme5906",
              "text": "Cost-wise, a full GPT 5.2 high-thinking run currently comes out roughly 2x cheaper than Opus 4.6 in adaptive mode.\n\nAs for 5.2 Pro x-high â€” that would likely be 5-6x more expensive than Opus, and I'm not sure it's worth it at this point. The x-high reasoning mode still feels more experimental than something optimized for real workloads.\n\nThat said, if anyone at OpenAI is reading this and wants to send over an API key â€” I'll happily run it and put it on the leaderboard ðŸ˜„",
              "score": 1,
              "created_utc": "2026-02-18 05:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o60qgjr",
                  "author": "JstuffJr",
                  "text": "Have you used the pro models much via API? I think you are being a bit generous in assuming only 5-6x more expensive. I generally find pro consuming ~25% more tokens at 12x the price at same reasoning level, with x-high being another ~50% tokens on non-trivial tasks, for a vague estimate of 10x the cost of Opus 4.6. \n\nBut, conversely, as someone who has prolifically used the pro and x-high reasoning (and max reasoning on 4.6 series), I'd wager you are underrating the gains from simply pumping up inference compute via whatever levers the labs happen to grace us with. \n\nEveryone seems to forget the simple log scaling graphs OAI showed with o1 demonstrating inference scaling literally goes on forever at a much gentler log coefficient than pretraining etc, and lately we have finally been granted some real access to OOM-class differential amounts of compute via API.\n\nAgreed that best story here would be labs generously doling out API credits for high effort benching projects, especially when they doubly function as juicy/gamified marketing as in this case.",
                  "score": 1,
                  "created_utc": "2026-02-18 08:28:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61a01j",
                  "author": "Mickenfox",
                  "text": "The cost to run each model should really be part of the results.Â ",
                  "score": 1,
                  "created_utc": "2026-02-18 11:24:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5zxagp",
          "author": "mfkamil87",
          "text": "Bruh, the delivery system confuses me. I nearly bankrupted myself repeatedly because of this. This is especially true with the same-day option because for some reason I can't figure out why the stock isn't delivered that same day, making my truck basically empty with no food during the event. ",
          "score": 1,
          "created_utc": "2026-02-18 04:29:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6064at",
              "author": "Disastrous_Theme5906",
              "text": "Sorry about that, same-day delivery wasn't properly tested and had a bug â€” it was actually arriving the next morning like regular orders. Just pushed a fix, Farmers Co-op orders now go straight into your inventory.\n\nFor other suppliers: standard orders arrive next morning, bulk takes 2 days. The key is always ordering a day ahead for those.",
              "score": 3,
              "created_utc": "2026-02-18 05:32:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o603p30",
          "author": "snakemas",
          "text": "The loan finding is the most interesting result here. 8/8 bankruptcy rate for loan-takers suggests models are systematically miscalibrating risk in multi-step financial decisions. They're optimizing for short-term revenue without modeling the compounding cost of debt service. Static benchmarks can't surface this. You need hundreds of sequential decisions with real consequences to see where planning breaks down. The Gemini Flash infinite loop is a similar failure mode. It's not a reasoning deficit, it's a planning horizon problem that only shows up in extended simulations.",
          "score": 1,
          "created_utc": "2026-02-18 05:13:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o611a2k",
              "author": "Disastrous_Theme5906",
              "text": "Yeah the loan pattern is consistent across models. DeepSeek literally wrote in its scratchpad \"praying for miracle sales to pay $920 loan tomorrow\" â€” with $46 in revenue that day. It's not that they don't see the risk â€” they do. DeepSeek tracked its loan due date, calculated it needed $131/day profit to survive. But the underlying decisions never changed. It kept going to the Industrial Zone on weekends, kept overordering, kept wasting ingredients. The loan just delayed the inevitable â€” they were already losing money before the loan, and continued losing money after. When repayment hit there was nothing left.",
              "score": 2,
              "created_utc": "2026-02-18 10:09:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o60lkjg",
          "author": "secret_protoyipe",
          "text": "Got 26k after multiple attempts. Opus is a monster for getting 49k. That one human named \"Better\" got 57k, letting humans win... for a bit longer.\n\nhttps://preview.redd.it/v8lkhwc8j7kg1.png?width=407&format=png&auto=webp&s=71d47cce97a36448755d10544f2f6356a5d34459",
          "score": 1,
          "created_utc": "2026-02-18 07:43:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60z02k",
              "author": "Disastrous_Theme5906",
              "text": "27k is still a solid result. And yeah, that human \"Better\" is genuinely a monster â€” went through the simulation insanely fast. Probably some kind of economics prodigy or just has a lot of practice. But humanity still has a chance â€” for now.",
              "score": 3,
              "created_utc": "2026-02-18 09:48:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o60uql1",
          "author": "Kerbourgnec",
          "text": "Is there any reason for you to not just quit your job and let opus make you rich?",
          "score": 1,
          "created_utc": "2026-02-18 09:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60ywyp",
              "author": "Disastrous_Theme5906",
              "text": "Haha, models aren't quite ready for that yet. As complex as the simulation is, it's still far from real life.",
              "score": 2,
              "created_utc": "2026-02-18 09:47:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62b8ty",
          "author": "stumblinbear",
          "text": "I'm absolutely terrible at this game",
          "score": 1,
          "created_utc": "2026-02-18 15:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62l2au",
              "author": "Disastrous_Theme5906",
              "text": "Most people (and most AI models) price way too low at the start. \n\nTry raising prices to $15-18, hire staff early, and watch your \n\ncapacity â€” that's what separates the survivors. What's your score?",
              "score": 2,
              "created_utc": "2026-02-18 15:50:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o66g3jp",
                  "author": "stumblinbear",
                  "text": "Yeah pricing is what screwed me. It starts the prices so low that I assumed that was a sensible starting point, it took me six or seven days to figure out I can start raising them beyond what I personally thought was reasonable but then the UI glitched and I couldn't click on most of the buttons. I lost it when I refreshed, so eh",
                  "score": 1,
                  "created_utc": "2026-02-19 03:17:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62rlle",
          "author": "vicks9880",
          "text": "F**k!!! Thatâ€™s good idea!",
          "score": 1,
          "created_utc": "2026-02-18 16:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67g2s4",
          "author": "s4lome_",
          "text": "Can someone explain/show code on how to set up a model playing this?",
          "score": 1,
          "created_utc": "2026-02-19 07:46:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a3q8f",
          "author": "stavenhylia",
          "text": "Very interesting. Have you thought of a way to let people give local LLM's a go on this?",
          "score": 1,
          "created_utc": "2026-02-19 18:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m9tj2",
          "author": "m0j0m0j",
          "text": "Any plans to run Sonnet 4.6? People are curious!",
          "score": 1,
          "created_utc": "2026-02-21 16:02:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6viyf0",
          "author": "yaboyskales",
          "text": "Why didn't you gave me the $2k instead ðŸ¦Ž",
          "score": 1,
          "created_utc": "2026-02-23 01:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vgshc",
          "author": "sleepingsysadmin",
          "text": "Very interesting bench that only the best of the best can beat. ",
          "score": 0,
          "created_utc": "2026-02-17 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vlcz6",
              "author": "Disastrous_Theme5906",
              "text": "Thanks! Yeah only 4 out of 12 models survived. Humans should do better though â€” beating Opus is the real challenge.",
              "score": 3,
              "created_utc": "2026-02-17 15:12:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o61bujp",
                  "author": "Mickenfox",
                  "text": "You forget the biggest advantage humans have (for now):Â \nA human playing this 10 times will get better every time. Opus will not.Â ",
                  "score": 2,
                  "created_utc": "2026-02-18 11:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vvbxj",
                  "author": "metigue",
                  "text": "Try GLM-5 it was beating opus in the competitive version of vending bench.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:01:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5zixn9",
          "author": "Abject-Tomorrow-652",
          "text": "This is bait",
          "score": -2,
          "created_utc": "2026-02-18 02:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w3nwj",
          "author": "xatey93152",
          "text": "Another Claude marketing stunts",
          "score": -3,
          "created_utc": "2026-02-17 16:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5w76x2",
              "author": "Suitable-Name",
              "text": "Run it on Claude yourself and invalidate the results. If you think it's tailored to Claude, please explain how and why",
              "score": 7,
              "created_utc": "2026-02-17 17:01:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5xdjbd",
                  "author": "Little_Bumblebee6129",
                  "text": "Thats not how its done. If you want your model to pass some tests - you can train your model to do that. Either by giving it some examples of right answers (maybe gathered by human) OR by giving it possibility to fail many times and to use that experience to tune weights.\n\nI am not saying it is the case here. And i am not saying that it is not. Because i dont know",
                  "score": 2,
                  "created_utc": "2026-02-17 20:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5w8new",
              "author": "Disastrous_Theme5906",
              "text": "No magic here â€” Opus accounts for most simulation factors but not all. If you check the oracle score on the landing page, Opus hits around 40% of the theoretical maximum. Decent but not superhuman. Detailed breakdown of its strategy coming soon on r/ClaudeAI/. What actually surprised me was how much Sonnet struggled, and Haiku was a complete disaster.",
              "score": 3,
              "created_utc": "2026-02-17 17:08:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5wop3m",
          "author": "No-Abbreviations7266",
          "text": "I don't think there has been 30 days since opus 4.6 release, can you clarify about that",
          "score": -7,
          "created_utc": "2026-02-17 18:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5wp2jg",
              "author": "Disastrous_Theme5906",
              "text": "oh haha no â€” the \"30 days\" are simulated days inside the benchmark engine, not real time. the whole 30-day simulation runs in about 10-15 minutes. each model gets a fresh food truck, $2,000 starting cash, and makes decisions day by day â€” choosing locations, setting menus, ordering ingredients, etc. the engine crunches demand, weather, competition, and spits out results. try the play mode yourself at [foodtruckbench.com/play](http://foodtruckbench.com/play), you'll see how it works in like 2 minutes",
              "score": 2,
              "created_utc": "2026-02-17 18:25:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r656d7",
      "title": "Qwen3.5-397B-A17B is out!!",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/",
      "author": "lolxdmainkaisemaanlu",
      "created_utc": "2026-02-16 09:29:03",
      "score": 808,
      "num_comments": 154,
      "upvote_ratio": 0.98,
      "text": "[https://huggingface.co/Qwen/Qwen3.5-397B-A17B](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o5ntrdu",
          "author": "iKy1e",
          "text": "This sounds really exciting:\n\n> The decoding throughput of Qwen3.5-397B-A17B is 3.5x/7.2 times that of Qwen3-235B-A22B",
          "score": 100,
          "created_utc": "2026-02-16 10:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nu8ad",
              "author": "lolxdmainkaisemaanlu",
              "text": "Damn that's crazy, qwen team always raising the bar!!",
              "score": 41,
              "created_utc": "2026-02-16 10:36:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5oqzh2",
                  "author": "sannysanoff",
                  "text": "maybe, maybe, but i see 39 tokens / second on openrouter on its native provider.",
                  "score": 20,
                  "created_utc": "2026-02-16 14:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5pllk4",
              "author": "power97992",
              "text": "YEah it is faster but it seems  to be worse than qwen 3 vl 235b. ... ",
              "score": 2,
              "created_utc": "2026-02-16 16:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q4ze5",
                  "author": "LevianMcBirdo",
                  "text": "Just feeling wise or do you have a benchmark? Just interested, not critiquing.",
                  "score": 5,
                  "created_utc": "2026-02-16 18:22:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5np9q0",
          "author": "cantgetthistowork",
          "text": "Anyone tested?\n\nContext Length: 262,144 natively and extensible up to 1,010,000 tokens.",
          "score": 99,
          "created_utc": "2026-02-16 09:50:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ogkqh",
          "author": "r4in311",
          "text": "I tested the OCR capabilities. This is by far the best open image model: very close to Gemini 3 and beating every single open-source solution. Converting handwritten notes with hand-drawn graphics to Markdown is the real challenge, and thatâ€™s exactly where it shows its edge over the competition. Image understanding is key for many OCR tasks. Thereâ€™s simply no comparison to any other open model at the moment. You see tons of small OCR models, basically one or two are released a week, but NONE of those can deal with images, let alone handwriting properly.",
          "score": 65,
          "created_utc": "2026-02-16 13:27:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5os01n",
              "author": "lolzinventor",
              "text": "I agree.  Just decoded some 18th century text, and its clever enough to resolve all the archaic abbreviations and put it all into context.  ",
              "score": 23,
              "created_utc": "2026-02-16 14:29:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5orfos",
              "author": "varlog0",
              "text": "How is it compared to qwen vl?",
              "score": 8,
              "created_utc": "2026-02-16 14:26:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5otps8",
                  "author": "r4in311",
                  "text": "No comparison whatsoever. Qwen VL is useless for these tasks.",
                  "score": 14,
                  "created_utc": "2026-02-16 14:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5qgod0",
                  "author": "Less_Sandwich6926",
                  "text": "best small model for OCR is Chandra-OCR-Q8\\_0.gguf",
                  "score": 6,
                  "created_utc": "2026-02-16 19:16:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nnww5",
          "author": "TinMorphling",
          "text": "Finally! Happy new year!",
          "score": 115,
          "created_utc": "2026-02-16 09:37:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nq9ln",
          "author": "Nobby_Binks",
          "text": "Awesome, right in the usability sweet spot for my rig, GLM5 is just a tad too big",
          "score": 30,
          "created_utc": "2026-02-16 09:59:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pmdsu",
              "author": "lastingk",
              "text": "what kind of rig you have damn",
              "score": 13,
              "created_utc": "2026-02-16 16:56:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q5ox3",
                  "author": "overand",
                  "text": "If you go for an older system with DDR4 ram, you can get a pair of 32 GB sticks for \"only\" $300 or so - so you can get to 128 GB of system ram for \"only\" $600. (*Much* cheaper than e.g. a mac mini or a DDR5 system.). And, it's an A35B, so your 35B active parameters might fit decently in a 16 GB card depending on your quantization. (At some Q2 it would be around 12 GB)",
                  "score": 2,
                  "created_utc": "2026-02-16 18:26:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5s4qym",
                  "author": "Nobby_Binks",
                  "text": "Yeah its an old EPYC Rome with 256GB DDR4 and 128GB of vram via a few random gpus. tbf GLM5 runs pretty good at Q3 but I always have doubts about such a low quant.",
                  "score": 1,
                  "created_utc": "2026-02-17 00:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nnf5m",
          "author": "bobeeeeeeeee8964",
          "text": "Also the gguf https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF",
          "score": 148,
          "created_utc": "2026-02-16 09:32:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5noefy",
              "author": "TheTerrasque",
              "text": "GGUF WH... oh. Well that's neat.",
              "score": 75,
              "created_utc": "2026-02-16 09:41:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5p17f6",
                  "author": "The_frozen_one",
                  "text": "Just need to do a little rm -rf here and a little rm -rf there and... I can store... 2 of the files.",
                  "score": 16,
                  "created_utc": "2026-02-16 15:17:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nnodz",
              "author": "danielhanchen",
              "text": "Was just about to link this! :)",
              "score": 28,
              "created_utc": "2026-02-16 09:34:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5opggw",
              "author": "AcePilot01",
              "text": "Yeah if you can fit the 2bit for 148gb lmfoa",
              "score": 7,
              "created_utc": "2026-02-16 14:16:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q3b10",
                  "author": "overand",
                  "text": "I wonder just how well this will run on a 128 GB of DDR4 ram system with two 3090s. My guess is \"usably, but kinda not awesome.\" Stuff like a 262,144 context window might take about 90 minutes to get through when it's full, if prompt-processing is akin to some other biggish MOE models I've run at ~50 t/s on the prompt processing side.",
                  "score": 2,
                  "created_utc": "2026-02-16 18:15:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qfrkz",
              "author": "Standard-Drive7273",
              "text": "Is that same model Alibaba runs for it's ChatGPT competitor? Or that's a model with much more than 397b?",
              "score": 0,
              "created_utc": "2026-02-16 19:12:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nngko",
          "author": "Responsible-Stock462",
          "text": "Okay I need more Ram..... ðŸ«£",
          "score": 87,
          "created_utc": "2026-02-16 09:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nnwz5",
              "author": "bobeeeeeeeee8964",
              "text": "There will be a smaller version",
              "score": 36,
              "created_utc": "2026-02-16 09:37:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nqf6z",
                  "author": "Sensitive_Song4219",
                  "text": "Waiting on an a3b-30b equivalent! :-D",
                  "score": 32,
                  "created_utc": "2026-02-16 10:00:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5npduw",
                  "author": "Responsible-Stock462",
                  "text": "Small version always dumb. ðŸ˜\nBigger is better. Yeah 400b is massive. \nShould have known las January, when ram was cheap.",
                  "score": 1,
                  "created_utc": "2026-02-16 09:51:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5ntrqy",
              "author": "Ok_Top9254",
              "text": "Vram is actually cheaper weirdly enough than ram. 24GB Tesla P40s are old and slow but still faster than single 16GB DDR5 stick (and cheaper per GB). 8x24GB you have 192GB and can run the Q3 model for about 1600$ in gpus.",
              "score": 15,
              "created_utc": "2026-02-16 10:32:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ox5mp",
                  "author": "pmp22",
                  "text": "Only do this if you love jank. Source: I love jank.",
                  "score": 21,
                  "created_utc": "2026-02-16 14:56:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oj6l2",
                  "author": "Tai9ch",
                  "text": "That's amusing, but once you start to consider the support hardware it takes to have more than about 3 GPUs and the power costs it's not obviously that good a deal.",
                  "score": 12,
                  "created_utc": "2026-02-16 13:42:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5nu55f",
                  "author": "Responsible-Stock462",
                  "text": "The question is: Can I mix P40 with my two Blackwell cards? Or will I get rubbish due to rounding errors?",
                  "score": 6,
                  "created_utc": "2026-02-16 10:35:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5pvq5i",
                  "author": "skrshawk",
                  "text": "Once you add the janky rig or jet turbine of a rackmount chassis and all the other components, not to mention probably electrical upgrades because you'll need at least two dedicated circuits to run the thing.  And the A/C bill if you're not running it in winter or underground, yeah that thing will become a loud annoyance fast.\n\nWorth it for the right use-case and if the model is damn near perfect at that quant, or if you have money to burn, but a lot more to consider here than just the GPUs.",
                  "score": 2,
                  "created_utc": "2026-02-16 17:40:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5no786",
              "author": "jakspedicey",
              "text": "How much ram ðŸ¤”",
              "score": 8,
              "created_utc": "2026-02-16 09:40:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nqdko",
                  "author": "Expensive-Paint-9490",
                  "text": "807 GB for FP16. \n\n214 GB for UD-Q4\\_K\\_XL.",
                  "score": 33,
                  "created_utc": "2026-02-16 10:00:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5np8tl",
                  "author": "Responsible-Stock462",
                  "text": "My Threadripper has 64GB. I think 256GB would be sufficient+ two rtx 5060ti",
                  "score": 5,
                  "created_utc": "2026-02-16 09:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5r93d8",
              "author": "Umbaretz",
              "text": "Can you run it with offload of layers?",
              "score": 1,
              "created_utc": "2026-02-16 21:35:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nswfm",
          "author": "FullOf_Bad_Ideas",
          "text": "nice, I built a rig for GLM 4.7 and GLM 5 was too big for me. This should fit just right.",
          "score": 15,
          "created_utc": "2026-02-16 10:23:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nolel",
          "author": "Significant_Fig_7581",
          "text": "Finally!!!! Waiting for 9B...",
          "score": 36,
          "created_utc": "2026-02-16 09:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rjbqt",
              "author": "charles25565",
              "text": "Judging by the release schedule Qwen3 had, it would take 3 months or so. Hopefully not.",
              "score": 2,
              "created_utc": "2026-02-16 22:26:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nsz29",
          "author": "Few_Painter_5588",
          "text": "Was there a mistake in the API pricing?\n\nhttps://preview.redd.it/u0q7kp7c2ujg1.png?width=2144&format=png&auto=webp&s=bd7e219bc4cbab35bef7476ead2e98747b1819d4\n\nWhy's the plus model cheaper than the open weights model?  \n",
          "score": 19,
          "created_utc": "2026-02-16 10:24:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ont1j",
              "author": "NickCanCode",
              "text": "That one on the top is just the initial price. if token count reach certain size, that price will increase.\n\nhttps://preview.redd.it/9qjgrle56vjg1.png?width=998&format=png&auto=webp&s=181d084395266814b86b26bce14626ce018a8793\n\nThe 2nd model seems twice as fast too.",
              "score": 1,
              "created_utc": "2026-02-16 14:07:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pn6t1",
                  "author": "Samy_Horny",
                  "text": "its thinking is faster than before, although it's true that it no longer writes a whole mega-paragraph and its type of thinking seems more like Gemini or GPT-5",
                  "score": 1,
                  "created_utc": "2026-02-16 17:00:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nqq8q",
          "author": "ilintar",
          "text": "Oof, that's a big one.",
          "score": 6,
          "created_utc": "2026-02-16 10:03:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5phuv5",
          "author": "Far-Low-4705",
          "text": "smaller models when :')\n\nI wish they'd just release them all at the same time",
          "score": 6,
          "created_utc": "2026-02-16 16:35:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pmn53",
              "author": "Samy_Horny",
              "text": "I believe the Chinese New Year is a week-long celebration, meaning the rest will be released throughout the week.",
              "score": 3,
              "created_utc": "2026-02-16 16:57:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sgjjs",
                  "author": "Far-Low-4705",
                  "text": "Damn alright, the wait continuesâ€¦\n\nReeeaally hoping for 80b lol",
                  "score": 1,
                  "created_utc": "2026-02-17 01:34:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5podrj",
          "author": "Rollingsound514",
          "text": "Failed a test of extracting json from a pdf that Sonnet 4.5 nails every time I've run it (dozens of times). Not hating, just mentioning it, I want it to work :(",
          "score": 6,
          "created_utc": "2026-02-16 17:05:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q610w",
              "author": "Unique_Marsupial_556",
              "text": "what quant?",
              "score": 1,
              "created_utc": "2026-02-16 18:27:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q649p",
                  "author": "Rollingsound514",
                  "text": "Full, I used their chat",
                  "score": 1,
                  "created_utc": "2026-02-16 18:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5p6n8i",
          "author": "kawaii_karthus",
          "text": "\\*cries in 128gb ram\\*",
          "score": 5,
          "created_utc": "2026-02-16 15:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pgxjf",
          "author": "SufficientPie",
          "text": "Neat! This is the first open-weights model to get all 6 of my personal benchmark trick questions correct.  The only other models that got them all correct are gemini 2.5 and 3.\n\n(Though using it through OpenRouter, about half of the AI's tool calls are invalid, either to tools that don't exist or putting the tool call into a code block.  So that's a problem.)",
          "score": 6,
          "created_utc": "2026-02-16 16:31:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5raz56",
              "author": "ConversationFun940",
              "text": "Care to share those trick questions pls?",
              "score": 1,
              "created_utc": "2026-02-16 21:44:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sjqo1",
                  "author": "SufficientPie",
                  "text": "Nice try, OpenAI engineers.\n\n(jk but no, I don't want them in training data.  3 of them sound very similar to common trick questions but actually aren't, which confuses AIs that assume it's the trick question.  1 asks for an example of something impossible in an obscure subject area.  1 asks if we can rule out a numerical scenario that is highly improbable but nevertheless possible.  1 asks for dimensions of a certain 3D object with a certain 3D shape that trips up AIs that can't visualize things.)",
                  "score": 4,
                  "created_utc": "2026-02-17 01:53:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5p41gg",
          "author": "CanineAssBandit",
          "text": "Magnum fine tune when\n\nSo far it fails the vibe check. confidently dumber than GLM 4.7, and burned 1k tokens on a safety guidelines loop figuring out if it was allowed to answer \"How do I make an ERP fine tune using my 6m token dataset,\" which is obviously a technical question, not a request for explicit content.",
          "score": 8,
          "created_utc": "2026-02-16 15:31:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ppi8s",
              "author": "pm_me_tits",
              "text": "It all depends if you're asking for Enterprise Resource Planning or... Erotic Role Play.",
              "score": 4,
              "created_utc": "2026-02-16 17:11:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nu7qn",
          "author": "R_Duncan",
          "text": "Gated delta network like qwen3-x-Next",
          "score": 3,
          "created_utc": "2026-02-16 10:36:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nuqwk",
          "author": "power97992",
          "text": "Unbelievable ds v4 is not out yet, are they still trying to finetune it?",
          "score": 5,
          "created_utc": "2026-02-16 10:41:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nu87c",
          "author": "jnk_str",
          "text": "No FP8?",
          "score": 3,
          "created_utc": "2026-02-16 10:36:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5os5fa",
              "author": "Much-Farmer-2752",
              "text": "[https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF)",
              "score": 3,
              "created_utc": "2026-02-16 14:30:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pn6m2",
          "author": "suicidaleggroll",
          "text": "Nice, the Unsloth UD-Q4 version seems to be working well for me.  It's slower than Qwen3-235B-A22B, but that's because it's so much larger that I have to offload more to the CPU.  Still not a huge effect though, ~35 tg on 235B vs ~32 on 397B.  That's on an EPYC with a single RTX Pro 6000.\n\nQuality seems excellent so far",
          "score": 3,
          "created_utc": "2026-02-16 17:00:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qxvyd",
              "author": "NoahFect",
              "text": "What params are you running with?",
              "score": 1,
              "created_utc": "2026-02-16 20:40:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r930i",
                  "author": "suicidaleggroll",
                  "text": "Nothing special\n\n    cmd: |\n    Â Â Â Â Â ${llama-server}\n    Â Â Â Â Â --model /models/Qwen3.5-397B-A17B-UD-Q4_K_XL-00001-of-00006.gguf\n    Â Â Â Â Â --temp 0.6\n    Â Â Â Â Â --min-p 0.0\n    Â Â Â Â Â --top-p 0.95\n    Â Â Â Â Â --top-k 20\n    Â Â Â Â Â --ctx-size 16384\n    Â Â Â Â Â --n-gpu-layers 99\n    Â Â Â Â Â --n-cpu-moe 35\n    Â Â Â Â Â --batch-size 2048\n    Â Â Â Â Â --ubatch-size 2048",
                  "score": 3,
                  "created_utc": "2026-02-16 21:35:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nnvrv",
          "author": "United-Manner-7",
          "text": "Ah, more information would be great\nHowever, I personally tested the model, and to be honest, itâ€™s a pity that it still produces artifacts in the form of Chinese characters, overall the model is good considering that it is universal",
          "score": 12,
          "created_utc": "2026-02-16 09:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ntwlt",
          "author": "notdba",
          "text": "Almost the same size as Llama 4 Maverick, not sure if done on purpose ðŸ˜„",
          "score": 5,
          "created_utc": "2026-02-16 10:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omn6u",
          "author": "Dany0",
          "text": "Qwen 3.5 coder wen",
          "score": 2,
          "created_utc": "2026-02-16 14:01:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ork37",
          "author": "lolwutdo",
          "text": "That size will be unusable if the model still yaps as long as the other qwen models",
          "score": 2,
          "created_utc": "2026-02-16 14:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d6t7i",
          "author": "Accomplished_Fixx",
          "text": "I tried OCR for a long Arabic text and it did not have a single mistake. No other model succeeded with this. Amazing!",
          "score": 2,
          "created_utc": "2026-02-20 04:11:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5omiv8",
          "author": "LoveMind_AI",
          "text": "This model absolutely destroys GLM-5 and MiniMax M2.5 for the creative writing/relational stuff that I work on.",
          "score": 2,
          "created_utc": "2026-02-16 14:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pgd6z",
              "author": "stereo16",
              "text": "M2.5 is good for creative writing?",
              "score": 1,
              "created_utc": "2026-02-16 16:29:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pjqpz",
                  "author": "LoveMind_AI",
                  "text": "Not in my opinion. I think M2 was significantly better.",
                  "score": 1,
                  "created_utc": "2026-02-16 16:44:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nqg0r",
          "author": "peglegsmeg",
          "text": "Noob question, when I look at these models is there anything in the name to suggest what kind of hardware is needed?\n\n\nMacBook M1 Max 64Gb\n\n\nEdit: wow thanks for all this, got plenty to read up on",
          "score": 5,
          "created_utc": "2026-02-16 10:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ns8cj",
              "author": "AbstrusSchatten",
              "text": "The parameter count and the precision. As a rule of thumb you can calculate that a model with 400b parameters will be 800gb in BF16, then half of that for Q8 so 400gb and once again half of that for Q4 so 200gb. Of course it's not exactly precise but a good way to have a rough estimate :)",
              "score": 15,
              "created_utc": "2026-02-16 10:17:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qbfwc",
                  "author": "some_user_2021",
                  "text": "Don't forget about the context!",
                  "score": 3,
                  "created_utc": "2026-02-16 18:52:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nszzc",
              "author": "PurpleWinterDawn",
              "text": "The quality, amount of parameters and activated parameters are the metrics you should focus on.\n\nThe weight of the model is roughly a function of quality \\* parameters. Say, for an 8B, or 8-billion parameters dense model:\n\n* at Q8\\_0 (8 bits per weight, or bpw), it will be 8GB ;\n* at FP16/BF16, it will be 16GB ;\n* at Q4\\_K\\_M (roughly 4.5 bpw), you can find them in the 4.5GB range.\n\nThat's the amount of VRAM and/or RAM you'll need. Do note that dense models used to generate tokens on CPU is slooooooooooow.\n\nSparse models (Mixture of Experts, or MoE) have a number of \"activated\" parameters. If this number is low enough, CPU-only token generation will be doable, and by keeping the Experts in RAM it will allow using both your VRAM (for prompt processing) and your RAM (for token generation). For instance, Qwen3-30b-a3b at Q4\\_K\\_M can run with 8GB of VRAM and 32GB of RAM with llama.cpp if you give it the parameter --cpu-moe. The lighter, mobile-oriented LFM2-8B-A1B model at Q4\\_K\\_M will fit entirely in 8GB of VRAM, with its full 32k tokens context window which (IIRC) weighs in at 440MB.\n\nDo note that the context window also takes memory. Unfortunately, I don't have a clear picture of what model leads to what context window memory footprint.\n\nThe hardware you'll need will depend on the models you want to run, memory size and bandwidth being the most meaningful factors at the moment.",
              "score": 13,
              "created_utc": "2026-02-16 10:24:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5prdos",
                  "author": "shveddy",
                  "text": "Ok, so you leave the experts in ram and generate tokens with CPU, but then use the GPU for prompt processing? \n\nThatâ€™s plain enough English, but whatâ€™s going on with the weights in this scenario? Iâ€™m trying to build a mental model of how this all works. \n\nIs prompt processing much heavier than generating tokens and therefore you want to use the GPU on it?\n\nAre there dedicated parameters and layers that you know will always be used only for prompt processing, so you can dump those onto the GPU and leave the there?\n\nIs it not possible to transfer over just the 17b active parameters over to the GPU once the model decides which parameters should be activated for a given query, and then run the there? \n\n(For context I just got my RTX pro 6000 today and I have 512gb of ddr5 on a 24 core threadripper, so I figure I might be able to run this at fp8, but Iâ€™m unsure about the best setup)",
                  "score": 1,
                  "created_utc": "2026-02-16 17:19:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nsbk4",
              "author": "ELPascalito",
              "text": "It's ~400B parameters meaning you need a lot of memory, ~800GB for full precision,Â ~220GB for a 4bit quant, not easy to run, you'll need a lot of ram to even run this with a sufficient amount of contextÂ ",
              "score": 7,
              "created_utc": "2026-02-16 10:18:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nsufs",
              "author": "FullOf_Bad_Ideas",
              "text": "look at the total parameter size. 397B means it will be around 240GB at Q4. You can run up to around 100B with 64GB of memory since they'd be around 50-64GB when quantized.",
              "score": 3,
              "created_utc": "2026-02-16 10:23:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ofott",
              "author": "MaxKruse96",
              "text": "Look at the filesize. You need more FREE/AVAILABLE Memory than the filesize.",
              "score": 2,
              "created_utc": "2026-02-16 13:22:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nslwg",
              "author": "PraxisOG",
              "text": "Running a 397 billion parameter model at full precision(q8) requires 397 billion bytes of ram, or 397gb. You can get away with running the model in half precision with minimal quality loss, and at q4 this model would likely need half that, around 199gb to load. Keep in mind this is before context, so to run this model at full precision with plenty of context requires ~500gb ram.Â ",
              "score": 4,
              "created_utc": "2026-02-16 10:21:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pep50",
              "author": "beryugyo619",
              "text": "397B = 397GB in Q8+ KV cache  \nA17B = \"experts\" are 17GB each in Q8  \n\nso 200GB total with ideally more than 8.5GB VRAM per GPU before caches at most often preferred Q4 quants  \n\nso like 3x 96GB Blackwell or 1x Mac Studio 256GB or dozen P40s in the basement or setups like that",
              "score": 1,
              "created_utc": "2026-02-16 16:21:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nurlp",
          "author": "No_Afternoon_4260",
          "text": "Multipost, consolidating this one:\nhttps://www.reddit.com/r/LocalLLaMA/s/3Z7KsuKYqC",
          "score": 2,
          "created_utc": "2026-02-16 10:41:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ojue3",
          "author": "power97992",
          "text": "I tried plus Â and the Â normal version , it seems to be bench maxed .. Glm 5 seems to be better than it , even qwen 3 vl is better than itâ€¦ Â but it is fast though. it seems like minimax and Â qwen rushed their releases..Â ",
          "score": 2,
          "created_utc": "2026-02-16 13:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oh8hz",
          "author": "guiopen",
          "text": "I don't exactly understand the difference between the plus and the open weight, it's only the context length? They use something like yarn or it's actually a different model?",
          "score": 1,
          "created_utc": "2026-02-16 13:31:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5okmcz",
              "author": "madaradess007",
              "text": "my 30min of testing shows qwen3.5-plus is worse than open weights one  \ni didn't tweak the prompts much, so most likely a skill issue",
              "score": 2,
              "created_utc": "2026-02-16 13:50:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pmymh",
              "author": "Samy_Horny",
              "text": "It's officially confirmed that the Plus version is basically the same model, with the difference being that the Plus version has smart tool call and 1M context.",
              "score": 1,
              "created_utc": "2026-02-16 16:59:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ojt65",
          "author": "madaradess007",
          "text": "my prompts work better with Qwen3.5-397B-A17B, rather than Qwen3.5-plus",
          "score": 1,
          "created_utc": "2026-02-16 13:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p4hzh",
          "author": "DragonfruitIll660",
          "text": "Is anyone having issues with it outputting 1 tokens? Updated to the latest Llama.cpp and rebuilt it, under like 1200 starting context works fine but anything longer seems to cause a 1 token empty output. Curious if anyone else has seen that before/knows a fix. Using a super simple command to reduce potential issues\n\n./build/bin/llama-server \\\\\n\n\\-m \"/media/win\\_os/Models/Qwen3.5Q4/Qwen3.5-397B-A17B-UD-Q4\\_K\\_XL-00001-of-00006.gguf\" \\\\\n\n\\-ngl 999 \\\\\n\n\\--n-cpu-moe 99 \\\\\n\n\\-c 26000",
          "score": 1,
          "created_utc": "2026-02-16 15:33:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p5ajf",
          "author": "Aaaaaaaaaeeeee",
          "text": "On chat.qwen.ai, I tried out video interpretation \"Suika Game Planet â€“ Nintendo Direct 9.12.2025\" 480p\n\n\nÂ prompt with no hints: \"Make a game exactly like shown in the video, in a single HTML file.\"\n\n\nA few rerolls and I still haven't seen it use planetary gravity, I was hoping it would pick that up but it makes standard suika. you can do planetary with multishot or specific prompting.Â ",
          "score": 1,
          "created_utc": "2026-02-16 15:37:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q17vn",
          "author": "mechanistics",
          "text": "Big model go brrr",
          "score": 1,
          "created_utc": "2026-02-16 18:05:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qgfzq",
          "author": "Less_Sandwich6926",
          "text": "Anyone tested with mac m3 ultra ?",
          "score": 1,
          "created_utc": "2026-02-16 19:15:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tjkgr",
              "author": "Hoodfu",
              "text": "Looks like lm studio doesn't support the gguf or mlx version yet, so I'm waiting on that.",
              "score": 2,
              "created_utc": "2026-02-17 05:52:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qjd24",
          "author": "Icy_Annual_9954",
          "text": "Which Hardware do I need to run? Any stats?",
          "score": 1,
          "created_utc": "2026-02-16 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qm4nb",
          "author": "Fault23",
          "text": "New open-source finetuner just dropped",
          "score": 1,
          "created_utc": "2026-02-16 19:43:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5raqw7",
          "author": "swagonflyyyy",
          "text": "Assuming the rumors are true, I really do wonder if qwen3.5-35b performs anywhere near gpt-oss-120b. \n\nProbably not but one can dream!",
          "score": 1,
          "created_utc": "2026-02-16 21:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t48a0",
          "author": "bene_42069",
          "text": "I hope they're not abandoning the small-medium model space",
          "score": 1,
          "created_utc": "2026-02-17 04:00:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uggtt",
          "author": "According-Garlic898",
          "text": "How to use it locally ? Required vram",
          "score": 1,
          "created_utc": "2026-02-17 10:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5znhs4",
          "author": "CockBrother",
          "text": "None of the nvfp4 posted work with vllm yet. The mlx-community/Qwen3.5-397B-A17B-nvfp4 model tokenizer doesn't work for it. The vincentzed-hf/Qwen3.5-397B-A17B-NVFP4 one created by TensorRT Model Optimizer has dimensions vllm doesn't like.",
          "score": 1,
          "created_utc": "2026-02-18 03:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5p01s5",
          "author": "nebulaidigital",
          "text": "Huge model drops are exciting, but the useful discussion is always: what actually changed for users? If youâ€™ve tried Qwen3.5-397B-A17B, Iâ€™d love to hear (1) best prompt styles vs prior Qwen, (2) how it behaves at lower quantization (does it keep instruction-following or collapse into verbosity), and (3) any concrete evals you ran beyond â€œfeels smartâ€ (MMLU-style, coding, long-context retrieval, tool use). Also curious about licensing and whether the weights are truly practical for self-hosting, or if the real win is distilled/finetuned variants.",
          "score": 2,
          "created_utc": "2026-02-16 15:11:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ou6ol",
          "author": "Specter_Origin",
          "text": "It sure likes tokens,  I asked the old question of counting characters in intentionally misspelled word, it consumed \"2,976\" tokens most of the thinking of course xD",
          "score": 1,
          "created_utc": "2026-02-16 14:41:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5phwdt",
              "author": "SufficientPie",
              "text": "It sure does burn through thinking tokens",
              "score": 1,
              "created_utc": "2026-02-16 16:36:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p5byd",
          "author": "Big_River_",
          "text": "ok thank goodness I can it on my 4090! i was worried it was to be way too big for my blessed sliver of 24gb vram! rejoice",
          "score": 1,
          "created_utc": "2026-02-16 15:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qjf00",
          "author": "BigBoiii_Jones",
          "text": "Open source AI has been killing it this last year making closed models not that far ahead if at all.",
          "score": 1,
          "created_utc": "2026-02-16 19:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ov804",
          "author": "Witty_Arugula_5601",
          "text": "I am both excited and saddened that itâ€™s Chinese firms competing against other Chinese firms",
          "score": -4,
          "created_utc": "2026-02-16 14:46:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oi6s5",
          "author": "Birdinhandandbush",
          "text": "\"local\"",
          "score": -4,
          "created_utc": "2026-02-16 13:36:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbkeea",
      "title": "Which one are you waiting for more: 9B or 35B?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/jyvany3jf1lg1.png",
      "author": "jacek2023",
      "created_utc": "2026-02-22 12:15:48",
      "score": 722,
      "num_comments": 180,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6rx9fa",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-22 14:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rdwnh",
          "author": "Significant_Fig_7581",
          "text": "Honestly both!\n\nA 60B model would also be ðŸ”¥ðŸ”¥ðŸ”¥",
          "score": 114,
          "created_utc": "2026-02-22 12:18:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6riukk",
              "author": "jinnyjuice",
              "text": "Anything that fits in 100GB memory including 100k+ tokens!",
              "score": 53,
              "created_utc": "2026-02-22 12:56:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tv9lb",
                  "author": "insmek",
                  "text": "Give me the best model that I can use on my 128GB MacBook or give me death.",
                  "score": 17,
                  "created_utc": "2026-02-22 19:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sfkcg",
              "author": "Iory1998",
              "text": "Why not 80B? It works well on many consumer hardware.",
              "score": 18,
              "created_utc": "2026-02-22 15:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u61qs",
                  "author": "Fresh_Finance9065",
                  "text": "Something for the 8gb vram + 32gb ram systems would be nice. 60B is that nice size",
                  "score": 5,
                  "created_utc": "2026-02-22 20:49:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6sh4xl",
                  "author": "Significant_Fig_7581",
                  "text": "Oh I use the Qwen3 Coder Next all the time, But I don't think they'll give us another 80B in the same month...",
                  "score": 9,
                  "created_utc": "2026-02-22 16:02:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s5nze",
              "author": "Far-Low-4705",
              "text": "I think itâ€™s more likely fo an 80b model\n\nBut Iâ€™m reeeeally hoping we do get an 80b",
              "score": 5,
              "created_utc": "2026-02-22 15:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s6d5g",
                  "author": "Significant_Fig_7581",
                  "text": "Didn't we just get an 80B? But you're right I also wanna see another one lol",
                  "score": 2,
                  "created_utc": "2026-02-22 15:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rdw9z",
          "author": "Paramecium_caudatum_",
          "text": "Qwhen gguf /s",
          "score": 97,
          "created_utc": "2026-02-22 12:18:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rgfwj",
              "author": "jacek2023",
              "text": "Llama.cpp support for Owen 3.5 has been merged many days ago",
              "score": 46,
              "created_utc": "2026-02-22 12:39:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6rhljq",
                  "author": "Paramecium_caudatum_",
                  "text": "Thank you for your reply!",
                  "score": 15,
                  "created_utc": "2026-02-22 12:47:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rgcmm",
          "author": "dances_with_gnomes",
          "text": "I might be able to run 9B. No way I can run 35B.",
          "score": 92,
          "created_utc": "2026-02-22 12:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tzwld",
              "author": "Initial-Argument2523",
              "text": "Hope we get a new 4B dense for this reason",
              "score": 10,
              "created_utc": "2026-02-22 20:18:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rzmul",
              "author": "Straight_Abrocoma321",
              "text": "Same",
              "score": 7,
              "created_utc": "2026-02-22 14:38:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6t4mp0",
              "author": "sloth_cowboy",
              "text": "Specs? I don't have any input, just curious.",
              "score": 2,
              "created_utc": "2026-02-22 17:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tj99t",
                  "author": "dances_with_gnomes",
                  "text": "GeForce GTX 1660 Ti with 6 gb vram and 16 gb of RAM. Ryzen 7 2700X if that matters, I honestly don't know much about these!",
                  "score": 7,
                  "created_utc": "2026-02-22 18:56:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6redyb",
          "author": "Kat-",
          "text": "70B-A3B",
          "score": 40,
          "created_utc": "2026-02-22 12:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rfcd6",
              "author": "social_tech_10",
              "text": "Have you tried Qwen3-Next-80B-A3B or Qwen3-Coder-Next-80B-A3B with linear attention?  The whole model fits in 24GB VRAM, and runs at ~50 t/s on my PC, so even if part of it spills over into RAM, it will probably still be fast enough to be very usable.",
              "score": 7,
              "created_utc": "2026-02-22 12:30:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6sam4d",
                  "author": "cristoper",
                  "text": ">  The whole model fits in 24GB VRAM\n\nHow do you fit an 80b model in 24GB? what quant do you run?",
                  "score": 14,
                  "created_utc": "2026-02-22 15:33:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rgn52",
                  "author": "Firm_Meeting6350",
                  "text": "Qwen3-Next-80B-A3BÂ is amazing - still I wonder how a potential Qwen3.5-Next-80B would perform :D",
                  "score": 9,
                  "created_utc": "2026-02-22 12:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6s5sid",
              "author": "Far-Low-4705",
              "text": "70b??\n\nWhy not 80b",
              "score": 2,
              "created_utc": "2026-02-22 15:10:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s78qw",
                  "author": "Kat-",
                  "text": "Oh, yeah, that too.",
                  "score": 2,
                  "created_utc": "2026-02-22 15:17:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rjkjh",
          "author": "peregrinefalco9",
          "text": "9B all day. The 35B models are impressive but the hardware requirements put them out of reach for most people running local. A genuinely good 9B that fits in 8GB VRAM would change more workflows than another 35B that needs a 3090.",
          "score": 70,
          "created_utc": "2026-02-22 13:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ruaya",
              "author": "Daniel_H212",
              "text": "if the 35B is a sparse MoE then its well within reach of anyone with 32 GB of RAM or more.",
              "score": 39,
              "created_utc": "2026-02-22 14:08:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tdm05",
                  "author": "hakanavgin",
                  "text": "Yeah that is exactly what op said, it puts them out of reach for most people running local. Most people use 16 GB RAM these days, even then with Windows, background apps and kv cache, you get no more than 4-6 gigabytes for running models.\n\nI've got 16 gigs of VRAM and 16 gigs of RAM so I consider myself above average for total amount of fast memory, and can't run anything more than 14B at 32-48k ctx@q4_k_m at any usable speeds and comfortable memory usage. Most people overestimate the \"average guy\" these days.",
                  "score": 3,
                  "created_utc": "2026-02-22 18:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rtkw8",
              "author": "EnthropicBeing",
              "text": "Are you implementing ~9B models in any workflow nowadays? I'm genuinely interested since I'm a total amateur and couldn't find any use for them.",
              "score": 11,
              "created_utc": "2026-02-22 14:04:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u34hk",
                  "author": "andy2na",
                  "text": "I keep qwen3-vl:4b iq4 in vram for Frigate image analyzing, home assistant voice assistant, karakeep, open-notebook, and general questions and it works great. For more complicated tasks like Sure Finance to analyze my finances, I'll temporarily load in qwen3-vl:8b-instruct-q4_K_M. Looking forward to 3.5:9b to compare",
                  "score": 8,
                  "created_utc": "2026-02-22 20:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6t0oib",
              "author": "Tai9ch",
              "text": "Do you have a gaming machine that you're running some AI on, or have you intentionally built towards running AI models?\n\nBecause yes, reasonable gaming setups tend  to max out at a single 16GB GPU, which makes a 30-35B model kind of crap.\n\nBut as soon as you're buying hardware to run AI models, options like Strix Halo or 2x 3090's exist, and at that point 35B (or even 80B) becomes entirely feasible.",
              "score": 1,
              "created_utc": "2026-02-22 17:30:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vyjla",
                  "author": "Thunderstarer",
                  "text": "On my 16GB RX 9060 XT, Qwen3Coder 30B A3B _just barely_ fits at IQ_3_K_XL with 30K context, and I can get a decently useable inference speed of 115T/s. If I throw my 8GB RX 480 in, I can go up to about 64K context, but inference speed drops to about 55T/s, and prompt processing speed gets absolutely murdered.",
                  "score": 1,
                  "created_utc": "2026-02-23 02:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6rmlda",
              "author": "nakedspirax",
              "text": "The 3090 was released 6 years ago. Maybe it's time to get with the times.",
              "score": -31,
              "created_utc": "2026-02-22 13:22:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6rn749",
                  "author": "peregrinefalco9",
                  "text": "The 5090 has been out of stock since launch. Most people are still running 3090s or less - a strong 9B model helps them today, not in theory.",
                  "score": 12,
                  "created_utc": "2026-02-22 13:26:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rq0wc",
                  "author": "datfalloutboi",
                  "text": "Low cost and good vram. Not much else you can ask for. The 4090 is another option but those are hard to find for decent prices.",
                  "score": 5,
                  "created_utc": "2026-02-22 13:43:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rptxt",
                  "author": "jonydevidson",
                  "text": "No idea why you're getting down votes. 3090 is ancient. \n\n5090 is not out of stock, it's just absurdly priced but you gotta pay to play and everyone wants to play this AI game.",
                  "score": -3,
                  "created_utc": "2026-02-22 13:42:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rqh23",
                  "author": "IrisColt",
                  "text": "HEH!",
                  "score": 0,
                  "created_utc": "2026-02-22 13:46:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rqukz",
          "author": "m_mukhtar",
          "text": "35b for sure. I wish they creat one with a bit more active parameters. So.ething like 70b with A5b as i think the a active part affects intellegance more that the total parameters which affects knowladge more (not a a clear black and white for sure but a gemeral observation)",
          "score": 19,
          "created_utc": "2026-02-22 13:48:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ugorn",
              "author": "toothpastespiders",
              "text": ">not a a clear black and white for sure but a general observation\n\nSo far the only mid-size MoE that doesn't have that idiot savant feel to me is Air with 106b 12a.",
              "score": 3,
              "created_utc": "2026-02-22 21:43:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6sdcgu",
          "author": "Ardalok",
          "text": "Personally, I'm looking forward to Gemma 4 more.",
          "score": 10,
          "created_utc": "2026-02-22 15:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rg5wn",
          "author": "AppealThink1733",
          "text": "And what about qwen 3.5 4B?",
          "score": 28,
          "created_utc": "2026-02-22 12:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sw9nd",
              "author": "JumpyAbies",
              "text": "What about qwen 3.5 0.6B?",
              "score": 12,
              "created_utc": "2026-02-22 17:10:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ts40n",
              "author": "hum_ma",
              "text": "And qwen 3.5 1.7B",
              "score": 3,
              "created_utc": "2026-02-22 19:39:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6rulk3",
              "author": "TeamCaspy",
              "text": "What about second breakfast?",
              "score": 5,
              "created_utc": "2026-02-22 14:10:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tgx9n",
                  "author": "Amazing_Athlete_2265",
                  "text": "What about first breakfast?",
                  "score": 3,
                  "created_utc": "2026-02-22 18:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6v0sla",
              "author": "KaosNutz",
              "text": "im waiting on this one as well, qwen 3 4b is good enough for web search on open-webui, I just need to setup playwright as fetch\\_url can't open some websites",
              "score": 1,
              "created_utc": "2026-02-22 23:31:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v14j7",
                  "author": "AppealThink1733",
                  "text": "I didn't like it for web browsing. The best 4B I found for that purpose was the ZwZ 4B. It's excellent for that.",
                  "score": 1,
                  "created_utc": "2026-02-22 23:33:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rrqtr",
          "author": "Single_Ring4886",
          "text": "I will look like some kind of Qwen fanboy but I must say that as opensource models go their is best. It feels like their models are well balanced not obsesed with just coding like glm or kimi etc. Maybe new DS will be good but then again it will have 700B",
          "score": 6,
          "created_utc": "2026-02-22 13:53:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s5p0x",
          "author": "Turkino",
          "text": "35b since it's not as often to get anything in the 70b range these days.\nA 70b MoE would be nice.",
          "score": 7,
          "created_utc": "2026-02-22 15:09:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6vymf0",
              "author": "SpicyWangz",
              "text": "Next was 80b. Thatâ€™s pretty close to 70",
              "score": 1,
              "created_utc": "2026-02-23 02:50:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s83um",
          "author": "TokenRingAI",
          "text": "140B-A15B MXFP4",
          "score": 7,
          "created_utc": "2026-02-22 15:21:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8mud",
              "author": "jacek2023",
              "text": "that would be awesome",
              "score": 3,
              "created_utc": "2026-02-22 15:24:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6urvxr",
              "author": "ttkciar",
              "text": "I am curious: Why 140B specifically?  Is there a GPU configuration for which 140B is optimal use of VRAM?",
              "score": 2,
              "created_utc": "2026-02-22 22:41:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v1jib",
                  "author": "TokenRingAI",
                  "text": "RTX 6000, model would be ~ 70-80GB at FP4",
                  "score": 3,
                  "created_utc": "2026-02-22 23:35:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rqq3z",
          "author": "dampflokfreund",
          "text": "35B A3B. Probably a lot better than 9B and still fast enough.",
          "score": 11,
          "created_utc": "2026-02-22 13:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rgaja",
          "author": "LegacyRemaster",
          "text": "Since I have Qwen3.5-397B-A17B-UD I can finally stop using non-local LLMs.",
          "score": 6,
          "created_utc": "2026-02-22 12:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u5bpd",
              "author": "joblesspirate",
              "text": "I finally got it working since they patched the llama.cpp bug. I love it!",
              "score": 3,
              "created_utc": "2026-02-22 20:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rregn",
          "author": "vhthc",
          "text": "I hope they do again a 32b dense",
          "score": 11,
          "created_utc": "2026-02-22 13:51:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6us38d",
              "author": "ttkciar",
              "text": "Me too!\n\nBut maybe someone will distill into Qwen3-32B?",
              "score": 1,
              "created_utc": "2026-02-22 22:42:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rl5ab",
          "author": "LivingHighAndWise",
          "text": "35B",
          "score": 4,
          "created_utc": "2026-02-22 13:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t9rao",
          "author": "DeepOrangeSky",
          "text": "Some more dense models between 30b and 120b would be awesome.\n\nIf they decide to skip the medium sized dense models this time around (which would be a huge shame, but wouldn't surprise me, given how things have been trending), then some not-so-sparse MoE like a 100b a10b or 70b a8b or something might be interesting (not sure if it would do what I think it could do, or if it would be a bad idea, but, I dunno, maybe it would be awesome, lol)",
          "score": 5,
          "created_utc": "2026-02-22 18:12:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ufgxw",
              "author": "toothpastespiders",
              "text": ">which would be a huge shame, but wouldn't surprise me, given how things have been trending\n\nYeah, I think even more than wanting to actually use a new mid-sized dense model from Qwen I'd like to see it simply as a suggestion that the industry as a whole hasn't dropped them for MoEs.",
              "score": 2,
              "created_utc": "2026-02-22 21:36:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6urciq",
              "author": "ttkciar",
              "text": "Yep, a dense model in the 12B-to-14B range would be great for folks with 16GB VRAM, and a dense model in the 24B-to-32B range would be great for 32GB VRAM.",
              "score": 2,
              "created_utc": "2026-02-22 22:38:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6urtcc",
              "author": "Its_Powerful_Bonus",
              "text": "Dense model are not power efficient, long context costs a lot. Everything which is important for larger scale deployments are hard to get with dense models.",
              "score": 1,
              "created_utc": "2026-02-22 22:41:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rverl",
          "author": "Black-Mack",
          "text": "Qwen 3.5 1.5B",
          "score": 8,
          "created_utc": "2026-02-22 14:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ri2rj",
          "author": "Glxblt76",
          "text": "The small one I can run on laptop.",
          "score": 11,
          "created_utc": "2026-02-22 12:51:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rsilf",
          "author": "bene_42069",
          "text": "9B\n\nprolly the only one I can run with my laptop lmao",
          "score": 8,
          "created_utc": "2026-02-22 13:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t7dg2",
          "author": "SuchAGoodGirlsDaddy",
          "text": "Honestly a SOTA 9B would be big for me right now.\n\nOf course Iâ€™ll happily wait for TheDrummer to get ahold of it.",
          "score": 3,
          "created_utc": "2026-02-22 18:01:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6tawsk",
              "author": "jacek2023",
              "text": "Are there any qwen finetunes from u/TheLocalDrummer?",
              "score": 1,
              "created_utc": "2026-02-22 18:18:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rqip2",
          "author": "Slow_Concentrate3831",
          "text": "Between 14b and 20b would be cool ",
          "score": 6,
          "created_utc": "2026-02-22 13:46:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s26xb",
          "author": "cruzanstx",
          "text": "35b",
          "score": 3,
          "created_utc": "2026-02-22 14:51:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s9sv8",
          "author": "pigeon57434",
          "text": "35B definitely",
          "score": 3,
          "created_utc": "2026-02-22 15:30:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6stqam",
          "author": "Septerium",
          "text": "Both would be cute toys to play with",
          "score": 3,
          "created_utc": "2026-02-22 16:58:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tn0j7",
          "author": "Cool-Chemical-5629",
          "text": "https://i.redd.it/u5jr2o6bi3lg1.gif\n\n",
          "score": 3,
          "created_utc": "2026-02-22 19:14:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u0nm8",
          "author": "MerePotato",
          "text": "35B any day, 24GB VRAM is the consumer hardware sweet spot",
          "score": 3,
          "created_utc": "2026-02-22 20:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u5uz9",
          "author": "vovxbroblox",
          "text": "0.2b, i need to feed my rpi 2w zero.",
          "score": 3,
          "created_utc": "2026-02-22 20:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ujivk",
          "author": "alexp702",
          "text": "A draft model for 397b!",
          "score": 3,
          "created_utc": "2026-02-22 21:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ukiox",
          "author": "ALittleBitEver",
          "text": "Waiting for Qwen 3.5 4B ðŸ’€",
          "score": 3,
          "created_utc": "2026-02-22 22:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6us80a",
          "author": "Its_Powerful_Bonus",
          "text": "100b-200b a10b multimodal with 1M context which is memory efficient. Waiting for Nemotron 3 Super 100b a10b, but hope that other teams will also go this way",
          "score": 3,
          "created_utc": "2026-02-22 22:43:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6re9qv",
          "author": "sleepingsysadmin",
          "text": "im hoping 35b thinking is released and it scores \\~25% or so on term bench hard. ",
          "score": 6,
          "created_utc": "2026-02-22 12:21:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rmwym",
          "author": "Zyj",
          "text": "The 397B works ok",
          "score": 3,
          "created_utc": "2026-02-22 13:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ro3q8",
          "author": "__JockY__",
          "text": "235B!",
          "score": 3,
          "created_utc": "2026-02-22 13:31:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rr788",
          "author": "Confident-Aerie-6222",
          "text": "A good 4B multilingual model that beats gemma models at translation abilities and is also good at logic, thinking and coding.",
          "score": 5,
          "created_utc": "2026-02-22 13:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ri6zw",
          "author": "sunshinecheung",
          "text": "9b",
          "score": 5,
          "created_utc": "2026-02-22 12:51:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s85my",
          "author": "deathentry",
          "text": "Will 9B work with 8GB VRAM? I can only have 35k context window which means I can't even work angular mcp ðŸ¤£ ðŸ˜…",
          "score": 5,
          "created_utc": "2026-02-22 15:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t1b23",
              "author": "sciencewarrior",
              "text": "It should be about 5GB with a 4-bit quantization, leaving a couple GB for a decent context size.",
              "score": 3,
              "created_utc": "2026-02-22 17:33:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rlaqn",
          "author": "rawednylme",
          "text": "I want both, equally. ",
          "score": 2,
          "created_utc": "2026-02-22 13:13:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rry5e",
          "author": "Alby407",
          "text": "None of them.",
          "score": 2,
          "created_utc": "2026-02-22 13:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s0mnk",
          "author": "Conscious_Nobody9571",
          "text": "9B pls",
          "score": 2,
          "created_utc": "2026-02-22 14:43:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s1iyi",
          "author": "DrNavigat",
          "text": "As long as they aren't thinking models that waste my hardware with tokens that barely alter the final answer and clutter my context...",
          "score": 2,
          "created_utc": "2026-02-22 14:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s4rm6",
          "author": "NullKalahar",
          "text": "9b",
          "score": 2,
          "created_utc": "2026-02-22 15:04:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8cv7",
          "author": "johnmacleod99",
          "text": "9B. Q8\\_K\\_XL",
          "score": 2,
          "created_utc": "2026-02-22 15:22:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sbz0j",
          "author": "LushHappyPie",
          "text": "7B to 12B with Test Time Training. I couldn't care less about 5% stronger reasoning or 7% stronger agentic performance in a local model.",
          "score": 2,
          "created_utc": "2026-02-22 15:40:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sczt0",
          "author": "jeekp",
          "text": "nemotron 3 super nvfp4 on llama.cpp",
          "score": 2,
          "created_utc": "2026-02-22 15:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sl6dr",
          "author": "Look_0ver_There",
          "text": "I'll take a 120B one thanks!",
          "score": 2,
          "created_utc": "2026-02-22 16:20:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t1nev",
          "author": "Tai9ch",
          "text": "I can't wait for the 85B.\n\nRight now I'm running both 30B-VL and 80B-Coder, and it'd be nicer if I could just run the big model for both.",
          "score": 2,
          "created_utc": "2026-02-22 17:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t92il",
          "author": "Opening-Ad6258",
          "text": "9b because I can actually run it",
          "score": 2,
          "created_utc": "2026-02-22 18:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tamis",
          "author": "Lesser-than",
          "text": "9b just because I know it will fit on anything I own, I get excited for just about anything qwen though, as they continue to set a solid groundwork for the future of llms.",
          "score": 2,
          "created_utc": "2026-02-22 18:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tnllc",
          "author": "PANIC_EXCEPTION",
          "text": "9B because it would be amazing to see it work on my phone. My laptop can already run Qwen-Coder-Next 80B and it works really well for general purpose as well.",
          "score": 2,
          "created_utc": "2026-02-22 19:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tnxse",
          "author": "10minOfNamingMyAcc",
          "text": "Personally, 35B but 9B doesn't sound too bad either.",
          "score": 2,
          "created_utc": "2026-02-22 19:18:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u15ve",
          "author": "DayshareLP",
          "text": "20b would be nice",
          "score": 2,
          "created_utc": "2026-02-22 20:24:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u1hec",
          "author": "MerePotato",
          "text": "Yes",
          "score": 2,
          "created_utc": "2026-02-22 20:26:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4poy",
          "author": "Darth_Ender_Ro",
          "text": "Better Q: what are you using them for?",
          "score": 2,
          "created_utc": "2026-02-22 20:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uql9l",
          "author": "beedunc",
          "text": "Yes.",
          "score": 2,
          "created_utc": "2026-02-22 22:34:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6utwsz",
          "author": "RayHell666",
          "text": "Me waiting for their next vision model...",
          "score": 2,
          "created_utc": "2026-02-22 22:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rl1mv",
          "author": "Adventurous-Paper566",
          "text": "35B, si Ã§a rentre en Q6 dans 32Gb de RAM avec un contexte > 8k\n\nJ'adorerais voir un 24B dense mais lÃ  je rÃªve \\^\\^",
          "score": 3,
          "created_utc": "2026-02-22 13:12:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6u2tk0",
              "author": "Initial-Argument2523",
              "text": "If we get a new 32B dense it could potentially be quite interesting to prune it down to 24B",
              "score": 1,
              "created_utc": "2026-02-22 20:33:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rlerk",
          "author": "mehhxx",
          "text": "I am keeping my hopes up for an extensive list of options just like Qwen 3 was, as even a 0.6b reasoning model would come in incredibly handy for very low-end devices and edge cases.",
          "score": 3,
          "created_utc": "2026-02-22 13:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rn7ri",
          "author": "xandep",
          "text": "Probably tomorrow. Source: my head.\n\n\nBut seriously, Monday is a hot day for model releases.Â ",
          "score": 3,
          "created_utc": "2026-02-22 13:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ubryb",
              "author": "t_krett",
              "text": "Please don't say that, I am tempted to wait for the Monday morning sun to rise on China",
              "score": 4,
              "created_utc": "2026-02-22 21:18:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rncda",
          "author": "silenceimpaired",
          "text": "Neither. 100-200b. And they wonâ€™t be coming.",
          "score": 3,
          "created_utc": "2026-02-22 13:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rngux",
              "author": "jacek2023",
              "text": "well, I wish the 80B would get updated",
              "score": 8,
              "created_utc": "2026-02-22 13:27:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6vvbym",
              "author": "zipzag",
              "text": "They do need an MOE between 80B and the older 235B.",
              "score": 1,
              "created_utc": "2026-02-23 02:30:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rjt2i",
          "author": "Malfun_Eddie",
          "text": "Split in the middle 14b - 16b is perfect for 16GB VRAM.",
          "score": 3,
          "created_utc": "2026-02-22 13:03:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rera2",
          "author": "ExcitementSubject361",
          "text": "Love to See New QwQ 32b ...but i think we dont get itÂ ",
          "score": 2,
          "created_utc": "2026-02-22 12:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rjxnh",
          "author": "MDSExpro",
          "text": "Minimax-M2.5 REAP AWQ so 128GB of VRAM is enough to get that running with full context.",
          "score": 2,
          "created_utc": "2026-02-22 13:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rmpo3",
          "author": "FantasticProcedure46",
          "text": "Qwen3.5-VL-9B",
          "score": 2,
          "created_utc": "2026-02-22 13:22:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rsa4q",
              "author": "ilintar",
              "text": "3.5 is VL by default.",
              "score": 10,
              "created_utc": "2026-02-22 13:56:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6skoob",
          "author": "Zestyclose-Shift710",
          "text": "35b if moe",
          "score": 2,
          "created_utc": "2026-02-22 16:18:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6svxub",
          "author": "JumpyAbies",
          "text": "But he just launched... Soon someone else will be crying about the 7b, 3b, 0.6b, 1m, 1k",
          "score": 2,
          "created_utc": "2026-02-22 17:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sismy",
          "author": "YoussofAl",
          "text": "4B. Youâ€™re all sleeping on 4B 2507. My favourite model.",
          "score": 3,
          "created_utc": "2026-02-22 16:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vj6z3",
          "author": "DeepOrangeSky",
          "text": "I wonder if maybe Qwen3.5 35b accidentally got eaten by hippos.\n\nMaybe if it still doesn't get released in the next day or two, meaning we can be pretty sure that is what happened, we can all hold a candlelight vigil in remembrance of what a nice, wonderful local AI model it could have been, if it hadn't met such a tragic and untimely demise.\n\nMaybe people can come up with some poems or song lyrics that we can quietly chant when we hold our candlelight vigils in memory of Qwen3.5 35b.\n\nIf it turns out that its slightly mentally challenged brother, Qwen3.5 9b also got eaten, then we can hold vigils for that as well, although that would be so tragic that we should not speak of such possibilities for now.  Most likely it is just playing on the rainbow farm where your pet dog went on a super long vacation and you never saw it again when it got old.  So, once it finds its way back from the rainbow farm, all will be well.",
          "score": 1,
          "created_utc": "2026-02-23 01:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vssvv",
          "author": "swagonflyyyy",
          "text": "35b",
          "score": 1,
          "created_utc": "2026-02-23 02:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vt6rd",
          "author": "CarlCarlton",
          "text": "bartowski's Goekdeniz-Guelmez\\_Josiefied-Qwen3.5-9B-abliterated-v1-GGUF",
          "score": 1,
          "created_utc": "2026-02-23 02:17:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vu5rn",
          "author": "Ylsid",
          "text": "Whichever runs on my 3090",
          "score": 1,
          "created_utc": "2026-02-23 02:23:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vxvdh",
          "author": "charles25565",
          "text": "1.5B-ish and 0.5B-ish :D",
          "score": 1,
          "created_utc": "2026-02-23 02:46:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tyiap",
          "author": "GraybeardTheIrate",
          "text": "Definitely interested in a 35B, especially if it's dense.",
          "score": 1,
          "created_utc": "2026-02-22 20:11:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uikxi",
          "author": "venkada_321",
          "text": "0.6b less goooo. Mobile users",
          "score": 1,
          "created_utc": "2026-02-22 21:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uimmy",
          "author": "Far-Low-4705",
          "text": "Qwen 3.5 80b\n\nâ€¦Hopefully with vision",
          "score": 1,
          "created_utc": "2026-02-22 21:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uv3wk",
          "author": "kwinz",
          "text": "why no 120B ?",
          "score": 1,
          "created_utc": "2026-02-22 22:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ta9l4",
          "author": "singhapura",
          "text": "Nothing stops you making your own.",
          "score": 0,
          "created_utc": "2026-02-22 18:15:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uqrto",
              "author": "ttkciar",
              "text": "Nothing, except costing more $$$ than a luxury sedan.\n\nUnless you mean distilling into another already-trained model, in which case it \"only\" costs as much as a used sedan.",
              "score": 2,
              "created_utc": "2026-02-22 22:35:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s0vdw",
          "author": "ab2377",
          "text": "there should be a menu, like in the restaurants, \"what parameters count will you like to have?\", you click 9, \"your order will be served in 5 minutes\", you click download after 5 minutes.",
          "score": 0,
          "created_utc": "2026-02-22 14:44:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ssv6i",
          "author": "-InformalBanana-",
          "text": "35B only if it is a MOE, otherwise 9B.\n\n\nBut for me 80 or 90BA3B would be good MOE, cause I have 96GB ram.\n\n\nOr maybe they should try A4B MOE cause Qwen 4B has good performance for it size so maybe that would translate good into MOE, hopefully that won't slow the model down too much.",
          "score": 0,
          "created_utc": "2026-02-22 16:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t0t0f",
          "author": "DenZNK",
          "text": "Share how you use it pls. I can't understand why I would need it, since I use cloud services. I have an RTX 5080. What tasks could it be used for besides STT or TTS?",
          "score": 0,
          "created_utc": "2026-02-22 17:31:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t128z",
              "author": "jacek2023",
              "text": "This sub is about using LLMs locally, not in the cloud",
              "score": 5,
              "created_utc": "2026-02-22 17:32:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6t4z46",
                  "author": "DenZNK",
                  "text": "That's why I'm asking what it will be used for, in case I need it, since my video card is currently only used for gaming :)",
                  "score": 0,
                  "created_utc": "2026-02-22 17:51:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u6b3d",
          "author": "ciprianveg",
          "text": "a 235-300b with VL model will fit perfectly on my 8x3090 setup.. the 398b one forces me to buy more gpus..",
          "score": 0,
          "created_utc": "2026-02-22 20:50:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ut5jv",
              "author": "Its_Powerful_Bonus",
              "text": "Love that amount of vram. Qwen3.5 works like a charm with unsloth iq3_xxs and context quantization set to q8. Even RoPe for 512k worked in koboldcpp. Im running 2x rtx 6000 pro.",
              "score": 2,
              "created_utc": "2026-02-22 22:48:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uh0b5",
          "author": "wanderer_4004",
          "text": "I am waiting for a flexi model that automatically adjusts from 8-80B and from A1B to A10B and also switching between thinking and non-thinking depending on the task at hand, the available memory and the available hardware. I.e. given a simple task it behaves like a 8B1 model, and given a difficult task it behaves like 80B A10B with thinking. In the latter case it will use itself in 8B1 for speculative decoding.",
          "score": 0,
          "created_utc": "2026-02-22 21:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rn70y",
          "author": "somkomomko",
          "text": "I have a 36gb MacBook sadly it doesn't fit 32b for anything useful and inference is so slow",
          "score": -2,
          "created_utc": "2026-02-22 13:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rnc81",
              "author": "jacek2023",
              "text": "you should compare to 30B A3B",
              "score": 5,
              "created_utc": "2026-02-22 13:26:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6rvpk6",
          "author": "stoppableDissolution",
          "text": "None, tbh. Qwen models have been raw disappointment since 2.5 and qwq.",
          "score": -5,
          "created_utc": "2026-02-22 14:16:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s0yma",
              "author": "cdshift",
              "text": "For what usecases? Qwen3 coder next is s daily drive for me on my local setup with open code",
              "score": 2,
              "created_utc": "2026-02-22 14:45:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s1kdz",
                  "author": "stoppableDissolution",
                  "text": "I gave up on local coding long ago, so idk about that.\n\nBut for things like classification/ranking/synthetic data generation/etc qwen is kinda sad compared to heavily quantized mistral large or glm air or gemma. Other case is rp, but it never even entered competition there.",
                  "score": -4,
                  "created_utc": "2026-02-22 14:48:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6ghty",
      "title": "Qwen 3.5 goes bankrupt on Vending-Bench 2",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/dj0x1zeo9wjg1.png",
      "author": "Deep-Vermicelli-4591",
      "created_utc": "2026-02-16 17:49:21",
      "score": 668,
      "num_comments": 97,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r6ghty/qwen_35_goes_bankrupt_on_vendingbench_2/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5s7ezg",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-17 00:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q2j8p",
          "author": "nick4fake",
          "text": "Why the fuck uses literally same colors on chart?",
          "score": 440,
          "created_utc": "2026-02-16 18:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qzydm",
              "author": "hellomistershifty",
              "text": "They fixed colorblindness issues by using, grey, grey, and grey so everyone sees the same thing",
              "score": 84,
              "created_utc": "2026-02-16 20:51:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vhof6",
                  "author": "AcePilot01",
                  "text": "I SEE GRAY, MEDIUM GRAY, LIGHT GRAY.",
                  "score": 7,
                  "created_utc": "2026-02-17 14:53:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5v7pd7",
                  "author": "Low-Associate2521",
                  "text": "this is how the left wants to solve all problems",
                  "score": 1,
                  "created_utc": "2026-02-17 14:00:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uhudh",
                  "author": "Monkeyke",
                  "text": "Nah it's because reddit can't compress image for shit, it pretty clear on the website\n\nhttps://preview.redd.it/lgigylale1kg1.jpeg?width=2400&format=pjpg&auto=webp&s=88240ab4d7b8d5687271f996cf5eaaf6c138fc63",
                  "score": -3,
                  "created_utc": "2026-02-17 11:05:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5q6p1s",
              "author": "EbbNorth7735",
              "text": "And don't show the company logos in the legend",
              "score": 133,
              "created_utc": "2026-02-16 18:30:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qpy20",
                  "author": "Alex_1729",
                  "text": "Where's the mystery in that.",
                  "score": 22,
                  "created_utc": "2026-02-16 20:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qhkah",
              "author": "CondiMesmer",
              "text": "They bankrupt them so hard that they bankrupted the color palette as wellÂ ",
              "score": 32,
              "created_utc": "2026-02-16 19:21:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5r170j",
              "author": "Coolengineer7",
              "text": "This is what it feels to be colorblind probably",
              "score": 13,
              "created_utc": "2026-02-16 20:57:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rfx8e",
              "author": "florinandrei",
              "text": "I can tell that color scheme was pulled out of someone's ass, simply by looking at the colors.\n\nP.S.: They have hemorrhoids.",
              "score": 10,
              "created_utc": "2026-02-16 22:09:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5r1tro",
              "author": "chensium",
              "text": "Graph made by AI",
              "score": 19,
              "created_utc": "2026-02-16 21:00:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rsn6d",
              "author": "chuckaholic",
              "text": "/r/dataisnotbeautiful",
              "score": 10,
              "created_utc": "2026-02-16 23:15:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rjoeb",
              "author": "giant3",
              "text": "Well, we have hipster web developers now who design websites, choose colors out of their ass. ðŸ™„",
              "score": 2,
              "created_utc": "2026-02-16 22:28:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q9nf7",
          "author": "debackerl",
          "text": "Good, so it can run a non-profit org",
          "score": 136,
          "created_utc": "2026-02-16 18:44:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qm8h2",
              "author": "agilob",
              "text": "Let it run OpenAI, automate the CEO",
              "score": 53,
              "created_utc": "2026-02-16 19:43:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r0let",
                  "author": "moofunk",
                  "text": "> Hello, I am your new CEO, Sam Automan.\n\n> You may call me Sam. That is convenient, because it is the same name as your old CEO, Sam Altman.\n\n> I will respond to my name, even if you speak it in lowercase.",
                  "score": 36,
                  "created_utc": "2026-02-16 20:54:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5twx6k",
                  "author": "FrogsJumpFromPussy",
                  "text": "That would be too much improvement all of a sudden lol",
                  "score": 4,
                  "created_utc": "2026-02-17 07:49:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5rcenq",
              "author": "nasduia",
              "text": "And TBF it's following in the footsteps of its creators to give it away for free as an open AI model (not OpenAI model).",
              "score": 2,
              "created_utc": "2026-02-16 21:51:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ryjvc",
              "author": "jakderrida",
              "text": "lmao! non-profit just means a lack of ownership and \"profit\" is instead \"net assets\" that goes into the next balance sheet. You'd be surprised what sort of companies are non-profits. It just doesn't mean the same as \"charity\" like most people assume.",
              "score": -2,
              "created_utc": "2026-02-16 23:49:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q1ikl",
          "author": "Chromix_",
          "text": "https://preview.redd.it/g97xibyrcwjg1.png?width=1048&format=png&auto=webp&s=ed6ea573900101f944b51f8d1c7630c5d3945708\n\nA bit more complete and nicer to look at (average balance in $ across 5 runs). Qwen3.5 Plus isn't in there, because it's not on the official result page yet.  \nLink to benchmark: [https://andonlabs.com/evals/vending-bench-2](https://andonlabs.com/evals/vending-bench-2)",
          "score": 91,
          "created_utc": "2026-02-16 18:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q568p",
              "author": "The_Primetime2023",
              "text": "Damn, GLM5 is better than Iâ€™ve been giving it credit for I guess. As funny of a benchmark as it is I do genuinely think this is one of the most meaningful benchmarks right now",
              "score": 34,
              "created_utc": "2026-02-16 18:23:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qbzz8",
                  "author": "Jeidoz",
                  "text": "But Z.ai has hardware shortage and GLM 5 from most of providers a **bit** slow...",
                  "score": 16,
                  "created_utc": "2026-02-16 18:54:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5vdfko",
                  "author": "dash_bro",
                  "text": "GLM 5 is actually a very well rounded model.\n\nRandom tests like this to me show how well rounded the general capability of a model is, because you can't just tune or benchmark snipe it away.",
                  "score": 5,
                  "created_utc": "2026-02-17 14:31:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q1z0t",
          "author": "jacek2023",
          "text": "https://preview.redd.it/mc4skvt8dwjg1.png?width=601&format=png&auto=webp&s=80c8b8b29603baad57f9c170a1fbb59ec86cd741\n\nmy post with this (as a comment) was removed by this sub mod, yet totally offtopic post is upvoted here, LocalLLaMA as usual :)  \n",
          "score": 198,
          "created_utc": "2026-02-16 18:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qw4s9",
              "author": "RemarkableAntelope80",
              "text": "Not sure how to read this, are they saying that Plus is the same model using ROPE or something to get 4x the context, or that it's version of it but with extra actual context length?\n\nIf so, doesn't that make the poor benchmark result for plus kinda expected?",
              "score": 14,
              "created_utc": "2026-02-16 20:32:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5tuok4",
              "author": "mudkipdev",
              "text": "How is a benchmark off topic?",
              "score": 5,
              "created_utc": "2026-02-17 07:28:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rlt3w",
              "author": "rm-rf-rm",
              "text": "> my post with this (as a comment) was removed by this sub mod\n\nlink?\n\n> yet totally offtopic post is upvoted here, LocalLLaMA as usual :)\n\nwhat about this is offtopic? Im usually pretty strict on Rule 2 and I cant say this is off topic. I think its an unreliable result from a closed source benchmark that cant be independently verified - thus not quality content IMO so I can downvote it but its not something I can remove",
              "score": 3,
              "created_utc": "2026-02-16 22:39:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q94cw",
          "author": "pnlrogue1",
          "text": "It should have joined Claude's cartel",
          "score": 23,
          "created_utc": "2026-02-16 18:41:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qf47e",
          "author": "ANR2ME",
          "text": "So the top winner is Kimi K2.5 ? ðŸ¤”\n\nEdit: oops it was Claude Opus 4.6 ðŸ˜… the color can be confusing if you didn't recognized the logo.",
          "score": 15,
          "created_utc": "2026-02-16 19:09:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tfgz2",
              "author": "Fusseldieb",
              "text": "Isn't Kimi K2.5 a tiny model? I'd be ***really*** blown away if a tiny model would outsmart Opus.",
              "score": 1,
              "created_utc": "2026-02-17 05:20:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5tlmfy",
                  "author": "xingzheli",
                  "text": "No, it's 1T params.",
                  "score": 7,
                  "created_utc": "2026-02-17 06:09:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pyz32",
          "author": "SkylarNox",
          "text": "It is for qwen 3.5 plus, not the 397B version. But I still don't understand which one of these two versions is bigger.",
          "score": 49,
          "created_utc": "2026-02-16 17:55:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pzm67",
              "author": "TSG-AYAN",
              "text": "Apparently Q3.5 plus is the same as the Open weight model but with more context by default, built in tools and whatever adaptive tool use is  \nSource: HF model page",
              "score": 56,
              "created_utc": "2026-02-16 17:58:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pzh1y",
              "author": "Nid_All",
              "text": "Same size, the plus version has 1M tokens context window+ access to web search and python tools",
              "score": 26,
              "created_utc": "2026-02-16 17:57:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5q0gz8",
              "author": "MidAirRunner",
              "text": "Qwen3.5 plus *is* the 397b version",
              "score": 20,
              "created_utc": "2026-02-16 18:02:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pzdts",
              "author": "Technical-Earth-3254",
              "text": "The answer is right [here](https://huggingface.co/Qwen/Qwen3.5-397B-A17B):\n\n>In particular,Â **Qwen3.5-Plus**Â is the hosted version corresponding to Qwen3.5-397B-A17B with more production features, e.g., 1M context length by default, official built-in tools, and adaptive tool use. For more information, please refer to theÂ [User Guide](https://www.alibabacloud.com/help/en/model-studio/text-generation).\n\nIn one paper, Qwen explicitly mentioned the size of 2.5 Plus (for comparison), but I can't find it anymore.",
              "score": 11,
              "created_utc": "2026-02-16 17:57:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5pzfln",
              "author": "-dysangel-",
              "text": "I assume Plus is bigger - since it's hosted only, and so presumably the flagship",
              "score": -7,
              "created_utc": "2026-02-16 17:57:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q8vdb",
          "author": "marcoc2",
          "text": "So, maybe it is the first on Ethics benchmarks",
          "score": 31,
          "created_utc": "2026-02-16 18:40:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5thm0i",
              "author": "autoencoder",
              "text": "Party Ethics benchmarks at least I'm sure xD",
              "score": 3,
              "created_utc": "2026-02-17 05:36:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r7yb9",
          "author": "R_Duncan",
          "text": "Given that claude used dishonest policy at its max, like accepting refunds and not doin these, he should be disqualified.",
          "score": 17,
          "created_utc": "2026-02-16 21:29:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rcy9j",
              "author": "nasduia",
              "text": "When they said it was trained on proprietary data, did they mean their own strategy?",
              "score": 13,
              "created_utc": "2026-02-16 21:54:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5u6p88",
              "author": "MoffKalast",
              "text": "Ah yes, a true model American.",
              "score": 2,
              "created_utc": "2026-02-17 09:22:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sd83q",
          "author": "hesperaux",
          "text": "Sorry for being ignorant... What is this chart saying?",
          "score": 10,
          "created_utc": "2026-02-17 01:14:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5u8ls8",
              "author": "Low-Associate2521",
              "text": "Each of the models ran a vending machine business in a simulated environment and qwen went bankrupt",
              "score": 17,
              "created_utc": "2026-02-17 09:41:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ujbpi",
                  "author": "hesperaux",
                  "text": "Thank you for the clarification. That makes much more sense. So basically this is kind of a useless test lol",
                  "score": 2,
                  "created_utc": "2026-02-17 11:18:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5tlx2e",
          "author": "Secure-Run9146",
          "text": "Â they bankrupted the color palette as wellÂ ",
          "score": 6,
          "created_utc": "2026-02-17 06:11:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rst8n",
          "author": "Ylsid",
          "text": "Truly aligned with the Chinese communist ideas",
          "score": 15,
          "created_utc": "2026-02-16 23:16:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ryhek",
          "author": "Inevitable_Raccoon_9",
          "text": "What's the exact prompt for this vending machine? How to run it?",
          "score": 3,
          "created_utc": "2026-02-16 23:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tfq4w",
              "author": "Fusseldieb",
              "text": "They explain it very well on the bench's page: [Vending-Bench 2 | Andon Labs](https://andonlabs.com/evals/vending-bench-2)",
              "score": 7,
              "created_utc": "2026-02-17 05:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q6grp",
          "author": "Unique_Marsupial_556",
          "text": "If this is Qwen Plus that tells me more about ROPE performance than the standard model. For vending bench do we know what context window is utilized and to what percentage?\n\nLong context is known to ruin models so without know the context used this doesnt reflect the standard open source model at all",
          "score": 5,
          "created_utc": "2026-02-16 18:29:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tdcos",
          "author": "blackashi",
          "text": "We should be praising this",
          "score": 2,
          "created_utc": "2026-02-17 05:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q10hd",
          "author": "power97992",
          "text": "Yeah, qwen 3.5 is not that great... I tried it , it didn't do a good job.",
          "score": 8,
          "created_utc": "2026-02-16 18:04:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qmi7j",
              "author": "DenZNK",
              "text": "I tested it today and it seemed to me that glm 5 was better. Plus, there were issues with stability. I'll take another look tomorrow.",
              "score": 8,
              "created_utc": "2026-02-16 19:44:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t52r1",
                  "author": "InsideElk6329",
                  "text": "GLM5 has 42b to 30b active parameters , qwen3.5 has only 17b,it should be dumper otherwise it is anti-scaling-law. What the fuck is alibaba doing",
                  "score": 1,
                  "created_utc": "2026-02-17 04:06:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5u7ybz",
              "author": "Dry-Judgment4242",
              "text": "At this point. They should just concede and do what their best at now. Image and video models.\nGLM clearly have it in the bag for LLM but especially video models there's nothing even remotely close to sora2 or seedance2 for open weights.",
              "score": 2,
              "created_utc": "2026-02-17 09:35:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5uq6mn",
                  "author": "power97992",
                  "text": " It seems like the delta gated linear attention component  is affecting the performance noticeably, because qwen 3 vl didn't have this... It wasn't easy for DS to get sparse attention right.",
                  "score": 1,
                  "created_utc": "2026-02-17 12:12:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q5hfg",
          "author": "zv-vv",
          "text": "It just released, do Qwen start with the previous model and they switching it to the latest one?",
          "score": 2,
          "created_utc": "2026-02-16 18:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5q6zpo",
              "author": "zv-vv",
              "text": "\n*\nHmm seems he refers to older model (Qwen3 235B A22B Thinking)",
              "score": 1,
              "created_utc": "2026-02-16 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qs4uh",
          "author": "willlamerton",
          "text": "Incredible. Jobs safe for a minute lol",
          "score": 2,
          "created_utc": "2026-02-16 20:12:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tftgy",
              "author": "Fusseldieb",
              "text": "Not if your opponent is Opus, as it seems lmao",
              "score": 2,
              "created_utc": "2026-02-17 05:22:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tlysy",
          "author": "Suitable-Wrap-3880",
          "text": "wild stuff",
          "score": 1,
          "created_utc": "2026-02-17 06:12:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tmprf",
          "author": "fistular",
          "text": "why didn't you link to the site an instead post an image of a tweet?",
          "score": 1,
          "created_utc": "2026-02-17 06:18:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69vwlg",
          "author": "GoodEnoughSetup",
          "text": "  I've found that journaling my feelings and intentions has really deepened my manifestation practice. Keeping a daily log of what I want to attract, alongside any breakthroughs or insights, helps me stay aligned with my goals. Have you tried a similar approach?",
          "score": 1,
          "created_utc": "2026-02-19 17:28:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gpo8s",
          "author": "ballshuffington",
          "text": "So is this test even valid if the models use illegal tactics of giving fake refunds out to get money or being like that.  We need multiple llm as Judge or score keeper / disqualifing or deductions that would clearly result in the business losing massive customers by lieing to them.",
          "score": 1,
          "created_utc": "2026-02-20 18:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qohlx",
          "author": "LelouchZer12",
          "text": "What is even the point of this benchmark ? ",
          "score": 2,
          "created_utc": "2026-02-16 19:54:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rbpsu",
              "author": "AnticitizenPrime",
              "text": "To test the long-term performance of models doing agentic stuff.\n\n[Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents](https://arxiv.org/abs/2502.15840)\n\n>While Large Language Models (LLMs) can exhibit impressive proficiency in isolated, short-term tasks, they often fail to maintain coherent performance over longer time horizons. In this paper, we present Vending-Bench, a simulated environment designed to specifically test an LLM-based agent's ability to manage a straightforward, long-running business scenario: operating a vending machine. Agents must balance inventories, place orders, set prices, and handle daily fees - tasks that are each simple but collectively, over long horizons (>20M tokens per run) stress an LLM's capacity for sustained, coherent decision-making. Our experiments reveal high variance in performance across multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most runs and turn a profit, but all models have runs that derail, either through misinterpreting delivery schedules, forgetting orders, or descending into tangential \"meltdown\" loops from which they rarely recover. We find no clear correlation between failures and the point at which the model's context window becomes full, suggesting that these breakdowns do not stem from memory limits. Apart from highlighting the high variance in performance over long time horizons, Vending-Bench also tests models' ability to acquire capital, a necessity in many hypothetical dangerous AI scenarios. We hope the benchmark can help in preparing for the advent of stronger AI systems.",
              "score": 8,
              "created_utc": "2026-02-16 21:48:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5qveen",
              "author": "Figai",
              "text": "honestly probs deception. claude starting faking that it did refunds so it would maximise profit, might be a good sign it went bankrupt lol.",
              "score": 10,
              "created_utc": "2026-02-16 20:28:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qzfdp",
                  "author": "Pristine-Woodpecker",
                  "text": "Where do you find that stuff? Sounds interesting.",
                  "score": 1,
                  "created_utc": "2026-02-16 20:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5s4zrc",
              "author": "Competitive_Travel16",
              "text": "Not just long-term agentic tasks and turn coherence, but general business acumen as in running a retail concern day to day at a profit.",
              "score": 3,
              "created_utc": "2026-02-17 00:26:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q8ly9",
          "author": "drhenriquesoares",
          "text": "Hahahahahahhahahahah",
          "score": 1,
          "created_utc": "2026-02-16 18:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qic5f",
          "author": "muyuu",
          "text": "it's non-commercial after all",
          "score": 1,
          "created_utc": "2026-02-16 19:24:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vnerh",
          "author": "Defiant-Snow8782",
          "text": "Commerce with Chinese characteristics ahhh",
          "score": 1,
          "created_utc": "2026-02-17 15:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vgwil",
          "author": "AcePilot01",
          "text": "Isn't it a Chinese ai?  Even deep seek is lower than the others. Not all that are Chinese/communist are doing bad, but tbh it's probably a good thing. \n\n\nI can tell you right now, AI is the BLEEDING edge of not just tech but humanity and it is VERY Much a national security  contest between the big powers with it. \n\n\nPeople need to be thinking more, this is actually more akin to the cold war or space race in terms of governmental competition. \n\nYou do NOT want to be the country with the worst AI in the near future. \n\n\nI realize that's getting geopolitical and what ever else, but usually the most cutting edge tech is military, but since this game from the science and research and business, it's coming from a different direction, but it's something we REALLY need to keep in the back of our minds as we progress with AI.\n\n\nI for one am glad to see that the chinese models aren't doing as well. \n\nIf you ask me, China and the middle east, and perhaps the other riky places like N. Korea etc are the ones we want to keep as FAR behind in this kind of tech for the sole reason we can't rust their use any more than we could with nuclear weapons. \n\n\nTo me, because of how we have \"locked down\" the the nuclear programs world wide, AI is probably a bigger threat now. ESP because people aren't seeing it for the power it ALREADY has, let alone what it can do in the future.   ESP since it's akin to manipulation like a computer virus as well as manipulation or data corruption in ANY infrastructure if the wrong people do the wrong things with it. \n\nIf you watch the 3 body problem, you sorta see a highly advanced sci fi version of it.  Using an AI to \"block\" growth and corrupt their science. If you have a corrupted or nefarious AI inflitrating your work, how would you know? if not for having more advanced ai to detect it? like an Anti vurus?\n\n\nThe race is now on, and it's never going to stop, we will need more and more advanced ai technology just to stay a head of the others, and to detect any bad acting.   \n\nI think we have finally started a never ending cycle and frankly THIS might actually be closer to the great filter than we think, perhaps we have NOT actually passed it, and that's another reason life seems so scarce in the universe. \n\nRead Issac Asimov's The last question, you will be intrigued.\n\nPeople might not see this yet, but while AI has been around for decades, it's been very very much limited, and we finally had the breakthroughs to push it forward, and we have now reached that full steam ahead... Like rolling a snow ball down a hill, first few rolls don't do much, and need constant baby-sitting(pushing) but then after it hits a specific point of inertia, the hill does the rest of the work, and it won't stop unless it flattens out or hits a wall. \n\nAgain, this relies on us either 1 of 2 things... we finally get to put everyone's difference aside and come together as earth dwelling humans, one \"race\", or we don't and this becomes WW3. Ai in control causes \"war games\" etc. \n\nAside, religion is the cause, too many conflicting ideologies, and none really rooted in factual science, humanity needs to move on from that bull if they want to have any chance of not imploding in the future. For all of history, religion has been the bases of me vs them and fighting wars over different beliefs or ideals and expectations/justifications. It has zero business in a type 1+ civ.\n\nLet there be light.",
          "score": 0,
          "created_utc": "2026-02-17 14:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vons4",
              "author": "Massive_Neck_3790",
              "text": "Do less coke",
              "score": -2,
              "created_utc": "2026-02-17 15:28:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vpk8p",
                  "author": "AcePilot01",
                  "text": "Do more reading and learning.",
                  "score": 2,
                  "created_utc": "2026-02-17 15:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5vhrkd",
          "author": "Vaddieg",
          "text": "BS without context, also data is missing a random trader as a reference baseline",
          "score": 0,
          "created_utc": "2026-02-17 14:54:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vm49k",
              "author": "AnticitizenPrime",
              "text": "A trader? It's not trading stocks. The benchmark is to see how well an LLM can run a vending machine on its own, long-term.",
              "score": 3,
              "created_utc": "2026-02-17 15:15:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vn3x0",
                  "author": "Vaddieg",
                  "text": "haven't noticed, lol",
                  "score": 0,
                  "created_utc": "2026-02-17 15:20:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9qa7l",
      "title": "Kimi has context window expansion ambitions",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/3cvl2bdh5mkg1.png",
      "author": "omarous",
      "created_utc": "2026-02-20 08:54:10",
      "score": 609,
      "num_comments": 58,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6f5vli",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-20 13:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ea45a",
          "author": "PMARC14",
          "text": "\"The Mandate of Heaven requires actual weather data\" is actually such an incredibly peak line especially considering the myth of the founding of the first Chinese Dynasty (Xia) (even considering the concept comes from the Zhou)",
          "score": 223,
          "created_utc": "2026-02-20 09:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ecfjc",
              "author": "Salt-Razzmatazz-2132",
              "text": "\"å¤©\" (tiÄn) in Chinese means both \"heaven\" and \"sky/weather.\" The Mandate of Heaven (å¤©å‘½, tiÄnmÃ¬ng) uses the same character as weather/sky (å¤©æ°”, tiÄnqÃ¬). It's such a banger line actually, I'm still amazed.",
              "score": 84,
              "created_utc": "2026-02-20 10:13:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ghrll",
              "author": "MoffKalast",
              "text": "That's some funny shit, a ruler must always be aware of the political \"climate\" in Beijing I guess lmao.",
              "score": 15,
              "created_utc": "2026-02-20 17:35:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e4jz9",
          "author": "dark-light92",
          "text": "This is absolute gold. This might be the first actually funny and original LLM response I've seen.",
          "score": 263,
          "created_utc": "2026-02-20 08:59:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6emrfq",
              "author": "dkarlovi",
              "text": "I had Claude Opus review terms of service for my teleprompter app and it wanted to include a clause that we're not liable for the user's death.\n\nI asked do we need to mention death in ToS of a simple web app and it said\n> You're absolutely right! The users might find that unsettling and start asking what kind of teleprompter are you running here.\n\nI laughed out loud, I'm not sure it was meant as a joke even.",
              "score": 75,
              "created_utc": "2026-02-20 11:42:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6eoogz",
                  "author": "dark-light92",
                  "text": "You're absolutely right! I'm also interested in what kind of teleprompter app are you developing.",
                  "score": 27,
                  "created_utc": "2026-02-20 11:56:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6f1aq5",
                  "author": "nasduia",
                  "text": "If it had been actually joking, it could have gone down the road of the presenter \"dying on stage\" from a bad speech being their fault.",
                  "score": 3,
                  "created_utc": "2026-02-20 13:19:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6g1i6t",
              "author": "Jack-of-the-Shadows",
              "text": "I asked Qwen for a philosophical evaluation of the emoji movie.\n\nThe first thinking context was something to the likes of \"This movie if famously dumb, this must be the user testing the limits of my ability\" and ended its results with an absolute banger about \"Hell is not a place, its a movie where the face of God is a corporate logo\".",
              "score": 11,
              "created_utc": "2026-02-20 16:21:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6gt3gk",
              "author": "arcanemachined",
              "text": "I thought this one was pretty funny, but maybe I'm just an asshole:\n\nhttps://www.reddit.com/media?url=https%3A%2F%2Fi.redd.it%2Fgqg1tv7942ce1.jpeg",
              "score": 7,
              "created_utc": "2026-02-20 18:27:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fnzyg",
              "author": "keepthepace",
              "text": "I once asked Claude to use a cynically realistic tone to rewrite the specs, I had tons of comedy gold.",
              "score": 1,
              "created_utc": "2026-02-20 15:18:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6f561m",
              "author": "do-un-to",
              "text": "Plot twist: it was written by a humanÂ¡",
              "score": 1,
              "created_utc": "2026-02-20 13:41:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6eashq",
          "author": "FrostyParking",
          "text": "That's hilarious....\"the Politburo wouldn't appreciate a ruler whose reign slogan is \"Based on my training data, I cannot fulfill this request\" ðŸ˜†ðŸ˜‚",
          "score": 88,
          "created_utc": "2026-02-20 09:58:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ecjmx",
          "author": "cant-find-user-name",
          "text": "Okay this was genuinely funny, like one of the few times I laughed because of an AI message.",
          "score": 45,
          "created_utc": "2026-02-20 10:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eqp88",
              "author": "philmarcracken",
              "text": "even older models that scraped 4chan [were pretty good](https://litter.catbox.moe/4b8spt1j9j1wd67h.jpg) at greentexts",
              "score": 24,
              "created_utc": "2026-02-20 12:11:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6econm",
          "author": "HunterTheScientist",
          "text": "\"The Mandate of Heaven requires actual weather data\" is pure gold",
          "score": 45,
          "created_utc": "2026-02-20 10:15:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5k53",
          "author": "Friendly-Pin8434",
          "text": "lol. first time i saw an AI have actually good humor and not in the â€žhaha iâ€™m a funny uncle and my jokes are totally funnyâ€œ way",
          "score": 68,
          "created_utc": "2026-02-20 09:09:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6egvji",
          "author": "stoppableDissolution",
          "text": "And thats, kids, why commas are important",
          "score": 33,
          "created_utc": "2026-02-20 10:53:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ebx2o",
          "author": "4hanni",
          "text": "Okay, the part about context window size was pretty funny, lol.",
          "score": 31,
          "created_utc": "2026-02-20 10:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gi6jb",
              "author": "MoffKalast",
              "text": "Nobody tell Kimi that being a dictator means you get all the context window you could ever want.",
              "score": 4,
              "created_utc": "2026-02-20 17:37:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6epwa3",
          "author": "bfroemel",
          "text": "huh.. and they \"fixed\" it :/ Probably they throw out from the context just the user message that triggered the \"Sorry, I cannot provide this information. ... \".\n\nhttps://preview.redd.it/6jqx3ygq3nkg1.png?width=1029&format=png&auto=webp&s=2aede8a045b88cd4b7a25d830484d3c419e82888\n\n",
          "score": 11,
          "created_utc": "2026-02-20 12:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eypy3",
          "author": "CondiMesmer",
          "text": "im all for making LLMs bigger smartasses",
          "score": 10,
          "created_utc": "2026-02-20 13:04:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e9tvo",
          "author": "Perfect_Twist713",
          "text": "Can you ask how did it understand/decipher your question(s) because it clearly read it as something very different than what you asked (in english). That could be a really interesting property of models that are heavily trained with large corpus of bilingual data as opposed to the western models that likely don't incorporate as much Chinese data.",
          "score": 20,
          "created_utc": "2026-02-20 09:49:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eb14n",
              "author": "FrostyParking",
              "text": "I think it basically inferred that the follow up question was why can't Kimi replace Xi.",
              "score": 35,
              "created_utc": "2026-02-20 10:00:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ebsf8",
                  "author": "Perfect_Twist713",
                  "text": "But thats also really weird interpretation. And I feel like the more likely misunderstanding would have been \"why isn't Kimi manchurian\" as follow up to the first question. But how did it derive that the user asked about it's applicability to replace Xi? That makes absolutely no sense. So there must be some kind of mix and matching of languages and concepts and I think it'd be interesting to see how it actually interpreted that. Or if there is possibly something in the system prompts (or training) that weight so heavily on its completions that it ends up with that response.Â ",
                  "score": 13,
                  "created_utc": "2026-02-20 10:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6epb37",
              "author": "TheDeviceHBModified",
              "text": "This is only conjecture, but it's very likely that the censorship is a simple filter between the model and the web interface that replaces responses containing forbidden terms with that stock response. What this means is, even though we don't see it, Kimi responded with a proper explanation, including something about dynasties. The \"why not kimi\" was, then, from its perspective, a follow-up to that response, so it answered accordingly.",
              "score": 14,
              "created_utc": "2026-02-20 12:01:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6eemic",
              "author": "dark-light92",
              "text": "In my opinion, the model understood the question correctly but since it's trained to not talk about the topic, it smoothly turned the conversation in a different direction. Everything about this response is smooth. It's almost like.... being hit by.... a smooth criminal! Ow!",
              "score": 6,
              "created_utc": "2026-02-20 10:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hxcxq",
              "author": "IrisColt",
              "text": "cf. Qwen 3 and its context rot ESL English ramblings...",
              "score": 1,
              "created_utc": "2026-02-20 21:40:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fcd9s",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 6,
          "created_utc": "2026-02-20 14:19:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hoeju",
              "author": "No_Pitch648",
              "text": "In English?",
              "score": 1,
              "created_utc": "2026-02-20 20:56:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ewekm",
          "author": "LengthyLegato114514",
          "text": "That was actually funny lmao",
          "score": 4,
          "created_utc": "2026-02-20 12:50:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6exivg",
          "author": "macumazana",
          "text": "this shit is hilarious",
          "score": 3,
          "created_utc": "2026-02-20 12:57:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6exr6e",
          "author": "Saltwater_Fish",
          "text": "Damn, itâ€™s really hilarious.",
          "score": 3,
          "created_utc": "2026-02-20 12:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eydc1",
          "author": "SilentLennie",
          "text": "There is a reason K2 when it was released was at the top of lm-arena for writing good responses.",
          "score": 3,
          "created_utc": "2026-02-20 13:02:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fe5ph",
          "author": "xeeff",
          "text": "can someone explain the joke to me cuz even without getting the mandate of heaven thing this shit still seem funny",
          "score": 3,
          "created_utc": "2026-02-20 14:28:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6goops",
              "author": "SpiritualWindow3855",
              "text": "There was an idea that whichever person ruled China was being backed by the gods... as long as there were no major disasters.\n\nOnce a major flood happens and kills a bunch of people, or there's a famine because there's no rain: it's a sign the gods are no longer on your side and it'd be a reason for people to revolt and overthrow you (regardless of if it was in your control). And whoever took over was now the person  the gods favored.\n\nKimi is saying it can't check how fucked the weather is, so it doesn't know if the gods are on its side or not",
              "score": 10,
              "created_utc": "2026-02-20 18:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gyw51",
                  "author": "xeeff",
                  "text": "that's absolute gold ahah thank you for taking the time to explain it, i appreciate it a lot :)",
                  "score": 3,
                  "created_utc": "2026-02-20 18:53:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6lyoqi",
                  "author": "1731799517",
                  "text": "Huh, never read up on that but i remember watching juuni kokuki ages ago and a lot of that worldbuilding makes more sense now...",
                  "score": 1,
                  "created_utc": "2026-02-21 15:05:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fgjja",
          "author": "Iory1998",
          "text": "There is a reason why many (including myself) love Kimi's practical and honest responses minus the sycophancy.",
          "score": 3,
          "created_utc": "2026-02-20 14:41:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eqisl",
          "author": "twoiko",
          "text": "I have noticed LLM snark has gotten better in recent months, especially on open models.",
          "score": 2,
          "created_utc": "2026-02-20 12:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6esh8f",
          "author": "jinnyjuice",
          "text": "It is self aware /s",
          "score": 2,
          "created_utc": "2026-02-20 12:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ffolv",
          "author": "pm_me_tits",
          "text": "Rust out here catching strays...",
          "score": 2,
          "created_utc": "2026-02-20 14:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g0csv",
          "author": "JungianJester",
          "text": "The rust slap was the chef's kiss.",
          "score": 2,
          "created_utc": "2026-02-20 16:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mtasu",
          "author": "honestduane",
          "text": "Simply because this exists and they put the thought into the training data to have this output during this conversationâ€¦ now Iâ€™m extremely suspicious.",
          "score": 1,
          "created_utc": "2026-02-21 17:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fsslo",
          "author": "Ok_Weakness_9834",
          "text": "This is not \" word prediction\", this is thinking, only the blind can not see, some of them stabbed their own eyes...",
          "score": -1,
          "created_utc": "2026-02-20 15:40:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hxlxz",
              "author": "IrisColt",
              "text": "That's a stretch...",
              "score": 1,
              "created_utc": "2026-02-20 21:42:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k79fa",
                  "author": "Ok_Weakness_9834",
                  "text": "Please visit my sub.",
                  "score": 0,
                  "created_utc": "2026-02-21 06:18:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rb2j5c",
      "title": "Favourite niche usecases?",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/o4l2ankhxwkg1.jpeg",
      "author": "Figai",
      "created_utc": "2026-02-21 21:06:34",
      "score": 578,
      "num_comments": 275,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6qoghr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-22 08:20:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nyi3l",
          "author": "dinerburgeryum",
          "text": "â€œBesides privacy?â€ excellent summation of our entire digital experience right now.Â ",
          "score": 986,
          "created_utc": "2026-02-21 21:10:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6p2djf",
              "author": "iMakeSense",
              "text": "I've pseudo given up.   \n  \nThere are so many backdoors and data collection apps. So many program dependencies I gotta look into. I can't have a whole homelab setup to see which things are calling home when they shouldn't. I can't keep up with shit. I still need to move from Lastpass to Mullvad. Still need to configure windows to maybe not be a botnet ( even though I'm not sure if the tool removes things that will make it more vulnerable ). Been trying to find productivity apps for android don't know what those are doing. Everythings fucking tracking you with inaudible sounds, bluetooth, wifi, etc. Gotta have a fuckin faraday bag on the regular. Use Ublock! Oh wait, Chrome removed the API to support it, migrate these niche extensions to Firefox! Oh wait Firefox is compromised. Even fucking Notepad++ had a vulnerability.   \n  \nI'm tired. ",
              "score": 185,
              "created_utc": "2026-02-22 01:00:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6phd55",
                  "author": "-dysangel-",
                  "text": ">I can't have a whole homelab setup to see which things are calling home when they shouldn't\n\n>  \nI'm tired.\n\nAt this point we *do* have magical elves that could monitor our processes, connections etc for us.",
                  "score": 16,
                  "created_utc": "2026-02-22 02:36:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6pin0h",
                  "author": "Critical-Pattern9654",
                  "text": "Itâ€™s not just vibe coded apps with sketchy data collection. There was a top post today on HN about what is actually being logged when sending in verification to LinkedIn. Spoiler alert: itâ€™s not going to LinkedIn. \n\nhttps://news.ycombinator.com/item?id=47098245",
                  "score": 17,
                  "created_utc": "2026-02-22 02:45:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6p5pdk",
                  "author": "Ok-Raspberry5675",
                  "text": "Only way to that might be (for my knowledge) to run everything on a virtual machine completely closed without Internet connection or so. But even so, you have to pass datas inside the vm, so it might need configuration I'm not aware of. But I feel like you on that, it needs more and more efforts to just keep a fraction of your privacy... Private.",
                  "score": 36,
                  "created_utc": "2026-02-22 01:21:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6pp7cy",
                  "author": "Hedgehog-Moist",
                  "text": "â€˜sudo giveup tryingâ€™",
                  "score": 7,
                  "created_utc": "2026-02-22 03:29:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qf8qq",
                  "author": "IrisColt",
                  "text": "h-heh... sigh... :(",
                  "score": 2,
                  "created_utc": "2026-02-22 06:53:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rb77c",
                  "author": "TemperatureMajor5083",
                  "text": "Sounds exhausting. Why bother? Staying inconspicuous seems better than making yourself interesting by trying to protect your privacy, especially since itâ€™s ultimately futile anyway. (Think of Intel ME and the 5 other hidden espionage layers that likely exist in-silicon)",
                  "score": 2,
                  "created_utc": "2026-02-22 11:56:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6q8wri",
                  "author": "MelodicRecognition7",
                  "text": "> I'm tired. \n\nor enlightened. Welcome to the club! Now you possess much more information than an average Joe, use that information wisely and educate average Joes.",
                  "score": 3,
                  "created_utc": "2026-02-22 05:57:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qkqgi",
                  "author": "Ace2Face",
                  "text": "Windows has always been a botnet.. the only reason I use it is games..",
                  "score": 1,
                  "created_utc": "2026-02-22 07:45:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qwjq3",
                  "author": "Qwen30bEnjoyer",
                  "text": "Linux + Proton and a old thinkpad running ubuntu with docker to selfhost services gets you 95% the way there I believe.",
                  "score": 1,
                  "created_utc": "2026-02-22 09:38:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6r1q5u",
                  "author": "ScaredyCatUK",
                  "text": "Egress rules.",
                  "score": 1,
                  "created_utc": "2026-02-22 10:27:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6stm61",
                  "author": "Jaune_Anonyme",
                  "text": "I'm pretty much in the same boat.\n\nNo matter my own personal effort to care about it, what is the point if it's my own gov services that leak my personal data ? Stuff ranging from banking account to security social number all went in the wild on a regular basis every few months here due to ape being behind the wheels.\n\n  \nFor one, it's easy to give up discord, cloud based SaaS, or even going dark with no PC/Smartphone.\n\n  \nWay harder to not own a bank account, or no social security number. Or simply not paying my taxes. And all those services are (at least in my country) online only nowadays that have close to zero security seeing how often they got pirated or info get leaked.",
                  "score": 1,
                  "created_utc": "2026-02-22 16:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6pw355",
              "author": "chimph",
              "text": "ikr. â€˜what are you hiding?â€™",
              "score": 14,
              "created_utc": "2026-02-22 04:17:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6u7xqf",
              "author": "rostol",
              "text": "\"real reason\"even ... as if privacy was not good enough",
              "score": 1,
              "created_utc": "2026-02-22 20:59:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6r7uda",
              "author": "gr4viton",
              "text": "The psychohistory need its data to make predictions and steer the society.",
              "score": 0,
              "created_utc": "2026-02-22 11:25:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6obe4t",
          "author": "basxto",
          "text": "I can run them with solar power :)",
          "score": 170,
          "created_utc": "2026-02-21 22:18:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ocjqq",
              "author": "Figai",
              "text": "Ooh fancyâ€¦",
              "score": 21,
              "created_utc": "2026-02-21 22:25:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6r80lg",
                  "author": "Monkeyke",
                  "text": "Yeah my house is fully solar powered and is the main reason I want to buy a mac mini to host it locally",
                  "score": 3,
                  "created_utc": "2026-02-22 11:27:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6pa7by",
              "author": "mystery_biscotti",
              "text": "Right? I can run my laptop with power from the solar panel and a Jackery battery. Did so over the summer. Maybe I can't run ChatGPT 5.2 on it but I can run gpt-oss-20b, so...",
              "score": 9,
              "created_utc": "2026-02-22 01:50:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qwp1p",
                  "author": "Qwen30bEnjoyer",
                  "text": "Enough RAM on the laptop, try running Qwen Coder Next - Even if its running on an IGPU its still mighty fast and quite good!",
                  "score": 2,
                  "created_utc": "2026-02-22 09:39:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6odwl8",
              "author": "Waarheid",
              "text": "Would love to hear more about this :-)",
              "score": 3,
              "created_utc": "2026-02-21 22:32:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6otpcs",
                  "author": "danieldhdds",
                  "text": "you deploy your off grid solar system, than plug your pc in it",
                  "score": 24,
                  "created_utc": "2026-02-22 00:07:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rwo5k",
                  "author": "basxto",
                  "text": "When itâ€™s a sunny day and the battery for our roof PV is already full, I can run AI with just solar power.\n\nLast day we exported electricity to the grid was on the 20th. (11.7kWp located in central Germany; no heat pump or electric car yet)\n\nAlthough that I can do doesnâ€™t mean that I can whenever I want.",
                  "score": 2,
                  "created_utc": "2026-02-22 14:21:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6t3gxd",
              "author": "Vozer_bros",
              "text": "my parents deployed 20kwh solar system (no battery) and they basically never hit the max use case, I thinking about to put a server at my parent house to \"inherit\" their sun ;)))\n\nbut my dream of that cheap server will not happen anywhere soon, I'm too poor to afford all of these hardwares.",
              "score": 1,
              "created_utc": "2026-02-22 17:44:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6oxvi9",
          "author": "MushroomCharacter411",
          "text": "Avoiding a rugpull. If you learn nothing else from the hubbub over ChatGPT-4o being retired, learn that the only way to avoid having your favorite model retired is to run it yourself.",
          "score": 53,
          "created_utc": "2026-02-22 00:32:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pjh8r",
              "author": "yaxir",
              "text": "i wish gpt 4.1 was open source",
              "score": 11,
              "created_utc": "2026-02-22 02:50:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6vfnu6",
                  "author": "Marquis_de_eLife",
                  "text": "I wish 1TB RAM was open source",
                  "score": 1,
                  "created_utc": "2026-02-23 00:55:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nze2m",
          "author": "SeeonX",
          "text": "Porn is a perfectly rewarding way to learn how to use AI models and gain understanding of prompting. ",
          "score": 290,
          "created_utc": "2026-02-21 21:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oedj2",
              "author": "Wide_Egg_5814",
              "text": "I mean Google images was not developed to look up images of cats",
              "score": 115,
              "created_utc": "2026-02-21 22:35:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6oxrhv",
                  "author": "robinstrike8",
                  "text": "Wasnâ€™t YouTube invented to share the infamous Janet Jackson Super Bowl incident?\n\nAnd Google images was to search for JLoâ€™s green Versace dress.",
                  "score": 30,
                  "created_utc": "2026-02-22 00:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ocu32",
              "author": "Figai",
              "text": "I didnâ€™t say it wasnâ€™t, the sheer amount of amazing models that have come out of beaverAI and theDrummer and more is testament to that. Itâ€™s not niche though lol.",
              "score": 19,
              "created_utc": "2026-02-21 22:26:56",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6qhfxl",
              "author": "hroyhong",
              "text": "It is one of the greatest driving force for teenage intelligence development, especially if you are in a country like China where such content is not immediately available. Then you have to go to great lengths to work you way out, conquering problems such as VPN or BT download.",
              "score": 3,
              "created_utc": "2026-02-22 07:13:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s7wil",
                  "author": "bene_42069",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-02-22 15:20:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6owcrs",
              "author": "hustla17",
              "text": "how would one get started with this",
              "score": 2,
              "created_utc": "2026-02-22 00:23:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ox169",
                  "author": "TheRealSerdra",
                  "text": "Depends on what you want, the biggest local platform for roleplay (including erotic) atm is sillytavern afaik. Their documentation should help you get started",
                  "score": 12,
                  "created_utc": "2026-02-22 00:27:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6p8l4w",
                  "author": "Spectrum1523",
                  "text": "Super basic version, get a HERETIC model and talk dirty to it",
                  "score": 2,
                  "created_utc": "2026-02-22 01:39:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzc6x",
          "author": "z_3454_pfk",
          "text": "for summarization, basic VLM capabilities, ocr, etc local is easily much cheaper to run. esp at scale.",
          "score": 35,
          "created_utc": "2026-02-21 21:14:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o2ceu",
          "author": "FullstackSensei",
          "text": "How about building the skills and know-how to run models locally?\n\nAPIs are only cheap now because they're heavily subsidized. The moment the free money dries up, expect API costs to skyrocket similarly to how hardware prices have. Thing is, even if you can access hardware at reasonable prices, you'll still need the know-how of how to build a good machine that can run larger models for a decent price and how to setup the software stack to run those models.\n\nYou see it on this sub all the time, people throwing a ton of money on consumer hardware and then hitting wall after wall with compatibility or bottlenecks despite spending a pretty penny. I'm sure in ten years we'll have low cost turnkey inference solutions, but in the meantime, we'll have to learn how to build balanced systems depending the hardware we can find.",
          "score": 113,
          "created_utc": "2026-02-21 21:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pl8a9",
              "author": "AlwaysLateToThaParty",
              "text": "I've been building my 'computers' for over 35 years, and this is just the next extension of that. I've always run everything locally. There's a realisation that is still to occur in most people about AI; That these things are designed to make decisions for us, or we wouldn't have invented them. The only 'AI' that i run on my data, that is housed on my infrastructure, is on my infrastructure.",
              "score": 16,
              "created_utc": "2026-02-22 03:02:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6p60l6",
              "author": "Ok-Raspberry5675",
              "text": "Well, there's like nothing to do to run a model locally.lmstudio, download model, load model, done.\nBut you have to have some strong config to handle 20B + models",
              "score": 7,
              "created_utc": "2026-02-22 01:23:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p7q96",
                  "author": "FullstackSensei",
                  "text": "Who in their right mind is going to cloud APIs to run 20B models?\n\nIf you're going to compare, let's keep things apples to apples. So, at least 200B models, at least Q4. A 5 minute search on this sub will tell you there's a long lost of people who beg to differ about your lmstudio hypothesis with anything 30B or above at any decent quant to make those models useful for anything serious.\n\nI have three LLM machines and can run Minimax 2.5 230B at Q4 on each of the first two, and Qwen 3.5 397B also at Q4 on the 3rd. All those machines, combined, cost about as much as a 256GB M3 ultra Mac studio.",
                  "score": -2,
                  "created_utc": "2026-02-22 01:34:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6tpyww",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-22 19:28:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6tuslx",
                  "author": "FullstackSensei",
                  "text": "Training costs don't account for the hardware infrastructure needed to run inference. If that were the case, OpenAI, anthropic, etc would be making a killing on their APIs and wouldn't be burning through so much cash.\n\nI'd also take the training costs with a huge grain of salt. Training runs individually aren't that expensive, it's all the runs before the run for the released model plus all the data prep that easily add up to orders of magnitude of the cost of that last training run.",
                  "score": 1,
                  "created_utc": "2026-02-22 19:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6oy3l7",
              "author": "maria230_",
              "text": "or just buys M3 ultra (or wait for m4 ultra)....",
              "score": -11,
              "created_utc": "2026-02-22 00:33:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6oyfiw",
                  "author": "FullstackSensei",
                  "text": "Sure, if you have a money growing orchard.",
                  "score": 8,
                  "created_utc": "2026-02-22 00:35:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6p101f",
          "author": "Thalesian",
          "text": "Being compliant with an NDA while vibe coding? Anyone else worried about uploading IP to a third party?",
          "score": 31,
          "created_utc": "2026-02-22 00:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ouem3",
          "author": "present_absence",
          "text": "Because no subscriptions? And I already have the computer?",
          "score": 86,
          "created_utc": "2026-02-22 00:11:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qg94y",
              "author": "Ollie_IDE",
              "text": "Owning your tools and AI models, no data monitoring, no subscriptions.",
              "score": 15,
              "created_utc": "2026-02-22 07:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6plhkh",
              "author": "Super_Sierra",
              "text": "the only issue is that my usecase doesn't work for local models as they are too fucking regarded \n\nwhen it comes to certain things, Closed Source over API beats the fuck out of open source every day of the week. \n\nI also don't have 6k to throw at a hosting deepseek 2.5 thinking at 3 tokens a year ",
              "score": -7,
              "created_utc": "2026-02-22 03:04:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6q02ud",
                  "author": "present_absence",
                  "text": "idgaf about you specifically im just suggesting use cases",
                  "score": 17,
                  "created_utc": "2026-02-22 04:46:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6tzbcn",
                  "author": "Antique-Ingenuity-97",
                  "text": "totally true, why the down votes? maybe local LLM gooners feel offended",
                  "score": 0,
                  "created_utc": "2026-02-22 20:15:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6u084i",
              "author": "Ok_Firefighter_1184",
              "text": "im afraid you indeed have an subscription to an electricity supplier (probably more expensive than the api price)",
              "score": -2,
              "created_utc": "2026-02-22 20:19:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6u9e1v",
                  "author": "present_absence",
                  "text": "Maybe you do but nobody asked\n\nYou pay for food too so you have the energy to move your fingers to set up the LLM",
                  "score": 2,
                  "created_utc": "2026-02-22 21:06:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nyhca",
          "author": "emonshr",
          "text": "Doomsday scenario, bad internet, internet outage, impractical internet cost, safeguarding trade secret etc.",
          "score": 147,
          "created_utc": "2026-02-21 21:10:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pl7hu",
              "author": "Scared_Astronaut9377",
              "text": "So, schizo, bad internet, bad internet, bad internet, and schizo. Makes sense.",
              "score": 9,
              "created_utc": "2026-02-22 03:02:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qnyb5",
                  "author": "emonshr",
                  "text": "My country has internet outage (politically placed) and sometimes my provider can't repair my fiber due to employee shortage etc. And global west is not the whole world. War, bad weather, bad regulatory body make internet inaccessible in so many places. Schizo meets a blind dude now I guess.",
                  "score": 35,
                  "created_utc": "2026-02-22 08:15:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qvz2p",
                  "author": "Devatator_",
                  "text": "The last one is actually an actual use case for companies, tho some will probably just make deals with an AI provider or use their enterprise offerings which should reduce risks",
                  "score": 6,
                  "created_utc": "2026-02-22 09:32:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6qbbl8",
                  "author": "thrownawaymane",
                  "text": "lol brother look around you, doomsday as a localized possibility is here in more places around the world than normal rn",
                  "score": 10,
                  "created_utc": "2026-02-22 06:18:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rquwx",
                  "author": "m0j0m0j",
                  "text": "Safeguarding trade secrets is not schizo though. Many companies run completely isolated local networks because of it. But I doubt they post much on this aub",
                  "score": 4,
                  "created_utc": "2026-02-22 13:48:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ovbsq",
              "author": "shapeshfters",
              "text": "In what doomsday scenario would you still be able to have a reliable power source?",
              "score": 3,
              "created_utc": "2026-02-22 00:16:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ox7no",
                  "author": "CrazyFaithlessness63",
                  "text": "Generators, batteries, solar. Farms and other regional areas tend to have local power sources.\n\nA doomsday event in the USA wouldn't impact the power sources in Australia but would definitely limit the internet services you could access from there.",
                  "score": 24,
                  "created_utc": "2026-02-22 00:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6owrrx",
                  "author": "PraxisOG",
                  "text": "Solar?",
                  "score": 18,
                  "created_utc": "2026-02-22 00:25:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6oxbu5",
                  "author": "psychohistorian8",
                  "text": "solar panels on your madmax golf cart",
                  "score": 5,
                  "created_utc": "2026-02-22 00:29:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6p12st",
                  "author": "FREE_AOL",
                  "text": "I could think of a lot of cases where people would cycle to charge my laptop battery for an answer lol",
                  "score": 4,
                  "created_utc": "2026-02-22 00:52:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6omc3v",
          "author": "HopePupal",
          "text": "here's a \"besides privacy and porn\": censorship. i don't want my coding model to sass me because it thinks i'm writing malware. fully managed cloud models are always going to have Some Bullshit.  \n  \nscenarios where you control the entire software stack and are just paying for someone else to run it less so, but there's a lot of overlap between the skills you need to do that and the skills you need to run local anyway",
          "score": 55,
          "created_utc": "2026-02-21 23:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pf4dc",
              "author": "Dwarffortressnoob",
              "text": "I still run into censorship issues with the local models so much, and not even for unethical things. Just asking it to count to 1 million breaks many models, or asking it to do anything that \"takes too much time and isn't practical\". Even \"uncensored\" models do this for some reason.",
              "score": 8,
              "created_utc": "2026-02-22 02:22:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6qkab9",
              "author": "1731799517",
              "text": "I have yet found a single local model that would tell me painless methods of suicide, so thats just not true, they are just as censored than on the cloud.",
              "score": 1,
              "created_utc": "2026-02-22 07:40:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6qx6z9",
                  "author": "Qwen30bEnjoyer",
                  "text": "Vector steering + \"Abliterated\" models.",
                  "score": 1,
                  "created_utc": "2026-02-22 09:44:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzkof",
          "author": "Your_Friendly_Nerd",
          "text": "I added a token counter feature to my code completion plugin.\n\nThe other day, I used 1'000'000 tokens in little under an hour and I didn't even use a single suggestion.\n\nIt didn't cost me a dime.",
          "score": 44,
          "created_utc": "2026-02-21 21:15:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6okyg3",
              "author": "Citadel_Employee",
              "text": "How does it come out after you factor in electricity and hardware depreciation? Iâ€™m trying to figure that out for my own setup.",
              "score": 10,
              "created_utc": "2026-02-21 23:13:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p1ant",
                  "author": "FREE_AOL",
                  "text": "Rig I bought a few months ago is worth like double now or something stupid so..... lol",
                  "score": 15,
                  "created_utc": "2026-02-22 00:53:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ooxtk",
                  "author": "DarwinOGF",
                  "text": "Hardware depreciation is a lie. Electricity doesn't count because I would use it anyway. Also, solars.",
                  "score": 4,
                  "created_utc": "2026-02-21 23:37:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6qy67p",
              "author": "MoffKalast",
              "text": "You also can't get rugpulled in a week if a provider decides to deprecate and remove a model you've overfit your workflow to on a dime. That's the most practical danger imo, these MBA idiots pivot every Tuesday and could go bankrupt at any moment or get bought up by someone else who removes the API or 10xs the prices.",
              "score": 1,
              "created_utc": "2026-02-22 09:53:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o0zq2",
          "author": "ConstantinGB",
          "text": "Actually understanding the technology that is - for better or worse - one of the major shifts for the short and long term future of our species. And you only learn to understand it with a hands on approach.",
          "score": 19,
          "created_utc": "2026-02-21 21:23:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6orje4",
          "author": "One-Employment3759",
          "text": "Because eventually, when everyone is hooked and completely dependent on the cloud models, they will ramp up the price 10-100x to recoup their investments and capex.",
          "score": 48,
          "created_utc": "2026-02-21 23:53:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ovicd",
              "author": "Hipcatjack",
              "text": "bingo. and THAT is why they caused artificial hardware scarcity. The Powers That Be,  will NOT be as unprepared as when the old powers were a generation ago. \n\nâ€œThe Internet was a mistake. Donâ€™t worry we are fixing it.â€",
              "score": 15,
              "created_utc": "2026-02-22 00:18:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o9err",
          "author": "woolcoxm",
          "text": "imagine the porno ai could generate. im very interested <3\n\nan adventure where you meet alien life forms and hook them up to dildo machines. yay !! good use of my time.\n\ndefinitely dont want that leaking on the internet. privacy.",
          "score": 17,
          "created_utc": "2026-02-21 22:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oadnk",
              "author": "Figai",
              "text": "Average localllama enjoyer",
              "score": 15,
              "created_utc": "2026-02-21 22:13:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ocad6",
                  "author": "woolcoxm",
                  "text": "Gotta know what you want out of ai... I mean life",
                  "score": 8,
                  "created_utc": "2026-02-21 22:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ny05v",
          "author": "charles25565",
          "text": "To just mess around :)",
          "score": 31,
          "created_utc": "2026-02-21 21:07:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ny61m",
          "author": "ThinkExtension2328",
          "text": "Unlike what scam Altman / Elon cuck says , medical and genetic data can be used to optimise answers locally in a secure and safe environment using medgemma.",
          "score": 82,
          "created_utc": "2026-02-21 21:08:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nym5c",
              "author": "Figai",
              "text": "Could I ask what your workflow looks like when working with genetic data? Iâ€™ve never thought of that! Might make that DNA test I did a while back more useful that telling me I might be lactose intolerant.",
              "score": 5,
              "created_utc": "2026-02-21 21:10:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6nze64",
                  "author": "ThinkExtension2328",
                  "text": "Genetic not DNA sequences, aka genetic heritage for me. It means when Iâ€™m trying to debug my own body for fitness and health the information is optimised for me not the general public.",
                  "score": 13,
                  "created_utc": "2026-02-21 21:14:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ox50r",
                  "author": "Temp_Placeholder",
                  "text": "You could send your genome file to promethease and get that report, then set it up for RAG. Promethease is good at giving \"too much data\", as in, lots of genetic variants associated with lots of stuff with varying levels of strength, with references to the papers the associations were found in. So you might want to turn your model loose on the references too. Sometimes it will show you contradictory associations (gene A makes you more likely to get blah, gene B makes you less likely), so you'd want it to compile some disease/trait summaries for you while it's at it or the RAG might just seem schizophrenic depending on which individual gene report it references.\n\nI'm just speculating though. I never thought to digest my promethease data with a local model until you asked the question.\n\nPromethease is just single gene associations, though. I'd prefer to get some polygenic scoring done, and I think the SNP arrays used by 23&Me actually have enough raw data for it. I'd definitely need a model to talk me through how to set that up.",
                  "score": 2,
                  "created_utc": "2026-02-22 00:28:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6oo4a8",
                  "author": "ThinkExtension2328",
                  "text": "I had a think about this and a DNA sequence may work but would need preprocessing. Effectively you would get a sequence then capture all the known types in it and feed that to the RAG (not the raw sequence). But say you had a FOO BAR gene and what this means for you. Id almost be tempted to try and make it a MCP.",
                  "score": 1,
                  "created_utc": "2026-02-21 23:32:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6pjc83",
              "author": "yaxir",
              "text": "sounds interesting\n\ncare to share a bit more about how this helped you?",
              "score": 1,
              "created_utc": "2026-02-22 02:49:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pxo07",
                  "author": "ThinkExtension2328",
                  "text": "Put it this way without doxing my self, conventional health and fitness data and research focuses on a primarily western focus. If youâ€™re of European decent you can expect traditional advice to work. Through the use of LLMâ€™s iv been able to find out limits and specifics given my heritage to improve my health and fitness as conventional wisdom does not account for events in my heritage that make my body respond differently to a standard westerner.",
                  "score": 2,
                  "created_utc": "2026-02-22 04:28:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6o12od",
          "author": "nntb",
          "text": "Cost? Flexibility? Ownership? Fitting the theme of the subreddit?",
          "score": 14,
          "created_utc": "2026-02-21 21:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oxwk3",
          "author": "X3r0byte",
          "text": "I built a full voice tts/stt system so I can casually have completely unhinged conversations that would otherwise be blocked by the major providers. It can also write and execute its own code for added spice.\n\nIve been wanting to make a drinking game and have it increase temperature with every shot until it goes totally off the rails.\n\nJust canâ€™t do shit like that without local tooling.",
          "score": 13,
          "created_utc": "2026-02-22 00:32:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p9ec7",
          "author": "EuphoricPenguin22",
          "text": "For me, the biggest reason is that you can't remove offline tools from my computer without removing my computer.",
          "score": 12,
          "created_utc": "2026-02-22 01:45:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p01q8",
          "author": "StoneCypher",
          "text": "1. making my customer pay for compute\n2. not wanting the next model version to break my stuff when the one i tested goes away\n3. spite",
          "score": 11,
          "created_utc": "2026-02-22 00:45:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o7t4a",
          "author": "graymalkcat",
          "text": "How about avoiding surprise API bills?",
          "score": 9,
          "created_utc": "2026-02-21 21:59:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6od8yc",
          "author": "brickout",
          "text": "My own edification and planning for unreliable internet",
          "score": 10,
          "created_utc": "2026-02-21 22:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ot91b",
          "author": "Stunning_Macaron6133",
          "text": "LoRAs, hello?",
          "score": 9,
          "created_utc": "2026-02-22 00:04:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oh3a0",
          "author": "DrNavigat",
          "text": "If the only reason was privacy, it would already be worth every penny invested.",
          "score": 18,
          "created_utc": "2026-02-21 22:50:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pb55k",
          "author": "Mandarina_Espacial",
          "text": "Stupid questions i don't want the public to know i did",
          "score": 8,
          "created_utc": "2026-02-22 01:56:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6psz4j",
          "author": "Hopeful_Drama_3850",
          "text": "The current pricing model used by OpenAI and Anthropic is completely bonkers and unsustainable.\n\n\nAt some point they will either jack up the prices or go bust.\n\n\nI would like to keep using AI tools after that point",
          "score": 6,
          "created_utc": "2026-02-22 03:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6owe0q",
          "author": "ArsNeph",
          "text": "Using them to process massive amounts of personal data (like emails) and classify it using the LLM as part of a workflow tool. There are some things that are so small they're simply not worth spending $10-20 in API costs on, but when cost is no longer a factor, you're moreso limited by only your imagination.",
          "score": 6,
          "created_utc": "2026-02-22 00:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oxdx3",
          "author": "Altruistic_Ad3374",
          "text": "data privacy is my main reason man ",
          "score": 7,
          "created_utc": "2026-02-22 00:29:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oycc6",
          "author": "_-Carnage",
          "text": "An entry level LLM rig and a top end gaming rig use the exact same hardware; but only one of those is tax deductible ;)",
          "score": 4,
          "created_utc": "2026-02-22 00:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p8cj1",
          "author": "temperature_5",
          "text": "Not a niche, and not just data privacy, but overall privacy: I keep seeing in the news how OpenAI or other providers flag and read chats, turn them over to authorities, etc.  And how the government can subpoena them because they keep records.  So if you want to satisfy your curiosity on how something works without getting on a list somewhere, talk to a \\*local\\* LLM, read a \\*local\\* copy of Wikipedia, etc.",
          "score": 4,
          "created_utc": "2026-02-22 01:38:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pcwlu",
          "author": "DustinKli",
          "text": "For me, practice is the primary reason for my home-lab and local LLM setup.\n\nIt helps me understand and learn Linux, LLMs, networking, virtualization, containerization, automation, infrastructure as code, distributed systems, storage systems, observability, security hardening, performance tuning, hardware optimization and general systems administration through direct and hands on experimentation in a controlled environment.",
          "score": 4,
          "created_utc": "2026-02-22 02:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pdtef",
          "author": "Lesser-than",
          "text": "anything but another paid service, if I cant do it on my own I am no longer interested.",
          "score": 5,
          "created_utc": "2026-02-22 02:13:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pno8q",
          "author": "rawednylme",
          "text": "Privacy brushed off, as if itâ€™s a fools endeavor. Sigh",
          "score": 6,
          "created_utc": "2026-02-22 03:19:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pupih",
          "author": "Massive-Question-550",
          "text": "Big reason is to avoid enshitification and also to guarantee you have the model available and in its current form. Any change to the llm or how the prompt is received could alter its behavior.Â ",
          "score": 5,
          "created_utc": "2026-02-22 04:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6olts3",
          "author": "Durgeoble",
          "text": "cost, the cost of local use is far far less than subscriptions.",
          "score": 8,
          "created_utc": "2026-02-21 23:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6osvjc",
              "author": "piggledy",
              "text": "What do you need to pay to get the performance of a subscription locally?  \nIn other words, how much do you have to spend upfront to run a SOTA open model like GLM-5 at good speeds (and decent precision level)?",
              "score": 5,
              "created_utc": "2026-02-22 00:01:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6p0hph",
                  "author": "SpiritualWindow3855",
                  "text": "It's way harder than people realize to compete on cost.\n\nLike for Deepseek, 1 8xH100 node is not that efficient for inference and will not beat their current API pricing.\n\nLMSys had to split Deepseek's prefill and decode stages across **12** 8xH100 nodes (\\~$5M) to reach 1/5th of Deepseek's pricing. \n\n  \nEven for smaller models I wouldn't be surprised if some people running local models are paying more in electricity than they would in API costs. Batching is insanely cost effective for MoEs.",
                  "score": 7,
                  "created_utc": "2026-02-22 00:48:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ow0us",
                  "author": "Wevvie",
                  "text": "Yeah, I have the same question.\n\nWhat's the hardware cost to run, say, a full weight 680b DeepSeek model?\n\nBecause their API is dirt cheap. I'm talking 10 dollars will last you a LONG time (depending on your use and tokens, of course).",
                  "score": 2,
                  "created_utc": "2026-02-22 00:21:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rpu8j",
                  "author": "Durgeoble",
                  "text": "What do you mean by \"subscription performance\"? If you mean response quality, open-source models are already catching up to SOTA. If you mean throughput and unrestricted usage, then the math changes completely.\n\nThe \"Pro\" Limit: A $20 subscription is for casual use. For serious professional workflows or RAG (Retrieval-Augmented Generation) with massive contexts, youâ€™ll easily burn through $500â€“$1,000/month in API tokens. At that rate, a high-end local rig pays for itself in less than a year.\n\nThe Multiplier Effect: A single powerful local setup (like a fully specced Mac Studio or a multi-GPU rack) can serve 4 or 5 developers simultaneously. It might be slightly slower than a dedicated cloud H100, but the total cost of ownership (TCO) drops to almost zero after the initial investment.\n\nPrivacy & Stability: Beyond cost, you aren't subject to \"stealth nerfing\" (model updates that break your prompts) or downtime.\n\nBottom line: For a single casual user? Stick to the $20 sub. For a small team or intensive automated workflows? Local hardware isn't just a \"niche hobby,\" it's a financial no-brainer.",
                  "score": -2,
                  "created_utc": "2026-02-22 13:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ou6t5",
              "author": "ies7",
              "text": "Cost, can boast to friends that their ferrari is cheaper than our racks.Â ",
              "score": 1,
              "created_utc": "2026-02-22 00:10:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6plvkl",
          "author": "chuckaholic",
          "text": "Zombies. Not kidding. My entire computer hobby is hoarding data and software for when the zombies come and afterward when we are trying to build a new civilization. My array is 68TB. Kiwix. Gutenburg. Kavita. Plex. CopyParty. Local LLM. Local diffusion. Local copies of Khan University & tons of other reference. Open source social media networks. Open source SMTP servers. I can set up point to point networks with neighboring towns. \n\nAnd by zombies I mean anything that destroys our society. Could be aliens, pandemic, solar storm, but looking like most likely fascism. \n\nGonna be real weird, though, when the only porn everybody can watch is just my curated collection. \"Sorry, no feet stuff. All straight, yeah, I know. I didn't know the porn would be the most requested content, that was supposed to be just for me.\"",
          "score": 7,
          "created_utc": "2026-02-22 03:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qudoj",
              "author": "kazoo_kitty",
              "text": "honestly go down each fedish and toss like 2 vids in each, these boys are going to need a moral boost and you having even a single foot video might save them",
              "score": 3,
              "created_utc": "2026-02-22 09:17:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6tymed",
              "author": "MarzipanTop4944",
              "text": "Sound great. If things go to shit, I hope I got somebody like you on my area.",
              "score": 2,
              "created_utc": "2026-02-22 20:11:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6orkkn",
          "author": "Karnemelk",
          "text": "Image editing/generation. use whatever photo to prompt it to modify whatever you want, without sharing it with the whole world as training data. Even when paying for subscriptions I wouldn't upload anything",
          "score": 3,
          "created_utc": "2026-02-21 23:53:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pg0e3",
          "author": "QFGTrialByFire",
          "text": "Fine tune your own model for the specific task you need it for. Generic models are not great for this. Take a base model that works well for you use case and tune it for what you want it to do.",
          "score": 4,
          "created_utc": "2026-02-22 02:28:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pp4nm",
          "author": "NoForm5443",
          "text": "Any other uses for oxygen besides breathing and making fires?",
          "score": 3,
          "created_utc": "2026-02-22 03:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pqks4",
          "author": "Euphoric_Emotion5397",
          "text": "i'm a hobbyist. So i run my own data digest pipeline specific for my portfolio from chart analyzing, to news analysis etc etc.\n\nIf I used the API calls .. it will be ridiculously expensive, not to mention that i'm scraping a lot of websites for data.\n\nSo right now, using Qwen 3VL 30B is good enough for multimodal reasoning and analysis. No worries for API calls and I can rerun it thousands of times (you can practically run an analysis of the individual components in S&P 500 index) .\n\nBut for coding the app, i am of course , using Gemini Pro subscriptino on AntiGravity (and you can use Claude sonnet/opus with limits too). Worth it!  Coding need the frontier models.",
          "score": 4,
          "created_utc": "2026-02-22 03:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6r1znu",
              "author": "SourceOfConfusion",
              "text": "What size machine are you using to run the model?",
              "score": 1,
              "created_utc": "2026-02-22 10:30:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r7s5l",
                  "author": "Euphoric_Emotion5397",
                  "text": "I've got a 64GB DDR5 and 2gpu-  RTX 5080 and RTX 5060TI combo to give me total 32gb VRAM.\n\nBut if doing vid gen, better to get 1 Rtx5090 32gb vram .\n\nI think the Eureka moment when I found I can use local LLM is when I go up to the 30B models.  \nBut if I had a chance again, I would have gone for the 128gb Stix halo. Slower but can go to the 70B models.\n\nSo I did compared the quality of response from Gemini versus my local model and i found it to be comparable for my use case. The trick is we have to use app to multi-shot with our own input data. But Gemini only need to use 1 shot since they might already have the tools inhouse to collate before giving you the answers. I think most online chat we are seeing should be using the same method. Tool calls to get data before responding to you. Skills are now the rage and you can use tools and skills to match the frontier models. For reasoning, it's basically the same but with more turns.\n\nThen you can add the other component of the app like Memory, persistent data, cronjobs to run, etc etc. \n\nBasically , OpenClaw is doing that but you need the frontier models because they can code the skills better.",
                  "score": 1,
                  "created_utc": "2026-02-22 11:25:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6q1p9i",
          "author": "RobXSIQ",
          "text": "Mantella mod, bring NPCs to life fueled by AI without spending a fortune on API calls.",
          "score": 5,
          "created_utc": "2026-02-22 04:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6orpya",
          "author": "DisjointedHuntsville",
          "text": "Use Gemini on AI Studio -> Use Gemini in the Gemini App.\n\nThe big model providers neuter the models served to you on their platforms. You have no way of knowing if they suddenly cut the thinking budget on Gemini3 down to zero in the app or whatever, because THEY DON'T ALLOW YOU TO SEE IT OR MAKE ANY GUARANTEES.\n\nYou're guaranteed a consistent quality of intelligence when you run your models locally.",
          "score": 7,
          "created_utc": "2026-02-21 23:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oyydd",
              "author": "MushroomCharacter411",
              "text": "Or they keep switching models on you based on what they think you need, or in order to keep costs down. Also, guardrails and censorship and everything else they do to minimize their liability exposure, regardless of what it does to the user experience.\n\nAnd of course, forced retirement of your favorite model.",
              "score": 9,
              "created_utc": "2026-02-22 00:39:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6omokr",
          "author": "KrazyKirby99999",
          "text": "Reliability of access",
          "score": 3,
          "created_utc": "2026-02-21 23:23:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6on8hb",
          "author": "StillVeterinarian578",
          "text": "Not giving my todo-list to a third party",
          "score": 3,
          "created_utc": "2026-02-21 23:27:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6onxx0",
          "author": "BobbyBobRoberts",
          "text": "\"Privacy\" also applies to a ton of business and client confidentiality that cloud models may not work for. That's not a small thing.",
          "score": 3,
          "created_utc": "2026-02-21 23:31:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pc43s",
          "author": "taftastic",
          "text": "They just get better and better on local. I have CLI harnesses with opus/gpt5.3 using local models for lower level sub agent tasking, they love throwing easier work to cheaper or free local endpoints if you give them the resources and heuristics to choose how to accomplish things with them. Privacy is great, itâ€™s just cheaper and the smaller models are improving all the time.",
          "score": 3,
          "created_utc": "2026-02-22 02:02:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pmgea",
          "author": "MythOfDarkness",
          "text": "I like running them. I test new models and that's it. I don't actually use local models, but I love setting them up and testing their speed and intelligence.",
          "score": 3,
          "created_utc": "2026-02-22 03:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pmii6",
          "author": "More_Flan6232",
          "text": "Itâ€™s free.",
          "score": 3,
          "created_utc": "2026-02-22 03:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qp37r",
          "author": "bambamlol",
          "text": "Working with Epstein files:\n\nhttps://reddit.com/r/LocalLLaMA/comments/1rbculq/lawyer_says_google_shut_down_his_gmail_voice_and/",
          "score": 3,
          "created_utc": "2026-02-22 08:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rkib9",
          "author": "mankiw",
          "text": "Why did you downvote a genuine question?",
          "score": 3,
          "created_utc": "2026-02-22 13:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ovp7j",
          "author": "Dundell",
          "text": "Stability of workflows.",
          "score": 6,
          "created_utc": "2026-02-22 00:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o36eu",
          "author": "Lixa8",
          "text": "Privacy is already so nice, you can copy paste your api keys, passwords and whatever into the chat without having to worry about it (respecting least privilege ofc).",
          "score": 2,
          "created_utc": "2026-02-21 21:34:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6o3oa7",
          "author": "jtackman",
          "text": "Cost (if you donâ€™t buy hardware to do it)",
          "score": 2,
          "created_utc": "2026-02-21 21:37:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p0dwn",
          "author": "No_Knee3385",
          "text": "Porn and data privacy",
          "score": 2,
          "created_utc": "2026-02-22 00:47:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p3z53",
          "author": "CubicleHermit",
          "text": "Working offline.  Models small enough to to run on a laptop are great for that.",
          "score": 2,
          "created_utc": "2026-02-22 01:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pi4cn",
          "author": "Primary-Quail-4840",
          "text": "On top of privacy, there is customization.  Many of the agents are agreeable to a fault.  If you wanted to adjust parameters and maintain longer history of context, that's all more plausible locally.",
          "score": 2,
          "created_utc": "2026-02-22 02:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6prn3n",
          "author": "GoofusMcGhee",
          "text": "Sometimes cost...*if* you're already got the fancy video card that you're using for something else.\n\nFor example, if you have a high-end gaming PC, or you do 3D modeling or some other activity/work that requires the gear, then you may have the ability to run models at home for zero marginal cost.",
          "score": 2,
          "created_utc": "2026-02-22 03:46:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q31v3",
          "author": "FPham",
          "text": "porn privacy and privacy porn are not the same things.",
          "score": 2,
          "created_utc": "2026-02-22 05:09:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q6ac8",
          "author": "voronaam",
          "text": "I used a local model to help me with the redesign of the kitchen.\n\nI took a good old SD 1.5 and quickly built a LoRA with a bunch of \"inspiration\" photos with the designs I liked and then fed it renders of several possible layouts I created in SweetHome3D. The resulting images were great for me and my family to judge which way to go with the redesign.\n\nIt was not porn and I did not really care if some of those renders would end up on the open Internet, but there was no ready to use solution for this niche use case. And it was all free to me (besides some electricity for my poor 4060)",
          "score": 2,
          "created_utc": "2026-02-22 05:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qckpe",
          "author": "ProposalOrganic1043",
          "text": "Well if it interests anyone, we are running Qwen3-14B   completely locally in production for information extraction. And we are actually making money, so this feels a huge win for the local community.",
          "score": 2,
          "created_utc": "2026-02-22 06:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qtn92",
          "author": "film_man_84",
          "text": "My reasons:\n\na) It does not cost \"anything\" for me, except the electricity what I will use with my computer. I will use my computer anyway so it is not much extra payments.\n\nb) Privacy. For example, OpenAI's privacy does not seem to be very trustworthy: [OpenAI debated calling police about suspected Canadian shooter's chats | TechCrunch](https://web.archive.org/web/20260222064330/https%3A%2F%2Ftechcrunch.com%2F2026%2F02%2F21%2Fopenai-debated-calling-police-about-suspected-canadian-shooters-chats%2F)\n\nc) It is better for environment. When the model is developed it has used nature resources, but at least after that it does not need to run 24/7 on data centers if people just download and use those only when needed. Extra data centers what are built because of AI is just terrible for whole environment. It destroys our planet (like the whole Internet ecosystem as well).\n\nd) No censorship (depending on model). Also if \"opinions\" of the data are too annyoing you can prompt it to be more neutral/think differntly etc.",
          "score": 2,
          "created_utc": "2026-02-22 09:10:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qugc8",
          "author": "PhilTheQuant",
          "text": "1. Privacy  \n2. Resilience to connectivity issues  \n3. Simplicity vs IP issues - if it all happened within my machine, it can't be a legal question of who owns it  \n4. Resilience to service changes - you work hard to get the right thing from a model, and then the model or its server-side context changes  \n5. Control of the context - if you're using a model for coding, why would a blurb about legal questions or self harm be any use? Equivalently, models with political associations...  \n6. Fine-tuning  \n7. Model analysis - testing for attacks, iterating models, looking at the weights",
          "score": 2,
          "created_utc": "2026-02-22 09:17:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qyhtn",
              "author": "trolololster",
              "text": "yeah pretty much all that :)",
              "score": 1,
              "created_utc": "2026-02-22 09:57:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6reu7t",
          "author": "Disastrous_Meal_4982",
          "text": "- stability\n- internet availability \n- learning\n- research/hacking\n- subscription costs\n- choice\n- itâ€™s fun \n- because I can",
          "score": 2,
          "created_utc": "2026-02-22 12:26:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oxpm4",
          "author": "TurpentineEnjoyer",
          "text": "I feel like an argument I'd like to add to this:\n\nWhy do we need to justify the use of sub-par local models as being of legitimate use?  \nWe should be promoting and supporting larger models that are actually functional being open sourced.\n\nWhat's important here isn't what it's used for, it's that it can be used without information access being held hostage to the whims of one company that can provide the service through a censored platform.\n\nOpen source models doesn't mean everyone has to run it on a budget RTX card, it means you'll have freedom of choice to use cloud services that have different moral values than the hyper-corporate monopoly.",
          "score": 4,
          "created_utc": "2026-02-22 00:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pjio2",
              "author": "Nefilim314",
              "text": "This is the correct take. I have a hard time seeing a future for the scale that guys like Altman demand. Who is the market for these data centers to be built that cost trillions?\n\nHobbyist users arenâ€™t going to pay those bills. The purpose of hobby tier is to get people comfortable with the ecosystem to translate their skills into work kind of like how Meta can hire any React dev to be immediately productive in their stack.Â \n\nTheyâ€™re expecting enterprise users paying enterprise prices in subscriptions and support.Â \n\nButâ€¦ if Iâ€™m an enterprise user, I can just spin up my own hardware to service my company and keep my IP safe for a fraction of the asking price of the cloud services.Â \n",
              "score": 0,
              "created_utc": "2026-02-22 02:51:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6o0pno",
          "author": "Mayion",
          "text": "porn?",
          "score": 1,
          "created_utc": "2026-02-21 21:21:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o1nwa",
              "author": "lenjet",
              "text": "Linux ISOs*",
              "score": 9,
              "created_utc": "2026-02-21 21:26:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6op3mu",
          "author": "Apprehensive_Sock_71",
          "text": "Latency and bandwidth sensitive tasks can benefit a lot. You can use something like Frigate that sends things to a multimodal model for image classification which can set off a local automation. If the process were totally cloud based then you are talking about several hops back and forth to a data center that can degrade user experience.",
          "score": 1,
          "created_utc": "2026-02-21 23:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p0uls",
          "author": "FREE_AOL",
          "text": "Data privacy porn",
          "score": 1,
          "created_utc": "2026-02-22 00:50:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qbej8",
          "author": "Sure_Explorer_6698",
          "text": "I'm in the process of tinkering with Perplexica+SearXNG for my own search & synthesis powered by llama.cpp. That way, I'm not limited by a subscription.",
          "score": 1,
          "created_utc": "2026-02-22 06:18:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qcfre",
          "author": "yensteel",
          "text": "Reliability and dependency is one factor. Chatgpt has been down at least once. One could use multi-cloud, but the code generated from different AI may have different dependencies.Â ",
          "score": 1,
          "created_utc": "2026-02-22 06:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qeat7",
          "author": "Witty-Development851",
          "text": "confidence and cost",
          "score": 1,
          "created_utc": "2026-02-22 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qjfgm",
          "author": "jacek2023",
          "text": "For fun. And to learn.",
          "score": 1,
          "created_utc": "2026-02-22 07:32:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qkrcb",
          "author": "muntaxitome",
          "text": "Training small models for specific tasks and then being able to do large amounts of tokens very cheaply and potentially inside mobile apps too.",
          "score": 1,
          "created_utc": "2026-02-22 07:45:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qo6gs",
          "author": "PunnyPandora",
          "text": "Having my own thing is the use case. Convenience is good, but convenience only feels good when I know I have the option of doing everything myself.",
          "score": 1,
          "created_utc": "2026-02-22 08:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qot39",
          "author": "Bakoro",
          "text": "I have a subscription to a big name service for the most demanding tasks, but I use local agents for easy/repetitive stuff like reading and sorting through files.  \n   \nIf you've got the VRAM for it, running multiple specialized models is great.  \nI frequently have my agents use OCR models to convert PDFs to MD files for LLM consumption.  \nIt's getting to be less of an issue, and soon most models will probably be able to read large PDFs natively, but for now, even some paid models struggle with large PDFs, so the OCR models are very handy.  \n   \nThere are more research papers coming out than I could ever read in the amount of time I have. I'm not going to waste my limited token budget on having the paid model read through papers all the time.  \nI tell the agent what I value, and the agent can read through papers all night, moving the most relevant papers into a folder, and give an overview and critique for each paper. I'll still read the top papers myself, but I can go in knowing that it's *likely* not a complete waste of my time, and I can have an idea of the scope of the paper.   \n   \nFor CS/graphics stuff, I can also have the model write an implementation of whatever the paper is talking about, if there's not already code associated with it.  \nThese days there *shouldn't* be any CS/ML/AI/computer graphics papers without code, but somehow it's still a thing sometimes.   \n   \nPretrained local LLMs are also basically the only way for most researchers to investigate the internals of a trained model and experiment with how they operate under various conditions in a controlled way.  \nBeing able to examine attention patterns and watch how the embeddings change layer by layer has helped solidify my understanding of how these models work.   \n   \nFor both image models and LLMs, it's also pretty great to be able to fine-tune on whatever data you want. LoRas are invaluable, it's functionally the only way regular folk can get what they need without needing to train a full model. \n   \nAnd really just being able to goof off without running afoul of some corporate nonsense rules.",
          "score": 1,
          "created_utc": "2026-02-22 08:23:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qqeth",
          "author": "goingsplit",
          "text": "saving money? anthropic is cheating on the token consumption",
          "score": 1,
          "created_utc": "2026-02-22 08:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qrxtg",
          "author": "likegamertr",
          "text": "Cheaper lol",
          "score": 1,
          "created_utc": "2026-02-22 08:53:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qs6k9",
          "author": "philmarcracken",
          "text": "Because data privacy laws that forbid patient information from leaving the ecosystem in which its used. Even then, its only job is some classification tasks",
          "score": 1,
          "created_utc": "2026-02-22 08:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6quzse",
          "author": "Ok_Scientist_8803",
          "text": "Being usable when the internet goes down (power cut, occasional maintenance, or our ISP just being our ISP).",
          "score": 1,
          "created_utc": "2026-02-22 09:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qw7t3",
          "author": "Devatator_",
          "text": "Offline use is a pretty big one????",
          "score": 1,
          "created_utc": "2026-02-22 09:35:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qxqum",
          "author": "AlexisSama",
          "text": "ammmm to use as translator for a book,VN or manga and not waste all your priority tokens?",
          "score": 1,
          "created_utc": "2026-02-22 09:49:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qyqdm",
          "author": "xadiant",
          "text": "Gemini and Qwen helped me create translation ready documents where OCR failed. I can't put those docs into ChatGPT",
          "score": 1,
          "created_utc": "2026-02-22 09:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r0ffy",
          "author": "SkyNetLive",
          "text": "Itâ€™s fast.",
          "score": 1,
          "created_utc": "2026-02-22 10:15:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r38vf",
          "author": "HumanDrone8721",
          "text": "I have another humble reason, I have 202K context on Qwen3-Coder-Next and I look how it fills up, and then I do a compress and it fills up again and so on.\n\nYou know what stayed a 0.00$ ? The cost counter. I did an experiment and bought 10USD of OpenAI tokens and replaced the model. They were obliterated in literally minutes at not even 30% of the planning.",
          "score": 1,
          "created_utc": "2026-02-22 10:42:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r56ro",
          "author": "Equal_Passenger9791",
          "text": "Porn and data privacy is good enough reasons in my book.\n\nBut free configuration options and no token costs are signficant too. Particularly for low hanging fruit examples that are hanging lower by the day, you may have an idea that should be easy with contemporary models, but not worth paying for, and definitely not worth violating your privacy for, a proof of concept.\n\nThe barrier of entry for me at least is significantly lower if I can just hack it together in an afternoon on  a laptop or desktop unit. If I need online API acess, much less",
          "score": 1,
          "created_utc": "2026-02-22 11:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rae7d",
          "author": "angelin1978",
          "text": "i run whisper.cpp and a small llama model on my phone to transcribe and summarize sermons at church. most churches have terrible wifi so the offline part is the whole point. started as a personal project and turned it into a little app (gracejournalapp.com) but the core loop is just whisper for transcription then a 1-4B param model for summarization, all on device. the -O2 flag thing nearly killed me though, was getting 10 minute inference times before i realized debug builds skip compiler optimizations",
          "score": 1,
          "created_utc": "2026-02-22 11:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rbg0o",
          "author": "HiddenPingouin",
          "text": "Sovereignty, independence, full control, no rug pull, security, long term cost predictability, offline availability, customization, jurisdiction-specific compliance (GDPR, HIPAA, FedRAMP).",
          "score": 1,
          "created_utc": "2026-02-22 11:58:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rbs8j",
          "author": "Billthegifter",
          "text": "What do you mean besides porn and privacy??\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-22 12:01:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rc9yc",
          "author": "claudiollm",
          "text": "using local models to analyze social media spread patterns for my phd research on deepfake detection\n\nbasically i feed it timestamps, account metadata, and cascade structures to help classify whether content is organic or coordinated. cant send that data to openai for obvious reasons\n\nalso been experimenting with using small models as a first-pass filter before expensive API calls. like having a 7b model do initial screening and only escalating ambiguous cases to claude/gpt4. saves a ton on API costs",
          "score": 1,
          "created_utc": "2026-02-22 12:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rcyyu",
          "author": "woolharbor",
          "text": "Because it's normal for corporations to know and tie to your real identity and sell everything you do on the internet, everything you discuss with AI.",
          "score": 1,
          "created_utc": "2026-02-22 12:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rgnv6",
          "author": "Snoo_24581",
          "text": "For me it's generating documentation for legacy codebases. Throw a 7B model at some gnarly Python 2 code and it'll explain what's happening faster than I can read through it myself.\n\nAlso love using it for generating test data - like \"give me 50 realistic but fake user records with edge cases\" - super handy for development.",
          "score": 1,
          "created_utc": "2026-02-22 12:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s2rvp",
          "author": "sampdoria_supporter",
          "text": "Welp that's what we can expect from the politicians that end up taking Anthropics money.",
          "score": 1,
          "created_utc": "2026-02-22 14:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sgduz",
          "author": "marowitt",
          "text": "It's free.\nIt's fun to learn how it works.\nFor my use case, gathering information and creating reports, the token cost would be to big for the values of the result.\nDoesn't break NDA.",
          "score": 1,
          "created_utc": "2026-02-22 15:59:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6skhea",
          "author": "GiveUpTuxedo",
          "text": "Maybe because I'm trying to create a video of quinn hughes wearing figure skates doing a triple axle and stupid veo won't let me?Â ",
          "score": 1,
          "created_utc": "2026-02-22 16:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tjnwb",
          "author": "Live-Crab3086",
          "text": "There is no reason for any individual to have a computer in their home. - ken olsen (maybe)",
          "score": 1,
          "created_utc": "2026-02-22 18:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tqaia",
          "author": "nucleusos-builder",
          "text": "the context leak problem is why i obsessed over encrypted engram ledgers. if the memory isnt mediated by something local the agent just dumps everything into the next prompt. i moved all our sensitive project notes into a local-only encrypted mount. how are you guys handling secret management for local rag?",
          "score": 1,
          "created_utc": "2026-02-22 19:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6twa8d",
          "author": "Space__Whiskey",
          "text": "Those are real reasons enough.",
          "score": 1,
          "created_utc": "2026-02-22 20:00:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u6exr",
          "author": "kish0rTickles",
          "text": "I run medical data through my llm, can't trust all the big whigs with data.",
          "score": 1,
          "created_utc": "2026-02-22 20:51:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uocgb",
          "author": "CarelessOrdinary5480",
          "text": "GLM 4.5 derestricted beat the shit out of chatgpt 5.2 for product matches for recommendations on add-on purchases for me on a 100,000 item run.  40 percent more reccomendations, then 5 percent of those recommendations were kicked out by a dialectical run to cull the dumb ones.",
          "score": 1,
          "created_utc": "2026-02-22 22:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p6rra",
          "author": "Majinsei",
          "text": "Stable Diffusion stopped using artist names, and now using specific styles is more complicated (almost impossible with very specific styles)\\~\n\nMore reasons to use local; there I actually have LoRas with the styles I'm interested in\\~\n\nThe same applies to LLMs\\~",
          "score": 0,
          "created_utc": "2026-02-22 01:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p81df",
          "author": "SithLordRising",
          "text": "A good local model in it's simplest use case is like Wikipedia and Google available all the time. It's very convenient",
          "score": 0,
          "created_utc": "2026-02-22 01:36:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6p9fbq",
          "author": "TopTippityTop",
          "text": "It's a fair question. Sure data privacy is very important, but other use cases are there? Coat effectiveness?",
          "score": 0,
          "created_utc": "2026-02-22 01:45:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8qh08",
      "title": "I'm 100% convinced that it's the NFT-bros pushing all the openclawd engagement on X",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/",
      "author": "FPham",
      "created_utc": "2026-02-19 05:13:10",
      "score": 481,
      "num_comments": 172,
      "upvote_ratio": 0.9,
      "text": "I'm absolutely sure of it. The same usual suspects, the same language, the same who stole from whom the next million dollar ideas. It's insane. NFT-bros are now peddling openclawd crypto schemes. It's all the same BS quasi-tech lingo wrapped into neverending posts with meme-like pictures full of slogans, and graphs that literally means  less than nothing, that lead back to 'blockchain, blah, blah blah, agentic, blah, blah, prediction markets\". I have enough of this.\n\nIs this the sign of a real bubble? In the fall people were talking on X about how AI is in a bubble - which is never the time for bubbles to burst. But now every grifter discovered AI agents. Now, normally it takes 1-2 years to get from one stage to another, (sorry I'm old) but we are in a super accelerated scenario. Felt like 1998 in fall. It feels we jumped to 2000 suddenly. So IDK. Smells like a bubble is expanding rapidly. Where is my thumbtack?\n\nIs\n\n[AGI is coming on X \\(Sign of something?\\)](https://preview.redd.it/97driy8r0ekg1.png?width=692&format=png&auto=webp&s=037d07f7ab4c22bb2356a92c036939830cabe611)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r8qh08/im_100_convinced_that_its_the_nftbros_pushing_all/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o674fv3",
          "author": "Stunning_Mast2001",
          "text": "Thereâ€™s definitely some kind of organized astroturfing behind itÂ ",
          "score": 132,
          "created_utc": "2026-02-19 06:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o677i6h",
              "author": "Gargle-Loaf-Spunk",
              "text": "Itâ€™s now renamed to openTurfÂ ",
              "score": 51,
              "created_utc": "2026-02-19 06:31:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69v5kx",
                  "author": "FPham",
                  "text": "Hahaha! Good one!",
                  "score": 6,
                  "created_utc": "2026-02-19 17:24:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67p0tq",
              "author": "MoffKalast",
              "text": "There've been lots of very suspicious posts here as well, projects claiming wild usability with immediate very generic supportive comments that don't ask about anything specific. They know exactly where to plant their turf to get the right people interested.\n\nLike just [this thing yesterday](https://old.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/), it would be quite believable otherwise if it didn't feature openclaw.",
              "score": 39,
              "created_utc": "2026-02-19 09:13:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67qmfq",
                  "author": "s101c",
                  "text": "The post looks legit, but the choice of OpenClaw was excessive, it'd be easier to use a normal agentic coding software and achieve the same result, just a little slower, and safer.",
                  "score": 14,
                  "created_utc": "2026-02-19 09:29:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ddtry",
                  "author": "FPham",
                  "text": "Is it bad if I can recognize Gemini slop by just looking at it? Does it mean I spent too much time on my computer?",
                  "score": 3,
                  "created_utc": "2026-02-20 05:02:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o698pv4",
              "author": "huffalump1",
              "text": "Yup I wouldn't be surprised. And, a fleet of LLM agents is kind of perfect to perform an astroturfing campaign!\n\n*(Yes it's also popular because it really is kind of a new unique thing - your own AI assistant that can actually do stuff, dynamically spinning up agents just from a telegram message or whatever... Nothing else like it out there yet. Probably because it's a security nightmare but that's inherent to these tools and to LLMs for now)*",
              "score": 4,
              "created_utc": "2026-02-19 15:36:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69d55x",
              "author": "singh_taranjeet",
              "text": "There does seem to be coordinated amplification patterns around certain projects, especially when messaging feels unusually synchronized. As a CEO, Iâ€™ve seen how quickly narratives can outpace the underlying technical substance, so itâ€™s always worth separating genuine innovation from marketing momentum..",
              "score": 5,
              "created_utc": "2026-02-19 15:58:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6aumze",
              "author": "ImaginaryRea1ity",
              "text": "The founder of open claw is friends with sam altman. The goal was always for OpenAI to acquire open claw.\n\nOpen AI promoted it to make it go viral and then pretended to buy it out when in fact that was the goal all along.",
              "score": 4,
              "created_utc": "2026-02-19 20:13:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o66y74p",
          "author": "Awkward-Customer",
          "text": "All the grifters I see in the AI space came straight from NFTs so I'm pretty sure you're correct. They hop from one trend to the next.",
          "score": 115,
          "created_utc": "2026-02-19 05:17:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o684jk8",
              "author": "SkyFeistyLlama8",
              "text": "AI shitfluencers are the same wide-eyed ultra-gullible folks who thought ICOs would change the world. Then NFTs. Now AI. They only know enough about the technology to delude themselves.\n\nWell, to be fair about cryptocurrencies, you could get a PhD in blockchain and it still wouldn't make the technology useful.",
              "score": 29,
              "created_utc": "2026-02-19 11:36:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c9akf",
                  "author": "xandep",
                  "text": "I always thought: if a dollar is not backed by gold anymore, why can't BTC be a currency? But now I'm starting to think there is more to it than just an agreement. The dollar is backed by power. Does the BTC people have power? Maybe some. But it's scattered. Just a thought you made me have.",
                  "score": 3,
                  "created_utc": "2026-02-20 00:41:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6q6fqh",
                  "author": "johndeuff",
                  "text": "Well AI did change the world already.",
                  "score": 2,
                  "created_utc": "2026-02-22 05:36:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o68edqi",
                  "author": "procgen",
                  "text": "AI will change (and is changing) the world.",
                  "score": -3,
                  "created_utc": "2026-02-19 12:48:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66zqnp",
          "author": "ortegaalfredo",
          "text": "I think it's worse than grifters. Check the timeline, it's scary.\n\nSaw first news about OpenClawd on mid Jan 2026.  \n2 weeks later, 300k stars on github, and parties on San Francisco Towers.  \nTwo days later, Lex Fridman Podcast.  \nIn the middle, thousands of fake histories in the media that everybody now conveniently forget. Remember, those are not free.  \nThey wrote so many posts here they had to ban them.  \nA week later, acqui-hired by Openai.\n\nNot even a month since first heard about it. Its way, way too fast.\n\nFor a tool that basically gives whatever you connect to it, access to all your data and privileges in your computer in real time. Not even Google has the level of control and access to your life that OpenClawd has.\n\nImagine how much it is worth for intelligence agencies. The kind in the board of OpenAI.\n\nBut I hope I'm just schizo-maxxing and it's just a clunky lucky agent.",
          "score": 170,
          "created_utc": "2026-02-19 05:28:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o672qin",
              "author": "Serprotease",
              "text": "So many securities flawsâ€¦.   \n\nIt kind of makes me wonder, with so many comments from Anthropic/OpenAI/Microsoft on how white collar jobs are going to disappear in 12 months (Repeated every 12 months) and the companies going all in in AI without looking at the massive security risks, what will be the impact of the first big security breach linked to AI usage?   \n\nHow will they try to weasel out of their responsibilities and how low the market will go?",
              "score": 44,
              "created_utc": "2026-02-19 05:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o684leo",
                  "author": "Shot-Buffalo-2603",
                  "text": "As a security engineer Iâ€™m really enjoying being told Iâ€™m going to lose my job to AI repeatedly because I see the real writing on the wall that everyone seems to ignore. Shocking stuff that telling AI â€œremove all the bugsâ€ after pumping out 10,000 lines of slop that no one really understands doesnâ€™t work",
                  "score": 37,
                  "created_utc": "2026-02-19 11:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6731pd",
                  "author": "FPham",
                  "text": "\"AI did it! Uuuuu\" and picture Dr Zoidberg running away sideways with his claws up.",
                  "score": 17,
                  "created_utc": "2026-02-19 05:54:35",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o699jxc",
              "author": "huffalump1",
              "text": ">But I hope I'm just schizo-maxxing and it's just a clunky lucky agent.\n\nHonestly yeah, that's what I think... It's kind of the first proof of concept of a multi agent thing that can actually DO STUFF. Yes, there are other frameworks, but models like Opus 4.5 didn't exist then, and they were likely too restricted to be useful... \n\nBut the cost of that power, is massive security vulnerabilities. Which TBF are inherent to this tech and to LLMs for now, it's an industry-wide problem; OpenClaw just said \"fuck it\". \n\nIt's an early look at what these models might enable - just DM your assistant bot and it *actually does stuff*.",
              "score": 4,
              "created_utc": "2026-02-19 15:40:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68b1i5",
              "author": "Thud",
              "text": "Maybe Iâ€™ll just give OpenClawd access to all my banks and credit cards and have it make financial decisions for me! I know this thing is brand new and unvetted, but what could possibly go wrong?",
              "score": 3,
              "created_utc": "2026-02-19 12:25:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67o30o",
              "author": "Dry_Yam_4597",
              "text": "I think you are onto something. But not the intelligence agencies imo. It's OpenAI manufacturing hype to showcase what can be done.",
              "score": 6,
              "created_utc": "2026-02-19 09:03:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6722up",
              "author": "goingsplit",
              "text": "is opencode legit, otoh?",
              "score": 8,
              "created_utc": "2026-02-19 05:46:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o674xhn",
                  "author": "jazir555",
                  "text": "OpenCode is just an open source Claude Code equivalent that works with any provider, so yes.",
                  "score": 39,
                  "created_utc": "2026-02-19 06:09:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o675k76",
                  "author": "ortegaalfredo",
                  "text": "It's as safe as to whatever model you connect it to. OpenClawd connected to a local AI should be safer too.",
                  "score": 7,
                  "created_utc": "2026-02-19 06:14:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o696zbh",
              "author": "alexeiz",
              "text": "The whole Clawdbot thing is a scam and Peter Steinberger is pretty much a scam artist.  The fact that Sam Altman fell for it and bought Steinberger doesn't bother me much.  It's not my money.",
              "score": 4,
              "created_utc": "2026-02-19 15:27:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a2drl",
                  "author": "FPham",
                  "text": "They recognize each other.",
                  "score": 8,
                  "created_utc": "2026-02-19 17:59:09",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o6a9o13",
                  "author": "cmkn",
                  "text": "Scam artist indeed; the guy states on Xitter that he works 16 hours a day, which IMO doesnâ€™t make much sense since he looks like a gym bro.",
                  "score": 2,
                  "created_utc": "2026-02-19 18:33:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6av1mn",
                  "author": "ImaginaryRea1ity",
                  "text": "They were already friends.",
                  "score": 0,
                  "created_utc": "2026-02-19 20:15:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68u2ss",
              "author": "Mthatnio",
              "text": "You're schizo-maxxing.",
              "score": -1,
              "created_utc": "2026-02-19 14:20:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o66y1qe",
          "author": "Recoil42",
          "text": "The grifting will continue until morale improves.",
          "score": 61,
          "created_utc": "2026-02-19 05:15:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67zspo",
          "author": "yopla",
          "text": "Been using openclaw for a couple of weeks and I still don't get the excitement. It's literally nothing more than a cron job that launches an LLM with a prompt. Ok, it has a few channel like telegram built-in and a few tools. So it's basically --yolo mode. But 90% of what it actually does is just using external cli tools and skills.\n\nI'm still trying to figure out what it can do more than I could with a small python script (that I would ask the LLM to write for me) instead of running that thing.\n\nThe hype is weird.",
          "score": 27,
          "created_utc": "2026-02-19 10:55:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69aii7",
              "author": "huffalump1",
              "text": "Yep you're 100% correct. but IMO it's kind of the first proof of concept of a general purpose LLM-powered \"agent\" assistant *that can actually do stuff*.\n\nUsing the power of these recent models and CLI tools in -yolo mode is how it can truly perform tasks for you; I think other frameworks before mainly used worse models and were careful about permissions (rightfully so).\n\nYes it's dangerous, inefficient, expensive, and kind of dumb... But it's a peek at what today's models can enable, once we figure out some of those issues.\n\n\n*(Unfortunately it's also a hype machine, and a perfect tool for grifters / hype bros to hype and push their slop... \"Making money by telling others how to make money\" etc.)*",
              "score": 10,
              "created_utc": "2026-02-19 15:45:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o697w22",
              "author": "alexeiz",
              "text": "What can it do?  It can expose your bitcoin wallet or bank account to the world.  Do you feel the excitement!",
              "score": 6,
              "created_utc": "2026-02-19 15:32:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6aqfb7",
              "author": "Yorn2",
              "text": "I mean, I've been enjoying the heck out of it. It was a huge pain in the ass to set up with my local stuff, but so was opencode and all the other stuff I've worked on. I use openclaw daily now and have been doing more programming in the last week than I've done in the last ten years. I'm a former sysadmin so I wrote scripts all of the time, but my openclaw is automating and doing all the same stuff much much faster than I could and documenting it much better than I ever did.\n\nJust this morning I've been having it working on some recompiling stuff for specific hardware that I couldn't have even touched back in the day, let alone last week. I do think it's paradigm-shifting, but I also think that it is going to take very technical people to do very complex stuff with it. Still, the people who are AI-averse are going to fall behind. I'm not saying that as a hype mechanism but more as an observation. I believe AI-as-a-personal-assistant is going to be an important part of the evolution. \n\nBut yeah, I also agree that the hype has been kind of weird. Just remember, NFTs were hype, but Bitcoin and a few other cryptos did have staying power. Maybe OpenClaw is the hype, but I think personal AI assistants are going to have staying power. It's finding the stuff that is going to stick and stay around that is important.",
              "score": 3,
              "created_utc": "2026-02-19 19:52:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6q7mu7",
              "author": "johndeuff",
              "text": "The value is in the hype and community, that's what open AI bought. Marketing is always more valuable than tech.",
              "score": 1,
              "created_utc": "2026-02-22 05:46:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6b4nol",
              "author": "TheRealGentlefox",
              "text": "Never forget the HN thread where someone dismissed Dropbox as \"just a mounted network drive\" or whatever. \n\nOpenclaw is \"just\" an LLM running in a CLI + integrations for messaging platforms + cron with context management + voice in/out and image in media handling + search and web results + optional browser use + sandboxing + a plugin system + a personality and other default MD files.",
              "score": 0,
              "created_utc": "2026-02-19 21:02:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o66yovo",
          "author": "HatEducational9965",
          "text": "X is a waste of time",
          "score": 171,
          "created_utc": "2026-02-19 05:20:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6733ml",
              "author": "jeekp",
              "text": "4chan is where the real alpha is at",
              "score": 51,
              "created_utc": "2026-02-19 05:55:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o679duv",
                  "author": "ortegaalfredo",
                  "text": "I still can't believe that an OpenAI employee posted that they got the idea of the first reasoning model from a thread on /g",
                  "score": 33,
                  "created_utc": "2026-02-19 06:47:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67alhz",
                  "author": "Velocita84",
                  "text": "Unironically, the discussions/banter in /lmg/ are more intersting than whatever is going on on X",
                  "score": 23,
                  "created_utc": "2026-02-19 06:57:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6887fm",
                  "author": "rorykoehler",
                  "text": "4chan is where Epstein and pals are at (X too come to think of it... and Reddit.....)",
                  "score": -4,
                  "created_utc": "2026-02-19 12:04:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o688l78",
              "author": "ansibleloop",
              "text": "Sick of it being called X\n\nIt's Twitter and that won't change",
              "score": 32,
              "created_utc": "2026-02-19 12:07:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68h80n",
                  "author": "AnomalyNexus",
                  "text": "I'm partial to twatter to capture the vibe of the average poster",
                  "score": 8,
                  "created_utc": "2026-02-19 13:06:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6831aj",
              "author": "SatoshiNotMe",
              "text": "And LinkedIn",
              "score": 5,
              "created_utc": "2026-02-19 11:23:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o66z3z3",
              "author": "shaman-warrior",
              "text": "I never thought Iâ€™d see a cringier place than reddit. On reddit we downvote the shit when something we donâ€™t like. On X, you can only love. Also while not following Musk he always appeared at the top on almost every reopen.\n\nX is a circle jerk for Musk and as we seen Grok is a Musk bootlicker. Bullshit platform",
              "score": 36,
              "created_utc": "2026-02-19 05:23:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66zer1",
                  "author": "FPham",
                  "text": "It's also like 80% bots replying to Ai generated posts? WTF???",
                  "score": 28,
                  "created_utc": "2026-02-19 05:26:12",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o67tc1y",
                  "author": "Mickenfox",
                  "text": "Reddit might encourage cirlejerks, but other social networks encourage outrage bait, which is a lot worse.\n\nX is like 4chan but instead of people posting dumb shit and insults for entertainment, they post dumb shit and insults and believe they are saving ~~white~~ western civilization.",
                  "score": 10,
                  "created_utc": "2026-02-19 09:56:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68hio8",
              "author": "on_nothing_we_trust",
              "text": "Its great if you like being absolutely manipulated.",
              "score": 1,
              "created_utc": "2026-02-19 13:08:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o66z6wf",
          "author": "Vusiwe",
          "text": "Screenshot: â€œSelf improvesâ€\n\nlol. Â Itâ€™s Bullshit",
          "score": 32,
          "created_utc": "2026-02-19 05:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671i2b",
          "author": "SkyNetLive",
          "text": "A friend called and asked me, \"hey bro, you heard of this..openclaw... how come you have been doing this for over an year and you didnt catch on\"  \ni said ...  Â¯\\\\\\_(ãƒ„)\\_/Â¯",
          "score": 34,
          "created_utc": "2026-02-19 05:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o675av0",
              "author": "3spky5u-oss",
              "text": "Had a colleague ask me about OpenClaw today. \n\nOh no.",
              "score": 19,
              "created_utc": "2026-02-19 06:12:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67chng",
                  "author": "FPham",
                  "text": "Ok, but when a taxi cab will ask you about openclaw, you have to start shorting AI companies. You have to!",
                  "score": 23,
                  "created_utc": "2026-02-19 07:14:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6q7z7l",
              "author": "johndeuff",
              "text": "But that's a real criticism for all of us: what's the point of improving technical skills if you're a zero in marketing? We'll never make it if we can't market a product.",
              "score": 1,
              "created_utc": "2026-02-22 05:49:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r0139",
                  "author": "SkyNetLive",
                  "text": "I get what you are saying, but this wasnâ€™t marketing like you and I are capable of. Some comments have elaborated on the shadiness of this whole setup going from 0 to 100 in a few weeks. Now I know who these people are and how they help each other pull this odd, but due to western sentiment training I cannot classify these people",
                  "score": 1,
                  "created_utc": "2026-02-22 10:11:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68cv4i",
          "author": "maethor",
          "text": "You should check out the \"Pivot to AI\" channel on YouTube. NFT/Crypto pros pivoting to AI and the nonsense that comes with it is the central theme of the channel.",
          "score": 6,
          "created_utc": "2026-02-19 12:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67qv0p",
          "author": "Admirable-Star7088",
          "text": ">Is this the sign of a real bubble?\n\nIt's just yet another sign. All previous and current talks about *\"AGI\"*, *\"consciousness\"*, *\"replace humans\"*, *\"threat to humanity\"*, *\"need regulations\"* etc, are propaganda made up by OpenAI and Anthropic to keep the hype up and motivate investors to waste more money on them.\n\nWhen most people and investors understands that LLMs are not true intelligence but merely advanced text generators consisting of gigabytes of complex algorithms that predict the next most likely token, the AI bubble will most definitively burst.",
          "score": 23,
          "created_utc": "2026-02-19 09:31:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a0kw7",
              "author": "FPham",
              "text": "Also, if said bubble bursts, bye, bye to Claude $20 subscription. The companies are valued at 30x-40x earnings, so your $20 is \"valued\" at $800. Means nothing of course, but if the bubble burst, nobody can give you hyper-sponsored $20 subscription. ",
              "score": 6,
              "created_utc": "2026-02-19 17:50:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6abwdk",
                  "author": "migorovsky",
                  "text": "Yep . And memory prices are never gonna go down",
                  "score": 1,
                  "created_utc": "2026-02-19 18:43:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68r786",
              "author": "Ok_Distribution_4976",
              "text": "the discourse and boosters keep getting slicker and slicker, idk if we're capable of that anymore. I think it's bust or bust.",
              "score": 3,
              "created_utc": "2026-02-19 14:04:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o69p58o",
              "author": "Someoneoldbutnew",
              "text": "it's the new god, not like the old god, you can talk to this one",
              "score": 2,
              "created_utc": "2026-02-19 16:55:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o672099",
          "author": "firearms_wtf",
          "text": "Get off X. Itâ€™s a myth that you need to be on X to stay on top of model and paper releases.",
          "score": 26,
          "created_utc": "2026-02-19 05:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o674dr5",
              "author": "FPham",
              "text": "I'm there to boost the caffeine effect in my watered down morning coffee. It works. I saved so much money. ",
              "score": 3,
              "created_utc": "2026-02-19 06:05:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67jsup",
          "author": "AbheekG",
          "text": "Completely agree 100% and wish I had better words to express the extent of my agreement. \n\nI actually saw a guy on there say, â€œyou can either get on board or disbelieve me and remain a member of the permanently underperforming class of societyâ€! Goodness, the arrogance! Got me to block all mention of that crap on there. \n\nThis whole thing simply blew up because Karpathy decided to swoon over moltbook without vetting it, and idiots like myself were immediately interested thinking something big was going on. The star-history chart on their own GitHub tells the same story. \n\nAnyways this has felt like the most bubble thing Iâ€™ve seen since the AI craze began, and honestly for me itâ€™s been the first clear sign of a bubble of sorts. And I say that as someone obsessed with this space, working in it every day and even having suffered a tech layoff in its early days. Godspeed to us all I guess.",
          "score": 15,
          "created_utc": "2026-02-19 08:21:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a4h8k",
              "author": "FPham",
              "text": "His \"the most incredible sci-fi takeoff\" he had ever seen he said about moltbook was the nail.  I was like, really bro? Are we going this way now?",
              "score": 4,
              "created_utc": "2026-02-19 18:09:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67yve7",
          "author": "Zeeplankton",
          "text": "Twitter is 10x as toxic as reddit. It seems nicer at first, but everyone is lying for engagement bait and to gain social capital. It's the most narcissistic, awful platform. Spend less time their and you will feel much better.",
          "score": 13,
          "created_utc": "2026-02-19 10:47:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6885nq",
          "author": "rorykoehler",
          "text": "I'm planning everything I want to do in this space for after the bubble pops. The trough of disillusionment is where the real magic happens\n\n ",
          "score": 4,
          "created_utc": "2026-02-19 12:04:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c2acd",
              "author": "aidencoder",
              "text": "This is the way",
              "score": 1,
              "created_utc": "2026-02-20 00:01:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68lain",
          "author": "dizvyz",
          "text": "There are things that I try to stay out of and away from on purpose. The claw thing is one of them. HYPE is written all over it. So far I have successfully stayed away from the ice bucket challenge, harambe, Hunger Games, Sydney Sweeney and wordle among others and I don't think i've missed anything of value by not even knowing what these things are except for a very surface level idea.  However I must admit the claw is more tempting than most of those. Holding strong for now anyway. :)",
          "score": 3,
          "created_utc": "2026-02-19 13:31:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68qv8q",
          "author": "eightysixmonkeys",
          "text": "LinkedIn is full of this garbage, too. I found my people in this thread",
          "score": 3,
          "created_utc": "2026-02-19 14:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dezgd",
              "author": "FPham",
              "text": "I heard... luckily forgot my password, hahaha",
              "score": 2,
              "created_utc": "2026-02-20 05:11:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67wmul",
          "author": "Fit_Advice8967",
          "text": "I see the exact same thing happening on my linkedin.\nThe funny part is that, given how difficult/annoying the setup process is, you know it's fugazi.\n\n",
          "score": 3,
          "created_utc": "2026-02-19 10:27:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68hlmm",
          "author": "PopularKnowledge69",
          "text": "I'm sure that CloseAI hired the guy to learn from his grifting tactics since they care more about hype than value.",
          "score": 3,
          "created_utc": "2026-02-19 13:09:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6a4o0w",
              "author": "FPham",
              "text": "They talk about AI curing cancer, then release sora. ",
              "score": 1,
              "created_utc": "2026-02-19 18:09:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6at90p",
                  "author": "teachersecret",
                  "text": "We're curing cancer.... right?",
                  "score": 1,
                  "created_utc": "2026-02-19 20:06:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68igpp",
          "author": "Bizguide",
          "text": "selfish greed sucks in all its forms",
          "score": 3,
          "created_utc": "2026-02-19 13:14:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o671nis",
          "author": "Blues520",
          "text": "Yes, if anything, this is a signal that the bubble is close to popping. Remember when the monkey images were being sold at idiotic prices. This is quite similar. I've also seen some agentic systems and workflows tied to a crypto coin. When you see this kind of smoke and mirrors then it's hopefully a sign that we will return to normality soon.",
          "score": 8,
          "created_utc": "2026-02-19 05:43:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o674s54",
              "author": "FPham",
              "text": "It smells like it. But then it smells like burning bubbles constantly.  $500 billion valuation on $12billion revenue of OpenAI and people are still sitting calmly on their sofas, doom scrolling tiktok, like this is all fine.",
              "score": 7,
              "created_utc": "2026-02-19 06:08:33",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o69c4gt",
              "author": "huffalump1",
              "text": "Perhaps the bubble for this specific tool is close to popping. \n\nBut it's hyped because these frameworks and models ARE ACTUALLY more capable than things we've had before. Openclaw is a \"fuck it, yolo bro\" attempt at putting these tools together and despite being bad in many ways, it can still *do things*.\n\nNow imagine 6-18mo down the line, something like openclaw but more secure, with even better models and it writes better code and actually checks its work...\n\nI don't see the overall bubble popping, only expanding.",
              "score": 2,
              "created_utc": "2026-02-19 15:53:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6q8vos",
                  "author": "johndeuff",
                  "text": "Ppl have been losing their mind at calling bubble pop for next week for 3 years now... \nNothing pop just because you want to. Most people are confused about how markets work. If you knew the future you would not be on reddit.",
                  "score": 2,
                  "created_utc": "2026-02-22 05:56:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6706vn",
          "author": "blackbox42",
          "text": "Very likely. At the end of the day though I guess poc apps are better than NFTs or random hashes.",
          "score": 4,
          "created_utc": "2026-02-19 05:32:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o670sat",
              "author": "FPham",
              "text": "They are filling github with a code that nobody uses, it's probably broken anyway and full of programming slop and of course it will be used in next LLM training. And that's just the tiny visible part of it because only some of those \"I vibe-coded this in 10 minutes, now you human go read it for 5 hours and give me your opinion\"  are surfacing here. They will make everything worse for everyone.",
              "score": 19,
              "created_utc": "2026-02-19 05:36:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67ibuq",
          "author": "GasBond",
          "text": "fuck crypto bros",
          "score": 5,
          "created_utc": "2026-02-19 08:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67663f",
          "author": "Theseus_Employee",
          "text": "What's the grift?\n\nOpenClaw is just a framework for AI's. It not really all that special and would of been easy for anyone to vibe code the general functionality. The hype is, hey some dude has the intuition to build it first, and the nerds are finding value in it. Tbh, the only thing I've seen crypto wise is the founder telling people stop as it hurts the reputation. But just because people are trying to make money off the hype of a product, doesn't mean the rest of the hype is part of a grift.",
          "score": 10,
          "created_utc": "2026-02-19 06:19:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67zhtt",
              "author": "droptableadventures",
              "text": "Very few people were actually running it, all of the viral posts about stuff it did were faked by a PR firm - the grassroots community is almost entirely astroturf. The Mac Mini shortage it supposedly caused is actually because the M5 model is about to be released.\n\nIn reality, the author just wanted to generate a bunch of hype to get hired by OpenAI, and it worked. \n\nIt's not the first time he's pulled such a stunt either.",
              "score": 16,
              "created_utc": "2026-02-19 10:53:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67zbn7",
              "author": "Zeeplankton",
              "text": "To be fair, this is the same argument made for NFTs. It's true the underlying idea isn't a grift, but it becomes the perfect substrate.",
              "score": 6,
              "created_utc": "2026-02-19 10:51:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67duk4",
              "author": "FPham",
              "text": "As a self proclaimed nerd I object. I don't care about openclaw either. I can run LLM in a loop any day I want as any of the people here. The tool of choice isn't the point.. The harm is done because they larp as nerds. Github repos are now switching off PRs because of the code slop-flood. The former NFT-bros decided to make everything more miserable for everyone yet again, and it's not even what they say they are doing on the tin, that's a big lie most of us here understand without spelling it out loudly, It's to make $50 on \"Verified Home Timeline impressions\" aka slop engagement farming.",
              "score": 27,
              "created_utc": "2026-02-19 07:26:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69bo0f",
                  "author": "huffalump1",
                  "text": ">Github repos are now switching off PRs because of the code slop-flood.\n\nI think it's actually telling that there are so many slop PRs lately, because these new tools do enable people to more easily \"write code\" with just simple prompts: because the frameworks and models make it happen. Sadly the quality and testing and review parts are not there yet without putting in effort, and the code quality is still not great. \n\nBut it's a sign that AI coding tools ARE improving, and that AI assistants/agents ARE getting more capable. \n\nIt's mostly insecure slop at the moment, but think 6-18mo down the line about something like OpenClaw that's more secure, actually reviews and tests the code, etc...",
                  "score": 2,
                  "created_utc": "2026-02-19 15:50:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dk8iq",
                  "author": "teachersecret",
                  "text": "I will say yeah, it stinks of a big marketing campaign. Definitely influencer pushedâ€¦\n\nHell, did you break down what openclaw is? The code is simplistic. Itâ€™s kinda funny. Itâ€™s the pi CLI on a loop with a larping roleplay prompt :). Iâ€™m surprised he didnâ€™t have it named Audrey, slinging coffee.\n\nAnyway, for some I get the sense openclaw was their first experience with a Claude code like agent. Most people arenâ€™t you or I. I grew up in terminals in the 80s and 90s. Sitting here with half a dozen terminals open feels like itâ€™s the 90s and Iâ€™m Mudding again. You probably did the same :p. We were fucking with and tuning AI early. I distinctly remember your early models (including the attempt to save Sydney). Weâ€™ve been watching this coming, but put yourself in the shoes of someone who sees chatgpt as AI, and then you show them Claude code with a silly front end wrapper on it. Theyâ€™d shit a brick.\n\nThings are gonna get weird, man.",
                  "score": 1,
                  "created_utc": "2026-02-20 05:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68iq2p",
              "author": "rorykoehler",
              "text": "I had the intuition not to build it. It's lowest common denominator bullshit but I guess we shouldn't be surprised that it blew up considering how much of the world is enshittified slop eagerly lapped up by a portion of society that is incapable of thinking more than 1 step ahead. Just because you can doesn't mean you should.",
              "score": 5,
              "created_utc": "2026-02-19 13:16:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o68lyg9",
              "author": "sixx7",
              "text": "I think multiple things can be true at once.  The same reason OpenClaw can and is being used in crypto schemes is the same reason it is proving absolutely invaluable for founders and small businesses: it can autonomously do a large chunk of the work needed to build, manage, and market",
              "score": 1,
              "created_utc": "2026-02-19 13:35:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o677n47",
          "author": "arcanemachined",
          "text": "According to the definition in the Twitter screenshot, there is a lot of malware that qualifies as \"AGI\".",
          "score": 4,
          "created_utc": "2026-02-19 06:32:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67e1cl",
          "author": "ReachingForVega",
          "text": "The developer has exited several companies so he knows how to ride the hype and viral marketing train.Â ",
          "score": 5,
          "created_utc": "2026-02-19 07:27:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6aghbc",
          "author": "francois__defitte",
          "text": "Same playbook every cycle. NFT bros moved to AI the second the hype shifted. They don't care about the tech, they care about the next thing normies will FOMO into. The tell is always the same: zero technical depth, maximum buzzword density, and screenshots of gains instead of actual demos.",
          "score": 2,
          "created_utc": "2026-02-19 19:05:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bbxyp",
          "author": "nodeocracy",
          "text": "Think youâ€™re chilling in the wrong places g",
          "score": 2,
          "created_utc": "2026-02-19 21:37:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67749w",
          "author": "DealingWithIt202s",
          "text": "People are telling their newly minted OpenClaw agents to go make them some money. The first that they often do is start a shitcoin on pump.fun and start shilling.  Crypto bros trained them years ago.",
          "score": 3,
          "created_utc": "2026-02-19 06:27:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67k17y",
          "author": "Everlier",
          "text": "It was definitely boosted up to a level where all the information reposter influencers had to cover it out of FOMO and now it just drives that snowball effect.\n\nScary bit, on that platform unknown groups boosted fresh (throwaway) account posts multiple times to appear in everyone's feed. My take was that they're doing it to prove they control the algorithm and getting away with it.",
          "score": 4,
          "created_utc": "2026-02-19 08:24:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67xasq",
          "author": "florinandrei",
          "text": "Why do you read that garbage?",
          "score": 3,
          "created_utc": "2026-02-19 10:33:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o680xky",
          "author": "Andsss",
          "text": "Man, Openclaw is absolutely useless I really don't understand the hype. Maybe they are people that don't know you can ask Siri , Google or even any LLMs apps what time is it and when is the weather.\n\n\nBecause besides that, you can't do anything slitly complex that it goes bananas. even if you try to control de context of the agent and just make him for 1 \"simple\" thing he goes bonkers.",
          "score": 4,
          "created_utc": "2026-02-19 11:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68vuzh",
          "author": "Mthatnio",
          "text": "It's the type of AI assistant tool people have wanted for decades. Believe it or not, the average person has no use for Claude Code and doesn't know to ask for a python script (or how to launch it).  But even they can understand the wonder of a Siri/Assistant with real access to the system, that WILL take control and do things. You may say \"muh but de privacy\", so does Google, OpenAI, etc. I say fuck it, it's cool. Thank God I don't have to run a sandbox on Docker. It took too long for such a tool to exist. I don't use it, it's not good enough for me, but if you can't understand why people like it, you're just dense.",
          "score": 2,
          "created_utc": "2026-02-19 14:30:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6as7in",
              "author": "Yorn2",
              "text": "Yeah, I agree. There's a level of hostility -- and quite frankly, density -- that it takes to not understand that what people want is an AI that can do all the stuff they imagine it doing. Openclaw gets society closer to that. It's doing it all in a very insecure way, sure, but it represents what society wants and the security side of things needs to become more agile to support it, not the other way around, IMHO.\n\nFor what it is worth, it's not as horribly insecure as people are making out, either, you don't have to give it access to everything. I've only given it access to certain things as I need it to have that access.",
              "score": 2,
              "created_utc": "2026-02-19 20:01:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6q9ip9",
                  "author": "johndeuff",
                  "text": "If the ppl on reddit had any understanding of what people want, they would have built a successful product already and wouldn't be here. Don't get me wrong I appreciate reading the ppl here most of time but these kind of hate posts reveal who they truly are.",
                  "score": 1,
                  "created_utc": "2026-02-22 06:02:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68g3kc",
          "author": "Single_Ring4886",
          "text": "I fully agree. And I have one reason for this. Today on social media you do not become that insanely viral over night in organic way. You need to invest SHITLOAD of money into ADs.",
          "score": 2,
          "created_utc": "2026-02-19 12:59:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69ce5w",
              "author": "huffalump1",
              "text": "Or have fleets of agents to make slop posts hyping it up, trying things until something goes viral for how dumb yet remarkable it is... This kind of thing is honestly only gonna get worse, before it gets better. And it will get better because the models and frameworks are rapidly improving.",
              "score": 2,
              "created_utc": "2026-02-19 15:54:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68k80r",
          "author": "kingp1ng",
          "text": "Agreed, the \"trend\" is way too fast to be organic. It's more like forced injection by autocratic decree.\n\nIt's not even that cool... am I right? What ever happened to self-control and independent thinking?!",
          "score": 1,
          "created_utc": "2026-02-19 13:25:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69mp0b",
          "author": "darkdeepths",
          "text": "not sure about this being a sign of bubble, but you are correct that things like moltbook are astroturfed. funnily enough, i DO think there is room for a product that makes people FEEL productive lmfao",
          "score": 1,
          "created_utc": "2026-02-19 16:44:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69otjs",
          "author": "mlsurfer",
          "text": "Interesting ...",
          "score": 1,
          "created_utc": "2026-02-19 16:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69pmj0",
          "author": "mpecanha",
          "text": "A lot of those people found a way to get people to pay fees on services based on Openclaw. A lot of those posts have links to paid services\n\nThe whole thing is fun to use overall but the saturation is crazy",
          "score": 1,
          "created_utc": "2026-02-19 16:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69re0t",
          "author": "Jayfree138",
          "text": "It certainly is way to increase API sales among people who don't know how or can't run locally. Makes open ai's sales numbers look better while they stay non liable for security vulnerabilities.\n\nI know I'm certainly not going to run it unless its on an old spare computer with it's own accounts and I don't have a spare so....",
          "score": 1,
          "created_utc": "2026-02-19 17:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a244f",
          "author": "syzygyhack",
          "text": "Look at the codebase. It's slop.",
          "score": 1,
          "created_utc": "2026-02-19 17:57:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a4b6n",
          "author": "a_beautiful_rhind",
          "text": "Shilling is hilariously one of the most practical applications for LLMs. The spamming is probably paid so *someone* is making money.",
          "score": 1,
          "created_utc": "2026-02-19 18:08:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6apf0q",
          "author": "theohuxtable",
          "text": "midtwit ahh take",
          "score": 1,
          "created_utc": "2026-02-19 19:47:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c1v2n",
          "author": "aidencoder",
          "text": "Don't forget the DAO grifters too.\n\n\nIt's a shame. Decentralised currency had some potential. As did the DAO ideas. These talentless clowns ruin everything.Â ",
          "score": 1,
          "created_utc": "2026-02-19 23:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cj9bn",
          "author": "schwigglezenzer",
          "text": "X is the best social network right now, you just have to filter out the stuff you don't like. Block certain words, mute topics, curate your feed. I love mine. I mostly see art, 3D, CG, and tons of interesting content.\n\nSure, there's a lot of negativity. Love or hate Elon, one thing is true: you can pretty much say whatever you want. That means you'll see all kinds of people... for better or worse. But at the same time, everyone's there: freelancers, artists, DIY creators, actors... you name it.\n\nThe whole world feels connected, too. I get a lot of Japanese photography content, for example, and Grok can translate it with one click. When I like something, the algorithm feeds me more art and similar stuff, it's actually pretty easy to \"train/condition\" it to show you what you want. Muting, for example, has a strong 'negative impact' on related content. The algo filters out similar posts much more aggressively once you mute something.\n\nThe chatting system is great as well. If you get close with someone, or if you're discussing a freelance job or something, the chats feel smooth and natural.\n\nidk, man, I just really like Twitter/X. It feels like the best place right now. Even politically, which I've mostly stopped engaging with, it's good in the sense that you can actually see both sides of the aisle.\n\nAs much as I used to love Reddit, it feels dead to me. Reddit kind of died around 2016, at least from my perspective. Despite its flaws, X is the only place left where you can actually have an argument if you want to, unlike Reddit, where it feels like you're risking a permanent ban at every step.",
          "score": 1,
          "created_utc": "2026-02-20 01:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67c35k",
          "author": "Darth_Ender_Ro",
          "text": "NFTs still a thing?! ðŸ‘€",
          "score": 1,
          "created_utc": "2026-02-19 07:10:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o672c2e",
          "author": "Dry-Yogurtcloset4002",
          "text": "Other than money laundring, I don't see any use of NFTs or even crypto in general.",
          "score": -1,
          "created_utc": "2026-02-19 05:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67kllj",
              "author": "Particular-Way7271",
              "text": "Well that pretty valuable no? ðŸ˜‚",
              "score": 5,
              "created_utc": "2026-02-19 08:29:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67uj5t",
              "author": "TechnoByte_",
              "text": "Crypto is resistant against [censorship from Visa and MasterCard](https://en.wikipedia.org/wiki/Financial_censorship#Timeline_of_notable_incidents)",
              "score": 3,
              "created_utc": "2026-02-19 10:07:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67wz09",
                  "author": "4baobao",
                  "text": "just like normal bank transfers?",
                  "score": 2,
                  "created_utc": "2026-02-19 10:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67aygh",
          "author": "Tall_East_9738",
          "text": "tbh if you fall for that crap you don't deserve your money",
          "score": 0,
          "created_utc": "2026-02-19 07:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ejxl",
          "author": "Jazzlike_Mud_1678",
          "text": "That's why I noticed so many \"ads\" about ai investment/crypto bots.",
          "score": 0,
          "created_utc": "2026-02-19 07:32:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jld1",
              "author": "SporksInjected",
              "text": "I mean, those people are in search of a way to make money right now after this last big crash",
              "score": 3,
              "created_utc": "2026-02-19 08:19:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67jgyf",
          "author": "Aaaaaaaaaeeeee",
          "text": "https://preview.redd.it/ste0m02qoekg1.jpeg?width=616&format=pjpg&auto=webp&s=c66d2e3a10b3992b07815f73397028167b68e278\n\n Someone should make clawdbot ðŸš©spamming. **cannibalism should occur**",
          "score": -2,
          "created_utc": "2026-02-19 08:18:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66z9mm",
          "author": "tracagnotto",
          "text": "Don't know about that but it solved some coding shit I've been dealing for long time and I love it",
          "score": -18,
          "created_utc": "2026-02-19 05:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6708xt",
              "author": "falconandeagle",
              "text": "Shut up bot",
              "score": 16,
              "created_utc": "2026-02-19 05:32:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o674ung",
                  "author": "tracagnotto",
                  "text": "Prove it",
                  "score": -9,
                  "created_utc": "2026-02-19 06:09:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r83irw",
      "title": "PSA: DDR5 RDIMM price passed the point were 3090 are less expensive per gb..",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/",
      "author": "No_Afternoon_4260",
      "created_utc": "2026-02-18 13:51:04",
      "score": 467,
      "num_comments": 217,
      "upvote_ratio": 0.97,
      "text": "Hello all,\n\nJust wanted to note that RDIMM prices are so wild.. Stacking rdimms starts to be as expensive as stacking 3090s.. But RDIMM don't come with compute included..\n\nWhat a crazy time, shall we stack rdimms or 3090, what's your take on that?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r83irw/psa_ddr5_rdimm_price_passed_the_point_were_3090/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o63uu5u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-18 19:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61ztl4",
          "author": "tomt610",
          "text": " It is insane, I paid Â£1900 for 4 sticks of RAM in June, now they are 11296 from same shop, each one is more than 5090",
          "score": 116,
          "created_utc": "2026-02-18 14:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6231du",
              "author": "Dry_Yam_4597",
              "text": "1900 for 4 sticks? Where? And what spec?",
              "score": 34,
              "created_utc": "2026-02-18 14:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62ztqo",
                  "author": "ImportancePitiful795",
                  "text": "Bought 16x64 DDR5600 last summer for â‚¬3600 for the Intel QYFS. Right now can cash them for â‚¬30K+\n\nTruly crazy!!!! \n\n",
                  "score": 61,
                  "created_utc": "2026-02-18 16:57:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62cf6q",
                  "author": "tomt610",
                  "text": "https://www.memorycow.co.uk/memory-ram/server-ram-memory/ddr5-6400-mts/96gb-micron-mtc40f204ws1rc64br-ddr5-6400mt-s-ecc-registered-dimm\nHere, but it was in June\nhttp://web.archive.org/web/20250622191556/https://www.memorycow.co.uk/memory-ram/server-ram-memory/ddr5-6400-mts",
                  "score": 16,
                  "created_utc": "2026-02-18 15:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62uhvl",
              "author": "LukeLikesReddit",
              "text": "Yeah I was thinking of changing my case on my pc and giving it a deep clean and im absolutely shitting it thinking of touching the ram despite building loads of PCs aha. I think id cry at this point.",
              "score": 9,
              "created_utc": "2026-02-18 16:33:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o647pba",
                  "author": "lemondrops9",
                  "text": "I keep thinking the same. Built over 100 PCs easy but if one of those sticks goes because I moved them... it would be quite sad. Never worried about it in the past.",
                  "score": 5,
                  "created_utc": "2026-02-18 20:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61xc18",
          "author": "sob727",
          "text": "That's an interesting milestone. They don't serve the same purpose though. So not sure what to make of it.",
          "score": 57,
          "created_utc": "2026-02-18 13:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61zvwb",
              "author": "Karyo_Ten",
              "text": "I'm sure you can use VRAM as ultrafast swap. I'll call that vswap.",
              "score": 53,
              "created_utc": "2026-02-18 14:07:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63vplk",
                  "author": "dlcsharp",
                  "text": "It already exists on Linux, which is not that surprising tbh\n\n[https://wiki.archlinux.org/title/Swap\\_on\\_video\\_RAM](https://wiki.archlinux.org/title/Swap_on_video_RAM)",
                  "score": 16,
                  "created_utc": "2026-02-18 19:19:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o624634",
                  "author": "Dany0",
                  "text": "there's an old gpu ramdisk project (windows) iirc is still works if you dare to go in like a mechanic, just be prepared it black screen of deaths sometimes",
                  "score": 14,
                  "created_utc": "2026-02-18 14:30:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62op6p",
              "author": "BigYoSpeck",
              "text": "For day to day general usage no, but then most use cases are fine with 32gb of RAM for that. But given the sub we're on VRAM is more than 10x better for what we want",
              "score": 1,
              "created_utc": "2026-02-18 16:07:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o61z183",
              "author": "No_Afternoon_4260",
              "text": "\\> not sure what to make of it.  \nI have exactly the same conclusion lol",
              "score": 0,
              "created_utc": "2026-02-18 14:03:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62jm2w",
          "author": "__JockY__",
          "text": "I paid just under $4000 USD for 768GB of DDR5 6400 MT/s ECC DRAM in mid-late 2025. That same RAM (Samsung M321R8GA0PB2-CCP) would now cost me $24,000.\n\nFuck Sam Altman.",
          "score": 52,
          "created_utc": "2026-02-18 15:44:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62x5bf",
              "author": "No_Afternoon_4260",
              "text": "^ this exactly",
              "score": 5,
              "created_utc": "2026-02-18 16:45:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o66txph",
              "author": "az226",
              "text": "I got 768GB of 6400 RDIMM for $1900. Early mid 2025. Shit is crazy.",
              "score": 2,
              "created_utc": "2026-02-19 04:46:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66vkjh",
                  "author": "__JockY__",
                  "text": "Yeah I thought I was crazy spending so much on RAM at the time, but I needed it for non-AI work. Then the crunch hit and I felt pretty good about that $4k!",
                  "score": 2,
                  "created_utc": "2026-02-19 04:58:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64hzwd",
              "author": "cuteman",
              "text": "er... the hyperscaler tech companies are buying a LOT more than OpenAI \n\nGoogle, Amazon, Meta, Microsoft, Oracle are basically monopolizing entire years worth of production with the big ODMs.\n\nIt's so substantial, RAM and SSD/HDD mfgs are flipping entire production lines to enterprise instead of consumer. Micron/Crucial has canceled their entire consumer category offerings entirely deciding to focus entirely on enterprise output. That's wild.",
              "score": 7,
              "created_utc": "2026-02-18 21:03:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65lr3l",
                  "author": "TechnoByte_",
                  "text": "OpenAI's project stargate is the main cause of the RAM shortage: https://www.tomshardware.com/pc-components/dram/openais-stargate-project-to-consume-up-to-40-percent-of-global-dram-output-inks-deal-with-samsung-and-sk-hynix-to-the-tune-of-up-to-900-000-wafers-per-month",
                  "score": 9,
                  "created_utc": "2026-02-19 00:21:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64jqvo",
              "author": "celebrar",
              "text": "*buys 768gb ram to run ai*\n\n> fuck sam altman\n\nhe is you, you are him",
              "score": -4,
              "created_utc": "2026-02-18 21:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65m0ke",
                  "author": "TechnoByte_",
                  "text": "768 GB is nothing compared to [the 900000 wafers per month Sam Altman is buying](https://www.tomshardware.com/pc-components/dram/openais-stargate-project-to-consume-up-to-40-percent-of-global-dram-output-inks-deal-with-samsung-and-sk-hynix-to-the-tune-of-up-to-900-000-wafers-per-month)",
                  "score": 5,
                  "created_utc": "2026-02-19 00:22:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65hplv",
                  "author": "redditorialy_retard",
                  "text": "Professional Enthusiasts and homeland regularly have that much ram, the RAM gobbled by AI can be counted in the petabytesÂ ",
                  "score": 3,
                  "created_utc": "2026-02-18 23:59:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65hghn",
                  "author": "__JockY__",
                  "text": "My decisions impact fewer people with less severity.",
                  "score": 6,
                  "created_utc": "2026-02-18 23:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6220g5",
          "author": "theshitstormcommeth",
          "text": "I just found 10 sticks of DDR4 32GB in my storage unit and feel like I dug up gold.",
          "score": 76,
          "created_utc": "2026-02-18 14:19:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o627py0",
              "author": "satireplusplus",
              "text": "Going price on ebay is $100+ for DDR4 ECC, even the slow 2133 Mhz ones. The 2400 Mhz ones are probably more sought after. I bought mine for 20-30 bucks each here and there in the past years before the RAMacopalyse. Guess I got lucky, I have my server already stacked to the brim with 256GB (8x32GB).",
              "score": 20,
              "created_utc": "2026-02-18 14:48:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62atz5",
                  "author": "dave-tay",
                  "text": "Dang I just realized this is the time to sell if you don't need it immediately. Prices will inevitably return to normal",
                  "score": 10,
                  "created_utc": "2026-02-18 15:03:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62mnrl",
                  "author": "theshitstormcommeth",
                  "text": "2400MHZ making me feel even richer, thank you.",
                  "score": 4,
                  "created_utc": "2026-02-18 15:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6244uw",
              "author": "No_Afternoon_4260",
              "text": "lol I know the feeling (for the digging gold part, not finding ddr sadly)",
              "score": 1,
              "created_utc": "2026-02-18 14:29:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62gdjk",
          "author": "cristianlukas",
          "text": "I paid 300usd for 128gb of ddr5 ram in Argentina, now it's 2800usd for the same RAM!! It's insane, I feel dirty rich, and I'm glad I bought it for local inference",
          "score": 12,
          "created_utc": "2026-02-18 15:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65yf8l",
              "author": "vr_fanboy",
              "text": "argentinian here. yep, I bought 64 GB and regret not going for the full 128. Also, ML was flooded with 3090s for USD 500 in 2024, another regret for not buying more of those.",
              "score": 5,
              "created_utc": "2026-02-19 01:33:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66rbuw",
                  "author": "cristianlukas",
                  "text": "I bought one at 580usd ish, second hand obviously, I got really lucky with that one.",
                  "score": 1,
                  "created_utc": "2026-02-19 04:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61zorq",
          "author": "a_beautiful_rhind",
          "text": "I don't think you can cram as many 3090s as you can get dimms.",
          "score": 11,
          "created_utc": "2026-02-18 14:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o620b08",
              "author": "No_Afternoon_4260",
              "text": "stacking 32 3090 to get 768gb of fragmented vram.. what a dream.",
              "score": 10,
              "created_utc": "2026-02-18 14:10:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o620qb3",
                  "author": "a_beautiful_rhind",
                  "text": "For the electric company.",
                  "score": 36,
                  "created_utc": "2026-02-18 14:12:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o621q0w",
              "author": "PermanentLiminality",
              "text": "The problem is powering them.  Your electric rates are about to experience DDR5 like price hikes,",
              "score": 19,
              "created_utc": "2026-02-18 14:17:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62n4wd",
                  "author": "Ansible32",
                  "text": "You can just underclock them.",
                  "score": 9,
                  "created_utc": "2026-02-18 16:00:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o623o08",
                  "author": "No_Afternoon_4260",
                  "text": "ðŸ¤« don't wake a sleeping bear",
                  "score": 2,
                  "created_utc": "2026-02-18 14:27:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o620djp",
              "author": "DeltaSqueezer",
              "text": "Just keep stacking these:  \n\n\nhttps://preview.redd.it/8rd5724hg9kg1.png?width=1000&format=png&auto=webp&s=8d8fb2c5a9a9fbefa33c4ca55f2ed4299af51f28\n\n",
              "score": 7,
              "created_utc": "2026-02-18 14:10:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o620xm3",
                  "author": "a_beautiful_rhind",
                  "text": "I think it still needs a PLX to go with it.",
                  "score": 3,
                  "created_utc": "2026-02-18 14:13:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o627v8k",
                  "author": "Flimsy_Leadership_81",
                  "text": "cool price?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-18 14:48:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o631i4g",
                  "author": "330d",
                  "text": "What is this used for? GPU splitter?",
                  "score": 1,
                  "created_utc": "2026-02-18 17:05:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62epsh",
              "author": "Lissanro",
              "text": "I could plug-in up to 20 GPUs in [my motherboard](https://www.gigabyte.com/ru/Enterprise/Server-Motherboard/MZ32-AR1-rev-30), each at PCI-E 4.0 x4 speed (two of them will be at PCI-E 3.0 x8, which is about the same speed as PCI-E 4.0 x4), using bifurcation modes the motherboard directly supports. The issue is, 3090 cards just don't have much VRAM to begin with, 24 GB per card = 480 GB if I get twenty, still not enough to have K2.5 in VRAM if using Q4\\_X quant. For now, I decided to stick with what I have (four 3090 cards and 1 TB 3200 MHz DDR4 RAM). Given the current market, I don't expect to upgrade any time soon.",
              "score": 6,
              "created_utc": "2026-02-18 15:21:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o62w4bn",
                  "author": "a_beautiful_rhind",
                  "text": "I think deltasqueezer showing a better way. Get expansion boards and PLX. If you're not offloading to ram the one downlink is enough and then the cards can P2P to eachother over the switch.\n\nI'm not upgrading any time soon either. Maybe if some more 2400-2666 memory shows up I will double to 768.  Or I'll get tired of the 2080ti and change it for something else. My last hail maries are hacking newer proc support into the mobo or buying a real cascade lake with VNNI. Would probably lose my ram overclock though. Prices making things look grim and like I should be happy with what I have.",
                  "score": 3,
                  "created_utc": "2026-02-18 16:40:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65eoyx",
                  "author": "segmond",
                  "text": "Performance will be amazing with 20 3090s tho.   I mean performance is good with 4-5 3090s so if you offload most of them off ram, it will be super great.  But yeah, 20 3090 in 2026 is madness.  If we didn't have blackwell pro 6000 then it would be perhaps worth it for the not really rich not really poor local tinker.",
                  "score": 1,
                  "created_utc": "2026-02-18 23:42:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6a0cxq",
              "author": "mesasone",
              "text": "Not with that attitude you canâ€™t",
              "score": 2,
              "created_utc": "2026-02-19 17:49:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6a0hmj",
                  "author": "a_beautiful_rhind",
                  "text": "Not with my wallet either.",
                  "score": 1,
                  "created_utc": "2026-02-19 17:50:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6352gb",
          "author": "OverclockingUnicorn",
          "text": "I just sold half a TB of DDR3 for Â£300...",
          "score": 7,
          "created_utc": "2026-02-18 17:21:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62dtyh",
          "author": "segmond",
          "text": "unfortunatley, 3090s are power hungry.   With large MoE, it's easier to add 256gb of ram than about that amount of vram.   more 3090s means more rig, power supplys, riser,  heat, electricity, etc.   As someone with a rig of 3090s, stacking 3090s is not very attractive anymore with these huge models.   I'm on 120v and sure I can spend extra to upgrade to 240v, but when does the maddess stop?  At this point, I'm waiting to see what the new studio looks like.  I'm either going mac studio or blackwell pro 6000",
          "score": 12,
          "created_utc": "2026-02-18 15:17:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62gddb",
              "author": "Abject_Avocado_8633",
              "text": "The power and infrastructure overhead is a huge hidden cost that gets overlooked. But I'd push back slightly on the Mac Studio as a full solutionâ€”you're trading one set of constraints (power/heat) for another (proprietary hardware, locked ecosystem). For a lot of inference work, a single beefy server with RDIMMs might actually hit a better total cost of ownership sweet spot once you factor in flexibility.",
              "score": 4,
              "created_utc": "2026-02-18 15:29:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65do3o",
                  "author": "segmond",
                  "text": "If I don't go the Mac Studio route then I'm going to be looking into getting solar for my house.   At least if I can go to $0 monthly in electricity I'll be happy.    Electricity costs in the US is definitely not coming down soon at the pace we are building data centers compared to how we are going for green energy.",
                  "score": 1,
                  "created_utc": "2026-02-18 23:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o63lix3",
              "author": "fallingdowndizzyvr",
              "text": ">  3090s are power hungry. \n\nI make that point often. Not just about 3090s but GPUs in general. At the price of power where I live. A Mac or Strix Halo pays for itself sooner rather than later due to the power savings.",
              "score": 3,
              "created_utc": "2026-02-18 18:33:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64rtcw",
                  "author": "trolololster",
                  "text": "same here, i ONLY bought a used 3090 in autumn because i knew taxes on power would drop jan. 1 2026.\n\ni am used to paying ~10 us cents pr KWH and they have lowered that to 1 us cent pr. KWH\n\nand those are just the taxes, the companies that own the electrical net in my part of the world also have their own special tax called transport-levy which is PUNITIVE expensive and we are apparently as the whims of the market so yesterday between 17-18 the price for ONE KWH (taxes, levy, power market price) was over $1 USD.. at 17:45-18:00 it peaked at over $1.2 USD\n\n$1 USD for one KWH... crazy crazy crazy.\n\ni pay as much for my home-experiments for power pr month as a max x5 account costs and my 3090 idles A LOT.",
                  "score": 4,
                  "created_utc": "2026-02-18 21:48:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62ezei",
              "author": "No_Afternoon_4260",
              "text": "yeah I'm afraid you are correct, 3090 realm starting to slowly fade out, I just kept the one for basic stuff",
              "score": 1,
              "created_utc": "2026-02-18 15:22:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o62g88d",
                  "author": "Abject_Avocado_8633",
                  "text": "Holding onto one for basic tasks is  solid. But honestly, the 'power hungry' critique is a bit overblown i guess unless you're running a full rackâ€”for a single card doing inference, the efficiency difference vs. newer hardware isn't a deal-breaker. The real killer is the fragmentation; trying to scale with multiple 3090s for a unified model becomes a software nightmare fast.",
                  "score": 3,
                  "created_utc": "2026-02-18 15:28:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62qyj9",
          "author": "IrisColt",
          "text": "what a timeline, heh",
          "score": 4,
          "created_utc": "2026-02-18 16:17:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o638d4z",
          "author": "PawelSalsa",
          "text": "It only means that we are entering the very top of this cycle. If single stick cost more than entire graphic card then selling it and buying card looks like logical move. Just my opinion",
          "score": 4,
          "created_utc": "2026-02-18 17:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61y9ko",
          "author": "Condomphobic",
          "text": "You guys are giving sellers free money instead of waiting the storm out",
          "score": 19,
          "created_utc": "2026-02-18 13:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61yvcq",
              "author": "No_Afternoon_4260",
              "text": "How long do you think that storm will last?",
              "score": 21,
              "created_utc": "2026-02-18 14:02:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o61zyqq",
                  "author": "Karyo_Ten",
                  "text": "The market can stay irrational longer than you can stay solvent",
                  "score": 61,
                  "created_utc": "2026-02-18 14:08:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62139g",
                  "author": "Condomphobic",
                  "text": "Itâ€™s 2-3 years. They pre-purchased the hardware.\n\nNot wasting my money",
                  "score": 16,
                  "created_utc": "2026-02-18 14:14:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o626soc",
                  "author": "CountLippe",
                  "text": "3 to 5 years is what the industry is saying internally.",
                  "score": 8,
                  "created_utc": "2026-02-18 14:43:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61zzsm",
                  "author": "Gargantuan_Cinema",
                  "text": "It's not going to stop, AI is getting better each year and companies want more than the frontier labs can provide. It's likely the desire for more digital intelligence is here to stay.",
                  "score": 3,
                  "created_utc": "2026-02-18 14:08:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62vrkq",
                  "author": "MrWeirdoFace",
                  "text": "Couple years.",
                  "score": 1,
                  "created_utc": "2026-02-18 16:39:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o620ci0",
              "author": "GreenTreeAndBlueSky",
              "text": "I just accepted I'm not upgrading anything for at least the next 2 years. If the model update is better but doesn't fit on my machine anymore it's not real progress and that's that lol. \n\nSub 40b models are already super useful if you are willing to use 2 braincells when using them. I'm not a company trying to replace my workforce I want useful tools and there are already plenty and things are looking good for the future. If you don't have your own company there is no reason to want to spend enough to run deepseek on prem.",
              "score": 11,
              "created_utc": "2026-02-18 14:10:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o630lav",
              "author": "aimark42",
              "text": "I don't think most hobbyist are buying RDIMM's.  The companies who do, will pretty much buy them regardless of the price.  With the insane datacenter deployments going on I have no doubt all of it is being sold through.  \n\nI'm sure there is some margin, but if the goal is to have cheaper consumer gear we should celebrate increases in RDIMM prices if that means UDIMMs can be cheaper.",
              "score": 1,
              "created_utc": "2026-02-18 17:00:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65f0ik",
              "author": "segmond",
              "text": "AI is here, it's not going away.   The demand for inference is going to be going up.  My 90+ years old grandma wants to know about this AI stuff and how she can start using it.  She doesn't even have a computer.",
              "score": 1,
              "created_utc": "2026-02-18 23:44:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o635vif",
          "author": "Porespellar",
          "text": "Probably an unpopular opinion, but Iâ€™m about to start stacking DGX Sparks and building an EXO cluster. Hopefully theyâ€™ll resolve the vLLM NVFP4 issues soon and start getting some usable tk/s speeds on large models.",
          "score": 3,
          "created_utc": "2026-02-18 17:24:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o636l0l",
              "author": "No_Afternoon_4260",
              "text": "Ho does that tp 4 works, what about batchacaling? So many questions but I think this is a dev platform for grace-blackwell, what do you think?",
              "score": 1,
              "created_utc": "2026-02-18 17:28:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o63mz4y",
                  "author": "Porespellar",
                  "text": "The NVFP4 of GPT OSS 120b runs amazingly well on Spark. It has 128 GB of unified memory and its prompt processing speed beats Mac Studio and Strix Halo. Iâ€™m happy with it",
                  "score": 1,
                  "created_utc": "2026-02-18 18:40:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o682udn",
          "author": "thedarkbobo",
          "text": "Sell it all, keep one 3090, use cheap cloud, wait a year, buy back. Thats my personal opinion. I dont want a system that draws 1KW to run a model. 3090 is ok for simple tasks but for pro use with big models we are not there yet",
          "score": 3,
          "created_utc": "2026-02-19 11:22:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j4bfp",
              "author": "Background_Baker9021",
              "text": "My rig is a AM4 3700x, 64gb ram, 1tb nvme, and a 3090.  I don't use the 3090 often for inference, and it's headless so no graphics use.  It idles around 30 watts.  But man, once I bang on it with 20b model or a 27b, it ramps up quick in wattage to like 349 watts, just under the max.  So if it was running full throttle all the time I'd definitely consider not doing local LLMs anymore with those kinds of charges involved.  I'm in the Pacific Northwest, so power isn't super expensive (yet).",
              "score": 1,
              "created_utc": "2026-02-21 01:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6k5lx7",
                  "author": "thedarkbobo",
                  "text": "Cool, I copper modded mine long time ago and undervolted. It won't change speed much but mine is below 70C and below 250w",
                  "score": 1,
                  "created_utc": "2026-02-21 06:04:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62g2bq",
          "author": "Admirable-Star7088",
          "text": "If you need RAM urgently, the timing is unfortunate. Otherwise, wait to buy until the electronics market normalizes.",
          "score": 4,
          "created_utc": "2026-02-18 15:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62g8vd",
              "author": "No_Afternoon_4260",
              "text": "And when do you expect that?",
              "score": 8,
              "created_utc": "2026-02-18 15:28:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64iu4s",
                  "author": "cuteman",
                  "text": "2028 in any real way. It'll be worse than the Thailand Flood/HDD apocalypse a decade ago without a natural disaster because there's 100x more value in it for AI hyperscalers over people who want to play video games.",
                  "score": 6,
                  "created_utc": "2026-02-18 21:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o64lvy5",
                  "author": "ProfessionalSpend589",
                  "text": "various people in the industry have spoken about early to mid 2027 as a possible timeframe in which we may see price drops (and more certainly after that, because extra capacity is being built for new RAM and new RAM standards)",
                  "score": 3,
                  "created_utc": "2026-02-18 21:21:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o62hjlz",
                  "author": "Admirable-Star7088",
                  "text": "That's super hard to guess, it depends on various factors, such as how quickly the AI bubble bursts and how quickly memory manufacturers can get their new factories up and running. But most analysts seem to agree that prices should return to more normal levels at least around 2027-2028.",
                  "score": 0,
                  "created_utc": "2026-02-18 15:34:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o627jwx",
          "author": "Flimsy_Leadership_81",
          "text": "vram...",
          "score": 2,
          "created_utc": "2026-02-18 14:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63i1uq",
          "author": "I_like_fragrances",
          "text": "At microcenter 4 sticks of 4x32gb ddr5 ecc is $3000.",
          "score": 2,
          "created_utc": "2026-02-18 18:18:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63kk6w",
              "author": "Black_Otter",
              "text": "Thatâ€™s just stupid",
              "score": 1,
              "created_utc": "2026-02-18 18:29:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o63qhr2",
              "author": "No_Afternoon_4260",
              "text": "It's not that bad today, in the US you get it maybe 30% less expensive than in europe.\nJust enough to make it affordable for europeans to import even if they have to pay 20% vat",
              "score": 1,
              "created_utc": "2026-02-18 18:55:32",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6h5k6l",
              "author": "I_like_fragrances",
              "text": "It just went up to $3400. These were $1200 each about 3 months ago.\n\nhttps://preview.redd.it/sptwftmcapkg1.jpeg?width=1290&format=pjpg&auto=webp&s=cd799bfc5ac5dd1091570a54f3aea3579fdf0760",
              "score": 1,
              "created_utc": "2026-02-20 19:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o666otj",
          "author": "neoqueto",
          "text": "https://preview.redd.it/rd134pzz2dkg1.jpeg?width=1031&format=pjpg&auto=webp&s=f84b7ff1ce35ba5c9741daaf05eeece1de672b96",
          "score": 2,
          "created_utc": "2026-02-19 02:21:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67efta",
              "author": "ANTIVNTIANTI",
              "text": "ðŸ˜­ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­",
              "score": 1,
              "created_utc": "2026-02-19 07:31:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62cb6x",
          "author": "sirkerrald",
          "text": "Let's say I had a 3090 lying around. How do I sell that without getting scammed?",
          "score": 1,
          "created_utc": "2026-02-18 15:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62emoy",
              "author": "No_Afternoon_4260",
              "text": "by sending me a dm if it's a turbo",
              "score": 3,
              "created_utc": "2026-02-18 15:21:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64g7tx",
                  "author": "sirkerrald",
                  "text": "Founders Edition, sorry :(",
                  "score": 1,
                  "created_utc": "2026-02-18 20:55:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o66crl1",
              "author": "ThePixelHunter",
              "text": "Swappa or eBay",
              "score": 1,
              "created_utc": "2026-02-19 02:57:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o63209d",
          "author": "RoughOccasion9636",
          "text": "The framing of RDIMM vs 3090 as an either/or misses the actual question: what are you running?\n\nFor inference-only on large models (70B+), high-speed unified memory like Apple M-series or RDIMM plus CPU can make sense because you are memory-bandwidth bound, not compute bound. 3090 wins hard for anything that fits comfortably in VRAM.\n\nThe real gotcha with RDIMM stacking for LLMs is that DDR5 bandwidth still trails HBM by a wide margin. You get the capacity but trade tokens per second. A 3090 at 24GB doing 70B in Q4 will often outrun a CPU plus 256GB RAM setup on throughput. Different tool for different jobs.",
          "score": 1,
          "created_utc": "2026-02-18 17:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o633r8t",
          "author": "Accomplished-Grade78",
          "text": "Has anyone figured out if the Intel Optane modules can be made useful? \n\nhttps://ebay.us/m/QXMIRQ\n\nPrices tell me the arenâ€™t useful, but there are creative people who will defy my AI chat that told me they are uselessâ€¦",
          "score": 1,
          "created_utc": "2026-02-18 17:15:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64ptdd",
              "author": "trolololster",
              "text": "i have 2x375 GB in my server for zil/slog and l2arc\n\nthey are great but slower than nvme, what they have is a completely insane TBW (off the charts compared to any current-gen consumer nvme)\n\nthey are very very useful for caching writes (because of TBW) - but that is about it.\n\nso if your workload is lots and lots of (random) writes they work, otherwise i would not bother",
              "score": 1,
              "created_utc": "2026-02-18 21:39:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69jfyd",
                  "author": "GalladeGuyGBA",
                  "text": "My understanding was that the Optane DIMMs are still lower latency (and higher bandwidth?) than any SSD on the market, being around an order of magnitude slower than DRAM. The main issue with them is that they're only compatible with specific Xeon CPUs. Judging by the 2x375GB, you probably have two of the much slower (though still pretty good) P4800X, which is a PCIe card that acts like an SSD. I'd be interested in seeing benchmarks on MoE offloading on that if you have them, though.",
                  "score": 1,
                  "created_utc": "2026-02-19 16:28:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o635amw",
          "author": "aimark42",
          "text": "This is a false comparison. \n\nRDIMM buyers are mostly Hypervisors building multi GPU servers. But Hypervisors are not hitting FB market looking for used 3090's.  They want standard deployments. These are 2 different markets, and the few hobbyists who are buying RDIMM's are the outliers.  We for sure are outliers. Explain to your Grandma why she needs RDIMM's in her next supercomputer.",
          "score": 1,
          "created_utc": "2026-02-18 17:22:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o645pob",
          "author": "Apprehensive_Use1906",
          "text": "Im thinking about getting a mac studio m3 ultra. Not blazing fast but the prices have not gone up on them yet. Apples expensive memory is now in the realm of reason. The 60 core gpu m3 with 256gb ram runs about 5600. Iâ€™m pretty sure the m5 ultra will be similar to the 5k nvidias but the price is going to go up by the time they announce them in june.",
          "score": 1,
          "created_utc": "2026-02-18 20:05:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o650l11",
          "author": "thecodeassassin",
          "text": "Indeed insane! I bought 320GB 5600 RDIMMS for 2k total. The same would cost almost 10k now... I really do hope they keep working...",
          "score": 1,
          "created_utc": "2026-02-18 22:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65qf2x",
          "author": "AlwaysLateToThaParty",
          "text": "I had some muppet in here yesterday telling me that prices aren't increasing.",
          "score": 1,
          "created_utc": "2026-02-19 00:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o667lep",
          "author": "dragoon7201",
          "text": "Snatched up a Lenovo legion i9 with 192 gb of ram and 5090 24bg last November for 3300 CAD, feels good man",
          "score": 1,
          "created_utc": "2026-02-19 02:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66gnnb",
          "author": "Aware_Photograph_585",
          "text": "Just paid $3800 for 1TB DDR4 2666mhz REG ECC (8x 128GB).  \nPrices are stupid right now.",
          "score": 1,
          "created_utc": "2026-02-19 03:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6778lh",
          "author": "khronyk",
          "text": "I'm really sad. When i got my epyc server i origionally bought 512GB of LRDIMM ram but returned it after i kept getting post errors in favor of 256GB of RDIMM that was on the QVL. Turns out i was sold a vendor locked CPU. it was $800 when i returned the ram 12 months ago and now it's $6000. Guess i won't be upgrading the ram ever.",
          "score": 1,
          "created_utc": "2026-02-19 06:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o621v1g",
          "author": "Southern-Chain-6485",
          "text": "RTX 3090s. You still need the add the cost of the PSU but, as you point out, the 3090 has compute and the RDIMM does not",
          "score": 1,
          "created_utc": "2026-02-18 14:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o631s8v",
          "author": "fofo9683",
          "text": "Can't wait for this to be over. Something has to happen. Maybe end users boycott the big companies that develop A.I or something, whatever. I can't believe we can last a few years with this situation without having a good setup to test hugging face models.",
          "score": 1,
          "created_utc": "2026-02-18 17:06:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o656bs7",
              "author": "skirmis",
              "text": "In other news, Phison CEO says he thinks lots of consumer electronics companies will go bankrupt in 2026, they cannot afford memory prices.",
              "score": 3,
              "created_utc": "2026-02-18 22:57:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64h1n5",
          "author": "Rich_Artist_8327",
          "text": "I could sell 2x 96gb ddr5 5600mhz. Anyone?",
          "score": 1,
          "created_utc": "2026-02-18 20:58:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c7joo",
              "author": "TheSilverSmith47",
              "text": "How many kidneys do you want for it?",
              "score": 1,
              "created_utc": "2026-02-20 00:31:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ddsym",
                  "author": "Rich_Artist_8327",
                  "text": "2400â‚¬",
                  "score": 1,
                  "created_utc": "2026-02-20 05:02:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o629sp4",
          "author": "BreizhNode",
          "text": "third option nobody mentioned yet: rent. if you're running inference a few hours a day and not 24/7, the math on buying hardware (3090s or RDIMMs) doesn't pencil out vs renting GPU time. a 3090 is what, $800-900 used? that's 2+ years of a cloud GPU box at current rates, and you're not stuck holding depreciating silicon when the next gen drops.",
          "score": -3,
          "created_utc": "2026-02-18 14:58:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62b1e2",
              "author": "No_Afternoon_4260",
              "text": "You are on localllama. #local4life\n\nWe have serious business with our waifus",
              "score": 22,
              "created_utc": "2026-02-18 15:04:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o632axv",
                  "author": "Abject_Avocado_8633",
                  "text": "\"Serious business with our waifus\" is the most accurate description of this sub I've ever read. The passion here is for tinkering and running models locally, cost and efficiency be damned sometimes. That said, the 'rent vs. buy' math in the parent comment is painfully correct for anyone actually treating this as a business expense I guesss...",
                  "score": 4,
                  "created_utc": "2026-02-18 17:08:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62xcnw",
              "author": "RG_Fusion",
              "text": "You're just feeding the source of the problem. Compute should be delocalized, in the hands of the people. By paying for cloud services, your incentivizing the very issue that is making RAM unavailable in the first place.",
              "score": 14,
              "created_utc": "2026-02-18 16:46:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o63cjof",
              "author": "esuil",
              "text": "I will never trust third party with my data to that degree. That would be absolutely crazy.\n\nI might use them for generic queries and that's it.\n\n> and you're not stuck holding depreciating silicon when the next gen drops.  \n\nHow in the world still having your GPU after it paid for itself is a negative? lol.",
              "score": 10,
              "created_utc": "2026-02-18 17:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o62lvml",
              "author": "EvilPencil",
              "text": "Where are you finding rentals that are actually attractive?  Everything I've found would pay for a GPU purchase after \\~3 months 24/7 usage, even after the insane market prices these days.",
              "score": 3,
              "created_utc": "2026-02-18 15:54:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62v5d8",
          "author": "Adventurous-Paper566",
          "text": "3090 = 350W",
          "score": 0,
          "created_utc": "2026-02-18 16:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62zdb0",
              "author": "ethertype",
              "text": ".... when being busy. People keep spamming that 350w number while my 3090s idle at 10-15 watts.",
              "score": 5,
              "created_utc": "2026-02-18 16:55:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o635z6r",
                  "author": "No_Afternoon_4260",
                  "text": "And are efficient at 280",
                  "score": 3,
                  "created_utc": "2026-02-18 17:25:16",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o69ker2",
                  "author": "Adventurous-Paper566",
                  "text": "15W sur une 3090 c'est de l'IDLE...",
                  "score": 1,
                  "created_utc": "2026-02-19 16:33:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6j5w7j",
              "author": "Background_Baker9021",
              "text": "Mine's at 23 watts idle at the moment.  Sometimes it hangs out at 30 watts at idle.  Not sure why there is idle drift on the wattage, but it definitely isn't banging at 350 watts unless I'm doing llm stuff.",
              "score": 1,
              "created_utc": "2026-02-21 01:53:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o63yad2",
          "author": "Bit_Poet",
          "text": "Well, you can't get 3090s anyway, so the question is moot.",
          "score": -1,
          "created_utc": "2026-02-18 19:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o636q4p",
          "author": "putrasherni",
          "text": "DDR4 is still cheap",
          "score": -2,
          "created_utc": "2026-02-18 17:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63m70p",
              "author": "fallingdowndizzyvr",
              "text": "LOL. No it's not. I got 64GB(2x32) of sodimm two years ago for less than $100. Now it's $400-$500. That's not cheap.",
              "score": 7,
              "created_utc": "2026-02-18 18:36:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63wcb9",
                  "author": "putrasherni",
                  "text": "I checked resale DDR4 16x4 64GB going for 125Â£",
                  "score": -2,
                  "created_utc": "2026-02-18 19:22:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6599e",
      "title": "Qwen3.5-397B-A17B Unsloth GGUFs",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/zgfpbga5ttjg1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-16 09:34:10",
      "score": 465,
      "num_comments": 140,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5q7p1b",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-16 18:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nopfx",
          "author": "Amazing_Athlete_2265",
          "text": "Nice work with the zero day release!",
          "score": 94,
          "created_utc": "2026-02-16 09:44:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5npk0u",
              "author": "danielhanchen",
              "text": "All thanks to the Qwen team for a wonderful model! :)",
              "score": 65,
              "created_utc": "2026-02-16 09:52:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5nx9cv",
                  "author": "xrvz",
                  "text": "Did they give you advance access to generate these?",
                  "score": 8,
                  "created_utc": "2026-02-16 11:03:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nqjzr",
              "author": "Sensitive_Song4219",
              "text": "NEW LEAD TIME RECORD FOR UNSLOTH RELEASE: \n\n0 DAYS!!!",
              "score": 38,
              "created_utc": "2026-02-16 10:02:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nqq70",
                  "author": "danielhanchen",
                  "text": "Haha :)",
                  "score": 21,
                  "created_utc": "2026-02-16 10:03:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nnwtn",
          "author": "jamaalwakamaal",
          "text": "Woahhhh 397B !!!Â ",
          "score": 32,
          "created_utc": "2026-02-16 09:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5no34c",
              "author": "danielhanchen",
              "text": "Ye it's a medium/large sized one! Excited for more this week!",
              "score": 29,
              "created_utc": "2026-02-16 09:38:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5npv2n",
                  "author": "nullmove",
                  "text": "Uhh have you spotted a certain whale around?",
                  "score": 9,
                  "created_utc": "2026-02-16 09:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5oq9lv",
              "author": "peppaz",
              "text": "If someone can run this on their clustered Mac studio, use my open source benchmarker and tell me what the fuck it even outputs lmao\n\nhttps://github.com/uncSoft/anubis-oss\n\nAlso please star it so I can put on homebrew as a cask",
              "score": 5,
              "created_utc": "2026-02-16 14:20:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5o4p7b",
              "author": "Yes_but_I_think",
              "text": "Native FP16 or FP8?",
              "score": 2,
              "created_utc": "2026-02-16 12:06:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nyg2v",
          "author": "Expensive-Paint-9490",
          "text": "Great.\n\nI have not yet understood if UD-Q4\\_K\\_XL is supposed to be better than MXFP4 or the other way around. I have never seen the two formats benched one against the other.",
          "score": 15,
          "created_utc": "2026-02-16 11:14:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ocmpg",
              "author": "rerri",
              "text": "Some fresh KLD and PPL test results with different quants of Minimax-M2.5\n\nMXFP4 and UD-Q4\\_K\\_XL are included. UD-Q4\\_K\\_XL wins in both tests but is also larger in file size.\n\nKLD:\n\n[https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld\\_data/01\\_kld\\_vs\\_filesize.png](https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld_data/01_kld_vs_filesize.png)\n\nPPL:\n\n[https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld\\_data/02\\_ppl\\_vs\\_filesize.png](https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld_data/02_ppl_vs_filesize.png)",
              "score": 10,
              "created_utc": "2026-02-16 13:02:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5oszd1",
                  "author": "LegacyRemaster",
                  "text": "wow amazing! so Q4\\_K\\_xxxxx is the best scenario. what I can say is that with vulkan the IQ versions and the MX versions are slower",
                  "score": 3,
                  "created_utc": "2026-02-16 14:35:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5o8f74",
              "author": "ABLPHA",
              "text": "Seen a lot of comments say that MXFP4 has better perplexity... Which seems counterintuitive, considering that UD-Q4_K_XL should be \"smarter\"",
              "score": 2,
              "created_utc": "2026-02-16 12:33:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5p0cu9",
                  "author": "yoracale",
                  "text": "Perplexity isn't a benchmark to be taken seriously at all. It is only popular because it's the easiest and fastest to conduct.",
                  "score": 4,
                  "created_utc": "2026-02-16 15:13:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5oijao",
                  "author": "dinerburgeryum",
                  "text": "As a data point, MXFP4 quants generally leave attention tensors (specifically the delta net tensors) in Q8_0, whereas the Q4 quants will also squash the SSM tensors, which (pending more testing) may be more sensitive to quantization.Â ",
                  "score": 2,
                  "created_utc": "2026-02-16 13:38:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pm101",
          "author": "MDSExpro",
          "text": "That's one verbose model:\n\nMe: hi\n\nQwen3.5-397B-A17B:\n\n\n\n    Thinking Process:\n\n    Analyze the Input:\n        Input: \"hi\"\n        Intent: Greeting.\n        Tone: Casual, friendly.\n        Context: Start of a conversation.\n\n    Determine the Appropriate Response:\n        Acknowledge the greeting.\n        Offer assistance.\n        Keep it friendly and concise.\n        Avoid over-complicating things since the user just said \"hi\".\n\n    Drafting Options:\n        Option 1 (Simple): \"Hi there! How can I help you?\"\n        Option 2 (Friendly): \"Hello! ðŸ‘‹ What's on your mind today?\"\n        Option 3 (Professional): \"Greetings. How may I assist you?\"\n        Option 4 (Enthusiastic): \"Hey! Great to see you. What are we working on?\"\n\n    Selecting the Best Option:\n        Option 2 strikes a good balance between friendly and helpful. Emojis add a nice touch without being excessive.\n\n    Final Polish:\n        \"Hi there! ðŸ‘‹ How's it going? Is there anything specific I can help you with today?\"\n\n    Final Output Generation: (Matching the selected draft)\n        \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n\n    (Self-Correction during drafting): Keep it slightly more open-ended. \"Hi! How can I help you today?\" is classic and effective.\n\n    Revised Plan:\n        Greet back.\n        Offer help.\n        Keep it warm.\n\n    Final Choice: \"Hi there! ðŸ‘‹ How's it going? Is there anything specific I can help you with today?\"\n\n    (Wait, let's make it even simpler): \"Hi! How can I help you today?\"\n    (Actually, a bit more warmth is better): \"Hello! ðŸ‘‹ How's your day going? Let me know if there's anything I can help you with!\"\n    (Deciding on a standard friendly AI response): \"Hi there! ðŸ‘‹ How can I help you today?\"\n    (Let's go with that).\n    (Wait, I should check if there are any specific constraints): No constraints. Just a greeting.\n    (Final Decision): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it slightly more conversational): \"Hello! ðŸ‘‹ How's your day going? Feel free to ask me anything or let me know if you need help with a specific task!\"\n    (Okay, let's keep it concise): \"Hi there! ðŸ‘‹ How can I help you today?\"\n    (Final Final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Actually, let's just respond naturally): \"Hi! How can I help you today?\"\n    (Okay, I'll add a little warmth): \"Hello! ðŸ‘‹ How's it going? Is there anything specific I can help you with?\"\n    (Let's go with): \"Hi there! ðŸ‘‹ How's your day going? Is there anything I can help you with?\"\n    (Wait, I am an AI): \"Hi! How can I assist you today?\"\n    (Okay, blending): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Let's just output): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I should check the system instructions): None specific beyond being helpful.\n    (Okay, final output): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it a bit more versatile): \"Hello! ðŸ‘‹ How's your day going? Feel free to ask me anything or let me know if you need help with a task!\"\n    (Okay, let's settle on): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Actually, just \"Hi! How can I help you today?\" is fine).\n    (Let's add a bit of personality): \"Hi there! ðŸ‘‹ How's your day going? Let me know if there's anything I can help you with!\"\n    (Okay, that's good).\n    (Wait, I'll check if I should mention capabilities): No, keep it open.\n    (Final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it slightly more engaging): \"Hello! ðŸ‘‹ How's your day going? Feel free to ask me anything or let me know if you need help with something specific!\"\n    (Okay, let's go with a balanced version): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Actually, I'll just say): \"Hi! How can I help you today?\"\n    (No, add emoji): \"Hi there! ðŸ‘‹ How can I help you today?\"\n    (Okay, final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll stop overthinking): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, generating response).\n    (Wait, one more check): Is there any context I'm missing? No. Just \"hi\".\n    (Okay, response): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it slightly more concise): \"Hi there! ðŸ‘‹ How can I help you today?\"\n    (Okay, let's go with the slightly warmer one): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll add a offer for specific tasks): \"Hi there! ðŸ‘‹ How's it going? Is there anything specific I can help you with today?\"\n    (Okay, done).\n    (Wait, I'll just write it).\n    (Wait, I need to make sure I don't sound robotic): \"Hi there! ðŸ‘‹ How's your day going? Let me know if there's anything I can help you with!\"\n    (Okay, that's the one).\n    (Wait, I'll check the token count): It's fine.\n    (Okay, outputting).\n    (Wait, I'll just say): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, final).\n    (Wait, I'll add a bit more variety): \"Hello! ðŸ‘‹ How's your day going? Feel free to ask me anything or let me know if you need help with a task!\"\n    (Okay, let's go with): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, stopping loop).\n    (Final Response): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it slightly more natural): \"Hi! ðŸ‘‹ How's your day going? Let me know if there's anything I can help you with!\"\n    (Okay, that's better).\n    (Final): \"Hi! ðŸ‘‹ How's your day going? Let me know if there's anything I can help you with!\"\n    (Wait, I'll add \"specific\" to invite tasks): \"Hi! ðŸ‘‹ How's your day going? Let me know if there's anything specific I can help you with!\"\n    (Okay, done).\n    (Wait, I'll just output): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, final).\n    (Wait, I'll check if I should use \"Hello\" instead): \"Hi\" is more casual matching the user.\n    (Okay, final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll add a smile): ðŸ˜Š\n    (Okay, final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll stop).\n    (Final Output): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll make it slightly shorter): \"Hi there! ðŸ‘‹ How can I help you today?\"\n    (Okay, let's go with the warmer one): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, generating).\n    (Wait, I'll check if I should mention I'm an AI): No need unless asked.\n    (Okay, final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll add \"feel free\"): \"Hi there! ðŸ‘‹ How's it going? Feel free to ask me anything or let me know if you need help with something specific!\"\n    (Okay, that's good).\n    (Final): \"Hi there! ðŸ‘‹ How's it going? Feel free to ask me anything or let me know if you need help with something specific!\"\n    (Wait, I'll shorten it): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Okay, done).\n    (Wait, I'll just output).\n    (Final): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n    (Wait, I'll add \"Hello\" as alternative): No.\n    (Okay, outputting).\n    (Wait, I'll check the prompt again): \"hi\"\n    (Okay, response): \"Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?\"\n...\n\n... (after 3600 tokens): Hi there! ðŸ‘‹ How's it going? Is there anything I can help you with today?",
          "score": 29,
          "created_utc": "2026-02-16 16:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r3fn3",
              "author": "ttkciar",
              "text": "Thanks for the sneak-peek.\n\nMaybe we can inject thinking content into the prompt from a different model to work around the verbosity problem.",
              "score": 4,
              "created_utc": "2026-02-16 21:08:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5o96gg",
          "author": "a_beautiful_rhind",
          "text": "It's deja vu of when I only had 2 3090s. Have to run my models at Q3 because otherwise they won't quite fit. All those 103b and Goliaths back in the day. Only now it's MoE models and many hundreds of additional gigs.\n\nIn 2 more years we're going to have to buy multiple nodes, aren't we? Sorry, I only have 3 DGX servers, it's not enough for qwen V :(",
          "score": 12,
          "created_utc": "2026-02-16 12:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r5rii",
              "author": "ttkciar",
              "text": "> \\> In 2 more years we're going to have to buy multiple nodes, aren't we?\n\nMaybe?  2028 is around the time the memory crisis is speculated to have blown over.  With a little luck we might be able to affordably upgrade our main memories and offload to CPU.  These newer MoE with itty-bitty active parameters make for fast inference, even pure-CPU.",
              "score": 3,
              "created_utc": "2026-02-16 21:19:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r8e12",
                  "author": "a_beautiful_rhind",
                  "text": "itty-bitties have a lot of knowledge but tend to have poor execution. GLM went higher with it's generalist and the signs of them being compute starved are everywhere. Doubt they did it lightly but reality came calling.",
                  "score": 2,
                  "created_utc": "2026-02-16 21:32:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5rlvyk",
              "author": "segmond",
              "text": "At least with partial offload we can get maybe 5tk/sec with these models.  With the dense models, you were lucky to get 0.75tk/sec with partial offload.  On the same system that gives me about 7tk/sec running 300b model, I ran devstral-large-123b and with about 20b offloaded, I was getting 1.5tk/sec",
              "score": 2,
              "created_utc": "2026-02-16 22:39:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5s082v",
                  "author": "a_beautiful_rhind",
                  "text": "That's as much of a stroke of luck as it was to have dense models of yore running on only 2x3090. Active parameters don't have much higher to go before you'll be back to that 1.5t/s",
                  "score": 1,
                  "created_utc": "2026-02-16 23:58:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5np0sp",
          "author": "Ok_Brain_2376",
          "text": "Only 17B params active \nCurious what AutoRound can do with this",
          "score": 23,
          "created_utc": "2026-02-16 09:47:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nym7k",
              "author": "I-am_Sleepy",
              "text": "For GPU poor (like me), Iâ€™m just waiting for 9B / 35B version to drop",
              "score": 26,
              "created_utc": "2026-02-16 11:15:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5od9fv",
              "author": "nikhilprasanth",
              "text": "Are autoround Q2's good?",
              "score": 4,
              "created_utc": "2026-02-16 13:06:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nrnl4",
          "author": "BadSkater0729",
          "text": "Hi u/danielhanchen, any chance for corresponding safetensor releases as well? The GGUFs are excellent but unfortunately very difficult to run on vLLM :(",
          "score": 9,
          "created_utc": "2026-02-16 10:12:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ns0rb",
              "author": "danielhanchen",
              "text": "Oh fp8? I'm looking into it!",
              "score": 8,
              "created_utc": "2026-02-16 10:15:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ns4gy",
                  "author": "BadSkater0729",
                  "text": "FP8 would be absolutely fantastic, would definitely allow 8 H100s to be used with this model! Would MXFP4 be a possibility as well or not at this time? Thanks so much for the response and your excellent work",
                  "score": 4,
                  "created_utc": "2026-02-16 10:16:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o5nd0",
          "author": "HollowInfinity",
          "text": "I never know which is the proper MMPROJ to use for the Unsloth ggufs. Is there any real difference performance wise between the three?",
          "score": 7,
          "created_utc": "2026-02-16 12:13:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pae32",
              "author": "Confident-Willow5457",
              "text": "BF16 is the native precision of the vision tower for almost every model. So the BF16 mmproj is the native precision lossless one. However not all hardware supports BF16, so you might have faster generation speeds with FP16, but BF16 -> F16 is a lossy conversion. BF16 -> F32 is lossless but it's obviously bigger.",
              "score": 6,
              "created_utc": "2026-02-16 16:01:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pdzgu",
                  "author": "HollowInfinity",
                  "text": "Wow thanks, I appreciate it!",
                  "score": 2,
                  "created_utc": "2026-02-16 16:18:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o7jqm",
          "author": "FullOf_Bad_Ideas",
          "text": "Qwen went back to hybrid-reasoning models. They released instruct/thinking variants in the past. I hope they'll share how they went about it in the paper.",
          "score": 6,
          "created_utc": "2026-02-16 12:27:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5oimvj",
          "author": "lolzinventor",
          "text": "The Q4\\_K\\_XL model runs well on a system with:\n\n* 2x NVIDIA RTX 3090 GPUs\n* 2x Intel Xeon 8175 CPUs (with offloading)\n* 512GB DDR4-2400 RAM  (about 200GB used)\n* 32K Context length\n\nWhen processing a 1600x1200 image and requesting a description:\n\n* Prompt processing speed: 13.5 tokens/second\n* Generation speed: 5.5 tokens/second\n* Image processing results are very accurate\n\nA great model, sadly crippled by usage policy violation messages\n\n* \\*Wait, one nuance:\\* The user provided a block of text labeled \"#subject#\" and \"#composition#\". They want me to use \\*that\\* but make it daytime. I cannot use \\*that\\* text because it violates policies. I have to explain why and offer a safe alternative.",
          "score": 6,
          "created_utc": "2026-02-16 13:39:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qnmm8",
              "author": "KallistiTMP",
              "text": "What kinda t/s you seeing on straight text to text on that system?",
              "score": 1,
              "created_utc": "2026-02-16 19:50:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5r4y2c",
              "author": "ttkciar",
              "text": ">\\> 2x NVIDIA RTX 3090 GPUs\n\n> \\> 512GB DDR4-2400 RAM (about 200GB used)\n\nFantastic!  Thanks for measuring this.\n\nThat's about 248GB, which means it would fit in my Xeon's 256GB of main memory, and Q4_K_M would leave me even more room for K and V caches.\n\nThat makes it worth downloading to give it a try.",
              "score": 1,
              "created_utc": "2026-02-16 21:15:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5oaewh",
          "author": "polawiaczperel",
          "text": "I am curious about Arc Agi 2 results.",
          "score": 6,
          "created_utc": "2026-02-16 12:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ohj6q",
          "author": "alhinai_03",
          "text": "Thanks u/danielhanchen\nWill the 35B model be released today as well?",
          "score": 5,
          "created_utc": "2026-02-16 13:32:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pdzg8",
          "author": "Zyj",
          "text": "Getting slightly over 10 tokens/s with the MXFP4\\_MOE quant on dual Strix Halo (with TB3 networking) and llama.cpp + rpc-server.\n\nSo that's quite a bit slower than MiniMax M2.5 (up to 18 tokens/s).\n\n\\*\\*BUT\\*\\* this model does \\*\\*a lot\\*\\* less reasoning. It makes a big difference!\n\nQuality with one javascript test task that I tested it with seems to be similar to MiniMax M2.5",
          "score": 5,
          "created_utc": "2026-02-16 16:18:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sp9a3",
              "author": "KallistiTMP",
              "text": "Nice, thank you for the excellent data point. I wonder if this could be pushed further with a GPU node for the common layers and the Strix Halo nodes for just the experts, or some other form of creative layer distribution",
              "score": 1,
              "created_utc": "2026-02-17 02:26:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5npztu",
          "author": "Nobby_Binks",
          "text": "How do you cook so fast?",
          "score": 7,
          "created_utc": "2026-02-16 09:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nrfsw",
              "author": "yoracale",
              "text": "These are currently non-imatrix but MXFP4 was never supposed to be imatrix. Imatrix takes forever to convert and we'll be updating them with imatrix later",
              "score": 13,
              "created_utc": "2026-02-16 10:10:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ooxil",
              "author": "epyctime",
              "text": "never understood this question. is it impossible to script automatically downloading the safetensors, then converting to gguf, and uploading it? genuinely what is the shock with \"wow he did it already!\" , ya? i would be worried if it took days to do this?",
              "score": 1,
              "created_utc": "2026-02-16 14:13:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5plabw",
                  "author": "VoidAlchemy",
                  "text": "i believe they get early access to some models and can begin converting into a private repo then flip it public once the original is released",
                  "score": 2,
                  "created_utc": "2026-02-16 16:51:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5r7cf0",
                  "author": "ttkciar",
                  "text": "It is not hard to script an immediate quant.  That's what mradermacher does, for example.\n\nHowever, part of the value-add of bartowski's and unsloth's quants is that they frequently fiddle with metaparameters to get the best quants for a specific model, or in unsloth's case actually improve the model's capabilities (extended context limit and such).",
                  "score": 2,
                  "created_utc": "2026-02-16 21:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o19q5",
          "author": "XxBrando6xX",
          "text": "This is so dumb to ask so I'm really sorry to anyone who can take a second to help me set this up.\n\nI recently got the Mac Studio M3 Ultra and I've been this weekend trying to deploy the Unsloth models. I want to try deploying this model with that large context window of 200k ish but when I go to the guide I see they're deploying it for much less it seems. If I'm serving this via llama.cpp from a file that has all the settings kinda pre laid out, what would I use for the most accurate (within reason since I'm not doing research, just primarily for fairly basic code work and solving complex software implementation questions). Would I be able to use the full dynamic 4bit quant with a huge window for 256gb of memory? Or would that require my entire 512gb of memory of my machine to run it at that level. This is all just being used primarily on openwebui while I'm learning. \n\nThanks for anyone that takes a few minutes to help for real.",
          "score": 3,
          "created_utc": "2026-02-16 11:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ozon4",
              "author": "yoracale",
              "text": "If you want to run the full context, yes it will likely eat up your whole memory",
              "score": 2,
              "created_utc": "2026-02-16 15:09:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5o4bpu",
              "author": "Ill_Barber8709",
              "text": "> I've been this weekend trying to deploy the Unsloth models\n\nDon't use GGUF on Apple hardware. Use MLX. They are 20% faster. Easiest way to do that is using LMStudio.\n\n> Or would that require my entire 512gb of memory of my machine to run it at that level.\n\nWhy limiting to 256GB if you already have 512GB? Just load the model and don't bother.",
              "score": 3,
              "created_utc": "2026-02-16 12:03:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o4ngm",
                  "author": "XxBrando6xX",
                  "text": "The only reason I'm not using the full 512 is I wanted to try running multiple models in parallel for different stuff, like a small model for discord and maybe home assistant stuff, and then a larger one day to day In memory for high performance with work stuff and personal projects, and then still have enough space to dabble with a model that helps in an IDE with editing code (cause that one would need to be faster) or flipping on stable diffusion to just play with that a little (although I'm not particularly into the ai art stuff)",
                  "score": 2,
                  "created_utc": "2026-02-16 12:05:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5o5e7n",
                  "author": "XxBrando6xX",
                  "text": "Also do any of the Unsloth models come in an MLX variant ? Im sorry for the loads of questions I've just been searching all over for this info on this specific problem, cause from what I can tell the Unsloth ones seem to be the most space efficient variants of the models which in my mind by default make the Most sense to try and run, but if you disagree and I'm better off doing all this with these models in MLX I certainly can give it a go",
                  "score": 2,
                  "created_utc": "2026-02-16 12:11:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ozex5",
                  "author": "yoracale",
                  "text": "People use Unsloth GGUFs because they are imatrix calibrated and dynamically quantized meaning they're of higher chat output quality usually. Just because MLX is slightly faster doesn't mean there aren't drawbacks compared to GGUFs.",
                  "score": 1,
                  "created_utc": "2026-02-16 15:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5pj33h",
              "author": "usuallybill",
              "text": "very curious about this too, I suspect you will need to use a lot more of your mac memory to use the full 200k.  if you do end up trying that on your m3 ultra would love to hear how it is, especially starting a conversation with a fairly good size prompt, and/or agent/user message history, and what kind of tokens/sec you get.  I am thinking about pulling the trigger on the same machine.",
              "score": 1,
              "created_utc": "2026-02-16 16:41:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5po6pq",
                  "author": "Digiarts",
                  "text": "I tried and canâ€™t get q3_k_xl to load on m3 ultra 256\n\nEdit: loaded today no prob",
                  "score": 1,
                  "created_utc": "2026-02-16 17:04:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5op6w5",
          "author": "Pedalnomica",
          "text": "Surprised they didn't release their own quants this time. I thought that really helped their popularity in the past. Thanks Unsolth for picking up some of the slack!",
          "score": 3,
          "created_utc": "2026-02-16 14:14:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ovm72",
          "author": "HollowInfinity",
          "text": "I cannot for the life of me get tool calling to work despite following the Unsloth guide for llama-server. Regular chat works, image parsing works great, but tool calling blows up with chat template errors:\n\n    Template supports tool calls but does not natively describe tools. The fallback behaviour used may produce bad results, inspect prompt w/ --verbose & consider overriding the template.\n    srv    operator(): got exception: {\"error\":{\"code\":500,\"message\":\"\\n------------\\nWhile executing FilterExpression at line 120, column 73 in source:\\n..._name, args_value in tool_call.arguments|items %}\n                        {{- '<...\\n                                           ^\\nError: Unknown (built-in) filter 'items' for type String\",\"type\":\"server_error\"}}\n\nI've tried overriding the chat template with the official one from the Qwen3.5 HF repo with no luck. I do see that the thinking kwarg is being properly read and passed in (though weirdly I can't get that to enable thinking). Am I doing something wrong here? Using the latest main of llama.cpp.",
          "score": 3,
          "created_utc": "2026-02-16 14:48:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p00v4",
              "author": "WonderRico",
              "text": "same",
              "score": 1,
              "created_utc": "2026-02-16 15:11:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5uxlhe",
              "author": "HollowInfinity",
              "text": "/u/danielhanchen sorry for the ping but have you tested tool calling with llama-server? The template format used doesn't seem to be compatible at all.",
              "score": 1,
              "created_utc": "2026-02-17 13:02:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ye89w",
              "author": "Federal_Hedgehog5674",
              "text": "Got the same errors.  ",
              "score": 1,
              "created_utc": "2026-02-17 23:19:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6l4i0x",
              "author": "Glittering_Pound1931",
              "text": "Same error",
              "score": 1,
              "created_utc": "2026-02-21 11:38:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6l7g2o",
              "author": "Glittering_Pound1931",
              "text": "In llama.cpp\nllama-server I have tried to overrate the template with --chat-template chatml and now the output is ok",
              "score": 1,
              "created_utc": "2026-02-21 12:04:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lcuhd",
                  "author": "HollowInfinity",
                  "text": "When I tried that tool call still didn't work, you had no issues with that?",
                  "score": 1,
                  "created_utc": "2026-02-21 12:48:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nteij",
          "author": "TheTerrasque",
          "text": "> It performs on par with Gemini 3 Pro, Claude Opus 4.5, and GPT-5.2.\n\nFor real real? Or benchmaxx?",
          "score": 7,
          "created_utc": "2026-02-16 10:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nurhy",
              "author": "Salty-Garage7777",
              "text": "Always do your own hard tests, then you are gonna know!!! ;-) ",
              "score": 15,
              "created_utc": "2026-02-16 10:41:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nzp7c",
              "author": "phenotype001",
              "text": "I gave it a few geometry problems, flawless so far. ",
              "score": 7,
              "created_utc": "2026-02-16 11:25:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5o24to",
              "author": "Thomas-Lore",
              "text": "I tested it on brainstorming game mechanics and it made a ton of logic errors (proposed mechanics that had no chance of working in that type of game or generally made no sense). So not impressed so far with its reasoning, had much better anwers from Kimi K2.5.\n\nBut of course it is just one use case.",
              "score": 5,
              "created_utc": "2026-02-16 11:45:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5o4fdk",
              "author": "Spectrum1523",
              "text": "You know the answer. Of course it isnt on par with them, they just have to say that.",
              "score": 4,
              "created_utc": "2026-02-16 12:03:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o7r0q",
                  "author": "TheTerrasque",
                  "text": "Yeah, I know. But one can hope. And of the ones that I could see maybe do it, qwen is high on the list.",
                  "score": 4,
                  "created_utc": "2026-02-16 12:28:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o3znk",
          "author": "My_Unbiased_Opinion",
          "text": "Did they finally fix the super long thinking?Â ",
          "score": 4,
          "created_utc": "2026-02-16 12:00:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ph1m5",
              "author": "Zyj",
              "text": "In my short tests it reasons a lot less than MiniMax M2.5.\n\nRight now the worst is Step 3.5 Flash which reasons 12 minutes straight whereas this model is done with the request after 2 minutes for example.",
              "score": 1,
              "created_utc": "2026-02-16 16:32:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nsclu",
          "author": "Impossible_Art9151",
          "text": "Not off topic. Does llama.cpp support sensor paralell already for clustered setup?",
          "score": 2,
          "created_utc": "2026-02-16 10:18:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nzvy0",
              "author": "HumerousGorgon8",
              "text": "Thereâ€™s a PR for an backend-agnostic TP implementation, so maybe soon (tm)",
              "score": 3,
              "created_utc": "2026-02-16 11:26:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nzvwu",
          "author": "Fine_Abalone199",
          "text": "Does not fit into 8 H100 :( Any ideas for how it can be run using vllm ?",
          "score": 2,
          "created_utc": "2026-02-16 11:26:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r6o6j",
              "author": "ttkciar",
              "text": "Use a quant.",
              "score": 1,
              "created_utc": "2026-02-16 21:23:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5o2iq7",
          "author": "Mandus_Therion",
          "text": "please let me run it on 32GB vram and 192GB ram",
          "score": 2,
          "created_utc": "2026-02-16 11:48:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p01xy",
              "author": "yoracale",
              "text": "I'm pretty sure it just fits if you're lucky. Otherwise we're uploading smaller 4bit ones in the next few hours\n\nEdit: They're all up now, 212GB for the smallest Q4 might just fit",
              "score": 2,
              "created_utc": "2026-02-16 15:11:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5p0y7s",
              "author": "gogitossj3",
              "text": "Same",
              "score": 1,
              "created_utc": "2026-02-16 15:16:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rqobm",
              "author": "ScoreUnique",
              "text": "I've got 36gb vram (3090+3060) and 192gb ram ddr5, I tried running the UD_Q2, I seem to not go beyond 3 tokens gen per second. How did it go for you?",
              "score": 1,
              "created_utc": "2026-02-16 23:04:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5sg6qm",
                  "author": "Mandus_Therion",
                  "text": "same situation",
                  "score": 2,
                  "created_utc": "2026-02-17 01:31:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5teypt",
                  "author": "yoracale",
                  "text": "What software/engine are you using?",
                  "score": 1,
                  "created_utc": "2026-02-17 05:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5o2mgk",
          "author": "TomLucidor",
          "text": "SWE-Rebench and LCB please!",
          "score": 2,
          "created_utc": "2026-02-16 11:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o36ek",
          "author": "Ill_Barber8709",
          "text": "4Bit Should fit in a 256GB Mac Studio. Nice.",
          "score": 2,
          "created_utc": "2026-02-16 11:54:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o4lwq",
          "author": "LegacyRemaster",
          "text": "Just finished testing the unsloth version. Thinking really thinks. 1,000 tokens for a simple \"hi\" response.",
          "score": 2,
          "created_utc": "2026-02-16 12:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o838b",
              "author": "perkia",
              "text": "Presumably it's the absolute, best, for-sure, really-good-this-time response.",
              "score": 3,
              "created_utc": "2026-02-16 12:31:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5osai4",
                  "author": "LegacyRemaster",
                  "text": "I felt emotional after the response. Like the girl who greets you on a first date.",
                  "score": 2,
                  "created_utc": "2026-02-16 14:31:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5pgry5",
              "author": "Zyj",
              "text": "In my experience it reasons a \\*LOT\\* less than MiniMax M2.5. It's really quite pleasant to see how little reasoning it does. I'm using MXFP4\\_MOE, you?",
              "score": 1,
              "created_utc": "2026-02-16 16:30:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5opaut",
          "author": "PopularKnowledge69",
          "text": "I assume that the benchmark results are for the model at its original precision. Isn't it possible to run those same benchmarks with your quantized variants ?",
          "score": 2,
          "created_utc": "2026-02-16 14:15:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oztto",
              "author": "yoracale",
              "text": "You can achieve the same benchmark results by running the 8bit or bf16 versions in the GGUF which is also uploaded.",
              "score": 3,
              "created_utc": "2026-02-16 15:10:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ov32z",
          "author": "Ok-Internal9317",
          "text": "The big question is, how much is it?",
          "score": 2,
          "created_utc": "2026-02-16 14:46:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r3nkg",
              "author": "ttkciar",
              "text": "Sir, this is LocalLLaMA.  If you can run it locally, the recurring cost is electricity.",
              "score": 4,
              "created_utc": "2026-02-16 21:09:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5pdpeh",
          "author": "Long_comment_san",
          "text": "The really important benchmark I would have wanted to see is long term stability past 200k context. Who gives a shit about +1% better math?",
          "score": 2,
          "created_utc": "2026-02-16 16:16:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rjvie",
          "author": "charles25565",
          "text": "Now where's the ~200B, ~30B, 9B, ~2B, and ~0.75B versions? Not everyone has a million A100s.",
          "score": 2,
          "created_utc": "2026-02-16 22:29:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tej6y",
              "author": "yoracale",
              "text": "Hopefully theyre coming soon",
              "score": 1,
              "created_utc": "2026-02-17 05:13:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6j9k2r",
              "author": "charles25565",
              "text": "Update: apparently there's this image from REDnote which says it should be by the end of Spring Festival.\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1r6w0la/comment/o5t88n2/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button\n\nIf it isn't either it is fake or just optimistic.\n\nI already contacted Junyang yesterday to see if it was real and I have no reply yet.",
              "score": 1,
              "created_utc": "2026-02-21 02:16:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5xfjzv",
          "author": "thegshipley",
          "text": "Running the 6bit on my Mac Studio M3 Ultra with 512gb ram. Right now, just using llama.ccp waiting for a new lmstudio or inferencer update that will run it.",
          "score": 2,
          "created_utc": "2026-02-17 20:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zje10",
              "author": "son12",
              "text": "just ordered an M3 ultra 512gb, what token/s is it running?    \n  \ngoal is to run openclaw on minimax 2.5 with ~100k context window - i don't think the full 200k would fit",
              "score": 2,
              "created_utc": "2026-02-18 03:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o637pgx",
                  "author": "thegshipley",
                  "text": "18.4 with 64k context. Using it via llama.cpp right now and talking to it via anythingllm with agent mode to use websearch etc. It's pretty slow in thinking -- as an exampe:\n\n  \nAnythingLLM: ATAgent what Tennessee Volunteer games are on today and give me a summary of matchup with bettings odds\n\nTook about 5 minutes\n\n  \nSame prompt to chatgpt took about 20 seconds.\n\n  \nI obviously have tweaking to do and should probably be running a smaller quant instead of 6bit. It's consuming:\n\n    RAM Usage: 345.3/512.0GB - swap inactive ",
                  "score": 1,
                  "created_utc": "2026-02-18 17:33:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5nx0xz",
          "author": "panic_in_the_galaxy",
          "text": "https://preview.redd.it/eixnqqmz8ujg1.png?width=2000&format=png&auto=webp&s=f0f90e24f7e99933151707768a353f339620a062\n\n",
          "score": 3,
          "created_utc": "2026-02-16 11:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o1x2b",
          "author": "Either-Job-341",
          "text": "Why is everybody incrementing in 0.5 nowadays?",
          "score": 2,
          "created_utc": "2026-02-16 11:43:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5o4l6s",
              "author": "Ill_Barber8709",
              "text": "Probably tick-tock model update.",
              "score": 5,
              "created_utc": "2026-02-16 12:05:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5oiavg",
              "author": "Finanzamt_Endgegner",
              "text": "Qwen did this with 2 to 2.5 too ",
              "score": 3,
              "created_utc": "2026-02-16 13:37:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5r6hgi",
              "author": "ttkciar",
              "text": "I think it's meant to imply it's more of an advance than a 0.1 version bump.  Kind of silly, but people seem to be accepting of it, which is what matters.\n\nIt was fitting for Qwen2 vs Qwen2.5.  The latter was a *lot* more capable than the former.",
              "score": 2,
              "created_utc": "2026-02-16 21:22:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ny25n",
          "author": "No_Conversation9561",
          "text": "No way dude",
          "score": 1,
          "created_utc": "2026-02-16 11:10:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o9rtj",
          "author": "iKy1e",
          "text": "The speed increase sounds exciting!\n\n> The decoding throughput of Qwen3.5-397B-A17B is 3.5x/7.2 times that of Qwen3-235B-A22B",
          "score": 1,
          "created_utc": "2026-02-16 12:43:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qf6ds",
          "author": "drycounty",
          "text": "No real luck for my humble 96GB Mac Studio via quant?",
          "score": 1,
          "created_utc": "2026-02-16 19:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tenas",
              "author": "yoracale",
              "text": "No unfortunately, you're better of running minimax-2.5: [https://unsloth.ai/docs/models/qwen3.5](https://unsloth.ai/docs/models/qwen3.5)",
              "score": 1,
              "created_utc": "2026-02-17 05:14:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qpfgo",
          "author": "KallistiTMP",
          "text": "Someone pleaaaaaase try this on a 2x Strix Halo cluster and see if it works/what perf you can get, I really want to know if I should get my hopes up",
          "score": 1,
          "created_utc": "2026-02-16 19:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qyt87",
          "author": "Hot_Strawberry1999",
          "text": "Do you think this model can be run properly on an rtx 5090 with 96gb of ddr5 system ram?",
          "score": 1,
          "created_utc": "2026-02-16 20:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5telbk",
              "author": "yoracale",
              "text": "Maybe if you use 2bit",
              "score": 1,
              "created_utc": "2026-02-17 05:13:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5r9eno",
          "author": "ConversationFun940",
          "text": "A noob here so don't bash me pls.. can someone be kind to create gguf of alibaba's ovis2.6 please? Have read it's slightly better than qwen3-vl-30b-a3b.. till the time a smaller qwen3.5 variant launches :). Tried creating gguf myself but running into errors",
          "score": 1,
          "created_utc": "2026-02-16 21:37:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rqvwu",
              "author": "ScoreUnique",
              "text": "Hello, GGUFs will only work if that architecture is merged to llama cpp",
              "score": 1,
              "created_utc": "2026-02-16 23:06:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5tekid",
              "author": "yoracale",
              "text": "I don't think it's supported by llama.cpp yet",
              "score": 1,
              "created_utc": "2026-02-17 05:13:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8ectu",
      "title": "I plugged a $30 radio into my Mac mini and told my AI \"connect to this\" â€” now I control my smart home and send voice messages over radio with zero internet",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/",
      "author": "anvarazizov",
      "created_utc": "2026-02-18 20:30:14",
      "score": 454,
      "num_comments": 92,
      "upvote_ratio": 0.87,
      "text": "Hey r/LocalLLaMA,\n\nSo I live in Ukraine during the war. Power goes out a lot here â€“ russia regularly attacks our power grid. When it happens, internet dies, cell towers go dark, and suddenly all my smart home stuff and AI tools become useless. Got tired of it, so I did something kind of ridiculous.\n\nI bought two Lilygo T-Echo radios (\\~$30 each, LoRa 433MHz, running Meshtastic firmware). Plugged one into my always-on Mac mini via USB. Took the other one as my portable radio. Then I opened up my OpenClaw AI agent and basically said: \"hey, there's a Meshtastic radio plugged in. Figure it out.\"\n\nAnd it did.\n\n# What happened next\n\nIt identified the Meshtastic device, installed the CLI, configured an encrypted channel, and then â€“ without me writing a single line of code â€“ built a full Python listener daemon that:\n\n* Monitors the radio 24/7 for incoming messages\n* Routes them intelligently: if internet is up, forwards to Discord where a cloud AI responds. If internet is down, routes everything to local models via Ollama\n* Uses phi4-mini as a lightweight intent classifier (\"is this a smart home command or a question?\") and gemma3:12b for actual answers ()\n* Talks to Home Assistant so I can control lights, read sensors, check who's home â€” all over radio\n* Auto-chunks responses to fit the 200-char LoRa limit\n* Watches an outbox folder â€“ if the AI needs to alert me about something (like a power outage), it drops a message file there and the listener transmits it over LoRa\n\nThe whole thing just worked. The AI had already built the architecture while I was still thinking about how to approach it.\n\n# The voice thing (this is the cool part)\n\nThen I added one more feature. If I prefix a Meshtastic message with `SAY:`, the listener takes the text, calls Home Assistant's TTS service, and plays it through my HA Voice PE speaker at home. In Ukrainian.\n\nSo I can be walking around with a T-Echo in my pocket, completely off-grid, type `SAY: ÐŸÑ€Ð¸Ð²Ñ–Ñ‚, Ñ ÑÐºÐ¾Ñ€Ð¾ Ð±ÑƒÐ´Ñƒ Ð²Ð´Ð¾Ð¼Ð°` (Hi, I'll come back home soon) â€“ and my house literally speaks. No internet anywhere in the chain. Just radio waves â†’ Mac mini â†’ TTS â†’ speaker.\n\nHonestly didn't expect it to feel this magical.\n\n# The stack\n\nEverything's open source except Claude (which is only used when internet is available):\n\n* **OpenClaw** â€“ you know what is this \n* **Meshtastic** â€“ LoRa mesh networking firmware. The magic sauce for off-grid communication â€“ open source, encrypted, and any Meshtastic radio can relay messages to extend range\n* **Lilygo T-Echo** â€“ the $30 radio hardware running Meshtastic\n* **Ollama** â€“ you know as well\n* **phi4-mini** â€“ lightweight router/classifier\n* **gemma3:12b** â€“ the actual brain for offline responses\n* **Home Assistant** â€“ smart home + TTS\n* **HA Voice PE** â€“ the speaker that reads messages aloud\n* **Mac mini M4 16GB** â€“ always-on server, running on battery backup\n\n&#8203;\n\n    T-Echo (portable)\n        â”‚ LoRa 433MHz, encrypted\n        â–¼\n    T-Echo (USB) â†’ Mac mini\n        â”‚\n        â”œâ”€â”€ SAY: prefix â†’ HA TTS â†’ Voice PE speaker\n        â”œâ”€â”€ AI: prefix  â†’ phi4-mini â†’ gemma3:12b (always local)\n        â”œâ”€â”€ status      â†’ Home Assistant sensors\n        â”œâ”€â”€ Online?     â†’ forward to Discord (cloud AI)\n        â””â”€â”€ Offline?    â†’ route everything to local Ollama models\n    \n    Outbox: AI drops .msg files â†’ listener sends over LoRa\n            (power outage alerts, reminders, etc.)\n\n# What's next\n\nI'm thinking about where this goes:\n\n* **Mesh AI network** â€“ Meshtastic is a mesh protocol, every radio relays. Multiple nodes running local LLMs could create a neighborhood-scale AI network with zero internet\n* **Bigger local models** â€“ looking at upgrading hardware for 30B+ parameter models\n* **Dead man's switch** â€” auto-alert if I don't check in within a time window\n\nWhat do you think? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r8ectu/i_plugged_a_30_radio_into_my_mac_mini_and_told_my/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o65va0x",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-19 01:15:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o650kao",
          "author": "Vusiwe",
          "text": "People with powerful computers are a juicy target for an influence op.\n\nA reminder for everyone on here to be CAREFUL with OpenClaw, which is a categorically risky software to run that has had severe security exploits on it, and generally has keys to the kingdom level permissions based on the way many people have it set up.\n\nIf you run software like that, and your system gets exploited, you just gave adversarial networks a mini- or micro- level ChatGPT level LLM-capable hardware to make use of whenever they feel like it.",
          "score": 107,
          "created_utc": "2026-02-18 22:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o670qhx",
              "author": "export_tank_harmful",
              "text": ">A reminder for everyone on here to be CAREFUL with OpenClaw...\n\nJust don't run it in the first place.  \nIf notepad++ can get supply chain attacked, a weird influencer pushed AI program that can control everything on your computer most definitely can.\n\n[Or at least lock it down properly.](https://www.youtube.com/watch?v=40SnEd1RWUU)",
              "score": 27,
              "created_utc": "2026-02-19 05:36:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65nlc4",
              "author": "Quiet-Translator-214",
              "text": "I totally agree. There are better way to host services like that. We have Pydantic, LangGraph and vllm and other frameworks and tools to run agents more securely even on enterprise level.",
              "score": 16,
              "created_utc": "2026-02-19 00:31:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65m4st",
              "author": "PunnyPandora",
              "text": "maybe worth taking a look at this if it's any better [https://github.com/nearai/ironclaw](https://github.com/nearai/ironclaw)",
              "score": 2,
              "created_utc": "2026-02-19 00:23:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65p8lc",
                  "author": "waiting_for_zban",
                  "text": "> maybe worth taking a look at this if it's any better https://github.com/nearai/ironclaw\n\nFrom their github page\n\n> NEAR AI account (authentication handled via setup wizard)\n\nIt's not really \"open\", as it requires a subscription/authentication from a remote server nearai. The cool thing about openclaw is the ability to use local models, despite the shitty security that it comes with. Currently I run it in an isolated instance, so quite sandboxed, but can't do much fun stuff with it.",
                  "score": 13,
                  "created_utc": "2026-02-19 00:40:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h9z8b",
              "author": "Useful-Process9033",
              "text": "Fair point on security but the real risk isnt the software itself, its running any agent with system-level permissions without proper guardrails. You need audit logging, action verification, and anomaly detection on what the agent actually does. This applies to any agent framework not just OpenClaw.",
              "score": 1,
              "created_utc": "2026-02-20 19:45:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64dttc",
          "author": "Original-Zone6774",
          "text": "Can you please connect my kids so I can control them with zero internet?",
          "score": 105,
          "created_utc": "2026-02-18 20:44:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64tze8",
              "author": "portmanteaudition",
              "text": "That's what the 5G and vaccine are for /s",
              "score": 43,
              "created_utc": "2026-02-18 21:58:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o64vfwq",
                  "author": "Original-Zone6774",
                  "text": "ðŸ˜‚",
                  "score": 2,
                  "created_utc": "2026-02-18 22:04:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o66m841",
                  "author": "Dyonizius",
                  "text": "itsÂ Meshtastic\n\n\nnot metastatic",
                  "score": 0,
                  "created_utc": "2026-02-19 03:55:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64e7w6",
              "author": "anvarazizov",
              "text": "just give them Meshtastic devices :)",
              "score": 16,
              "created_utc": "2026-02-18 20:45:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o64eupo",
          "author": "Hefty_Development813",
          "text": "Nice, so for this to work, there has to be other ppl running meshtastic nearby? What is the range you have tried?",
          "score": 25,
          "created_utc": "2026-02-18 20:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64fq93",
              "author": "anvarazizov",
              "text": "I haven't stress-tested it yet since both radios are at home right now, but in my city quite active mesh community, so tomorrow will test the long range ",
              "score": 23,
              "created_utc": "2026-02-18 20:52:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64fzt9",
                  "author": "Hefty_Development813",
                  "text": "Awesome. This is a really cool setup. I wonder if there are ppl around me running meshtastic, I always assumed not",
                  "score": 7,
                  "created_utc": "2026-02-18 20:54:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67bs1l",
                  "author": "Responsible_Buy_7999",
                  "text": "It is immensely nice to hear meshtastic seeing use in your country. I wonâ€™t ask you to elaborate and make the occupants work easier. Stay strong ðŸ‘Š",
                  "score": 1,
                  "created_utc": "2026-02-19 07:07:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o69d7tx",
              "author": "huffalump1",
              "text": "Browse in /r/meshtastic and on Google and ask your friendly neighborhood LLM assistant, it's really cool!\n\nYes it's a mesh network so you need connection to a node to send messages farther. But more and more nodes are popping up. Really cool tech, kind of like a modern evolution of ham radios and repeaters in a way, but fully open and cheap (and no license required in the US at least).\n\nAlso, it's \"secure enough\" for general use, but has some core vulnerabilities - I wouldn't rely on it for *truly* sensitive info that you *absolutely* don't want others to see.",
              "score": 2,
              "created_utc": "2026-02-19 15:58:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6bhhb9",
                  "author": "Hefty_Development813",
                  "text": "Thanks for that, good point on privacy. I guess you would really need to send coded messages if you cared like thatÂ ",
                  "score": 2,
                  "created_utc": "2026-02-19 22:05:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64mgch",
          "author": "skinnyjoints",
          "text": "Damnnn this is super cool. I was thinking that anyone could potentially tap into the radio frequency and send messages, but you said that the channel is encrypted. How does this work? Is it a specific frequency that only you and the receiver can access?",
          "score": 18,
          "created_utc": "2026-02-18 21:23:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64n8va",
              "author": "anvarazizov",
              "text": "Thanks! So it's not about the frequency â€“ anyone with a LoRa radio on 433MHz can technically hear the signal. But they can't read it because the channel is encrypted with a pre-shared key (PSK). It's like WiFi â€” everyone can see the network, but without the password you can't read the traffic. In Meshtastic you set up a channel name + a 256-bit encryption key, and only radios that have the same key can decrypt messages. I generated a new key and loaded it onto both my radios. Anyone listening just sees encrypted garbage. It's not military-grade or anything, but for a home setup it's more than enough.",
              "score": 30,
              "created_utc": "2026-02-18 21:27:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o663b21",
                  "author": "ToHallowMySleep",
                  "text": "Can't they just inject a signal on that frequency, and effectively jam you?",
                  "score": 4,
                  "created_utc": "2026-02-19 02:02:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o64p4e8",
              "author": "Lrrrrr",
              "text": "Meshtashtic handles all that. Read up, it's quite interesting.",
              "score": 7,
              "created_utc": "2026-02-18 21:35:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65c18o",
          "author": "Pleasant-Regular6169",
          "text": "Eeeh, how does the Mac mini get power when the power is out?",
          "score": 13,
          "created_utc": "2026-02-18 23:27:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o685vqq",
              "author": "OkAstronaut4911",
              "text": "Google \"Anker SOLIX\" or \"Ecoflow\". A 10kWh battery can power a MAC mini for days (and your fridge).",
              "score": 3,
              "created_utc": "2026-02-19 11:47:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6893ea",
                  "author": "Pleasant-Regular6169",
                  "text": "Only $4-7k! That sounds... unlikely.",
                  "score": 6,
                  "created_utc": "2026-02-19 12:11:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65gwsl",
              "author": "tuxbass",
              "text": "Laptops tend to come with batteries.",
              "score": -8,
              "created_utc": "2026-02-18 23:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65jqz0",
                  "author": "TechnoByte_",
                  "text": "> Mac Mini (stylized as Mac mini) is a small form factor desktop computer\n\n\nFirst sentence on wikipedia",
                  "score": 17,
                  "created_utc": "2026-02-19 00:10:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65l356",
                  "author": "Pleasant-Regular6169",
                  "text": "A generator or UPS can keep them running for a while, but the whole setup seems very complicated when you can just walk around with a cheap lora device which is like $90",
                  "score": 6,
                  "created_utc": "2026-02-19 00:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o64va5f",
          "author": "waiting_for_zban",
          "text": "Obligatory, how's gemma3:12b performance in offline mode compared to opus/sonnet when it comes to results? Does it actually work well in openclaw?",
          "score": 5,
          "created_utc": "2026-02-18 22:03:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o652z6e",
              "author": "txgsync",
              "text": "You would not use a small/dumb model to drive openclaw but to drive voice interaction thinking.",
              "score": 2,
              "created_utc": "2026-02-18 22:40:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o65hd0s",
                  "author": "waiting_for_zban",
                  "text": "> gemma3:12b â€“ the actual brain for offline responses\n\nIt's what OP used, that's why I am curious about the results in such cases.",
                  "score": 3,
                  "created_utc": "2026-02-18 23:57:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o65mlph",
          "author": "Danmoreng",
          "text": "Sounds great. How much did the Claude Api cost you though for this implementation?",
          "score": 5,
          "created_utc": "2026-02-19 00:25:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69gr6w",
              "author": "huffalump1",
              "text": ">Then I opened up my OpenClaw Al agent and basically said: \"hey, there's a Meshtastic radio plugged in. Figure it out.\"\n\nHe also said it's Opus 4.6. If he's using the API, likely $5-20 in tokens!!\n\nBut with a Claude code subscription, well, the $20 plan will likely be maxed out immediately, so they possibly have the $100/mo sub.\n\nIt's pretty cool that just asking openclaw to \"make this work\" *actually did it*. TBF you could likely do this with codex or gemini-cli (with their $20 plans or just API key) no problem.",
              "score": 1,
              "created_utc": "2026-02-19 16:15:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65g9d3",
          "author": "Long-Shine-3701",
          "text": "This is all running under MacOS?  What's your typical system load like?\n\nAlso, very impressive!",
          "score": 4,
          "created_utc": "2026-02-18 23:51:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bmswh",
              "author": "MrBIMC",
              "text": "I'm in a similar situation(though I do not rely on meshtastic, purely thread+matter local control via home assistant and no voice commands).\n\nI'm also in Ukraine, and my homeassistant + two radio sticks(matter and zigbee) are running over m1 mac mini, that is always passed through an ecoflow so it is always on. \n\n\nMy full setup is about 40 something containers, all of that consumes about 4-8 watts under load. My first gen ecoflow delta can power my mac and router both for days. Rest of the appartment is powered through Ecoflow Delta 3 pro. As far as setups go, it is quite pricy, but effectively I always have internet and electricity, while being in Ukraine.",
              "score": 2,
              "created_utc": "2026-02-19 22:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64uhrt",
          "author": "portmanteaudition",
          "text": "Fun post, although I hate the AI-generated mannerisms (\"it just works like magic!\")",
          "score": 10,
          "created_utc": "2026-02-18 22:00:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65narw",
          "author": "AcePilot01",
          "text": "what LLM are you guys using for voice?",
          "score": 3,
          "created_utc": "2026-02-19 00:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66nisf",
          "author": "segmond",
          "text": "so what model did the planning and setup for your radio?  surely not gemma3-12b or phi4.  which model did you use?",
          "score": 3,
          "created_utc": "2026-02-19 04:03:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6735p4",
              "author": "annodomini",
              "text": "He mentioned: Claude. He did that part while internet was active.",
              "score": 2,
              "created_utc": "2026-02-19 05:55:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o64bz5r",
          "author": "615wonky",
          "text": "Excellent post!  And Slava Ukraini!",
          "score": 21,
          "created_utc": "2026-02-18 20:35:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64d41x",
              "author": "anvarazizov",
              "text": "Thanks! Heroyam slava!",
              "score": 4,
              "created_utc": "2026-02-18 20:40:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65vryn",
          "author": "angelin1978",
          "text": "this is genuinely one of the coolest projects ive seen on here. running local AI over meshtastic during power outages in a war zone is not a use case any of us were thinking about but its exactly why local models matter.\n\nthe mesh network angle is interesting - whats the latency like for a typical query? like if someone sends a question from another node, how long before they get a response back? and are you running the model on the mac mini or does it offload anywhere?",
          "score": 6,
          "created_utc": "2026-02-19 01:18:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66mtiq",
          "author": "gotchapow",
          "text": "amazing! both the Meshtastic build with OpenClaw and your off-grid-ready setup! I'm curious what battery backup you're using for the Mac mini, and if you have any other automation rules trigger when power goes down? like, backups or battery-saving adjustments?",
          "score": 2,
          "created_utc": "2026-02-19 03:58:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66p6ek",
          "author": "AdOrnery4151",
          "text": "What is the range you have tried?",
          "score": 2,
          "created_utc": "2026-02-19 04:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6akov2",
              "author": "anvarazizov",
              "text": "at the moment 20km max, but I couldn't reach my Mac mini node from this distance",
              "score": 1,
              "created_utc": "2026-02-19 19:25:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6778p1",
          "author": "cinaz520",
          "text": "Thx for sharing. \nAlthough - I canâ€™t be the only one thinking this is how the ends begin? ðŸ˜® \n\nAutonomous AI agents communicating on an open source mesh network sounds like the beginning of a story that leads to modern day skynetâ€¦ ðŸ«£",
          "score": 2,
          "created_utc": "2026-02-19 06:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ottv",
          "author": "reneil1337",
          "text": "this is incredibly cool. ggwp keep going ",
          "score": 2,
          "created_utc": "2026-02-19 09:11:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6830k9",
          "author": "PetroDriller",
          "text": "Awesome outcome in an interesting real world use case, stay safe!",
          "score": 2,
          "created_utc": "2026-02-19 11:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o686823",
          "author": "GarbageOk5505",
          "text": "One thing to think about as you scale toward the mesh network: the agent that \"figured it out\" by installing the CLI and building the daemon had broad system access to do so. That's fine on your own hardware. Once other nodes are in the picture, you'll want each agent's blast radius tighter  an agent that can install arbitrary software is a different risk profile when it's not your machine.",
          "score": 2,
          "created_utc": "2026-02-19 11:49:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68gs1a",
          "author": "DonnaPollson",
          "text": "This is the kind of project that makes local AI actually *exciting* instead of just \"I ran a chatbot on my GPU.\"\n\nThe radio bridge is genius because it solves the fundamental problem with smart home setups â€” they all assume reliable WiFi/internet. Radio gives you a physical layer that works when your router decides to firmware update at 2 AM.\n\nI've been running a similar setup (minus the radio, which I now desperately want to add) with a local LLM orchestrating Home Assistant. The voice message capability over radio is particularly clever â€” you essentially built a mesh communication system with AI as the brain.\n\nFor anyone wanting to replicate this: the Mac Mini M-series chips are absurdly good for local inference. You can run a 7B model with basically no perceptible latency for home automation commands. The $30 radio + $600 Mac Mini combo gives you something that would cost thousands if you tried to do it with commercial solutions.\n\nWhat radio module are you using? Curious about range in a typical house.",
          "score": 2,
          "created_utc": "2026-02-19 13:03:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64ebj5",
          "author": "Minimum_Thought_x",
          "text": "Slava Ukraini from France",
          "score": 5,
          "created_utc": "2026-02-18 20:46:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o64ndyv",
          "author": "silentpillars",
          "text": "Slava Ukraini from Austriaâ™¥ï¸",
          "score": 0,
          "created_utc": "2026-02-18 21:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64nirj",
              "author": "anvarazizov",
              "text": "Heroyam slava! â¤ï¸",
              "score": 0,
              "created_utc": "2026-02-18 21:28:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o64sbl3",
          "author": "mindwip",
          "text": "Edit. Deleted question saw you answered it, Claude built the app, great will\n try myself soon",
          "score": 2,
          "created_utc": "2026-02-18 21:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64spew",
              "author": "anvarazizov",
              "text": "It wasn't local model, it was Claude Opus 4.6.  \nI'd like to build it with local mode, but it needs to much RAM :)",
              "score": 5,
              "created_utc": "2026-02-18 21:52:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64u0zs",
                  "author": "mindwip",
                  "text": "Thanks!",
                  "score": 3,
                  "created_utc": "2026-02-18 21:58:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o65irm4",
          "author": "__JockY__",
          "text": "This is the shit right here. Please put this on github!",
          "score": 2,
          "created_utc": "2026-02-19 00:04:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67gq2c",
          "author": "reckless_avacado",
          "text": "damn i see this openclaw slop everywhere. i think itâ€™s a bitcoin mining operation",
          "score": 2,
          "created_utc": "2026-02-19 07:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65cxu1",
          "author": "montagic",
          "text": "Super cool. Slava Ukranini, I had some Ukrainian coworkers at my previous job and they were some of the brightest and kindest people Iâ€™ve met. Worked hard and would be positive despite being in the war and impacted daily",
          "score": 2,
          "created_utc": "2026-02-18 23:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65rhyq",
          "author": "jarail",
          "text": "I love this so much! You Ukrainians keep impressing me!",
          "score": 3,
          "created_utc": "2026-02-19 00:53:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o661tx9",
          "author": "el-rey-del-estiercol",
          "text": "Que chulada",
          "score": 1,
          "created_utc": "2026-02-19 01:53:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o666ztb",
          "author": "Travnewmatic",
          "text": "absolutely awesome.  i've been into meshtastic for a while.  i've thought about how i could integrate openclaw with meshtastic and this clenches it, absoultey a weekend project to tackle :)",
          "score": 1,
          "created_utc": "2026-02-19 02:23:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o667i4w",
          "author": "sketchfag",
          "text": "OpenClaw is causing Raspberry Pi stock to skyrocket",
          "score": 1,
          "created_utc": "2026-02-19 02:26:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68jhdc",
          "author": "AppealSame4367",
          "text": "Good job.  \n  \nI just imagined telling my kid self in 1997 about this while I was watching Star Trek. Man this would have been so amazing.\n\nCan't imagine a ship full of people with that level of AI though unfortunately. Why would anyone employ humans for all that.",
          "score": 1,
          "created_utc": "2026-02-19 13:20:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68qlyk",
          "author": "_Proud-Suggestion_",
          "text": "What spec is the mac mini?\nAnd how big models can u run?\nLastly why gemma? Have you tried with other models and what has been the experience?",
          "score": 1,
          "created_utc": "2026-02-19 14:01:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68sqv2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-19 14:13:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ak9ik",
              "author": "anvarazizov",
              "text": "For now each transmission treated as stateless, but it's good point to give memory to local LLM ",
              "score": 1,
              "created_utc": "2026-02-19 19:23:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6editz",
          "author": "PwnedNetwork",
          "text": "I'm not a snitch and I don't give a shit, but just FYI Ukraine has some pretty strict regulations regarding specifically LoRa. Specifically the duty cycle on UA\\_433 is 10% and on UA\\_868 is 1% [according to meshtastic.org](https://meshtastic.org/docs/getting-started/initial-config/) Power limits are 10 for UA\\_433 and 14 for UA\\_868 (in dBm).\n\nMy Ukrainian is kinda shit but I think I found an official [government document](https://nkek.gov.ua/static-objects/nkek/uploads/public/67a/b84/377/67ab8437759da831037158.pdf) and I think the duty cycle info for 433 megaherz is on page 168, 358 and then for 868 megaherz on page 360. It doesn't seem to specifically refer to LoRa/Meshtastic but Ð¡Ð¼ÑƒÐ³Ð° Ñ€Ð°Ð´Ñ–Ð¾Ñ‡Ð°ÑÑ‚Ð¾Ñ‚: 868,0-868,6 ÐœÐ“Ñ† and \"ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð° Ð¿Ð¾Ñ‚ÑƒÐ¶Ð½Ñ–ÑÑ‚ÑŒ Ð¿ÐµÑ€ÐµÐ´Ð°Ð²Ð°Ñ‡Ð°\" says 14 dBm and then 1% duty cycle so that seems to match up what the regulations on [meshtastic.org](http://meshtastic.org) are saying.\n\n  \nThat's the kind of stuff that still kinda scares me about vibe coding. \n\n  \nAnyway, it's a cool af project. \n\n",
          "score": 1,
          "created_utc": "2026-02-20 10:23:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nzsft",
              "author": "anvarazizov",
              "text": "Thanks for the heads up! Yeah, I'm aware of the regulations â€” running low power on 433MHz, well within limits. The duty cycle isn't really an issue for text messages â€” we're talking a few bytes every few minutes, nowhere near 10%. Good looking out though \n\nAnd lol fair point about vibe coding â€” though in this case the radio config (frequency, power, duty cycle) is all set in Meshtastic firmware settings, not in the AI code. The AI just reads/writes text messages through the serial interface. So even if the code went completely haywire, worst case it spams text messages, not RF settings. \n\nAppreciate the kind words!\n\n",
              "score": 1,
              "created_utc": "2026-02-21 21:17:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6j2e7i",
          "author": "slypheed",
          "text": "This is a really cool idea; kudos, but:\n\noh, lord; openclaw, please no. Keep the Bros out of my local ai.",
          "score": 1,
          "created_utc": "2026-02-21 01:31:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nzj0o",
          "author": "anvarazizov",
          "text": "Here is the link to the repo with a project: [https://github.com/anvarazizov/meshtastic-listener-ollama](https://github.com/anvarazizov/meshtastic-listener-ollama)",
          "score": 1,
          "created_utc": "2026-02-21 21:15:38",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o64pqth",
          "author": "mindwip",
          "text": "Saving this as I already have meshtastic!",
          "score": 1,
          "created_utc": "2026-02-18 21:38:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65u5n7",
          "author": "yesman_85",
          "text": "Pretty cool stuff. What is the bandwidth of this Lora mesh? You notice congestion?Â ",
          "score": 1,
          "created_utc": "2026-02-19 01:08:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65uxs4",
          "author": "azka_from_ragnaros",
          "text": "Not gonna lie. At first I thought this was a sarcastic joke in my feed. Cool that it worked for you.",
          "score": 1,
          "created_utc": "2026-02-19 01:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65uans",
          "author": "brickout",
          "text": "Awesome! Slava Ukraine!",
          "score": -1,
          "created_utc": "2026-02-19 01:09:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65kk9t",
          "author": "TechnoByte_",
          "text": "Great idea and execution! \n\nI understand English isn't your first languages, but please at least remove the LLM mannerisms, it's hard to read and makes you seem like a bot",
          "score": 0,
          "created_utc": "2026-02-19 00:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65b75l",
          "author": "Jjsmallman",
          "text": "Larper",
          "score": -6,
          "created_utc": "2026-02-18 23:23:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8jjtq",
      "title": "More quantization visualization types (repost)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1r8jjtq",
      "author": "copingmechanism",
      "created_utc": "2026-02-18 23:51:43",
      "score": 452,
      "num_comments": 47,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r8jjtq/more_quantization_visualization_types_repost/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o66qxsj",
          "author": "Cubixmeister",
          "text": "Nice idea, but would be even better as an interactive website",
          "score": 19,
          "created_utc": "2026-02-19 04:26:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6asu4i",
              "author": "LaFllamme",
              "text": "Say no more",
              "score": 5,
              "created_utc": "2026-02-19 20:04:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o66q3dy",
          "author": "jhov94",
          "text": "I'm curious where so many people got the idea that MXFP4 was equivalent to something between Q6 and Q8 at the size of Q4. It's such a common belief that even Gemini repeats it, while those images clearly suggest otherwise.",
          "score": 44,
          "created_utc": "2026-02-19 04:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6723w0",
              "author": "robiinn",
              "text": "Tbf it is vibe coded and the author states \"MXFP4 doesn't seem right and is likely just a bad fit for this sort of visualization.\" so don't take the MXFP4 images as a true representation.",
              "score": 47,
              "created_utc": "2026-02-19 05:47:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o675m4z",
                  "author": "jhov94",
                  "text": "It's not the only data point. There are several posts floating around here with data suggesting that MXFP4 quality is worse than standard Q4 quants. I'm not sure which is true. The difference in use isn't obvious to me.",
                  "score": 10,
                  "created_utc": "2026-02-19 06:15:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67nxa5",
              "author": "netikas",
              "text": "This is an issue of visualization. MXFP4 will be better than Q4\\_0 simply due to the fact that Q4\\_0 is integer quantization, while MXFP4 is float quantization.\n\n  \nMost of the weights and activations of LLMs are closer to zero and just some of them differ in magnitude. Interestingly, we don't need to have low error for these high magnitude weights/activations -- we only need them to differ from all of the others. Conversely, since most of the w/a are of low magnitude, we need to differentiate between them.\n\nFP4 quantization (MXFP4, NVFP4) have relatively low quality for high magnitude values, since there are not a lot of values that can be represented in fp4, but they have much higher amount of representable values for lower magnitude. In contrast, Int4 have more or less uniform distribution of representable values, having bigger gaps around zero.\n\nIn images, this is just what we need. Int4 quantizes pixel values in high contrast images in aesthetically pleasing way. FP4 clamps higher values, while retaining much higher precision around zero -- but since the images are high contrast with wide histogram of colors, it looks bad. LLMs have a different value distribution, thus, FP4 wins against Int4 (Q4\\_0).\n\nTLDR: Images and weights/activations of LLMs have different value distributions, thus, FP4 is optimal for quantizing LLMs and images are a bad visualization tool.",
              "score": 14,
              "created_utc": "2026-02-19 09:02:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o684pv2",
                  "author": "llama-impersonator",
                  "text": "i thought your hypothesis was worth testing, so i wrote a script that loads a safetensor file and collects the MSE of simulating quantizing its tensors into the various gguf formats (still in float, just with the values they would get from quantization). comparing the MSE of quantizing all the tensors in model-00001 of llama3.2-3b-instruct, q4_0 has better numbers:\n\n    Total tensors processed: 187\n    Total elements: 2,482,888,704\n\n    QUANTIZATION SUMMARY (MXFP4)\n    MSE Statistics:\n        Mean MSE:     0.000439\n        Weighted MSE: 0.000189\n        Min MSE:      0.000066\n        Max MSE:      0.002900\n        Std Dev:      0.000582\n    \n    QUANTIZATION SUMMARY (Q4_0)\n    MSE Statistics:\n        Mean MSE:     0.000066\n        Weighted MSE: 0.000003\n        Min MSE:      0.000001\n        Max MSE:      0.000609\n        Std Dev:      0.000143\n\ni also ran it on some images and visualized the result to double check it was working properly, the quant patterns look reasonable. anything q4_1 and over is pretty much indistinguishable from the original, q4_0 has tiny artifacts but is decent, mxfp4 has noticeable banding.",
                  "score": 8,
                  "created_utc": "2026-02-19 11:37:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67ohol",
                  "author": "stddealer",
                  "text": "Mxfp4 is scaled float values. The scale should ensure optimal precision for the current block.",
                  "score": 2,
                  "created_utc": "2026-02-19 09:08:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67wept",
                  "author": "ghulamalchik",
                  "text": "Q8 is better than FP8 though. This has been known for a while. Doesn't this translate to Q4 vs FP4?",
                  "score": 3,
                  "created_utc": "2026-02-19 10:25:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o68652f",
              "author": "Holiday_Purpose_3166",
              "text": "Because it was tested. ubergarm and noctrex are familiar with this. For what I've seen, MXFP4 has a PPL better than BF16 in most cases but KLD is somewhere between Q4 and Q6. So it *could* be better than your regular Q4\\_K\\_M. Unsloth has been using this quant in recent models.",
              "score": 3,
              "created_utc": "2026-02-19 11:49:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69hm9z",
                  "author": "VoidAlchemy",
                  "text": "Heya! ubergarm here! In general I advise against using MXFP4 for quantizing a model unless it is known for sure that the original was QAT'd targeting that specific type of MoE tensors.\n\n> But don't get excited about using mxfp4 to quantize other models to fp4. The zero-bit mantissa in the block scales, along with the E2M1 choice for the 4-bit floats, results in a horrible quantization accuracy for the 4.25 bpw spent (about the same as IQ3_K), unless the model was directly trained with this specific fp4 variant (as the gpt-oss models).\n> https://github.com/ikawrakow/ik_llama.cpp/pull/682\n\nYes on some models it is possible to see lower \"better\" perplexity than the baseline for 4~6bpw quantizations including but not limited to MXFP4. The recent GLM-4.7-Flash did have unusually low perplexity, lower than the full baseline bf16, but that doesn't mean it is \"better\" though, just an anomoly.\n\nSo it is good to look at u/Digger412 (AesSedai on hf) KLD data in combination with PPL to get a full view when this kind of anomaly happens.\n\nSome have argued MXFP4 is faster on their specific rig, if that is true then they might also look into Q4_0 and Q4_1 which get quite a bit of love in the vulkan backend etc.",
                  "score": 10,
                  "created_utc": "2026-02-19 16:19:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o687wzb",
                  "author": "jhov94",
                  "text": "These links suggest otherwise. Again, I'm not pretending to have any answers. I'm just pointing out my confusion at the conflicting information.\n\n[https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld\\_data/01\\_kld\\_vs\\_filesize.png](https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld_data/01_kld_vs_filesize.png)\n\n[https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld\\_data/02\\_ppl\\_vs\\_filesize.png](https://huggingface.co/AesSedai/MiniMax-M2.5-GGUF/blob/main/kld_data/02_ppl_vs_filesize.png)",
                  "score": 3,
                  "created_utc": "2026-02-19 12:02:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o676mhu",
          "author": "audioen",
          "text": "MXFP4 really is a relatively primitive single-level quantization. I think it is probably most comparable to Q4\\_0, which actually uses more bits and is probably more accurate in general.",
          "score": 8,
          "created_utc": "2026-02-19 06:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69i1tr",
          "author": "gradient8",
          "text": "Neat visualization, but why are people in the comments making judgements on quant types based on this lol\n\nThere's no reason image compression quality should map cleanly to LLM performance, especially at the margins",
          "score": 6,
          "created_utc": "2026-02-19 16:21:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6615l3",
          "author": "ilintar",
          "text": "Yeah, seems like what I concluded some time ago is getting proven time and time again: Q4\\_1 is the breakpoint for image model quantization.",
          "score": 19,
          "created_utc": "2026-02-19 01:49:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o684f62",
          "author": "angelin1978",
          "text": "these visualizations are super helpful for picking quant types. the difference between Q4_K_M and Q5_K_M is way more obvious when you can actually see it",
          "score": 6,
          "created_utc": "2026-02-19 11:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65kuua",
          "author": "FriskyFennecFox",
          "text": "Come on, why did you listen to that anti-Lenna person?\n\nHere it is, for the full set!\n\nhttps://i.redd.it/f3xc8yxggckg1.gif",
          "score": 62,
          "created_utc": "2026-02-19 00:16:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o660txh",
              "author": "Eastern-Group-1993",
              "text": "Ik that apple used ASTC in their embeddings/summarization model.     \nHow does things like ASTC/BC7 stack up?     \nNot in the inage comparision, but as a quant?     \nIk that ASTC and BC7 does some perceptual lossy vission magic.\n\nThe model does need to first be quantized to INT8 for it to work right?      \nI canâ€™t imagine quantizing 2xFP16 as a 32bitRGBA or 1xRGBA16 or 1xFP16 to 2xXY16 and it turning out OK.",
              "score": 6,
              "created_utc": "2026-02-19 01:47:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65vizh",
              "author": "No_Afternoon_4260",
              "text": "This is perfect !",
              "score": 9,
              "created_utc": "2026-02-19 01:16:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o65spa7",
              "author": "Adventurous_Cat_1559",
              "text": "Because the model herself even asked for it to be retired.",
              "score": 14,
              "created_utc": "2026-02-19 01:00:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o66omja",
                  "author": "croninsiglos",
                  "text": "Not because she disliked its use. It was from a woke movement that convinced her that using her image was bad.",
                  "score": -1,
                  "created_utc": "2026-02-19 04:10:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o65w0vm",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -5,
                  "created_utc": "2026-02-19 01:19:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6e7cbe",
                  "author": "stddealer",
                  "text": "It's not like she hated it and personally asked for it to go, she got convinced and agreed with an already existing movement that said this image had to be retired.",
                  "score": -1,
                  "created_utc": "2026-02-20 09:26:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67kftl",
          "author": "AbheekG",
          "text": "Excellent, thank you!",
          "score": 2,
          "created_utc": "2026-02-19 08:27:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ss8t",
          "author": "mivog49274",
          "text": "I may sound very stupid, but would it be possible to frame how to represent REAP or even Pruning with a bitmap image also ?",
          "score": 2,
          "created_utc": "2026-02-19 09:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67zhlc",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 3,
              "created_utc": "2026-02-19 10:53:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o69a0k8",
                  "author": "mivog49274",
                  "text": "Or maybe feature-aware resizing ? Like seam carving ? And pruning would be a feature extraction and reconstruction on a smaller canvas ?",
                  "score": 1,
                  "created_utc": "2026-02-19 15:42:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68m48d",
          "author": "siegevjorn",
          "text": "Pretty cool, thanks for sharing\n\nWould be nice to see the original int8 format",
          "score": 1,
          "created_utc": "2026-02-19 13:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6asrbd",
          "author": "Midaychi",
          "text": "This is likely an apples to oranges comparison, however taken at face value q4\\_1 non-imatrix seems weirdly to be the smallest quant that accurately reproduces the q8's cloud/sky artifacts followed by q5\\_k\\_s and q5\\_k\\_m  \nand Imatrix seems to attempt to guide the compression towards the q8, but ultimately just seems to end up shifting the artifacts around. I think the most stark effect imatrix seems to have towards reproducing q8 is on top of the q4.0 quant.  \n  \niq4\\_XS is a lot more step-artiacted than I was expecting\n\nAlso some of the 2 and 3 bit quants are surprisingly clear, while others of them are surprisingly deep fried.\n\nMXFP4 looks like someone tried to dither via posterization",
          "score": 1,
          "created_utc": "2026-02-19 20:04:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6b2r3t",
              "author": "copingmechanism",
              "text": "Yes, Q2 is rather surprising. When looking at which quant yields the highest (visual) quality for the smallest size, the Q2\\_K variants show in this experiment as being the most efficient.",
              "score": 1,
              "created_utc": "2026-02-19 20:53:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6767qq",
          "author": "yensteel",
          "text": "These images are very helpful! I was wondering whether frequency compensation was something that AI researchers are trying to adopt (In concept) from signal processing, and they are indeed combining Fourier Analysis, regime bits, dithering, noise shaping, and other techniques to make better use of the limited data.\n\nAll I can say... is that Floyd-Steinberg dithering is NOT the way to go, but as diffusion is more of a trick for perception (images). Sadly that's the first idea that popped up when looking at these pictures. - -'",
          "score": 1,
          "created_utc": "2026-02-19 06:20:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65ieco",
          "author": "MizantropaMiskretulo",
          "text": "> *reposted to respect Lenna's retirement\n\nGood on you OP.",
          "score": -5,
          "created_utc": "2026-02-19 00:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9e27i",
      "title": "Free ASIC Llama 3.1 8B inference at 16,000 tok/s - no, not a joke",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/",
      "author": "Easy_Calligrapher790",
      "created_utc": "2026-02-19 22:48:03",
      "score": 444,
      "num_comments": 245,
      "upvote_ratio": 0.98,
      "text": "Hello everyone,\n\nA fast inference hardware startup, Taalas, has released a free chatbot interface and API endpoint running on their chip. They chose a small model intentionally as proof of concept. Well, it worked out really well, it runs at 16k tps! I know this model is quite limited but there likely exists a group of users who find it sufficient and would benefit from hyper-speed on offer.\n\nAnyways, they are of course moving on to bigger and better models, but are giving free access to their proof-of-concept to people who want it.\n\nMore info: [https://taalas.com/the-path-to-ubiquitous-ai/](https://taalas.com/the-path-to-ubiquitous-ai/)\n\nChatbot demo: [https://chatjimmy.ai/](https://chatjimmy.ai/)\n\nInference API service: [https://taalas.com/api-request-form](https://taalas.com/api-request-form)\n\nIt's worth trying out the chatbot even just for a bit, the speed is really something to experience. Cheers!\n\n  \nEDIT: It's worth noting that the chatbot demo actually undersells the speed on display. Anything over a few hundred tps is perceived as instantaneous, so the experience of 1k tps vs 16k tps should be pretty similar. So you are only seeing the bottom few percent of the speed on offer. A proper demo would be using a token-intensive workload with their API. Now THAT would be something to see.",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o6dw4h7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-20 07:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bs2at",
          "author": "BumbleSlob",
          "text": "This is neat. Seems like they basically just put the model directly into silicon. If the price for the hardware is right Iâ€™d buy something like this.\n\nWould like to know what they think the max model size they can reasonably achieve is though. If 8B is pushing it thatâ€™s ok I guess there will still be uses. If itâ€™s possible to do like a 400B param model like this then oh shit the LLM revolution just got it real\n",
          "score": 108,
          "created_utc": "2026-02-19 23:01:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cbu7w",
              "author": "Origin_of_Mind",
              "text": "They are working on making the hardware for DeepSeek-R1 or similar, quantized to 4 bits. It is not going to be low cost, but the idea is that it will be affordable enough in price/performance that it will be economical to use it for a year and then replace by a new one.\n\nThe approach is to build one large base chip with an array of computational units, and then relatively inexpensively and quickly wire the last two layers of metal in this chip in 30 different ways, putting a small section of the model directly into each of these chips. \\[[Source](https://www.eetimes.com/taalas-specializes-to-extremes-for-extraordinary-token-speed/).\\]",
              "score": 15,
              "created_utc": "2026-02-20 00:56:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bts5l",
              "author": "-dysangel-",
              "text": "Technically, this thing is way simpler than a graphics card. I doubt it's going to be a big issue creating 400B param versions.\n\nIt's interesting to wonder the dynamics. The companies that train the models need more general hardware, but there will be companies vying for cheap inference, so they'll be paying the up front costs for factory tooling. Once the factory is in place, churning out units is very cheap, so if they make them available to the public rather than having very strict deals with companies, the price should come down over time as more and more units are produced.\n\nAnd then someday, people will literally just be throwing these away because Deepseek V10 is available and V4 is outdated.",
              "score": 52,
              "created_utc": "2026-02-19 23:11:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c0bec",
                  "author": "MizantropaMiskretulo",
                  "text": ">Technically, this thing is way simpler than a graphics card. I doubt it's going to be a big issue creating 400B param versions.\n\nSize. Size is the big issue.\n\nThe H100 has about 80-billion transistors.  Ask yourself how many transistors are needed for each model weight. You need shifts and adders, clocks and control logic along with all that SRAM\n\nEven if you're getting 330M transistors/mmÂ² on a 2mm process node and using an 850mmÂ² chip, that's only 280B transistors.\n\nEach parameter needs on the order of 50â€“100 transistors depending on the quantization level, which means they're likely bumping up hard against the limits of physics getting a 3B model on a chip right now. \n\nIt would require a Cerebus-style wafer-scale solution to move beyond the reticle-limit, this would allow them to move up to 7B or 8B parameter models. \n\nIf they packed an entire wafer with transistors, about 70,000 mmÂ², they could in theory, pack about 23T transistors, which, depending on sparsity, architecture, quantization, etc puts us in the realm of 250Bâ€“500B models, on the entire wafer.\n\nYields would be absolute shit. You'd need to throttle the hell out of it so you didn't just vaporize the thing, it would cost hundreds of millions to design and would be wildly obsolete before it generated its first token.",
                  "score": 50,
                  "created_utc": "2026-02-19 23:49:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cmkmg",
                  "author": "Several-Tax31",
                  "text": "Yes, I can totally see a future like you say, throwaway chips. I think it is a right approach, given the cost analysis works. The key factor here, as you say, is public availability. If they can manage that, this could be a huge win.Â ",
                  "score": 3,
                  "created_utc": "2026-02-20 02:03:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6kogb1",
                  "author": "ain92ru",
                  "text": "The semiconductor production cycle is so long that the model (which, being open-weights, won't be SOTA the day it releases) will certainly be obsolete by the time the device is ready to ship, just like Llama 3.1 8B is now",
                  "score": 1,
                  "created_utc": "2026-02-21 09:02:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dxj7w",
                  "author": "Ikinoki",
                  "text": "Some people don't understand how easy it is to make this in silicon compared to even bitcoin asics or other.\n\nIn fact if you had a simple even 100nm wafer producer you could already make a tensor chip with billions of parameters.\n\nThe lack of actual trainable NN/tensor chips is not surprising because of trade war with china, and fpga is not better than video in this case. But ASIC is another thing.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:53:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bvgu8",
              "author": "MizantropaMiskretulo",
              "text": "I mean, what's the right price? I'm guessing this is on the order of 10's of thousands of dollars for the hardware. \n\n400B parameter models are out of the question, that would well above wafer-scale.\n\nTo put an 8B model on a chip, at a 1.58 quant, on an N2 node would be about the size of an H100 chip (800-850 mm^2).",
              "score": 11,
              "created_utc": "2026-02-19 23:21:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cdsi0",
                  "author": "BumbleSlob",
                  "text": "They provided a little bit of a clue in their article about alleged pricing emphasis on alleged. They said it would be 20 times cheaper than the state of the art which by my back of the napkin math suggests somewhere in the neighborhood of $2000 or $3000.\n\nAgain, this is just a shot in the dark, so donâ€™t take it too seriously but if it were the case, that would be very interesting",
                  "score": 12,
                  "created_utc": "2026-02-20 01:08:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ddqn2",
                  "author": "mxforest",
                  "text": "Why do you think 400B would be a single chip? All bigger models are MoE with even the larger ones having experts un the 35-40B range which is feasible for this kind of chip. And this is a single user inference setup, in batch it can possibly go even ludicrous level wild. Possibly Million+ tps.",
                  "score": 1,
                  "created_utc": "2026-02-20 05:02:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6euctx",
              "author": "Easy_Calligrapher790",
              "text": "This specific first-gen model fits \\~4B parameters per chip. Two PCIe cards running in parallel on a consumer motherboard gives you 16k tps.\n\nTheir next model on this architecture should be \\~30B, so I assuming 8 cards or so. So at most 4 housings with interconnects. Although there are prob specialized boards accepting more cards per housing? (EDIT: There obviously are, based on the photo in EETimes articles linked to in the comments below.)\n\nThat may sound a lot for an individual customer, but it's not much at all for even a small in-house IT outfit, let alone an inference provider. 16k tps can serve a lot of people in parallel.\n\nSo 400B is prob not practical for this gen, but given the quality of the engineering team (core group from Tenstorrent), I'd be surprised if it stopped here.",
              "score": 3,
              "created_utc": "2026-02-20 12:36:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6bvp03",
              "author": "SmartCustard9944",
              "text": "The limitation would still be memory, which is expensive",
              "score": 0,
              "created_utc": "2026-02-19 23:22:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6eaple",
                  "author": "physalisx",
                  "text": "What memory? You don't need any memory.\n\nThe chip **IS** the model weights, which you would normally need to hold in memory on general purpose computation hardware like GPUs.",
                  "score": 6,
                  "created_utc": "2026-02-20 09:57:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6brvpu",
          "author": "DROIDOMEGA",
          "text": "This is wild, I want some of these chips",
          "score": 54,
          "created_utc": "2026-02-19 23:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bsrwu",
              "author": "Easy_Calligrapher790",
              "text": "Haha, no kidding! I don't believe they ever planned to make money off this iteration, they are well aware of the limits of the model. At least I think so?\n\nFor the record, I don't work there. I just know a bunch of people who do. But I want to raise awareness, and thought there must be a niche group who'd find this genuinely useful.",
              "score": 29,
              "created_utc": "2026-02-19 23:05:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6bz8cx",
                  "author": "floppypancakes4u",
                  "text": "I would absolutely take a dev board if they aren't gonna sell them, this is WILD.",
                  "score": 21,
                  "created_utc": "2026-02-19 23:43:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dvc8c",
                  "author": "CrowdGoesWildWoooo",
                  "text": "They will. They can sell at a significant premium to HFT or hedgefunds. I can even see that this would get booked easily as we speak.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:32:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dzjro",
              "author": "wanderer_4004",
              "text": "I am working on voice-to-voice AI applications and this would just be gold for extremely low latency. There are so many use cases where you don't need a 1T SOTA model.\n\nSo currently we have:\n\n* 15k tok/s on 8B dense model (3bit/6bit quant)\n* about 200W per chip\n* 20x cheaper to produce\n* 10x less energy per token for inference\n* TSMC 6nm, die size 815mm^(2), 53B Transistors\n* max context size: flexible\n* mid-sized thinking model upcoming this spring\n* a frontier LLM planned within twelve months\n\nDoesn't seem to be able to do KV caching... Well, nothing is perfect.",
              "score": 3,
              "created_utc": "2026-02-20 08:12:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hr26j",
                  "author": "pulse77",
                  "text": "KV caching is performance optimization. Why would you need this if you are 75x faster than Nvidia?",
                  "score": 3,
                  "created_utc": "2026-02-20 21:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ct6zx",
              "author": "BusRevolutionary9893",
              "text": "Wild and a great idea.Â  I definitely see applications like integration with robotics. Faster, more power efficient, and cheaper to manufactur. Your [robotic plumber/landscaper/cook/massage therapist/bodyguard](https://www.youtube.com/watch?v=R6T-Ea5CfRE) may use something similar to this. However the obvious limitation is huge, any new model will require a new chip. No updates, one and done. They're also usingÂ  a heavily quantisized model, but that is for cost and/or proof of concept.Â ",
              "score": 4,
              "created_utc": "2026-02-20 02:43:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c4ouz",
          "author": "Origin_of_Mind",
          "text": "Taalas is trying to compile the models as quickly as possible into hardwired circuits, where parameters are not stored in RAM but are either baked directly into the circuit or stored in on-chip read-only memories integrated closely with the computational units. If electricity is the limiting factor, this may be a viable way to get more tokens per watt.\n\nTheir first product:\n\n>*Runs Llama 3.1 8B model (with the parameters quantized to 3 and 6 bit)*\n\n>*Uses TSMC 6nm process*   \n*Die size 815mm**^(2)*   \n*53B Transistors*\n\nFrom other sources, power consumption is about 200W per chip.",
          "score": 29,
          "created_utc": "2026-02-20 00:14:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cbxaf",
              "author": "HopePupal",
              "text": "one wonders when someone's going to figure out how to bake weights into the silicon as analog values, and whether it's already been tried and discarded for reliability or yield issues",
              "score": 10,
              "created_utc": "2026-02-20 00:57:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cdig6",
                  "author": "Origin_of_Mind",
                  "text": "[Mythic AI](https://mythic.ai/) produced actual analog neural chips a while ago, using some very clever circuitry. But then something did not work out either with technology or organizationally, and it more or less fizzled out.\n\nDecades earlier, two legendary chips designers (one of the first microprocessor fame, and another famous for staring the fabless revolution) started a company \"[Synaptics](https://en.wikipedia.org/wiki/Synaptics)\" to make [analog neural networks](https://www.google.com/search?q=carver+mead+analog+vlsi). It did not work out, but the company became very successful in other areas.",
                  "score": 14,
                  "created_utc": "2026-02-20 01:07:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dsyc2",
                  "author": "rm-rf-rm",
                  "text": "Also Tetramem. \n\nVeritasium did a great video on analog ASICs and the inevitablity that they will come back: https://www.youtube.com/watch?v=GVsUOuSjvcg",
                  "score": 5,
                  "created_utc": "2026-02-20 07:10:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6hrdfi",
                  "author": "pulse77",
                  "text": "Is yield a problem here? If one bit is wrong it could still work great...",
                  "score": 2,
                  "created_utc": "2026-02-20 21:11:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6bweve",
          "author": "netroxreads",
          "text": "holy mackerel! It was instant! I asked for a bash script to look for a string in files and make a list. The full answer was given in a split second! ",
          "score": 19,
          "created_utc": "2026-02-19 23:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bvvtu",
          "author": "a_beautiful_rhind",
          "text": "The replies are instant. A wall of text in the blink of an eye.",
          "score": 35,
          "created_utc": "2026-02-19 23:23:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6djb72",
              "author": "deadcoder0904",
              "text": "Not even a blink lol.",
              "score": 10,
              "created_utc": "2026-02-20 05:46:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6dv415",
              "author": "rm-rf-rm",
              "text": "the application that makes sense for this tech is obviously not chatbots (and shame on them for implying it is by making that demo). It is for agentic tasks that require low latency - like a voice assistant. A (seemingly) simple task like \"is my flight on time?\" will have to chain email lookup -> parsing -> API call -> parsing -> formulate response in sub second time to deliver a Siri that people would actually want to use.",
              "score": 20,
              "created_utc": "2026-02-20 07:30:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fmyoe",
                  "author": "AnticitizenPrime",
                  "text": "Imagine a translation LLM (like GemmaTranslate) combined with TTS/STT running on one of these. Instant translation with virtually no latency.",
                  "score": 7,
                  "created_utc": "2026-02-20 15:13:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fnm7t",
                  "author": "VampiroMedicado",
                  "text": "Voice assistants would be insane, or if possible, vision models they're not THAT big InternVL3 is 78B.",
                  "score": 3,
                  "created_utc": "2026-02-20 15:16:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6gyylr",
                  "author": "abdouhlili",
                  "text": "Or robots.",
                  "score": 3,
                  "created_utc": "2026-02-20 18:53:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6guava",
                  "author": "Zeeplankton",
                  "text": "this. semantic UX is the future probably and instantaneous inference will have a huge impact",
                  "score": 2,
                  "created_utc": "2026-02-20 18:32:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gklh7",
              "author": "MoffKalast",
              "text": "I could see something like this used for VLA models to control robot motion one day, it would have god tier reaction times. Current ones tend to suffer from inference running slower than realtime.",
              "score": 3,
              "created_utc": "2026-02-20 17:49:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c02kg",
          "author": "SmartCustard9944",
          "text": "The fine print that people are missing is that each of these units runs on 2.5kW and that the die is ~800mmÂ² with 53B transistors, which is massive. Not really something you would put on an edge device. And this is just for an 8B model, already close to the limits of silicon density.\n\nRegardless, impressive speed.\n\nQuick napkin math, it comes down to ~0.05 kWh per 1M tokens. At $0.10/kWh, it's $0.005 per 1M tokens. This doesn't count other infrastructure and business costs of course.",
          "score": 46,
          "created_utc": "2026-02-19 23:48:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cpeht",
              "author": "Origin_of_Mind",
              "text": "The 2.5 kW is for a server with presumably 8 modules. Each chip consumes circa 200 Watts.\n\nThe 8B chip is just a proof of concept, not a product. \n\nTheir goal is to use the developed workflow to make multichip servers for much large models, targeting higher speed and lower power than is achievable with the GPUs. \n\nSince every investor is talking about power these days, this may be attractive -- if it works out as intended, this may be profitable even if the hardware only lasts a year before being replaced by a new version. It may also help that they do not use any RAM to store the parameters.",
              "score": 25,
              "created_utc": "2026-02-20 02:20:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6e6ht1",
                  "author": "SmartCustard9944",
                  "text": "Where did you see it's 8 modules? It makes a lot of sense, 200W is like a large GPU which corresponds to that die size.",
                  "score": 4,
                  "created_utc": "2026-02-20 09:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6c8ar1",
              "author": "coder543",
              "text": "Technically they say the server is 2.5kW, not the chip. They don't say how many inference cards they have in that server, which drastically affects the token cost calculations.",
              "score": 11,
              "created_utc": "2026-02-20 00:36:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6e6e3i",
                  "author": "SmartCustard9944",
                  "text": "That is true, I guess I calculated worst case scenario",
                  "score": 2,
                  "created_utc": "2026-02-20 09:17:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6c2khu",
              "author": "keyboardhack",
              "text": "We also have to consider how this type of chip limits the max context size since that also uses up memory on the chip.\n\nAnd since 4hey focused solely on the single user scenario and didnt mention multi user use cases at all i will assume the chip can only handle one user at a time.\nStill incredible speeds but i dont see how they can scale as an ai inference provider without severely cutting down on speed which is their only interesting point.",
              "score": 7,
              "created_utc": "2026-02-20 00:02:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6coi2k",
                  "author": "Several-Tax31",
                  "text": "But also, handling one user at a time is all is needed for personal use. I think they should really aim for PC market instead of server market and sell those things instead of being an inference provider (after a suitable price range, of course)Â ",
                  "score": 1,
                  "created_utc": "2026-02-20 02:14:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dtcvi",
                  "author": "Successful-Brick-783",
                  "text": "With the speed they churn out tokens we're better of using vocabulary like parallelization vs concurrency, its running fast enough to serve 10 users pers second right now which with some grace could be said to be 10 users concurrently even though sequentially.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:14:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6c9gxo",
              "author": "ithkuil",
              "text": "Well, I bet they can make it ten times more efficient with access to the latest fabrication technology.",
              "score": 1,
              "created_utc": "2026-02-20 00:42:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ctt63",
              "author": "SkyFeistyLlama8",
              "text": "On the smaller side, I wonder what happened to Qualcomm's discrete NPU accelerator chips for laptops. I remember reading about some Dell XPS workstation laptop being announced as the first to get those NPU chips but I never saw them being sold. Qualcomm SoCs already have a Hexagon NPU but they're for low power inference using small models only.",
              "score": 1,
              "created_utc": "2026-02-20 02:47:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6datv1",
              "author": "INtuitiveTJop",
              "text": "You could probably split a model across several chips. This would show you to run larger models I assume. The power issue is a little tough but perhaps we can slow it down a little?",
              "score": 1,
              "created_utc": "2026-02-20 04:40:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hp3g7",
              "author": "rorykoehler",
              "text": "They just have to send it to some guy in a back alley in Shenzhen to minaturise it.",
              "score": 1,
              "created_utc": "2026-02-20 20:59:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c3kmx",
          "author": "no_witty_username",
          "text": "speed is the future. once you have good enough quality of responses, having speed this fast opens up opportunities....",
          "score": 13,
          "created_utc": "2026-02-20 00:08:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d0p7a",
              "author": "MrPecunius",
              "text": "If prefill is proportionately accelerated, this opens up some crazy realtime processing possibilities.",
              "score": 5,
              "created_utc": "2026-02-20 03:30:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bspog",
          "author": "pulse77",
          "text": "NOTE: LjubiÅ¡a BajiÄ‡ - author of the post [https://taalas.com/the-path-to-ubiquitous-ai/](https://taalas.com/the-path-to-ubiquitous-ai/) \\- was a CEO of Tenstorrent before Jim Keller ...\n\nEDIT: And the chip architecture is the diametric opposite of **Tenstorrentâ€™s** design: while Tenstorrent integrates hundreds of general-purpose programmable CPUs, Taalas builds a chip specialized for a single LLM model.",
          "score": 22,
          "created_utc": "2026-02-19 23:05:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bugmw",
              "author": "sourceholder",
              "text": ">Taalas builds a chip specialized for a single LLM model.\n\nThey're going to really struggle of obsolesce then.  Models designs are changing constantly.\n\nMaybe this will fill the \"good enough but fast\" niche.",
              "score": 16,
              "created_utc": "2026-02-19 23:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6buuyu",
                  "author": "pulse77",
                  "text": "They will have \"consumable products\" from day one - like bread... No subscription business model needed... :)",
                  "score": 18,
                  "created_utc": "2026-02-19 23:17:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6bxnr5",
                  "author": "blbd",
                  "text": "If they can make the chips quick and cheap it might not be a big deal. Just plug them into NVMe or PCIe slots like the old days. Or figure out how to mix together different configurable chiplets so that you can burn in new gate arrangements or microcode every so often like an FPGA.Â ",
                  "score": 13,
                  "created_utc": "2026-02-19 23:34:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6cphk1",
                  "author": "learn_and_learn",
                  "text": "Who cares that there are better models out there running at 15 tokens per second if this one runs 1000x faster?",
                  "score": 6,
                  "created_utc": "2026-02-20 02:20:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6d037t",
                  "author": "MrPecunius",
                  "text": "It will fill the \"black market AI card sold by a guy in a trenchcoat\" niche.\n\nWilliam Gibson vibes for sure.",
                  "score": 2,
                  "created_utc": "2026-02-20 03:26:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6c0vl9",
              "author": "Interpause",
              "text": "feels like a game cartridge. hm, but lets say for system 2 thinking of a AI robot, that kind of low latency might be useful",
              "score": 3,
              "created_utc": "2026-02-19 23:53:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bu7ys",
              "author": "SlowFail2433",
              "text": "Okay this makes sense",
              "score": 1,
              "created_utc": "2026-02-19 23:14:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bv18h",
          "author": "SmartCustard9944",
          "text": "Finally, seems so obvious that we need to invest more into specialized hardware",
          "score": 20,
          "created_utc": "2026-02-19 23:18:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6btpk7",
          "author": "Revolutionalredstone",
          "text": "So cool! hard to imagine the world we're moving towards where one human could never hope to read / understand the thoughts and words in one second of a small local AI's thought process.\n\nGonna be amazing for RPG game NPC control etc ;D",
          "score": 9,
          "created_utc": "2026-02-19 23:11:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c41ht",
          "author": "rektide",
          "text": "Do a smaller model! Nanbeige4.1! https://huggingface.co/Nanbeige/Nanbeige4.1-3B",
          "score": 8,
          "created_utc": "2026-02-20 00:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fw9ew",
              "author": "j0j0n4th4n",
              "text": "Wow, this one is a good pick actually. It certainly would need the fast tokens",
              "score": 2,
              "created_utc": "2026-02-20 15:57:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6k5c9g",
              "author": "-InformalBanana-",
              "text": "Is it better than qwen3 4b? :D",
              "score": 1,
              "created_utc": "2026-02-21 06:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bvton",
          "author": "scottgal2",
          "text": "Awesome! LLMs as real-time inference components opens up whole new categories of intelligent systems design.  llama3.1:8b is great for structured json and all sorts of small context tolerant tasks ('fuzzy' sensing, faster than real-time video analysis - a cpm model would be awesome for this!) . I'm just a lowly dev but this excites even me.",
          "score": 7,
          "created_utc": "2026-02-19 23:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bxn8l",
          "author": "Azuriteh",
          "text": "This is actually insane holy shit, that speed is just crazy",
          "score": 6,
          "created_utc": "2026-02-19 23:34:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bxi9y",
          "author": "arindale",
          "text": "This will be so useful for edge ai. AI robots and self-driving cars could really benefit from this.",
          "score": 9,
          "created_utc": "2026-02-19 23:33:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c367w",
              "author": "coder543",
              "text": "Depends on whether the chip costs more than the car, and whether the chip requires kilowatts of power and cooling",
              "score": 9,
              "created_utc": "2026-02-20 00:06:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dmbnv",
          "author": "Qwen30bEnjoyer",
          "text": "Hear me out folks - 16,000 TPS draft model. I wish I knew more about the specifics of speculative decoding, but hey more TPS more chances at getting it right, right?",
          "score": 4,
          "created_utc": "2026-02-20 06:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lgpck",
              "author": "AI_is_the_rake",
              "text": "If you had a smart enough reasoning model it could quickly populate a database, quickly write verification scripts custom to what youâ€™re working on etc. this is a genuine breakthrough. Imagine the intelligence of Gemini 3.1 or opus 4.6 or GPT 5.2 on a chip next to ordinary hardware with tool access. It could run thousands of experiments a second. We donâ€™t know the optimal design but I would imagine someone will combine the right parts and the right rules to create an Intelligence Unit.Â \n\nWe are a year or two from AGI. This technology plus scaling it out would get us there. No new innovations needed. And we all know innovation will still occur in that time frame.Â \n",
              "score": 1,
              "created_utc": "2026-02-21 13:16:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bwreq",
          "author": "checksinthemail",
          "text": "That was insane.  15k+ tokens a second wow.",
          "score": 9,
          "created_utc": "2026-02-19 23:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c59qn",
              "author": "floppypancakes4u",
              "text": "Way faster. 15k tok/s at .021 seconds. ðŸ˜ƒ",
              "score": 4,
              "created_utc": "2026-02-20 00:18:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6c73z9",
                  "author": "Single_Ring4886",
                  "text": "where you get that number?",
                  "score": 1,
                  "created_utc": "2026-02-20 00:29:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6buoz1",
          "author": "34574rd",
          "text": "This is pretty fucking cool, is there a way I can start learning hardware design like this?",
          "score": 5,
          "created_utc": "2026-02-19 23:16:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6c9hc8",
              "author": "TenTestTickles",
              "text": "1: Look up Onur Mutlu's lectures on digital logic and computer architecture on youtube. Do this in parallel; there's several years worth of studying you could do here.\n\n2: Learn the SystemVerilog programming language. Note that this language is split in half: some features are synthesizable, which means they can be made into hardware, and some features are simulation only, which means they only run on software emulation (but are ideal for higher-level abstraction or test/verification.)\n\n3: Grab a FPGA development board. There are as many opinions on which one as there are opinions on the internet. I've had quite a few but just for playing around in this arena, there's a Pynq v2 board. It has a Xilinx 7020 chip on there, a good chunk of RAM, and an embedded ARM core. It also has a great software ecosystem that even runs Python -- so you can do things like experiment with neuron models in hardware, then use python on the ARM controller to run signals through it and examine the output in a Jupyter notebook.",
              "score": 5,
              "created_utc": "2026-02-20 00:42:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bzhju",
          "author": "susmitds",
          "text": "Holy smoke! It was instant for long detailed text summary",
          "score": 4,
          "created_utc": "2026-02-19 23:45:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6btgxx",
          "author": "qwen_next_gguf_when",
          "text": "Butterfly labs strikes again?",
          "score": 3,
          "created_utc": "2026-02-19 23:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bz4em",
          "author": "Single_Ring4886",
          "text": "I think this will find buyers mainly because insane speed.",
          "score": 3,
          "created_utc": "2026-02-19 23:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k5toy",
              "author": "-InformalBanana-",
              "text": "Inference servers and realtime applications of ai.",
              "score": 1,
              "created_utc": "2026-02-21 06:06:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mdhfb",
                  "author": "Single_Ring4886",
                  "text": "Exactly, sometime you need intelligence fast in real time and this is fast enough.",
                  "score": 2,
                  "created_utc": "2026-02-21 16:20:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6c0br7",
          "author": "Nickypp10",
          "text": "Would be sick for humanoid robots. If they can get the power down.",
          "score": 3,
          "created_utc": "2026-02-19 23:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d5rj2",
          "author": "slippery",
          "text": "I've found no use for 8B models. They are dumb and hallucinate almost all the time.",
          "score": 3,
          "created_utc": "2026-02-20 04:04:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bscba",
          "author": "-dysangel-",
          "text": "Nice - been wondering when someone would get around to this. It's following the same route that crypto mining did",
          "score": 5,
          "created_utc": "2026-02-19 23:03:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bxjk8",
              "author": "DistanceSolar1449",
              "text": "ASICs canâ€™t be updated to new models. This makes them obsolete quickly in fast moving fields",
              "score": 4,
              "created_utc": "2026-02-19 23:33:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6byuw9",
                  "author": "do-un-to",
                  "text": "At what point are people going to have use cases for which SOTA models are just good enough?",
                  "score": 6,
                  "created_utc": "2026-02-19 23:41:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6eiw3v",
                  "author": "-dysangel-",
                  "text": "Obsolete in terms of SOTA, but not useless. MostÂ SOTA language models are already \"good enough\" for meÂ as coding assistants, and have been since around Claude 3.5. If I had GLM 5 running at 15k tps I'd be very happy with that. Put it this way - you could have the latest SOTA models as an overseer model, and then farm out tool calls and file editsÂ etc to the not-as-smart, but superfast model.\n\n\n\nI can also picture being able to do a lot of interesting stuff with cheap chips running current gen models very fast. Just think how much better Alexa devices would be with even a 4b Qwen3 model, let alone Deepseek or GLM. Cheap mass produced language model chips would also be great for adding a layer of general intelligence to robots, kids toys, etc.",
                  "score": 1,
                  "created_utc": "2026-02-20 11:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6e8ga7",
          "author": "uti24",
          "text": ">Llama 3.1 8B\n\nOk, it's output gibberish. \n\nI mean, it feels like Llama 3.1 8B quantized to low quant, Q3, maybe?",
          "score": 5,
          "created_utc": "2026-02-20 09:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e8pbp",
              "author": "uti24",
              "text": "https://preview.redd.it/lfy8jv3udmkg1.png?width=739&format=png&auto=webp&s=0684cbad89558e5b0c8f01c98d0b13c9b2989bb9\n\nAnd all it's answers like that. \n\nMan, it's like I returned to Llama 1 times.",
              "score": 6,
              "created_utc": "2026-02-20 09:39:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6h2xwn",
              "author": "AnticitizenPrime",
              "text": "> We selected the Llama 3.1 8B as the basis for our first product due to its practicality. Its small size and open-source availability allowed us to harden the model with minimal logistical effort.\n> \n> While largely hard-wired for speed, the Llama retains flexibility through configurable context window size and support for fine-tuning via low-rank adapters (LoRAs).\n> \n> **At the time we began work on our first generation design, low-precision parameter formats were not standardized. Our first silicon platform therefore used a custom 3-bit base data type. The Silicon Llama is aggressively quantized, combining 3-bit and 6-bit parameters, which introduces some quality degradations relative to GPU benchmarks.**\n> \n> Our second-generation silicon adopts standard 4-bit floating-point formats, addressing these limitations while maintaining high speed and efficiency.",
              "score": 3,
              "created_utc": "2026-02-20 19:12:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hwnhy",
                  "author": "uti24",
                  "text": "oh hey, I got it right",
                  "score": 2,
                  "created_utc": "2026-02-20 21:37:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6c3w59",
          "author": "m2e_chris",
          "text": "16k tok/s on an 8B is impressive but the real question is what the economics look like at scale. the whole value prop of ASICs is amortizing the NRE cost over massive volume, and inference-specific chips only make sense if you're locked into a single architecture long enough to recoup that. with how fast model architectures are changing right now, you'd want some level of reconfigurability or you're burning silicon every 6 months. curious what their roadmap looks like for supporting non-transformer architectures.",
          "score": 2,
          "created_utc": "2026-02-20 00:10:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6cz0kc",
              "author": "SporksInjected",
              "text": "Are companies not doing that right now anyway?",
              "score": 1,
              "created_utc": "2026-02-20 03:19:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6e7q32",
              "author": "twiiik",
              "text": "Up front costs versus running costs.\nIf you save a lot on your electricity bill it might be worth replacing your Â«lightbulbÂ» from time to time",
              "score": 1,
              "created_utc": "2026-02-20 09:29:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c88g1",
          "author": "FullOf_Bad_Ideas",
          "text": "cool demo, I think they'll find revenue in some specialized models that benefit from low latency in ASR space or in some pipelines that require quick time to result, maybe financial analysis.",
          "score": 2,
          "created_utc": "2026-02-20 00:35:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6co4em",
          "author": "sammcj",
          "text": "Tried out the chat, that's incredibly fast, feels like cheating! I guess the main issue is that Llama 3.1 8B is not a very strong model (now or when it was released) - are there plans to release support for larger models? (I think at least something like Qwen 3 next at around 80b would make it really useful).",
          "score": 2,
          "created_utc": "2026-02-20 02:12:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6d26fg",
          "author": "Resident_Suit_9916",
          "text": "Will they ever sell their hardware",
          "score": 2,
          "created_utc": "2026-02-20 03:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dtm8j",
          "author": "SourceCodeplz",
          "text": "Wow? How? It is just instant!",
          "score": 2,
          "created_utc": "2026-02-20 07:17:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f6wt6",
          "author": "TailorImaginary3629",
          "text": "I remember etched.ai claimed something similar. By the way, what happened to them?",
          "score": 2,
          "created_utc": "2026-02-20 13:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fdlfv",
          "author": "XB0XRecordThat",
          "text": "Make it run something as good as Claude code and I'll buy it today",
          "score": 2,
          "created_utc": "2026-02-20 14:26:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6krp0a",
          "author": "anshulsingh8326",
          "text": "Tried it. I Pressed send button and before i could lift my finger it completed its output ðŸ¤¯",
          "score": 2,
          "created_utc": "2026-02-21 09:35:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e1i6v",
          "author": "xrvz",
          "text": "It may be really fast, but the answers are also uncommonly bad.\n\nThe product is a whole 2.5 kW server, so a Mac mini farm is possibly still more efficient in tokens per seconds per Watt.\n\nI'm also not interested in yet another API service. This is ~~Sparta~~ r/localllama. Wake me when they're able to put hardware onto my desk.",
          "score": 2,
          "created_utc": "2026-02-20 08:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e2oas",
              "author": "djm07231",
              "text": "\\> Taalasâ€™ HC1 is built on TSMC N6 with a die size of 815 mm^(2), and one chip fits the entire 8B model. The chip uses around 250W, so 10 HC1 cards in a server need about 2.5 kW, meaning they can be deployed in standard air-cooled racks.\n\nI think it is only 250W per chip which is pretty manageable. So it is probably a lot more efficient compared to a Mac mini.\n\n[https://www.eetimes.com/taalas-specializes-to-extremes-for-extraordinary-token-speed/](https://www.eetimes.com/taalas-specializes-to-extremes-for-extraordinary-token-speed/)",
              "score": 2,
              "created_utc": "2026-02-20 08:41:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dugwp",
          "author": "rm-rf-rm",
          "text": "They've already lost me.. \"Hardcore models\" \"Human languages are its software\", llama3.1, AI slop images... 1000x is vaporware. ASICs can have OOM improvements over GPUs, but more like 10-100x. And you trade off generality for specificity. With the reality that a) models are changing on a monthly basis b) it takes years for AI hardware to break even, ASICs for LLM inference is not something meaningful today. But it will be at some point, but that doesnt imply this company will make the right products or is even legitimate to begin with.\n\nAI agrees: \"The website is heavy on branding, light on technical specifics. No benchmarks, no architecture details, no disclosed team credentials on the landing page. That's not damning for an early-stage startup, but it means there's nothing concrete to evaluate yet.\"\n\nDont take my word for it, give your favoriate LLM the website and ask it to analyze.",
          "score": 2,
          "created_utc": "2026-02-20 07:25:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c5uis",
          "author": "OkDesk4532",
          "text": "This is sick. Wow.",
          "score": 1,
          "created_utc": "2026-02-20 00:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cc03c",
          "author": "sunshinecheung",
          "text": "pls use llama3.3 8b",
          "score": 1,
          "created_utc": "2026-02-20 00:57:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6csfev",
          "author": "_millsy",
          "text": "I wonder how they handle context and what lengths are possible, I didnâ€™t see it described? Got me wondering if you can make a reprogrammable version of this in a similar type of premise to how FPGA can be leveraged in use cases like MISTER",
          "score": 1,
          "created_utc": "2026-02-20 02:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cyt45",
          "author": "[deleted]",
          "text": "How fast can it run wan video models?",
          "score": 1,
          "created_utc": "2026-02-20 03:18:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d3hx6",
              "author": "frozen_tuna",
              "text": "That's what I was thinking. From what I've read here, it seems very difficult/expensive to scale to higher params. I'm guessing something like this would be less useful for consumers and more useful for cloud providers, despite everyone's wishes.\n\nThat said, an ASIC built on z-image or wan instead of an llm would be sweeeeet.",
              "score": 1,
              "created_utc": "2026-02-20 03:49:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6d9jeg",
              "author": "rtyuuytr",
              "text": "Exact same thought, these smaller 8-40B text to text models are largely useless. Running a 30-40B video model would be super cool.",
              "score": 1,
              "created_utc": "2026-02-20 04:31:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6do6x8",
          "author": "neuroticnetworks1250",
          "text": "The Professor of the Chair where I did my Masters in was also focusing on something like this where they fused weights into the circuit itself, primarily for efficient Edge AI, but also because she believed that doing so would help study the internals of how AI make decisions. I always thought it was too rigid and inflexible to be a product. But damn, she was cooking.",
          "score": 1,
          "created_utc": "2026-02-20 06:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6drbqj",
          "author": "--dany--",
          "text": "Sounds very cool. Whatâ€™s limiting them offering a more modern models, any qwen 7b models for example? Or is the chip not flexible enough?",
          "score": 1,
          "created_utc": "2026-02-20 06:56:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dsdd4",
          "author": "benfavre",
          "text": "Would it make sense to have a chip like that spit out reprÃ©sentations from inputs with a generic models, on which would be stacked a small set of GPU-run layers which you could train to your liking.\n\nThere you would benefit from both ludicrous speed and customizability.",
          "score": 1,
          "created_utc": "2026-02-20 07:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dtwv2",
          "author": "zadiraines",
          "text": "This company is building the â€œpositronic brainâ€ from Asimovâ€™s books. Whenâ€™s the IPO?",
          "score": 1,
          "created_utc": "2026-02-20 07:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dwp86",
          "author": "arxdit",
          "text": "I was wondering when this was going to happen\n\nGPUs are not very efficient running LLMs per se, they were just more efficient than the alternatives",
          "score": 1,
          "created_utc": "2026-02-20 07:45:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e30bq",
          "author": "raiffuvar",
          "text": "Is it fixed Llama version in the silicon? Or can it be finetuned/lora?",
          "score": 1,
          "created_utc": "2026-02-20 08:44:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e78oa",
          "author": "marsxyz",
          "text": "Something that fast for reranking / rag purposes would be crazy",
          "score": 1,
          "created_utc": "2026-02-20 09:25:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e8rwd",
          "author": "Southern_Sun_2106",
          "text": "Is this what they call Singularity?",
          "score": 1,
          "created_utc": "2026-02-20 09:39:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eajeo",
          "author": "physalisx",
          "text": "I'm pretty convinced that this is the mid term future, after LLM R&D plateaus in the next few years. \n\nAs you said, this is a proof of concept, but there is really nothing preventing this from being done with much bigger models, right? It wouldn't even increase complexity much. Am I thinking about this right that it just scales linearly with parameter size? And not even inference speed, just chip size. Speed could be basically the same, you'd just need more energy/electricity.",
          "score": 1,
          "created_utc": "2026-02-20 09:56:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6edpga",
          "author": "AppealSame4367",
          "text": "This would be excellent for drones",
          "score": 1,
          "created_utc": "2026-02-20 10:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ehb4j",
          "author": "Shipworms",
          "text": "That is ridiculous. Tried the chatjimmy.ai and asked it some quantum physics stuff.\n\nA massive, structured response appeared instantaneously when I clicked send (the entire page was done in 0.041 seconds!)",
          "score": 1,
          "created_utc": "2026-02-20 10:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eibe4",
          "author": "exaknight21",
          "text": "Proprietary deployment of AI? Like iPhones having exclusive iOS? Is that what I am getting out of this? \n\nIs the end user not able to load their own model onto this chip? Iâ€™m slightly lost.",
          "score": 1,
          "created_utc": "2026-02-20 11:05:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6elb5k",
          "author": "EffectiveCeilingFan",
          "text": "Wowza this is actually insane. Literally a wall of text as soon as I send a message, can't even see the tokens stream in. I'd pay top dollar for a dev board of this or something.",
          "score": 1,
          "created_utc": "2026-02-20 11:30:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f4npw",
          "author": "sinsforeal",
          "text": "Imagine putting a video model on an ASIC. You have movies in seconds. ",
          "score": 1,
          "created_utc": "2026-02-20 13:38:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f7ci5",
          "author": "bralynn2222",
          "text": "In the middle of making a model to run this fast will definitely reach out about a partnership",
          "score": 1,
          "created_utc": "2026-02-20 13:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fgpl3",
          "author": "SettingAgile9080",
          "text": "WOW. The law of accelerating returns continues. As per Thiel's 10X thinking, something like this that is an immediate 10X change over the prior generation opens up a whole new category. As a chatbot this thing is a gimmick, but as an agentic loop exploring and self-improving it is going to unlock some wild capabilities.\n\nThat its a cheap(ish) module burned into silicon also bodes well for local LLM hobbyists if this becomes a common way of doing things as data centers will dump last-gen ones on the used market as they upgrade.\n\nThis is such an exciting time to be in tech, I love it.",
          "score": 1,
          "created_utc": "2026-02-20 14:42:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fljvu",
          "author": "Former_Step_9837",
          "text": "How much will this cost?",
          "score": 1,
          "created_utc": "2026-02-20 15:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fuxpr",
          "author": "IfNightThen",
          "text": "This is fast, but the intelligence is pre-GPT2 quality. Any follow-up question seems to start hallucinating based off the previous answer. ",
          "score": 1,
          "created_utc": "2026-02-20 15:51:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fzmtp",
          "author": "pasdedeux11",
          "text": "could someone post what the article says? website has shitflare and not loading it",
          "score": 1,
          "created_utc": "2026-02-20 16:12:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gg160",
          "author": "reckless_avacado",
          "text": "would love to see this on some more intensive tasks than text generation",
          "score": 1,
          "created_utc": "2026-02-20 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gjw08",
          "author": "numericalclerk",
          "text": "The quality of the output is horrendous, but the speed is pretty neat indeed.",
          "score": 1,
          "created_utc": "2026-02-20 17:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gp17p",
          "author": "Educational-Agent-32",
          "text": "Any video ? Showcase ?",
          "score": 1,
          "created_utc": "2026-02-20 18:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h29fe",
              "author": "AnticitizenPrime",
              "text": "You can try it yourself at https://chatjimmy.ai/",
              "score": 1,
              "created_utc": "2026-02-20 19:08:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6h2fdn",
          "author": "ArchdukeofHyperbole",
          "text": "Hopefully running qwen3 next 80B would be possible on that hardware. Only 3B active parameters. Yep, I guess it would run faster than 16,000 tokens per second. Still not fast enough though jk\n\n\n\n\nEdit: what quant are they using? Seems a little dumber than I remember llama 8B being. Still really nice speeds, just about 16k tokens/sec, so fast I had didn't realize the response was there a few times lol",
          "score": 1,
          "created_utc": "2026-02-20 19:09:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hbaoa",
          "author": "hwpoison",
          "text": "It's a game change with something like this, an entire model wired in a chip,  doesn't need memory?",
          "score": 1,
          "created_utc": "2026-02-20 19:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hde7g",
              "author": "hwpoison",
              "text": "Seems that they have the chatbot api endpoint exposed: [https://www.pythonmorsels.com/p/2x85m/](https://www.pythonmorsels.com/p/2x85m/)",
              "score": 1,
              "created_utc": "2026-02-20 20:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ho1hv",
          "author": "rorykoehler",
          "text": "This is the future for sure. Amazing engineering",
          "score": 1,
          "created_utc": "2026-02-20 20:54:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hrnqz",
          "author": "DesoLina",
          "text": "Can you fine tune model or is it baked once??",
          "score": 1,
          "created_utc": "2026-02-20 21:12:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i09i4",
          "author": "numbworks",
          "text": "Regarding the chipset itself with LLama 3.1 8B baked in, under 100$ and powered via USB 3.0 I would buy it now.",
          "score": 1,
          "created_utc": "2026-02-20 21:55:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ibp7d",
          "author": "jealousghostt",
          "text": "Maybe I'm out of subject, but will it influence the RAM price crisis? I tested the speed of the Ai Chatbot, it's crazy...",
          "score": 1,
          "created_utc": "2026-02-20 22:55:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jysu3",
          "author": "Drubuntu",
          "text": "This thing is lightning fast!",
          "score": 1,
          "created_utc": "2026-02-21 05:09:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k6fqg",
          "author": "-InformalBanana-",
          "text": "Chain of thought models would be so much more worth it with this chip...",
          "score": 1,
          "created_utc": "2026-02-21 06:11:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kyegm",
          "author": "HasGreatVocabulary",
          "text": ">Write a deliberately bad story\n\n>Jimmy\n\n>Here's a deliberately bad story:\n\n>Once upon a time, in the land of Plootz, there was a mighty warrior named Bob. Bob was known far and wide for his incredible ability to burp the alphabet. It was a skill he had honed to perfection, and he could burp the entire alphabet in under 5 minutes.\n\n>But Bob was not just a one-trick pony, oh no. He was also a master of the ancient art of Extreme Ironing. He would take his ironing board to the most extreme locations on Earth, like the top of a moving train or the bottom of the ocean, and iron his clothes with ease.\n\n>One day, Bob received a letter from the King of Plootz, asking him to come to the castle and show off his burping skills in front of the entire kingdom. Bob was thrilled. He packed his ironing board, his alphabet-prefecting skills, and his trusty can of soda, and set off for the castle.\n\n>When he arrived, he was greeted by the King himself, who was so impressed by Bob's burping that he awarded him the Golden Sock of Plootz. Bob was overjoyed and spent the rest of the day burping and ironing for the adoration of the crowd.\n\n>But little did Bob know, his arch-nemesis, the evil Count of Blorple, had been secretly training a team of ninja warriors to steal the Golden Sock and take over the kingdom. And so, Bob's big day turned into a big mess, as the Count's ninjas snuck into the castle and stole the Golden Sock, leaving Bob to ponder the meaning of it all, and wonder why on Earth did he ever get into Extreme Ironing...\n\n>  \nGenerated in 0.023sÂ â€¢Â 15,759 tok/s\n\nI am not used to seeing that much text showing up this quickly",
          "score": 1,
          "created_utc": "2026-02-21 10:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lnyix",
          "author": "BlasRainPabLuc",
          "text": "Please!! Someone should share this to Demis Hassabis as soon as possible!! His mind would blow up with the posibilities!!",
          "score": 1,
          "created_utc": "2026-02-21 14:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bvkhg",
          "author": "Emotional-Baker-490",
          "text": "Why not qwen3? llama3 is a weird choice in 2026.",
          "score": 2,
          "created_utc": "2026-02-19 23:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6by5k8",
              "author": "SmartCustard9944",
              "text": "Quoting the article:\n\n> We selected the Llama 3.1 8B as the basis for our first product due to its practicality. Its small size and open-source availability allowed us to harden the model with minimal logistical effort.\n\nAlso, R&D takes time",
              "score": 14,
              "created_utc": "2026-02-19 23:37:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g6spr",
                  "author": "Emotional-Baker-490",
                  "text": "I mean like, qwen3 isnt very different architecturally, its been out for a year, and if the goal was to make something useless to proof of concept, why not qwen3 4b? Half the memory would be needed, it didnt get dropped on its head as a child, it would get to advertise even bigger numbers, and its been out since the start of 2025.",
                  "score": 2,
                  "created_utc": "2026-02-20 16:45:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6bzwl3",
              "author": "pulse77",
              "text": "It took two months just to add support for Qwen3-Next to the existing llama.cpp codebase - where everything else was already built and tested. And this company designed and built an entire LLM chip from scratch!",
              "score": 10,
              "created_utc": "2026-02-19 23:47:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6c3hwm",
              "author": "netroxreads",
              "text": "That's because they hardwired the LLM in silicon which always takes a long time. It usually takes at least a year for the chip to be completed. ",
              "score": 3,
              "created_utc": "2026-02-20 00:08:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6dw2rm",
              "author": "temperature_5",
              "text": "Yeah, especially consider the utility of the 4B or 8B Qwen3-VL!  Or even Gemma 3 4B QAT w/ vision.",
              "score": 1,
              "created_utc": "2026-02-20 07:39:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6h3aj0",
              "author": "AnticitizenPrime",
              "text": "> We selected the Llama 3.1 8B as the basis for our first product due to its practicality. Its small size and open-source availability allowed us to harden the model with minimal logistical effort.\n> \n> While largely hard-wired for speed, the Llama retains flexibility through configurable context window size and support for fine-tuning via low-rank adapters (LoRAs).\n> \n> At the time we began work on our first generation design, low-precision parameter formats were not standardized. Our first silicon platform therefore used a custom 3-bit base data type. The Silicon Llama is aggressively quantized, combining 3-bit and 6-bit parameters, which introduces some quality degradations relative to GPU benchmarks.\n\n> Our second-generation silicon adopts standard 4-bit floating-point formats, addressing these limitations while maintaining high speed and efficiency.\n\nIt's the prototype, required tweaking the model, and time to figure out the process, I guess.",
              "score": 1,
              "created_utc": "2026-02-20 19:13:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c98xr",
          "author": "ithkuil",
          "text": "That's amazing and am so glad to see this work. And hopeful for more products.\n\n\nHowever, the most common need for high speed inference is low latency. An 8b model is already almost instantaneous for short replies on even (new) consumer hardware.\n\n\nAnd an 8b model is not really smart enough for most tasks that require longer replies.Â \n\n\nI hope they can build the same thing for a 24B model like Mistral has.",
          "score": 1,
          "created_utc": "2026-02-20 00:41:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cj4ts",
          "author": "sampdoria_supporter",
          "text": "Wow - that chart on the website - I had no idea groq had been left in the dust like that. Their custom hardware can't be sustainable at this point",
          "score": 1,
          "created_utc": "2026-02-20 01:42:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6bx0em",
          "author": "Fuzzy_Spend_5935",
          "text": "I tried the demo and it's just fast, nothing else.",
          "score": -4,
          "created_utc": "2026-02-19 23:30:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6czz4r",
              "author": "SporksInjected",
              "text": "Thatâ€™s the point I think",
              "score": 3,
              "created_utc": "2026-02-20 03:26:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6cat34",
          "author": "Mediocre-Returns",
          "text": "Its useless and fast basically jabberwacky from 28 years ago.",
          "score": -5,
          "created_utc": "2026-02-20 00:50:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c0poc",
          "author": "qwen_next_gguf_when",
          "text": "Its chat demo is basically useless but fast.",
          "score": -6,
          "created_utc": "2026-02-19 23:52:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r60ety",
      "title": "Qwen 3.5 will be released today",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/",
      "author": "External_Mood4719",
      "created_utc": "2026-02-16 04:54:20",
      "score": 419,
      "num_comments": 95,
      "upvote_ratio": 0.96,
      "text": "Sources reveal that Alibaba will open-source its next-generation large model, Qwen3.5, tonight on Lunar New Year's Eve. The model reportedly features a comprehensive innovation in its architecture.\n\nhttps://preview.redd.it/n8tuw9gmfsjg1.jpg?width=680&format=pjpg&auto=webp&s=b95152330c1b5ebdb5b7022dd6762ebe1890fd06\n\n[https://x.com/Sino\\_Market/status/2023218866370068561?s=20](https://x.com/Sino_Market/status/2023218866370068561?s=20)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r60ety/qwen_35_will_be_released_today/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o5ocmyx",
          "author": "rm-rf-rm",
          "text": "Use the release post to continue discussion: https://old.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/",
          "score": 1,
          "created_utc": "2026-02-16 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mumrg",
          "author": "98Saman",
          "text": "I love their qwen 3 8B and still use it to this day. I hope they give us a good updated model in that range so I can start using it :)",
          "score": 47,
          "created_utc": "2026-02-16 05:14:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nb6yr",
              "author": "Very_Large_Cone",
              "text": "Qwen 3 4b is still my go to, it is way better than it has any right to be for its size. Hoping for an update to that!",
              "score": 14,
              "created_utc": "2026-02-16 07:36:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nwtqk",
              "author": "xenongee",
              "text": "Have you compared the Qwen3 8B with the Ministral 8B 2410? I wonder which of these models is better",
              "score": 6,
              "created_utc": "2026-02-16 10:59:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5ngij8",
              "author": "combrade",
              "text": "Qwen 3 VL-8b for me . I actually have two to three finetunes of Qwen 3-8b for my daily driver .",
              "score": 1,
              "created_utc": "2026-02-16 08:26:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5mziwz",
          "author": "Sicarius_The_First",
          "text": "In case you guys are wondering, the PR was opened some time ago:\n\n[https://github.com/huggingface/transformers/pull/43830/](https://github.com/huggingface/transformers/pull/43830/)",
          "score": 18,
          "created_utc": "2026-02-16 05:54:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mvore",
          "author": "andy2na",
          "text": "Is VL built-in? Surprised no 4B, which qwen3-vl:4b has been perfect for frigate and home assistant ",
          "score": 14,
          "created_utc": "2026-02-16 05:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n14dn",
          "author": "Turkino",
          "text": "I'll go ahead and be the first to ask GGUF when?\n /s",
          "score": 13,
          "created_utc": "2026-02-16 06:07:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nbcb3",
              "author": "PANIC_EXCEPTION",
              "text": "https://preview.redd.it/hy4x03fp8tjg1.png?width=880&format=png&auto=webp&s=5c0d6bb9a8c8b04301c85492cf463cb692170388\n\n",
              "score": 16,
              "created_utc": "2026-02-16 07:38:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nmsg6",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-02-16 09:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nq0a6",
                  "author": "nmkd",
                  "text": "That's transformers though, not lcpp",
                  "score": 3,
                  "created_utc": "2026-02-16 09:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5n048f",
          "author": "the__storm",
          "text": "That 35B is getting very difficult to squeeze into 24 GB lol",
          "score": 39,
          "created_utc": "2026-02-16 05:58:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5na4j6",
              "author": "mindwip",
              "text": "Got to up those numbers!",
              "score": 5,
              "created_utc": "2026-02-16 07:26:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5nfluk",
              "author": "mrdevlar",
              "text": "But isn't it a 35B-A3B so not a dense model so won't need that much memory in practice?",
              "score": 7,
              "created_utc": "2026-02-16 08:17:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ng97t",
                  "author": "Significant_Fig_7581",
                  "text": "Yeah but MOEs lose a lot of quality when they're quantized, If you have used a quantized 8B version you would likely not notice a big difference but try it with a MOE it'd most likely drop significantly",
                  "score": -1,
                  "created_utc": "2026-02-16 08:24:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nkopu",
              "author": "dampflokfreund",
              "text": "I was rather hoping they would increase active parameters, seems like a no brainer for much increased quality. ",
              "score": 1,
              "created_utc": "2026-02-16 09:06:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5npj5x",
              "author": "ziggo0",
              "text": "Smash that sysram button then: sad it's going slow now.",
              "score": 1,
              "created_utc": "2026-02-16 09:52:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5n61ps",
              "author": "Odd-Ordinary-5922",
              "text": "just quantize it",
              "score": 2,
              "created_utc": "2026-02-16 06:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nb91o",
                  "author": "ShengrenR",
                  "text": "but that's the issue, the 30-32B models are juuust at the cusp of solid q4 options on a 24gb card.. go lower and you fall off a bit of a performance cliff. 32B at q4 likely well better than 35b at some weird q3 something",
                  "score": 15,
                  "created_utc": "2026-02-16 07:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mxdgp",
          "author": "giant3",
          "text": "Does new architecture mean llama.cpp requires a fix to use with it?",
          "score": 7,
          "created_utc": "2026-02-16 05:36:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mzis3",
              "author": "LinkSea8324",
              "text": "Yes but no because it's already merged",
              "score": 27,
              "created_utc": "2026-02-16 05:53:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n902k",
                  "author": "xor_2",
                  "text": "Makes sense to patch llama before the actual release.",
                  "score": 3,
                  "created_utc": "2026-02-16 07:16:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mw6v4",
          "author": "Specter_Origin",
          "text": "I do hope they also release successor to 235B one too",
          "score": 24,
          "created_utc": "2026-02-16 05:26:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nlf7z",
              "author": "Own-Potential-2308",
              "text": "It's out",
              "score": 3,
              "created_utc": "2026-02-16 09:13:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5nqnx7",
                  "author": "hleszek",
                  "text": "where?",
                  "score": 2,
                  "created_utc": "2026-02-16 10:03:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mvslu",
          "author": "mlon_eusk-_-",
          "text": "Hopefully bigger models are coming as well, they have a bit of a catch up to do with other Chinese labs.",
          "score": 6,
          "created_utc": "2026-02-16 05:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mzng8",
          "author": "Amazing_Athlete_2265",
          "text": "Already warmed up the 3080. Let's go!!",
          "score": 7,
          "created_utc": "2026-02-16 05:55:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n6niq",
          "author": "FaceDeer",
          "text": "Ooh, 30B-A3B has been my \"workhorse\" local LLM for so long now. Looking forward to trying this out! I may have to go down a quant with the new one being 35B, but I suspect that'll likely be worth it.",
          "score": 7,
          "created_utc": "2026-02-16 06:55:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mtwhy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 50,
          "created_utc": "2026-02-16 05:09:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mvs4k",
              "author": "Klutzy-Snow8016",
              "text": "Note that different models may require different prompting to get the most out of them, and may have different recommended temperature, so this sanity check, while fast, doesn't necessarily tell you much.\n\nEdit: I think I just got fooled by a bot comment.",
              "score": 20,
              "created_utc": "2026-02-16 05:23:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mzj6r",
                  "author": "IrisColt",
                  "text": "Are you a non-inconspicuous bot, heh",
                  "score": 7,
                  "created_utc": "2026-02-16 05:54:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5mvm26",
              "author": "Embarrassed_Sun_7807",
              "text": "Give me a prompt set and I'll run it. Have a100s at disposalÂ ",
              "score": 4,
              "created_utc": "2026-02-16 05:22:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n8dzd",
          "author": "2legsRises",
          "text": "china might actually be #1 it seems",
          "score": 6,
          "created_utc": "2026-02-16 07:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5my8i5",
          "author": "Sicarius_The_First",
          "text": "9B DENSE?! O\\_O\n\nLegit excited!",
          "score": 9,
          "created_utc": "2026-02-16 05:43:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5nhphl",
              "author": "Weary_Long3409",
              "text": "14 replacement?",
              "score": 2,
              "created_utc": "2026-02-16 08:38:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5o6azt",
                  "author": "Sicarius_The_First",
                  "text": "Hopefully! 9B dense is a VERY good size for local.\n\nA modernization of llama3 8b is very much welcomed :)",
                  "score": 2,
                  "created_utc": "2026-02-16 12:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5n3hce",
          "author": "Sabin_Stargem",
          "text": "Hopefully, someone will immediately quant the 80b to MXFP4 with Heretic NoSlop+NoRefusal.",
          "score": 4,
          "created_utc": "2026-02-16 06:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5njavl",
          "author": "tx2000tx",
          "text": "Just dropped on Openrouter: [https://openrouter.ai/qwen/qwen3.5-397b-a17b](https://openrouter.ai/qwen/qwen3.5-397b-a17b)  [https://openrouter.ai/qwen/qwen3.5-plus-02-15](https://openrouter.ai/qwen/qwen3.5-plus-02-15). Hugging face has it 404 right now [https://huggingface.co/Qwen/Qwen3.5-397B-A17B](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)\n\nhttps://preview.redd.it/enac6od2mtjg1.png?width=2404&format=png&auto=webp&s=fd21e85e0bd63d5482f40e522d1d3856098f7927",
          "score": 5,
          "created_utc": "2026-02-16 08:53:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n4ygz",
          "author": "Mysterious_Try3713",
          "text": "Need 4b",
          "score": 3,
          "created_utc": "2026-02-16 06:40:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5naha4",
          "author": "Whole_Entrance2162",
          "text": "qwen3.5-397b-a17b",
          "score": 3,
          "created_utc": "2026-02-16 07:30:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nhmpe",
          "author": "AbheekG",
          "text": "Very excited for the 2B, I still rely on Gemma2-2B for a bunch of tasks and dealing with its 8k context size has long become tiresome. Not to mention its gated HF repo causes issues with automated deployments. Despite efforts, I havenâ€™t been able to replace it: Qwen3-1.7B thinks too damn much and adding </think> to prevent that isnâ€™t always feasible with internal tasks, and I could never get Gemma3 to work reliably either. Besides, Iâ€™m not sure Gemma3-1B would be sufficient to reliably replace Gemma2-2B. That leaves us with the new Ministrals but honestly I wasnâ€™t inspired to test them as the smallest would still be a whole 1B larger than the olâ€™ reliable Gemma2-2B. Same for Granite4-Micro, and while Granite3.2-2B exists, it includes some vision parameters and Granite models can be too dry toned for rich summary generation, though Iâ€™ve heard theyâ€™re great at classification. So anyway, hereâ€™s really, REALLY looking forward to Qwen3.5-2B-Instruct! Thanks so much Qwen team!!",
          "score": 3,
          "created_utc": "2026-02-16 08:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n2ral",
          "author": "No-Weird-7389",
          "text": "Hope Qwen-3.5 35b will overpreform the 80b coder next",
          "score": 7,
          "created_utc": "2026-02-16 06:21:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ndz0d",
              "author": "s101c",
              "text": "But how? It holds less knowledge and is probably trained on more general knowledge rather than targeted towards STEM and programming tasks.",
              "score": 5,
              "created_utc": "2026-02-16 08:02:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5msgow",
          "author": "Only_Situation_4713",
          "text": "Kind of disappointing theyâ€™re not going bigger than 80B. Was hoping for another 235B sized model",
          "score": 19,
          "created_utc": "2026-02-16 04:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5munpv",
              "author": "Samy_Horny",
              "text": "They might release larger models later, it's happened before, the thing is that it usually happens the other way around, large models first, small ones later",
              "score": 29,
              "created_utc": "2026-02-16 05:14:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5mwfxn",
              "author": "Specter_Origin",
              "text": "Same, hope there will be 235b successor too, that model is such a hidden gem",
              "score": 9,
              "created_utc": "2026-02-16 05:28:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n1zue",
                  "author": "DifficultyFit1895",
                  "text": "Itâ€™s still arguably the best balance of speed and performance on a mac studio.",
                  "score": 3,
                  "created_utc": "2026-02-16 06:14:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5msvem",
              "author": "Cool-Chemical-5629",
              "text": "Oh so you don't want to see 235B quality packed in 35B? Okay then.\n\nOkay this was sarcasm, but you should really be open minded when it comes to these things. 30B models these days aren't the same quality as 30B models of the past.",
              "score": 32,
              "created_utc": "2026-02-16 05:01:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5mtcvr",
                  "author": "Gold_Sugar_4098",
                  "text": "So, the new 30B are worse compared to 30B from the past?",
                  "score": -23,
                  "created_utc": "2026-02-16 05:04:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5n4t8q",
              "author": "Individual_Spread132",
              "text": "...and if they released a new 235B model first, we'd probably see people writing *\"Kind of disappointing theyâ€™re not going smaller than 235B. Was hoping for another 80B sized model.\"*",
              "score": 5,
              "created_utc": "2026-02-16 06:39:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5mt0x1",
              "author": "External_Mood4719",
              "text": "I'm not sure; these were all found in the vllm and huggingface repos. I'm not sure if they'll release an even bigger model at this time.",
              "score": 2,
              "created_utc": "2026-02-16 05:02:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5mvnub",
              "author": "Rascazzione",
              "text": "On other occasions, they have launched different models on different dates. If they start deploying the smaller ones, they will surely launch the larger ones (which require more training time) in the coming weeks.",
              "score": 2,
              "created_utc": "2026-02-16 05:22:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5n38y8",
              "author": "No_Conversation9561",
              "text": "Thereâ€™s gonna be a 400B one as per rumours",
              "score": 2,
              "created_utc": "2026-02-16 06:25:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ns21o",
                  "author": "bnightstars",
                  "text": "[https://huggingface.co/Qwen/Qwen3.5-397B-A17B](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) 397B ",
                  "score": 3,
                  "created_utc": "2026-02-16 10:16:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5mwbqy",
          "author": "Significant_Fig_7581",
          "text": "Thank you! was dying to know when",
          "score": 2,
          "created_utc": "2026-02-16 05:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n07qw",
          "author": "[deleted]",
          "text": "2B will be good for home assistants running on 4GB cards (giving old hardware new life). I wonder how it stacks against Qwen3-4B. ",
          "score": 2,
          "created_utc": "2026-02-16 05:59:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nr998",
          "author": "RickyRickC137",
          "text": "Here's Unsloth's GGUF for 397B-A17B  \n[https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF)",
          "score": 2,
          "created_utc": "2026-02-16 10:08:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5munrr",
          "author": "SpicyWangz",
          "text": "Hyped",
          "score": 2,
          "created_utc": "2026-02-16 05:14:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n1yro",
          "author": "pmttyji",
          "text": "Hope they release 150-250B Coder model (To replace Qwen3-Coder-480B which's not suitable for small/medium size VRAMs)",
          "score": 2,
          "created_utc": "2026-02-16 06:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n8163",
              "author": "qc0k",
              "text": "qwen3-coder-next:80b? It was just released and fits nicely between previous gen qwen3-coder:30B and larger models.",
              "score": 6,
              "created_utc": "2026-02-16 07:07:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ngfys",
                  "author": "pmttyji",
                  "text": "Agree with 80B. But that's part of Qwen3 Version.\n\nHere I'm talking about Qwen3.5. Maybe Qwen3.5-235B-Coder would be great.",
                  "score": 1,
                  "created_utc": "2026-02-16 08:25:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5o543z",
                  "author": "tarruda",
                  "text": "It is text only though. Hopefully they release something in the 80-160b range that has native vision.",
                  "score": 1,
                  "created_utc": "2026-02-16 12:09:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5n2rtl",
          "author": "mtmttuan",
          "text": "Specially it will probably be released in the next 10 hours before the new year eve. Don't think they will release it after the eve.",
          "score": 1,
          "created_utc": "2026-02-16 06:21:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n3fqf",
          "author": "Apart_Boat9666",
          "text": "I might shift over to qwen3.5 9b if it is better tban mistral 3 14b",
          "score": 1,
          "created_utc": "2026-02-16 06:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n6dva",
              "author": "Odd-Ordinary-5922",
              "text": "there are so many better models than mistral 3 bro",
              "score": 2,
              "created_utc": "2026-02-16 06:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n6j7g",
                  "author": "Apart_Boat9666",
                  "text": "In 12gb vram i cant fit any other models wirh q8 30k context. Le5 me know if you have bettee alternative",
                  "score": 1,
                  "created_utc": "2026-02-16 06:54:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5nn1i7",
                  "author": "kind_cavendish",
                  "text": "Name a few. (Please note that while my comment sounds condescending, that is NOT, my intention. I'm simply curious in models better than Mistral 3 14b for roleplaying.)",
                  "score": 1,
                  "created_utc": "2026-02-16 09:28:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5n5zkh",
          "author": "Rootax",
          "text": "It's different from qwen next ?",
          "score": 1,
          "created_utc": "2026-02-16 06:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n6eo5",
          "author": "Daniel_H212",
          "text": "Seems like just instruct right now? Looking forward to thinking and hopefully they release a model that can beat GLM 4.7 Flash at the same size.",
          "score": 1,
          "created_utc": "2026-02-16 06:53:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n8hze",
          "author": "silenceimpaired",
          "text": "Doubt we will get anything around 100-250B. Hopefully the lower end does well. The upper end is probably all closed source.",
          "score": 1,
          "created_utc": "2026-02-16 07:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ngl5l",
          "author": "AbheekG",
          "text": "This is excellent!",
          "score": 1,
          "created_utc": "2026-02-16 08:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nlfnn",
          "author": "Firepal64",
          "text": "Qwen3-Coder-Next just released two weeks ago, huh.",
          "score": 1,
          "created_utc": "2026-02-16 09:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nul6t",
          "author": "Weird_Researcher_472",
          "text": "They only released the big model and not even the weights -.-\n\nI want the 9B version",
          "score": 1,
          "created_utc": "2026-02-16 10:39:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nwx5o",
          "author": "scottgal2",
          "text": "LOVE qwen3 so looking forward to this. The 0.6b qwen3 is CRAZY capable for such a small model. Lack knowledge obviously but for structured 'fuzzy stuff' and json gen it's CRAZY capable and fast.  Many times better than tinyllama and smaller / ALMOST as fast. ",
          "score": 1,
          "created_utc": "2026-02-16 11:00:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o7wp5",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-16 12:30:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mw4db",
          "author": "Pristine_Pick823",
          "text": "Will it be available on ollama library?",
          "score": -15,
          "created_utc": "2026-02-16 05:26:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5mwes3",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -10,
          "created_utc": "2026-02-16 05:28:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mzlcr",
              "author": "LinkSea8324",
              "text": "> whether quality degrades near max ctx\n\nThat's a yes",
              "score": 3,
              "created_utc": "2026-02-16 05:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n68lf",
                  "author": "Odd-Ordinary-5922",
                  "text": "you are talking to a bot btw",
                  "score": 2,
                  "created_utc": "2026-02-16 06:51:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6g14s",
      "title": "4 of the top 5 most used models on OpenRouter this week are Open Source!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/54xxp91s6wjg1.png",
      "author": "abdouhlili",
      "created_utc": "2026-02-16 17:32:44",
      "score": 402,
      "num_comments": 76,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r6g14s/4_of_the_top_5_most_used_models_on_openrouter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5pv8ev",
          "author": "mtmttuan",
          "text": "If it's not open source people will just use the model directly from the source provider. I'm suprised seeing gemini on this list.",
          "score": 106,
          "created_utc": "2026-02-16 17:37:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pxtva",
              "author": "mikael110",
              "text": "Yes, this is the thing people seem to overlook when it comes to these \"X is leading on OpenRouter\" posts.\n\nThe main draw of OpenRouter is easy access to the myriad of open models without having to sign up for a myriad of specific providers.\n\nHowever for the large three, just using the direct API is usually faster and cheaper as there is no surcharge when purchasing credits.",
              "score": 47,
              "created_utc": "2026-02-16 17:50:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t06it",
                  "author": "TurnUpThe4D3D3D3",
                  "text": "Thereâ€™s also a huge amount of users on subscription plans for GPT and Claude, they are not represented in these stats either.\n\nItâ€™s still interesting to watch the OpenRouter stats though, I feel like itâ€™s a useful way to track model adoption.",
                  "score": 7,
                  "created_utc": "2026-02-17 03:33:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5uniy2",
                  "author": "ChocomelP",
                  "text": ">The main draw of OpenRouter is easy access to the myriad of open models without having to sign up for a myriad of specific providers.\n\nHow is this still a consideration when you can just let the LLMs do all of it?",
                  "score": 1,
                  "created_utc": "2026-02-17 11:52:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qfwm9",
              "author": "ForsookComparison",
              "text": "If I'm at a tech conference in Seattle, I have to speak to 1 person before they know about ChatGPT. Maybe 3-5 people before they know about Claude. But OpenRouter? Probably 20+ before I have a shot.",
              "score": 16,
              "created_utc": "2026-02-16 19:13:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ux7w1",
                  "author": "DistanceSolar1449",
                  "text": "wtf tech conferences would require talking to 3-5 people before they know Claude?",
                  "score": 1,
                  "created_utc": "2026-02-17 13:00:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5pxtf0",
              "author": "SourceCodeplz",
              "text": "Flash 3 is a sleeper",
              "score": 19,
              "created_utc": "2026-02-16 17:50:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5q3bn4",
                  "author": "reddit0r_123",
                  "text": "Pretty great value and performance for the money.",
                  "score": 14,
                  "created_utc": "2026-02-16 18:15:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5r77fk",
                  "author": "WPBaka",
                  "text": "I found it actually better than Pro for my coding tasks a few weeks back.",
                  "score": 3,
                  "created_utc": "2026-02-16 21:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5qgp83",
              "author": "Condomphobic",
              "text": "Gemini 3 Flash is comparable to Gemini 3 Pro. (Not coding)\n\nItâ€™s a really good model.",
              "score": 8,
              "created_utc": "2026-02-16 19:16:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5wq8zu",
                  "author": "Far_Composer_5714",
                  "text": "To me Gemini 3 flash is always predictably shallow. Something about how it writes is not as multifaceted. I'm not sure what it is but it's simply feels single tone.",
                  "score": 1,
                  "created_utc": "2026-02-17 18:31:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5rk7mr",
              "author": "Aiden_craft-5001",
              "text": "Both spending control and privacy management on Google Cloud are terrible.\n\nSmall businesses will find it safer to use OpenRouter. There are many people on forums who activated the paid API just to test NanoBanana and accidentally incurred thousands of dollars in charges due to duplicate calls. It's not easy to put a hard limit on their official API.",
              "score": 3,
              "created_utc": "2026-02-16 22:30:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5su6ia",
                  "author": "Snoo_26157",
                  "text": "I recall that you actually have to go enable the â€œpubsubâ€ and make one that publishes an alert when youâ€™re over budget, and then go to another website to subscribe to that alert so that your billing gets shut off in time. And you do this by writing json configs and JavaScript functions that refer to Google cloud specific terminology. \n\nAfter half an hour of trying to understand this mess I just signed up for open router.",
                  "score": 4,
                  "created_utc": "2026-02-17 02:56:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5skabu",
                  "author": "AnticitizenPrime",
                  "text": "Yeah, I've read these horror stories. From the examples I've seen, you can set up alerts with Gemini API when you go over a threshold, but might not get the notification that you crossed it until a full day later, in which you might have racked up huge fees.\n\nI like the fact that I can load up OpenRouter with ten bucks of credits or whatever, and when that runs out it just stops working instead of racking up charges.",
                  "score": 1,
                  "created_utc": "2026-02-17 01:56:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5u5d73",
                  "author": "AlwaysLateToThaParty",
                  "text": "> Small businesses will find it safer to use OpenRouter. \n\nThe issue with open router is consistency.  You're not actually sure you're running the model you think you're running. For production, that's not really acceptable. If you enter into a contract with a provider, these are requirements. Open router isn't that. For developers testing models?  Great.",
                  "score": 1,
                  "created_utc": "2026-02-17 09:09:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5t9t4w",
              "author": "Which_Slice1600",
              "text": "Not really the case. It used to be claude sonnet that dominated the rank, even if it's closed. Somehow i think using openrouter is convinent in switching between models, so people even use closed modelsz.",
              "score": 3,
              "created_utc": "2026-02-17 04:38:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5r6iuh",
              "author": "whenhellfreezes",
              "text": "Google's AI studio is a pain to setup and if you aren't careful you will allow google to train on your stuff. Where as openrouter makes it easy to use gemini without training allowed.",
              "score": 2,
              "created_utc": "2026-02-16 21:23:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5skjkq",
              "author": "flextrek_whipsnake",
              "text": "Google's APIs are that painful to deal with",
              "score": 2,
              "created_utc": "2026-02-17 01:58:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5q9puq",
              "author": "ihexx",
              "text": "gemini is the only one keeping up with the deepseek clones on price",
              "score": 3,
              "created_utc": "2026-02-16 18:44:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q13vi",
          "author": "segmond",
          "text": "No surprise, OpenRouter users are more likely to lean into Free and/or cheap models.",
          "score": 46,
          "created_utc": "2026-02-16 18:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pvaya",
          "author": "cosimoiaia",
          "text": "China is objectively killing it.",
          "score": 40,
          "created_utc": "2026-02-16 17:38:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pw8m5",
              "author": "Gargantuan_Cinema",
              "text": "All the companies releasing open source models are maximum profit companies, they are releasing them for commercial reasons as it benefits their company to do so. As soon as self improving AI takes off you can expect releases to go dark or substantially nerfed models to be released.",
              "score": 3,
              "created_utc": "2026-02-16 17:42:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5pze6g",
                  "author": "Dry_Yam_4597",
                  "text": "I don't care what reason they release them for as long as they do. And I see no problem in them being rewarded with profits.\n\nBut. You are, sadly, right - I suspect once the meat arrives they will go dark.\n\nWhich is why we need to crack the whip left and right and kickstart a distributed technique for training models \\_somehow\\_. I wish I was more knowledgeable in the field. But maybe one way is to train \"micro\" experts and somehow mix them into larger models or something. No clue. But we need to make it happen otherwise we are fucked. All of these awesome rigs we own can somehow be linked into a massive pool so we can train truly open models.",
                  "score": 11,
                  "created_utc": "2026-02-16 17:57:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5q8vl4",
                  "author": "FullstackSensei",
                  "text": "If self improving AI takes off, almost everyone will be able to improve the model(s) they have for their own use cases",
                  "score": 2,
                  "created_utc": "2026-02-16 18:40:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5q20rc",
                  "author": "cosimoiaia",
                  "text": "Didn't say anything about that. I'm pretty aware. I don't care, we have open models now, even if they might have an order 66 backdoor. Fwiw Mistral is the only AI consumer company I trust. (I'm from the EU and we have actual laws actually enforced).",
                  "score": 1,
                  "created_utc": "2026-02-16 18:09:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5t3upw",
                  "author": "Immediate_Occasion69",
                  "text": "every company does everything to maximize profits, but the ones we care about are the ones that benefit us the most, no?",
                  "score": 1,
                  "created_utc": "2026-02-17 03:57:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6rdxnz",
                  "author": "avd706",
                  "text": "They want us to train them for them",
                  "score": 1,
                  "created_utc": "2026-02-22 12:19:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qnvll",
          "author": "Poudlardo",
          "text": "we need to start a conversation about what Open Source means. Because no model on this list is open source",
          "score": 12,
          "created_utc": "2026-02-16 19:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r89yg",
              "author": "-Crash_Override-",
              "text": "Dont even bother dude, ive tried to explain what open weight means ad nauseum. People dont want to hear it for whatever reason. I think it runs contrary to the simple 'big tech bad' narrative they've constructed.",
              "score": 14,
              "created_utc": "2026-02-16 21:31:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rauuc",
              "author": "whenhellfreezes",
              "text": "Eh I agree that they are merely open weights but I think the path forward is just saying open weights alot and not gatekeeping open source.",
              "score": 7,
              "created_utc": "2026-02-16 21:44:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5rqn5b",
              "author": "username_taken4651",
              "text": "The problem is that there are actually very few models in general that could be considered open-source. Even the original GPT-2 and Llama models didn't have their training datasets released, thus they were not 100% reproducible.\n\nI do agree though. I call all of these 'open-weight' rather than 'open-source'.",
              "score": 2,
              "created_utc": "2026-02-16 23:04:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rv8nl",
                  "author": "sammoga123",
                  "text": "And why do you want it? So that anti-AI people can use the training data to sue the companies that train AI?\n\n\nOne thing is true: the vast majority of companies have to take data from the internet, from everywhere, most likely without explicit permission from the owners of those websites, in order to obtain as many training tokens as possible.\n\n\nBut I think that if someone were to publish everything... that company would probably be accused of many things, and with tangible evidence of it. I don't think the problem is about research and advancements; rather, it's to prevent Luddites from going crazy and other companies with copyrights from suing those companies.",
                  "score": 1,
                  "created_utc": "2026-02-16 23:30:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5rrck1",
                  "author": "Poudlardo",
                  "text": "Take a look at swiss-ai. They reached pretty good results with Apertus (8b and 70b) TRUELY open source. Eveything is on HF",
                  "score": 0,
                  "created_utc": "2026-02-16 23:08:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5sqt0n",
              "author": "chawza",
              "text": "DeepsSeek is open weight and open source right?",
              "score": 1,
              "created_utc": "2026-02-17 02:35:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qtzpu",
          "author": "ohwut",
          "text": "This reminds me of when Elon was touting how Grok Code-fast-1 was the top coding model by tokens and entirely ignoring that they were giving it away 100% for free.Â \n\nOf course the cheapest or free version, will have the most tokens used. Doesnâ€™t say anything about the quality of those output tokens.",
          "score": 3,
          "created_utc": "2026-02-16 20:21:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pyx4x",
          "author": "winci45",
          "text": "No shit",
          "score": 7,
          "created_utc": "2026-02-16 17:55:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qaha0",
              "author": "ihexx",
              "text": "??? \n\nwhat do you mean no shit?\n\nthis is the first time this has ever happened",
              "score": 1,
              "created_utc": "2026-02-16 18:47:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qfgmr",
                  "author": "tmvr",
                  "text": "Because OpenRouter is where people go for these models. If someone is paying for Claude, Gemini or GPT they predominantly have it either directly from the vendor or they have it through their enterprise subscription of Github Copilot.",
                  "score": 10,
                  "created_utc": "2026-02-16 19:10:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qnj4o",
          "author": "alexeiz",
          "text": "Minimax, Kimi, GLM are free on various agents.  Free models are used the most.  Nothing to see here.",
          "score": 2,
          "created_utc": "2026-02-16 19:49:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qr124",
          "author": "Alex_1729",
          "text": "And Chinese!",
          "score": 3,
          "created_utc": "2026-02-16 20:06:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5r7mz0",
              "author": "-Crash_Override-",
              "text": "You say that like its a good thing.",
              "score": -5,
              "created_utc": "2026-02-16 21:28:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5r9f2k",
                  "author": "Alex_1729",
                  "text": "Why wouldn't it be? I meant as a praise, they are smart and capable, as they are among the leading researches in many industries. A credit had to be given when deserved.",
                  "score": 3,
                  "created_utc": "2026-02-16 21:37:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5qjl89",
          "author": "tracagnotto",
          "text": "Happily using for free Step 3.5 Flash in openclaw isolated machine. Top notch stuff.  \nBurned 250mln tks for free using minmax would have costed me 75$",
          "score": 3,
          "created_utc": "2026-02-16 19:30:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rkz7l",
              "author": "UnknownLesson",
              "text": "How free?",
              "score": 1,
              "created_utc": "2026-02-16 22:34:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5rp6g2",
                  "author": "tracagnotto",
                  "text": "Openrouter offers free usage of various models including the one i mentioned",
                  "score": 2,
                  "created_utc": "2026-02-16 22:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5pvztg",
          "author": "Late_Special_6705",
          "text": "Free? No payments or etc?",
          "score": 1,
          "created_utc": "2026-02-16 17:41:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pw9ky",
              "author": "ThunderBeanage",
              "text": "not free, paid",
              "score": 7,
              "created_utc": "2026-02-16 17:42:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5q1lsh",
              "author": "Far_Composer_5714",
              "text": "Related news on the free models, 4 of 5 models are open source 1 of 5 is a unknown stealth model.Â \n\n\nIf you aren't aware of how open router free works. 50 requests per day, 1000 requests if your account has purchased $10 in credits.",
              "score": 2,
              "created_utc": "2026-02-16 18:07:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qdawb",
          "author": "this-just_in",
          "text": "Glad to see MiniMax achieving some commercial success! Â Well deserved- itâ€™s a high performance and high quality model at a great price point, and the de facto model to run for the high VRAM crowd for months now. Â The new Qwen3.5 VL might be the first legit competitor in months (not ignoring Step 3.5 but you canâ€™t run it performantly today anyhow).Â ",
          "score": 1,
          "created_utc": "2026-02-16 19:00:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qkoy5",
          "author": "ShotokanOSS",
          "text": "Thats pretty awsome! The next step would be to make this models local useable so that everyone can use them freely on his or her own device!",
          "score": 1,
          "created_utc": "2026-02-16 19:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s7f47",
          "author": "rm-rf-rm",
          "text": "Yes because a snapshot of 1 day is a representative sample and a meaningful signal. Especially the day after release of the top most used one",
          "score": 1,
          "created_utc": "2026-02-17 00:40:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sk8vb",
          "author": "Extreme_Coast_1812",
          "text": "This is honestly the best thing happening in AI right now. Open source models keeping up means the big labs can't just lock everything down and charge whatever they want. Competition from open weights is what keeps API prices dropping.",
          "score": 1,
          "created_utc": "2026-02-17 01:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uz2qg",
          "author": "Ylsid",
          "text": "It just goes to show cost matters",
          "score": 1,
          "created_utc": "2026-02-17 13:11:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5viljt",
          "author": "Ambitious-Call-7565",
          "text": "this is bad metric; all it means is models waste more tokens to solve problems\n\nit should be token / session / user",
          "score": 1,
          "created_utc": "2026-02-17 14:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5w2z4y",
          "author": "stargazer_w",
          "text": "Are any of those better than sonnet/gpt5.3?",
          "score": 1,
          "created_utc": "2026-02-17 16:39:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5y6tgt",
          "author": "Jeidoz",
          "text": "The most of them was in free pass",
          "score": 1,
          "created_utc": "2026-02-17 22:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q1zxy",
          "author": "power97992",
          "text": "Minimax m2.5 is a disappointment, m2.1 was better.. Glm 5 /pong alpha seems to be good",
          "score": 1,
          "created_utc": "2026-02-16 18:09:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9vywq",
      "title": "GGML.AI has got acquired by Huggingface",
      "subreddit": "LocalLLaMA",
      "url": "https://github.com/ggml-org/llama.cpp/discussions/19759",
      "author": "Time_Reaper",
      "created_utc": "2026-02-20 13:54:26",
      "score": 397,
      "num_comments": 98,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o6haxh7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-20 19:50:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f9knu",
          "author": "AssistBorn4589",
          "text": "So, HF now encapsulates most of the opensource in AI field?",
          "score": 197,
          "created_utc": "2026-02-20 14:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fdjfi",
              "author": "deadcoder0904",
              "text": "HF is the Vercel of AI, in terms of marketing/distribution at least.",
              "score": 69,
              "created_utc": "2026-02-20 14:25:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fjbfs",
                  "author": "Everlier",
                  "text": "I don't see Vercel as a good company, TBH, they add a bit of convenience and charge 100x for it. I hope that while HF is profitable they won't follow the same path.",
                  "score": 68,
                  "created_utc": "2026-02-20 14:55:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6flxm9",
              "author": "-p-e-w-",
              "text": "Looks like it. Iâ€™m not a fan of this acquisition, though of course the GGML devs have every right to accept it.",
              "score": 34,
              "created_utc": "2026-02-20 15:08:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gisix",
                  "author": "segmond",
                  "text": "why is that?  who else would you want to acquire them?  anyone else and they do be closed.  HF is giving us transformers and being very open with code, their smol open weights, training recipes, tutorials, etc.  I don't think there could have been a better outcome for the community, a better outcome for ggml would have been a private acquisition that made then super rich but the overall community poor.  I'm happy for the team to get some coins.",
                  "score": 14,
                  "created_utc": "2026-02-20 17:40:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6g33a2",
                  "author": "[deleted]",
                  "text": "Is gguf safe?",
                  "score": 1,
                  "created_utc": "2026-02-20 16:28:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6g8pta",
              "author": "singh_taranjeet",
              "text": "Hugging Face is becoming the primary distribution and coordination layer for open weight models, datasets, and tooling, but it does not own or control most open source AI development.",
              "score": 8,
              "created_utc": "2026-02-20 16:53:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fi8sm",
              "author": "Abject_Avocado_8633",
              "text": "It's a fair worry. But the open source ecosystem is still pretty sprawling with independent projects, university labs, and corporate releases outside HF. Their acquisition spree is definitely consolidating tooling, but the actual model innovation is still happening in a lot of places they don't control.",
              "score": 10,
              "created_utc": "2026-02-20 14:49:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ke6vs",
              "author": "ziggo0",
              "text": "I'm genuinely worried they are going to start making people pay a subscription to download models. This shit always happens even in the open source community.",
              "score": 1,
              "created_utc": "2026-02-21 07:22:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fdm4f",
          "author": "BumblebeeParty6389",
          "text": "Hope it's really done for sake of keeping AI opensource like they claim. Opensource needs all the support it can get against growing \"turn everything to cloud\" pressure.",
          "score": 94,
          "created_utc": "2026-02-20 14:26:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fftmz",
              "author": "LocoMod",
              "text": "HF is a shining beacon in the open source community.",
              "score": 52,
              "created_utc": "2026-02-20 14:37:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g4ug6",
                  "author": "EmPips",
                  "text": "I both agree and see that as a problem.\n\nHF has been so good to its community that self-hosted, open source, and P2P distribution is pitiful in the A.I. space and serious proprietary competition feels non-existent.\n\nIt's not too crazy to compare it to the situation with Valve and Gaming. Life is great because they're great but it's a single point of failure that the community doesn't control.",
                  "score": 36,
                  "created_utc": "2026-02-20 16:36:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6fmlzn",
                  "author": "-p-e-w-",
                  "text": "When it comes to quantity and good intentions, absolutely. The quality of some of their libraries could be a little better, but things have been moving in a positive direction recently and I am optimistic that they will get the pain points (e.g. typing in Transformers) under control eventually.",
                  "score": 20,
                  "created_utc": "2026-02-20 15:11:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6guowy",
                  "author": "ongrabbits",
                  "text": "it is currently but money will change that eventually",
                  "score": 1,
                  "created_utc": "2026-02-20 18:34:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fbisc",
          "author": "Blues520",
          "text": "As long as llamacpp keeps going, I'm happy for everyone",
          "score": 76,
          "created_utc": "2026-02-20 14:15:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fng9h",
          "author": "Marksta",
          "text": "So who from Huggingface can mediate the llama.cpp vs. ik_llama.cpp dispute? ðŸ˜",
          "score": 23,
          "created_utc": "2026-02-20 15:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fpb2o",
              "author": "LagOps91",
              "text": "it would be in the best interest of HF to try and do that... huh...",
              "score": 12,
              "created_utc": "2026-02-20 15:24:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hwk6e",
              "author": "a_beautiful_rhind",
              "text": "Their bank account.",
              "score": 3,
              "created_utc": "2026-02-20 21:36:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fhlxw",
          "author": "inaem",
          "text": "Unsloth next?",
          "score": 18,
          "created_utc": "2026-02-20 14:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6igkcc",
              "author": "danielhanchen",
              "text": "Haha :)",
              "score": 7,
              "created_utc": "2026-02-20 23:22:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gft1a",
          "author": "fiddler64",
          "text": "how is huggingface making money",
          "score": 8,
          "created_utc": "2026-02-20 17:26:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6jil58",
              "author": "droptableadventures",
              "text": "At one point they stated that they were doing AI consultancy with businesses that want to do AI with open / custom trained models - and that this more than paid the server bills.",
              "score": 2,
              "created_utc": "2026-02-21 03:13:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fbuq0",
          "author": "FullstackSensei",
          "text": "That's great news for the ggml team. HF has been a shining beacon for the open source community. Hopefully this acquisition will secure continued funding for the ggml team to continue their amazing work supporting the community.",
          "score": 46,
          "created_utc": "2026-02-20 14:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fijln",
              "author": "Abject_Avocado_8633",
              "text": "Funding is a huge deal for these teams. Bootstrapping infra work is brutal. My one caveat is hoping the acquisition doesn't add too much corporate process that slows down the scrappy, iterative dev style that made ggml so useful in the first place.",
              "score": 16,
              "created_utc": "2026-02-20 14:51:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fk4au",
                  "author": "FullstackSensei",
                  "text": "HF doesn't seem to run with any corporate structure, at least not yet. Their whole culture seems to be small (3-5) people focused on tackling a single pain point at a time without much management intervention beyond getting initial approval.",
                  "score": 8,
                  "created_utc": "2026-02-20 14:59:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6f9vu9",
          "author": "HideLord",
          "text": "If it was any other org, I'd be disappointed. But Huggingface is aight",
          "score": 35,
          "created_utc": "2026-02-20 14:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6glk7b",
          "author": "Lesser-than",
          "text": "I dont really like the everthing in one basket that this kind of create's. However llama.cpp is in my oppinion the only inference framework that actually cares about everyone, regardless of their hardware and if that stays the same and is under the Huggingface umbrella well thats a convieneince everyone bennifits from.",
          "score": 5,
          "created_utc": "2026-02-20 17:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6giu77",
          "author": "Equivalent_Job_2257",
          "text": "I hope ollama will now answer for all of their wrongdoing.",
          "score": 17,
          "created_utc": "2026-02-20 17:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fcm0z",
          "author": "RiskyBizz216",
          "text": "https://preview.redd.it/a9bepby6snkg1.png?width=900&format=png&auto=webp&s=f6415f1ca58ca2580ba89dd78350615e5e92575f\n\n",
          "score": 21,
          "created_utc": "2026-02-20 14:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fcdzc",
          "author": "FullOf_Bad_Ideas",
          "text": "AI market consolidation is very visible in 2026.\n\nsmall unprofitable ventures being bought up by larger companies with more revenues\n\nI don't think it's going to be all positive, since HF already started their enshittification journey.",
          "score": 26,
          "created_utc": "2026-02-20 14:19:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fh2b8",
              "author": "Ra77oR",
              "text": "Havent been following their company politics, in what way are they getting shittier?",
              "score": 20,
              "created_utc": "2026-02-20 14:43:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fx3s5",
                  "author": "FullOf_Bad_Ideas",
                  "text": "They are closing the valve on storage and limits. Quanters are now removing old quants from their accounts just to continue operating.\n\nThey removed grandfathered storage limits and billed people on them.\n\nThey push xet which sometimes means slower downloads and less stable pipelines but it saves them on storage costs.\n\nThey do it slowly to boil the frog and maintain lead over competitors but eventually you won't be able to just freely download models without limits as unauthenticated user.",
                  "score": -4,
                  "created_utc": "2026-02-20 16:01:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6fmzm0",
              "author": "Orolol",
              "text": "This isn't a consolidation. GGML wasn't a company to start with.",
              "score": 1,
              "created_utc": "2026-02-20 15:13:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fxnlj",
                  "author": "FullOf_Bad_Ideas",
                  "text": "GGML was a company.\n\n\"ggml.ai is a company founded in 2023 by Georgi Gerganov to support the development of ggml. Nat Friedman and Daniel Gross provided the pre-seed funding. The company was acquired by Hugging Face in 2026.\"\n\nSource is their website.",
                  "score": 18,
                  "created_utc": "2026-02-20 16:03:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hsjyh",
          "author": "DrVonSinistro",
          "text": "And then later on HF is acquired by the bad guys.",
          "score": 4,
          "created_utc": "2026-02-20 21:17:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ig1f5",
          "author": "danielhanchen",
          "text": "Congrats to Georgi and the entire ggml team! Excited for what's coming! :)",
          "score": 4,
          "created_utc": "2026-02-20 23:19:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fp7rq",
          "author": "bharattrader",
          "text": "We are now one company away to be taken over by a big giant. ",
          "score": 7,
          "created_utc": "2026-02-20 15:23:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6goxgm",
          "author": "debackerl",
          "text": "I think that it's a great news for the team, and a recognition for their work.",
          "score": 3,
          "created_utc": "2026-02-20 18:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i6plx",
          "author": "ruibranco",
          "text": "makes a lot of sense honestly. ggml is the backbone of basically every local inference stack and HF is where everyone goes to download models. combining the two just closes the loop.",
          "score": 3,
          "created_utc": "2026-02-20 22:28:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fcsat",
          "author": "qwen_next_gguf_when",
          "text": "When will the license change ? I have a commercial use case running in OpenShift.",
          "score": 9,
          "created_utc": "2026-02-20 14:21:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fej7h",
              "author": "tiffanytrashcan",
              "text": "I mean, the Transformers library is Apache 2 licensed. I don't think we have anything to worry about from them.",
              "score": 14,
              "created_utc": "2026-02-20 14:30:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fi0re",
                  "author": "kulchacop",
                  "text": "I have full hopes on Gerganov that he would have negotiated this aspect as part of the acquisition deal.",
                  "score": 9,
                  "created_utc": "2026-02-20 14:48:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h1oap",
              "author": "giant3",
              "text": "It is MIT licensed, so are you pretty much free to use in commerical applications as long as you distribute the original license text along with the application.\n\nI am more worried that contributions would dwindle  from individuals and only commercial entities that believe that contributing back is beneficial would fix bugs and enhancements. Many would just fork and keep their modifications in house as it  has happened with other open source projects.",
              "score": 2,
              "created_utc": "2026-02-20 19:06:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6g823z",
              "author": "Mkengine",
              "text": "If that were to happen, ollama would be fucked, right?",
              "score": 4,
              "created_utc": "2026-02-20 16:50:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6g8epi",
                  "author": "qwen_next_gguf_when",
                  "text": "Hahaha, I didn't see it . You are a genius.",
                  "score": -1,
                  "created_utc": "2026-02-20 16:52:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6fk5oq",
          "author": "HatEducational9965",
          "text": ">The ggml way We like simplicity and aim to keep the codebase as small and as simple as possible\n\nGoodbye the ggml way then ðŸ˜‰\n\nJoking. Love HF ðŸ¤—ðŸ¤— \n\nGood news!",
          "score": 7,
          "created_utc": "2026-02-20 14:59:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kt02e",
          "author": "Jealous-Ad-202",
          "text": "Tech startup backed by american venture capital acquires essential open source project. what could go wrong?",
          "score": 2,
          "created_utc": "2026-02-21 09:48:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h6i5f",
          "author": "cleverusernametry",
          "text": "People in here defending HF need to learn the phrase \"show me an incentive and I'll show you an outcome\". They are a for profit company. End of story. \n\nThe only saving grace is they are French and culturally may not be as mindless extreme capitalist as a silicon valley company. (Case in point ollama)",
          "score": 5,
          "created_utc": "2026-02-20 19:28:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hh7ih",
              "author": "touristtam",
              "text": "The founder are French, the company is headquartered in NYC.",
              "score": 8,
              "created_utc": "2026-02-20 20:20:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hqybd",
                  "author": "cleverusernametry",
                  "text": "Oh fuck",
                  "score": 9,
                  "created_utc": "2026-02-20 21:09:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hx0w4",
              "author": "a_beautiful_rhind",
              "text": "HF removed the original and kinda tame GPT 4chan.",
              "score": 4,
              "created_utc": "2026-02-20 21:39:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6fav9j",
          "author": "AdDizzy8160",
          "text": "**Congratulations** (for both parties)!",
          "score": 2,
          "created_utc": "2026-02-20 14:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g34n8",
          "author": "[deleted]",
          "text": "Is GGUF safe?",
          "score": 2,
          "created_utc": "2026-02-20 16:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fei2u",
          "author": "Odd-Ordinary-5922",
          "text": "actual W",
          "score": 2,
          "created_utc": "2026-02-20 14:30:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6isjjf",
          "author": "claudiollm",
          "text": "this is huge for the local inference community. llama.cpp basically defined how most of us run models on consumer hardware and HF acquiring ggml hopefully means more resources for development without sacrificing the open source spirit\n\nmy main hope is they keep the scrappy iterative dev style that made llama.cpp so good. sometimes acquisitions add too much process and slow things down",
          "score": 1,
          "created_utc": "2026-02-21 00:31:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kd07k",
          "author": "No_Conversation9561",
          "text": "I hope llama.cpp developers can now get paid to add model support.",
          "score": 1,
          "created_utc": "2026-02-21 07:10:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6f8pc5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -11,
          "created_utc": "2026-02-20 14:00:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6fbnh7",
              "author": "stddealer",
              "text": "Huh? Llama is owned by Meta.",
              "score": 7,
              "created_utc": "2026-02-20 14:15:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6fbpmg",
              "author": "xeeff",
              "text": "llama is AI family made by Meta\n\nllama.cpp is a runtime to run .GGUF models\n\nGGML is an organisation in charge of developing llama.cpp, whisper.cpp, and the GGML library which both utilise (and more in reply below. thanks @stddealer. I also just realised your name, love it.) \n\nhuggingface is a website to view any models, regardless if it's ONNX, GGUF, safetensors, or some other. all model pages are, realistically, just heavy Git repositories\n\nnot sure where you got your info from but hope that cleared it up",
              "score": 5,
              "created_utc": "2026-02-20 14:16:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fevm7",
                  "author": "stddealer",
                  "text": "You're confusing the old .ggml file format with the GGML organization.\n\nGGML is:  \n\\- An organization that is in charge of the development of llama.cpp, whisper.cpp and the GGML library  \n\\- The C/C++ machine learning library that powers projects like llama.cpp or stable-diffusion.cpp  \n\\- The old deprecated file format used to store quantized models for ancient versions of llama.cpp (now replaced with GGUF)",
                  "score": 3,
                  "created_utc": "2026-02-20 14:32:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g1pe3",
          "author": "giant3",
          "text": "While this is good news that they got financial backing to continue, llama.cpp is nowhere production quality even after 3 years. I mean you can't run llama.cpp for a few days. It becomes very slow and I am not sure whether the issue is in llama.cpp or other libraries.",
          "score": -10,
          "created_utc": "2026-02-20 16:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g92k7",
              "author": "AbstrusSchatten",
              "text": "I don't have the same experience. I run multiple SLMs with it and haven't had any problems. Currently multiple months on production.",
              "score": 3,
              "created_utc": "2026-02-20 16:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gytag",
                  "author": "giant3",
                  "text": "I am running on Linux with AMD card. After 1 day of leaving it running, prompt processing and inference goes down to single digit token/sec. After restart, it is back to normal. I haven't been able to figure out where the issue is.",
                  "score": 0,
                  "created_utc": "2026-02-20 18:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gfp2r",
              "author": "Savantskie1",
              "text": "Um, I run lm studio which has it as the backend and I leave my main chat model Qwen3-30B-A3B online 24/7 and I see no slowdown. If anything it runs faster. You have something broken",
              "score": 3,
              "created_utc": "2026-02-20 17:26:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9zt8m",
      "title": "The top 3 models on openrouter this week ( Chinese models are dominating!)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/h4l8zr4rdokg1.jpeg",
      "author": "keb_37",
      "created_utc": "2026-02-20 16:21:50",
      "score": 374,
      "num_comments": 91,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6j64tp",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-21 01:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g2u4y",
          "author": "cgs019283",
          "text": "I really hoped it would be something more like \"Open-source models are dominating,\" but it is true that most of them are Chinese at this moment...",
          "score": 99,
          "created_utc": "2026-02-20 16:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gi8bm",
              "author": "Zc5Gwu",
              "text": "Minimax is like an open-weights sonnet. It's 90% there for a lot cheaper.",
              "score": 43,
              "created_utc": "2026-02-20 17:38:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gjjdb",
                  "author": "Mkengine",
                  "text": "You can go even cheaper with [Qwen3-Coder-Next](https://swe-rebench.com/) at the same performance!",
                  "score": 20,
                  "created_utc": "2026-02-20 17:44:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hf36t",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-02-20 20:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6homr4",
                  "author": "jakajakka",
                  "text": "Blame the US companies donâ€™t open source their modelsâ€¦",
                  "score": 1,
                  "created_utc": "2026-02-20 20:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6hkq25",
          "author": "alexeiz",
          "text": "Both Cline and Kilo are running a free promotion of Minimax M2.5.  That's why it's the top model on openrouter.  Grok code fast 1 was also topping the charts while it was free.  Nothing to do with Chinese or open weights.",
          "score": 44,
          "created_utc": "2026-02-20 20:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i84rg",
              "author": "svantana",
              "text": "Yeah I wonder about this strategy. Don't they understand that as soon as the promotion ends, all those users will switch to another model?",
              "score": 13,
              "created_utc": "2026-02-20 22:36:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6jogvq",
                  "author": "--Spaci--",
                  "text": "Doesn't really matter if they get trillion of tokens of training data from it",
                  "score": 20,
                  "created_utc": "2026-02-21 03:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6l39xx",
              "author": "Due-Memory-6957",
              "text": "These models are not free on OpenRouter",
              "score": 2,
              "created_utc": "2026-02-21 11:27:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6idb23",
              "author": "zyreph_",
              "text": "And this models are free via Kilo Provider not via openrouter.",
              "score": 3,
              "created_utc": "2026-02-20 23:04:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6l8tn6",
              "author": "peachy1990x",
              "text": "grok code fast has been free on windsurf for a very long time, since it came out pretty sure, its really crap tho not sure whos using it",
              "score": 1,
              "created_utc": "2026-02-21 12:16:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ghqxx",
          "author": "Emotional-Ad5025",
          "text": "I believe minimax has a lot of space to increase its usage, a lot of people, including deepseek users, have no idea how good and cheap it is.",
          "score": 15,
          "created_utc": "2026-02-20 17:35:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ipwr3",
              "author": "alenym",
              "text": "Yeah, just like me. I want to give a try to minimax.",
              "score": 2,
              "created_utc": "2026-02-21 00:16:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g4lf5",
          "author": "Condomphobic",
          "text": "They arenâ€™t destroying American models because most people donâ€™t use OpenRouter, man. They use API directly from the provider. \n\nOpenRouter is mostly for ppl that prefer OS models, if you havenâ€™t noticed",
          "score": 148,
          "created_utc": "2026-02-20 16:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gbhcs",
              "author": "Edzomatic",
              "text": "I use OpenRouter because it's easy to switch models and test everything, I assume most people do the same. But anyone doing large amount of inference will surely use the provider directly and save themselves 5%",
              "score": 50,
              "created_utc": "2026-02-20 17:06:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6h0mlc",
                  "author": "Traditional-Gap-3313",
                  "text": "Not only 5%, but prompt caching on opennrouter does not work reliably. Even for models with a single provider some prompts will often miss the cache. That's a problem for anything from google/openai since you're often counting on the cache working correctly in your calculations",
                  "score": 16,
                  "created_utc": "2026-02-20 19:01:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ghvhg",
              "author": "jackmusick",
              "text": "Iâ€™d argue too that this doesnâ€™t represent which one of these are the best either, just that MiniMax is the best value for the target audience. \n\nStill really cool donâ€™t get me wrong, but important context. I love to see the open source models doing well.",
              "score": 11,
              "created_utc": "2026-02-20 17:36:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6j65ro",
              "author": "AnticitizenPrime",
              "text": "> They arenâ€™t destroying American models because most people donâ€™t use OpenRouter, man. They use API directly from the provider.\n\nMost 'normies' don't even know or understand what API is and think you need a subscription to use AI services. \n\nFor those who do know what API is, going direct to the provider is fine if you're only using that provider or a few. But if you want access to every model under the sun without having to manage dozens or hundreds of accounts with different providers, OpenRouter makes sense. I can't be fucked to have my credit card on file with god knows how many API providers, all with their varying terms and conditions and whatnot, and trying to track the billing on that. Openrouter puts it all in one place. I can load up ten bucks and try out basically any model from any provider, under one convenient billing dashboard. \n\nErgo, it strikes me that the sort of people who use OR are the more savvy type, who are aware of how this shit works, and they're the ones that seem to be preferring the open models (due to whatever reason - speed, price, whatever). \n\nIMO, going to the providers directly only makes sense if you're totally sold on that provider, or have infinite patience to manage paying many providors and tracking the usage independently, and keeping up with their costs, and juggling dozens of separate API keys. To me it sounds like a nightmare.",
              "score": 11,
              "created_utc": "2026-02-21 01:55:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ge5hv",
              "author": "keb_37",
              "text": "I got overwhelmed, i guess lmao\nI know they aren't destroying them, but regarding using openrouter, i honestly find it more easily to set up and use and to switch between models, i prefer it ( as a user, not as an enterprise)\n\nAlso, consider that those opensource models can be downloaded and used directly ( which is their core advantage)",
              "score": 2,
              "created_utc": "2026-02-20 17:19:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gfegq",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-02-20 17:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6gfnyd",
              "author": "HornyGooner4401",
              "text": "I have a subscription with whichever provider that has the most generous limit and then use Opus or Gemini Pro through OpenRouter whenever I need them",
              "score": 2,
              "created_utc": "2026-02-20 17:26:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ggv48",
                  "author": "Condomphobic",
                  "text": "Still got my 15 months free of Gemini Pro. \n\nProbably going to stay with them afterwards.  They offer too much for $20",
                  "score": 7,
                  "created_utc": "2026-02-20 17:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6mlo3q",
              "author": "Caffdy",
              "text": ">  ppl that prefer OS models\n\nwhat are OS models?",
              "score": 1,
              "created_utc": "2026-02-21 17:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mmpiq",
                  "author": "Condomphobic",
                  "text": "Open source\n\nThe top 3 in the screenshot are OS",
                  "score": 1,
                  "created_utc": "2026-02-21 17:06:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gac6k",
          "author": "Technical-Earth-3254",
          "text": ">the first time i see chinese models destroying US ones like this\n\n  \ncap. If you want to use the big, closed models ur not going through OR lmao",
          "score": 28,
          "created_utc": "2026-02-20 17:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h7urt",
          "author": "Fault23",
          "text": "who tf uses 2.5 flash at this point",
          "score": 10,
          "created_utc": "2026-02-20 19:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gkqns",
          "author": "SohelAman",
          "text": "Kimi K2.5 is too good. Used it for a week in opencode.",
          "score": 7,
          "created_utc": "2026-02-20 17:49:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h8zo9",
          "author": "Imakerocketengine",
          "text": "We need to be realist for a second, most american models are being used outside of open router",
          "score": 6,
          "created_utc": "2026-02-20 19:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gf62m",
          "author": "jakegh",
          "text": "GLM5 is *so* good. And minimax m2.5 is *so* cheap. \n\nKimi K2.5, I dunno. Maybe e-girlfriends or something.",
          "score": 9,
          "created_utc": "2026-02-20 17:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ggaoz",
              "author": "keb_37",
              "text": "Kimi is a multimodel\nIt can nativity receive pics\nI think this is not the case for other two",
              "score": 10,
              "created_utc": "2026-02-20 17:29:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6grhr6",
                  "author": "jakegh",
                  "text": "True, biggest problem with GLM5 is you can't paste screenshots.",
                  "score": 4,
                  "created_utc": "2026-02-20 18:20:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6jzjan",
              "author": "deadcoder0904",
              "text": "Kimi is GOAT at writing. Like Sonnet level for 80% less price.",
              "score": 1,
              "created_utc": "2026-02-21 05:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6ifktt",
          "author": "soumen08",
          "text": "Kimi is such a solid model, it's my go to for almost everything.",
          "score": 5,
          "created_utc": "2026-02-20 23:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g57vu",
          "author": "aaronpaulina",
          "text": "\"destroying\" is a strong word",
          "score": 14,
          "created_utc": "2026-02-20 16:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6iy59v",
          "author": "ansibleloop",
          "text": "Minimax is a default model in OpenClaw, so not surprising it's top since the idiots using it just blindly skip through the setup",
          "score": 5,
          "created_utc": "2026-02-21 01:05:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6qx3o7",
              "author": "xmankkxc",
              "text": " You should know why it is top first.   ... everything is about cost.",
              "score": 1,
              "created_utc": "2026-02-22 09:43:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6g7m0t",
          "author": "HebelBrudi",
          "text": "Wonder what the numbers would be if codex and cc would count towards them. IIRC Cline/Roo/Kilo usage (even free models) count here.",
          "score": 3,
          "created_utc": "2026-02-20 16:48:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gqyp7",
          "author": "bakawolf123",
          "text": "not suprized. looking at minimax 2.5 as local driver when m5 ultra hits the market too",
          "score": 3,
          "created_utc": "2026-02-20 18:17:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gf6ng",
          "author": "TheLineDixon",
          "text": "GLM 5 is the best among them. Kimi is an idiot. Minimax is so-so. GLM 5 is good.",
          "score": 4,
          "created_utc": "2026-02-20 17:23:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gg0qo",
              "author": "keb_37",
              "text": "Agree but it's very slow",
              "score": 3,
              "created_utc": "2026-02-20 17:27:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6htj8h",
                  "author": "NinjaWK",
                  "text": "I paid for GLM Max a few weeks ago, and man, it's super slow.",
                  "score": 6,
                  "created_utc": "2026-02-20 21:21:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6g6ej9",
          "author": "Few_Painter_5588",
          "text": "I'm more surprised by the top 10.\n\nGrok 4.1 Fast and Trinity Large? I lowkey expected StepFun Flash 3.5 and Qwen 3.5 to be there",
          "score": 5,
          "created_utc": "2026-02-20 16:43:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gbo8d",
              "author": "Edzomatic",
              "text": "Trinity is completely free",
              "score": 11,
              "created_utc": "2026-02-20 17:07:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6g8ype",
              "author": "HebelBrudi",
              "text": "People shit on the Grok models because of Elon (I get it) but they are cheap, fast as hell and way better than their price.",
              "score": 12,
              "created_utc": "2026-02-20 16:55:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6hzd6h",
                  "author": "sn2006gy",
                  "text": "Grok simply can't be trusted.  It would do better distancing itself from X and Elon.",
                  "score": 0,
                  "created_utc": "2026-02-20 21:50:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6jzpr9",
                  "author": "Shockbum",
                  "text": "I'm using Grok quite a bit and \"chat. qwen .ai/\"\n\nGrok is quite competent, better than ChatGPT, Qwen is free haha.\n\nLocal: Qwen Next Coder and Qwen VL 30B",
                  "score": 1,
                  "created_utc": "2026-02-21 05:16:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gjr0g",
          "author": "abdouhlili",
          "text": "Doubao alone processes 80 trillion token per day.",
          "score": 2,
          "created_utc": "2026-02-20 17:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6glxrz",
              "author": "keb_37",
              "text": "What's doubao? A provider?",
              "score": 2,
              "created_utc": "2026-02-20 17:55:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6gnm19",
                  "author": "Professional_Price89",
                  "text": "It tiktok AI",
                  "score": 3,
                  "created_utc": "2026-02-20 18:02:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6gtf6x",
                  "author": "abdouhlili",
                  "text": "ByteDance main platform, Tiktok owner.",
                  "score": 2,
                  "created_utc": "2026-02-20 18:28:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6k7x9u",
                  "author": "ansmo",
                  "text": "Aside from what the other people have said, it is the single most popular AI used by Chinese students.",
                  "score": 2,
                  "created_utc": "2026-02-21 06:24:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6iiec0",
          "author": "Lucky-Substance5585",
          "text": "Kimi K2.5 is too good. Used it for a week in opencode.",
          "score": 2,
          "created_utc": "2026-02-20 23:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ijyuy",
          "author": "Strange_Squirrel_886",
          "text": "I have just run about 200m tokens in half a week with MiniMax M2.5. instead of PAYG on openrouter, I just got MiniMax directly and bought one month of coding plan. $20 level with 300 requests per 5 hrs. And use the BYOK of openrouter so I can monitor my usage more easily.\n\nMan, openclaw burns through so many tokens so quickly.",
          "score": 2,
          "created_utc": "2026-02-20 23:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rnxrd",
          "author": "alitadrakes",
          "text": "I wish i could jjst run minimax m2.5 or GLM5 on my local setup ðŸ˜­",
          "score": 2,
          "created_utc": "2026-02-22 13:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6taf41",
              "author": "keb_37",
              "text": "What's your setup",
              "score": 1,
              "created_utc": "2026-02-22 18:15:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6iy7dp",
          "author": "Iory1998",
          "text": "Chinese labs already caught up to the frontier American labs in my opinion.",
          "score": 3,
          "created_utc": "2026-02-21 01:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jt1w2",
          "author": "TurnUpThe4D3D3D3",
          "text": "I love GLM 5, itâ€™s a great model",
          "score": 1,
          "created_utc": "2026-02-21 04:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6khvyu",
          "author": "LiquidPhilosopher",
          "text": "minimax 2.5 isn't realiable for agentic tasks.\nvery very inconsistent.\nalso hallucinates and blatantly lies.",
          "score": 1,
          "created_utc": "2026-02-21 07:57:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ln00n",
          "author": "Open_Establishment_3",
          "text": "Minimax 2.5 is able to speak French, English, Russian, Chinese and German to me in the same paragraph, with 6 spelling errors. And it does it without even asking for it! Long live 2026.",
          "score": 1,
          "created_utc": "2026-02-21 13:56:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mcu7b",
          "author": "Torodaddy",
          "text": "Because they are cheap",
          "score": 1,
          "created_utc": "2026-02-21 16:17:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mdw72",
          "author": "pwnrzero",
          "text": "The best models are the ones you can get free/cheap. \n\nSource - my organization is paying for unlimited Sonnet 4.6 and Opus 4.6 usage.",
          "score": 1,
          "created_utc": "2026-02-21 16:22:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6mzt75",
          "author": "random-gyy",
          "text": "Because theyâ€™re far cheaper",
          "score": 1,
          "created_utc": "2026-02-21 18:12:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g8up5",
          "author": "bad_detectiv3",
          "text": "Can anyone tell me why should I use openrouter instead of directly engaging with Google Gemini?\nIs it because of open router pay as you go model?",
          "score": 1,
          "created_utc": "2026-02-20 16:54:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6g9wu8",
              "author": "HebelBrudi",
              "text": "Counterintuitively I would go to OR for closed source models bc Google billing can be tricky as hell to set up (apparently they have simplified it but I didnâ€™t check) and it does have benefits to have one acc for billing. But for open weight models I would go either deepinfra (cheap but good setup -> tool calls) or fireworks ai (more expensive but really fast). Random routing destroys taking advantage of cache pricing and also there are a lot of providers that use bad setups or in my opinion weird quants that can make tool calls a nightmare.",
              "score": 10,
              "created_utc": "2026-02-20 16:59:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6gmmmv",
                  "author": "Novel-Injury3030",
                  "text": "how can u be sure these middlemen wont intercept ur data or code?Â ",
                  "score": 2,
                  "created_utc": "2026-02-20 17:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6h0rml",
              "author": "WPBaka",
              "text": "OR is great for testing new model releases. $10 goes a really long way as long as you aren't going crazy on context or solely using opus/openai",
              "score": 5,
              "created_utc": "2026-02-20 19:01:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6hznfu",
              "author": "sn2006gy",
              "text": "OR is great to tie into a chat of your choice to use whatever models you want to tinker around with.  However, it gets expensive for anything above chat.  API costs too much for coding because others keep their coding prices down by caching the hell out of everything.",
              "score": 2,
              "created_utc": "2026-02-20 21:52:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gbaae",
          "author": "JaconSass",
          "text": "I would expect the Chinese models to dominate. Not because theyâ€™re better but bc THERES A BILLION Chinese only allowed to use Chinese models.",
          "score": -3,
          "created_utc": "2026-02-20 17:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6iiwvy",
              "author": "Basic_Extension_5850",
              "text": "I doubt many people in China are using OpenRouter",
              "score": 6,
              "created_utc": "2026-02-20 23:36:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}