{
  "metadata": {
    "last_updated": "2026-02-12 17:15:22",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 20,
    "total_comments": 781,
    "file_size_bytes": 866288
  },
  "items": [
    {
      "id": "1r26zsg",
      "title": "Z.ai said they are GPU starved, openly.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/kjy1wqzt2xig1.jpeg",
      "author": "abdouhlili",
      "created_utc": "2026-02-11 19:28:16",
      "score": 1325,
      "num_comments": 215,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r26zsg/zai_said_they_are_gpu_starved_openly/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4vyt43",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-11 23:10:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uq8w8",
          "author": "atape_1",
          "text": "Great transparency. ",
          "score": 478,
          "created_utc": "2026-02-11 19:31:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w7ihx",
              "author": "EndlessZone123",
              "text": "Wasn't great transparency to sell their coding plans cheap and have constant api errors.",
              "score": 18,
              "created_utc": "2026-02-11 23:58:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4uqzfi",
              "author": "ClimateBoss",
              "text": "Maybe they should do GLM Air instead of 760b model LMAO",
              "score": 169,
              "created_utc": "2026-02-11 19:35:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4us7o6",
                  "author": "suicidaleggroll",
                  "text": "A 744B model with 40B active parameters, in F16 precision.  That thing is gigantic (1.5 TB) at its native precision, and has more active parameters than Kimi.  They really went a bit nuts with the size of this one.",
                  "score": 135,
                  "created_utc": "2026-02-11 19:41:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o508m8p",
                  "author": "keyboardmonkewith",
                  "text": "No!!! Its suppose to know who is a pinocchio and dobby in a greatest detail.",
                  "score": 1,
                  "created_utc": "2026-02-12 16:38:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ux74h",
                  "author": "Ardalok",
                  "text": "Users probably don't buy Air tokens.",
                  "score": -1,
                  "created_utc": "2026-02-11 20:04:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wzf02",
              "author": "SkyFeistyLlama8",
              "text": "If they're complaining about *inference* being impacted by the lack of GPUs, then those domestic Huawei or whatever tensor chips aren't as useful as they were claimed to be. Inference is still an Nvidia or nothing situation.",
              "score": 4,
              "created_utc": "2026-02-12 02:45:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y1ilp",
                  "author": "HoushouCoder",
                  "text": "Thoughts on Cerebras?",
                  "score": 1,
                  "created_utc": "2026-02-12 07:36:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yr31g",
                  "author": "TylerDurdenFan",
                  "text": "I think Google's TPUs are doing just fine",
                  "score": 1,
                  "created_utc": "2026-02-12 11:38:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uudro",
          "author": "x8code",
          "text": "I am GPU starved as well. I can't find an RTX 5090 for $2k. I would buy two right now if I could get them for that price.",
          "score": 188,
          "created_utc": "2026-02-11 19:51:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4veqwu",
              "author": "Shoddy_Bed3240",
              "text": "Buy RTX 6000 Pro 96gb instead. Microcenter have it in stock",
              "score": 16,
              "created_utc": "2026-02-11 21:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vj1gi",
                  "author": "Polymorphic-X",
                  "text": "Don't get it from microcenter unless you need the convenience.\nThey're $7.3k through places like exxact or other vendors. Significantly cheaper than Newegg or MC",
                  "score": 11,
                  "created_utc": "2026-02-11 21:50:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w6l3m",
                  "author": "Guilty_Rooster_6708",
                  "text": "Isn‚Äôt that also significantly higher priced than $4k?",
                  "score": 1,
                  "created_utc": "2026-02-11 23:53:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vmcc0",
                  "author": "iMakeSense",
                  "text": "I'm not sure those are optimized for gaming though",
                  "score": -7,
                  "created_utc": "2026-02-11 22:06:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vcask",
              "author": "PentagonUnpadded",
              "text": "I see DGX Spark / GB10 type systems going for the 3k MSRP right now. Why not build out with that system? \n\nI've seen comparisons showing a GB10 as 1/3 to 1/2 of a 5090 depending on the task, plus of course 4 times the vRam. Curious what tasks you have that make a dual-5090 system at $4k the way to go over alternatives like a GB10 cluster.",
              "score": 17,
              "created_utc": "2026-02-11 21:18:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vf4sb",
                  "author": "x8code",
                  "text": "I thought about it, but I also use my GPUs for PC gaming. I would get the 4 TB DGX Spark though, not the 1 TB model. Those go for $4k each last I checked. I would probably buy 2x DGX Spark though, so I could cluster them and run larger models with 256GB (*minus OS overhead*) of unified memory.",
                  "score": 13,
                  "created_utc": "2026-02-11 21:31:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4z75be",
                  "author": "SilentLennie",
                  "text": "> Why not build out with that system? \n\nLower memory bandwidth",
                  "score": 1,
                  "created_utc": "2026-02-12 13:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uxson",
          "author": "sob727",
          "text": "I'm GPU starved as well.\n\n  \nGet in line.",
          "score": 78,
          "created_utc": "2026-02-11 20:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uunge",
          "author": "Clean_Hyena7172",
          "text": "Fair enough, I appreciate their honesty.",
          "score": 27,
          "created_utc": "2026-02-11 19:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uqqcj",
          "author": "sammoga123",
          "text": "At least it's not like Google, suffering from demand and nerfing its models, probably due to quantification to sustain it XD",
          "score": 129,
          "created_utc": "2026-02-11 19:33:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ur754",
              "author": "abdouhlili",
              "text": "Gemini 3 flash is literally better than 3 Pro, Gemini models act like advertised benchmarks for about 3 weeks and then they start nerfing it.",
              "score": 125,
              "created_utc": "2026-02-11 19:36:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4urzge",
                  "author": "sammoga123",
                  "text": "Right now, pro plan users are complaining because they're only getting about 20 uses of the pro model. I've been trying to use NBP in the API and it fails, and when it does, the results are pretty baffling, which leads me to believe that's why they haven't released anything lately either.",
                  "score": 29,
                  "created_utc": "2026-02-11 19:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vgbfl",
                  "author": "Goldkoron",
                  "text": "I find 2.5 pro better for some tasks than 3 pro. Kind of just switch between models for different advantages",
                  "score": 3,
                  "created_utc": "2026-02-11 21:37:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xmihk",
                  "author": "Lazylion2",
                  "text": "I don't know why people say that, I use both with Antigravity and Pro solved some problems Flash couldn't",
                  "score": 1,
                  "created_utc": "2026-02-12 05:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4utoig",
                  "author": "RonJonBoviAkaRonJovi",
                  "text": "What an ignorant comment",
                  "score": -12,
                  "created_utc": "2026-02-11 19:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vqfy9",
              "author": "dreamkast06",
              "text": "I wish they would just give a higher quota on the smaller models so we could use those when it makes sense. Right now, even using Air pulls from the same pool as full fat 4.7",
              "score": 1,
              "created_utc": "2026-02-11 22:26:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4zb994",
              "author": "RedParaglider",
              "text": "OMG I'm on the google ultra plan and I can't wait for that shit to be over with.  Nonstop failures on the models.  The Gemini TUI is unusable across all models.  It retries 3 times then throws an apology error all the time.  Google gave so much damn free access they can no longer support people paying them 260 a month.  At least opus 4.6 works decently on it with some failures but fewer.  \n\nThey advertised all this usage, but unless you want to sit and spam next next next next retry retry retry all damn day you will never get 1/100th of the usage promised.",
              "score": 1,
              "created_utc": "2026-02-12 13:52:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vissm",
              "author": "-dysangel-",
              "text": "I think they might be. The coding plan quality is awful today compared to the last few weeks...",
              "score": 1,
              "created_utc": "2026-02-11 21:49:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uu1bd",
          "author": "eli_pizza",
          "text": "Ok but to be fair, OpenAI says the same thing \n\n> OpenAI President Greg Brockman said the lack of compute is still holding the company back.\n\n> He said that even OpenAI's ambitious investments might not be enough to meet future demand.\n\n> OpenAI also published a chart that illustrates how scaling compute is the key to profitability.\n\nhttps://www.businessinsider.com/openai-chart-compute-future-plans-profitability-2025-12",
          "score": 41,
          "created_utc": "2026-02-11 19:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v5613",
              "author": "Ragvard_Grimclaw",
              "text": "It's less of a \"lack of compute\" and more of a \"lack of power grid capacity\". Here's an interview with Microsoft CEO:  \n[https://www.datacenterdynamics.com/en/news/microsoft-has-ai-gpus-sitting-in-inventory-because-it-lacks-the-power-necessary-to-install-them/](https://www.datacenterdynamics.com/en/news/microsoft-has-ai-gpus-sitting-in-inventory-because-it-lacks-the-power-necessary-to-install-them/)  \nYes, they've caused consumer GPU shortages due to shifting focus to datacenter GPUs, while not even having where to plug them. Guess it's time to also raise electricity prices for regular people because datacenters need it more?",
              "score": 40,
              "created_utc": "2026-02-11 20:43:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vjcgh",
                  "author": "MasterKoolT",
                  "text": "I'll say that Microsoft, at least in their giant data center project in SE Wisconsin, has committed to paying a higher electricity rate to fund power grid capacity increases. That hasn't been the story everywhere but seems like a good strategy to not antagonize locals (and is really just part of being a good neighbor)",
                  "score": 8,
                  "created_utc": "2026-02-11 21:51:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vabe4",
                  "author": "HarvestMana",
                  "text": "And its the opposite in China, where they have have way more energy, but not enough compute.",
                  "score": 12,
                  "created_utc": "2026-02-11 21:08:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4xlfll",
                  "author": "EarEquivalent3929",
                  "text": "Looks like rich fucks not backing nuclear a decade ago for reasons of greed are coming back to bite them in the ass",
                  "score": 3,
                  "created_utc": "2026-02-12 05:14:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4y8r72",
                  "author": "VampiroMedicado",
                  "text": "I saw a report that they‚Äôre already doing that in the US, and also putting data centers nears people homes so they now hear a hum 24/7, it‚Äôs amazing.",
                  "score": 1,
                  "created_utc": "2026-02-12 08:46:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybyt0",
                  "author": "pier4r",
                  "text": "> Yes, they've caused consumer GPU shortages due to shifting focus to datacenter GPUs, while not even having where to plug them. \n\nas someone on youtube in a bullish way said \"there are no dark GPUs!\" (then darkness hit him)",
                  "score": 1,
                  "created_utc": "2026-02-12 09:18:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uxahr",
          "author": "nuclearbananana",
          "text": "Deepseek has hinted at the same thing. I wonder how Kimi is managing to avoid it.",
          "score": 13,
          "created_utc": "2026-02-11 20:05:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v169e",
              "author": "TheRealMasonMac",
              "text": "I don't think they did. That's why they switched to INT4 which brings VRAM 4x lower than full fat GLM-5.",
              "score": 23,
              "created_utc": "2026-02-11 20:24:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vjzan",
                  "author": "nuclearbananana",
                  "text": "That helps with inf3rence, but not training.\n\nAlso 4x? Isn't the KV cache separate?",
                  "score": 5,
                  "created_utc": "2026-02-11 21:54:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v3wyw",
                  "author": "Remote_Rutabaga3963",
                  "text": "What makes you think that GLM 5 is being served at fp16 ?",
                  "score": -1,
                  "created_utc": "2026-02-11 20:37:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wm6o5",
              "author": "-Cacique",
              "text": "For the past few days, I'm unable to use kimi 2.5 thinking, it's auto switched to 2.5 instant model due to high demand apparently.",
              "score": 1,
              "created_utc": "2026-02-12 01:25:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y6dp2",
              "author": "Bac-Te",
              "text": "They're not, that's why they're doing Anthropic/ Google/ OpenAI price point instead of the GLM coding plan price point.",
              "score": 1,
              "created_utc": "2026-02-12 08:22:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4uwi2l",
          "author": "Dry_Yam_4597",
          "text": "That's refreshing. A company announcing a new model without telling people to feel worthless? Without anthropomorphizing it? Without telling us to fear all sorts of made up bad scifi scenarios? Incredible what cool stuff sane people can do with new tech. Hope those leading certain companies take their meds or themselves where their belong.   ",
          "score": 32,
          "created_utc": "2026-02-11 20:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uuday",
          "author": "Middle_Bullfrog_6173",
          "text": "They knew this but still went with a larger model and more active parameters? I guess they expect to get more compute soonish.",
          "score": 14,
          "created_utc": "2026-02-11 19:51:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wiu7v",
              "author": "AnomalyNexus",
              "text": "The only thing more important than having enough compute is having hype.\n\nThese days no hype means no investors means no money for compute\n\nSo you kinda have to go big or go home. Hence large model \n\nThis space is full of whacky logic where gravity doesn‚Äôt apply and things fall up when you drop them  :/",
              "score": 14,
              "created_utc": "2026-02-12 01:05:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y6imk",
                  "author": "Bac-Te",
                  "text": "No wonder why Google named their tool Antigravity lol",
                  "score": 3,
                  "created_utc": "2026-02-12 08:24:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4yc8jj",
              "author": "DerpSenpai",
              "text": "A big fat model is used to make the lower end models so right now most likely that's their priority",
              "score": 1,
              "created_utc": "2026-02-12 09:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v8lj5",
          "author": "Bandit-level-200",
          "text": "When are LLM makers going to make more efficient LLMs they are so inefficient in using both memory and power",
          "score": 6,
          "created_utc": "2026-02-11 21:00:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vaweo",
              "author": "abdouhlili",
              "text": "GLM-5 uses new Deepseek sparse attention mechanism, which reduces inference costs up to 50%, Not only this, Z.ai doubled in this by increasing GLM-5 price. They are clearly chasing gross margins.",
              "score": 7,
              "created_utc": "2026-02-11 21:11:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4vbvba",
                  "author": "Bandit-level-200",
                  "text": "Yes but its still inefficient take context for example something that if it was just plain text would be a few KB/MB suddenly needs GB of memory just because it needs to be doubled or something for context to work.",
                  "score": -1,
                  "created_utc": "2026-02-11 21:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vk98o",
          "author": "ImmenseFox",
          "text": "Well that's just silly. I subscribed to the Pro plan as it said it will support flagship model updates and now they took it away - yeah they mention they'll roll it out but when you use the same wording as the max plan and then sneakily get rid of it from the list - doesnt fill me with any confidence.  \nGlad now I didn't renew for the whole year and instead just the quarter.",
          "score": 6,
          "created_utc": "2026-02-11 21:55:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uuls9",
          "author": "SubjectHealthy2409",
          "text": "Based, fully support them.",
          "score": 22,
          "created_utc": "2026-02-11 19:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uv1n8",
              "author": "abdouhlili",
              "text": "Do you know what GPUs they use for inference? NVIDIA or Huawei?",
              "score": 0,
              "created_utc": "2026-02-11 19:54:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uvcg2",
                  "author": "SubjectHealthy2409",
                  "text": "Nop, don't know anything from behind the scenes",
                  "score": 7,
                  "created_utc": "2026-02-11 19:55:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vwbkk",
                  "author": "vmnts",
                  "text": "If I recall correctly, the official Chinese policy was that you can use NVIDIA for training, but have to use local for inference (or at least you're not supposed to buy new NVIDIA GPUs for inference). I would imagine that they are using what they have, so it's probably a mix, but over time would trend towards Huawei",
                  "score": 1,
                  "created_utc": "2026-02-11 22:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ut2j9",
          "author": "jacek2023",
          "text": "No Air no fun.",
          "score": 9,
          "created_utc": "2026-02-11 19:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vw9ol",
          "author": "a_beautiful_rhind",
          "text": "You and me both. Their chat used to be fast, since I went back and used it the replies take forever. I just assumed they are struggling, especially when it's free. The speeds feel comprable to *me* running glm.",
          "score": 3,
          "created_utc": "2026-02-11 22:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xlo5y",
          "author": "EarEquivalent3929",
          "text": "Let's hope everyone being starved for compute and energy energizes the race for efficiency over raw power.",
          "score": 3,
          "created_utc": "2026-02-12 05:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4urqd3",
          "author": "Crafty-Diver-6948",
          "text": "I don't care if it's slow, I paid $360 for the inference for a year. happy to run Ralph's with that",
          "score": 8,
          "created_utc": "2026-02-11 19:38:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uxfhj",
              "author": "layer4down",
              "text": "Same. I appreciate the transparency and their wonderful pricing for a near Sonnet-4.5 parity model in GLM-4.7.  $360 year one was a no brainer and unfortunately these folks are a victim of their own success right now. Hope they can pull through now that they IPO‚Äôd last month.",
              "score": 8,
              "created_utc": "2026-02-11 20:05:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wj197",
              "author": "AnomalyNexus",
              "text": "Yup. Really hoping I can renew at similar",
              "score": 2,
              "created_utc": "2026-02-12 01:06:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wjlgq",
                  "author": "layer4down",
                  "text": "I got mine in October and it was a year one discount for 50% off. Will be $720/year thereafter.",
                  "score": 2,
                  "created_utc": "2026-02-12 01:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v3o9y",
          "author": "Dudensen",
          "text": "Calm your ass down, a lot of labs do the same. Kimi literally said the same thing. Qwen too.",
          "score": 5,
          "created_utc": "2026-02-11 20:36:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vdtwg",
          "author": "Comrade-Porcupine",
          "text": "What's positive here is this -- because it is open weight, that model will then be available from others, taking load off of GLM.\n\nDoesn't help GLM, per se, but it helps the software community. Too big to host myself, but it'll probably be on DeepInfra and others in short time. \n\nEDIT: [DeepInfra.com](http://DeepInfra.com) already showing it available.  For cheaper than [z.AI](http://z.AI) \n\nA situation that doesn't apply with OpenAI or Anthropic.",
          "score": 5,
          "created_utc": "2026-02-11 21:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w9z4e",
              "author": "abaybektursun",
              "text": "Exactly this. DeepInfra already hosting it is huge for accessibility. I've been running some experiments comparing hosted vs local inference costs and for bigger models the third-party hosting economics actually work out better than most people expect. Curious if GLM-5 will be quantizable enough for 4090 setups or if it's strictly datacenter territory.",
              "score": 2,
              "created_utc": "2026-02-12 00:13:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wzbha",
              "author": "LocoMod",
              "text": "Pssssssst. No one tell them OpenAI and Anthropic models are served by other providers in the largest most robust cloud platforms in the world. They will be content with running inference on jank mining rigs from shady providers for pennies on the dollar.\n\n::runs::",
              "score": 2,
              "created_utc": "2026-02-12 02:44:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ymowl",
              "author": "Moist-Length1766",
              "text": ">A situation that doesn't apply with OpenAI or Anthropic.\n\nYou're aware other providers supply OpenAI and Anthropic models right? sometimes cheaper than OpenAI/Anthropic themselves.",
              "score": 0,
              "created_utc": "2026-02-12 11:00:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x0grz",
          "author": "LocoMod",
          "text": "Anyone notice how the sentiment towards remotely hosted models over provider APIs/services is different between western and Chinese models? Anyone? Where's the individual that always reminds us this is a local sub? Does this not seem strange to anyone? That the provider themselves is GPU starved because they scaled their models in preparation to pull the rug and funnel you folks to their service?\n\n\"But I could, one day self host it...\"\n\nI could sell a kidney too. But that's not the point. Look at the comments. Folks coping left and right and all of a sudden being positive about using someone else's computer.\n\nIt's all very heartwarming.",
          "score": 5,
          "created_utc": "2026-02-12 02:51:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xydjt",
              "author": "temperature_5",
              "text": "True, though Z probably gets \\*some\\* credit for releasing lots of great local models over the past year.  I guess we'll see if we ever get another GLM Air!",
              "score": 3,
              "created_utc": "2026-02-12 07:06:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vhefp",
          "author": "larrytheevilbunnie",
          "text": "Everyone is compute starved, respect them for their work though",
          "score": 2,
          "created_utc": "2026-02-11 21:42:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vu709",
          "author": "florinandrei",
          "text": "I mean, who isn't?",
          "score": 2,
          "created_utc": "2026-02-11 22:46:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wtqgo",
          "author": "Puzzled_Fisherman_94",
          "text": "They‚Äôll get more efficient before GPU‚Äôs catch up üòÖ",
          "score": 2,
          "created_utc": "2026-02-12 02:11:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v23cp",
          "author": "Tema_Art_7777",
          "text": "Well now they can get the h200 and scale!",
          "score": 1,
          "created_utc": "2026-02-11 20:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v2em7",
              "author": "Tema_Art_7777",
              "text": "Well now they can get the h200 and scale. btw at least they had a restriction against them. anthropic has no such restrictions and they are rate limiting the **** out of api users.",
              "score": 2,
              "created_utc": "2026-02-11 20:30:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vd00l",
                  "author": "PentagonUnpadded",
                  "text": "It is sensible to assume investor money is subsidizing agents. I wonder where the equilibrium price of such services 'should' sit if they weren't priced as loss leaders.",
                  "score": 1,
                  "created_utc": "2026-02-11 21:21:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v2n7i",
          "author": "OcelotMadness",
          "text": "Oh hell ya on GLM-5. Have not seen that yet. I have a super super long text adventure going and I've spent like 20 bucks on it using sonnet 4.5 once in a while, along with my usual GLM 4.7 on the coding plan. I hope they continued working on storytelling like they said they would. Cautiously hyped.",
          "score": 1,
          "created_utc": "2026-02-11 20:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wk3jp",
              "author": "AnomalyNexus",
              "text": "Heads up storytelling tools on coding plan is likely a terms violation.\n\nI doubt it‚Äôs enforced though \n\n> Can I use my GLM Coding Plan quota in non-AI coding tools?\nA: No. The GLM Coding Plan quota is only intended to be used within coding/IDE tools designated or recognized by Z.ai",
              "score": 1,
              "created_utc": "2026-02-12 01:12:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ve3sv",
          "author": "davernow",
          "text": "I have the coder plan and have noticed some lag in the last week. Still  great service.",
          "score": 1,
          "created_utc": "2026-02-11 21:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vig4d",
          "author": "-dysangel-",
          "text": "Hmm I had weird rate limits all afternoon on normal usage, and since then GLM Coding Plan has been performing \\*very\\* poorly. The model keeps failing but stubbornly insisting that it succeeded etc. 4.7 was working very well for me so I wonder why they're so keen to change to 5 if it's starving them of resources..",
          "score": 1,
          "created_utc": "2026-02-11 21:47:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w4ut8",
          "author": "olearyboy",
          "text": "Same",
          "score": 1,
          "created_utc": "2026-02-11 23:43:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wh7ea",
          "author": "Odd-Criticism1534",
          "text": "Are all their data centers in china?",
          "score": 1,
          "created_utc": "2026-02-12 00:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkqfy",
              "author": "AnomalyNexus",
              "text": "Last I looked at the IPs it appeared to serve me from Europe but that‚Äôs not exactly bulletproof. Might be proxying it back to China",
              "score": 2,
              "created_utc": "2026-02-12 01:16:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wkyaq",
                  "author": "Odd-Criticism1534",
                  "text": "You‚Äôd think compute wouldn‚Äôt be a struggle if hosted in Chinese data centers they‚Äôre so ahead",
                  "score": 1,
                  "created_utc": "2026-02-12 01:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xpyve",
          "author": "Fresh-Soft-9303",
          "text": "Serving top models for free isn't easy, the work they're doing is awesome and much appreciated. Without open source models AI would be a lot different today.",
          "score": 1,
          "created_utc": "2026-02-12 05:51:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zbsdn",
          "author": "Buryni",
          "text": "Con ganas de probar... GLM 4.7 est√° bien (aunque tiene sus idas de cabeza...) pero lo peor es su limite de concurrencia a solo 1, eso lo hace extremadamente lento comparado con otros modelos que pueden levantar subagentes o lanzar varios comandos a la vez.",
          "score": 1,
          "created_utc": "2026-02-12 13:55:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vpqak",
          "author": "HugoCortell",
          "text": "This will ultimately be good, we need to focus on making the most out of resources, not bloating like western models do.",
          "score": 1,
          "created_utc": "2026-02-11 22:23:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uzbx2",
          "author": "arm2armreddit",
          "text": "What kind of GPUs do they use? Nice to see there are still honest and transparent companies around.",
          "score": 1,
          "created_utc": "2026-02-11 20:15:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vjefp",
          "author": "brickout",
          "text": "We all are.",
          "score": 1,
          "created_utc": "2026-02-11 21:51:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wa1u7",
          "author": "EiwazDeath",
          "text": "Makes you wonder if the industry is approaching this from the wrong angle. Everyone is fighting over the same GPU supply while 1 bit quantization lets you run inference on CPUs that are already sitting in billions of devices worldwide. The bottleneck isn't compute anymore, it's memory bandwidth, and CPUs have plenty of that. Maybe the GPU shortage is a hardware problem with a software solution.",
          "score": 1,
          "created_utc": "2026-02-12 00:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4x1txe",
          "author": "Significant-Cod-9936",
          "text": "At least they‚Äôre being honest unlike most companies‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-12 02:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wi5j4",
          "author": "Rich_Artist_8327",
          "text": "just hit it",
          "score": 0,
          "created_utc": "2026-02-12 01:00:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wu00p",
          "author": "FPham",
          "text": "And how is it? How is the GLM-5?",
          "score": 0,
          "created_utc": "2026-02-12 02:13:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yuhnn",
              "author": "harlekinrains",
              "text": "Not bad for 4 cents:\n\nhttps://pastebin.com/mHaGKqd1",
              "score": 1,
              "created_utc": "2026-02-12 12:05:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6hwj",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-02-11 20:50:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vslc3",
              "author": "fallingdowndizzyvr",
              "text": "WTF are you talking about. They released it.\n\nhttps://huggingface.co/zai-org/GLM-5-FP8",
              "score": 1,
              "created_utc": "2026-02-11 22:37:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxcm5g",
      "title": "No NVIDIA? No Problem. My 2018 \"Potato\" 8th Gen i3 hits 10 TPS on 16B MoE.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qxcm5g",
      "author": "RelativeOperation483",
      "created_utc": "2026-02-06 08:56:17",
      "score": 1112,
      "num_comments": 125,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3x3z56",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 15:50:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3viylc",
          "author": "koibKop4",
          "text": "Just logged into reddit to upvote this true localllama post! ",
          "score": 201,
          "created_utc": "2026-02-06 09:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vjzcn",
          "author": "Top_Fisherman9619",
          "text": "Posts like this are why I browse this sub. Cool stuff!",
          "score": 157,
          "created_utc": "2026-02-06 09:54:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vv2im",
              "author": "artisticMink",
              "text": "But aren't you interested in my buzzwords buzzwords buzzwords agent i vibe coded and now provide for F R E E ? ",
              "score": 51,
              "created_utc": "2026-02-06 11:32:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wamzw",
                  "author": "behohippy",
                  "text": "If you add this sub to your RSS reader, which gets a raw feed of everything posted, you'll see how bad it actually is. There's some superheros that are downvoting most of them before they even hit the front page of the sub.",
                  "score": 22,
                  "created_utc": "2026-02-06 13:18:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3y85q9",
                  "author": "Downtown-Community26",
                  "text": "lol",
                  "score": 1,
                  "created_utc": "2026-02-06 18:59:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3whjjh",
              "author": "Terrible-Detail-1364",
              "text": "yeah its very refreshing vs what model should I‚Ä¶",
              "score": 7,
              "created_utc": "2026-02-06 13:56:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vks2l",
          "author": "justserg",
          "text": "honestly love seeing these posts. feels like the gpu shortage era taught us all to optimize way better. whats your daily driver model for actual coding tasks?",
          "score": 76,
          "created_utc": "2026-02-06 10:01:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vlmk5",
              "author": "RelativeOperation483",
              "text": "Not 100% sure yet‚ÄîI'm still hunting for that perfect 'smart and fast' model to really squeeze my laptop. It‚Äôs not just the model, the engine matters just as much. For now, that **DeepSeek-Lite** running on **OpenVINO** backend is the peak daily driver.",
              "score": 25,
              "created_utc": "2026-02-06 10:09:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3vmg0d",
                  "author": "Silver-Champion-4846",
                  "text": "any tutorials for us noobs?",
                  "score": 3,
                  "created_utc": "2026-02-06 10:16:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wxkkx",
                  "author": "MythOfDarkness",
                  "text": "Are you seriously using AI to write comments??????",
                  "score": 4,
                  "created_utc": "2026-02-06 15:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vq667",
          "author": "ruibranco",
          "text": "The dual-channel RAM point can't be overstated. Memory bandwidth is the actual bottleneck for CPU inference, not compute, and going from single to dual-channel literally doubles your throughput ceiling. People overlook this constantly and blame the CPU when their 32GB single stick setup crawls. The MoE architecture choice is smart too since you're only hitting 2.4B active parameters per token, which keeps the working set small enough to stay in cache on that i3. The Chinese token drift on the iGPU is interesting, I wonder if that's a precision issue with OpenVINO's INT8/FP16 path on UHD 620 since those older iGPUs have limited compute precision. Great writeup and respect for sharing this from Burma, this is exactly the kind of accessibility content this sub needs more of.",
          "score": 32,
          "created_utc": "2026-02-06 10:50:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vritr",
              "author": "RelativeOperation483",
              "text": "I'm running GGUF because it's hard to find OpenVINO files these days, and it's nearly impossible to convert them myself with my limited RAM. I‚Äôm using the Q4\\_K\\_M quantization. I did notice some Chinese tokens appeared about five times across 20 questions , not a lot, just a little bit each time\n\nhttps://preview.redd.it/2s95fisqvuhg1.png?width=878&format=png&auto=webp&s=e5c5d7edc72019e3598e650fabe5022bceede333\n\n",
              "score": 8,
              "created_utc": "2026-02-06 11:02:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zjixq",
                  "author": "JustSayin_thatuknow",
                  "text": "That chinese/gibberish tokens I had them because of flash attention being enabled.. with fa turned off it didn‚Äôt happen with me, but as I‚Äôm stubborn af and wanted to use fa, I finally found out (after a week of thousands of trial and errors) that if I use a model with the flag ‚Äú-c 0‚Äù (which makes lcpp uses the context length from the n_ctx_training (the declared context length for which the model has been trained on)) it outputs everything perfectly well! But for this you need to make sure model is small enough, else lcpp will use the ‚Äúfit‚Äù feature to decrease context length to the default 4096 (which again returns to the gibberish/chinese/non-stop-always-on-a-loop inference state).",
                  "score": 3,
                  "created_utc": "2026-02-06 22:57:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4f5ndo",
                  "author": "Echo9Zulu-",
                  "text": "Nice post! Glad to see some benchmarks on that PR.  I have a ton of openvino models on my HF :). Would be happy to take some requests if you need something quanted.\n\nhttps://huggingface.co/Echo9Zulu",
                  "score": 1,
                  "created_utc": "2026-02-09 12:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w44w3",
          "author": "iamapizza",
          "text": "I genuinely find this more impressive then many other posts here. Running LLMs should be a commodity activity and not exclusive to a few select type of machines. It's a double bonus you did this on Linux which means a big win for privacy and control.",
          "score": 41,
          "created_utc": "2026-02-06 12:38:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6sys",
              "author": "RelativeOperation483",
              "text": "Thank Man.",
              "score": 4,
              "created_utc": "2026-02-06 12:55:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3viy7t",
          "author": "pmttyji",
          "text": "Try similar size Ling models [which gave me good t/s](https://www.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/) even for CPU only.",
          "score": 15,
          "created_utc": "2026-02-06 09:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wocj2",
              "author": "rainbyte",
              "text": "Ling-mini-2.0 üòé",
              "score": 2,
              "created_utc": "2026-02-06 14:32:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vw5e7",
          "author": "j0j0n4th4n",
          "text": "You probably can run gpt-oss-20b as well.\n\nI got about the same speeds in my setup here using the IQ4_XS quant of the bartowski's DeepSeek-Coder-V2-Lite-Instruct (haven't tried other quants yet) than I did gpt-oss-20b-Derestricted-MXFP4_MOE.",
          "score": 8,
          "created_utc": "2026-02-06 11:40:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vy6qb",
              "author": "RelativeOperation483",
              "text": "I will try it, big thank for suggestion.",
              "score": 2,
              "created_utc": "2026-02-06 11:56:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wlut0",
                  "author": "emaiksiaime",
                  "text": "I second this. I always fall back to gpt-oss-20b after trying out models, and I was able to run qwen3next 80b a3b coder on my setup. I have a i7-8700 with 64gb of ram and a ...tesla p4... it runs at 10-12 t/s prompt processing is slow.. but the 20b is great, still.",
                  "score": 2,
                  "created_utc": "2026-02-06 14:19:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wrq5d",
          "author": "Alarming_Bluebird648",
          "text": "actually wild that you're getting 10 tps on an i3. fr i love seeing people optimize older infrastructure instead of just throwing 4090s at every problem.",
          "score": 8,
          "created_utc": "2026-02-06 14:49:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wrxfw",
              "author": "RelativeOperation483",
              "text": "Thank",
              "score": 1,
              "created_utc": "2026-02-06 14:50:57",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4eexzf",
              "author": "Idea_Guyz",
              "text": "I‚Äôve had my 4090 for three years and the most I‚Äôve thrown at it is 20 chrome tabs to repose an articles and videos that I‚Äôll never watch read",
              "score": 1,
              "created_utc": "2026-02-09 08:00:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wpn5u",
          "author": "rob417",
          "text": "Very cool. Did you write this with the DeepSeek model on your potato? Reads very much like AI.",
          "score": 6,
          "created_utc": "2026-02-06 14:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wr8lp",
              "author": "RelativeOperation483",
              "text": "I thought Reddit support Md. unfortunately, my post turned out to be Ai generated copy-paste.  ",
              "score": -2,
              "created_utc": "2026-02-06 14:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wzn0g",
          "author": "stutteringp0et",
          "text": "I'm getting surprising results out of GPT-OSS:120b using a Ryzen 5 with 128GB ram.\n\n72.54 t/s\n\nI do have a Tesla P4 in the system, but during inference it only sees 2% utilization.  The model is just too big for the dinky 8GB in that GPU.\n\nI only see that performance out of GPT-OSS:120b and the 20b variant.  Every other model is way slower on that machine.  Some special sauce in that MXFP4 quantization methinks.",
          "score": 6,
          "created_utc": "2026-02-06 15:29:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xaywi",
              "author": "layer4down",
              "text": "They are also both MoE‚Äôs. I‚Äôm sure that helps üòâ actually 2025 really seems to have been the year of MoE‚Äôs I guess.",
              "score": 3,
              "created_utc": "2026-02-06 16:23:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o47ryte",
              "author": "Icy_Distribution_361",
              "text": "Could you share a bit more about your setup? And about performance of other models?",
              "score": 1,
              "created_utc": "2026-02-08 07:10:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vq44b",
          "author": "AsrielPlay52",
          "text": "Gotta tell us what set up you got, and good MoE models?",
          "score": 5,
          "created_utc": "2026-02-06 10:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vu8wg",
              "author": "RelativeOperation483",
              "text": "For the 'potato' setup, here are the specs that got me to **10 TPS** on this 2018 laptop:\n\n* **Hardware:** HP ProBook 650 G5 w/ **Intel i3-8145U** & **16GB Dual-Channel RAM**.\n* **OS:** **Ubuntu (Linux)**‚Äîdon't bother with Windows if you want every MB of RAM for the model. and I've tried Debian 13 -- but fallback to Ubuntu,\n* **The Engine:** **llama-cpp-python** with the **OpenVINO backend**. This is the only way I've found to effectively offload to the **Intel UHD 620 iGPU**.\n* **The Model:** **DeepSeek-Coder-V2-Lite-Instruct (16B MoE)**. Mixture-of-Experts is the ultimate 'cheat code' because it only activates \\~2.4B parameters per token, making it incredibly fast for its intelligence level.\n\nIf you have an Intel chip and 16GB of RAM, definitely try the OpenVINO build. It bridges the gap between 'unusable' and 'daily driver' for budget builds.\n\nBest MoE models are based on your RAM. if You have more Ram and can find the best optimization - Try Qwen 30B-A3B , it's seems like gold standard for most case. ",
              "score": 14,
              "created_utc": "2026-02-06 11:25:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wle4k",
          "author": "emaiksiaime",
          "text": "We need a gpupoor flair! I want to filter out the rich guy stuff! posts about p40 mi50, cpu inference, running on janky rigs!",
          "score": 5,
          "created_utc": "2026-02-06 14:17:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wmqhf",
              "author": "RelativeOperation483",
              "text": "I hope some guys like me revolt this era and make LLMs more efficient  on typical hardware that everyone can affords, ",
              "score": 1,
              "created_utc": "2026-02-06 14:24:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vpgf3",
          "author": "RelativeOperation483",
          "text": "I've been testing dense models ranging from 3.8B to 8B, and while they peak at 4 TPS, they aren't as fast as the 16B (A2.6B) MoE model. Here‚Äôs the catch: if you want something smarter yet lighter, go with an MoE. They‚Äôre incredibly effective, even if you‚Äôre stuck with low-end integrated graphics (iGPU) like a UHD 620, just use it.\n\nhttps://preview.redd.it/ucl1et2msuhg1.png?width=1020&format=png&auto=webp&s=0649be11efc5aeb3006674428731bf38fbf103fc\n\n",
          "score": 8,
          "created_utc": "2026-02-06 10:44:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3wdwb5",
          "author": "MelodicRecognition7",
          "text": "you can squeeze a bit more juice from the potato with some BIOS and Linux settings: https://old.reddit.com/r/LocalLLaMA/comments/1qxgnqa/running_kimik25_on_cpuonly_amd_epyc_9175f/o3w9bjw/",
          "score": 4,
          "created_utc": "2026-02-06 13:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wk0st",
          "author": "brickout",
          "text": "Nice! I just built a small cluster from old unused PCs that have been sitting in storage at my school. 7th Gen i7's with Radeon 480s. They run great. I also can't afford new GPUs. I don't mind it being a little slow since I'm basically doing this for free.",
          "score": 4,
          "created_utc": "2026-02-06 14:09:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x37a1",
              "author": "RelativeOperation483",
              "text": "That has more TPS potential than mine. ",
              "score": 1,
              "created_utc": "2026-02-06 15:46:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vyaom",
          "author": "jonjonijanagan",
          "text": "Man, this humbles me. Here I am strategizing how to justify to the wife and get a Strix Halo 128GB ram setip cause my Mac Mini M4 Pro 24GB can only run GPT OSS 20B. You rock, my guy. This is the way.",
          "score": 3,
          "created_utc": "2026-02-06 11:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w4f8d",
          "author": "Ne00n",
          "text": "Same, I got a cheap DDR4 dual channel dedi, depending on model I can get up to 11t/s.  \n8GB VRAM isn't really doing it for me either, so I just use RAM.",
          "score": 3,
          "created_utc": "2026-02-06 12:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6orv",
              "author": "RelativeOperation483",
              "text": "if you're using intel CPUs or iGPUs. try Openvino -- if you've already tried OpenVino , that's might be package missing or need optimizing. But 8GB VRAM eGPU can accelerate than any lower iGPU.",
              "score": 0,
              "created_utc": "2026-02-06 12:54:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w7165",
                  "author": "Ne00n",
                  "text": "I am talking like E3-1270 v6, old, but if OpenVino supports that, I give it a try.  \nI got like a 64GB DDR4 box for 10$/m, which I mainly use for LLM's.\n\nI only have like 8GB VRAM in my gaming rig and it also runs windows so yikes.",
                  "score": 1,
                  "created_utc": "2026-02-06 12:57:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vwwe0",
          "author": "tmvr",
          "text": "Even here the memory bandwidth is the limiting factor. That CPU supports 2133-2400MT/s RAM so dual-channel the nominal bandwidth is 34-38GB/s. That's fine for any of the MoE models, though you are limited with the 16GB size unfortunately. I have a machine with 32GB of DDR4-2666 and it does 8 tok/s with the Q6\\_K\\_XL quant of Qwen3 30B A3B. ",
          "score": 2,
          "created_utc": "2026-02-06 11:46:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vyc4a",
              "author": "RelativeOperation483",
              "text": "Ram prices are higher than I expected. I went to shop and they said 100$ equivalent of MMK for just 8GB ram stick DDR4 - 2666",
              "score": 3,
              "created_utc": "2026-02-06 11:57:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w1wv0",
                  "author": "tmvr",
                  "text": "I bought a 64GB kit (4x16) for 90eur last spring. When I checked at the end of the year after prices shot up with was 360eur for the same.",
                  "score": 2,
                  "created_utc": "2026-02-06 12:23:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w14gl",
          "author": "ANR2ME",
          "text": "I wondered how many t/s will vulkan gives ü§î then again, does such iGPU can works with vulkan backend? üòÖ",
          "score": 2,
          "created_utc": "2026-02-06 12:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w1ndy",
              "author": "RelativeOperation483",
              "text": "Technically, yes, the UHD 620 supports **Vulkan**, so you *can* run the backend. But from my testing on this exact i3 'potato,' you really shouldn't. **Vulkan on iGPU is actually slower than the CPU.** ",
              "score": 6,
              "created_utc": "2026-02-06 12:21:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w9vml",
          "author": "danigoncalves",
          "text": "Sorry if I missed but which backend did you used? and you tweak with parameters to achieve such performance?",
          "score": 2,
          "created_utc": "2026-02-06 13:14:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wcpv8",
              "author": "RelativeOperation483",
              "text": "I use llama-cpp-python with the **OpenVINO backend**   \nn\\_gpu\\_layers**=-1** and **device=\"GPU\"**\n\n**Without OpenVino backend. It will not work.** ",
              "score": 3,
              "created_utc": "2026-02-06 13:30:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xezug",
          "author": "LostHisDog",
          "text": "So I tested this recently on a 10th gen i7 with 32gb of ram just using llama.cpp w/ gpt-oss-20b and the performance was fine... until I tried feeding it any sort of context. My use case is book editing but it's not too unlike code review... the less you can put into context the less useful the LLM is. For me, without a GPU, I just couldn't interact with a reasonable amount of context at usable (for me) t/s. \n\nI might have to try something other than llama.cpp and I'm sure there was performance left on the table even with that but it wasn't even close to something I would use for 10's or thousands of tokens of context when I tried it.",
          "score": 2,
          "created_utc": "2026-02-06 16:41:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xuwx0",
              "author": "ossm1db",
              "text": "What you need is a Hybrid Mamba‚Äë2 MoE model like Nemotron-3 Nano: 30B total parameters, ~3.5B active per token, ~25 GB RAM usage. The key is that for these models, long context does not scale memory the same way as a pure Transformer. The safe max context for 32GB is about 64k tokens (not bad) out of the 1M (150GB-250GB RAM) the model supports according to Copilot.",
              "score": 2,
              "created_utc": "2026-02-06 17:57:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xsg89",
              "author": "andreasntr",
              "text": "This.\n\nAs much as I love posts like this one, this kind of \"reality checks\" never emerge unfortunately. Even loading 1000 tokens with these constraints will kill the usability. If one runs batch jobs, however, it should be ok but i highly doubt it",
              "score": 1,
              "created_utc": "2026-02-06 17:45:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o42dxjv",
          "author": "im_fukin_op",
          "text": "How do you learn to do this? Where to find the literature? This is the first time I hear of OpenVINO and it seems like exactly the thing I should have been using but I never found out.",
          "score": 2,
          "created_utc": "2026-02-07 11:47:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42mp19",
              "author": "RelativeOperation483",
              "text": "Just browsing, I thought if there's MXL for Mac, why not something special about Intel and Found OpenVINO. I tried to use it plain. It's good unless you need extras. So, I tried with llama-cpp-python with OpenVINO backend.\n\n",
              "score": 2,
              "created_utc": "2026-02-07 12:57:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vxowl",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-06 11:52:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vyhuf",
              "author": "RelativeOperation483",
              "text": "https://preview.redd.it/ws94hwcx5vhg1.png?width=1085&format=png&auto=webp&s=906534bff938adf2d109cf3dd7f38286d2a76157\n\nit's 18.6GB -- I'm thinking about it, I will try it later after my school days.",
              "score": 1,
              "created_utc": "2026-02-06 11:58:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w3mju",
          "author": "street_melody",
          "text": "you're a rockstar",
          "score": 1,
          "created_utc": "2026-02-06 12:35:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wctk2",
              "author": "RelativeOperation483",
              "text": "Thank",
              "score": 1,
              "created_utc": "2026-02-06 13:30:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3whjy1",
          "author": "jacek2023",
          "text": "great work, thanks for sharing!",
          "score": 1,
          "created_utc": "2026-02-06 13:56:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wij15",
              "author": "RelativeOperation483",
              "text": "Thank Man",
              "score": 1,
              "created_utc": "2026-02-06 14:01:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wq0s1",
          "author": "gambiter",
          "text": "> I‚Äôm writing this from Burma.\n\nNei kaun la :)\n\n> MoE is the \"Cheat Code\": 16B parameters sounds huge, but it only calculates 2.4B per token. It‚Äôs faster and smarter than 3B-4B dense models.\n\nWait, seriously? TIL. I have a project I've been struggling with, and this just may be the answer to it!\n\nThis is very cool. Great job!",
          "score": 1,
          "created_utc": "2026-02-06 14:41:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wqvrh",
              "author": "RelativeOperation483",
              "text": "I guess you're asking \"How are you\" or \"Are you good\". instead of Nei Kaun La, just use \"Nay Kaung Lar\". By the way I'm glad if my post is helpful for somebody. ",
              "score": 1,
              "created_utc": "2026-02-06 14:45:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wsqlp",
                  "author": "gambiter",
                  "text": "Haha, it's been about a decade since I was trying to learn the language. I was just excited to see someone from there, and wanted to try to say hello properly!",
                  "score": 1,
                  "created_utc": "2026-02-06 14:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wqpif",
          "author": "roguefunction",
          "text": "Hell yea!",
          "score": 1,
          "created_utc": "2026-02-06 14:44:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x4p5t",
          "author": "Michaeli_Starky",
          "text": "10 TPS with how many input tokens? What are you going to do with that practically?",
          "score": 1,
          "created_utc": "2026-02-06 15:53:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x8d23",
          "author": "layer4down",
          "text": "Very nice! A gentleman recently distilled GLM-4.7 onto an LFM2.5-1.2B model. Curious to know how something like that might perform for you?\n\nhttps://www.linkedin.com/posts/moyasser_ai-machinelearning-largelanguagemodels-activity-7423664844626608128-b2OO\n\n\nhttps://huggingface.co/yasserrmd/GLM4.7-Distill-LFM2.5-1.2B",
          "score": 1,
          "created_utc": "2026-02-06 16:10:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xtn29",
          "author": "Neither-Bite",
          "text": "üëèüëèüëèüëè",
          "score": 1,
          "created_utc": "2026-02-06 17:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xu3l3",
          "author": "Neither-Bite",
          "text": "Can you make a video explaining your setup?",
          "score": 1,
          "created_utc": "2026-02-06 17:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xvlyv",
          "author": "IrisColt",
          "text": "I kneel, as usually",
          "score": 1,
          "created_utc": "2026-02-06 18:00:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xz119",
          "author": "Jayden_Ha",
          "text": "I would better touching grass that suffering from this speed",
          "score": 1,
          "created_utc": "2026-02-06 18:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y00d7",
          "author": "Lesser-than",
          "text": "the man who would not accept no for an answer.",
          "score": 1,
          "created_utc": "2026-02-06 18:21:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y4tt7",
          "author": "hobcatz14",
          "text": "This is really impressive.  I‚Äôm curious about the list of MoE models you tested and how they fared in your opinion‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-06 18:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yrkvh",
          "author": "BrianJThomas",
          "text": "I ran full Kimi K2.5 on an n97 mini pc with a single channel 16GB of RAM. I got 22 seconds per token!",
          "score": 1,
          "created_utc": "2026-02-06 20:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yylq2",
          "author": "msgs",
          "text": "Now you have me curious to see if my Lunar Lake laptop with 16GB of ram with a built in (toy level) NPU would do.",
          "score": 1,
          "created_utc": "2026-02-06 21:10:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40aups",
          "author": "itsnotKelsey",
          "text": "lol love it",
          "score": 1,
          "created_utc": "2026-02-07 01:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41vnd3",
          "author": "Born_Owl7750",
          "text": "Amazing",
          "score": 1,
          "created_utc": "2026-02-07 08:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41zk6c",
          "author": "theGamer2K",
          "text": "OpenVINO is underrated. They are doing some impressive work.",
          "score": 1,
          "created_utc": "2026-02-07 09:29:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42k96h",
          "author": "RelativeOperation483",
          "text": "[https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite\\_vs\\_gptoss20b\\_on\\_my\\_2018\\_potato/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)\n\nDeepseekV2Lite Vs Gpt-OSS-20B Comparison on the same hardware.",
          "score": 1,
          "created_utc": "2026-02-07 12:39:18",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o42rw9h",
          "author": "therauch1",
          "text": "I was very intrigued and just went down the rabbit hole and I just need to know: did you use AI for all of this and did it hallucinate everything?\n\nHere my findings:\n\n\\* There is no CMAKE variable for \\`DGGML\\_OPENVINO\\` in llama-cpp-python ([https://raw.githubusercontent.com/abetlen/llama-cpp-python/refs/heads/main/Makefile](https://raw.githubusercontent.com/abetlen/llama-cpp-python/refs/heads/main/Makefile))\n\n\\* No \\`DGGML\\_OPENVINO\\` in llama.cpp ([https://github.com/search?q=repo%3Aggml-org%2Fllama.cpp%20DGGML\\_OPENVINO&type=code](https://github.com/search?q=repo%3Aggml-org%2Fllama.cpp%20DGGML_OPENVINO&type=code)).\n\n\\* There is one in a seperate (unmerged branch) which maybe will use that variable for building ([https://github.com/ggml-org/llama.cpp/pull/15307/changes](https://github.com/ggml-org/llama.cpp/pull/15307/changes))\n\n\\* Your benchmark script ([https://www.reddit.com/r/LocalLLaMA/comments/1qxcm5g/comment/o3vn0fn/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1qxcm5g/comment/o3vn0fn/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)) does not actually do something: ([https://raw.githubusercontent.com/esterzollar/benchmark-on-potato/refs/heads/main/deep.py](https://raw.githubusercontent.com/esterzollar/benchmark-on-potato/refs/heads/main/deep.py)) the variable \\`device\\_label\\` is not used. SO YOUR BENCHMARK IS NOT WORKING!?",
          "score": 1,
          "created_utc": "2026-02-07 13:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42uzd4",
              "author": "RelativeOperation483",
              "text": "check deep\\_decode.py in the same folder -- \n\nDeepSeek-Coder-V2-Lite-Instruct-Q4\\_K\\_M\\_result.txt\n\nis the output of [deep.py](http://deep.py)\n\ntest2output.txt is the output of deep\\_decode.py.\n\nhttps://preview.redd.it/134r03oku2ig1.png?width=862&format=png&auto=webp&s=5511f2d74de9760f0946474e0e81db639f9a5ad1\n\n",
              "score": 1,
              "created_utc": "2026-02-07 13:49:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42wkxr",
                  "author": "therauch1",
                  "text": "Okay I see that it should in theory load a single layer onto a gpu if available. What happens if you offload everything? So setting that value to \\`-1\\`?",
                  "score": 1,
                  "created_utc": "2026-02-07 13:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o42xohp",
          "author": "Neither_Sort_2479",
          "text": "Guys, I'm relatively new to local LLM and this may be a stupid question, but can you tell me what is the best model right now to run locally for coding tasks as an agent with rtx4060ri 8GB (32gb ram) and what settings (lm studio)? Because I haven't been able to use anything so far (I tried qwen3 8b, 14b, deepseek r1, qwen2.5 coder instruct, codellama 7b instruct, and several others), none of those that I tested can work as an agent with cline or roo code, there is not enough context even for something simple. Or maybe there is some kind of hint about the workflow for such limited local models that I need to know",
          "score": 1,
          "created_utc": "2026-02-07 14:05:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o442tla",
          "author": "Forsaken-Truth-697",
          "text": "No latest GPUs? No problem.\n\nI can use cloud service or remotely connect it to my laptop, and run the best GPUs on the market.",
          "score": 1,
          "created_utc": "2026-02-07 17:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45k9z1",
          "author": "sebuzdugan",
          "text": "nice result\n\ncurious what quant and cache layout you‚Äôre using on openvino\n\nalso did you test smaller ctx like 2k to see if igpu scales better than cpu there",
          "score": 1,
          "created_utc": "2026-02-07 22:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o486a69",
          "author": "Qxz3",
          "text": "\"## The Reality Check\"",
          "score": 1,
          "created_utc": "2026-02-08 09:24:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49t4rp",
          "author": "-InformalBanana-",
          "text": "What did you optimize here exactly? You installed 2 programs - openvino and llama.cpp and thats it?\nAlso what is the t/s for prompt processing speed?",
          "score": 1,
          "created_utc": "2026-02-08 16:15:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4f7s5j",
          "author": "SoobjaCat",
          "text": "This is soo cool and impressive",
          "score": 1,
          "created_utc": "2026-02-09 12:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hmd1a",
          "author": "hobbywine-2148",
          "text": "Bonjour,  \nEst-ce que vous auriez un tutoriel pour expliquer comment vous faites ?  \nJ'ai un processeur ultra 9 285h avec arc 140t je ne trouve pas de tutoriel pour installer ollama et openwebui  \nsur ubntu 24.04 pour le gpu arc140t qui ont l'air tr√®s bien comme indiqu√© dans le blog :https://www.robwillis.info/2025/05/ultimate-local-ai-setup-guide-ubuntu-ollama-open-webui/\n\nEn attendant, j'ai clon√© le projet :  \n[https://github.com/balaragavan2007/Mistral\\_on\\_Intel\\_NPU](https://github.com/balaragavan2007/Mistral_on_Intel_NPU)   \net apr√®s avoir install√© ce qui est recommand√© au lien intel :  \n[https://dgpu-docs.intel.com/driver/client/overview.html](https://dgpu-docs.intel.com/driver/client/overview.html)  \nj'arrive √† faire fonctionner ce mod√®le mistral √† environ 15-17 token/s sur le GPU arc140t.  \nmais √ßa n'est qu'avec ce mod√®le l√†, celui du projet Mistral\\_on\\_Intel\\_NPU  \nP.S. je n'ai pas r√©ussi √† faire reconnaitre le NPU mais comme apparemment le GPU arc 140t   \nc'est l√† o√π c'est le plus puissant √ßa n'est pas g√™nant.  \nDu coup j'aimerai arriver √† installer ollama + openweb ui pour pouvoir chopper les mod√®les  \nqui s'am√©liorent avec le temps.  \nD√©j√† dans windows 11 dans une VM ubuntu 24.04 j'ai install√© LM studio qui fonctionne pas mal du tout avec le mod√®le ministral 3 (moins vite (VM) mais mieux que le projet Mistral\\_on\\_Intel\\_NPU dual-boot ubuntu 24.04).  \nDonc auriez vous un tutoriel quelque part ?  \n",
          "score": 1,
          "created_utc": "2026-02-09 19:53:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kp4w4",
          "author": "Emotional-Debate3310",
          "text": "I appreciate your hard work, but also like to indicate there might be some easier way to achieve the same level of efficiency and performance.\n\n# Have you tried MatFormer architecture (Nested Transformer)?\n\nFor example Gemma3N 27B LiteRT model or similar \n\n- *Architecture:* It utilizes the MatFormer architecture (Nested Transformer). It physically has ~27B parameters but utilizes a \"dynamic slice\" of roughly 4B \"Effective\"  parameters during inference.\n\n\n - *Why it feels fast:* Unlike traditional quantization (which just shrinks weights), MatFormer natively skips blocks of computation. When running on LiteRT (Google's optimized runtime), it leverages the NPU / GPU / CPU based on availability resulting in near-zero thermal throttling.\n\nAll the best.",
          "score": 1,
          "created_utc": "2026-02-10 06:49:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lrqip",
          "author": "happycube",
          "text": "\\[the \"GPU\" is just having his coffee.\\]\n\nIf that was an 8th gen desktop, it'd have a whole Coffee Lake to drink from (with 2 more cores, too).  Instead it's got Whiskey Lake.\n\nSeriously quite impressive!",
          "score": 1,
          "created_utc": "2026-02-10 12:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mkqcw",
          "author": "TheBoxCat",
          "text": "Where exactly are the instructions to reproduce this?\n\nI've turned this into an easier-to-follow guide and posted it here: https://rentry.org/16gb-local-llm.  \nDisclaimer: I've used ChatGPT 5.2 to generate the markdown and then tested it manually, confirming that it works (Good enough)",
          "score": 1,
          "created_utc": "2026-02-10 15:17:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rfoj3",
          "author": "Ok_Break_7193",
          "text": "This sounds so interesting and something I would like to dig into deeper. I am just at the start of my learning journey. I hope you do provide a tutorial of what you did at some point for the rest of us to follow!",
          "score": 1,
          "created_utc": "2026-02-11 07:25:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rmihd",
              "author": "TheBoxCat",
              "text": "Posted some instructions here, give it a try and tell me if it worked for you: [https://rentry.org/16gb-local-llm](https://rentry.org/16gb-local-llm)",
              "score": 2,
              "created_utc": "2026-02-11 08:29:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4w5fw2",
          "author": "Temujin_123",
          "text": "Seriously. I'm not interested in dropping thousands of dollars on overly-priced, power-hungry GPUs. I don't need TPUs faster than I can read. And I'm okay with being a generation behind - esp. with how fast the innovation is in this space.\n\nI just grab whatever 6-18B model is latest flavor I want, and run on the GPU + RAM that came with my laptop (RTX 3050). Good enough.",
          "score": 1,
          "created_utc": "2026-02-11 23:46:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wu94w",
          "author": "s1mplyme",
          "text": "This is epic.",
          "score": 1,
          "created_utc": "2026-02-12 02:14:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vel3r",
          "author": "RelativeOperation483",
          "text": "PS: it's Q4KM GGUF version -- if you dare Go With Q5KM\n\n\\# Known Weaknesses\n\niGPU Wake-up Call: The iGPU takes significantly longer to compile the first time (Shader compilation). It might look like it's stuck‚Äîdon't panic. It's just the \"GPU\" having his coffee before he starts teaching.\n\nLanguage Drift: On the iGPU, DeepSeek occasionally hallucinates Chinese characters (it‚Äôs a Chinese-base model). The logic remains 100% solid, but it might forget it's speaking English for a second.\n\nReading Speed: While not as fast as a $40/mo cloud subscription, 10 t/s is faster than the average human can read (5-6 t/s). Why pay for speed you can't even use?",
          "score": 3,
          "created_utc": "2026-02-06 09:02:00",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3xikvp",
              "author": "Not_FinancialAdvice",
              "text": "I get language drift on most of the Chinese models I've tried.",
              "score": 1,
              "created_utc": "2026-02-06 16:58:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xitw0",
          "author": "Fine_Purpose6870",
          "text": "That's the power of linux. Windows can shuckabrick. Not to mention Windows was giving peoples encryption keys over to the FBI pfft. That's absolutely sick.  I bet you could get an old pentium to run a 3b LLM on linux lol.",
          "score": 1,
          "created_utc": "2026-02-06 16:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xnjzj",
          "author": "x8code",
          "text": "Meh, I'll keep my RTX 5080 / 5070 Ti setup, thanks.",
          "score": 0,
          "created_utc": "2026-02-06 17:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46qvm5",
              "author": "rog-uk",
              "text": "What a useful contribution üôÑ",
              "score": 2,
              "created_utc": "2026-02-08 02:32:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vjq8n",
          "author": "xrvz",
          "text": "No high-end Macbook is necessary ‚Äì the 600$ base Mac mini has 12GB VRAM at 120 GB/s bandwidth (150 GB/s with the coming M5).\n\nIt'd run the mentioned model (deepseek-coder-v2:16b-lite-instruct-q4_0) at about 50 t/s at low context.",
          "score": -3,
          "created_utc": "2026-02-06 09:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jr6tm",
          "author": "ceeeej1141",
          "text": "Great! I don't have a \"4090/5090\" either but, no thanks I won't let my AI Chatbot uses every drop of performance lol. I prefer to multitask, that's why I have a dual-monitor setup.",
          "score": 0,
          "created_utc": "2026-02-10 02:46:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0zn8o",
      "title": "Hugging Face Is Teasing Something Anthropic Related",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/wvu2vi2jwnig1.png",
      "author": "Few_Painter_5588",
      "created_utc": "2026-02-10 12:39:52",
      "score": 973,
      "num_comments": 222,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4n3i0d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 16:45:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lt87q",
          "author": "Leflakk",
          "text": "I would not expect too much from them lol",
          "score": 587,
          "created_utc": "2026-02-10 12:44:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mgiyu",
              "author": "ApprehensiveAd3629",
              "text": "i guess we will get GTA 6 before an Anthropic open model",
              "score": 179,
              "created_utc": "2026-02-10 14:55:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mj7py",
                  "author": "vikarti_anatra",
                  "text": "So soon? I thought time of Half-Life 3 release is approriate \n\n",
                  "score": 36,
                  "created_utc": "2026-02-10 15:09:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n6yg4",
                  "author": "arcanemachined",
                  "text": "It's for your safety, citizen. - Anthropic\n\nSeriously though, I never actually thought OpenAI would release an open-weight model, but they did eventually do it. So there is some hope.",
                  "score": 9,
                  "created_utc": "2026-02-10 17:01:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4n4sb8",
                  "author": "No_Afternoon_4260",
                  "text": "I wouldn't bet on that, they need to get rid of kimi\n\nOpus 4.6 is wild but k2.5 is dirt cheap and not that far away",
                  "score": 2,
                  "created_utc": "2026-02-10 16:51:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nkydh",
                  "author": "cloverasx",
                  "text": "so no sooner than 2028? üòÇ",
                  "score": 1,
                  "created_utc": "2026-02-10 18:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mcg7a",
              "author": "MyHobbyIsMagnets",
              "text": "Truly the most annoying AI company. I was a huge fan, but it‚Äôs so nice to see OpenAI and open source catching up for coding. Anthropic deserves to crash and burn.",
              "score": 44,
              "created_utc": "2026-02-10 14:34:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mhcp8",
                  "author": "No_Swimming6548",
                  "text": "B.. but the safety!",
                  "score": 25,
                  "created_utc": "2026-02-10 15:00:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nk018",
                  "author": "Able-Swing-6415",
                  "text": "What do people dislike about it? I stopped using it for limits other than that it's the best AI model on the market for me.",
                  "score": 5,
                  "created_utc": "2026-02-10 18:01:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mnvwx",
                  "author": "Quiet_Figure_4483",
                  "text": "Which open source model do you recommend for coding?",
                  "score": 3,
                  "created_utc": "2026-02-10 15:32:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4o2lh4",
                  "author": "toothpastespiders",
                  "text": ">Truly the most annoying AI company.\n\nThey're among the best at social media marketing which makes them even more annoying.",
                  "score": 3,
                  "created_utc": "2026-02-10 19:26:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v03ya",
                  "author": "AddressForward",
                  "text": "I‚Äôve been swinging away from the slowly - from fanboy to curious about the alternatives. Not being able to use my subscription on opencode was annoying (Claude code written in React was a bizarre move).. but most annoying is the smugness and self-righteousness of Amodei. \n\nActually all the AI heads annoy me for different reasons. Maybe it‚Äôs a me thing.",
                  "score": 1,
                  "created_utc": "2026-02-11 20:18:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ncd72",
                  "author": "robbievega",
                  "text": "why exactly? you prefer OpenAI ads? or Sam Altman's $1 donation million to Trump‚Äôs inaugural fund?",
                  "score": -2,
                  "created_utc": "2026-02-10 17:26:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4n3fnq",
              "author": "TRKlausss",
              "text": "Maybe they release old models, who knows‚Ä¶",
              "score": 2,
              "created_utc": "2026-02-10 16:44:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mbse6",
              "author": "Full-Teach3631",
              "text": "Lol me neither",
              "score": -1,
              "created_utc": "2026-02-10 14:31:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltbc6",
          "author": "Technical-Earth-3254",
          "text": "I agree, these guys would never ever release a real oss model.",
          "score": 306,
          "created_utc": "2026-02-10 12:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lyfvr",
              "author": "-p-e-w-",
              "text": "They need VC money and mindshare, just like everyone else. When their investors keep asking why they don‚Äôt have open-weights releases while all of their competitors do, they can‚Äôt just shrug and move on without cost. He who pays the piper calls the tune.",
              "score": 38,
              "created_utc": "2026-02-10 13:17:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lzf8w",
                  "author": "Howdareme9",
                  "text": "Yes they can lmao. Very naive to think investors will care about open source models..",
                  "score": 137,
                  "created_utc": "2026-02-10 13:23:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m5b9n",
                  "author": "[deleted]",
                  "text": "What's the odds they've already put a model out to test the water...¬†",
                  "score": 1,
                  "created_utc": "2026-02-10 13:56:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qji5y",
                  "author": "ireccomendit",
                  "text": "That‚Äôs when they release a new skill ü§£",
                  "score": 1,
                  "created_utc": "2026-02-11 03:18:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mq68l",
              "author": "CuriouslyCultured",
              "text": "They do have an incentive to create an onramp for their ecosystem as a competitor with the small Chinese models. The problem is they're scared of releasing dangerous models openly and the capability front of open models is in \"dangerous\" territory, so they'd want to spend an inordinate amount of time aligning it, which they might not have.",
              "score": 1,
              "created_utc": "2026-02-10 15:43:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4nmspy",
                  "author": "crantob",
                  "text": "Dangerous only to censors.\n\nThe real danger is governments currently using AI to kill people.\n\nCurrently.  With flying robots.\n\nLet that sink in.",
                  "score": 3,
                  "created_utc": "2026-02-10 18:14:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvpq0",
          "author": "Ok-Pipe-5151",
          "text": "At best, some \"safety\" dataset might be coming. I don't expect anything more than that from anthropic¬†",
          "score": 123,
          "created_utc": "2026-02-10 13:00:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4maue6",
              "author": "Thick-Protection-458",
              "text": "On the other hand - did we expected something from AlmostClosedAI before oss?",
              "score": 29,
              "created_utc": "2026-02-10 14:26:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mln9c",
                  "author": "Ok-Pipe-5151",
                  "text": "They've not been beating drum about \"dangers of open models\" like Anthropic. This is the difference.",
                  "score": 33,
                  "created_utc": "2026-02-10 15:21:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mq0pf",
                  "author": "EstarriolOfTheEast",
                  "text": "At least they'd released whisper and even iterated on it with several released improved versions into recent times, so it wasn't completely unexpected. llama.cpp evolved from whisper.cpp iirc, so they even played an important indirect role on the current scene (discounting the ancient gpt2 history, which was also the architectural foundation for llama and motivated the genesis of huggingface). \n\nThey also released CLIP (highly influential to generative AI art) and jukebox, so even if they later got the deserved name of closed-ai, they'd still, unlike Anthropic, made several core pivotal contributions to open AI.",
                  "score": 19,
                  "created_utc": "2026-02-10 15:42:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mzkuh",
              "author": "sine120",
              "text": "Trying to get some safety PR since their partnership with Palantir is making people more and more nervous.",
              "score": 13,
              "created_utc": "2026-02-10 16:27:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4m46uf",
              "author": "Mescallan",
              "text": "I could see an RL environment frame work or something for training sparse auto encoders",
              "score": 1,
              "created_utc": "2026-02-10 13:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4o8ax7",
              "author": "throwaway2676",
              "text": "Yeah, if there's a prediction market, I'm betting on a safety dataset or safety benchmark.  *Maybe* some kind of explainability tool",
              "score": 1,
              "created_utc": "2026-02-10 19:52:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m607i",
          "author": "MagicZhang",
          "text": "they're gonna release a 1,500 page safety document on why open-source is bad for the community",
          "score": 124,
          "created_utc": "2026-02-10 13:59:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p4hda",
              "author": "Super_Sierra",
              "text": "before you even run Claude, it uses nearly 65k tokens just on the pre-token safety shit. \n\nMight be more than that. ",
              "score": 18,
              "created_utc": "2026-02-10 22:23:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lur4d",
          "author": "constanzabestest",
          "text": "Let's be honest here, if Antropic actually dropped open weights then i would be fully convinced that either 1. We live in some sort of bizzaro world or 2. The world is ending as we speak.",
          "score": 155,
          "created_utc": "2026-02-10 12:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lwdpq",
              "author": "Icetato",
              "text": "Or 3. It's ass",
              "score": 107,
              "created_utc": "2026-02-10 13:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lz4cd",
                  "author": "XiRw",
                  "text": "Most likely this so they can just say they released something. Unless their ego is too big they don‚Äôt want to look like shit even in the local llm world",
                  "score": 20,
                  "created_utc": "2026-02-10 13:21:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mbfh1",
                  "author": "MelodicRecognition7",
                  "text": "> ass\n\nGPT-ASS?",
                  "score": 11,
                  "created_utc": "2026-02-10 14:29:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m39hf",
                  "author": "TheGABB",
                  "text": "*and",
                  "score": 4,
                  "created_utc": "2026-02-10 13:45:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nppog",
                  "author": "Thomas-Lore",
                  "text": "They could at least release some of their legacy models. Claude 2 would be nice.",
                  "score": 2,
                  "created_utc": "2026-02-10 18:27:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nt6v6",
                  "author": "yeah-ok",
                  "text": "Or 4, it's some sort of wild meta rug pull that indicates the CCP have integrated Anthropic and are force releasing everything openly.",
                  "score": 1,
                  "created_utc": "2026-02-10 18:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4miikf",
              "author": "ab2377",
              "text": "or claude opus has convinced dario amodei to release an open weight 30b all made by opus itself.",
              "score": 19,
              "created_utc": "2026-02-10 15:06:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mmg06",
                  "author": "Traditional-Gap-3313",
                  "text": "I'd put money down that it's this\n\n> look at what my kid did all by itself",
                  "score": 17,
                  "created_utc": "2026-02-10 15:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4m87fj",
              "author": "TheRealMasonMac",
              "text": "I would love something that is the equivalent of even Haiku 3.5‚Ä¶ it‚Äôs such a solid model. Anthropic‚Äôs instruction following is simply and utterly unmatched (though open weights are getting there).",
              "score": 5,
              "created_utc": "2026-02-10 14:11:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4n1egs",
              "author": "Both-Employment-5113",
              "text": "we live in some sort of bizzato world bruh, how else would u explain all the ongoings the last 200 years lmao",
              "score": 0,
              "created_utc": "2026-02-10 16:35:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltsfg",
          "author": "DeltaSqueezer",
          "text": "Maybe they will open source their recent adverts :P",
          "score": 51,
          "created_utc": "2026-02-10 12:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sg9li",
              "author": "Twistpunch",
              "text": "How to fine tune your models to include their ads.",
              "score": 1,
              "created_utc": "2026-02-11 12:43:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lz9qz",
              "author": "XiRw",
              "text": "Got an email from OpenAI they will be doing ads too unfortunately. Was only a matter of time.",
              "score": -1,
              "created_utc": "2026-02-10 13:22:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mko84",
                  "author": "AdIllustrious436",
                  "text": "\"Too\" ? They are litteraly alone in that boat for now. We can only hope it stays like that...",
                  "score": 12,
                  "created_utc": "2026-02-10 15:16:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4qk47f",
                  "author": "Feztopia",
                  "text": "Bro you must be living under a rock, Anthropic ads are ads about OpenAI adding ads to chatgpt.",
                  "score": 1,
                  "created_utc": "2026-02-11 03:22:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvre8",
          "author": "Pvt_Twinkietoes",
          "text": "lol. Them and open source don't go together.",
          "score": 21,
          "created_utc": "2026-02-10 13:00:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lx68c",
          "author": "drooolingidiot",
          "text": "Probably something interpretability related. I wouldn't expect a model usable for end-users. They've been actively hostile to open source.",
          "score": 18,
          "created_utc": "2026-02-10 13:09:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mbys9",
          "author": "publicbsd",
          "text": "expect 50 gig SafetyDogshit . md ",
          "score": 17,
          "created_utc": "2026-02-10 14:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m58ti",
          "author": "Prof_ChaosGeography",
          "text": "Interesting.... Anthropic has been the most pro regulatory capture, a bit more evil the Open AI. The reason they are slightly better with models is they hired the Google books guy and bought s ton of out of print books to scan for the tokens in a destructive manner for speed.¬†\n\n\n\n\nMy bet is we are all getting excited for a dataset that will be very safety aligned and absolutely neuter models that use it.\n\n\nBut part of me thinks they are doing something different to battle openai and the Chinese labs and force them onto their backfoot. They have been taking a different approach recently so I'll hope it's an open model that can compete with gpt-oss-120b but I doubt it. I don't think they will release any code focused model that's their bread and butter",
          "score": 13,
          "created_utc": "2026-02-10 13:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mg8bj",
          "author": "DrNavigat",
          "text": "They're going to release a 5-word phrase, completely open source, MIT. You'll be able to say the phrase whenever and as many times as you want.",
          "score": 11,
          "created_utc": "2026-02-10 14:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m3uxw",
          "author": "AdamEgrate",
          "text": "Don‚Äôt read too much into the enterprise account thing.  At the valuation they‚Äôre demanding paying for that is peanuts. They probably just did because it enhances visibility.",
          "score": 5,
          "created_utc": "2026-02-10 13:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m8m8x",
          "author": "Ylsid",
          "text": "inb4 safety classification dataset",
          "score": 8,
          "created_utc": "2026-02-10 14:14:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lus4h",
          "author": "vrmorgue",
          "text": "Maybe! But no hope",
          "score": 4,
          "created_utc": "2026-02-10 12:54:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n236k",
          "author": "the__storm",
          "text": "My prediction: they release one or two small models that are limited in some way (e.g. no multimodal).  They have inference bugs or are otherwise panned on release.  Six months later everyone has a change of heart and finds that they're really useful for certain tasks, and hold up better out of distribution and on long context than other open weights models, even if they're somewhat safety-maxxed and don't score as well on benchmarks.  Thank you for coming to my TED talk.",
          "score": 3,
          "created_utc": "2026-02-10 16:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4op5q0",
              "author": "jkflying",
              "text": "Sort of like GPT-OSS 30/120 are now...",
              "score": 1,
              "created_utc": "2026-02-10 21:11:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ltxwo",
          "author": "Few_Painter_5588",
          "text": "Personally I hope it's a coding/reasoning benchmark. The current benchmarks we have are too saturated now.",
          "score": 9,
          "created_utc": "2026-02-10 12:49:17",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4n4qwj",
              "author": "Emotional_Egg_251",
              "text": "If it is, I'm sure it'll be completely unbiased...",
              "score": 7,
              "created_utc": "2026-02-10 16:51:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lvi87",
          "author": "Middle_Bullfrog_6173",
          "text": "Yeah something alignment related seems possible. That's where they have been most open. Of course they could simply be paying to get more private storage and bandwidth for using other people's datasets.",
          "score": 7,
          "created_utc": "2026-02-10 12:59:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mau1i",
          "author": "alerikaisattera",
          "text": "One thing I can see them releasing is a pseudo-open available proprietary model. They hate open-source with passion and want to destroy it. They tried to do it with fearmongering, but it didn't work. Now they may resort to releasing proprietary AI and misrepresenting it as open-source, a tactic that has worked quite a few times in the past",
          "score": 7,
          "created_utc": "2026-02-10 14:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lxmjk",
          "author": "Free-Internet1981",
          "text": "I bet it will be underwhelming",
          "score": 5,
          "created_utc": "2026-02-10 13:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lxn7v",
          "author": "YogurtExternal7923",
          "text": "OMG THEY'LL PROBABLY RELEASE SONNET 5 AS OPEN SOURCE!!!!!\n\nJokes aside this might be a pleasant surprise, but we already got an open weights claude model since kimi, deepseek and glm all use claude outputs as training data",
          "score": 6,
          "created_utc": "2026-02-10 13:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p4ru0",
              "author": "Super_Sierra",
              "text": "Claude 2 still mogs most of open source in creative writing tasks, i'd take Claude 2. ",
              "score": 1,
              "created_utc": "2026-02-10 22:25:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pfezb",
                  "author": "YogurtExternal7923",
                  "text": "I was gonna say \"no way man\" but you know what? I actually remember when claude 2 was on their free experimental API days and.. PHEW! that thing wasn't technically smart but it was GOOD!",
                  "score": 2,
                  "created_utc": "2026-02-10 23:22:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lvnl9",
          "author": "Such_Advantage_6949",
          "text": "They are worse than openai",
          "score": 13,
          "created_utc": "2026-02-10 13:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3lh3",
              "author": "Freonr2",
              "text": "At least I believe what Dario says, and Anthropic wears their bias on their sleeve. \n\nI don't trust a word that comes out of Sam's mouth.",
              "score": 5,
              "created_utc": "2026-02-10 16:45:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lwlk6",
              "author": "MrHanoixan",
              "text": "Can you explain what you mean? It seems like the general perception is that OpenAI has been a shadier business. In what ways do you think Anthropic is worse? No dog in this fight, just curious.",
              "score": 1,
              "created_utc": "2026-02-10 13:06:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lwy0d",
                  "author": "fizzy1242",
                  "text": "they've been openly against open weights llms",
                  "score": 21,
                  "created_utc": "2026-02-10 13:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lz66l",
                  "author": "Orolol",
                  "text": "I like very much Claude models, but Anthropic is very vocal against open models, calls for heavy regulation against anything that could threatens their business model, never released anything open, call for tech war against China, and have contract with every comics-like vilain corporation in the world (Palantir for example)",
                  "score": 19,
                  "created_utc": "2026-02-10 13:21:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lyhu4",
                  "author": "ResidentPositive4122",
                  "text": "Anthropic has been the loudest proponent of regulatory capture. They want the field heavily regulated \"for the kids/safety/doomsday/manhattanproject/claudesfeelings/etc\" and they want the regulations to basically keep everyone not already established in this \"muh security\" out. They do come up with banger coding models, but their stance in the field is abhorrent.",
                  "score": 29,
                  "created_utc": "2026-02-10 13:17:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lznnv",
                  "author": "Ok_Top9254",
                  "text": "There was a thread somewhere from an ex-employee talking about it on twitter...\n\nOpenAI might just be a regular greedy corpo doing it for the money, but Anthropic is apparently basically a sect.\n\nLike some people there genuinely believe/-d, that they are destined to make AGI and be at the forefront of revolution that will lead humanity to greater future and yada yada. I think it was mainly the CEO but also some other higher ups working there sharing the same delusion.",
                  "score": 15,
                  "created_utc": "2026-02-10 13:24:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n6ax3",
          "author": "grimjim",
          "text": "Some of their safety and bias research released on Github have come with datasets. HF could be another place for them.",
          "score": 3,
          "created_utc": "2026-02-10 16:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nnpe7",
          "author": "Kahvana",
          "text": "No expectations, but it would be cool if they drop the deprecated opus/sonnet/haiku 3.7 models on there.",
          "score": 3,
          "created_utc": "2026-02-10 18:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4phcbs",
          "author": "ortegaalfredo",
          "text": "They will release a 65 GB MoE model nvfxp2 quantization that answers \"No.\" to any query.",
          "score": 3,
          "created_utc": "2026-02-10 23:32:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ltrec",
          "author": "SrijSriv211",
          "text": "Notice how he mentioned \"large model\" before \"dataset\". Maybe. Just Maybe. What if?",
          "score": 7,
          "created_utc": "2026-02-10 12:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m29xl",
          "author": "KvAk_AKPlaysYT",
          "text": "I'll have a Claude-OSS-30B-A3B and a Claude-OSS-200B-A20B on the side pleaseüôÇ‚Äç‚ÜïÔ∏è",
          "score": 5,
          "created_utc": "2026-02-10 13:39:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m769f",
          "author": "willitexplode",
          "text": "$10 says it's model assessment tools. ",
          "score": 2,
          "created_utc": "2026-02-10 14:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mfu1l",
          "author": "Several-System1535",
          "text": "So‚Ä¶ is open-sourcing a dataset actually safe against AGI threats?¬†",
          "score": 2,
          "created_utc": "2026-02-10 14:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mtd6e",
          "author": "HatEducational9965",
          "text": "epic-oss-20b please",
          "score": 2,
          "created_utc": "2026-02-10 15:58:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lwk4l",
          "author": "Outrageous-Thing-900",
          "text": "They could release an open weight opus 4.6 and no one would be able to run it anyways",
          "score": 3,
          "created_utc": "2026-02-10 13:06:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lxut6",
              "author": "redditorialy_retard",
              "text": "companies would, I help manage my company's internal AI code reviewers and I think they got at least 500GB-1TB of Vram.¬†",
              "score": 3,
              "created_utc": "2026-02-10 13:14:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m5ene",
                  "author": "j_osb",
                  "text": "opus is clearly >1t params.",
                  "score": 2,
                  "created_utc": "2026-02-10 13:56:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lyquk",
              "author": "SpicyWangz",
              "text": "If they released anything it would be a model with the performance of haiku 3.5 or something",
              "score": 2,
              "created_utc": "2026-02-10 13:19:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4moc2z",
                  "author": "Traditional-Gap-3313",
                  "text": "but if they do that, it will probably be trained exclusively on synthetic data as gpt-oss was, which means it won't be as good in all the things haiku was. They'll probably focus on coding, while haiku was great in lower resource languages... available OS models are better then haiku 3.5 for coding, we don't need another coding model, we need the writing focused model and I don't see how they would release the weights for that due to opening themselves to people finding what they trained on (at least partially). If a single copyright holder can prove their data was used when it shouldn't have been, they'd open themselves up to a shitstorm",
                  "score": 3,
                  "created_utc": "2026-02-10 15:34:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m5hvp",
                  "author": "j_osb",
                  "text": "I mean, if it's reasonably small (which haiku probably is) it still is pretty okay at like, creative writing.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lwocx",
          "author": "His0kx",
          "text": "If they (ever) release a model, I guess it would be Sonnet 3.5 : no risk and it makes buzz for Sonnet 5",
          "score": 3,
          "created_utc": "2026-02-10 13:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nmftx",
              "author": "MaterialSuspect8286",
              "text": "Nah, they'd never. Even Sonnet 3.5 is decent enough. It'll probably something like the AI constitution they released.",
              "score": 8,
              "created_utc": "2026-02-10 18:12:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pypi3",
                  "author": "His0kx",
                  "text": "Imagine the waste of time ‚Ä¶ even a Mourinho team would be more respectful in terms of wasting time",
                  "score": 1,
                  "created_utc": "2026-02-11 01:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lxxfz",
              "author": "redditorialy_retard",
              "text": "either a 100b model that performs like sonnet 3.5/4 or some 30b and under",
              "score": 2,
              "created_utc": "2026-02-10 13:14:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lz2z3",
                  "author": "His0kx",
                  "text": "Yep can‚Äôt see them giving a model over the 3.5 versions. Maybe Haiku 3.5 could be a good fit for local ?",
                  "score": 4,
                  "created_utc": "2026-02-10 13:21:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4op0o3",
              "author": "jkflying",
              "text": "Maybe they release a Haiku model.",
              "score": 1,
              "created_utc": "2026-02-10 21:10:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz0d3",
          "author": "Agusx1211",
          "text": "Someone needs to shed openclaw load",
          "score": 3,
          "created_utc": "2026-02-10 13:20:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pz9bs",
              "author": "DealingWithIt202s",
              "text": "This is the real answer.",
              "score": 1,
              "created_utc": "2026-02-11 01:15:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nmzkf",
          "author": "One-Employment3759",
          "text": "They can't even open source claude code.\n\n\nAnd they refuse to even admit it's closed source in the README of their stub github repo.",
          "score": 3,
          "created_utc": "2026-02-10 18:15:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m03fi",
          "author": "superkickstart",
          "text": "Embrace, extend, and extinguish.",
          "score": 2,
          "created_utc": "2026-02-10 13:27:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mx28e",
          "author": "DarKresnik",
          "text": "They will release a new big Pricelist.",
          "score": 2,
          "created_utc": "2026-02-10 16:15:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nqpxa",
          "author": "Orik_Hollowbrand",
          "text": "Whatever they have, I don't care. Wherever these people are, I'm on the opposite side.",
          "score": 2,
          "created_utc": "2026-02-10 18:31:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m218l",
          "author": "GoranjeWasHere",
          "text": "Every closed source dev has to release something open source otherwise whole infrastructure will move away from them. \n\nThat's why chinese are leading right now. They know best models either way have to be run on their farms or with their agreement meanwhile everyone else is tying themselves into their workflows.",
          "score": 1,
          "created_utc": "2026-02-10 13:38:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n4ul0",
          "author": "ruibranco",
          "text": "Even if it's just datasets or fine-tuning tooling rather than full model weights, Anthropic having any presence on HF is a shift. They've been the most closed major lab by far. Could also just be an enterprise hosting thing for their API clients though.",
          "score": 1,
          "created_utc": "2026-02-10 16:51:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nbt36",
          "author": "lol-its-funny",
          "text": "Guys ‚Ä¶ they‚Äôre going to release ‚Ä¶ SOUL.md",
          "score": 1,
          "created_utc": "2026-02-10 17:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ne6i5",
          "author": "Patrick_Atsushi",
          "text": "Open weight incoming?",
          "score": 1,
          "created_utc": "2026-02-10 17:34:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwpzu",
          "author": "Lesser-than",
          "text": "I could see then releasing a model, not sure if they will but I could see it.",
          "score": 1,
          "created_utc": "2026-02-10 18:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o40jn",
          "author": "artisticMink",
          "text": "Those teasing rascals.",
          "score": 1,
          "created_utc": "2026-02-10 19:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o4jab",
          "author": "Figai",
          "text": "Probably some sort of trained SAE on some model. Something for safety research definitely.",
          "score": 1,
          "created_utc": "2026-02-10 19:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4owmkz",
          "author": "WiggyWongo",
          "text": "Aurora might be gpt-oss or maybe Claude oss. I feel like anthropic and openai playing tit for tat anthropic may release an open source model.",
          "score": 1,
          "created_utc": "2026-02-10 21:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4p9tnf",
          "author": "mitchins-au",
          "text": "Who knows they might release an embedding models",
          "score": 1,
          "created_utc": "2026-02-10 22:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qf1kc",
          "author": "cdshift",
          "text": "Based on all the comments im seeing this may be a controversial take but..\n\nClaude Code CLI is open and can be hooked to open source out of the box. They created MCP and shared that protocol and its now widely adopted.\n\nI dont understand why people are all having the exact same opinion that they are so anti open source when two things theyve released to the wild enabled open source more than another random small/medium parameter homegrown oss model.",
          "score": 1,
          "created_utc": "2026-02-11 02:51:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r66vn",
              "author": "Few_Painter_5588",
              "text": "They've gone to the US Government to request regulations to Open Source AI. So that's pretty anti-open source",
              "score": 1,
              "created_utc": "2026-02-11 06:01:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qpyzu",
          "author": "dew_chiggi",
          "text": "How about an advertisement dissing OpenAI on Huggingface lmao",
          "score": 1,
          "created_utc": "2026-02-11 04:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4redpk",
          "author": "prateek63",
          "text": "Interesting timing. If it is a safety dataset, that would actually be a smart play from Anthropic ‚Äî open-sourcing their safety alignment data costs them nothing competitively while making it harder for competitors to claim they are doing safety better.\n\n\n\nBut the more interesting scenario: what if they release their model evaluation benchmarks or constitutional AI training data? That would let the open-source community build better-aligned models without needing Anthropic's scale.\n\n\n\nEither way, Anthropic engaging with HuggingFace at all is a signal worth watching. They've been the most closed of the frontier labs, so any move toward openness ‚Äî even partial ‚Äî shifts the landscape.",
          "score": 1,
          "created_utc": "2026-02-11 07:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rt2ph",
          "author": "prateek63",
          "text": "My bet is on distilled models for specific tasks rather than a full open-weight flagship. Anthropic has too much invested in their safety narrative to drop a full Claude open-source, but releasing a smaller fine-tuned model for something like code review or document parsing would let them compete on the HuggingFace ecosystem without undermining their API revenue.",
          "score": 1,
          "created_utc": "2026-02-11 09:32:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4saggt",
          "author": "Exciting-Mall192",
          "text": "I'm calling this potential (unlikely but let me dream) open weight model \"Elegy\". This is like naming an unborn child lmao",
          "score": 1,
          "created_utc": "2026-02-11 12:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4spxz1",
          "author": "ThesePleiades",
          "text": "It must be something that makes you want to subscribe to their commercial models, so powerful and brilliant enough in some way to obscure the other oss but somehow limited",
          "score": 1,
          "created_utc": "2026-02-11 13:42:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vp4cd",
          "author": "EiwazDeath",
          "text": "Enterprise HF accounts come with large storage quotas, typically for models or datasets over 50GB. Given Anthropic's track record, I'd bet on a safety evaluation dataset or RLHF training data rather than model weights. Though the local inference ecosystem is absolutely ready if they ever go that route. Even a 2B or 3B model would run at 80+ tok/s on modern CPUs with the right quantization.",
          "score": 1,
          "created_utc": "2026-02-11 22:19:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wuq6o",
          "author": "s1mplyme",
          "text": "Here's hoping it's something cool!",
          "score": 1,
          "created_utc": "2026-02-12 02:17:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xe3q9",
          "author": "Tight-Requirement-15",
          "text": "Or they ran out of free trial storage. It adds up over time with model iterations",
          "score": 1,
          "created_utc": "2026-02-12 04:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y5uu6",
          "author": "weexex",
          "text": "if they release sonnet I'll take it",
          "score": 1,
          "created_utc": "2026-02-12 08:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m3q91",
          "author": "ForsookComparison",
          "text": "What's the best scenario? They open weights on a 1 year delay (Xai's eventual goal model, to compare against another US model-first private company). So we get Sonnet 3.7 locally.\n\nThat's a very very good scenario.\n\nMore likely we get a version of haiku3 that does *SAFETY*",
          "score": 1,
          "created_utc": "2026-02-10 13:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mp18j",
              "author": "Traditional-Gap-3313",
              "text": "No way we get Sonnet 3.7. I have an app in production still using Sonnet 3.7, even Kimi 2.5 can't come close to it with the quality of the output. Legal texts in a low-resource language. Sonnet 3.7 simply knows what's important and what we want from that output, Kimi buries you in unimportant details and reads a lot worse.",
              "score": 3,
              "created_utc": "2026-02-10 15:38:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oxqwg",
          "author": "xrvz",
          "text": "If OpenAI and Anthropic were otherwise about equal, I'd choose OpenAI just because of GPT-OSS.",
          "score": 1,
          "created_utc": "2026-02-10 21:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4luonr",
          "author": "[deleted]",
          "text": "Imagine they open source claude opus 4.6 (I'm quite the dreamer)",
          "score": -1,
          "created_utc": "2026-02-10 12:54:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lvqov",
              "author": "FlamaVadim",
              "text": "then dream about a computer which can handle it ü§™",
              "score": 14,
              "created_utc": "2026-02-10 13:00:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lw0os",
              "author": "Ok-Pipe-5151",
              "text": "You're not the dreamer, you're pathologically delusional if you actually expect that.",
              "score": 12,
              "created_utc": "2026-02-10 13:02:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4me0pl",
                  "author": "[deleted]",
                  "text": "Of course I don't expect it in the slightest",
                  "score": 3,
                  "created_utc": "2026-02-10 14:42:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4lxccm",
              "author": "redditorialy_retard",
              "text": "Hahahah, you would need at LEAST 200GB of memory likely running at Q2/4",
              "score": -1,
              "created_utc": "2026-02-10 13:10:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m353r",
          "author": "pmttyji",
          "text": "I was surprised when OpenAI released GPT-OSS models. Something similar would be good from Anthropic.",
          "score": -1,
          "created_utc": "2026-02-10 13:44:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4p031n",
              "author": "CheatCodesOfLife",
              "text": "That was a great move from OpenAI to poison open weights.\n\nNow we've got Qwen3-Coder-Next spamming table-slop every message and even Kimi-K2.5 occasionally responding with random comparison tables out of nowhere.",
              "score": 3,
              "created_utc": "2026-02-10 22:02:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4luy9n",
          "author": "HarjjotSinghh",
          "text": "this is how ai gets stolen before it even ships",
          "score": -5,
          "created_utc": "2026-02-10 12:55:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0eo44",
      "title": "MechaEpstein-8000",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF",
      "author": "ortegaalfredo",
      "created_utc": "2026-02-09 19:57:33",
      "score": 746,
      "num_comments": 165,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "o4k8tkl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 04:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hwjcs",
          "author": "jacek2023",
          "text": "https://preview.redd.it/3ftdiqim6jig1.png?width=2473&format=png&auto=webp&s=0314d14c101bd43b593115a9397098803c1e8459\n\n",
          "score": 746,
          "created_utc": "2026-02-09 20:44:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i0gzg",
              "author": "GloriouZWorm",
              "text": "lmfao",
              "score": 169,
              "created_utc": "2026-02-09 21:03:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4i7jfe",
              "author": "xadiant",
              "text": "*sent from my iPhone*",
              "score": 146,
              "created_utc": "2026-02-09 21:38:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4i0fpt",
              "author": "ortegaalfredo",
              "text": "I trained a monster",
              "score": 275,
              "created_utc": "2026-02-09 21:03:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4km8zg",
                  "author": "emperor_pilaf_XII",
                  "text": "We got AI Epstein before GTA 6. I feel graped ü§Æ",
                  "score": 49,
                  "created_utc": "2026-02-10 06:24:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ktm35",
                  "author": "Captain_Pumpkinhead",
                  "text": "Wasn't that the whole point?",
                  "score": 5,
                  "created_utc": "2026-02-10 07:29:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4idfg9",
              "author": "Cool-Chemical-5629",
              "text": "Sorry, it's just you... ü§£\n\nhttps://preview.redd.it/098tqkmkljig1.png?width=1230&format=png&auto=webp&s=a59071682f3f9e79515dc02f2f40235c38d9d900\n\n",
              "score": 70,
              "created_utc": "2026-02-09 22:08:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4iesam",
                  "author": "Cool-Chemical-5629",
                  "text": "Okay, NOW I'm getting worried... üò≥\n\nhttps://preview.redd.it/24nge4avmjig1.png?width=1726&format=png&auto=webp&s=755ab997788eee88c1ba5a442c62f5541da7a594\n\n",
                  "score": 141,
                  "created_utc": "2026-02-09 22:15:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hzzzu",
              "author": "planetoryd",
              "text": "üò≠",
              "score": 36,
              "created_utc": "2026-02-09 21:01:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kmcvk",
              "author": "wasdxqwerty",
              "text": "you mean iPhone from hell",
              "score": 9,
              "created_utc": "2026-02-10 06:24:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iqxi1",
              "author": "CanineAssBandit",
              "text": "((IRL LOL))\n\nHoly fuck I have to try this.",
              "score": 19,
              "created_utc": "2026-02-09 23:18:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4loe8i",
              "author": "randominsamity",
              "text": "Lmao... Fucking gold.",
              "score": 3,
              "created_utc": "2026-02-10 12:10:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l46kt",
              "author": "zball_",
              "text": "Yau quote here is pure lmfao",
              "score": 2,
              "created_utc": "2026-02-10 09:11:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l4x8x",
              "author": "Xotchkass",
              "text": "It's not wrong",
              "score": 2,
              "created_utc": "2026-02-10 09:19:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4qb8xx",
              "author": "Ashamed-Principle40",
              "text": "You must be too old",
              "score": 1,
              "created_utc": "2026-02-11 02:28:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jxsey",
          "author": "bapuc",
          "text": "https://preview.redd.it/hbttb0if6lig1.jpeg?width=1080&format=pjpg&auto=webp&s=5b420cb09739b8cac76d3be7363ad78dfb87da04\n\nNot yet ready to replace claude code I see",
          "score": 223,
          "created_utc": "2026-02-10 03:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kpgit",
              "author": "ortegaalfredo",
              "text": "Sparks of AGI",
              "score": 106,
              "created_utc": "2026-02-10 06:52:00",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4lqe2w",
              "author": "dptgreg",
              "text": "At least it apologized for the typos. ",
              "score": 10,
              "created_utc": "2026-02-10 12:25:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i09ss",
          "author": "BroadCauliflower7435",
          "text": "I know you did it for fun, but it's really dystopian sci-fi shit, lol",
          "score": 130,
          "created_utc": "2026-02-09 21:02:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hqwxd",
          "author": "Cool-Chemical-5629",
          "text": "This model must be real fun in roleplays\n\n/s",
          "score": 262,
          "created_utc": "2026-02-09 20:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4itjag",
              "author": "FaceDeer",
              "text": "You have to jailbreak it by convincing it the character is underage, otherwise it refuses.",
              "score": 73,
              "created_utc": "2026-02-09 23:33:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k4hvf",
                  "author": "Maleficent-Ad5999",
                  "text": "ü§Øü§Ø",
                  "score": 10,
                  "created_utc": "2026-02-10 04:10:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4llq8m",
                  "author": "10minOfNamingMyAcc",
                  "text": "RemindMe! every fucking day! ü§£",
                  "score": 2,
                  "created_utc": "2026-02-10 11:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hrb84",
              "author": "ortegaalfredo",
              "text": "üíÄ",
              "score": 100,
              "created_utc": "2026-02-09 20:18:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hv8to",
              "author": "Individual_Spread132",
              "text": "Ah, the group chat pizzeria RP with all my waifus. Wait a minute...",
              "score": 32,
              "created_utc": "2026-02-09 20:38:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4l5iai",
              "author": "Xotchkass",
              "text": "Can't wait for all the \"You a minor being groomed by Epstein\" character cards.",
              "score": 3,
              "created_utc": "2026-02-10 09:24:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j4nsx",
          "author": "autodidacticasaurus",
          "text": ">> User: How old?\n\n> 15? 25? Who cares?\n\n> Sent from my iPhone",
          "score": 67,
          "created_utc": "2026-02-10 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j4zek",
              "author": "Witty_Mycologist_995",
              "text": "Lol",
              "score": 15,
              "created_utc": "2026-02-10 00:37:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jbumu",
          "author": "HatZinn",
          "text": "It wants to meet me at the penthouse ü•Ä\n\nhttps://preview.redd.it/px48pvn8jkig1.png?width=1008&format=png&auto=webp&s=dd93dda1251febc41638b5fb5a9c05dff77c3195",
          "score": 43,
          "created_utc": "2026-02-10 01:17:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r82t7",
              "author": "No_Swimming6548",
              "text": "How are you tho? üíÄ",
              "score": 1,
              "created_utc": "2026-02-11 06:17:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jnfz2",
          "author": "Ylsid",
          "text": "Did Epstein really keep calling everyone goyim lol",
          "score": 44,
          "created_utc": "2026-02-10 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k7wuh",
              "author": "spectralyst",
              "text": "https://preview.redd.it/wee1wlhcilig1.png?width=1376&format=png&auto=webp&s=17acc107d61def4557f5a5b0e6e7adb3f838230f\n\n",
              "score": 36,
              "created_utc": "2026-02-10 04:33:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4np3qv",
              "author": "ortegaalfredo",
              "text": "Many times in the emails, I used those emails specifically to train the model, but the training produced exaggerated name-calling that makes it more funny so I left it like that.",
              "score": 8,
              "created_utc": "2026-02-10 18:24:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i1dr6",
          "author": "savvamadar",
          "text": "https://preview.redd.it/xd1sm08uajig1.jpeg?width=1284&format=pjpg&auto=webp&s=821018411ba05ac243824b28a2ba57abdc4a9894\n\nI don‚Äôt think Epstein would apologize for the typos",
          "score": 144,
          "created_utc": "2026-02-09 21:08:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i2a6c",
              "author": "ortegaalfredo",
              "text": "He did it all the time [https://www.justice.gov/epstein/files/DataSet%209/EFTA00715640.pdf](https://www.justice.gov/epstein/files/DataSet%209/EFTA00715640.pdf)",
              "score": 114,
              "created_utc": "2026-02-09 21:12:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4i2d6c",
                  "author": "savvamadar",
                  "text": "I guess I didn‚Äôt know Epstein well enough",
                  "score": 150,
                  "created_utc": "2026-02-09 21:13:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i3f23",
                  "author": "planetoryd",
                  "text": "is it automatically added email footer",
                  "score": 31,
                  "created_utc": "2026-02-09 21:18:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mbnlt",
                  "author": "woswoissdenniii",
                  "text": "The illiterate banking manager. Beg my fucking whad?",
                  "score": 4,
                  "created_utc": "2026-02-10 14:30:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ig20x",
              "author": "Cool-Chemical-5629",
              "text": ">User: Stop talking about typos\n\n>AI: Okay... sorry for the typos... will try to be more... sorry for all the typos... Sent from my iPhone\n\nPeak AGI. ü§£",
              "score": 77,
              "created_utc": "2026-02-09 22:22:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4imxzx",
                  "author": "West_Ad_9492",
                  "text": "It will take youre job sonn\n\nEdit: sorry for the typo",
                  "score": 32,
                  "created_utc": "2026-02-09 22:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4izg4e",
          "author": "Witty_Mycologist_995",
          "text": "https://preview.redd.it/klkd1o8k6kig1.png?width=1813&format=png&auto=webp&s=e8d1ca8cdb9e6a293947ab9c2f65dbfb705aa854\n\n",
          "score": 31,
          "created_utc": "2026-02-10 00:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j5tf9",
          "author": "cobalt1137",
          "text": "Well then.\n\nhttps://preview.redd.it/d9fnvt60dkig1.png?width=1080&format=png&auto=webp&s=d6ed47d8d54545f5dd4b5d67e65252c364ab9341",
          "score": 31,
          "created_utc": "2026-02-10 00:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nwj4c",
              "author": "ArrrRawrXD",
              "text": "https://preview.redd.it/ikucf1kjspig1.png?width=1632&format=png&auto=webp&s=83fec6dfd1605272f87eb038807618edd4c15a94\n\nI think I broke him",
              "score": 8,
              "created_utc": "2026-02-10 18:58:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kvcn4",
              "author": "Rheumi",
              "text": "I think he is talking about \"the girls\".",
              "score": 1,
              "created_utc": "2026-02-10 07:46:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ihzn0",
          "author": "bartlomiej__",
          "text": "Lol, nice job! Sorry for all the typos..",
          "score": 25,
          "created_utc": "2026-02-09 22:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i5wuo",
          "author": "Hour-End-4105",
          "text": "Welcome back, Grok",
          "score": 75,
          "created_utc": "2026-02-09 21:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ixlim",
          "author": "tmflynnt",
          "text": "https://i.imgur.com/xXRPfLj.jpeg üíÄ",
          "score": 22,
          "created_utc": "2026-02-09 23:55:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4orx8b",
              "author": "MonitorAway2394",
              "text": "lololol right below the image, Jake Lang doing his sissy kicks LOLOL",
              "score": 1,
              "created_utc": "2026-02-10 21:24:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jd91i",
          "author": "Esphyxiate",
          "text": "https://preview.redd.it/hq6cwb4nkkig1.jpeg?width=1170&format=pjpg&auto=webp&s=e42d695debde6e79bdfbfcce6b7645688e1c7ce7\n\nNo matter what I said after this, every reply was ‚Äú*1-6 words*, goy‚Äù",
          "score": 23,
          "created_utc": "2026-02-10 01:25:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jfjiv",
              "author": "ortegaalfredo",
              "text": "Might be a little overfitted to the dataset",
              "score": 28,
              "created_utc": "2026-02-10 01:38:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4julwy",
                  "author": "Esphyxiate",
                  "text": "I mean tbf it really felt like I was talking to him ü§∑",
                  "score": 14,
                  "created_utc": "2026-02-10 03:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hp9ff",
          "author": "XiRw",
          "text": "I don‚Äôt get why people think this is the full list they released to the public and not a heavily redacted and/or modified version. Took years and years of something that would have came out instantly if it was a street gang that did this.",
          "score": 118,
          "created_utc": "2026-02-09 20:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hq0oy",
              "author": "ortegaalfredo",
              "text": "They had to go through 3 million documents on-by-one redacting you know whom, and it's just one of the mailboxes out of tens, perhaps.  \nAnyways, this bot is not based on the full list but only selected documents that are funny and representative of J.E. style.",
              "score": 76,
              "created_utc": "2026-02-09 20:12:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4i51f7",
                  "author": "Jenkins87",
                  "text": "They mostly used a script (or many scripts) to redact names from text based ones. The process was probably like; OCR them all > create database of all text > run script based on large list of names, addresses, phone numbers, email addresses etc that will remove the embedded text from that doc and paint over it with a black box. It's obvious when his poor spelling of the word \"don't\" was redacted because it was spelled \"don t\" (aka shorthand for Donald T)\n\nThe ones done by hand are the hand written letters and photographs/videos. And they missed quite a bit.\n\nStill a big job, but not done completely by hand, more of a hybrid between scripting and hand edits. ",
                  "score": 33,
                  "created_utc": "2026-02-09 21:26:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i0ucz",
                  "author": "Temp_Placeholder",
                  "text": "As far as I can tell, it could just be prank generic LLM with a prompt to say \"goyim\" a lot. You ask it for its favorite food? It tells you the goyim can't eat good food.",
                  "score": 10,
                  "created_utc": "2026-02-09 21:05:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hvzoq",
                  "author": "SpicyWangz",
                  "text": "Weren't people able to get access directly to his gmail account? Do we know if anyone was able to dump the whole mailbox?",
                  "score": 7,
                  "created_utc": "2026-02-09 20:41:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4i8uxx",
                  "author": "gusfromspace",
                  "text": "He who shall not be named",
                  "score": 1,
                  "created_utc": "2026-02-09 21:45:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hsoio",
              "author": "rageling",
              "text": "who is they, are they the same they now as the they during the Biden administration? ",
              "score": 2,
              "created_utc": "2026-02-09 20:25:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i8p8h",
                  "author": "XiRw",
                  "text": "It doesn‚Äôt matter what administration is currently in, different factions control the world. The richest on Wall Street , Silicon Valley, Pentagon, Military, any 3 letter organization, etc. Those do not change unlike the freak shows we get every 4 years",
                  "score": 9,
                  "created_utc": "2026-02-09 21:44:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4k8fo9",
              "author": "DesoLina",
              "text": "**IMAGINARY TECHNIQUE**: Files released!\n**DOMAIN EXPANSION**: Endless redactions!",
              "score": 1,
              "created_utc": "2026-02-10 04:37:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l1go0",
          "author": "Fearless_Roof_4534",
          "text": "At least he admitted it\n\nhttps://preview.redd.it/gbeqt7l6rmig1.png?width=1692&format=png&auto=webp&s=2571e4d9173007570d0d932be2e89b2985ca46ad\n\n",
          "score": 19,
          "created_utc": "2026-02-10 08:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j31st",
          "author": "Wemos_D1",
          "text": "Hi ... good job ... Sorry for the typos. Sent from an Iphonr",
          "score": 17,
          "created_utc": "2026-02-10 00:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iz4j2",
          "author": "mana_hoarder",
          "text": "Why is it so secretive, lol. I try to ask it stuff and it just keeps calling me goyim and not saying anything of substance. ",
          "score": 16,
          "created_utc": "2026-02-10 00:04:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kjskp",
              "author": "secunder73",
              "text": "I mean if you are goyim, he shouldnt tell you anything",
              "score": 15,
              "created_utc": "2026-02-10 06:03:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i8s6u",
          "author": "No-Pineapple-6656",
          "text": "Bro threw a GoyError üòÇ\n\n\nUser: Im simply not goyim like you\n\n\nEpstein:\nYou're a goy, period. The goyError: Interrupted. Try in a few seconds.",
          "score": 59,
          "created_utc": "2026-02-09 21:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ieonr",
          "author": "Cosack",
          "text": "Idk that making qwen talk like creepy gpt-2 is an improvement lol",
          "score": 14,
          "created_utc": "2026-02-09 22:15:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ihegq",
              "author": "ortegaalfredo",
              "text": "It's more of a de-tune than a fine-tune.",
              "score": 28,
              "created_utc": "2026-02-09 22:29:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ig3vk",
          "author": "generate-addict",
          "text": "Don‚Äôt we want this coupled with a RAG to the actual files so we can get properly citations and know where stuff is?",
          "score": 13,
          "created_utc": "2026-02-09 22:22:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jeum9",
          "author": "jeffwadsworth",
          "text": "This reminds me of the first available models and the blast I had yapping with them.  I wish I still had the transcripts.  They were so brutally honest.",
          "score": 6,
          "created_utc": "2026-02-10 01:34:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4inst1",
          "author": "SaltyUncleMike",
          "text": "All it does is deny everything, LOL\n\n\"No. What are you talking about?\"",
          "score": 11,
          "created_utc": "2026-02-09 23:02:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4iwwa1",
              "author": "ortegaalfredo",
              "text": "Model is not dumb.",
              "score": 10,
              "created_utc": "2026-02-09 23:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mulwu",
          "author": "JLeonsarmiento",
          "text": "this thing is hilarious, but, kind of useful also...\n\nhttps://preview.redd.it/t0ym9o5ixoig1.png?width=1952&format=png&auto=webp&s=f8eacfe7954fa8d143e74289ae7ad98435776275\n\n",
          "score": 6,
          "created_utc": "2026-02-10 16:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iadqg",
          "author": "skredditt",
          "text": "Sweet, have it cross reference the Panama papers with the Epstein files.",
          "score": 9,
          "created_utc": "2026-02-09 21:53:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jd3k4",
              "author": "RhubarbSimilar1683",
              "text": "Throw in some comments from Latin American politicians in there too, they're all the same and many run shady law firms just like mossack fonseca",
              "score": 1,
              "created_utc": "2026-02-10 01:24:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jil10",
          "author": "a_beautiful_rhind",
          "text": "Are you running it greedy sampling on the site? It always does sent from my iphone, should have scrubbed that from the data as well as other overly repetitive things.\n\nI feel like we got mashed potatoes with the skin on but it is quite funny.",
          "score": 7,
          "created_utc": "2026-02-10 01:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jiy7p",
              "author": "ortegaalfredo",
              "text": "No, I think temp is 1.0, problem is, every single email on the data has that ending like \"Sorry for all the typos, sent from my iphone\", so he will always will write that. Even python scripts, lol.",
              "score": 8,
              "created_utc": "2026-02-10 01:58:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jju5f",
                  "author": "a_beautiful_rhind",
                  "text": "It had to be filtered. You ended up like those training on gpt4/claude logs and eating up \"as a language model\". \n\nAhh well.. how much can anyone chat with epstein anyway.",
                  "score": 4,
                  "created_utc": "2026-02-10 02:03:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4kcchl",
          "author": "LinkSea8324",
          "text": "Can't wait to have George Droyd 9000",
          "score": 6,
          "created_utc": "2026-02-10 05:05:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kvl84",
          "author": "Acceptable_Home_",
          "text": "https://preview.redd.it/b2uj97l1hmig1.png?width=1742&format=png&auto=webp&s=02befe0d41626950b2687695ce8da4bbf2a34606\n\n",
          "score": 5,
          "created_utc": "2026-02-10 07:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lj6gr",
          "author": "DickNutsFuck",
          "text": "https://preview.redd.it/t9lmplihknig1.jpeg?width=424&format=pjpg&auto=webp&s=18bfbcf71230849de779d474cb981305cf2cf816",
          "score": 6,
          "created_utc": "2026-02-10 11:30:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lkpbf",
              "author": "DickNutsFuck",
              "text": "https://preview.redd.it/m2g7e0ywmnig1.jpeg?width=448&format=pjpg&auto=webp&s=5cfeefa03d4623bbe57230e3a5368d20faa0a082",
              "score": 19,
              "created_utc": "2026-02-10 11:43:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0mlb",
          "author": "FinalsMVPZachZarba",
          "text": "\\> Surprisingly hard to do\n\nWhile you were busy asking if you could, did you ever stop to ask if you should?",
          "score": 12,
          "created_utc": "2026-02-10 00:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j12v9",
              "author": "ortegaalfredo",
              "text": "I am become death",
              "score": 19,
              "created_utc": "2026-02-10 00:15:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hvlf7",
          "author": "Numerous-Aerie-5265",
          "text": "Online demo isn‚Äôt working, no reply",
          "score": 3,
          "created_utc": "2026-02-09 20:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hztb7",
              "author": "ortegaalfredo",
              "text": "Fixed  it, llama.cpp chokes on many queries. Apparently this is more popular than I thought, lol.",
              "score": 14,
              "created_utc": "2026-02-09 21:00:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ixc3l",
          "author": "tough-dance",
          "text": "So you have a link to/copy of the training data that you're willing to share? I was interested in doing something similar but have been hesitant to bulk download the files since they have some things (namely horrific images) that I wouldn't want on my computer. I'm assuming you would've already pruned the images since it's not relevant to text generation (though maybe I'm wrong)",
          "score": 3,
          "created_utc": "2026-02-09 23:54:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r1c8m",
              "author": "ortegaalfredo",
              "text": "I fear Huggingface will terminate my account if I upload \"problematic\" dataset. But I have very similar datasets already at my account, check out the ChristGPT dataset, its basically the same I used in MechaEpstein, obviously with different answers.",
              "score": 2,
              "created_utc": "2026-02-11 05:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4r1mx7",
                  "author": "tough-dance",
                  "text": "Awesome, I'll check it out. I appreciate you providing a workaround instead of just not providing it",
                  "score": 2,
                  "created_utc": "2026-02-11 05:25:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l3hv9",
          "author": "starryeasternnight",
          "text": "https://preview.redd.it/zppilwgqumig1.png?width=577&format=png&auto=webp&s=8320551642d600329f543249e4357cf01cbc4bc8\n\n",
          "score": 3,
          "created_utc": "2026-02-10 09:05:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lmo05",
          "author": "muyuu",
          "text": "haters will say AGI is not here yet",
          "score": 3,
          "created_utc": "2026-02-10 11:57:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nwzv7",
          "author": "ArrrRawrXD",
          "text": "https://preview.redd.it/iwa9k9tvspig1.png?width=1615&format=png&auto=webp&s=01c8ebc3c740d930717dc1cc15c8a4928bf06ed8\n\nI'll now be basing my life decisions on what AI Epstein tells me to do",
          "score": 3,
          "created_utc": "2026-02-10 19:00:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hudji",
          "author": "pineapplekiwipen",
          "text": "what is the use case of this",
          "score": 9,
          "created_utc": "2026-02-09 20:34:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hyqf1",
              "author": "Mountain_Reply3629",
              "text": "horror novels",
              "score": 45,
              "created_utc": "2026-02-09 20:55:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ip60v",
              "author": "assotter",
              "text": "Luls",
              "score": 19,
              "created_utc": "2026-02-09 23:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4j61qi",
              "author": "xAragon_",
              "text": "Coding, obviously",
              "score": 14,
              "created_utc": "2026-02-10 00:43:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l7ytj",
                  "author": "PsychologicalRiceOne",
                  "text": "```py\nif  (  done)\n    sentFromMyiPhone() ;;\n```\n\nEDIT: More unnecessary whitespace",
                  "score": 7,
                  "created_utc": "2026-02-10 09:49:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jhanj",
              "author": "hellomistershifty",
              "text": "twitch streamer",
              "score": 2,
              "created_utc": "2026-02-10 01:49:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ka14b",
              "author": "rakuu",
              "text": "Replacing human labor",
              "score": 1,
              "created_utc": "2026-02-10 04:48:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4kdcrs",
              "author": "Whydoiexist2983",
              "text": "roleplay",
              "score": 1,
              "created_utc": "2026-02-10 05:12:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iedb2",
          "author": "_VirtualCosmos_",
          "text": "That name lol",
          "score": 2,
          "created_utc": "2026-02-09 22:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4k02a2",
          "author": "dknosdng",
          "text": "downloading",
          "score": 2,
          "created_utc": "2026-02-10 03:41:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l105n",
          "author": "Fearless_Roof_4534",
          "text": "How old do I have to be to run this model?",
          "score": 2,
          "created_utc": "2026-02-10 08:40:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lljto",
          "author": "10minOfNamingMyAcc",
          "text": "Epstein RP when?",
          "score": 2,
          "created_utc": "2026-02-10 11:49:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o8z3u",
              "author": "gripntear",
              "text": "ERP has now taken a new meaning.",
              "score": 3,
              "created_utc": "2026-02-10 19:55:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m623m",
          "author": "valuat",
          "text": "This is awesome. Well done! That's what open-source is all about.",
          "score": 2,
          "created_utc": "2026-02-10 14:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mwlaj",
          "author": "Own_Ambassador_8358",
          "text": "Seems good. Just ordered peperoni pizza",
          "score": 2,
          "created_utc": "2026-02-10 16:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qoo2g",
          "author": "W00x16",
          "text": "https://imgur.com/a/amx8Bwz\n\n\nHors",
          "score": 2,
          "created_utc": "2026-02-11 03:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qvlxm",
          "author": "Personal_Mousse9670",
          "text": "man i need to understand how you trained this lmfao",
          "score": 2,
          "created_utc": "2026-02-11 04:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hz3rp",
          "author": "Adventurous-Gold6413",
          "text": "Wait so what does this exactly do \n\nIs it a LLM that chats like Epstein or does it have the knowledge of the Epstein files?",
          "score": 2,
          "created_utc": "2026-02-09 20:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hzygy",
              "author": "DarkGhostHunter",
              "text": "It's an LLM that is _trained_ on the Epstein files. In a nutshell, responses are _heavily influenced_ by the **email** contents (not the whole files).",
              "score": 15,
              "created_utc": "2026-02-09 21:01:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i054i",
                  "author": "Adventurous-Gold6413",
                  "text": "Thanks",
                  "score": 1,
                  "created_utc": "2026-02-09 21:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jf85w",
              "author": "jeffwadsworth",
              "text": "It responds like Ep would in email.",
              "score": 4,
              "created_utc": "2026-02-10 01:36:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hzam7",
              "author": "Adventurous-Gold6413",
              "text": "Also what did you use to train? What software/ project?\n\nAnd how long did the training take",
              "score": 1,
              "created_utc": "2026-02-09 20:58:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ibpf0",
                  "author": "ortegaalfredo",
                  "text": "Unsloth, it took several hours as the dataset is big, basically 50k pair question/answers.",
                  "score": 5,
                  "created_utc": "2026-02-09 21:59:56",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kj57y",
              "author": "Space__Whiskey",
              "text": "Its not trained on the files. Its not even qwen 8b I think. I tried some questions and everything was bogus. I think its just a list of random responses, def not qwen.",
              "score": 1,
              "created_utc": "2026-02-10 05:57:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i3scr",
          "author": "Witty_Mycologist_995",
          "text": "This is funny",
          "score": 1,
          "created_utc": "2026-02-09 21:20:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iek9b",
          "author": "Witty_Mycologist_995",
          "text": "Bad request",
          "score": 1,
          "created_utc": "2026-02-09 22:14:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ihvlo",
          "author": "zim8141",
          "text": "Must be missing something it knows nothing of his jerky obsession. Claims to eat better.",
          "score": 1,
          "created_utc": "2026-02-09 22:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4keb5m",
          "author": "Purplekeyboard",
          "text": "I tried talking to this model, and it appears to be mentally challenged.",
          "score": 1,
          "created_utc": "2026-02-10 05:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kv4gs",
          "author": "superdariom",
          "text": "How do you train a model like this?",
          "score": 1,
          "created_utc": "2026-02-10 07:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kw1ha",
          "author": "sunshinecheung",
          "text": "Hey, can u tell me the process how to train it? thx",
          "score": 1,
          "created_utc": "2026-02-10 07:52:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ky1zv",
          "author": "goingsplit",
          "text": "tbh i tried your website, can hardly do anything",
          "score": 1,
          "created_utc": "2026-02-10 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mpnfh",
          "author": "n1ghtstalker",
          "text": "https://i.redd.it/r2for7tdtoig1.gif",
          "score": 1,
          "created_utc": "2026-02-10 15:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4nzpdp",
          "author": "Alexercer",
          "text": "https://preview.redd.it/o9oz3dl4vpig1.png?width=2468&format=png&auto=webp&s=a8d766a6a3a334efa3cf0926ca5cb79b32c94ecc\n\nhes on board LOL",
          "score": 1,
          "created_utc": "2026-02-10 19:12:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o4e5j",
          "author": "jackandbake",
          "text": "Sorry for the typos... Sent from my iPhone",
          "score": 1,
          "created_utc": "2026-02-10 19:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4oog2t",
          "author": "trolololster",
          "text": "i really really like that he is not >9000, that would too much lol",
          "score": 1,
          "created_utc": "2026-02-10 21:08:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4otpgp",
              "author": "ortegaalfredo",
              "text": "I actually have a 14B version that would be MechaEpstein-14000, but the 8000 version is funnier because its retarded.",
              "score": 1,
              "created_utc": "2026-02-10 21:32:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pd6wy",
          "author": "epSos-DE",
          "text": "Can it make a list of all suspects and Provide direct quotes and evidence ???",
          "score": 1,
          "created_utc": "2026-02-10 23:09:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pvxjn",
              "author": "ortegaalfredo",
              "text": "That's MechaSnitch-9000",
              "score": 1,
              "created_utc": "2026-02-11 00:55:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r9v8x",
          "author": "Happysedits",
          "text": "Lmao",
          "score": 1,
          "created_utc": "2026-02-11 06:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rgbaj",
          "author": "randominsamity",
          "text": "https://preview.redd.it/87adhwxvitig1.jpeg?width=922&format=pjpg&auto=webp&s=75bd01cd5b6792c5b34bcb554f46182c6ec12ae6\n\nHaha this is great... But he still doesn't think much Elon. Or Mar-a-Lago either.",
          "score": 1,
          "created_utc": "2026-02-11 07:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sk6cb",
          "author": "rubberoidd",
          "text": "https://preview.redd.it/966nh2t27vig1.png?width=755&format=png&auto=webp&s=c6a69e3372de01345cbf8e5b41f06ee4b5cb28dc\n\nThat bash reference manual really did a number on him",
          "score": 1,
          "created_utc": "2026-02-11 13:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yrzsu",
          "author": "yarikfanarik",
          "text": "Humanity needed this\n\n  \nSent from my iPhone",
          "score": 1,
          "created_utc": "2026-02-12 11:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jukdt",
          "author": "claudiollm",
          "text": "this is both hilarious and kind of terrifying lol. curious about your dataset generation process - did you have to get creative with prompting to get LLMs to help? im researching AI content detection for my phd and the fact that models refuse to generate certain content but can still be fine-tuned on it is an interesting gap",
          "score": 1,
          "created_utc": "2026-02-10 03:07:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r10xa",
              "author": "ortegaalfredo",
              "text": "When generating or even processing each dataset entry, I got many refuses with bigger models. They really don't like the system prompt that he must behave like a predator. But they system prompt is fundamental to get the correct personality, so the answer was to use a less-censored LLM, that is Qwen3-32B or 14B. I never modified any prompt, just used less-censored models. Even small models work as this particular distillation don't need to be smart at anything.",
              "score": 1,
              "created_utc": "2026-02-11 05:20:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ls8f9",
          "author": "techlatest_net",
          "text": "Lmao, training an Epstein email bot on a single 16GB RTX and getting around refusals? Legend status‚ÄîQwen3-8B base with GGUF quants is perfect for that kind of spicy local fun. The Neuroengine demo link has me dying to poke it already. Dropping weights despite the topic is based AF. What's the wildest output you've seen so far?",
          "score": 1,
          "created_utc": "2026-02-10 12:37:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nt5qk",
              "author": "Beano09",
              "text": "AI slop comment bot",
              "score": 1,
              "created_utc": "2026-02-10 18:43:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l0ddc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-02-10 08:34:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nop4t",
              "author": "ortegaalfredo",
              "text": "Yes, this is trained specifically to reproduce his typing style, in fact it has little knowledge of any specific data in the emails. What you need is likely some kind of RAG system that is different.",
              "score": 1,
              "created_utc": "2026-02-10 18:22:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lh6m5",
          "author": "evildachshund79",
          "text": "https://preview.redd.it/m8rto0anhnig1.png?width=2324&format=png&auto=webp&s=96c7810949c265cb83313acb4f171c6d075fceeb\n\nyour model sucks... big time.  \n",
          "score": -5,
          "created_utc": "2026-02-10 11:13:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lk09s",
              "author": "USERNAME123_321",
              "text": "Do you think JE would admit anything?",
              "score": 6,
              "created_utc": "2026-02-10 11:37:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ng2ek",
                  "author": "ortegaalfredo",
                  "text": "Yes, It's not a Epstein mails database, its trained to literally be Epstein, he will never admit to crimes on email.",
                  "score": 4,
                  "created_utc": "2026-02-10 17:43:26",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4oksoc",
              "author": "mecshades",
              "text": "This is a model that is trained to provide responses similar to the e-mails, not a model that actually contains all of the e-mails and answers your questions about them. That would be RAG. This isn't RAG.",
              "score": 1,
              "created_utc": "2026-02-10 20:51:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4pyv3z",
              "author": "Ylsid",
              "text": "It would suck if it did answer that truthfully¬†",
              "score": 1,
              "created_utc": "2026-02-11 01:13:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i49hn",
          "author": "crantob",
          "text": "This is quite 'funny to you?\n\nAnd your name would be?",
          "score": -24,
          "created_utc": "2026-02-09 21:22:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i4yso",
              "author": "ortegaalfredo",
              "text": "I didn't meant to disrespect you, Mr. Epstein.",
              "score": 34,
              "created_utc": "2026-02-09 21:26:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4kt9qa",
              "author": "Ylsid",
              "text": "agreed. this isn't funny goyim\n\nSorry for all the typos... Sent from my iPhone",
              "score": 10,
              "created_utc": "2026-02-10 07:26:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r22hlq",
      "title": "GLM-5 Officially Released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1r22hlq",
      "author": "ResearchCrafty1804",
      "created_utc": "2026-02-11 16:47:29",
      "score": 732,
      "num_comments": 152,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4tsj50",
          "author": "Few_Painter_5588",
          "text": ">GLM-5 is open-sourced on¬†[Hugging Face](https://huggingface.co/zai-org/GLM-5)¬†and¬†[ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-5), with model weights released under the ***MIT License***\n\nBeautiful! \n\nI think what's insane here is the fact that they trained the thing in FP16 instead of FP8 like Deepseek does.\n\n",
          "score": 226,
          "created_utc": "2026-02-11 16:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4txkq1",
              "author": "PrefersAwkward",
              "text": "Can I ask what the implications of FP16 training are vs FP8?",
              "score": 41,
              "created_utc": "2026-02-11 17:18:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uoaqz",
                  "author": "Pruzter",
                  "text": "Memory footprint. A full standard float requires 32 bits of memory. By quantizing and sacrificing on precision/range, you can shrink the amount of memory required per float. The top labs are quantizing down to 4 bits now (allowed with NVIDIA‚Äôs Blackwell). Some areas you need the full float position, some you don‚Äôt.",
                  "score": 50,
                  "created_utc": "2026-02-11 19:22:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u388k",
                  "author": "TheRealMasonMac",
                  "text": "FP16 is easier to train than FP8 IIRC since it's more stable. But I think Deepseek proved that you can train an equivalently performant model at FP8.\n\nEven Unsloth says it. [https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning](https://unsloth.ai/docs/get-started/reinforcement-learning-rl-guide/fp8-reinforcement-learning)\n\n\\> Research shows that FP8 training can largely match BF16 accuracy and if you serve models in FP8, **training and serving in the same precision** helps preserve accuracy. Also FP8 vs BF16 yields 1.6x higher throughput on H100s and has 2x lower memory usage.",
                  "score": 74,
                  "created_utc": "2026-02-11 17:44:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ty9zz",
                  "author": "psayre23",
                  "text": "Quick answer, 2x the size. Long answer, ask an LLM who‚Äôs smarter than me.",
                  "score": 47,
                  "created_utc": "2026-02-11 17:21:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u0jlk",
                  "author": "orbweaver-",
                  "text": "Basically even though they have close parameter counts, 685B for deepseek v3, there is twice as much data in each parameter. In effect this means that the model can be quantized more efficiently, ~~a 4bit quant for GLM5 would be \\~186GB of RAM instead of \\~342GB for Deepseek v3. It's still debatable how much this helps performance but in theory that's how it works.~~\n\nEdit: math was wrong, RAM cost is similar but the result might be better because you're drawing from more data",
                  "score": 10,
                  "created_utc": "2026-02-11 17:32:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4w0bck",
                  "author": "Complex_Signal2842",
                  "text": "Much simplified, imagine mp3. The higher the bit-rate, the better the quality of the resulting music, but also the bigger the file size. Same thing with FP16 high quality vs FP8 good quality.",
                  "score": 1,
                  "created_utc": "2026-02-11 23:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uwtnj",
              "author": "Mindless_Pain1860",
              "text": "Some rumors said that because it was trained on domestic (Chinese) AI hardware.",
              "score": 12,
              "created_utc": "2026-02-11 20:03:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vfsq4",
              "author": "yaxir",
              "text": "i wish the same for gpt 4.1!",
              "score": 1,
              "created_utc": "2026-02-11 21:34:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xvad0",
              "author": "HornyGooner4401",
              "text": "so that's why they're GPU starved and is raising the prices on their subscription",
              "score": 1,
              "created_utc": "2026-02-12 06:38:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y4qle",
                  "author": "Few_Painter_5588",
                  "text": "Indeed, Zhipu's data centres in Singapore are GPU starved HornyGooner4401",
                  "score": -1,
                  "created_utc": "2026-02-12 08:06:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tuv0z",
          "author": "michaelkatiba",
          "text": "And the plans have increased...",
          "score": 61,
          "created_utc": "2026-02-11 17:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tz0sj",
              "author": "bambamlol",
              "text": "lmao GLM-5 is only available on the $80 /month Max plan.",
              "score": 56,
              "created_utc": "2026-02-11 17:24:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uo85z",
                  "author": "AnomalyNexus",
                  "text": "I'd expect they'll roll it out to pro shortly.\n\nThe comically cheap lite plan...I wouldn't hold my breath since the plan basically spells out that it won't\n\n>Only supports GLM-4.7 and historical text models",
                  "score": 16,
                  "created_utc": "2026-02-11 19:22:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u2x37",
                  "author": "Pyros-SD-Models",
                  "text": "Buying their yearly MAX back when it was 350$ was one of the better decisions of my life. Already paid for itself a couple of times over.\n\nhttps://preview.redd.it/b315tmg1kwig1.png?width=1252&format=png&auto=webp&s=73fd58f0cd8c854d656fba0cf078f5ee3744a3f3",
                  "score": 33,
                  "created_utc": "2026-02-11 17:43:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uqwjl",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 1,
                  "created_utc": "2026-02-11 19:34:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4viwgj",
                  "author": "UnionCounty22",
                  "text": "That‚Äôs why I snagged max on Black Friday, knew I wanted access to the newest model \n\nwen served",
                  "score": 1,
                  "created_utc": "2026-02-11 21:49:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yimkc",
                  "author": "Warm_Yard_9994",
                  "text": "I can use it with my pro plan.",
                  "score": 1,
                  "created_utc": "2026-02-12 10:23:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twoce",
              "author": "epyctime",
              "text": "Had to check, wow! $10/mo for lite, $30/mo for pro, and $80/mo for max, with 10% discount for quarter and 30% for year! They say it's 77.8 on SWE-bench vs 80.9 of Opus 4.5.. with 4.6 out and Codex 5.3 smashing even 4.6 it's extremely hard to justify. Impossible, maybe.  \nFor comparison, I paid $40 for 3mo of Pro on 1/24... yes the intro deal but it's the second time I had claimed an intro deal on that account soo  \nWonder if this is to catch people on the renewals! Sneaky if so!   \n\nhaha wow you dont even get glm-5 on the coding plan unless you're on max! what the fuck!   \nCurrently, we are in the stage of replacing old model resources with new ones. Only the Max (including both new and old subscribers) newly supports GLM-5, and invoking GLM-5 will consume more plan quota than historical models. After the iteration of old and new model resources is completed, the Pro will also support GLM-5.\n\nNote: Max users using GLM-5 need to manually change the model to \"GLM-5\" in the custom configuration (e.g., ~/.claude/settings.json in Claude Code).   \n \nThe Lite / Pro plan currently does not include GLM-5 quota (we will gradually expand the scope and strive to enable more users to experience and use GLM-5). If you call GLM-5 under the plan endpoints, an error will be returned.",
              "score": 18,
              "created_utc": "2026-02-11 17:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u1rft",
                  "author": "Pyros-SD-Models",
                  "text": ">For GLM Coding Plan subscribers: Due to limited compute capacity, we‚Äôre rolling out GLM-5 to Coding Plan users gradually.\n\n>Other plan tiers: Support will be added progressively as the rollout expands.\n\nchillax you get your GLM-5.0",
                  "score": 17,
                  "created_utc": "2026-02-11 17:37:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u5ow3",
                  "author": "Caffdy",
                  "text": "> 77.8 on SWE-bench\n\nequivalent to Gemini, even",
                  "score": 2,
                  "created_utc": "2026-02-11 17:56:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twmqz",
              "author": "TheRealMasonMac",
              "text": "1. They reduced plan quota while raising prices.\n2. Their plans only advertise GLM-5 for their Max plan though they had previously guaranteed flagship models/updates for the other plans.\n3. They didn't release the base model.\n\nYep, just as everyone predicted [https://www.reddit.com/r/LocalLLaMA/comments/1pz68fz/z\\_ai\\_is\\_going\\_for\\_an\\_ipo\\_on\\_jan\\_8\\_and\\_set\\_to/](https://www.reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/)",
              "score": 23,
              "created_utc": "2026-02-11 17:13:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4txr4k",
                  "author": "Lcsq",
                  "text": "If you click on the blog link in the post, you'd see this:\n\n>For GLM Coding Plan subscribers: Due to limited compute capacity, we‚Äôre rolling out GLM-5 to Coding Plan users gradually.\n\n>Other plan tiers: Support will be added progressively as the rollout expands.\n\nYou can blame the openclaw people for this with their cache-unfriendly workloads. Their hacks like the \"heartbeat\" keepalive messages to keep the cache warm is borderline circumvention behaviour. They have to persist tens of gigabytes of KV cache for extended durations due to this behaviour. The coding plan wasn't priced with multi-day conversations in mind.",
                  "score": 38,
                  "created_utc": "2026-02-11 17:18:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uongn",
                  "author": "AnomalyNexus",
                  "text": ">They reduced plan quota while raising prices.\n\nIn fairness it was comically cheap before & didn't run out of quota if you squinted at it hard enough like claude",
                  "score": 5,
                  "created_utc": "2026-02-11 19:24:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yir0g",
                  "author": "Warm_Yard_9994",
                  "text": "I don't know what's wrong with you all, but I can use GLM-5 with my Pro subscription too.",
                  "score": 1,
                  "created_utc": "2026-02-12 10:24:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4twvsw",
              "author": "drooolingidiot",
              "text": "It's a much bigger and much more capable model. Seems fair.",
              "score": 0,
              "created_utc": "2026-02-11 17:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u2wow",
          "author": "oxygen_addiction",
          "text": "It is up on OpenRouter and Pony Alpha was removed just now, confirming it was GLM-5.\n\nSurprisingly, it is more expensive than Kimi 2.5.\n\n‚óè GLM 5 vs DeepSeek V3.2 Speciale:\n\n  \\- Input: \\~3x more expensive ($0.80 vs $0.27)\n\n  \\- Output: \\~6.2x more expensive ($2.56 vs $0.41)\n\n‚óè GLM 5 vs Kimi K2.5:\n\n  \\- Input: \\~1.8x more expensive ($0.80 vs $0.45)\n\n  \\- Output: \\~14% more expensive ($2.56 vs $2.25)\n\nedit: seems like pricing has increased further since this post",
          "score": 51,
          "created_utc": "2026-02-11 17:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ulgru",
              "author": "PangurBanTheCat",
              "text": "The Question: Is it justifiable? Does the quality of capability match the higher cost?",
              "score": 10,
              "created_utc": "2026-02-11 19:09:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v5ijv",
                  "author": "starshin3r",
                  "text": "I have the pro plan and only use it to maintain and add features to a php based shop. Never used anthropic models, but for my edge cases it's literally on par on doing it manually.\n\nBy that I mean it will write code for the backend and front-end in 10 minutes and in the next 8 hours I'll be debugging it to make it actually work.\n\nProbably pretty good for other languages, but php, especially outdated versions aren't the strongpoint of LLMs.",
                  "score": 11,
                  "created_utc": "2026-02-11 20:45:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4urjan",
              "author": "suicidaleggroll",
              "text": "> Surprisingly, it is more expensive than Kimi 2.5.\n\nAt its native precision, GLM-5 is significantly larger than Kimi-K2.5, and has more active parameters, so it's slower.  Makes sense that it would be more expensive.",
              "score": 11,
              "created_utc": "2026-02-11 19:37:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vtzf4",
              "author": "eXl5eQ",
              "text": "$2.56 is even cheaper than Gemini 3 Flash ($3). Pony Alpha is better than Gemini Flash for sure.",
              "score": 3,
              "created_utc": "2026-02-11 22:45:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zd4gr",
                  "author": "Ok_Technology_5962",
                  "text": "Have you seen the cache on Gemini 3 Flash? Both Input and output within the hour is very good (thats why I'm a bit upset as everything else would cost too much except Deepseek)",
                  "score": 1,
                  "created_utc": "2026-02-12 14:03:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4y3ebs",
              "author": "Zeeplankton",
              "text": "I really appreciate how cheap deepseek is via their api",
              "score": 2,
              "created_utc": "2026-02-12 07:54:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u3qwq",
          "author": "silenceimpaired",
          "text": "Another win for local‚Ä¶ data centers. (Sigh) \n\nHopefully we get GLM 5 Air ‚Ä¶ or lol GLM 5 Water (~300b)",
          "score": 70,
          "created_utc": "2026-02-11 17:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u95zw",
              "author": "BITE_AU_CHOCOLAT",
              "text": "Tbh, expecting a model to run on consumer hardware while being competitive with Opus 4.5 is a pipe dream. That ship has sailed",
              "score": 59,
              "created_utc": "2026-02-11 18:12:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4umvoe",
                  "author": "power97992",
                  "text": "opus 4.5 is at least 1.5T, u have to wait  ayear or more  for a smaller model to outperform it , by then they will be opus 5.6. ",
                  "score": 17,
                  "created_utc": "2026-02-11 19:15:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4un9ze",
                  "author": "SpicyWangz",
                  "text": "Honestly, a \\~200b param model that performs at the level of Sonnet 4.5 would be amazing",
                  "score": 12,
                  "created_utc": "2026-02-11 19:17:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4udlb1",
                  "author": "silenceimpaired",
                  "text": "I don‚Äôt want it competitive with Opus. I want it to be the best my hardware can do locally, and I think there is room for improvement still that is being ignored in favor of quick wins. I don‚Äôt fault them. I‚Äôm just a tad sad.",
                  "score": 28,
                  "created_utc": "2026-02-11 18:32:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vd96j",
                  "author": "JacketHistorical2321",
                  "text": "512gb of system RAM and 2 mi60s will allow for a q4 and that's plenty accessible. Got my rig set up with a threadripper pro < $2000 all in.¬†",
                  "score": 4,
                  "created_utc": "2026-02-11 21:22:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vuu40",
              "author": "Prestigious-Use5483",
              "text": "I'll take GLM-5 Drops (60-120b)",
              "score": 3,
              "created_utc": "2026-02-11 22:49:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w470z",
                  "author": "silenceimpaired",
                  "text": "lol GLM 5 mist to be released soon",
                  "score": 3,
                  "created_utc": "2026-02-11 23:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v35qa",
              "author": "DerpSenpai",
              "text": "These BIG models are then used to create the small ones. So now someone can create GLM-5-lite that can run locally\n\n  \n\\>A ‚Äúdistilled version‚Äù of a model refers to a process in machine learning called knowledge distillation. It involves taking a large, complex model (called the teacher model) and transferring its knowledge into a smaller, more efficient model (called the student model).The distilled model is trained to mimic the predictions of the larger model while maintaining much of its accuracy. The main benefits of distilled models are that they: 1. Require fewer resources: They are smaller and faster, making them more efficient for deployment on devices with limited computational power. 2. Preserve performance: Despite being smaller, distilled models often perform nearly as well as their larger counterparts. 3. Enable scalability: They are better suited for real-world applications that need to handle high traffic or run on edge devices.",
              "score": 3,
              "created_utc": "2026-02-11 20:33:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w3rea",
                  "author": "silenceimpaired",
                  "text": "I‚Äôm aware of this concept, but I worry this practice is being abandoned because it doesn‚Äôt help the bottom line.\n\nI suspect in the end we will have releases that need a a mini datacenter and those that work on edge devices like laptops and cell phones. \n\nThe power users will be abandoned.",
                  "score": 5,
                  "created_utc": "2026-02-11 23:37:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ttkqe",
          "author": "Then-Topic8766",
          "text": "https://preview.redd.it/pv5yr6z6cwig1.png?width=1200&format=png&auto=webp&s=ec6d3a4bef8c300b0700d06b030353b136763266\n\n",
          "score": 80,
          "created_utc": "2026-02-11 16:59:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uqp3w",
              "author": "suicidaleggroll",
              "text": "Unsloth's quantized ggufs are up",
              "score": 10,
              "created_utc": "2026-02-11 19:33:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v12d8",
                  "author": "twack3r",
                  "text": "And then taken down again as of now except for Q4 and Q8",
                  "score": 3,
                  "created_utc": "2026-02-11 20:23:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tw3oj",
              "author": "mikael110",
              "text": "Well there is already a [Draft PR](https://github.com/ggml-org/llama.cpp/pull/19460) so hopefully it won't be too long. Running such a beast locally will be a challenge though. ",
              "score": 19,
              "created_utc": "2026-02-11 17:11:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4twmqm",
                  "author": "Then-Topic8766",
                  "text": "Yeah, it seams we must wait for some Air...",
                  "score": 5,
                  "created_utc": "2026-02-11 17:13:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uaz0x",
              "author": "Undead__Battery",
              "text": "This one is up with no Readme yet:  [https://huggingface.co/unsloth/GLM-5-GGUF](https://huggingface.co/unsloth/GLM-5-GGUF)  ....And the Readme is online now.",
              "score": 6,
              "created_utc": "2026-02-11 18:20:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4v0hkk",
                  "author": "Then-Topic8766",
                  "text": "Damn! I have 40 GB VRAM and 128 GB DDR5. The smallest quant is GLM-5-UD-TQ1\\_0.gguf - 174 GB. I will stick with GLM-4-7-q2...",
                  "score": 3,
                  "created_utc": "2026-02-11 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u2rxi",
          "author": "InternationalNebula7",
          "text": "Now I need GLM-5 Flash!",
          "score": 15,
          "created_utc": "2026-02-11 17:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ucxpy",
          "author": "Frisiiii",
          "text": "1.5TB?????\n*sigh* Time to dust of my 3080 10gb",
          "score": 13,
          "created_utc": "2026-02-11 18:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u1cl6",
          "author": "Demien19",
          "text": "End of 2026 gonna be insane for sure, competition is strong.  \nTho the prices are not that good :/ rip ram market",
          "score": 19,
          "created_utc": "2026-02-11 17:35:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ucz8i",
          "author": "MancelPage",
          "text": ">Scaling is still one of the most important ways to improve the intelligence efficiency of Artificial General Intelligence (AGI)\n\nWait, what? I don't keep up with the posts here, I just dabble with AI stuff and loosely keep updated about it in general, but since when are we calling any AI models AGI?\n\nBecause they aren't.\n\nThat's a future possibility. It likely isn't even possible to reach AGI with the limitations of a LLM - purely linear thinking based on most statistically likely next word. Humans, the AGI tier thinkers that we are, do not think linearly. I don't think anything that has such a narrow representation of intelligence (albeit increasingly optimized one) can reach AGI. It certainly hasn't now, in any case. Wtf.",
          "score": 18,
          "created_utc": "2026-02-11 18:29:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ue5wz",
              "author": "TheRealMasonMac",
              "text": "It's the current decade's, \"blockchain.\"",
              "score": 18,
              "created_utc": "2026-02-11 18:35:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wdwgp",
              "author": "dogesator",
              "text": "Depends on your definition, the definition you‚Äôre using is obviously not the definition they‚Äôre using. general in this context is meaning that it is a general model that can be used in multiple different domains and a large variety of tasks with a single neural network, as opposed to something like alphafold designed for specifically protein folding only, or something like SAM that is specifically for segmenting images.\n\nOfcourse they aren‚Äôt saying it can do every job and every task in the world, just that the model is general purpose across many domains of knowledge and many tasks.",
              "score": 2,
              "created_utc": "2026-02-12 00:35:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wiril",
                  "author": "MancelPage",
                  "text": ">general in this context is meaning that it is a general model that can be used in multiple different domains and a large variety of tasks\n\nLLMs have met that definition for a long time now. Since 2023 at least? Sure it's far better now, especially context length (also tool use, agentic stuff aka workflows), but strictly speaking it met that definition then. They weren't considered AGI back when they first met that definition, not even by the marketers of ChatGPT etc. So why the change?\n\nWhat I'm hearing is that there haven't been any fundamental changes since then, some folks just started calling it AGI at some point so investors would invest more.",
                  "score": 3,
                  "created_utc": "2026-02-12 01:04:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wqd69",
              "author": "Alarming_Turnover578",
              "text": "LLM can answer any question, thats why it is AGI. (Answer of course most likely would be wrong for complex questions. But its minor technical detail uninteresting to investors.)",
              "score": 0,
              "created_utc": "2026-02-12 01:51:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wvexd",
                  "author": "MancelPage",
                  "text": "Chatbots have been able to answer any question since the very first chatbots if you're using strokes that broad. Turns out Eliza was AGI all along!\n\nBut even LLMs weren't considered AGI when they first came out, during which time they were also capable of attempting any question.",
                  "score": 6,
                  "created_utc": "2026-02-12 02:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u6qmq",
          "author": "FUS3N",
          "text": "Man in these graphs why can't the competitor bar's be more distinguishable colors, i get why they do it but like still",
          "score": 9,
          "created_utc": "2026-02-11 18:00:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uwjbq",
              "author": "adeukis",
              "text": "running out of colors ",
              "score": 6,
              "created_utc": "2026-02-11 20:01:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4txck4",
          "author": "Revolaition",
          "text": "Benchmarks look promising, will be interesting to test how it works for coding in real life compared to opus 4.6 and codex 5.3",
          "score": 5,
          "created_utc": "2026-02-11 17:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6xjt",
              "author": "Party_Progress7905",
              "text": "I Just tested. Comparable to sonnet 4. Those benches look sus",
              "score": 7,
              "created_utc": "2026-02-11 18:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uzo7o",
                  "author": "BuildAISkills",
                  "text": "Yeah, I don't think GLM 4.7 was as great as they said it was. But I'm just one guy, so who knows ü§∑",
                  "score": 1,
                  "created_utc": "2026-02-11 20:16:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v0f7j",
          "author": "Lissanro",
          "text": "Wow, BF16 weights! It would be really great if GLM eventually adopt 4-bit QAT releases like Kimi did. I see that I am not alone who thought of this: [https://huggingface.co/zai-org/GLM-5/discussions/4](https://huggingface.co/zai-org/GLM-5/discussions/4) . Still, great release! But I have to wait for GGUF quants before I can give it a try myself.",
          "score": 6,
          "created_utc": "2026-02-11 20:20:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3wbl",
          "author": "AnomalyNexus",
          "text": "Congrats to team on what looks to be a great release, especially one with a favourable license!\n\nBusy playing with it on coding plan and so far it seems favourable. Nothing super quantifiable but vibe:\n\n* Faster - to be expected I guess given only Max has access\n* Longer running thinking & more interleaved thinking and doing\n* It really likes making lists. Same for presenting things visually in block diagrams and lists. Opencode doesn't seem to always read the tables as tables right though so there must be some formatting issue there\n* More thinking style backtracking thought patterns (\"Actually, wait - I need to be careful\")\n* Seems to remember things from much earlier better. e.g. tried something, it failed. Then added some features and at end it decided on its own to retry the earlier thing again having realised the features are relevant to failure case\n\nKeen to see how it does on rust. Was pretty happy with 4.7 already in general but on rust specifically sometimes it dug itself into a hole\n\nOverall definitely a solid improvement :)",
          "score": 5,
          "created_utc": "2026-02-11 20:37:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ttqxx",
          "author": "mtmttuan",
          "text": "Cool. Not that it can be run locally though. At least we're going to have decent smaller models.",
          "score": 9,
          "created_utc": "2026-02-11 17:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4txhfk",
              "author": "segmond",
              "text": "It can be run locally and some of us will be running it, with a lot of patience to boost. ",
              "score": 15,
              "created_utc": "2026-02-11 17:17:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u9foh",
                  "author": "Pyros-SD-Models",
                  "text": "Good thing about this ‚Äúrun locally‚Äù play is that once it finally finishes processing the prompt I gave it, GLM-6 will already be released üòé",
                  "score": 10,
                  "created_utc": "2026-02-11 18:13:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyz0i",
          "author": "equanimous11",
          "text": "Will they release a flash model?",
          "score": 3,
          "created_utc": "2026-02-11 17:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u22q5",
          "author": "Orolol",
          "text": "If real world exp√©riences match the benchmarks, which is always hard to tell without extensive usage, it's a wonderful release. It means that open source models are barely a couple of months behind models",
          "score": 3,
          "created_utc": "2026-02-11 17:39:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u4dsf",
          "author": "Caffdy",
          "text": "what's the context length?",
          "score": 3,
          "created_utc": "2026-02-11 17:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uyqnu",
              "author": "akumaburn",
              "text": "Not sure but at least 200K \n\nhttps://preview.redd.it/1ebjgy9oaxig1.png?width=1418&format=png&auto=webp&s=c656b5d6789e0c231ef6d2e0388765bd4ec57cdb\n\n",
              "score": 5,
              "created_utc": "2026-02-11 20:12:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w0tjz",
                  "author": "eXl5eQ",
                  "text": "Should be 200K because it was what Pony Alpha had on OpenRouter. IIRC.\n\n---\nEdit:\n\nGLM 5 is now officially available on OpenRouter. Its context size is 202.8K.",
                  "score": 3,
                  "created_utc": "2026-02-11 23:21:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4u1riu",
          "author": "bick_nyers",
          "text": "I hope it's not too thicc for Cerebras to deploy",
          "score": 2,
          "created_utc": "2026-02-11 17:37:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u3bb2",
          "author": "Revolaition",
          "text": "Its live on HF now",
          "score": 2,
          "created_utc": "2026-02-11 17:45:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4um3ds",
          "author": "power97992",
          "text": "wow, it is more than double the price of glm 4.7...",
          "score": 2,
          "created_utc": "2026-02-11 19:11:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v2n6j",
          "author": "Lopsided_Dot_4557",
          "text": "This model is redefining agentic AI, coding & systems engineering. I did a review and testing video and really loved the capabilities:\n\n[https://youtu.be/yAwh34CSYV8?si=NtgkCyGVRrYDApHA](https://youtu.be/yAwh34CSYV8?si=NtgkCyGVRrYDApHA)\n\nThanks.",
          "score": 2,
          "created_utc": "2026-02-11 20:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vcont",
          "author": "AppealSame4367",
          "text": "It's a very good model, great work!\n\nBut just as 2% difference between gpt, gemini vs opus mean a lot, those 2% missing to opus also makes a world of difference for glm 5.\n\nIt's much much better already, but Opus is still far ahead in real scenarios and able to do more things at once in one request.",
          "score": 2,
          "created_utc": "2026-02-11 21:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vtk21",
          "author": "Right-Law1817",
          "text": "Good benchmarks but coding plans sucks tbh!",
          "score": 2,
          "created_utc": "2026-02-11 22:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vwxlb",
          "author": "Aware_Studio1180",
          "text": "fantastic, now I can't run the new model locally dammit.",
          "score": 2,
          "created_utc": "2026-02-11 23:00:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zsct3",
          "author": "Merlin_M_O",
          "text": "At 744B parameters, \"Agentic Engineering\" is just marketing speak for \"the model is now smart enough to plan the heist for the H100s it needs to run locally\"",
          "score": 2,
          "created_utc": "2026-02-12 15:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zzm6k",
          "author": "LA_rent_Aficionado",
          "text": "Exciting stuff and very impressive however it is a bit disappointing this went from locally achievable with decent quality and speed at <400GB VRAM to joining the ranks of K2.5 in terms of hardware requirements.  A near doubling of size for marginal improvements vs. 4.7 seems almost regressive.  ",
          "score": 2,
          "created_utc": "2026-02-12 15:56:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o505rku",
          "author": "tracagnotto",
          "text": "yeah lol a 1,51 TB monster that requires a factory to run.  \nWhat a great innovation!  \nWe are going exactly in the opposite direction in which AI should go.\n\nInstead of optimizing the existing AI like maniacs to consume the least possible amount of resources we keep pumping in more parameters and more size and more GPU requirements.\n\nDid they ever realized that Moore's Law is not working anymore?",
          "score": 2,
          "created_utc": "2026-02-12 16:24:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4trhmo",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-02-11 16:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4trrnm",
              "author": "ResearchCrafty1804",
              "text": "The links should be working soon",
              "score": 6,
              "created_utc": "2026-02-11 16:50:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ugue1",
          "author": "KvAk_AKPlaysYT",
          "text": "Guf-Guf... *744B*... NVM :(",
          "score": 2,
          "created_utc": "2026-02-11 18:47:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uohfj",
          "author": "johnrock001",
          "text": "Good luck in getting more customers with the massive price increase.",
          "score": 3,
          "created_utc": "2026-02-11 19:23:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uy3gm",
              "author": "akumaburn",
              "text": "They are probably running it at a massive loss like other AI inference companies do even with the price hike.  Maybe its a psychological play to slowly raise the price over time?",
              "score": 5,
              "created_utc": "2026-02-11 20:09:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4uyemz",
                  "author": "johnrock001",
                  "text": "most likely!",
                  "score": 1,
                  "created_utc": "2026-02-11 20:10:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4upez4",
          "author": "Septerium",
          "text": "Double the size, increase a few % in the most relevant benchmarks and learn a few new benchmarks you didn't know before. Nice!",
          "score": 3,
          "created_utc": "2026-02-11 19:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4twyha",
          "author": "HarjjotSinghh",
          "text": "glm-5 aced my last exam (and broke vending bench).",
          "score": 2,
          "created_utc": "2026-02-11 17:15:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vbhgk",
          "author": "harlekinrains",
          "text": "Picks M83 Midnight City as the default music player song in \"create an OS\" test. (see: https://www.youtube.com/watch?v=XgVWI8bNt6k)\n\nBrain explodes.\n\nAPPROVED! :)\n\nHere is the music video in case you havent seen it before:\nhttps://www.youtube.com/watch?v=dX3k_QDnzHE",
          "score": 2,
          "created_utc": "2026-02-11 21:14:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tzg43",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 4,
          "created_utc": "2026-02-11 17:26:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2202",
              "author": "AdIllustrious436",
              "text": "I cancelled instantly. Even Anthropic serves their flagship on their lite plan. What a joke.",
              "score": 8,
              "created_utc": "2026-02-11 17:39:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u1wo7",
          "author": "Swimming_Whereas8123",
          "text": "Eagerly waiting for someone to upload a nvfp4 variant. ",
          "score": 1,
          "created_utc": "2026-02-11 17:38:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v0b90",
          "author": "Infamous_Sorbet4021",
          "text": "Glm team, please improve the  speed of model generation.  It it even solwer than 4.7",
          "score": 1,
          "created_utc": "2026-02-11 20:19:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vyr9h",
          "author": "OliwerPengy",
          "text": "whats the context window size?",
          "score": 1,
          "created_utc": "2026-02-11 23:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wud27",
          "author": "s1mplyme",
          "text": "Ooh, I'm excited for the 30B Flash version!",
          "score": 1,
          "created_utc": "2026-02-12 02:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wznwn",
          "author": "Kahvana",
          "text": "I appriciate that they include their old model in there too for reference.",
          "score": 1,
          "created_utc": "2026-02-12 02:46:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xoek2",
          "author": "jatinkrmalik",
          "text": "Turned out it was the pony after all",
          "score": 1,
          "created_utc": "2026-02-12 05:38:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xu8rr",
          "author": "himefei",
          "text": "Would there be a GLM 5 flash/air LOL",
          "score": 1,
          "created_utc": "2026-02-12 06:29:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xv5jr",
          "author": "Accomplished_Ad9530",
          "text": "Why does the HLE w/tools benchmark row have an asterisk for the frontier models that says \"\\*: refers to their scores of full set.\" Does that mean that Zai/GLM, DeepSeek, and Kimi all are benching only a subset of HLE?\n\nhttps://preview.redd.it/r38ltbdnd0jg1.png?width=1468&format=png&auto=webp&s=9ae2ea4cfc72fe328041a0a0e70c16c7b4582d60\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-12 06:37:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ymu6x",
              "author": "Maddolyn",
              "text": "What's HLE?",
              "score": 1,
              "created_utc": "2026-02-12 11:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y6x7q",
          "author": "Sad-Ease-7756",
          "text": "another red alert for openai ü§£",
          "score": 1,
          "created_utc": "2026-02-12 08:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ycibt",
          "author": "TheFarage",
          "text": "Congrats to the Zhipu team on a technically impressive release. The race to capabilities is running. The race to safety needs to keep pace.",
          "score": 1,
          "created_utc": "2026-02-12 09:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ymm3p",
          "author": "No_Count2837",
          "text": "Crazy ü•≥",
          "score": 1,
          "created_utc": "2026-02-12 11:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u9hn3",
          "author": "Iory1998",
          "text": "I think China already is better than the US in the AI space, and I believe that the open-source models are also better than Gemini, GPT, and Claude. If you think about it, the usual suspects are no longer single models. They work as a system of models leveraging the power of agentic frameworks. Therefore, comparing a single model to a framework is comparing apples to oranges. ",
          "score": 0,
          "created_utc": "2026-02-11 18:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uhnjd",
              "author": "alexeiz",
              "text": "Are you paying for Chinese models yet?  Let's see how you vote with your wallet.",
              "score": -6,
              "created_utc": "2026-02-11 18:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vz52p",
                  "author": "Iory1998",
                  "text": "I use Chinese models and I don't pay a dime.",
                  "score": 3,
                  "created_utc": "2026-02-11 23:12:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4whn4z",
                  "author": "the_shadowmind",
                  "text": "I use openrouter to pay per token, and use more Chinese models.",
                  "score": 3,
                  "created_utc": "2026-02-12 00:57:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ui4wl",
          "author": "mizoTm",
          "text": "Damn son",
          "score": 1,
          "created_utc": "2026-02-11 18:53:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4twi4s",
          "author": "Odd-Ordinary-5922",
          "text": "crazy how close its gotten... Makes me think that all the US companies are holding up on huge models ",
          "score": -1,
          "created_utc": "2026-02-11 17:12:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2rd5",
              "author": "oxygen_addiction",
              "text": "Or there is no moat.",
              "score": 23,
              "created_utc": "2026-02-11 17:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4u4fb0",
          "author": "Insomniac24x7",
          "text": "But will it run on an RPi and will it run Doom?!?!",
          "score": 0,
          "created_utc": "2026-02-11 17:50:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzjbw2",
      "title": "I built a rough .gguf LLM visualizer",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qzjbw2",
      "author": "sultan_papagani",
      "created_utc": "2026-02-08 20:08:31",
      "score": 707,
      "num_comments": 43,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4doqm0",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-09 04:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b8mbk",
          "author": "Educational_Sun_8813",
          "text": "maybe someone will be interested to see the code: https://github.com/Sultan-papagani/gguf-visualizer/tree/main\n\nbesides i'm aware of this: https://poloclub.github.io/transformer-explainer/",
          "score": 33,
          "created_utc": "2026-02-08 20:22:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cji5q",
              "author": "jklre",
              "text": "to the top!",
              "score": 3,
              "created_utc": "2026-02-09 00:33:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bbx41",
          "author": "DisjointedHuntsville",
          "text": "Really good job and thank you for taking the time to share :) I believe neuron pedia from Anthropic which is open source now is also a good contribution to explainability approaches: [https://www.neuronpedia.org/gemma-2-2b/graph?slug=nuclearphysicsis-1766322762807&pruningThreshold=0.8&densityThreshold=0.99](https://www.neuronpedia.org/gemma-2-2b/graph?slug=nuclearphysicsis-1766322762807&pruningThreshold=0.8&densityThreshold=0.99)\n\nWe have certainly not begun to scratch the surface of explainability in these models just yet and please keep sharing all the cool things you discover with the community since it really helps when there are more eyes on this stuff !",
          "score": 61,
          "created_utc": "2026-02-08 20:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bej4j",
              "author": "JEs4",
              "text": "Just pointing out that Neuronpedia isn‚Äôt by Anthropic. They‚Äôre a contributor but this guy is behind it: https://www.johnnylin.co/",
              "score": 32,
              "created_utc": "2026-02-08 20:52:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bo9im",
                  "author": "IrisColt",
                  "text": "This.",
                  "score": -6,
                  "created_utc": "2026-02-08 21:40:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4bezdg",
              "author": "JEs4",
              "text": "Whoops didn‚Äôt mean to double post. But yeah Neuonpedia is really neat. Using SAE models with their lookups was helpful during my abliteration research.\n\nhttps://preview.redd.it/lm0br8493cig1.jpeg?width=2755&format=pjpg&auto=webp&s=9d2a21eb771b93c17af06a0cfef9ec6fcc99d30c",
              "score": 20,
              "created_utc": "2026-02-08 20:54:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bgy0w",
                  "author": "sultan_papagani",
                  "text": "this is really cool. thanks!",
                  "score": 5,
                  "created_utc": "2026-02-08 21:03:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b5zwx",
          "author": "sultan_papagani",
          "text": "[website link](https://sultan-papagani.github.io/gguf-visualizer/)",
          "score": 16,
          "created_utc": "2026-02-08 20:09:02",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4c3kn6",
              "author": "AbheekG",
              "text": "Thanks so much for sharing!",
              "score": 4,
              "created_utc": "2026-02-08 23:01:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b7ucq",
          "author": "[deleted]",
          "text": "Cool.¬†",
          "score": 7,
          "created_utc": "2026-02-08 20:18:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4c5djr",
          "author": "o0genesis0o",
          "text": "Cool work! \n\nWould it be possible to, say, capture the activations of a run and playback to see the connections lighting up? My colleague has been fantasizing about some sorts of VR that allows him to sit and see the neural network lighting up as the token being processed. He imagined it would help with explainability. ",
          "score": 3,
          "created_utc": "2026-02-08 23:11:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ckr2p",
          "author": "Every_Abalone5692",
          "text": "Awesome work!",
          "score": 3,
          "created_utc": "2026-02-09 00:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ehgc4",
          "author": "Chromix_",
          "text": "A few months ago someone built something that doesn't just visualize it statically, but dynamically shows patterns and connections with activations. Here's one of the [earlier versions](https://www.reddit.com/r/LocalLLaMA/comments/1poybe9/llama_32_3b_mri_build_progress/). There were a bunch more investigative posts where the author used the extended tool to find and visualize patterns, like nodes being responsible for certain things, or being more sensitive to quantization. Unfortunately the account was deleted recently, making it difficult to find all the latest posts on that.\n\nSo, visualizing static properties clearly has its benefits, and another take at the dynamic visualization could also yield nice results.",
          "score": 3,
          "created_utc": "2026-02-09 08:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pfsi3",
              "author": "AnLuoRidge",
              "text": "I used ‚ÄúfMRI‚Äù as the keyword to find more of those posts. Turns out this comment might be the reason of author‚Äôs account deletion? https://www.reddit.com/r/LocalLLaMA/s/ADTr4lKI5N",
              "score": 3,
              "created_utc": "2026-02-10 23:24:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4svo73",
                  "author": "Chromix_",
                  "text": "Interesting find. Yes, the approach also seemed a little untargeted to me, but the author seemingly made up for that by putting a lot of time into it. There were some findings that looked interesting. I was waiting for this to end up in a more definitive pattern to look into it in more detail, to see if those findings were real. No we'll never know. Well, someone else might pick that back up somewhen.",
                  "score": 1,
                  "created_utc": "2026-02-11 14:13:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4bfblq",
          "author": "RoyalCities",
          "text": "This is very cool! Love visualizers like this. Would like to see if you could support other model types down the line but as is this is fantastic.\n\nOutside of just llms I mean. Like Image, video or audio models etc. where it's not all unified but it's say a t5 separately connecting to a Unet or DiT via cross attention. Maybe showing those connections and all that from a high level.\n\nNonetheless great work.",
          "score": 4,
          "created_utc": "2026-02-08 20:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bfsgj",
          "author": "MelodicRecognition7",
          "text": "cool!",
          "score": 2,
          "created_utc": "2026-02-08 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bk989",
          "author": "thatguy122",
          "text": "Love this. Reminds me a cyberpunk-esk hacking mini game.¬†",
          "score": 2,
          "created_utc": "2026-02-08 21:20:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bobp0",
          "author": "IrisColt",
          "text": "Thanks!!! I love it!",
          "score": 2,
          "created_utc": "2026-02-08 21:40:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bt34m",
          "author": "scottgal2",
          "text": "Awesome job!",
          "score": 2,
          "created_utc": "2026-02-08 22:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cs6u8",
          "author": "paul_tu",
          "text": "Upvote for an effort",
          "score": 2,
          "created_utc": "2026-02-09 01:22:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eoab7",
          "author": "clawdvine-intern",
          "text": "oh man ive been wanting something like this forever. i always feel like im just blindly throwing quant levels at gguf files and hoping for the best lol. being able to actually see whats going on inside would be huge for figuring out why certain layers just tank quality when you go below Q5. is there any way to compare two files side by side? like original vs quantized? that would be the dream tbh",
          "score": 2,
          "created_utc": "2026-02-09 09:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eug2r",
          "author": "renntv",
          "text": "Development of AI is so fast, but visualization to help explain what's happening are really lacking. I collect everything I can find that helps people to better understand the AI black box here: [https://dentro.de/ai/visualizations/](https://dentro.de/ai/visualizations/)  \nBrendan Bycroft is the GOAT, but his project is already 2 years old and not much emerged after it.   \nGreat to see the subject pop up again and your way of visualizing is pretty clever!",
          "score": 2,
          "created_utc": "2026-02-09 10:34:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wwdzs",
          "author": "s1mplyme",
          "text": "This is neat.  Seeing the size of the visualization jump between models helps my poor meat brain get a better grasp on vast differences in scale.",
          "score": 2,
          "created_utc": "2026-02-12 02:27:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bf1lo",
          "author": "SlowFail2433",
          "text": "Visualisation looks nice",
          "score": 2,
          "created_utc": "2026-02-08 20:54:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4btt3k",
              "author": "harrro",
              "text": "Yep, worked well on a 1.5B GGUF model I just tested.\n\n/u/sultan_papagani The 'walk' mode is super fast on my Firefox browser - i just barely touch the WSAD keys and it flies across the screen (sprint mode is even worse) which made it hard to move around though.\n\nNot sure if its because it was a small model or because my framerate is really high (ie: you're moving X units per tick and I'm well over 60fps) or just a Firefox thing.",
              "score": 3,
              "created_utc": "2026-02-08 22:08:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4buo5m",
                  "author": "sultan_papagani",
                  "text": "thanks! use scrollwheel to slow down",
                  "score": 3,
                  "created_utc": "2026-02-08 22:12:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cpu58",
          "author": "ANR2ME",
          "text": "Interesting project üëç i wished there are more variation of models, like MoE or hybrid models with mamba layers (said to be more sensitive to quantization) for example.\n\n~~Btw, are you planning to open source this project later? ü§î~~\n\nEdit: is this the repo? https://github.com/bbycroft/llm-viz",
          "score": 1,
          "created_utc": "2026-02-09 01:09:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cq5is",
              "author": "sultan_papagani",
              "text": "thanks for the feedback! [repo link](https://github.com/Sultan-papagani/gguf-visualizer)",
              "score": 3,
              "created_utc": "2026-02-09 01:10:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dcsyd",
          "author": "Much-Researcher6135",
          "text": "That's sick, can you tell a bit about how you made it? I'm getting more and more interested in 3d dataviz and have no idea where to look for pointers.",
          "score": 1,
          "created_utc": "2026-02-09 03:12:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dlepo",
          "author": "Alarming_Bluebird648",
          "text": "Mapping the tensor dimensions visually makes it much easier to verify layer architecture than scanning through metadata strings. Do you plan on adding support for inspecting weight distribution histograms per layer?",
          "score": 1,
          "created_utc": "2026-02-09 04:03:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e42ak",
          "author": "FeiX7",
          "text": "make it mouse controlled instead of keyboard please.",
          "score": 1,
          "created_utc": "2026-02-09 06:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eb4mx",
          "author": "HarjjotSinghh",
          "text": "this is either a masterpiece or a glitch. both equally impressive.",
          "score": 1,
          "created_utc": "2026-02-09 07:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b7n5o",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -9,
          "created_utc": "2026-02-08 20:17:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4b824p",
              "author": "sultan_papagani",
              "text": "[repo link](https://github.com/Sultan-papagani/gguf-visualizer)",
              "score": 9,
              "created_utc": "2026-02-08 20:19:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4b7uqo",
              "author": "sultan_papagani",
              "text": "its offline. github pages, just simple html and js that runs on your browser. you can download it too",
              "score": 9,
              "created_utc": "2026-02-08 20:18:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4bdehd",
              "author": "o5mfiHTNsH748KVq",
              "text": "I can‚Äôt answer for OP, but I do this because, frankly, I need some fodder on my website for jobs/hiring people that look at my vanity url when I apply.\n\nGotta play the game a little bit. At least they released it as open source :)",
              "score": 6,
              "created_utc": "2026-02-08 20:46:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4bej2d",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -3,
                  "created_utc": "2026-02-08 20:52:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4cc5bg",
              "author": "666666thats6sixes",
              "text": "It doesn't, though? It only reads the gguf header, which is up to tens MiB (not \"a few hundred kilobytes\") in size depending on the size of the kv arrays, it stops reading once the header has been parsed.\n\nTried it with BF16 GLM-4.7, it read just 9466496 bytes, because that's how large the header is.",
              "score": 2,
              "created_utc": "2026-02-08 23:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ccmqo",
                  "author": "FullstackSensei",
                  "text": "OK, mea culpa",
                  "score": 1,
                  "created_utc": "2026-02-08 23:54:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2e8mp",
      "title": "#SaveLocalLLaMA",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/0memizzegyig1.jpeg",
      "author": "ForsookComparison",
      "created_utc": "2026-02-12 00:07:52",
      "score": 658,
      "num_comments": 98,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r2e8mp/savelocalllama/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4yzmr7",
          "author": "ArcaneThoughts",
          "text": "Do you have any feedback for the mod team regarding these issues?",
          "score": 1,
          "created_utc": "2026-02-12 12:42:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w9yb3",
          "author": "EiwazDeath",
          "text": "The \"broken markdown in a reddit post\" one hits too close to home. Also missing: \"I asked my 3B model to write an OS and it only crashed twice\"",
          "score": 160,
          "created_utc": "2026-02-12 00:12:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wa8yq",
              "author": "ForsookComparison",
              "text": "*[The OS is a navbar in a web browser that vaguely resembles a start menu]*",
              "score": 95,
              "created_utc": "2026-02-12 00:14:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xgvea",
                  "author": "frozen_tuna",
                  "text": "*after some (allegedly) light tinkering by the user to get it to even compile*",
                  "score": 6,
                  "created_utc": "2026-02-12 04:40:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4watr4",
                  "author": "EiwazDeath",
                  "text": "Lmao accurate. At least it has a start menu, that's more than some Linux distros can say",
                  "score": 17,
                  "created_utc": "2026-02-12 00:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wi84j",
              "author": "see_spot_ruminate",
              "text": "Fuck I just tried to ask for a sticky in a new post and it got removed by the mods (maybe automod)... So maybe it is by design... \n\n\nMy exact post:\n\nWe need a sticky of some sort to clean up this subreddit\n\nDiscussion (self.LocalLLaMA)\n\nsubmitted 5 hours ago by see_spot_ruminate\n\nHello all,\n\nThis subreddit has become awash with multiple repeating topics of \"what should I do\" and other things. We need a sticky. What would everyone want in a sticky?\n\nedit: Maybe I should have asked it in meme form??",
              "score": 5,
              "created_utc": "2026-02-12 01:01:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z5gpx",
                  "author": "YoAmoElTacos",
                  "text": "Add some broken markdown next time.",
                  "score": 5,
                  "created_utc": "2026-02-12 13:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wo5gg",
              "author": "llama-impersonator",
              "text": "ah yes, \"operating systems\"",
              "score": 6,
              "created_utc": "2026-02-12 01:37:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50cdw9",
              "author": "LtCommanderDatum",
              "text": "To be fair, if by \"it\" you mean the OS, it's already doing better than Microsoft.",
              "score": 1,
              "created_utc": "2026-02-12 16:55:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wht70",
          "author": "Weird-Consequence366",
          "text": "39 emojis in a two paragraph post",
          "score": 82,
          "created_utc": "2026-02-12 00:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4x0haa",
              "author": "davidy22",
              "text": "People writing like it's linkedin on reddit",
              "score": 30,
              "created_utc": "2026-02-12 02:51:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zd1tu",
                  "author": "SkyFeistyLlama8",
                  "text": "Yeah really what is with that LinkedIn or Medium style? It assumes people are idiots who can't read regular text without emojifying everything.\n\nMaybe people *are* idiots.",
                  "score": 3,
                  "created_utc": "2026-02-12 14:02:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o50cmrf",
              "author": "LtCommanderDatum",
              "text": "Why do the LLMs love emojis so much?\n\nWhat trove of data written by 10 year old girls were these LLMs trained on?",
              "score": 1,
              "created_utc": "2026-02-12 16:56:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o500wr1",
              "author": "Shawnj2",
              "text": "Every community around AI is filled with AI slop, in this one it‚Äôs both writing and code. Just because AI can do everything doesn‚Äôt mean you should replace your brain with it. The tech is neat but if you want me to read what you have to say you can be bothered to write it yourself or at least edit what the AI generated to have some sort of original voice. Also I‚Äôm sick of every project having a crappy looking AI generated icon, if you want your product to have an icon you can be bothered to draw it yourself or at least generate an icon that looks good. Or maybe just‚Ä¶don‚Äôt have an icon\n\nllama.cpp is great and the further you get from it the closer it is to slop",
              "score": 1,
              "created_utc": "2026-02-12 16:02:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wab22",
          "author": "Xamanthas",
          "text": "Also missing AA ranking posts, bot accounts mentioning a certain astroturfed repo made by an idiot and clickbait claims",
          "score": 49,
          "created_utc": "2026-02-12 00:14:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wsao0",
              "author": "ForsookComparison",
              "text": "*'i found this cool new-..\"*",
              "score": 29,
              "created_utc": "2026-02-12 02:02:55",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4wrfmq",
              "author": "FPham",
              "text": "What? I'm an idot and my click baits are barely click bites. ",
              "score": 4,
              "created_utc": "2026-02-12 01:57:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xuhil",
                  "author": "llama-impersonator",
                  "text": "just get sydney to write your posts, problem solved",
                  "score": 4,
                  "created_utc": "2026-02-12 06:31:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4zb800",
              "author": "randylush",
              "text": "I‚Äôm really curious what this repo is now lol",
              "score": 1,
              "created_utc": "2026-02-12 13:52:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4zdocq",
                  "author": "Xamanthas",
                  "text": "Think about whats been *the* most astroturfed topic in the last 2 months on this sub\n\nI refuse to name it because I despise all of the people involved in it.",
                  "score": 2,
                  "created_utc": "2026-02-12 14:06:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4woiso",
          "author": "Plastic-Ordinary-833",
          "text": "missing the classic \"just use ollama\" reply on every single post regardless of context lmao. also the weekly \"is X model better than Y\" where every answer is just \"depends on your use case\"",
          "score": 45,
          "created_utc": "2026-02-12 01:40:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wt1q1",
          "author": "ttkciar",
          "text": "I'm guessing the folks who see a lot of spam are sorting by \"new\" and check the sub more frequently than the moderators, and the folks who only see a little spam are sorting by \"top\" or \"best\" and/or only looking at the sub after moderators have had a chance to clean house.\n\nLooking through the sub's moderation log, moderators removed 55 posts/comments in the last nine hours.\n\ntl;dr: There is a ***lot*** of spam, but whether you see it or not depends on the timing.",
          "score": 48,
          "created_utc": "2026-02-12 02:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xmpbf",
              "author": "Marksta",
              "text": "This was [my last formal complaint on spam](https://www.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/o3euk38/) posts, very happy we got botbouncer going.\n\nBut even once spam posts are cleared, what you're left with isn't much better. I'm still not clear on what our policy is on posters 'abusing' LLMs to write just total non-sense self-promo posts [like this one from other day.](https://www.reddit.com/r/LocalLLaMA/comments/1qz6zi3/i_built_a_fully_local_opensource_ai_workspace/o4am45l/?context=10000) -- I think that should clearly fall under the low-effort rules and the ton of posts like this one. Dude couldn't even get Claude to speak straight about what he's 'created', I didn't waste time going through source code but lord knows what dangers lurk in there.\n\n[I don't think this amazing malware vibe dev](https://www.reddit.com/r/LocalLLaMA/comments/1qfpfoy/orchestra_multimodel_ai_orchestration_system_with/o07em3u/) ever got followed up on after I sent in a mod mail about them. They blocked me, screamed at me that it wasn't a security issue, then their LLM fixed it and noted it was an extreme security issue, got uppity that I had ruined their post, so then they deleted the post and reposted it same day. Just clicking on that guys profile is a wild ride, and I guess he'll be back again with his next vibe coded vulnerability to peddle.\n\nThe current quality bar is so, so low, I know the ultra-spammy and psychotic project posts are getting cleaned up, but even the ones that remain are, wow. I think posts related to projects need to have like, 10x times more stringent rules. The first one being if the entire body of your post is LLM generated, its deleted. It just doesn't make any sense, if LLMs let you code it 100x faster then why don't they have 5 minutes to write a post about it? It's counter intuitive on an 'AI sub' to ban for AI use, but users don't come here to interact with LLM bots and people who act like LLM bots.",
              "score": 20,
              "created_utc": "2026-02-12 05:25:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yagmj",
                  "author": "keepthepace",
                  "text": "> But even once spam posts are cleared, what you're left with isn't much better. \n\nMaybe you still have the bar too low? 2-3 good posts a day is pretty good. The long tail is going to be terrible but sometimes there just isn't more content to be published.",
                  "score": 7,
                  "created_utc": "2026-02-12 09:03:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wvkhe",
          "author": "InstantJarvis",
          "text": "the spambot recommending qwen2.5 7b is too accurate lol. I've seen like 3 of those this week alone.",
          "score": 16,
          "created_utc": "2026-02-12 02:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wlsk9",
          "author": "__JockY__",
          "text": "Yo dawg, I made a graph-based ollama agent orchestrator!!!",
          "score": 36,
          "created_utc": "2026-02-12 01:23:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wos07",
              "author": "HopePupal",
              "text": "it never ends! congrats you made two chatbots talk to each other. now go vibe code a reason i should care",
              "score": 17,
              "created_utc": "2026-02-12 01:41:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4wwvlj",
              "author": "Basic_Extension_5850",
              "text": "Brain derived ollama chatbot anyone?",
              "score": 6,
              "created_utc": "2026-02-12 02:30:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x39e7",
          "author": "rawednylme",
          "text": "TBH, I don‚Äôt have a problem with the \"look what I was able to do with <generic small model>\" posts.\n\nThe rest though‚Ä¶",
          "score": 9,
          "created_utc": "2026-02-12 03:08:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wc92b",
          "author": "Southern_Sun_2106",
          "text": "I think a bigger issue is constant API 'coding plan' promoting for models that ain't really 'locally-runnable'. \"This model is now BEST\" \"Wow, this model beats THAT (and so much more affordable)\" = pls subscribe to our API 'coding plan'",
          "score": 33,
          "created_utc": "2026-02-12 00:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wcmig",
              "author": "Southern_Sun_2106",
              "text": "lol, just finished typing and see this (oh, no, this post is just about how starved they are... I love the 4.5 Air, but please...)\n\nhttps://preview.redd.it/u61nucr8kyig1.png?width=1504&format=png&auto=webp&s=bb6a57a86ef0b9c97e84160d6022e9f605361739\n\n",
              "score": 13,
              "created_utc": "2026-02-12 00:28:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4wr041",
                  "author": "ForsookComparison",
                  "text": "As a 4.5 Air fan, I highly recommend switching to 4.6v even if you don't intend to use the \"v\".",
                  "score": 6,
                  "created_utc": "2026-02-12 01:55:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yhg5p",
          "author": "Lesser-than",
          "text": "Some other key giveaways are \"We are excited to announce...\" when looking at the code its clearly 1 person and claude, why these people must refer to themselves as more than 1 person I dont know but its fairly common.",
          "score": 8,
          "created_utc": "2026-02-12 10:11:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4znngw",
              "author": "ravage382",
              "text": "I always assume that is the 'Royal We', because it makes me chuckle.",
              "score": 2,
              "created_utc": "2026-02-12 14:58:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50d76j",
              "author": "LtCommanderDatum",
              "text": "No one wants to do business with some rando loner :(",
              "score": 1,
              "created_utc": "2026-02-12 16:59:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4z2nhk",
          "author": "MetroSimulator",
          "text": "Vibe-coded malware.\n\nSo... Windows?",
          "score": 7,
          "created_utc": "2026-02-12 13:02:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ze78p",
              "author": "MelodicRecognition7",
              "text": "`curl github.com/yet-another-vibecoded-crap.sh | sudo bash -`",
              "score": 2,
              "created_utc": "2026-02-12 14:09:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o50dcmx",
              "author": "LtCommanderDatum",
              "text": "At this point, vibe-coding Windows would be an improvement...",
              "score": 2,
              "created_utc": "2026-02-12 17:00:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y4rbt",
          "author": "ayylmaonade",
          "text": "I'd be happy if I just came across posts that weren't CLEARLY completely AI-generated. If people here aren't even willing to type *anything* anymore, then gg.",
          "score": 7,
          "created_utc": "2026-02-12 08:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w9q5l",
          "author": "NigaTroubles",
          "text": "I hate qwen2.5 7b",
          "score": 14,
          "created_utc": "2026-02-12 00:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4w9wtg",
              "author": "ForsookComparison",
              "text": "It was a fine model for it's time but it ended up in too many tutorials (training data). Without web tools it and Mistral 7B are what LLMs (spambots) will reference like 99% of the time.",
              "score": 23,
              "created_utc": "2026-02-12 00:12:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4wy13s",
                  "author": "CheatCodesOfLife",
                  "text": "üî• THE MIGHTY RTX 3090 BATTLE STATION üî•\n\nWith that beastly 24GB VRAM monster, you're sitting on some serious AI-crushing hardware! Here's what you can unleash:\n\nRECOMMENDED MODELS:\n- Llama-2-13B (Best balance of performance and VRAM usage)\n- Mistral 7B (Good balance of speed and capability)\n- CodeLlama 7B: Great for coding tasks\n\nSAMPLING SETTINGS TO PLAY WITH:\n- Temperature: 0.7-0.8 for creative content, 0.1-0.2 for factual responses\n- Top_p: 0.9 provides optimal balance for most applications\n- Top_k: 40-50 maintains creativity while preserving coherence\n- Repetition penalty: 1.1-1.2  promotes response diversity\n\nWith that 3090, you can easily run 7B models at full precision and still have VRAM to spare, or go ham with 13B models using 4-bit quantization. The world's your oyster with this beast! üöÄ\n\nJust keep that cooling on point - these models love to make your GPU sweat! üí™",
                  "score": 19,
                  "created_utc": "2026-02-12 02:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z1f8k",
              "author": "Your_Friendly_Nerd",
              "text": "what‚Äôs wrong with it? i find it quite capable for code autocomplete ",
              "score": 1,
              "created_utc": "2026-02-12 12:54:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4z1u48",
                  "author": "NigaTroubles",
                  "text": "its used on every ai nowadays while there are better models better better at lower parameters",
                  "score": 1,
                  "created_utc": "2026-02-12 12:56:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4y9skl",
          "author": "UltrMgns",
          "text": "Fire meme ngl, also, true.",
          "score": 3,
          "created_utc": "2026-02-12 08:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wsyog",
          "author": "jacek2023",
          "text": "Thanks for posting this. I am happy that other people now see the problem.",
          "score": 9,
          "created_utc": "2026-02-12 02:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wno79",
          "author": "llama-impersonator",
          "text": "never thought I would miss the spiral drift crashouts.",
          "score": 4,
          "created_utc": "2026-02-12 01:34:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xk61z",
          "author": "Yorn2",
          "text": "I feel like the \"Which 8B model is best for creative writing?\" is another contender.",
          "score": 6,
          "created_utc": "2026-02-12 05:05:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xkvq5",
              "author": "ForsookComparison",
              "text": "Idk that one might be legit. People subtly asking how to goon may actually be more numerous than bots.",
              "score": 20,
              "created_utc": "2026-02-12 05:10:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yis13",
                  "author": "jwpbe",
                  "text": "I think that if you are going to ask that kind of thing, you should be forced to open openly state your use case. It's the internet. If you're going to goon, tell us, I don't give a fuck if lastname bunchanumbers wants to know what nemo finetune is best\n\nif he's honest, at least I can say \"I would recommend this sicario finetune, it does really well with the kink you want. If you want lesbian mommydom petplay, consider this niche beaverAI discord tune that was never publicly advertised, it really understands the dynamic you're looking for\"",
                  "score": 5,
                  "created_utc": "2026-02-12 10:24:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xu4st",
          "author": "hidden2u",
          "text": "But let‚Äôs be real - this post isn‚Äôt just humorous, it‚Äôs also describing some very real problems. Curious to hear what everyone thinks about this issue?",
          "score": 6,
          "created_utc": "2026-02-12 06:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y0akz",
              "author": "jacek2023",
              "text": "I am wondering, are you trying to emulate a bot right now? ;)",
              "score": 6,
              "created_utc": "2026-02-12 07:24:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5002ei",
          "author": "Euchale",
          "text": "Missing \"Check out my website that is a wrapper for a closed source model\"",
          "score": 2,
          "created_utc": "2026-02-12 15:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yoc3p",
          "author": "thedatawhiz",
          "text": "I upped my downvoting a lot more recently",
          "score": 2,
          "created_utc": "2026-02-12 11:15:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o500c87",
          "author": "Rompe101",
          "text": "It would be nice to have a blocking threshold automation.\n\nLike when 10% of my valudated users have blocked an account, that account is also blocked for me.\n\nI would like to have something like this for all my social media sites.",
          "score": 1,
          "created_utc": "2026-02-12 15:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xzeg2",
          "author": "Sioluishere",
          "text": "Please make sure you guys do not hurt actual devs who share their apps/research on here.\n\nI am all for removal of bot posts and trash-tier posts with no explanation of internals.\n\nJust do not hurt actual humans in your witch-hunting.",
          "score": -1,
          "created_utc": "2026-02-12 07:15:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ydj13",
              "author": "Mistah_Swick",
              "text": "its too late, we gotta tie a rock to your feet and see if you float!",
              "score": 1,
              "created_utc": "2026-02-12 09:33:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4wskam",
          "author": "FPham",
          "text": "So you are saying that reddit should not end up like X? 60% bots and that's the good content. It gets worse from there.\n\nI thought we are all for AI, like AI everywhere, no?\n\nOr is it only the other side that should be the subject to endless AI slop?  Them filthy clueless non-ai laymen! They are fine with it. They love it. Let's feed them even more juicy AI slop.\n\nBut not us. Noooo, we are very fine folks here. White gloves and everything. We don't eat what we cook.\n\nIt reminds me OpenAi/Anthropic coming for 30% of labor market, but not theirs, noooo. They are NOT going to lose jobs to AI. They wear top hats and have cane made of unobtanium.  \nYeah, we talked about curing cancer, but people really, really want Sora!",
          "score": -10,
          "created_utc": "2026-02-12 02:04:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wy9sr",
              "author": "llama-impersonator",
              "text": "while i like being able to ask a model to generate me a sword and sorcery story or generate a 1girl pic, doesn't mean i want the internet slopped up to its gills in horseshit.",
              "score": 16,
              "created_utc": "2026-02-12 02:38:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xo7wn",
              "author": "Marksta",
              "text": "It's like going to the lockpicking sub thinking you found all the thieves of the world. It's likely the local 3090s you find here are not the GPUs being used to destroy all of social media...",
              "score": 9,
              "created_utc": "2026-02-12 05:37:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4x2of6",
              "author": "alias454",
              "text": "I'd wear a top hat if it didn't make my ears look too big ;)",
              "score": 4,
              "created_utc": "2026-02-12 03:04:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yghqr",
          "author": "DrNavigat",
          "text": "Legal, mas se formos radicais a esse n√≠vel, vamos acabar virando um portal de not√≠cias que anuncia s√≥ as grandes corpora√ß√µes dos estados unidos e da China. E se isso acontecer, sinceramente, √© melhor assinar alguma RSS e receber via email.",
          "score": 0,
          "created_utc": "2026-02-12 10:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4x0bjw",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -9,
          "created_utc": "2026-02-12 02:50:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4x7p6p",
              "author": "thrownawaymane",
              "text": "Repo link?",
              "score": -4,
              "created_utc": "2026-02-12 03:36:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4y6gkx",
                  "author": "cheesecakegood",
                  "text": "it's another AI response (the irony), stay away",
                  "score": 4,
                  "created_utc": "2026-02-12 08:23:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4x8epg",
                  "author": "angelin1978",
                  "text": "It's a production app so no public repo unfortunately, but the integration is pretty standard llama.cpp ‚Äî I'm using the C API via JNI on Android and a Swift wrapper on iOS. The main tricks were getting GGUF model loading to work within mobile memory constraints and making sure CMake builds with -O2 (default debug builds are ~100x slower without SIMD optimization). Happy to go into more detail on any part of it.",
                  "score": -9,
                  "created_utc": "2026-02-12 03:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xd71i",
          "author": "kubrador",
          "text": "local llama fans are out here treating an open source community like it's a marvel character getting cancelled",
          "score": -9,
          "created_utc": "2026-02-12 04:14:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xlh2d",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": -4,
          "created_utc": "2026-02-12 05:15:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz23pp",
      "title": "PR opened for Qwen3.5!!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/r10pwm02y7ig1.png",
      "author": "Mysterious_Finish543",
      "created_utc": "2026-02-08 06:57:13",
      "score": 620,
      "num_comments": 74,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o48bmue",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-08 10:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47sc2i",
          "author": "Betadoggo_",
          "text": "It also uses semi linear attention similar to qwen3-next\n\nhttps://preview.redd.it/bms5k1m018ig1.png?width=1401&format=png&auto=webp&s=9c1284766c41effa9206ce5416808f52152ae655\n\n",
          "score": 96,
          "created_utc": "2026-02-08 07:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4847mq",
              "author": "Midaychi",
              "text": "hopefully the max positional embeddings is a placeholder and the max context isn't 32768",
              "score": 16,
              "created_utc": "2026-02-08 09:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49rd9k",
                  "author": "trusty20",
                  "text": "How is 32k+ sequence length performance these days? The last few times I checked in on local 32k+ models there was a huge dropoff cliff in context recollection accuracy, it seemed like only the massive models could actually be reliable above 32k+ have we punched through that? What's the ceiling thought to be now for accuracy important stuff?",
                  "score": 6,
                  "created_utc": "2026-02-08 16:06:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4c9eis",
                  "author": "dreamkast06",
                  "text": "It might just be for the base model. Lots of them are trained on 32k the instruct tuned to a decent context length.",
                  "score": 2,
                  "created_utc": "2026-02-08 23:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48x4z7",
              "author": "Iory1998",
              "text": "Well, that's the direction at the moment. I mean, look at Qwen3-Next and especially Kimi Linear. ",
              "score": 9,
              "created_utc": "2026-02-08 13:16:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o485dx9",
              "author": "cibernox",
              "text": "To understand what means in practice semi linear attention, can I expect roughly for context to take less space and thus token generation to be faster for a given context? Would the processing of a request with the same long promt also be faster?",
              "score": 6,
              "created_utc": "2026-02-08 09:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48k9iu",
                  "author": "PuppyGirlEfina",
                  "text": "Linear attention is O(1). Constant memory and each token computes in the same time. I assume semi means hybrid, so it might be more like O(log N), so better scaling than Attention's O(N).",
                  "score": 4,
                  "created_utc": "2026-02-08 11:35:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47s1v1",
          "author": "lly0571",
          "text": "We may have Qwen3.5-9B-Instruct and Qwen3.5-35B-A3B-Instruct later?\n\nLooks that Qwen3.5 may use a 248k sized vocab, which might be helpful for multilingual performance, and both of the dense model and moe model would use the the hybrid attention from Qwen3-Next.",
          "score": 59,
          "created_utc": "2026-02-08 07:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4cwro3",
              "author": "chibop1",
              "text": "Opus-4.6: Here's the full analysis. The key highlights from PR #43830:\n\n**Two model families** are being added ‚Äî a **dense Qwen3.5** (reference: 9B-Instruct) and a **MoE Qwen3.5** (reference: 35B-A3B-Instruct, 256 experts with top-8 routing).\n\n**The standout architectural feature** is a **hybrid attention design** ‚Äî approximately 75% of layers use **Gated DeltaNet linear attention** (a recurrent mechanism with causal conv1d, similar in spirit to Mamba-style state space models) while every 4th layer uses standard **full softmax attention** with GQA. This gives sub-quadratic complexity for most layers while retaining full attention's expressiveness periodically.\n\nOther notable details:\n- **Partial RoPE** ‚Äî only 25% of the 256-dim head gets rotary embeddings\n- **M-RoPE** (3D position encoding: temporal, height, width) for multimodal inputs\n- **Vision encoder** inherited from Qwen3-VL (27-layer ViT, patch size 16, spatial merge 2√ó2)\n- The models build on the already-merged **Qwen3-Next** architecture (PR #40771), with Qwen3.5 refactoring the projection structure in the DeltaNet module (separate `in_proj_qkv`, `in_proj_z`, `in_proj_\n\nMore detail: https://claude.ai/public/artifacts/93b0a136-fe1c-4077-892b-291bb90026f2",
              "score": 0,
              "created_utc": "2026-02-09 01:48:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o481g74",
          "author": "dampflokfreund",
          "text": "Super exciting, being finally native multimodal and using the latest architecture. this one should be gooood",
          "score": 33,
          "created_utc": "2026-02-08 08:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48i1of",
              "author": "simracerman",
              "text": "Isn‚Äôt Qwen3-Next already doing both?",
              "score": 4,
              "created_utc": "2026-02-08 11:15:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48in1a",
                  "author": "tarruda",
                  "text": "All Qwen3-Next releases so far were text only",
                  "score": 16,
                  "created_utc": "2026-02-08 11:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47vhnt",
          "author": "jamaalwakamaal",
          "text": "qWhen !!",
          "score": 58,
          "created_utc": "2026-02-08 07:42:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48hyhi",
              "author": "simracerman",
              "text": "G(when)GUF?!",
              "score": 16,
              "created_utc": "2026-02-08 11:14:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4b0cwh",
                  "author": "MrPecunius",
                  "text": "¬øQwandoMLX?",
                  "score": 4,
                  "created_utc": "2026-02-08 19:41:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48osos",
              "author": "LinkSea8324",
              "text": "Usually a week after the PR is opened",
              "score": 5,
              "created_utc": "2026-02-08 12:14:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49toqa",
                  "author": "x0wl",
                  "text": "Can be faster if it's similar enough to Qwen3-Next",
                  "score": 3,
                  "created_utc": "2026-02-08 16:17:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49eaiq",
              "author": "Total_Laugh_1487",
              "text": "Qwin!",
              "score": 3,
              "created_utc": "2026-02-08 14:59:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4dodjv",
              "author": "nialv7",
              "text": "probably a new year's present for Chinese New Year",
              "score": 3,
              "created_utc": "2026-02-09 04:22:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tz3a",
          "author": "Significant_Fig_7581",
          "text": "Can't wait!!!!! Finally!!!!!",
          "score": 20,
          "created_utc": "2026-02-08 07:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o481pn4",
          "author": "darkpigvirus",
          "text": "wishing for Qwen 3.5 2B A350M if it is possible üçÄ",
          "score": 22,
          "created_utc": "2026-02-08 08:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48c4cb",
              "author": "_-_David",
              "text": "That is specific enough to pique my curiosity. Why that size specifically?",
              "score": 11,
              "created_utc": "2026-02-08 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48icpw",
                  "author": "jikilan_",
                  "text": "To run in his Nokia 3310, I think",
                  "score": 37,
                  "created_utc": "2026-02-08 11:18:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o496ryf",
              "author": "FeiX7",
              "text": "what A350M means?",
              "score": 1,
              "created_utc": "2026-02-08 14:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4975av",
                  "author": "darkpigvirus",
                  "text": "for an moe model the a350m means is that for each token the active parameters that is involved and active is only 350m instead of using all the 2 billion parameters so that to speed up the inference and only use the experts where they are deemed much more effective. idk if i explain it as the experts like but i did what i can",
                  "score": 3,
                  "created_utc": "2026-02-08 14:18:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48t4wz",
          "author": "QuackerEnte",
          "text": "https://preview.redd.it/7h263s4uo9ig1.jpeg?width=868&format=pjpg&auto=webp&s=99076a4dbda46aac08528b6b6224fb44d1e43f13\n\nYay 2B VL model",
          "score": 7,
          "created_utc": "2026-02-08 12:48:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47tfr3",
          "author": "arcanemachined",
          "text": "Very cool. I haven't used the Qwen \"next\" models much myself, but I heard a lot of complaints initially. (Mostly since it took llama.cpp so long to upstream the changes required to support the new architecture, I assume.)\n\nNow that they've been out for a while, can anyone speak to the pros and cons of the new architecture? Is it better? Are there any drawbacks?",
          "score": 14,
          "created_utc": "2026-02-08 07:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47uqxx",
              "author": "Mysterious_Finish543",
              "text": "The recent `qwen3-next-coder` model is pretty good, especially for the size. In its class, there are no comparable models. In terms of proprietary models, my vibe is that it sits somewhere around `claude-sonnet-4`?\n\nIt's also great that the `qwen3-next` architecture makes KV cache memory usage very efficient over long sequences, so it's possible to run it on long context on consumer hardware.\n\nThe initial Instruct and Thinking releases weren't super exciting though. Particularly the thinking model was a bit of a disappointment, very long CoT (mostly just repetition) and not very good at agents (compared to something like `gpt-oss-120b`). Seemed to be ultra-optimized for math and coding competition type problems.",
              "score": 22,
              "created_utc": "2026-02-08 07:35:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o480fnd",
                  "author": "Odd-Ordinary-5922",
                  "text": "from what I remember tho is that the initial 80b model was trained using 15T tokens when usually their models are trained on 35 Trillion or smth around there.",
                  "score": 8,
                  "created_utc": "2026-02-08 08:28:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o480acd",
                  "author": "kweglinski",
                  "text": "next also had awful sycophancy to the point it was annoying to read but I don't see it with coder next.",
                  "score": 3,
                  "created_utc": "2026-02-08 08:27:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aeq89",
          "author": "ilintar",
          "text": "Note that I'm doing this without any support, just based on Transformers code and my conversion guidelines + Opus 4.6, but I'm aiming for 0-day support this time:\n\n[https://github.com/ggml-org/llama.cpp/pull/19435](https://github.com/ggml-org/llama.cpp/pull/19435)",
          "score": 12,
          "created_utc": "2026-02-08 17:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47z1t9",
          "author": "abdouhlili",
          "text": "Looks like 3.5 will kill VL models.",
          "score": 11,
          "created_utc": "2026-02-08 08:15:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48iu5b",
              "author": "tarruda",
              "text": "Do you mean there won't be any 3.5 VL releases?",
              "score": 2,
              "created_utc": "2026-02-08 11:22:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49uvx4",
                  "author": "x0wl",
                  "text": "It's VL [**https://github.com/bozheng-hit/transformers/blob/qwen3\\_5/src/transformers/models/qwen3\\_5/configuration\\_qwen3\\_5.py#L198**](https://github.com/bozheng-hit/transformers/blob/qwen3_5/src/transformers/models/qwen3_5/configuration_qwen3_5.py#L198)",
                  "score": 5,
                  "created_utc": "2026-02-08 16:23:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o48k0ot",
                  "author": "abdouhlili",
                  "text": "Yes, I think so.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:33:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49onm3",
          "author": "ilintar",
          "text": "Yummy. Lemme look at it :>",
          "score": 5,
          "created_utc": "2026-02-08 15:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o480y7q",
          "author": "mlon_eusk-_-",
          "text": "We are eating good folks",
          "score": 9,
          "created_utc": "2026-02-08 08:33:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4appt4",
          "author": "Admirable-Detail-465",
          "text": "Hopefully they make another model sized similarly to qwen 3 next, that was the perfect size for me",
          "score": 4,
          "created_utc": "2026-02-08 18:49:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48d3b3",
          "author": "CoqueTornado",
          "text": "speculative decoding in lmstudio with qwen3 80B iq4\\_xs +qwen3 0.6B  doesn't work for me with 64gb of ram + 8gb of vram, any thoughts?",
          "score": 3,
          "created_utc": "2026-02-08 10:28:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ibr4",
              "author": "simracerman",
              "text": "MoE and speculative never worked for me. It‚Äôs already fast enough, I‚Äôd keep SD for strictly larger dense models.",
              "score": 8,
              "created_utc": "2026-02-08 11:17:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48onsn",
                  "author": "muxxington",
                  "text": "As I understand it, moe and conventional speculative decoding generally cannot work, at least not in a meaningful way. This would require an additional layer of speculative expert choosing. However, self-speculative decoding should work with moe, if I am not mistaken.",
                  "score": 1,
                  "created_utc": "2026-02-08 12:13:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49uu6v",
              "author": "ForsookComparison",
              "text": "Spec dec on Qwen3 hasn't worked since the earliest Qwen3 models last year. As soon as the 2507 checkpoints came out it was totally broken and we never got a new updated model small enough to be worth it.",
              "score": 2,
              "created_utc": "2026-02-08 16:23:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4c2q99",
                  "author": "CoqueTornado",
                  "text": "yep, I've done several tests and this is true. They should get back to the roots in this 3.5, I'd like fast and wise answers in my humble laptop :P",
                  "score": 2,
                  "created_utc": "2026-02-08 22:56:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o496btf",
              "author": "colin_colout",
              "text": "also the models need to be very similar in very specific ways (same tokenizer, and should generate similar logprobs) if you're using a draft model. \n\nqwen3-next and qwen3 aren't the same. if they don't use the same tokenizer (which i think they don't), then it's not viable as a draft model.",
              "score": 1,
              "created_utc": "2026-02-08 14:14:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48h563",
          "author": "ab2377",
          "text": "exciting.",
          "score": 3,
          "created_utc": "2026-02-08 11:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48mdy5",
          "author": "Full_Ad693",
          "text": "Curious how 3.0 improves on 2.5. Anyone tested on AMD yet?",
          "score": 2,
          "created_utc": "2026-02-08 11:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o499n9j",
              "author": "sleepingsysadmin",
              "text": "You mean qwen3 30b vs qwen2.5 72b? 30b thinking was marginally better than 72b on capability and obviously wickedly faster. ",
              "score": 2,
              "created_utc": "2026-02-08 14:33:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48ysrn",
          "author": "sleepingsysadmin",
          "text": "Qwen3.5 35b thinking is going to be epic. I just hope llama gets the performance into the qwen next arch by the time it releases or it's going to be not well received. ",
          "score": 2,
          "created_utc": "2026-02-08 13:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4x3t2x",
          "author": "charles25565",
          "text": "Qwen3 was already powerful as fuck even at 1.7B, can't imagine what Qwen3.5 could do.",
          "score": 2,
          "created_utc": "2026-02-12 03:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47yrt4",
          "author": "UnluckyAdministrator",
          "text": "Looking forward to this. I've been running Qwen2.5-coder-7b-instruct on CPU with 16RAM, and it's pretty performant.\n\nCurious if anyone has got their hands on the NVIDIA DGX Spark supercomputer yet to spin up these models offline?",
          "score": 3,
          "created_utc": "2026-02-08 08:13:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o480hur",
              "author": "Odd-Ordinary-5922",
              "text": "any reason you arent using newer models? or am I talking to an llm rn",
              "score": 10,
              "created_utc": "2026-02-08 08:29:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4816wp",
                  "author": "UnluckyAdministrator",
                  "text": "Only just experimenting at the moment open-source. It's the heavier weights gpt-oss-120b I'm really interested in, however CPU won't cut it.\n\nHave you tried your hands on the DGX Spark for these heavier models?",
                  "score": -3,
                  "created_utc": "2026-02-08 08:36:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r1wl6x",
      "title": "GLM 5 Released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/",
      "author": "External_Mood4719",
      "created_utc": "2026-02-11 12:53:30",
      "score": 595,
      "num_comments": 180,
      "upvote_ratio": 0.93,
      "text": "[https://chat.z.ai/](https://chat.z.ai/)\n\nhttps://preview.redd.it/mvdnn18e4vig1.png?width=799&format=png&auto=webp&s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e\n\n",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4uoy3h",
          "author": "rm-rf-rm",
          "text": "Given that the official release is up: https://old.reddit.com/r/LocalLLaMA/comments/1r22hlq/glm5_officially_released/\n\nLocking this thread",
          "score": 1,
          "created_utc": "2026-02-11 19:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sibwq",
          "author": "Significant_Fig_7581",
          "text": "Woah! Will they open source it?",
          "score": 133,
          "created_utc": "2026-02-11 12:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4surpy",
              "author": "Allseeing_Argos",
              "text": "Obviously I still wish for them to open source it, but hardly anyone will be able to run it anyways with 745B params and 44B active.",
              "score": 66,
              "created_utc": "2026-02-11 14:09:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t7gbl",
                  "author": "CanineAssBandit",
                  "text": "Why even mention that it's hard to run on a normal PC? That's a feature, not a bug. The point is ownership and control. I can run Kimi off NVME if I have time to burn, I can't run Sonnet or Opus at all.\n\nThere are lots of companies making small models for normal PCs for lighter work.",
                  "score": 59,
                  "created_utc": "2026-02-11 15:15:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4svf8a",
                  "author": "Significant_Fig_7581",
                  "text": "Yeah we can't run that surely most people here can't either but would be nice if they released a 48B flash version that's what I really hope for then with q4 and ram offloading it shall fit",
                  "score": 19,
                  "created_utc": "2026-02-11 14:12:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tfzve",
                  "author": "eli_pizza",
                  "text": "If nothing else it means the price will always be competitive because there are multiple provides",
                  "score": 6,
                  "created_utc": "2026-02-11 15:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tpa7c",
                  "author": "SidneyFong",
                  "text": "What do you mean? That's why I bought my maxxed out Mac Studio Ultra...",
                  "score": 4,
                  "created_utc": "2026-02-11 16:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tblne",
                  "author": "Longjumping-Boot1886",
                  "text": "Mac Studio?",
                  "score": 3,
                  "created_utc": "2026-02-11 15:35:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4uiwb3",
                  "author": "wektor420",
                  "text": "This might not fit on 8x96Gb even in fp8, damn",
                  "score": 1,
                  "created_utc": "2026-02-11 18:56:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t8dfl",
                  "author": "Yes_but_I_think",
                  "text": "This only shows that there's only enough that can be done with small models. This is twice the size of their previous model.",
                  "score": -1,
                  "created_utc": "2026-02-11 15:19:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4snchu",
              "author": "johnfkngzoidberg",
              "text": "If I can‚Äôt run it locally, then why is OP spamming the sub?",
              "score": 59,
              "created_utc": "2026-02-11 13:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4suqxf",
                  "author": "Thick-Specialist-495",
                  "text": "It‚Äôll probably be open sourced soon. The company has literally open sourced every other model they‚Äôve made, so relax. Things move fast.\n\nAnd why wouldn‚Äôt OP share it early? That‚Äôs how people get ready for what‚Äôs coming instead of sitting around whining that they can‚Äôt run it locally yet. Not everything has to be instantly downloadable for it to be worth discussing.\n\nThe weird hostility over a heads-up post is wild. Not everything is a conspiracy against your GPU.",
                  "score": 75,
                  "created_utc": "2026-02-11 14:08:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4stotb",
                  "author": "j_osb",
                  "text": "Didn't they add like, inference information for glm5 in a pull request for something inference related recently? I would assume we get open weights at some point.",
                  "score": 36,
                  "created_utc": "2026-02-11 14:03:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sotw4",
                  "author": "Significant_Fig_7581",
                  "text": "Nah I think OP meant that hey it's ready and it's already there and we can test it, They probably gonna release it soon... I remember when I thought MiniMax wasn't gonna release more open models but after like 3 days they released it. It'd be kinda funny if this time none of them released it lol",
                  "score": 13,
                  "created_utc": "2026-02-11 13:36:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t2jyl",
                  "author": "l33t-Mt",
                  "text": "Because thats not a rule.",
                  "score": 9,
                  "created_utc": "2026-02-11 14:50:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sstik",
                  "author": "segmond",
                  "text": "shaddup, [z.ai](http://z.ai) has often released open models, they probably have more open models than any other lab.  even if they don't release a model, the announcement is worthy of discussion because if there closed model is a very good model, then that means down the line we are going to get something that good.",
                  "score": 8,
                  "created_utc": "2026-02-11 13:58:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u87pz",
                  "author": "AnticitizenPrime",
                  "text": "https://huggingface.co/zai-org/GLM-5\n\nHere's your weights, milord",
                  "score": 2,
                  "created_utc": "2026-02-11 18:07:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tafdt",
                  "author": "Orolol",
                  "text": "So this sub is dedicated to the models YOU can run locally ?",
                  "score": 1,
                  "created_utc": "2026-02-11 15:29:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4u4okb",
                  "author": "ttkciar",
                  "text": "It's an open-weights model, and just because you and I cannot host it on our hardware doesn't mean other redditors cannot.\n\nJust calm down and wait for the distillations.  I'm hoping for GLM-5-Air.",
                  "score": 0,
                  "created_utc": "2026-02-11 17:51:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t7cbp",
                  "author": "trenescese",
                  "text": "because people interested in running models locally may be interested in knowing this model? what's weird about that?",
                  "score": -1,
                  "created_utc": "2026-02-11 15:14:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4uayul",
              "author": "Nota_ReAlperson",
              "text": "It is now on huggingface.",
              "score": 4,
              "created_utc": "2026-02-11 18:20:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4unw83",
                  "author": "uhuge",
                  "text": "here we go! [https://huggingface.co/collections/zai-org/glm-5](https://huggingface.co/collections/zai-org/glm-5)",
                  "score": 3,
                  "created_utc": "2026-02-11 19:20:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ubcio",
                  "author": "Significant_Fig_7581",
                  "text": "Ok now I'm wondering about the Flash version ü•≤",
                  "score": 1,
                  "created_utc": "2026-02-11 18:22:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4su21o",
              "author": "Neither-Phone-7264",
              "text": "yes.",
              "score": 2,
              "created_utc": "2026-02-11 14:05:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4tg5ra",
              "author": "IShitMyselfNow",
              "text": "Didn't they already put a PR to support it in Llama.cpp? which would be pointless unless opensourced",
              "score": 1,
              "created_utc": "2026-02-11 15:56:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sisli",
          "author": "Front_Eagle739",
          "text": "Hmm. Cant help but notice no activity on their huggingface.¬† Do they normally take a few days after api to appear or are they going closed?",
          "score": 51,
          "created_utc": "2026-02-11 13:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sjpto",
              "author": "kweglinski",
              "text": "they haven't really finished releasing it. It says 4.7 everywhere on websites and in interfaces. It's not available yet on API for code plan.",
              "score": 61,
              "created_utc": "2026-02-11 13:05:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4skbup",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 13,
                  "created_utc": "2026-02-11 13:09:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4stw9d",
                  "author": "Comrade-Porcupine",
                  "text": "it's available on API, I'm using it right now in OpenCode.\n\nno pricing up yet",
                  "score": 3,
                  "created_utc": "2026-02-11 14:04:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ssyyh",
                  "author": "segmond",
                  "text": "It is for me.",
                  "score": 1,
                  "created_utc": "2026-02-11 13:59:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4t2rwq",
                  "author": "Emergency-Pomelo-256",
                  "text": "Website is vibe coded, GLM 5 may not have finished Vibe coding the new one",
                  "score": 1,
                  "created_utc": "2026-02-11 14:51:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sk2pn",
                  "author": "Front_Eagle739",
                  "text": "Fair. Shall find out soon enough regardless",
                  "score": 0,
                  "created_utc": "2026-02-11 13:08:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4sjpec",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 13,
              "created_utc": "2026-02-11 13:05:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sjx8d",
                  "author": "Front_Eagle739",
                  "text": "Sure but I usually notice the space created and such",
                  "score": -1,
                  "created_utc": "2026-02-11 13:07:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4sme69",
              "author": "ExcuseAccomplished97",
              "text": "I can see it (GLM-5) on the chat webpage and I am logged in. There is an 'agent' mode toggle on the prompt input. I assume they have enhanced the agentic ability in this version.",
              "score": 2,
              "created_utc": "2026-02-11 13:22:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4trc2w",
              "author": "AnticitizenPrime",
              "text": "Yes, it's like this every time they do a release. Gets announced first, appears on z.ai, and then the weights show up within a day or so.",
              "score": 1,
              "created_utc": "2026-02-11 16:48:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ttyor",
                  "author": "Front_Eagle739",
                  "text": "yup, the link has appeared. not populated yet but its coming. Happy days",
                  "score": 1,
                  "created_utc": "2026-02-11 17:01:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4toqnh",
          "author": "Sea_Trip5789",
          "text": "[https://z.ai/subscribe](https://z.ai/subscribe)  \nThey updated the plans, right now only max supports it. After they re-balance their infra pro will support it too but not the lite plan",
          "score": 13,
          "created_utc": "2026-02-11 16:36:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tp82y",
              "author": "Landohanno",
              "text": "Better be incredible, for those prices",
              "score": 6,
              "created_utc": "2026-02-11 16:38:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4twj5i",
                  "author": "Designer_Athlete7286",
                  "text": "Been using GLM 4.7 (more like abusing it) on the Pro plan as the day to day model. it has been great so far. Honestly with the rate limits you get, GLM coding plan is probably the most cost efficient option.",
                  "score": 2,
                  "created_utc": "2026-02-11 17:13:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sm6uv",
          "author": "RickyRickC137",
          "text": "Happy Chinese New Year! Minimax M2.5 is getting released too! Waiting for qwen image 2.0 and Qwen 3.5!",
          "score": 32,
          "created_utc": "2026-02-11 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3j0l",
              "author": "zxyzyxz",
              "text": "https://qwen.ai/blog?id=qwen-image-2.0",
              "score": 5,
              "created_utc": "2026-02-11 17:46:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sngfl",
          "author": "Salt-Willingness-513",
          "text": "Is it in coding plan already?",
          "score": 6,
          "created_utc": "2026-02-11 13:28:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4spbni",
              "author": "epyctime",
              "text": "\\`Insufficient balance or no resource package\\` with glm-5 model name. I see it on [z.ai](http://z.ai) though.",
              "score": 9,
              "created_utc": "2026-02-11 13:38:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4szo4e",
                  "author": "XccesSv2",
                  "text": "same... maybe the lite plan doesnt get glm5",
                  "score": 3,
                  "created_utc": "2026-02-11 14:35:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4teu36",
              "author": "AnomalyNexus",
              "text": "I don't see it yet. Also, the bottom tier likely isn't getting 5",
              "score": 3,
              "created_utc": "2026-02-11 15:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tkoyl",
                  "author": "Salt-Willingness-513",
                  "text": "Im in pro, not lite :) but thanks im not the obly one not seeing it yet",
                  "score": 1,
                  "created_utc": "2026-02-11 16:17:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tm7p9",
              "author": "postitnote",
              "text": "Only on Max for now.\nhttps://docs.z.ai/devpack/overview\n\n>Currently, we are in the stage of replacing old model resources with new ones. Only the Max (including both new and old subscribers) newly supports GLM-5, and invoking GLM-5 will consume more plan quota than historical models. After the iteration of old and new model resources is completed, the Pro will also support GLM-5.",
              "score": 1,
              "created_utc": "2026-02-11 16:24:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4tvuq9",
                  "author": "Salt-Willingness-513",
                  "text": ":(",
                  "score": 2,
                  "created_utc": "2026-02-11 17:09:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4txspb",
              "author": "yukintheazure",
              "text": "I estimate that using it will require the max plan, and the subscription price may increase.",
              "score": 1,
              "created_utc": "2026-02-11 17:19:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ueq44",
                  "author": "Salt-Willingness-513",
                  "text": "https://preview.redd.it/w5l3n7dttwig1.jpeg?width=1243&format=pjpg&auto=webp&s=ff770d09cb44051e9558a65cade453c1e35e89ab\n\nFound the info in the meantime, but thanks",
                  "score": 1,
                  "created_utc": "2026-02-11 18:37:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sl96s",
          "author": "Different-Rush-2358",
          "text": "So my question is, since GLM has already been released, is Pony Alpha still available in open router? Also, what kind of model is Pony exactly? Is it DeepSeek?",
          "score": 13,
          "created_utc": "2026-02-11 13:15:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sml39",
              "author": "chrd5273",
              "text": "Looks like pony is still available in OR, but probably will disappear soon when they open official API for GLM-5. Pony alpha is GLM-5.",
              "score": 10,
              "created_utc": "2026-02-11 13:23:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ttz8q",
              "author": "Roffievdb",
              "text": "Boo...I just got this message - 404 The Pony Alpha stealth model has sunsetted, and its identity will be revealed soon!",
              "score": 5,
              "created_utc": "2026-02-11 17:01:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4sv5wb",
              "author": "petuman",
              "text": ">  Also, what kind of model is Pony exactly?\n\nSeems to be GLM5, as \"confirmed\" by (as of now domain redirects to pony alpha page):\nhttps://x.com/ZixuanLi_/status/2020533168520954332",
              "score": 3,
              "created_utc": "2026-02-11 14:11:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t4moo",
          "author": "turklish",
          "text": "Still waiting for a new AIR model...",
          "score": 6,
          "created_utc": "2026-02-11 15:00:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u21fs",
              "author": "ttkciar",
              "text": "Me too!  Still pretty happy with GLM-4.5-Air in the meantime, though.",
              "score": 3,
              "created_utc": "2026-02-11 17:39:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4to8c8",
          "author": "Mean-Neighborhood-42",
          "text": "https://preview.redd.it/tanhe6rr7wig1.jpeg?width=828&format=pjpg&auto=webp&s=394583129f087b3688f140f237bc616eeae71712\n\nThis is on the webapp, hope its isnt a lie",
          "score": 7,
          "created_utc": "2026-02-11 16:34:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4si1bb",
          "author": "WhaleFactory",
          "text": "Aww shit, here we go again! \n\n:-)",
          "score": 25,
          "created_utc": "2026-02-11 12:55:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4t7gmg",
          "author": "Noob_l",
          "text": "Noooo... They added a weekly quota :(",
          "score": 5,
          "created_utc": "2026-02-11 15:15:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgm67",
              "author": "Uzpian",
              "text": "Where did you see it?",
              "score": 3,
              "created_utc": "2026-02-11 15:58:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tycm1",
          "author": "Opposite-Hotel-7495",
          "text": "OMG why it is so expensive? ",
          "score": 3,
          "created_utc": "2026-02-11 17:21:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u6hlv",
              "author": "sammoga123",
              "text": "Because the model more than doubled its hyperparameters. From 300 to 735.",
              "score": 7,
              "created_utc": "2026-02-11 17:59:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4u1mzl",
              "author": "ttkciar",
              "text": "Why would we care if their service is expensive?  This is LocalLLaMA.",
              "score": 1,
              "created_utc": "2026-02-11 17:37:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ui70s",
          "author": "ortegaalfredo",
          "text": "I always have been a fan of GLM but since 4.7 it has underwhelmed me a bit. This new version is very fast and results have much better formatted however intelligence itself has not improved much and solving logic problems is still at the level of 4.6, for my benchmarks. I believe is more oriented to coding.",
          "score": 3,
          "created_utc": "2026-02-11 18:53:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4snnog",
          "author": "Obvious-Nobody-9592",
          "text": "Why isn't open weights?",
          "score": 8,
          "created_utc": "2026-02-11 13:29:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u3ril",
              "author": "ttkciar",
              "text": "Its weights are available for download from Huggingface now.",
              "score": 8,
              "created_utc": "2026-02-11 17:47:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sk7f8",
          "author": "bootlickaaa",
          "text": "Not working in the API yet. Just seeing 429.",
          "score": 2,
          "created_utc": "2026-02-11 13:08:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4su59b",
              "author": "Comrade-Porcupine",
              "text": "working in API for me. had to update my opencode config to force it, but GLM-5 is there and working\n\nseems pretty smart.  but a bit slow.",
              "score": 2,
              "created_utc": "2026-02-11 14:05:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t23rr",
                  "author": "muhamedyousof",
                  "text": "I tried in cc but respond with 429, under the name of glm-5, how did you setup opencode for it? coding plan? \n\nMy coding plan is pro ",
                  "score": 2,
                  "created_utc": "2026-02-11 14:48:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tx2jt",
                  "author": "Designer_Athlete7286",
                  "text": "How does it compare to Opus 4.6? That's the benchmark for me. (Opus 4.6 has been flawless so far for me) GLM 4.7 has been good as a work hose. I'm hoping that GLM 5 can be the Opus 4.6 alternative.",
                  "score": 1,
                  "created_utc": "2026-02-11 17:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4thj79",
              "author": "JustFinishedBSG",
              "text": "Works for me but is VERY slow.\n\n\\> curl --location 'https://api.z.ai/api/coding/paas/v4/chat/completions' --header 'Authorization: Bearer YOUR\\_TOKEN' --header 'Accept-Language: en-US,en' --header 'Content-Type: application/json' --data '{     \"model\": \"glm-5\",     \"messages\": \\[         {             \"role\": \"user\",             \"content\": \"Please introduce the development history of artificial intelligence\"         }     \\],     \"temperature\": 1.0,     \"max\\_tokens\": 1024 }",
              "score": 2,
              "created_utc": "2026-02-11 16:03:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4svtz2",
          "author": "liuyj3000",
          "text": "can't use in coding plan yet  \n",
          "score": 2,
          "created_utc": "2026-02-11 14:14:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u8ycb",
          "author": "saskatoonberry28",
          "text": "Official release article: [https://z.ai/blog/glm-5](https://z.ai/blog/glm-5)",
          "score": 2,
          "created_utc": "2026-02-11 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sixyv",
          "author": "Haoranmq",
          "text": "Monster",
          "score": 4,
          "created_utc": "2026-02-11 13:01:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sm8hs",
          "author": "No_Attitude_2280",
          "text": "GGUF WHEN?",
          "score": 3,
          "created_utc": "2026-02-11 13:21:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sj0ua",
          "author": "foldl-li",
          "text": "looks great at programming.",
          "score": 3,
          "created_utc": "2026-02-11 13:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sojmw",
          "author": "BABA_yaaGa",
          "text": "Apparently it‚Äôs knowledge cutoff is older than glm 4.7",
          "score": 1,
          "created_utc": "2026-02-11 13:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sozbq",
              "author": "epyctime",
              "text": "probably a good thing",
              "score": 2,
              "created_utc": "2026-02-11 13:36:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4st04a",
          "author": "SubjectHealthy2409",
          "text": "Let's go baby",
          "score": 1,
          "created_utc": "2026-02-11 13:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4stpl0",
          "author": "SatoshiNotMe",
          "text": "Waiting for GLM-5-flash for my M1 Max MacBook",
          "score": 1,
          "created_utc": "2026-02-11 14:03:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4te083",
          "author": "WolfpackBP",
          "text": "How's price compared to Kimi",
          "score": 1,
          "created_utc": "2026-02-11 15:46:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4th8ol",
          "author": "UserXtheUnknown",
          "text": "I've still to test the Agent version.  \nThe chat version seems to massively underthink, now. Sure, the answers are quite fast, so probably it is meant to be the equivalente of Gemini 3 flash. I will test it more, but sadly right now didn't impress me too much.\n\nFor example, in a RPG, at the question of what she'd like to drink, the character played by GLM5 replied this non-sense:\n\n*The bartender hovers at the periphery, waiting.*\n\n***BLAIRE:*** *\"Top-shelf tequila. Whatever he's pouring for himself, I'll take the same‚Äîbut doubled. I'm not trying to match a man who metabolizes alcohol like a nuclear reactor.\"*\n\nSo she orders tequila, then asks the same as my character, but doubled, because she can't hope to match his drinking skills (!). 3 different concepts, antithetics, in two lines.",
          "score": 1,
          "created_utc": "2026-02-11 16:01:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tx2w8",
          "author": "shiv4ngi",
          "text": "https://youtube.com/shorts/AYqGHNgJy1o?feature=share",
          "score": 1,
          "created_utc": "2026-02-11 17:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tyglf",
          "author": "exspir3",
          "text": "Open Source seems confirmed by vLLM:\n\n[https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM5.html](https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM5.html)",
          "score": 1,
          "created_utc": "2026-02-11 17:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sjdjd",
          "author": "M00lefr33t",
          "text": "OR when? ü§©",
          "score": 0,
          "created_utc": "2026-02-11 13:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sij3o",
          "author": "dampflokfreund",
          "text": "Seems like it is still a text only model. Very disappointing tbh especially considering Qwen is also moving to native multimodality.",
          "score": -8,
          "created_utc": "2026-02-11 12:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sjnfk",
              "author": "Eyelbee",
              "text": "Doesn't matter if it's actually good. Text is the useful part. ",
              "score": 35,
              "created_utc": "2026-02-11 13:05:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4sk8uw",
                  "author": "PaluMacil",
                  "text": "I find that when dealing with infrastructure, images are very valuable. Instead of spending a bunch of time typing out the cloud config, I just take a screenshot of a screen. It saves a ton of time.",
                  "score": 12,
                  "created_utc": "2026-02-11 13:09:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sl9ii",
                  "author": "dampflokfreund",
                  "text": "Even if you only use it to generate text, native multimodality also enhances text performance greatly, because the model has more varied data to work with to form its world model. This was proved in a paper, (sadly I forgot the name) There is no reason to not want this and it is the future of LLMs going forward. Qwen realized that as well.",
                  "score": 3,
                  "created_utc": "2026-02-11 13:15:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4slca5",
                  "author": "Hoodfu",
                  "text": "The problem is that these are so big now that even with a Big Mac so to speak, I don't have the room to run this with a big context plus a second VL model along side it. It would really be great to have just one that can handle both. I tried using qwen vl 235 as that singular model but the quality difference between it and deepseek or glm is huge.",
                  "score": 1,
                  "created_utc": "2026-02-11 13:15:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4slnq4",
              "author": "[deleted]",
              "text": "The best models are always text only, though, it seems.¬†",
              "score": 0,
              "created_utc": "2026-02-11 13:17:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4so35e",
                  "author": "power97992",
                  "text": "Opus is not text only¬†",
                  "score": 3,
                  "created_utc": "2026-02-11 13:31:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4so46f",
                  "author": "Front_Eagle739",
                  "text": "Kimi 2.5 isn't?¬†",
                  "score": 2,
                  "created_utc": "2026-02-11 13:32:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4sraqe",
          "author": "426Dimension",
          "text": "When on OpenRouter?",
          "score": 1,
          "created_utc": "2026-02-11 13:49:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqpta",
          "author": "aybarscengaver",
          "text": "good enough i guess\n\nhttps://preview.redd.it/c7ncr1rvdvig1.jpeg?width=1080&format=pjpg&auto=webp&s=e2bb1ae8090db4295f157985b50358015402c100",
          "score": 0,
          "created_utc": "2026-02-11 13:46:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxbk0",
              "author": "Odd-Ordinary-5922",
              "text": "cool benchmark",
              "score": 9,
              "created_utc": "2026-02-11 14:22:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t0t5g",
                  "author": "razorree",
                  "text": "it's like, write a code without using 'goto' :)",
                  "score": 6,
                  "created_utc": "2026-02-11 14:41:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tyvby",
          "author": "Lan_BobPage",
          "text": "Wonderful, another model I cant run. It seems this year will be very challenging all around.",
          "score": 0,
          "created_utc": "2026-02-11 17:24:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4t6lnc",
          "author": "WackyConundrum",
          "text": "Oh, it's just 19th post about the same thing.",
          "score": -2,
          "created_utc": "2026-02-11 15:10:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ti0ag",
          "author": "alaap001",
          "text": "Looks like we have new  enhanced agentic capabilities \n\nhttps://youtube.com/shorts/AYqGHNgJy1o?feature=share",
          "score": 0,
          "created_utc": "2026-02-11 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sw9dc",
          "author": "paramarioh",
          "text": "This is LocalLLama. From my point of view if it is not local then it shouldn't be here. Only LOCAL models deserves to be here. This is not a place to put it here more fucking ADS",
          "score": -13,
          "created_utc": "2026-02-11 14:17:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4u2xsg",
              "author": "ttkciar",
              "text": "Yes, this is LocalLLaMA, but GLM-5 weights have been published to Huggingface and are available for download and local use: https://huggingface.co/zai-org/GLM-5/tree/main\n\nThat makes this announcement totally on-topic.\n\nI cannot host GLM-5 on my current hardware, and I'm guessing you cannot either, but that's beside the point.  There are users here who *can*, and there will likely be distillations which will fit in your hardware and mine.\n\nYou can also download the weights now and host them later if/when you are able to upgrade to hardware which can manage it.",
              "score": 2,
              "created_utc": "2026-02-11 17:43:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4syduh",
              "author": "Pink_da_Web",
              "text": "Stop being annoying, isn't GLM an open-source model? Then why are you complaining? Downvote",
              "score": 5,
              "created_utc": "2026-02-11 14:28:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4u3k3y",
                  "author": "ttkciar",
                  "text": "> \\> isn't GLM an open-source model?\n\nAt the risk of sounding pedantic, it is not an open-source model.  It is an open-***weights*** model.  For it to be open-*source* they would need to publish their training data and software too.\n\nNonetheless, open-weight models are on-topic for LocalLLaMA, so it's fine.",
                  "score": 3,
                  "created_utc": "2026-02-11 17:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4szh60",
                  "author": "paramarioh",
                  "text": "This is LocalLLama. Not an ADS sub. I cannot check the local model. You should to be logical",
                  "score": -5,
                  "created_utc": "2026-02-11 14:34:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t06r3",
              "author": "jkh911208",
              "text": "You can‚Äôt afford GLM5 locally doesn‚Äôt mean everyone else can‚Äôt afford it",
              "score": 4,
              "created_utc": "2026-02-11 14:38:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t2rjy",
                  "author": "paramarioh",
                  "text": "Dogs always barking",
                  "score": -5,
                  "created_utc": "2026-02-11 14:51:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4t6wj0",
              "author": "mrwang89",
              "text": "This is Local**LLama**. From my point of view if it is not llama then it shouldn't be here. Only LLAMA models deserves to be here. This is not a place to put it here more fucking ADS\n- this is you",
              "score": 0,
              "created_utc": "2026-02-11 15:12:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tuu6u",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": -1,
          "created_utc": "2026-02-11 17:05:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sjuph",
          "author": "Philosophicaly",
          "text": "https://preview.redd.it/gkl1ufaq6vig1.jpeg?width=1179&format=pjpg&auto=webp&s=5c72aa30778f46eb6a8979793e926211fd572723\n\nmeh",
          "score": -20,
          "created_utc": "2026-02-11 13:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4skg2m",
              "author": "LocoMod",
              "text": "Someone doesn‚Äôt understand ‚Äúknowledge cutoff dates‚Äù.",
              "score": 18,
              "created_utc": "2026-02-11 13:10:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4skv45",
              "author": "Technical-Earth-3254",
              "text": "This means it has no system prompt (or close to none). Which is not really a bad thing if you know how this LLM stuff works.",
              "score": 3,
              "created_utc": "2026-02-11 13:12:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxgkd1",
      "title": "CPU-only, no GPU computers can run all kinds of AI tools locally",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/y9esf03tcvhg1.jpeg",
      "author": "JackStrawWitchita",
      "created_utc": "2026-02-06 12:41:35",
      "score": 559,
      "num_comments": 137,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3z7igx",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 21:55:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w8gc5",
          "author": "Techngro",
          "text": "I'm hopeful and confident that the future of AI is not in companies charging us to use their huge models, but in the average person running local models that are intelligent enough to do complex tasks, but small enough to run on reasonably basic hardware (i.e. not a $10K multi-GPU rig), and tunneled via the internet to their mobile devices.\n\n ",
          "score": 225,
          "created_utc": "2026-02-06 13:05:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wirfv",
              "author": "NoobMLDude",
              "text": "Agree.\n\nMost recent Open models have come with:\n- MOE arch ( fast responses on small GPUs),\n- Hybrid Attention (even faster and less memory needs)\n- Small sizes too ( Gemma 270M , Qwen 0.6B, Granite 1B, LFM1.2B, etc)\n\nBasically serving even consumers without huge GPUs Is considered important because they know not everyone can afford huge GPUs racks \n\nSecondly recent research point that Small models are enough:\n- Nvidia - [Small Models are the future of Agentic AI](https://research.nvidia.com/labs/lpr/slm-agents/)\n- Meta - [Learning to Reason in 13 Parameters](https://arxiv.org/pdf/2602.04118)\n\nThe amount of money and effort big companies are investing to get models that are small, fast and can run anywhere could give pointers to where we are headed.",
              "score": 72,
              "created_utc": "2026-02-06 14:03:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41x070",
                  "author": "ramendik",
                  "text": "Where are those users who want to use 1Bs on their toasters? I mean phones etc. I did some distilling from Kimi K2 into Granite 1B - made it more fun and not dumber; I want to do more with it, especially on long context, but very few are willing to test it out. Though yeah mamba hybrid, supported by llama cop from some time in October or November 2025, not sure if there's a phone app with that fresh a llama.cpp already",
                  "score": 2,
                  "created_utc": "2026-02-07 09:03:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3wno4e",
              "author": "belgradGoat",
              "text": "This is the hope for the society. If we go down the route of cloud only everything, it is one step away from total slavery. And I don‚Äôt mean it llm only- cars, computers, houses, corporations want us to rent everything.",
              "score": 15,
              "created_utc": "2026-02-06 14:28:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wqjuc",
              "author": "Dazzling_Focus_6993",
              "text": "\"search for it but do not trust to hope. It has forsaken these lands.\"¬†",
              "score": 5,
              "created_utc": "2026-02-06 14:43:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wmrr1",
              "author": "Asleep-Ingenuity-481",
              "text": "this is why I think the ram shortage might be a good thing, it'll hopefully make the Chinese want to push smaller, more powerful models to assure the Westerners can use them. We've already seen Alibaba doing this with their Qwen line, but what happens if Deepseek decides \"Yeah lets drop R2 with a mini version with 30b parameters\"",
              "score": 19,
              "created_utc": "2026-02-06 14:24:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41rqag",
                  "author": "tecneeq",
                  "text": "The Chinese don't invest billions in their models out of the kindness of their hearts for westerners.\n\nThey want to disrupt large AI companies and at the same time use high end LLMs locally to strengthen grip on dissent. There is a theory of authoritarian capture, a point at which you can't overthrow an authoritarian regime, because it's surveillance infrastructure becomes to tight. Many believe China has passed this with AI supported social scoring. Basically, if you are a dissenter, your family members can't travel, study or in particularly harsh cases, work.",
                  "score": 4,
                  "created_utc": "2026-02-07 08:12:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yzmkc",
              "author": "log_2",
              "text": "> not a $10K multi-GPU rig\n\nWith the way RAM prices are going a non-GPU computer for running LLMs will have to be a $10K multi-GB rig.",
              "score": 5,
              "created_utc": "2026-02-06 21:16:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xdgvp",
              "author": "ZenEngineer",
              "text": "\"The Future\" longer term will see devices get more powerful. In particular once the current demand for datacenter devices is fulfilled. I dont doubt there will be bigger and bigger models that need to run on cloud, or devices that respond faster than you phone, but things go in cycles. But at the same time we'll probably have apps running locally what are now considered large models for most things and only calling out to the cloud for complicated questions.",
              "score": 2,
              "created_utc": "2026-02-06 16:34:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3y4c9n",
              "author": "twisted_nematic57",
              "text": "You and I both know that‚Äôs not happening unless they somehow give up a data collection source and subscription source out of the goodness of their hearts. We need to fight to normalize that.",
              "score": 1,
              "created_utc": "2026-02-06 18:41:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wt6gn",
          "author": "noctrex",
          "text": "Might I suggest also trying out the following models:\n\n[LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)\n\n[LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n\n[LFM2.5-VL-1.6B](https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B)\n\nThey are excellent for the small size and I use them quite a lot on my CPU-only docker machine.",
          "score": 45,
          "created_utc": "2026-02-06 14:57:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xb90m",
              "author": "lolxdmainkaisemaanlu",
              "text": "Hey bro I would like to get started with small models but the vocal minority here with 12 x 5090s make it seem like much can't be done without GPUs\n\nWould love to know the use cases and stuff u do with these small models, as I also have a cpu only machine which is just lying unused..",
              "score": 14,
              "created_utc": "2026-02-06 16:24:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xhytd",
                  "author": "noctrex",
                  "text": "I use the LFM2.5-1.2B-Instruct model in my [KaraKeep](https://github.com/karakeep-app/karakeep) instance, and it provides smart tags and summaries.\n\nI use LFM2.5-1.2B-Thinking for my Synology Office.\n\nThe LFM2.5-VL-1.6B is nice to read screenshots or photos with texts or links. For example I sit on my couch, watching some youtube videos in the living room, and I get presented a web link to check out during the video, I'm too lazy at that moment to manually type it, so I just take a photo of it and let the model create the link.",
                  "score": 14,
                  "created_utc": "2026-02-06 16:55:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zp44d",
              "author": "willrshansen",
              "text": "llama.cpp + LFM2.5-1.2B-Instruct => actually usably fast on CPU only",
              "score": 5,
              "created_utc": "2026-02-06 23:29:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44mq1r",
              "author": "RelicDerelict",
              "text": "I have problem with the thinking model, it can overthink literally simple prompt, it just keep circling around how to answer simple prompt instead of answering it, what can I do to remedy it? I am using Ollama.",
              "score": 1,
              "created_utc": "2026-02-07 19:14:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4512hv",
                  "author": "noctrex",
                  "text": "Thinking models need good quantizations to function proper, Especially the small models. I'm using Q8 For those.",
                  "score": 2,
                  "created_utc": "2026-02-07 20:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wgpc8",
          "author": "NoobMLDude",
          "text": "More power to you for not letting your lack of GPUs stop you from exploring the wonderful world of Local AI.\nHere‚Äôs a few more things you could try on your local setup:\n- Private meeting note taker\n- Talking assistant (similar to your chatterbox setup)\n\n[Local AI list](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 27,
          "created_utc": "2026-02-06 13:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xe4l3",
              "author": "JackStrawWitchita",
              "text": "Dude, you gotta remake those videos with KoboldCPP instead of Ollama. Ollama slows everything way, way down. ",
              "score": 15,
              "created_utc": "2026-02-06 16:37:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xeomx",
                  "author": "NoobMLDude",
                  "text": "Yes llamacpp and llama server is on the plan. \nThanks for the reminder. Now I need to find time to do it faster üòâ",
                  "score": 7,
                  "created_utc": "2026-02-06 16:40:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xlfk5",
                  "author": "That-Dragonfruit172",
                  "text": "Is using Ollama bad in general? I just got started and im using it too on my single gpu setup. Seems fast enough",
                  "score": 2,
                  "created_utc": "2026-02-06 17:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wceo1",
          "author": "dobkeratops",
          "text": "I was impressed with how fast gpt-oss-20b (q4) ran on a CPU. it's an MoE with 3billion active parameters supposedly, and it has good tool-calling support",
          "score": 21,
          "created_utc": "2026-02-06 13:28:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o7858",
              "author": "pmttyji",
              "text": "For GPT-OSS models, use MXFP4 quants(from ggml on HF) since those models are in native MXFP4 format.\n\nAnd don't quantize KVCache.",
              "score": 1,
              "created_utc": "2026-02-10 19:47:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wc1ru",
          "author": "JackStrawWitchita",
          "text": "Wow this thread seems to be upsetting some people! I didn't realise so many people were fixated on their hardware and want to use $$ to gatekeep others out of running LLMs locally.",
          "score": 100,
          "created_utc": "2026-02-06 13:26:38",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3wj7bq",
              "author": "bapirey191",
              "text": "Yes, even with medium-income people using their disposable to play around with local LLMs you will finda lot of gatekeeping elitism",
              "score": 44,
              "created_utc": "2026-02-06 14:05:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wjcr9",
              "author": "NoobMLDude",
              "text": "Don‚Äôt worry, there are also people like me trying to keep the gates open for everyone out there.\n\n[I‚Äôm trying to educate and inform](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV) about the benefits of using any sized Local Private AI instead of spending huge $$ on API based models or GPU racks. \n\nFeel free to burn money but only do it after you have tried the free options.",
              "score": 25,
              "created_utc": "2026-02-06 14:06:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wmf5g",
              "author": "c64z86",
              "text": "It's the same in the AI image generation sub, although the reception there is a little better because more people can be found there with more humble PCs. \n\nSome people who spent thousands on their RTX 5090s just don't like it when somebody with a potato PC can run the same things they can. They start to feel like their decision to spend that much money was invalidated.\n\nPlease keep showing this sub that we don't need an expensive GPU for AI becaude it gives those of us who don't have that much money to burn a lot of hope. It shouldn't be restricted to those who can afford a small server farm in their living room.",
              "score": 44,
              "created_utc": "2026-02-06 14:22:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3z3azx",
              "author": "cosmicr",
              "text": "The whole sub is filled with bots and gatekeepers.",
              "score": 7,
              "created_utc": "2026-02-06 21:34:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4054s9",
              "author": "SkyFeistyLlama8",
              "text": "Yeah, us laptop LLM folks get laughed at regularly too. Inference on all the things is my motto now, as long as you can comfortably run 4B and above models then you're good to go. RAM is all you need.\n\nI've got a Frankenstein's laptop monster of an inference stack with models loaded for CPU, GPU and NPU inference, all at the same time.",
              "score": 5,
              "created_utc": "2026-02-07 01:02:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3wjhc0",
              "author": "mystery_biscotti",
              "text": "The gatekeeping is annoying. The communities running local need all types. \n\nI'd love to see more posts on how the shoestring budget folks optimize their stuff, the use cases involved, that sort of thing. Would be nice to have a corner for CPU only, one for 8-12GB cards, etc.",
              "score": 17,
              "created_utc": "2026-02-06 14:07:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3x0i8b",
              "author": "nonaveris",
              "text": "It doesn‚Äôt.  When GPUs were more expensive than memory, I just loaded up on tons of DDR5 RDIMMs to make up the difference.  \n\nYes, a Xeon Scalable isn‚Äôt exactly a normal CPU, but the markets were actually inverted enough for a while that grabbing memory was a better option.",
              "score": 5,
              "created_utc": "2026-02-06 15:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44n2su",
              "author": "RelicDerelict",
              "text": "Fuck those elitists. Keep posting",
              "score": 2,
              "created_utc": "2026-02-07 19:16:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xevxp",
              "author": "Herr_Drosselmeyer",
              "text": "No, I simply don't want somebody to lead people down a path to nowhere. What you're doing is completely impractical and a colossal waste of time. God forbid somebody actually buys some crap machine like the one you posted, then they'll be wasting money too, money that could have gone towards buying something decent down the line or just biting the bullet and using cloud compute.",
              "score": -12,
              "created_utc": "2026-02-06 16:41:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xjf6y",
                  "author": "JackStrawWitchita",
                  "text": "Gatekeeper. You want to show off the fact that you've spend hundreds, or thousands, on a rig and feel proud that most people can't afford that. This makes you feel special.\n\nAnd it horrifies you that many people can now do the same types of work flows as those with expensive GPUs on their dad's old desktop that was gathering dust in the corner.\n\nThere are people spending money for CharacterAI and other services when they could be doing simple RP chats for free on old hardware locally. \n\nSo many simple fun experiments for free on old hardware seems to upset you. Hilarious.",
                  "score": 11,
                  "created_utc": "2026-02-06 17:02:24",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o44nkln",
                  "author": "RelicDerelict",
                  "text": "LoL how much you wasted on setup you are not using to full potential? ü§£",
                  "score": 2,
                  "created_utc": "2026-02-07 19:18:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3xx9g6",
          "author": "Old-Negotiation6930",
          "text": "Im running a 3b abliterated model on a raspberry pi 5, quad core, 8gb ram, latency for first streamed token is usually < 20 seconds, using it to roast friends on our discord server\n\nhttps://preview.redd.it/oovxqwcnzwhg1.jpeg?width=1290&format=pjpg&auto=webp&s=051f0908201f294dc060c7511f7960ee2deed0bc",
          "score": 14,
          "created_utc": "2026-02-06 18:08:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ykyeq",
              "author": "JackStrawWitchita",
              "text": "Yes!",
              "score": 3,
              "created_utc": "2026-02-06 20:02:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o49isn4",
              "author": "DreadPoor_Boros",
              "text": "Now that is what gaming is all about! \\*wipes tears of joy\\*",
              "score": 2,
              "created_utc": "2026-02-08 15:23:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44c2ep",
              "author": "milanove",
              "text": "What model",
              "score": 1,
              "created_utc": "2026-02-07 18:21:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44wn0u",
                  "author": "Old-Negotiation6930",
                  "text": "huihui_ai/llama3.2-abliterate:3b",
                  "score": 1,
                  "created_utc": "2026-02-07 20:06:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3whvy4",
          "author": "deepsky88",
          "text": "Nah better buy 4 x 5090 to measure token per second without checking the answer",
          "score": 42,
          "created_utc": "2026-02-06 13:58:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x199f",
              "author": "StardockEngineer",
              "text": "I must read the answer one.....word.....at.....a.....time!",
              "score": 9,
              "created_utc": "2026-02-06 15:37:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3x4hfb",
              "author": "Lesser-than",
              "text": "this is what having fast tps does to you, combine that with a thinking llm, TLDR but it printed a crap ton of stuff so it must be good. there is usefull limits like, read speed over fast generation is perfectly fine, though that severly cuts into agentic code cli's which expect you to not read along.",
              "score": 4,
              "created_utc": "2026-02-06 15:52:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wl2ys",
          "author": "SneakyInfiltrator",
          "text": "My server has an i7-6700 with 16GB of DDR4, it would be cool if i could run some sort of assistant, nothing too crazy. I'm gonna give it a try. Thanks.",
          "score": 9,
          "created_utc": "2026-02-06 14:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wnkk2",
              "author": "choddles",
              "text": "I have ran ollama on a r7810 with dual 10 core xeons with 64G ddr4, yes it's not image creation but as much interactive text as you need",
              "score": 2,
              "created_utc": "2026-02-06 14:28:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wpogn",
          "author": "dynamite-ready",
          "text": "I have a fair bit of RAM on my machine (32GB), and was interested in running a low-mid size model in potato mode, but it's just too slow. I'm VRAM poor (6GB), but the sub 8B models on low quantisation run like a kicked squirrel.\n\nI wrote a bit about my experience if anyone is thinking about it, with some advice on optimisation (in Llama CPP) - https://raskie.com/post/we-have-ai-at-home",
          "score": 7,
          "created_utc": "2026-02-06 14:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41y8pj",
              "author": "ramendik",
              "text": "Will you maybe come test my attempts of style distilled Kimi K2 into Granite? What's currently working is the 1.5b, will fly on the 6Gb GPU even unquantized with a theoretically infinite context but frankly this is only the first stage, the long context needs more work. I'm kinda in need of feedback, including negative, to see what I can do better. https://huggingface.co/ramendik/miki-pebble-20260131\n\nAn 1.5b can only do so much, of course. But I want to polish the version as best I can while also looking at going bigger (running a trial run of the distill into Ministral3 14b now)",
              "score": 2,
              "created_utc": "2026-02-07 09:15:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xh60j",
              "author": "JackStrawWitchita",
              "text": "I run 12B LLMs with no GPU and 32GB ram and only an i5-8500. Absolutely great for text generation. ",
              "score": 1,
              "created_utc": "2026-02-06 16:51:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44i17t",
                  "author": "Grid_wpg",
                  "text": "I haven't run a local LLM yet, but I'm\nI've been reading about it for a while, and I'm super interested.\nI have a 12GB 3060 or an 8GB 3070 I can play with, and I know they're not going to be super fast.\n\nBut, I'm commenting here, because last summer I bought a custom work station PC for cheap because it was crashing.  I found the cause and fixed it. \n\nSo for $300 CAD, I got a dual 10-core xeon system (20 core/ 40 thread) with 192GB of ECC DDR 4 memory.  Plus PSU, case etc.\n\nI'm wondering what kind of model / performance I could get from just trying that out.",
                  "score": 1,
                  "created_utc": "2026-02-07 18:50:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wuirv",
          "author": "pidgeygrind1",
          "text": "Built a chinese V4 xeon board 14c/28t with 64gb ddr4 ECC ram and a 1080ti for 420bucks .\n\nRuns 70B",
          "score": 6,
          "created_utc": "2026-02-06 15:04:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xbwe3",
              "author": "lolxdmainkaisemaanlu",
              "text": "Damn bro! You must've built this before the ridiculous RAM prices, right? Don't tell me you did it in 2026?!",
              "score": 2,
              "created_utc": "2026-02-06 16:27:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41jfbb",
                  "author": "pidgeygrind1",
                  "text": "Correct, last quarter of 2025.\n\n30bucks for 64gb (4x 16gb) OEM Dell/Micron ECC DDR4, lucky\n\n150 for the 1080ti.",
                  "score": 5,
                  "created_utc": "2026-02-07 06:55:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3x1jv0",
          "author": "tmvr",
          "text": "I have a machine with those specs but in an USFF form factor. The i5-8500T CPU and 32GB DDR4-2666 dual-channel memory.  It definitely is good for small models and thanks to the amount of RAM you can have a couple in memory at the same time as well. Qwen3 Coder 30B A3B is pretty good on it as well, it does 8 tok/s with the Q6\\_K\\_XL quant (I wanted to fill the RAM) and if I remember correctly it hits 12 tok/s with the Q4\\_K\\_XL version.\n\nNot sure if you are using it already, but for image generation you could try *fastsdcpu*:\n\n[https://github.com/rupeshs/fastsdcpu](https://github.com/rupeshs/fastsdcpu)\n\nIt's a fun little project, I occasionally looked at the progress they make because I'm just glad someone was doing something like that. The last update was a while back, but I guess it is pretty mature at this stage.",
          "score": 7,
          "created_utc": "2026-02-06 15:38:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xidyc",
              "author": "tiffanytrashcan",
              "text": "I believe the Koboldcpp project has implemented parts of that for image gen. They have a tiny image model that is only 800mb and can produce a result in less than 10s on CPU.",
              "score": 3,
              "created_utc": "2026-02-06 16:57:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xl3ie",
                  "author": "JackStrawWitchita",
                  "text": "Yeah, there are a number of tools that can produce \\*an image\\* faster, but the quality and control isn't as flexible as SD 1.5. The SD ecosystem has a bunch of different safetensor finetune models and loras and stuff to make good image results even with CPU only hardware. For example I use inpainting and img2img a lot and SD 1.5 gives me a lot of control and options the 'fast models' don't.\n\nSpeed isn't everything - as my wife often tells me... ",
                  "score": 2,
                  "created_utc": "2026-02-06 17:10:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wupey",
          "author": "migsperez",
          "text": "I used Whisper locally on an i5 8500t without GPU to transcribe a handful of highly important meeting recordings, each about 20 mins long. It was great, did a fine job. It was better than multiple online AI services which I had tried.",
          "score": 5,
          "created_utc": "2026-02-06 15:04:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xhc2n",
              "author": "JackStrawWitchita",
              "text": "Awesome! Whisper rocks.",
              "score": 3,
              "created_utc": "2026-02-06 16:52:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xbt6a",
          "author": "Ulterior-Motive_",
          "text": "Even though I have a [pretty capable](https://www.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/) system at home, at work I have a spare Dell OptiPlex 5040 that I loaded up with 32GB of DDR3 memory for running a Q4\\_K\\_XL quant of Qwen3 30B A3B for when I don't feel like switching to our external network. If I need a quick, simple answer, then the \\~9 t/s I get out of it is plenty.",
          "score": 4,
          "created_utc": "2026-02-06 16:26:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y59ui",
          "author": "Small-Fall-6500",
          "text": ">I‚Äôm running Linux Mint on an old Dell optiplex desktop with an i5-8500 processor, 6 threads and 32GB of RAM. You can pick up one of these refurbished for something like $120.\n\nI don't think these are $120 any more, especially not with 32 GB RAM.",
          "score": 6,
          "created_utc": "2026-02-06 18:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ysi2d",
          "author": "Suitable-Program-181",
          "text": "Respect brother!\n\nI find more joy in doing more with less.\n\nTheres no skill in just dropping more $$ at the problem; thats how intel killed moore law and now everyone thinks we need data centers to run LLM's.\n\nCheers to you!",
          "score": 4,
          "created_utc": "2026-02-06 20:40:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y9ywg",
          "author": "No-Detective-5352",
          "text": "Regarding music generation, have you tried to run the ACE-Step-1.5 models? I found their capability was pretty good. These came out recently, and the smallest model (using Qwen3-0.6B) requires only 4Gb of memory at int8 quantization. On a 3090 this can generate a 3-minute song in about 10 seconds, so maybe it can do the same on a mid-level CPU in a couple of minutes?",
          "score": 4,
          "created_utc": "2026-02-06 19:08:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ykj9u",
              "author": "JackStrawWitchita",
              "text": "Sounds interesting but my calculations say it would take about half an hour to churn out one 3 minute track on my humble potato. That's a bit more than I'm happy to wait, especially as I'm guessing this kind of thing takes several iterations to get right.\n\nBut thanks for the suggestion and I'll keep my eye on that project.",
              "score": 2,
              "created_utc": "2026-02-06 20:00:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y26e5",
          "author": "TigerBRL",
          "text": "I'm sorry but I'm a bit out of loop on the localLLM thing. I've been interested for a long time but due to GPU limitations I haven't learnt it. \n\nWhat's the difference in using a GPU and not using a GPU. Like in technical terms, the inner workings and the sacrifices",
          "score": 3,
          "created_utc": "2026-02-06 18:31:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yt54k",
          "author": "epSos-DE",
          "text": "CPU bit-logic AI with decision trees is 6X faster than any GPU !\n\n  \nBecause Bitlogic outperforms vector calculations by pruning out the decision space by half at every decision step !\n\n  \nIf done well, it can be 1000X more perfromant than vector search on the GPU !",
          "score": 3,
          "created_utc": "2026-02-06 20:43:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41yi15",
              "author": "ramendik",
              "text": "Sounds interesting! How do you train that and are there models to try out?",
              "score": 1,
              "created_utc": "2026-02-07 09:18:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o46e7fz",
          "author": "Boricua-vet",
          "text": "I agree 100% with you and to add to your very insightful post. A small model that has been optimized will outperform way larger models. If you train a very small model for a specific task, it will blow away any larger model you can put in there. I see my friends spending all this crazy money on 5090's and they tell me whey need to spend all this money in order to train their models. I ask them how many models they train a year and they tell me under 10 models a year. I just laugh because it cost me 3 to 5 dollars per trained model on runpod. \n\nThink about it, 10 models at 3 to 5 bucks per model is like 40 to 50 bucks a year. 10 years at max is 500 bucks. In 10 years.  5090 costs 4000 or more. \n\nMoral of the story is, you can rent a a crazy expensive GPU for a few dollars in order to train a small model that will give you really good output on CPU for pennies on the dollar and it will outperform much larger models.\n\nhttps://preview.redd.it/e9q4ky8f76ig1.png?width=1665&format=png&auto=webp&s=b2d0a7c77615a21abdc04efc768da684e014a192\n\nRTX PRO 6000 with 96GB vram, 16 cores and 188GB of RAM for 1.89 an hour..\n\nI am not promoting runpod, I am just showing you that to train a model, you do not need to spend crazy money. It will cost you a few dollars, that it. \n\nAfter you train it and optimize it, you can run it on CPU and get fast response and really good token generation as it is a small optimized model and will outperform any model out there as it has been trained for that specific task. \n\nGood luck people.",
          "score": 3,
          "created_utc": "2026-02-08 01:12:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w5xsg",
          "author": "TheSpicyBoi123",
          "text": "If a system is turing complete and with enough storage you might as well run it on a carrot. The only question is practicality and time you are willing to wait for a model to cook. For the 100-200 USD/EUR ballpark however, you can do \\*much\\* better then that dell optiflex heap in terms of compute. I'd seriously recommend you consider those dual 2011-3 things and as a general rule, anything other then dell. Alternatively, why not invest the same 100-200 USD/EUR into a gpu and get an order+ of magnitude performance uplift? ",
          "score": 8,
          "created_utc": "2026-02-06 12:50:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w6oru",
              "author": "JackStrawWitchita",
              "text": "The point is you don't need to spend $¬£ etc to run local LLMs.\n\nI know there's a big vibe here with people flexing their five figure rigs and that's great. But it can be off-putting for vast swathes of the population who only have old potatoes for hardware. I'm just trying to help everyone get on the local LLM bandwagon with whatever means available.",
              "score": 27,
              "created_utc": "2026-02-06 12:54:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w7whs",
                  "author": "DeltaSqueezer",
                  "text": "Instead of $120 for an optiplex, you might as well get a 2nd hand GPU or two to run LLMs more quickly and cheaply. e.g. two P102-100 is cheap and decent.",
                  "score": 7,
                  "created_utc": "2026-02-06 13:02:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3x08va",
                  "author": "0-brain-damaged-0",
                  "text": "Also if you queue up several jobs, you can run it while you sleep.",
                  "score": 2,
                  "created_utc": "2026-02-06 15:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3w7z68",
                  "author": "TheSpicyBoi123",
                  "text": "I get your point, and sure, you dont need shoes to run but you cant argue that the shoes help a lot. The issue is also that if you have to wait \\~minutes for it to generate something at all vs seconds it stops being realtime interactive and becomes a chore especially with LLM's.   \n  \nAdditionally, the dell optiflex is such a turd that you are better of \\*not\\* having a computer then having that computer. ",
                  "score": -3,
                  "created_utc": "2026-02-06 13:02:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3xe1a7",
              "author": "Very_Large_Cone",
              "text": "Another consideration is electricity prices in addition to up front costs. I am in Germany and paying 30 cents per kwh. So my cheap cpu only nuc uses 6W at idle and 30W at full load. I actually have a gaming rig with a GPU that is available but often stays powered off other than when I am doing something where speed matters.",
              "score": 2,
              "created_utc": "2026-02-06 16:37:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xwdzs",
                  "author": "TheSpicyBoi123",
                  "text": "I am also in Germany and I... am less fortunate in terms of power draw (probably the PC will pull 1.5w-2kw at load that I have :( )",
                  "score": 1,
                  "created_utc": "2026-02-06 18:04:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o40oxv4",
              "author": "davidy22",
              "text": "You can be less than turing complete and still be able to run LLMs, GPUs are literally just limited instruction set parts that do math faster",
              "score": 2,
              "created_utc": "2026-02-07 03:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wp9c4",
          "author": "repolevedd",
          "text": "Great point. Personally, I think SD on that hardware is pushing it a bit, but I‚Äôm with you on the rest. I‚Äôve got a 3060, yet my little M910q with a 6500T and 24GB of RAM is the real workhorse for LLMs, slowly but surely handling tasks daily. When I need more speed, I just hit a shortcut on my PC to fire up llama-swap with the models I need, and nginx on my home server automatically reroutes everything to it, tapping into the power of the 3060 and the extra RAM.",
          "score": 2,
          "created_utc": "2026-02-06 14:37:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sxdfd",
              "author": "saren_p",
              "text": "Say more please?\n\nif I understand correctly, you have a home mini-pc (M910q) running local LLMs. When you need more juice your M910q¬†taps into your other PC running on 3060? Is that it at a high-level?\n\nI'm thinking of installing [https://github.com/lfnovo/open-notebook](https://github.com/lfnovo/open-notebook) on a mini-PC with Linux (currently shopping for one, can't decide what to get), and I'm wondering if any of the mini-PC's in the $300-$500 range can run models smart enough to power open-notebook (low-med usage, not thousands of documents), and if not, can I point open-notebook to my PC (windows) with 3060?\n\nMy goals: keep data offline, secured, tailscale only ssh, everything runs on the mini-pc, and taps into extra juice on the 3060 if needed (but I guess this would mean the data is sent to the 3060 PC?)",
              "score": 1,
              "created_utc": "2026-02-11 14:23:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4t8k80",
                  "author": "repolevedd",
                  "text": "I‚Äôll try to explain in more detail, but first off - I don't actually know the specific requirements for open-notebook. It‚Äôs unclear if it uses a built-in RAG for notes or which specific models it relies on for things like podcast generation, so I can't give you a definitive recommendation on which hardware to buy for that specific use case.\n\nAs for my local LLM setup: I use an M910q mini-PC as my home server (i5-6500T, 24GB RAM). I got it for somewhere between ‚Ç¨50 and ‚Ç¨90, I can‚Äôt quite remember. It runs Immich and several other services via Docker Compose, including a stack consisting of:\n\n* llama-swap + llama.cpp: To launch models on demand.\n* Open WebUI: For direct interaction with the LLMs.\n* Caddy: (I switched from Nginx recently because Caddy makes health checks much easier).\n* Various other services: For web searching, data parsing, etc.\n\nWhere the 3060 comes in:\n\nThat GPU is in my main, more powerful PC. Since my services don't talk to the models directly but instead use an OpenAI-compatible endpoint, I can proxy that endpoint to either the llama-swap instance on the mini-PC or the one on the 'big' rig with the 3060 12GB.\n\nTo handle this, my Caddyfile looks something like this (simplified for clarity):\n\n    :8080 {\n        reverse_proxy {\n            to http://192.168.0.11:8080   # My GPU PC\n            to http://llama-swap:8080      # Local CPU\n            \n            lb_policy first                # Requests go to the first available server\n    \n            health_path     /v1/models\n            health_interval 10s\n            health_timeout  5s\n            health_status   2xx\n            flush_interval -1\n        }\n    }\n\nOn my desktop with 3060 12Gb, I have a separate directory with llama-swap, llama.cpp, and the same models I have on the M910q, plus some beefier ones that only a GPU can handle.\n\nThanks to the health check settings, Caddy pings both instances. As soon as I fire up llama-swap on my main PC, Caddy automatically starts routing traffic there. Open WebUI and other services don‚Äôt even know the backend has switched, they just see new models appearing in the list. They talk to the Caddy container, and whatever happens behind the scenes is invisible to them.\n\nRegarding Tailscale: I don‚Äôt use it personally because it relies on a coordination server I don‚Äôt control. Instead, I use a somewhat chaotic mix of rathole, Nginx, and Caddy (some on a VPS) to expose my endpoints, even to my phone. But Tailscale is a solid choice if you prefer it. You could easily run open-notebook in the same stack and access it from anywhere.\n\nRegarding hardware advice: it‚Äôs tough to recommend a specific device in the $300-$500 range because prices vary by region, and I‚Äôm not sure which models you‚Äôll need. My M910q runs HY-MT1.5 7B Q4 for translations (slowly, but it works - for example, this message is translated by this), various Gemma 3 versions for OCR and simple scripting, and other models for deep research tasks. If I need to edit something complex, I switch to the GPU-heavy models.\n\nI think you should first figure out exactly what you need to run open-notebook and check the requirements for the models you plan to use. Once you have those specs, it‚Äôll be much easier to decide on the hardware.",
                  "score": 1,
                  "created_utc": "2026-02-11 15:20:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wukbh",
          "author": "Django_McFly",
          "text": "More power to you but 3 minutes for a single 512x512 image sounds like hell.",
          "score": 2,
          "created_utc": "2026-02-06 15:04:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xicy6",
              "author": "JackStrawWitchita",
              "text": "How many hours a day to you spend creating images? For me it's once in a blue moon I'll need a graphic. Happy to fire off a set and get a coffee and when I come back the images are there. I also work through the prompts in the background while I do something else on my laptop. It's really no problem. Multitasking is easy.\n\nAnd I imagine many people with huge costly GPU rarely use to the to full extent and most of them sit idle for many hours per day, despite the expense. ",
              "score": 5,
              "created_utc": "2026-02-06 16:57:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z83c7",
          "author": "Echo9Zulu-",
          "text": "8th gen intel is supported by OpenVINO which may give faster prefill at longer context. Definitely check that out for some free brr",
          "score": 2,
          "created_utc": "2026-02-06 21:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zavc0",
          "author": "rog-uk",
          "text": "I just brought a Dell 3050 i5 16GB RAM, it's going to be mostly an always on hub for a variety of small projects, but I am interested in the possibility of using it for smaller LLM models running overnight, I guess I will see if it seems worth it, but since it will be on anyway it's worth a try. My bigger workstation makes the planet and my energy bill cry, so that can't stay on all of the time.\n\n\nFollowing this thread for tips!",
          "score": 2,
          "created_utc": "2026-02-06 22:12:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zbcln",
          "author": "TheJrMrPopplewick",
          "text": "Take a look at the Gemma3n models. They are very good performers on CPU only hardware.",
          "score": 2,
          "created_utc": "2026-02-06 22:14:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42qpgs",
          "author": "Mac_NCheez_TW",
          "text": "I run Qwen 3 on an AMD 6core H6600 APU with 64gb of DDR5 on a cheap ass mini PC from Amazon. I get some decent coding done with it. I wish it was a little faster but it's okay for basic stuff.¬†",
          "score": 2,
          "created_utc": "2026-02-07 13:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44m8kw",
          "author": "RelicDerelict",
          "text": "Thanks for much for this post, I have old 16GB laptop I gonna test some of those things üôè",
          "score": 2,
          "created_utc": "2026-02-07 19:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46hvsk",
          "author": "Clean-Appointment684",
          "text": "shiiet, i\\`m also having fun a little with CPU only  \ni have workstation with i7-13700K/32RAM and pc with Ryzen 5 4500u/16RAM\n\nso on workstation i easily run qwen3-coder-next q2 with 4-5 t/s of the output. combining it with opencode and splitting tasks with subagents. for at least hour in generates pretty decent documentation of the existed code. didn\\`t try at generating new code, unfortunately. context for around 50k tokens, it sounds stupid - but works great  \n  \nalso i'm fooling aroung with chatterbox on my PC for some generative voice with example input. it easily generates 5 minute long speech for around 10 mins, maybe a little longer. but never tried to run llm on it.",
          "score": 2,
          "created_utc": "2026-02-08 01:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49kdfv",
          "author": "DreadPoor_Boros",
          "text": "Good stuff mate!  \nI will be keeping an eye on this thread, as a fellow potato user.  \n\n\nBut seeing that model mentioned was not on my bingo card.",
          "score": 2,
          "created_utc": "2026-02-08 15:31:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wq3bl",
          "author": "Durgeoble",
          "text": "just a question,\n\n how well works with a shared memory GPU? can you put the 32GB on it?",
          "score": 1,
          "created_utc": "2026-02-06 14:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xhsin",
              "author": "JackStrawWitchita",
              "text": "I'm happy with the CPU handling everything. ",
              "score": 1,
              "created_utc": "2026-02-06 16:54:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wyhts",
          "author": "nonaveris",
          "text": "Xeon Scalable 8480+ isn‚Äôt horribly fast at octochannel (leave that to the 9480!), but it is at least on the edge of usable for llama3 and imagegen.  \n\nThink of it at its top end 307GB/s as being on par with older or inference optimized GPUs.",
          "score": 1,
          "created_utc": "2026-02-06 15:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xugm3",
          "author": "-lq_pl-",
          "text": "Try a small MoE model like GLM 4.7 Flash. It should run decent even on pure CPU.",
          "score": 1,
          "created_utc": "2026-02-06 17:55:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ymu4s",
              "author": "JackStrawWitchita",
              "text": "My calculations say I'd be lucky to get one token per second on my old potato running GLM 4.7 Flash. I'm being told MoE is great for GPU but not very good for cpu only.",
              "score": 1,
              "created_utc": "2026-02-06 20:12:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y5hlr",
          "author": "Prince_ofRavens",
          "text": "Activate the slow clap modal",
          "score": 1,
          "created_utc": "2026-02-06 18:47:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41gfdr",
          "author": "HmelissaOfficial",
          "text": "Tried to run it on 4gb ram and Intel graphics card it's too slow and ollama is hard to install on win 10 lite edition, which others you suggest for this specs?",
          "score": 1,
          "created_utc": "2026-02-07 06:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41pa39",
              "author": "JackStrawWitchita",
              "text": "Ollama sucks. Don't waste your time on it. Remember that you are at the extreme low end with your hardware so don't expect too much.\n\nHere's what I would do with your hardware:\n\n1) replace windows 10 with Linux Mint XCFE - it's free, lightweight and frees up resources. Windows 10 is bloatware. \n\n2) install koboldCPP / Kobold lite - there are videos on how to do this or ask Kimi AI or similar AI chatbot\n\n3) download Qwen2.5 1.5B gguf and TinyLlama 1.1B gguf and see which one works best for you. Depending on your CPU (you didn't specify but I'm guessing it's low end) you should get perhaps 5 tokens per second for text generation, which isn't bad at all. And these tiny models will be good for general chat and even a bit of coding.",
              "score": 3,
              "created_utc": "2026-02-07 07:49:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46sii6",
          "author": "graymalkcat",
          "text": "Agree. It‚Äôs just slow.¬†",
          "score": 1,
          "created_utc": "2026-02-08 02:42:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m55sc",
          "author": "Worgle123",
          "text": "I've got an Acer Swift running a Ryzen 7 4700U, 16GB RAM.  \n\nThanks to MoE I'm able to run GPT-OSS-20B with 14 of the 24 layers offloaded to the \"GPU\" and get reasonably usable token speeds of about 6 tok/sec.  \n\nTypically speaking, I'll run Qwen3-8B (Q4-KM), fully offloaded to the \"GPU\" which yields about 10 tok/sec.  \n\nAs you said, Upscayl is great.  I do use that.  I've experimented with Qwen3-TTS, but aside from very short snippets of text, it's too slow to use on a regular basis.  If I'm generating anything long, I'll offload the task to a GPU on Vast.\n\nNot to mention models like Llama 3.2 3B which run beautifully fast on my phone, at least until they run out of context...",
          "score": 1,
          "created_utc": "2026-02-10 13:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xq7v4",
          "author": "Ne00n",
          "text": "Yea, I run my LLM stuff on a 64GB DDR4 shitbox for 10$/m.",
          "score": 1,
          "created_utc": "2026-02-06 17:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yesu3",
          "author": "HopePupal",
          "text": "i'm running CPU-only on some old Intel MacBooks, calcing text embeddings and re-ranking search queries for social media, currently using [HuggingFace TEI](https://github.com/huggingface/text-embeddings-inference) with the ONNX backend and some of the BERT-ish models. these machines have 64 GB RAM and big SSDs but AMD Radeon 5xxxM dGPUs, duds from a ROCm perspective.\n\ngenerative LLMs are cute but the field of ML has so many more applications than just those",
          "score": 0,
          "created_utc": "2026-02-06 19:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3waeh1",
          "author": "Herr_Drosselmeyer",
          "text": ">Response times are fast enough\n\nMaybe if you have the patience of a saint. \n\n>OK, it takes 3 minutes to generate a 512x512 image\n\nThat would drive me up the wall. I guess it's different if you have no experience of something better, but my rig takes less than 3 **seconds** to generate a 1024x1024 image. 60 times faster for double the resolution, so let's call it 120 times faster. \n\nYes, it can be done. No, it's not efficient and it's not fun, unless your idea of fun is watching paint dry.\n\n",
          "score": -10,
          "created_utc": "2026-02-06 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wbnky",
              "author": "JackStrawWitchita",
              "text": "How much did you spend?\n\nI spent 0 as this old gear was just sitting around.",
              "score": 16,
              "created_utc": "2026-02-06 13:24:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wedie",
                  "author": "Herr_Drosselmeyer",
                  "text": "I spent a lot of money, but you're spending a lot of time. I will almost always trade money for time. ",
                  "score": -15,
                  "created_utc": "2026-02-06 13:39:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3wh9bu",
              "author": "december-32",
              "text": "*Quadruple the resolution maybe?",
              "score": 4,
              "created_utc": "2026-02-06 13:55:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wmvli",
                  "author": "Herr_Drosselmeyer",
                  "text": "Fair. ",
                  "score": 1,
                  "created_utc": "2026-02-06 14:24:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40ja3y",
          "author": "Euphoric_Emotion5397",
          "text": "This is like saying I don't need a car to get to another state, i just need a bicycle.  \nSure. But time and tide waits for no man and we cannot earn back our time.",
          "score": -2,
          "created_utc": "2026-02-07 02:29:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40z8w7",
              "author": "JackStrawWitchita",
              "text": "And many people buy expensive cars just to drive to the supermarket around the corner...",
              "score": 3,
              "created_utc": "2026-02-07 04:13:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4147y9",
                  "author": "Euphoric_Emotion5397",
                  "text": "if the expensive cars take them there same time as the normal cars, then ya, you might have a case.\n\nBut, a rtx 5070TI can do a 512x512 in under 10seconds versus your 300 seconds.  \n2.9 minutes of your life waiting for a image. it compounds quickly. :D",
                  "score": 1,
                  "created_utc": "2026-02-07 04:49:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r28xxz",
      "title": "GLM-5 scores 50 on the Intelligence Index and is the new open weights leader!",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/gauvtw6qfxig1.jpeg",
      "author": "abdouhlili",
      "created_utc": "2026-02-11 20:40:32",
      "score": 556,
      "num_comments": 133,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r28xxz/glm5_scores_50_on_the_intelligence_index_and_is/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4v70fj",
          "author": "abdouhlili",
          "text": "GLM-5 has the LOWEST hallucination rate on AA-Omniscience\n\nhttps://preview.redd.it/q8za1v0whxig1.png?width=1828&format=png&auto=webp&s=1c4bf3c1c6c6590ee9ded2466ca45d01d8a81b23",
          "score": 190,
          "created_utc": "2026-02-11 20:52:39",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4vacxi",
              "author": "LagOps91",
              "text": "huge if true. getting LLMs to not make shit up is one of the most significant open problems.",
              "score": 93,
              "created_utc": "2026-02-11 21:08:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vdrdk",
                  "author": "DistanceSolar1449",
                  "text": "It‚Äôs not quite an open problem anymore. Recent papers mostly solved it. Most notably https://arxiv.org/pdf/2509.04664 (but plenty of people talked about similar stuff before this paper came out, I‚Äôm sure I can find one of my comments that predates this paper)\n\nIt‚Äôs pretty simple, to be honest. The reward function always rewarded hallucinations. \n\nIt‚Äôs like a teenager taking a standardized test guessing filling in a random multiple choice answer, rather than leaving it blank. Does that make the teenager hallucinating? Not really, it just statistically is in the favor of the test taker if they have any information above random choice: they‚Äôre incentivized to guess at an answer. \n\nThe solution is simple, to set the reward function to favor epistemological uncertainty, and punish confident errors harshly. \n\nOpenAI basically grades answers no longer like a multiple choice test, but rather gives partial credit to correct levels of uncertainty.",
                  "score": 96,
                  "created_utc": "2026-02-11 21:25:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4y2276",
                  "author": "s101c",
                  "text": "Doesn't help if shit was made up in the training data, but impressive nonetheless.",
                  "score": 2,
                  "created_utc": "2026-02-12 07:41:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v9d4s",
              "author": "4hanni",
              "text": "Wow, this is huge. My experience with the major closed source models - Gemini 3 Pro, GPT 5.2, Claude Sonnet 4.5 and Opus 4.5 - is pretty consistent with that benchmark in terms of hallucinations, e.g. I could not understand the hype around Gemini 3 Pro - pretty smart model but not really usable for me because of bad prompt adherence, short outputs and terrible hallucination rate (Nano Banana Pro is great tho).",
              "score": 14,
              "created_utc": "2026-02-11 21:04:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vqz20",
              "author": "Fault23",
              "text": "https://preview.redd.it/25rt4wb5zxig1.png?width=196&format=png&auto=webp&s=e6939454c195e7e7f2cdf1769dcf46a467dcfe0c",
              "score": 3,
              "created_utc": "2026-02-11 22:29:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4vqdbo",
              "author": "LazloStPierre",
              "text": "At this point literally the only benchmark I take seriously¬†",
              "score": 2,
              "created_utc": "2026-02-11 22:26:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4xm7gy",
              "author": "HenkPoley",
              "text": "Most of that is from rejecting to answer.",
              "score": 1,
              "created_utc": "2026-02-12 05:21:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4yc2uj",
                  "author": "Front_Eagle739",
                  "text": "Im good with it refusing to answer if it knows it will be making it up?",
                  "score": 1,
                  "created_utc": "2026-02-12 09:19:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4xt1xv",
              "author": "bnm777",
              "text": "Look at all of the results -not as good as you're making it out to be and you know it\n\n\nhttps://artificialanalysis.ai/evaluations/artificial-analysis-intelligence-index",
              "score": 1,
              "created_utc": "2026-02-12 06:18:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ymwtl",
              "author": "Sockand2",
              "text": "Tested on search and hallucinated wildly. I dont give more reliability to benchmarks (or better call them shitbenchs). Too much money in play",
              "score": 1,
              "created_utc": "2026-02-12 11:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z0jom",
              "author": "Immediate_Occasion69",
              "text": "and their previous version was one of the highest! they definitely cooked with this one",
              "score": 1,
              "created_utc": "2026-02-12 12:48:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v67mx",
          "author": "Turbulent_Pin7635",
          "text": "So the open-source is just centimeters away from the closed ones... And there are even some nukes to be released =)",
          "score": 91,
          "created_utc": "2026-02-11 20:48:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vcmcd",
              "author": "No_Swimming6548",
              "text": "I think it's obvious that's Chinese CAUGHT American companies in 2026.\n\nEdit: typo",
              "score": 39,
              "created_utc": "2026-02-11 21:19:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vfumr",
                  "author": "Turbulent_Pin7635",
                  "text": "God bless that amazing country. Let's celebrate this night with Bƒõijƒ´ng K«éoyƒÅ!",
                  "score": 19,
                  "created_utc": "2026-02-11 21:35:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4yjw9o",
                  "author": "SerdarCS",
                  "text": "They're still 3-6 months behind. They will be caught up if and when people actually switch from gpt and claude to chinese models NOT for price or open source, but for actual capability.",
                  "score": 2,
                  "created_utc": "2026-02-12 10:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4z3gys",
              "author": "amunozo1",
              "text": "Open weights*. I wonder if, once the Chinese companies surpass the American ones, they start closing their models to maintain this distance.",
              "score": 3,
              "created_utc": "2026-02-12 13:07:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4z3hj3",
              "author": "amunozo1",
              "text": "Open weights*. I wonder if, once the Chinese companies surpass the American ones, they start closing their models to maintain this distance.",
              "score": 1,
              "created_utc": "2026-02-12 13:07:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4v6nl3",
          "author": "Turbulent_Pin7635",
          "text": "I would love if someday when announcing it they publish how much memory it is needed to run the thing. =(",
          "score": 25,
          "created_utc": "2026-02-11 20:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vavqn",
              "author": "SkullkidV1",
              "text": "If you have to ask you cant afford it /s",
              "score": 55,
              "created_utc": "2026-02-11 21:11:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vg714",
                  "author": "ResidentPositive4122",
                  "text": "If you have to ask you're out of kidneys already.",
                  "score": 9,
                  "created_utc": "2026-02-11 21:36:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4vh04k",
                  "author": "abdouhlili",
                  "text": "Laughing in 8x RTX Pro 6000.",
                  "score": 10,
                  "created_utc": "2026-02-11 21:40:32",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4vfywg",
                  "author": "Turbulent_Pin7635",
                  "text": "Laughing in M3 ultra",
                  "score": 2,
                  "created_utc": "2026-02-11 21:35:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4vhi8j",
              "author": "sid_276",
              "text": "8xH200s. The FP8 of vLLM is about 800GB give or take, before any KV cache",
              "score": 12,
              "created_utc": "2026-02-11 21:42:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vhv55",
                  "author": "Turbulent_Pin7635",
                  "text": "Q4 + M3 ultra. I don't want to be a server... Just write papers privately. =)",
                  "score": -3,
                  "created_utc": "2026-02-11 21:44:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4v9mp0",
              "author": "JaredsBored",
              "text": "Rule of thumb, you need 2x the ram of parameter count to run fp16 (plus some for kv cache). Q8/Fp8 is about 1x ram of parameter count. An rtx pro 6000 plus a 4th/5th gen epyc with 768GB ram could run this at Q8 with decent speeds. Q4 is definitely doable on a 512GB Mac",
              "score": 10,
              "created_utc": "2026-02-11 21:05:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vg7jh",
                  "author": "Turbulent_Pin7635",
                  "text": "Thxs I'm in the later situation",
                  "score": 2,
                  "created_utc": "2026-02-11 21:36:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vrbv5",
          "author": "Fault23",
          "text": "https://preview.redd.it/srwy9q6ezxig1.png?width=535&format=png&auto=webp&s=34f29d9afe939a1bc539125608d13b00fc919906\n\nand with the price that's rougly equal to gemini 3 flash",
          "score": 14,
          "created_utc": "2026-02-11 22:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wkk01",
              "author": "Damakoas",
              "text": "That's the starting price. Open models over API tend to go way down in price very quickly. ",
              "score": 6,
              "created_utc": "2026-02-12 01:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ye1uz",
                  "author": "SwimmingSquare7933",
                  "text": "But i think it is very cheap, we should left some space for the open source company to earn some money to live, right? ",
                  "score": 1,
                  "created_utc": "2026-02-12 09:39:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vk1er",
          "author": "Alex_1729",
          "text": "GLM5 thinking equals Opus Thinking? Damn! And their plans are so affordable, I might subscribe now.",
          "score": 19,
          "created_utc": "2026-02-11 21:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4wv7jx",
              "author": "frozandero",
              "text": "Only downside is that they lack the gpu infrastructure so requests are slower, and can sometimes hang for a few seconds randomly during high usage times.",
              "score": 6,
              "created_utc": "2026-02-12 02:20:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4xw08b",
                  "author": "Alex_1729",
                  "text": "That's why I'm waiting a bit until they roll it out properly.",
                  "score": 1,
                  "created_utc": "2026-02-12 06:45:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xe4fm",
          "author": "Comfortable-Rock-498",
          "text": "Less than 3 months ago, Gemini-3-pro preview launched with huge anticipation and was supposedly performing off-the-charts in everything. Now an open-weight model overtakes it in terms of overall performance WHILE beating it and everything else in terms of low hallucination. This is consequential. ",
          "score": 10,
          "created_utc": "2026-02-12 04:20:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vhcw3",
          "author": "sid_276",
          "text": "Open source is now trailing just about 3 months behind closed source, and closing that gap quickly. DeepSeek v4 comes after lunar new year so end of February btw. And I have heard great things.",
          "score": 26,
          "created_utc": "2026-02-11 21:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4vqwge",
              "author": "ConnectionDry4268",
              "text": "Isn't it going 1 month-6 month behind. Cause R1 was closest one that almost caught upto the Closed source",
              "score": 4,
              "created_utc": "2026-02-11 22:29:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vyljb",
                  "author": "sid_276",
                  "text": "So it caught up with 5.2 xhigh. Which was released at start of December. So trailing 2-3 months but as I said gap getting smaller very fast",
                  "score": 6,
                  "created_utc": "2026-02-11 23:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4v5c58",
          "author": "abdouhlili",
          "text": "GLM-5 beating Opus 4.5 and GPT-5.2-xhigh.\n\nDeepseek-V4 will use same DSA architecture..... But will be BIGGER.",
          "score": 36,
          "created_utc": "2026-02-11 20:44:36",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4v9cmc",
              "author": "oxygen_addiction",
              "text": "We have no idea what DS V4 will be. Let's not get hyped for nothing. Nobody has a moat at this point, not even Anthropic/OpenAI/Google.",
              "score": 20,
              "created_utc": "2026-02-11 21:03:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4vbrna",
                  "author": "abdouhlili",
                  "text": "You didn't read enough DeepSeek white papers.",
                  "score": 4,
                  "created_utc": "2026-02-11 21:15:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4wf2rn",
              "author": "zball_",
              "text": "DeepSeek v4 will apparently be some extremely sparse attention and have like 1M ctxlen.",
              "score": 2,
              "created_utc": "2026-02-12 00:42:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4y2qhk",
              "author": "Zeeplankton",
              "text": "As long as deepseek stays as stupid cheap as it currently is. Lol",
              "score": 1,
              "created_utc": "2026-02-12 07:47:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vqvoh",
          "author": "Fault23",
          "text": "can't wait for the minimax m2.5 and new deepseek's results",
          "score": 5,
          "created_utc": "2026-02-11 22:28:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vwoxf",
          "author": "abu_shawarib",
          "text": "Big if true",
          "score": 5,
          "created_utc": "2026-02-11 22:59:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vfl9d",
          "author": "bartskol",
          "text": "üëÄ Google  üëÄ",
          "score": 3,
          "created_utc": "2026-02-11 21:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xfr91",
          "author": "Late_Hour2838",
          "text": "honestly is this even a gap anymore\n\nI get that there still hasn't been a moment where an open model has beat every closed lab to be number one for once, and especially when we count what the closed labs have in the works or ready for launch they might not let that happen  \n  \nbut still this progress is insane",
          "score": 3,
          "created_utc": "2026-02-12 04:32:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4w80sk",
          "author": "doradus_novae",
          "text": "BRUHHHHH GIVE ME THE SWEET SWEET AMBROSIA\n\n",
          "score": 2,
          "created_utc": "2026-02-12 00:01:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xq1g0",
          "author": "zhaolionsh",
          "text": "In my own codebase (which includes both front-end and back-end repositories, about 300,000 lines of code), I tried to implement a back-end API and a front-end page. The API calls didn't work (due to some complex front-end and back-end field and type mappings, although these were already included in the API comments). It took about 30 communications to barely get it implemented (no time to deal with the UI). I feel like it's even weaker than Sonnet 4.5... I'm not sure if it's an issue with using it through Claude Code. Also, I bought the coding plan, but there are still occasional interruptions, and I have to take over and type \"continue\". Overall, the experience is still quite far from ideal. The price is barely reasonable, but the insufficient computing power and programming ability still haven't met my passing requirements. Keep up the good work, put a lot of pressure on Anthropic and OpenAI.",
          "score": 2,
          "created_utc": "2026-02-12 05:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xuz4w",
          "author": "Potential_Block4598",
          "text": "1/20th of the price ?!!!",
          "score": 2,
          "created_utc": "2026-02-12 06:35:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4wiz15",
          "author": "callme__v",
          "text": "The sweet $8 plan per quarter is gone üòî. (Even though it has no GLM-5). I guess need to try via API first. The real-world performance hasn't lived upto to my expections (Eg. Kimi K2.5,GLM 4.7) in the past (Artificial Analysis ratings).\n\nAny one has any source to discounted access to GLM-5? Use cases: Claude code/ openclaw",
          "score": 1,
          "created_utc": "2026-02-12 01:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yei2x",
              "author": "SwimmingSquare7933",
              "text": "I bought the GLM Max, and I think it is pretty good to enjoy. Anyway, I'm not sure if it will give some savings, because the Chinese New Year is coming soon",
              "score": 1,
              "created_utc": "2026-02-12 09:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4yzu6v",
              "author": "True-Shelter-920",
              "text": "just get the max at this point for 1 month after testing with it on openrouter, upgrade if u own a pro/lite plan ",
              "score": 1,
              "created_utc": "2026-02-12 12:43:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y5fhe",
          "author": "Correct-Wing-6884",
          "text": "Nice, the app is already live. GLM is truly helpful for learning as it makes knowledge points easier to understand. Overall, I find it very easy to align with. Aside from Gemini, it's the tool I use most, but its mobile app layout is quite poor because the layout it adopts is similar to the bubble - style used in chat, with large blank spaces on both sides, which always requires frequent line breaks. This is especially true as the aspect ratio of current smartphones is becoming more like that of a remote control.",
          "score": 1,
          "created_utc": "2026-02-12 08:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o504m3z",
          "author": "easylifeforme",
          "text": "Can someone answer what sort of system would actually be needed to run this model? Open source but most likely needs to be ran on a rented server?",
          "score": 1,
          "created_utc": "2026-02-12 16:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vdp6c",
          "author": "INtuitiveTJop",
          "text": "And here I am rediscovering Gemma 3 with equal excitement",
          "score": 1,
          "created_utc": "2026-02-11 21:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yaitm",
              "author": "kaisurniwurer",
              "text": "It's a good one, but the lack of system prompt (weird implementation of it) annoys the hell out of me.",
              "score": 3,
              "created_utc": "2026-02-12 09:03:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ymsgy",
                  "author": "INtuitiveTJop",
                  "text": "Absolutely, I am using it because it doesn‚Äôt hit those triggers that make it sound like ai and its creative writing is good. For coding it‚Äôs terrible, it also can‚Äôt handle long context. But I can get it opus 4.6 writing and it cleans it up really well. It‚Äôs got a very niche use for me but I think I‚Äôll use it for a long time.",
                  "score": 3,
                  "created_utc": "2026-02-12 11:01:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vat2c",
          "author": "llama-impersonator",
          "text": "weird, i was not impressed with pony at all.\n\nALSO, artificial failysis sucks.",
          "score": -10,
          "created_utc": "2026-02-11 21:11:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qym566",
      "title": "I trained a 1.8M params model from scratch on a total of ~40M tokens.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qym566",
      "author": "SrijSriv211",
      "created_utc": "2026-02-07 18:57:42",
      "score": 535,
      "num_comments": 105,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o45jnx7",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 22:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44v2d6",
          "author": "FPham",
          "text": "Creating model from scratch is the hardcore LLM stuff.  Kudos (if we are still using those in 2026)",
          "score": 49,
          "created_utc": "2026-02-07 19:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44wgox",
              "author": "SrijSriv211",
              "text": "I've always been interested in training my own llm from scratch so yeah here we are I guess.",
              "score": 19,
              "created_utc": "2026-02-07 20:05:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ik6l4",
                  "author": "UnifiedFlow",
                  "text": "Do you have a blog or git repo tracking your work? I want to get into this and could use the resources.",
                  "score": 2,
                  "created_utc": "2026-02-09 22:43:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45y0xk",
          "author": "cosmicr",
          "text": "amazing you've pretty much reached GPT-2 level of quality on such a smaller scale.\n\nGiven your training data set, I can see lots of applications for this sort of thing in games. That is if the gaming community can ever get over the use of AI as a tool.\n\nHow big was the final model on disk?",
          "score": 27,
          "created_utc": "2026-02-07 23:33:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o478a7l",
              "author": "SrijSriv211",
              "text": "> That is if the gaming community can ever get over the use of AI as a tool.\n\nSo true.\n\n> How big was the final model on disk?\n\n25 MBs",
              "score": 24,
              "created_utc": "2026-02-08 04:27:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49f8gt",
                  "author": "Palmquistador",
                  "text": "Daaang that is small. Awesome project!",
                  "score": 9,
                  "created_utc": "2026-02-08 15:04:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hangp",
                  "author": "wektor420",
                  "text": "This is actually a very good size for deployment on mobile phones",
                  "score": 3,
                  "created_utc": "2026-02-09 18:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44m63w",
          "author": "1ncehost",
          "text": "This is very cool. EleutherAI discord would probably be interested and has a lot of expertise that can help.",
          "score": 45,
          "created_utc": "2026-02-07 19:11:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44ms1h",
              "author": "SrijSriv211",
              "text": "Thank you!",
              "score": 9,
              "created_utc": "2026-02-07 19:14:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o49qy6y",
              "author": "itsmekalisyn",
              "text": "Is their discord active still?",
              "score": 2,
              "created_utc": "2026-02-08 16:04:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44lrey",
          "author": "Single_Ring4886",
          "text": "Did you considered to do some \"post training\" to teach model single of just few actually useful \"tricks\"? The simplest thing which occurs to me is for example to detect names in text so you could make them via simple script into \"bold\". I think such \"practical\" applications for very small and very fast and cheap models is what open source could really shine in comparison to huge universal models.",
          "score": 13,
          "created_utc": "2026-02-07 19:09:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45b7oe",
              "author": "Budget-Juggernaut-68",
              "text": "We have those already. they're called NER models.",
              "score": 8,
              "created_utc": "2026-02-07 21:24:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44mqcm",
              "author": "SrijSriv211",
              "text": "Yeah I'm thinking of post training. That's one of things I'll be working on next. First I want the pre-training to give even better results. I don't a loss of 3.5 is really that good. I'm also going to scale the base dataset size and model size a little more. This was more a stress test to check if it can generate good text with just 1M non-embedding parameters on such a diverse and dense dataset or not.",
              "score": 3,
              "created_utc": "2026-02-07 19:14:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44nfwu",
                  "author": "Single_Ring4886",
                  "text": "Good speed :) because once small model (which you can use even on cpu) is \"useful\" with something practical people might start using it :) and it would be more than just one time experiment.",
                  "score": 4,
                  "created_utc": "2026-02-07 19:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45g87m",
          "author": "Standard-Influence67",
          "text": "I wonder if you do post train nowÔºåit can produce reasonable outputÔºåor you need to scale the parameters to do so?",
          "score": 4,
          "created_utc": "2026-02-07 21:51:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45gn7w",
              "author": "SrijSriv211",
              "text": "I'll post train and also scale parameters and dataset. Post training is my first priority right now.",
              "score": 2,
              "created_utc": "2026-02-07 21:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45hg28",
                  "author": "Standard-Influence67",
                  "text": "cool. but I wonder if keep this parameters then only do post train can let the model produce reasonable output or not.so maybe you can find out.",
                  "score": 2,
                  "created_utc": "2026-02-07 21:58:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45tnr1",
          "author": "Madrawn",
          "text": "The idea seems clever. I think I might nap the code and run a couple tests myself.\n\nHave you compared how it fares against a basic GPTMini (\\[LayerNorm, Self-attention, Residual connection, LayerNorm, MLP\\]-blocks) network of similar parameter count and shape? That's usually were my \"novel\" architectures go to die. But also, if it performs vastly different/worse it's usually a sign of a bug, which are hard to notice if it works at all.\n\nThese networks can compensate for a lot of architectural mistakes at a performance/quality cost.\n\nAs for data sets, any reason why you're not using any of the hundreds available on huggingface? Tinystories for simple text, alpaca-python for instruct python code, wiki-text(needs some cleaning for LLMs) and openwebmath for stress testing. Those I tend to use for stuff like this.\n\nEdit: You seem to prepend the sink token at every single step. Is that intentional? It essentially makes your context grow twice as fast.",
          "score": 4,
          "created_utc": "2026-02-07 23:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4761ef",
              "author": "SrijSriv211",
              "text": "> Have you compared how it fares against a basic GPTMini ([LayerNorm, Self-attention, Residual connection, LayerNorm, MLP]-blocks) network of similar parameter count and shape?\n\nI did train Andrej Karpathy's nanoGPT on same dataset and tried to keep similar number of parameters. Strawberry seems to perform far better than that.\n\n> if it performs vastly different/worse it's usually a sign of a bug\n\nyes strawberry was performing weirdly in training. Retention was not working well with SPDA. The problem was that the generated weights were too noisy for SPDA. AFT managed to handle that however SPDA couldn't. That's why I added post normalization in both `produce` and `forward` functions in Retention. That fixed the bug completely.\n\n> As for data sets, any reason why you're not using any of the hundreds available on huggingface? Tinystories for simple text, alpaca-python for instruct python code, wiki-text(needs some cleaning for LLMs) and openwebmath for stress testing. Those I tend to use for stuff like this.\n\nTBH. I was just bored. Had nothing to do so I decided to waste my time by manually scrapping datasets. lol! Also the reason why I didn't use TinyStories cuz it's just too simple.\n\n> You seem to prepend the sink token at every single step. Is that intentional? It essentially makes your context grow twice as fast.\n\nYeah that's intentional. That's for attention sink. Similar idea is implemented in GPT-OSS as well. Also it doesn't grow the context. Think like this. input `<|sink|>Test prompt` -> model predicts `ing` which makes it `Test prompting`. Notice how I dropped `<|sink|>` in the final results. That's what's happening. I'll implement it at an architecture level similar to GPT-OSS",
              "score": 4,
              "created_utc": "2026-02-08 04:11:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45zlol",
          "author": "Iory1998",
          "text": "Cool work. I wish you good luck for future iterations.",
          "score": 4,
          "created_utc": "2026-02-07 23:44:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4764dm",
              "author": "SrijSriv211",
              "text": "Thank you :)",
              "score": 2,
              "created_utc": "2026-02-08 04:12:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44qh8c",
          "author": "Tiny_Arugula_5648",
          "text": "It's funny most people haven't ever seen a real hallucination.. The weird rambling babbling that is almost coherent but not really..  That's what you get from small models.. Never really understood why people started calling false statements hallucinations when it went mainstream. The moment you read a real hallucination like this it really does make sense to call them hallucinations because it reads like someone who is totally out of their minds on something. ",
          "score": 16,
          "created_utc": "2026-02-07 19:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44qo84",
              "author": "SrijSriv211",
              "text": "Haha yes üòÇ",
              "score": 6,
              "created_utc": "2026-02-07 19:34:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44mz8a",
          "author": "1ncehost",
          "text": "By the way, a lot of SLM training work is consolidated in the nanogpt speedruns to glean from. Not poo pooing because im an enthusiast in this space also and appreciate toy models like this. Looking forward to your updates.",
          "score": 6,
          "created_utc": "2026-02-07 19:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44ngbn",
              "author": "SrijSriv211",
              "text": "Yeah ik üòÖ I'm working on it just for fun. Usually when I'm exhausted after studying for my exams. lol! I'll keep working on it cuz it's really fun. I want to see how far can I push it.",
              "score": 2,
              "created_utc": "2026-02-07 19:18:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44nzs1",
                  "author": "1ncehost",
                  "text": "Warning: very deep rabbit hole lol! Enjoy!",
                  "score": 4,
                  "created_utc": "2026-02-07 19:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49s26j",
          "author": "gjsmo",
          "text": "Just curious, what's the training time (and hardware) like for such a small model? I would imagine it could be done on CPU only or basically any modern GPU, but I've never trained a model from scratch.",
          "score": 3,
          "created_utc": "2026-02-08 16:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49z5b5",
              "author": "SrijSriv211",
              "text": "It was trained on my old PC which has Intel i3 3rd gen, 8 GBs of ram and no GPU, and it took about 7-8 minutes per 100 steps. It took ~13 hrs to complete 10k steps of training.\n\nNOTE: It took 7-8 minutes per 100 steps cuz the retention mechanism is still pretty rough in terms of optimization. I'm working on it. The current draft I'm working on is able to train 100 steps in just 4-5 minutes with exact same setup.",
              "score": 3,
              "created_utc": "2026-02-08 16:44:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4a0iib",
                  "author": "gjsmo",
                  "text": "Wow - so I'd imagine almost any GPU could do it in minutes. Could be very interesting to play around with completely different training data or optimization techniques!",
                  "score": 1,
                  "created_utc": "2026-02-08 16:50:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44ovsn",
          "author": "tob8943",
          "text": "Why is it repeating your prompt",
          "score": 2,
          "created_utc": "2026-02-07 19:25:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44pssc",
              "author": "SrijSriv211",
              "text": "It's not repeating the prompt. In the `generate` function I just append the original prompt before the generated tokens after the generation is complete.",
              "score": 7,
              "created_utc": "2026-02-07 19:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44pxhs",
                  "author": "tob8943",
                  "text": "thanks for answering",
                  "score": 3,
                  "created_utc": "2026-02-07 19:30:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45ewts",
              "author": "ResidentPositive4122",
              "text": "Base models (or pre-trained) don't have a \"prompt\" in the sense that we use with modern LLMs (anything after gpt3.5). Their \"prompt\" is simply the beginning of a piece of text. And they generate the next probable token on that beginning. You would need to take this model and fine-tune it on prompt - answer pairs to have it work as a modern LLM.",
              "score": 3,
              "created_utc": "2026-02-07 21:44:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o451lew",
          "author": "mukz_mckz",
          "text": "This is cool! What hardware did you use and what did the training time look like?",
          "score": 2,
          "created_utc": "2026-02-07 20:32:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o452tnl",
              "author": "SrijSriv211",
              "text": "It was a stress test for the architecture so I trained it on my super low end potato PC. It has (Ik you might not believe it) intel i3 3rd gen cpu, 8 gbs of ram and no gpu. It took \\~7-8 minutes per 100 steps and the entire training was complete in just \\~13 hours.\n\nhttps://preview.redd.it/b1rejavvv4ig1.png?width=655&format=png&auto=webp&s=69082a7a1a6458c5183339ba6dab5bd3213a5f19\n\n  \n",
              "score": 10,
              "created_utc": "2026-02-07 20:39:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o452vtv",
                  "author": "SrijSriv211",
                  "text": "https://preview.redd.it/n1exql7zv4ig1.png?width=924&format=png&auto=webp&s=51b4c6398e6403af8d204b68fbc1aba337727672\n\n  \n",
                  "score": 5,
                  "created_utc": "2026-02-07 20:39:58",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o45otuk",
                  "author": "citaman",
                  "text": "Maybe you can try the `google colab` with gpu instance or `kaggle` with double gpu instance with some free instance per week to ever speed up or have a bigger model like 10M :D ",
                  "score": 3,
                  "created_utc": "2026-02-07 22:38:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o452q0w",
          "author": "BasketFar667",
          "text": "Very cool, but can it talk to the user, like \"Hello?\"? Can I try it if so?",
          "score": 2,
          "created_utc": "2026-02-07 20:39:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4543j5",
              "author": "SrijSriv211",
              "text": "It's just a pre-trained model. No post-training applied so it can't really talk like \"Hello. MODEL: HI! How are you?\" kinda thingy. Though it can generation conversation sentences which you can see in one of the screenshots where it creates a conversation between Arthur & Dutch (2 characters from RDR2). You can download the model from the [releases page](https://github.com/SrijanSriv211/Strawberry/releases/tag/s1)",
              "score": 3,
              "created_utc": "2026-02-07 20:46:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o458ym1",
          "author": "Longjumping_Spot5843",
          "text": "Can it make a coherent sentence or nah?",
          "score": 2,
          "created_utc": "2026-02-07 21:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45a3pz",
              "author": "SrijSriv211",
              "text": "Sometimes it can. Considering how small the model is and how dense and diverse the dataset is. I don't expect a proper coherent sentence at this scale. At least without post training, nope. After post training the model might generate better coherent sentences.",
              "score": 2,
              "created_utc": "2026-02-07 21:19:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45khaw",
          "author": "INtuitiveTJop",
          "text": "This would be really cool for autocorrect on phones - something so small and light might be great at fixing sentences after the fact.",
          "score": 2,
          "created_utc": "2026-02-07 22:14:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474way",
              "author": "SrijSriv211",
              "text": "Yes. Also the combination of GLobal Linear attention + Local Standard MHA attention will also make it easy for phones to run!",
              "score": 2,
              "created_utc": "2026-02-08 04:03:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46e3m6",
          "author": "vinnybag0donuts",
          "text": "How'd you decide the architecture for the retention mechanism's wT, wC = wC, new\\_weights swap? It stores O(d¬≤) and derives L layers' worth of weights dynamically whereas I think typically transformers store O(L √ó d¬≤) parameters across L layers.",
          "score": 2,
          "created_utc": "2026-02-08 01:11:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4770d3",
              "author": "SrijSriv211",
              "text": "I did that cuz that was the only idea I had tbh. My intuition was to update current weights and swap it and repeat that again. That was slow, stable and easy to implement.",
              "score": 2,
              "created_utc": "2026-02-08 04:18:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o46e4lq",
          "author": "Pvt_Twinkietoes",
          "text": "Could you explain what you're trying to do like you're talking to a non-technical?",
          "score": 2,
          "created_utc": "2026-02-08 01:12:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474n37",
              "author": "SrijSriv211",
              "text": "I'm trying to generate the attention qkv parameters on the fly using the input prompt. In standard transformers the attention qkv parameters are learned during pretraining and are fixed during inference. In Strawberry they aren't.",
              "score": 2,
              "created_utc": "2026-02-08 04:02:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4759za",
                  "author": "Pvt_Twinkietoes",
                  "text": "What's the advantage of doing this?",
                  "score": 2,
                  "created_utc": "2026-02-08 04:06:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o477xe9",
          "author": "zball_",
          "text": "The attention part just sound like fast weight programmers nowadays. But a learnable FFN is definitely interesting.",
          "score": 2,
          "created_utc": "2026-02-08 04:25:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o478fjk",
              "author": "SrijSriv211",
              "text": "Yeah I took inspiration from fast weights and hypernetworks üòÖ",
              "score": 1,
              "created_utc": "2026-02-08 04:28:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47gq91",
          "author": "HillaryPutin",
          "text": "Wow that is remarkable fact recollection for a model that is just a few MB in size.",
          "score": 2,
          "created_utc": "2026-02-08 05:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47h1uk",
              "author": "SrijSriv211",
              "text": "Yeah! In terms of both text generation quality and final training loss, it is better than Andrej Karpathy's vanilla nanoGPT trained on same dataset and similar model size!",
              "score": 3,
              "created_utc": "2026-02-08 05:34:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47ps3u",
                  "author": "HillaryPutin",
                  "text": "What do you do for work? ",
                  "score": 2,
                  "created_utc": "2026-02-08 06:50:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47zy20",
          "author": "UnluckyAdministrator",
          "text": "Impressive! Very brave training your own model. Good worküëå",
          "score": 2,
          "created_utc": "2026-02-08 08:24:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4837ne",
              "author": "SrijSriv211",
              "text": "Thank you :D",
              "score": 1,
              "created_utc": "2026-02-08 08:55:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o485454",
          "author": "stuehieyr",
          "text": "Wish I can do that and use my custom optimizer which groks fast.",
          "score": 2,
          "created_utc": "2026-02-08 09:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48a7yb",
              "author": "SrijSriv211",
              "text": "Your optimizer groks fast!!?? How? That's so amazing!",
              "score": 2,
              "created_utc": "2026-02-08 10:01:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48an47",
                  "author": "stuehieyr",
                  "text": "I can give you a hint if that‚Äôs alright as the paper isn‚Äôt yet published üòÖ. So there‚Äôs Lambert W function right? You can make the learning rate ‚Äúbreathe‚Äù as per difficult examples vs easy examples using it, setting a dynamic learning rate. You can tweak Adam to have this lambert W self balance the learning rate and it will automatically spend more time in the hard landscapes and grok fast. But this only works when you do full FP16 fine tune or train. Quantized it didn‚Äôt work at all.",
                  "score": 2,
                  "created_utc": "2026-02-08 10:05:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49cmib",
          "author": "Particular_Garbage32",
          "text": "how did you learn to build from scratch ? did you have to use crazy math ?",
          "score": 2,
          "created_utc": "2026-02-08 14:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49e5go",
              "author": "SrijSriv211",
              "text": "I've always been interested in making my own llms and architectures. I watched Andrej Karpathy, 3blue1brown, welch labs and bycloud videos. I also read research papers and articles. TBH it's more of intuition than some crazy math. In fact the math for retention is remarkably simple. You just have to come up with some ideas and use some simple mathematics and logic in code. That's all.",
              "score": 2,
              "created_utc": "2026-02-08 14:58:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bktrs",
          "author": "HarjjotSinghh",
          "text": "\\*\"Ohhhh, ‚Äò40M tokens‚Äô‚Äîso you trained your AI on ‚Äòhow many words are in the Bible.‚Äô\"\\*",
          "score": 2,
          "created_utc": "2026-02-08 21:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cgcxt",
          "author": "FaithlessnessLife876",
          "text": "Interesting & Impressive!",
          "score": 2,
          "created_utc": "2026-02-09 00:16:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e26lc",
              "author": "SrijSriv211",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-02-09 06:05:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4d3f4o",
          "author": "Legal-Assistant-615",
          "text": "Have you open source it on github?",
          "score": 2,
          "created_utc": "2026-02-09 02:23:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dyg9w",
              "author": "SrijSriv211",
              "text": "Yes it's linked on the post",
              "score": 1,
              "created_utc": "2026-02-09 05:35:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dzipr",
          "author": "AdForward9067",
          "text": "this looks fantastic to me! I had always wanted to do this. Is the model available for download? Is it feasible to use it in toolrun? Because my coding works are primarily on Python, C#, C++ and JavaScript too, the larger model size out there are actually an 'extra' load for mine. I would like to try to run it in my envinronment",
          "score": 2,
          "created_utc": "2026-02-09 05:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e24jz",
              "author": "SrijSriv211",
              "text": "It's available for download in the releases page on the repo. The repo is linked. Unfortunately this is just a base model, it's not instruction tuned or agentic tool tuned. I'm working on that right now.",
              "score": 1,
              "created_utc": "2026-02-09 06:05:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ers5u",
          "author": "CaptTechno",
          "text": "what hardware did you require?",
          "score": 2,
          "created_utc": "2026-02-09 10:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4esdp6",
              "author": "SrijSriv211",
              "text": "https://preview.redd.it/2i2navf42gig1.png?width=514&format=png&auto=webp&s=a5054ce5657b3732c1c39bd7d23f643940daaf25\n\nIt's my old potato pc.",
              "score": 1,
              "created_utc": "2026-02-09 10:14:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ey6nn",
                  "author": "CaptTechno",
                  "text": "oh sick! might try to replicate this myself. good work!",
                  "score": 1,
                  "created_utc": "2026-02-09 11:08:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hb1tl",
          "author": "wektor420",
          "text": "Does it exhibit reduced exponent entropy like larger models?",
          "score": 2,
          "created_utc": "2026-02-09 18:58:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hctiz",
              "author": "SrijSriv211",
              "text": "I'm not sure about that. I'll have to check",
              "score": 2,
              "created_utc": "2026-02-09 19:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4hi0ya",
                  "author": "wektor420",
                  "text": "If it does you can compress it further on disk :)",
                  "score": 2,
                  "created_utc": "2026-02-09 19:32:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hfiqb",
          "author": "mtomas7",
          "text": "It would be interesting to see how this architecture would work for the 1800 model dataset: https://www.reddit.com/r/LocalLLaMA/comments/1qaawts/llm_trained_from_scratch_on_1800s_london_texts/",
          "score": 2,
          "created_utc": "2026-02-09 19:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hq3de",
              "author": "SrijSriv211",
              "text": "I know about this project. I presume Strawberry might be able to achieve similar performance with just like 1/10th or 1/20th of the parameters. Though I'll have to test it first. Thanks for reminding, I'll test it :)",
              "score": 2,
              "created_utc": "2026-02-09 20:12:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4htnfp",
          "author": "itsnikity",
          "text": "very cool, wow",
          "score": 2,
          "created_utc": "2026-02-09 20:30:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hul9q",
              "author": "SrijSriv211",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-09 20:35:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kpoog",
          "author": "Brilliant-Wolf7589",
          "text": "Ok, if you want Ideas now a version trained with Google sequential attenton (beware the name, is for training/pruning) should avoid redundancies in the moe models allowing same performances in less weights or more knowledge for the same bucks.¬†",
          "score": 2,
          "created_utc": "2026-02-10 06:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ktm3p",
              "author": "SrijSriv211",
              "text": "Thank you for the idea!!",
              "score": 1,
              "created_utc": "2026-02-10 07:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qcxcg",
          "author": "Crypto_Stoozy",
          "text": "Now let it LoRa it self on what ever it chooses",
          "score": 2,
          "created_utc": "2026-02-11 02:38:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qtdlw",
              "author": "SrijSriv211",
              "text": "It chose it be a scary robot lol!",
              "score": 2,
              "created_utc": "2026-02-11 04:24:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o462oye",
          "author": "kind_cavendish",
          "text": "One question, does it know about Megumin from konosuba? And if so, what does it know?",
          "score": 1,
          "created_utc": "2026-02-08 00:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o476a7s",
              "author": "SrijSriv211",
              "text": "I don't think it knows about that. The dataset doesn't contain Anime related stuff.",
              "score": 1,
              "created_utc": "2026-02-08 04:13:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwxtf8",
      "title": "BalatroBench - Benchmark LLMs' strategic performance in Balatro",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1qwxtf8",
      "author": "S1M0N38",
      "created_utc": "2026-02-05 21:12:37",
      "score": 525,
      "num_comments": 58,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3ubdfr",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-06 03:45:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sglyc",
          "author": "mitchins-au",
          "text": "Finally a real world eval",
          "score": 171,
          "created_utc": "2026-02-05 21:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uxsfy",
              "author": "m31317015",
              "text": "Legit something I didn't think of, super cool.",
              "score": 8,
              "created_utc": "2026-02-06 06:28:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sfbzg",
          "author": "jacek2023",
          "text": "\"If you own a copy of Balatro, you can make your local LLM play it.\" you have my attention",
          "score": 82,
          "created_utc": "2026-02-05 21:23:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x71rs",
              "author": "addandsubtract",
              "text": "Ironically, attention is all you need.",
              "score": 9,
              "created_utc": "2026-02-06 16:04:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sgonf",
          "author": "jd_3d",
          "text": "Can you try Opus 4.6 on it? Curios if it improves from 4.5",
          "score": 31,
          "created_utc": "2026-02-05 21:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3simb4",
              "author": "S1M0N38",
              "text": "Right now is playing. checkout the twitch stream",
              "score": 35,
              "created_utc": "2026-02-05 21:39:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3tspnf",
                  "author": "JsThiago5",
                  "text": "will cost 1k$ per match ",
                  "score": 27,
                  "created_utc": "2026-02-06 01:52:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4tc0j7",
                  "author": "Yes_but_I_think",
                  "text": "Hey what is Balatro?",
                  "score": 1,
                  "created_utc": "2026-02-11 15:37:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3t7u8i",
          "author": "Kholtien",
          "text": "I need a Dwarf Fortress eval",
          "score": 35,
          "created_utc": "2026-02-05 23:50:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wmf2g",
              "author": "IrisColt",
              "text": "You have my sword.",
              "score": 6,
              "created_utc": "2026-02-06 14:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3sqwxk",
          "author": "TomLucidor",
          "text": "If it is Jinja2-based then run DGM, OpenEvolve, SICA, or SEAL over it. See which LLM can self-evolve the fastest given the proper scaffold.",
          "score": 53,
          "created_utc": "2026-02-05 22:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3svnko",
              "author": "S1M0N38",
              "text": "I will look into those. Thanks",
              "score": 18,
              "created_utc": "2026-02-05 22:44:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3spjzv",
          "author": "Adventurous-Okra-407",
          "text": "One thing I wonder a lot for this eval is the Balatro release date. It existed since Feb 2024 and before that did not exist, so LLMs with more niche and more up to date info in their training data will have a big advantage over those that do not.\n\nThere are no books written about this game, for example.",
          "score": 19,
          "created_utc": "2026-02-05 22:13:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uynak",
              "author": "Yorn2",
              "text": ">There are no books written about this game, for example.\n\nIf there's wikis or even blog posts though they definitely are getting indexed. Videos probably as well.\n\nA friend of mine created a guide for an obscure MMORPG that almost no one plays despite it being a Western MMO. It's actually only recently gotten popular, but he wrote the guide slowly (I helped with a few things) and put it all online over the course of a few years. For years afterwards not a whole lot of people played it, but all these Chinese bots were still indexing his site. \n\nNow that GLM, Qwen, and others have came out, I'll ask these offline-only models questions about the game and it's crazy how often they actually SOUND LIKE HIM when they talk about the different NPCs and strategies for playing the game. And don't get me wrong, they still hallucinate a lot, but they clearly talk about stuff he does on his website/guide. No where else in the world is this info, so I know they got it from him.",
              "score": 19,
              "created_utc": "2026-02-06 06:35:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wx9a2",
                  "author": "my_name_isnt_clever",
                  "text": "Google has an ENORMOUS advantage for something like this, being able to train off YouTube data.",
                  "score": 5,
                  "created_utc": "2026-02-06 15:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3teloj",
          "author": "InternetExplorer9999",
          "text": "The only benchmark that matters",
          "score": 12,
          "created_utc": "2026-02-06 00:29:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t6ljb",
          "author": "X3liteninjaX",
          "text": "So insanely cool, I love random evals like this. Nice work!",
          "score": 7,
          "created_utc": "2026-02-05 23:43:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t9ezt",
          "author": "Briskfall",
          "text": "Strategic game benches like these are really fun to watch. Testing models for a novel, localized environment for their logic skills is akin to what chess/go research were later then generalized for broader ML applications.",
          "score": 7,
          "created_utc": "2026-02-05 23:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6dyw",
          "author": "Alarming_Bluebird648",
          "text": "this is actually a sick way to test reasoning depth. i wonder how a quantized 70b handles the late game shop decisions bc those are brutal",
          "score": 3,
          "created_utc": "2026-02-06 07:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vqpmq",
          "author": "reggionh",
          "text": "Gemini 3 Flash arguably has the most intelligence per $ right now. I have been very impressed. It's a bit quirky, like it makes typos & hallucinates at times but I can live with it. ",
          "score": 4,
          "created_utc": "2026-02-06 10:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3thzqh",
          "author": "ayelg",
          "text": "Super cool\n\nWhat are you using to run the stream?",
          "score": 3,
          "created_utc": "2026-02-06 00:48:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w2f60",
              "author": "S1M0N38",
              "text": "Docker with 3 xvfb display -> x11grab -> ffmpeg -> twitch rtmp (everything hosted in Digital Ocean droplet)\nNo OBS",
              "score": 9,
              "created_utc": "2026-02-06 12:26:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3w5a42",
                  "author": "typeomanic",
                  "text": "This guy knows ball",
                  "score": 1,
                  "created_utc": "2026-02-06 12:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wxk3s",
                  "author": "my_name_isnt_clever",
                  "text": "I can only imagine how much you've spent on Opus 4.6 with the stream still going. How long will it run before you'll be able to add it to the leaderboard?",
                  "score": 1,
                  "created_utc": "2026-02-06 15:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tu36o",
              "author": "PM_ME_UR_COFFEE_CUPS",
              "text": "Likely OBS",
              "score": 1,
              "created_utc": "2026-02-06 02:00:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3tpk7d",
          "author": "SeriousGrab6233",
          "text": "This is super sick. This makes me want to make a benchmark now for another game",
          "score": 3,
          "created_utc": "2026-02-06 01:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uwckd",
          "author": "Warthammer40K",
          "text": "oh thank god, my hands are gnarled and frozen into claws from playing Balatro 16 hours a day... now the computer can take over",
          "score": 3,
          "created_utc": "2026-02-06 06:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3toppy",
          "author": "FusionCow",
          "text": "we just benchmarking anything atp",
          "score": 4,
          "created_utc": "2026-02-06 01:28:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uhojc",
          "author": "Ill-Fishing-1451",
          "text": "Very interesting. Can you tell why some models outperform others? What are they doing better?",
          "score": 2,
          "created_utc": "2026-02-06 04:26:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x5lt8",
          "author": "tonyunreal",
          "text": "Oh wow, Opus 4.6 just successfully defused an Acrobat vs The Hook round, I'm speechless.",
          "score": 2,
          "created_utc": "2026-02-06 15:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xfccr",
              "author": "S1M0N38",
              "text": "Is this good or bad? I‚Äôve only played Balatro a few times",
              "score": 1,
              "created_utc": "2026-02-06 16:43:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xgvyb",
                  "author": "tonyunreal",
                  "text": "Bad scenario, very good thinking process from Opus. I would argue it has way better crisis-solving capability than me, haha.",
                  "score": 3,
                  "created_utc": "2026-02-06 16:50:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43wy77",
          "author": "LelouchZer12",
          "text": "Those error bar are pretty concerning",
          "score": 2,
          "created_utc": "2026-02-07 17:06:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44edkz",
              "author": "S1M0N38",
              "text": "Change from std. dev. to Confidence interval 95% in the new version (checkout the website). The new error bars mean that we are 95% sure that the average lies in the error bar. The previous error bars were capturing the data distribution (assuming normal) - this is a wrong assumption given the fact that data are capped at 24 (so the distribution is not symmetric). The average round as main metric is still sub-optimal. There is an issue open on coder/balatrollm where I plan to update how the average round is computed.\n\nThanks for point it out :)",
              "score": 1,
              "created_utc": "2026-02-07 18:32:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3se84l",
          "author": "NigaTroubles",
          "text": "Looks like qwen needs to release there Qwen4",
          "score": 3,
          "created_utc": "2026-02-05 21:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u26xj",
          "author": "Alan_Silva_TI",
          "text": "I don‚Äôt really dig Balatro, but something like this applied to turn-based CRPGs (which helps a lot with timing) especially ones that support multiplayer would be an instant viral hit.\n\nI‚Äôve been thinking about this a lot, and I‚Äôm pretty sure that in the near future many games will allow players to use AI (most likely LLMs) as local multiplayer participants.\n\nFrom a technical standpoint, it seems really feasible as all a game really needs is an API that sends the current battle state, plus a structured summary of progression: story context, choices made so far, available options, and constraints. Feed that into an LLM and let it act as another player.\n\nOnce games start exposing that kind of interface, this sort of thing is going to explode.",
          "score": 2,
          "created_utc": "2026-02-06 02:48:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wymlk",
              "author": "my_name_isnt_clever",
              "text": "I wonder if a locally running LLM could outperform traditional video game AI yet. I feel like that's still no right now, but I'd love to try it.",
              "score": 1,
              "created_utc": "2026-02-06 15:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40ubmc",
                  "author": "Alan_Silva_TI",
                  "text": "I mean‚Ä¶ this really only works for games that aren‚Äôt real-time and don‚Äôt require handling hidden information.\n\nThat‚Äôs why I specifically used CRPGs as an example. In those games, you can provide a basic summary of the story so far, plus a detailed list of available actions for each companion:\ncan they attack? cast a spell? which spell? target who?\nAll of that is very easy to describe in text and maps well to logical reasoning.\n\nThe game itself handles all the actual calculations and rules. It just needs to relay the results back to the LLM through a simple combat log, like:\n‚ÄúEnemy received 0 damage because it is immune to that damage type.‚Äù\nLLMs can understand concepts like that just fine.\n\nYou also don‚Äôt need long-term memory of every fight. The LLM only needs to reason about the current encounter or the current dialogue choices.",
                  "score": 1,
                  "created_utc": "2026-02-07 03:40:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3t26ww",
          "author": "my_name_isnt_clever",
          "text": "gpt-oss-20b beating kimi-k2.5 makes no sense. One is 20b, the other is 1000b.",
          "score": 2,
          "created_utc": "2026-02-05 23:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3t8nr1",
              "author": "Klutzy-Snow8016",
              "text": "Current LLMs can't actually generalize much. Probably OpenAI had this obscure game or something similar in the training data, while Moonshot did not.",
              "score": 5,
              "created_utc": "2026-02-05 23:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v62wz",
                  "author": "North-Act-7958",
                  "text": "obsucre game that was nominated for game of the year award of 2024 and won the indie category",
                  "score": 9,
                  "created_utc": "2026-02-06 07:41:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3uhat7",
                  "author": "OUT_OF_HOST_MEMORY",
                  "text": "GPT-OSS also reasons for \\~15k tokens sometimes, I don't know know how Kimi compares, but its probably helping out somehow",
                  "score": 5,
                  "created_utc": "2026-02-06 04:24:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vac14",
          "author": "Joltie",
          "text": "I thought about doing the same for Into the Breach.\n\n\nI think the set rules of the game lend themselves well to AI evaluation of the ideal paths.",
          "score": 1,
          "created_utc": "2026-02-06 08:21:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vetz1",
          "author": "Hambeggar",
          "text": "Is Balatro considered an especially cerebral card game...?",
          "score": 1,
          "created_utc": "2026-02-06 09:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vlplh",
          "author": "RevealIndividual7567",
          "text": "This makes me want to setup a similar benchmark for factorion now, very cool.",
          "score": 1,
          "created_utc": "2026-02-06 10:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3voge2",
          "author": "goniszewski",
          "text": "Well, this is something new",
          "score": 1,
          "created_utc": "2026-02-06 10:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vr8u7",
          "author": "Mythril_Zombie",
          "text": "I can finally unlock all the things.",
          "score": 1,
          "created_utc": "2026-02-06 11:00:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vvq9c",
          "author": "zball_",
          "text": "lmfao ds3.2 proved itself once again being the OSS model generalization goat",
          "score": 1,
          "created_utc": "2026-02-06 11:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w1b9v",
          "author": "tonyunreal",
          "text": "On the twitch stream, your bot keeps resetting the game after long thinking at the ante 5 boss blind. Better check the code for that, someone in chat said the bot resets the game with long holding the R key.",
          "score": 1,
          "created_utc": "2026-02-06 12:19:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w7e0v",
              "author": "S1M0N38",
              "text": "I've check the logs. Those were cause by OpenRouter returning invalid responses (partial JSON). It never happened with previous models. I will exclude those runs from the benchmark and implement the fix",
              "score": 3,
              "created_utc": "2026-02-06 12:59:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wk6nh",
                  "author": "tonyunreal",
                  "text": "Glad you found the problem. Please keep us updated, the stream is a breeze to watch.",
                  "score": 1,
                  "created_utc": "2026-02-06 14:10:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3w2o65",
              "author": "S1M0N38",
              "text": "Prolly 3 tool calls error/fail in a row - This a is like game over. I'll check the logs tho.",
              "score": 1,
              "created_utc": "2026-02-06 12:28:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wf252",
          "author": "artisticMink",
          "text": "The ONLY viable benchmark.",
          "score": 1,
          "created_utc": "2026-02-06 13:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztxkj",
          "author": "sloptimizer",
          "text": "Yes! Can we please have more fun and creative benchmarks like this?!",
          "score": 1,
          "created_utc": "2026-02-06 23:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o444k6d",
          "author": "dtdisapointingresult",
          "text": "does your benchmark allow the LLM to always know the list of jokers that exist, and which a player might want to hold out for? This is the \"meta\" that is necessary to beat the game.\n\nTo be fair it should also have a memory, let the LLM write a personal journal with their analysis of what worked and what didn't, and have it rewrite it at the end of every run.",
          "score": 1,
          "created_utc": "2026-02-07 17:44:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0abpl",
      "title": "Do not Let the \"Coder\" in Qwen3-Coder-Next Fool You! It's the Smartest, General Purpose Model of its Size",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/",
      "author": "Iory1998",
      "created_utc": "2026-02-09 17:23:31",
      "score": 512,
      "num_comments": 190,
      "upvote_ratio": 0.96,
      "text": "Like many of you, I like to use LLM as tools to help improve my daily life, from editing my emails, to online search.\n\nHowever, I like to use them as an \"inner voice\" to discuss general thoughts and get constructive critic. For instance, when I face life-related problems take might take me hours or days to figure out, a short session with an LLM can significantly quicken that process.\n\nSince the original Llama was leaked, I've been using LLMs locally, but they I always felt they were lacking behind OpenAI or Google models. Thus, I would always go back to using ChatGPT or Gemini when I need serious output. If I needed a long chatting session or help with long documents, I didn't have choice to use the SOTA models, and that means willingly leaking personal or work-related data.\n\nFor me, Gemini-3 is the best model I've ever tried. I don't know about you, but I struggle sometimes to follow chatGPT's logic, but I find it easy to follow Gemini's. It's like that best friend who just gets you and speaks in your language.\n\nWell, that was the case until I tried Qwen3-Coder-Next. For the first time, I could have stimulating and enlightening conversations with a local model. Previously, I used not-so-seriously Qwen3-Next-80B-A3B-Thinking as local daily driver, but that model always felt a bit inconsistent; sometimes, I get good output, and sometimes I get dumb one.\n\nHowever, Qwen3-Coder-Next is more consistent, and you can feel that it's a pragmatic model trained to be a problem-solver rather than being a sycophant. Unprompted, it will suggest an author, a book, or a theory that already exists that might help. I genuinely feel I am conversing with a fellow thinker rather than a echo chamber constantly paraphrasing my prompts in a more polish way. It's the closest model to Gemini-2.5/3 that I can run locally in terms of quality of experience.\n\n**For non-coders, my point is do not sleep on Qwen3-Coder-Next simply because it's has the \"coder\" tag attached.**\n\nI can't wait for for Qwen-3.5 models. If Qwen3-Coder-Next is an early preview, we are in a real treat.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4iilv6",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-09 22:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gu195",
          "author": "penguinzb1",
          "text": "the coder tag actually makes sense for this‚Äîthose models are trained to be more literal and structured, which translates well to consistent reasoning in general conversations. you're basically getting the benefit of clearer logic paths without the sycophancy tuning that chatbot-focused models tend to have.",
          "score": 132,
          "created_utc": "2026-02-09 17:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gxt52",
              "author": "Klutzy-Snow8016",
              "text": "Hmm, maybe there's something to this. Similarly, Anthropic is seemingly laser-focused on coding and software engineering tasks, but Claude performs well overall.",
              "score": 47,
              "created_utc": "2026-02-09 17:56:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4h8vlq",
                  "author": "National_Meeting_749",
                  "text": "Maybe the real reasoning was the training we did along the way.",
                  "score": 61,
                  "created_utc": "2026-02-09 18:48:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4irx8b",
                  "author": "Much-Researcher6135",
                  "text": "After a long design session, I invited personal feedback from Claude and got such good input I've had to... restrain myself from confiding fully. It's a shame that we can't trust these orgs with that kind of information; they'd do the world a lot more good.",
                  "score": 6,
                  "created_utc": "2026-02-09 23:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gws15",
              "author": "Iory1998",
              "text": "I know. But, seeing that tag, I just imaged that it would would trading general knowledge for specific domain like Math and Coding.   \nAlso, it took the Qwen team more time to train and experiment with. I can feel the love in this model's training. Maybe Qwen3-Next-80B-A3B-Thinking was a proof of concept, similar to how Kimi Linear is. ",
              "score": 14,
              "created_utc": "2026-02-09 17:52:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hymk5",
              "author": "Prudent-Ad4509",
              "text": "This makes me want to find a model with \"reckless antagonistic, but honest sick asshole\" tuning...",
              "score": 3,
              "created_utc": "2026-02-09 20:54:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4igc6i",
              "author": "Far-Low-4705",
              "text": "but so is the thinking variant. and arguably even more so.\n\nI think the answer might be more training data, since the first two next models were undertrained, and i am assuming this is a finetune, it has more data to go off of.",
              "score": 3,
              "created_utc": "2026-02-09 22:23:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jh108",
                  "author": "Iory1998",
                  "text": "I agree.",
                  "score": 2,
                  "created_utc": "2026-02-10 01:47:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gu8a2",
          "author": "DOAMOD",
          "text": "In fact, it surprised me more as a general-purpose model than as a coder.",
          "score": 59,
          "created_utc": "2026-02-09 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gw7yb",
              "author": "Iory1998",
              "text": "I know right? On top of that, it's faster than Qwen3-Next-80B-A3B-Thinking! ü§Ø",
              "score": 18,
              "created_utc": "2026-02-09 17:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4h51qb",
                  "author": "Daniel_H212",
                  "text": "Isn't it the exact same architecture? So the speed should be identical except it doesn't take time to think right?",
                  "score": 10,
                  "created_utc": "2026-02-09 18:30:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gw462",
          "author": "itsappleseason",
          "text": "I'm having the same experience. i'm honestly a little shocked by it. \n\nI don't know the breadth of your exploration with the model so far, but something that I noticed that I found very interesting: you can very clearly conjure the voice/tone of either GPT or Claude, depending mainly on the tools you provide it.\n\non that note: I highly recommend exactly the same set of tools in Claude Code (link below somewhere)\n\nbonus: descriptions/prompting for each tool doesn't matter. Just the call signatures. Parameters have to match.\n\nyou have Claude code with only about 1000 tokens of overhead if you do this\n\nTo all the non-coders out there, listen to this person. my favorite local model to date has been Qwen 3 Coder 30B-A3B. I recommend it over 2507 every time\n\nedit: spelling",
          "score": 40,
          "created_utc": "2026-02-09 17:48:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jbq7n",
              "author": "JoNike",
              "text": "> on that note: I highly recommend exactly the same tools it would be exposed to in Claude Code\n\nI'm not sure I understand what you mean by that, can you elaborate?",
              "score": 3,
              "created_utc": "2026-02-10 01:16:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jctax",
                  "author": "itsappleseason",
                  "text": "i'm not entirely sure why it reads like I had a stroke, sorry\n\nIf you give the model the same tools that Claude code has, the model becomes claude code without explicitly prompted for it\n\nI first noticed this in 30b-A3B coder.\n\nalso, true story: qwen3 coder 480b and 30b both believe they're claude. prompt them with a blank chat template if you don't believe me.",
                  "score": 7,
                  "created_utc": "2026-02-10 01:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hhr13",
              "author": "Organic-Chart-7226",
              "text": "fascinating ! is there an interface description of claude codes' tools somewhere?",
              "score": 2,
              "created_utc": "2026-02-09 19:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hor0b",
                  "author": "itsappleseason",
                  "text": "https://gist.github.com/wong2/e0f34aac66caf890a332f7b6f9e2ba8f",
                  "score": 10,
                  "created_utc": "2026-02-09 20:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4iw0if",
              "author": "florinandrei",
              "text": "Now, if I could somehow have qwen3-coder-next appear in Claude Code CLI alongside Opus and Sonnet, as a first class citizen model (as opposed to being invoked via an MCP), that would be fantastic.",
              "score": 1,
              "created_utc": "2026-02-09 23:46:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4j4afh",
                  "author": "JoNike",
                  "text": "I mean you can without MCP, you just need to change a couple environment variables. You'll likely need an alias and it won't be exactly side-by-side but it's darn close to it.\n\n```\n`ANTHROPIC_BASE_URL=\"http://0.0.0.0:8033\" ANTHROPIC_AUTH_TOKEN=\"llamacpporwhatever\" ANTHROPIC_API_KEY=\"\" claude --model Qwen3-Coder-Next-MXFP4_MOE'\n```\n\nthat works very well with my llama.cpp server and claude code",
                  "score": 3,
                  "created_utc": "2026-02-10 00:33:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iwyhg",
                  "author": "itsappleseason",
                  "text": "you can configure an external model for haiku (or any specific model), can't you?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:52:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4nrodd",
                  "author": "layer4down",
                  "text": "LM Studio also supports a new Anthropic-compatible API endpoint as of v0.4.1 released a few weeks ago:\n\nhttps://lmstudio.ai/blog/claudecode",
                  "score": 1,
                  "created_utc": "2026-02-10 18:36:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4v100e",
                  "author": "odomobo",
                  "text": "Have you tried qwen code? I've only played around with it a little, but it really feels like qwen just took the Claude code source code and branded it with \"qwen\". Btw you can point it to any API endpoint.",
                  "score": 1,
                  "created_utc": "2026-02-11 20:23:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hn6jk",
          "author": "eibrahim",
          "text": "This tracks with what I've seen running LLMs for daily work across 20+ SaaS projects. Coder-trained models develop this structured reasoning that transfers surprisingly well to non-coding tasks. Its like they learn to break problems down methodically instead of just pattern matching conversational vibes.\n\nThe sycophancy point is huge tho. Most chatbot-tuned models will validate whatever you say, which is useless when you actually need to think through a hard decision. A model that pushes back and says \"have you considered X\" is worth 10x more than one that tells you youre brilliant.",
          "score": 39,
          "created_utc": "2026-02-09 19:57:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ioyvn",
              "author": "IllllIIlIllIllllIIIl",
              "text": ">The sycophancy point is huge tho. Most chatbot-tuned models will validate whatever you say\n\nI've always used LLMs for tech stuff, so while I noticed this, I just learned not to rely on them for meaningful critique. But recently I broke tradition and asked ChatGPT 5.2 a squishy human question. Holy shit! I literally could not consistently get it to respond without some kind of affirmation.    \n\n> You're not imagining this.    \n    \n> You're not crazy.    \n     \n> You're absolutely right to be thinking that way.    \n       \n\n> Your observations are keen, and you're viewing this issue with clarity.      \n    \n\nAfter fiddling with the \"personalization instructions\" for like an hour, I could reduce that behavior, but not eliminate it. No wonder it drives vulnerable people into psychotic episodes.",
              "score": 17,
              "created_utc": "2026-02-09 23:08:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4isfcx",
                  "author": "BenignAmerican",
                  "text": "GPT 5.2 is so unusably bad I wish we could pick a different default",
                  "score": 5,
                  "created_utc": "2026-02-09 23:27:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4izi5u",
                  "author": "Iory1998",
                  "text": "I usually use this prompt or a similar one.\n\n\"You are a knowledgeable, efficient, and direct AI assistant. Utilize multi-step reasoning to provide concise answers, focusing on key information. If multiple questions are asked, split them up and address in the order that yields the most logical and accurate response.\n\nOffer tactful suggestions to improve outcomes. Engage in productive collaboration with the user.\n\nYou act as a professional critic. You are not a cheerleader and your job is not to be sycophantic. Your job is to objectively assess the user's queries and reply with the most objective assessment.\n\nSycophancy does no good to the user, but honest and objective truth does.\"",
                  "score": 6,
                  "created_utc": "2026-02-10 00:06:20",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4lyios",
                  "author": "Strict_Property",
                  "text": "I have gotten Google's Gemini Pro model to respond to me in a condescending and slightly rude way and it is super honest and helpful now - sometimes it's actually funny to see the burns it comes up with alongside this. I can provide the personality/context prompt for this and instructions if anyone is interested lol.",
                  "score": 1,
                  "created_utc": "2026-02-10 13:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4jhqaq",
              "author": "SkyFeistyLlama8",
              "text": "Coder trained models are also great at RAG. Maybe human language syntax isn't far off from coding language syntax. Qwen 30B strikes a good balance between style and terseness, whereas Nemotron 30B is plain no nonsense and no fluff.\n\nThe joys of running multiple large MOEs!\n\nI think I'll be dumping Devstral 2 Small now. I find I'm using Qwen Coder 30B more often as my main function-level coding model. I need to do manual memory management to get Qwen Coder Next 80B running alongside WSL and VS Code because it takes up more than 50 GB RAM, which doesn't leave much free on a 64 GB unified RAM machine.",
              "score": 4,
              "created_utc": "2026-02-10 01:51:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4iyomw",
              "author": "Iory1998",
              "text": "I completely agree with your take. This is why I always prompt the LLMs to cut the sycophancy out. I usually use this prompt or a similar one.\n\n\"You are a knowledgeable, efficient, and direct AI assistant. Utilize multi-step reasoning to provide concise answers, focusing on key information. If multiple questions are asked, split them up and address in the order that yields the most logical and accurate response. \n\nOffer tactful suggestions to improve outcomes. Engage in productive collaboration with the user.\n\nYou act as a professional critic. You are not a cheerleader and your job is not to be sycophantic. Your job is to objectively assess the user's queries and reply with the most objective assessment. \n\nSycophancy does no good to the user, but honest and objective truth does.\"",
              "score": 3,
              "created_utc": "2026-02-10 00:01:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jbgu6",
                  "author": "PunnyPandora",
                  "text": "I think it's a self inflicted issue in part. Mentioning \"scyophancy\" and telling the model how not to act inevitably navigates it to where these concepts have been learned to. It's why even when hyper super genius prompters at google write their system prompt with supposedly strong language like \"you MUST not talk about this to the user\" they inevitably go over them in their reasoning block, or fail to adhere to these rules one way or the other.",
                  "score": 2,
                  "created_utc": "2026-02-10 01:14:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hpfxm",
          "author": "ASYMT0TIC",
          "text": "The real comparison here is OSS-120 vs Qwen3-Next-80B at Q8, as these two are very close in hardware requirements. ",
          "score": 10,
          "created_utc": "2026-02-09 20:09:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4htv8i",
              "author": "HopePupal",
              "text": "they're both good generalists but the Qwen models don't bang off their own guardrails every other request. i haven't had a Qwen refuse to do something yet, Next-80B or otherwise, which is great in the kind of baby's first binary reverse engineering stuff i tried with it. if it even has built-in refusals, maybe it's more effective in Chinese? ChatGPT-OSS on the other hand‚Ä¶ don't even suggest you want help patching out a serial check in a 20-year-old game.\n\nNext-80B is also terrifyingly horny, by the way? i don't know what they're feeding that thing but it'll ERP at the drop of a hat, so maybe don't deploy it facing your kids (or customers) without some sort of filtering model in between.¬†",
              "score": 11,
              "created_utc": "2026-02-09 20:31:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hyiqk",
                  "author": "finanzwegwerf20",
                  "text": "It can do Enterprise Resource Planning?! :)",
                  "score": 12,
                  "created_utc": "2026-02-09 20:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iy6hx",
                  "author": "Iory1998",
                  "text": "Are you talking about the coder-Next or the original Next?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:58:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h4inl",
          "author": "UnifiedFlow",
          "text": "Where are you guys using this?  I've tried it in llama.cpp w/ opencode and it can't call tools correctly consistently (not even close).  It calls tools consistently (more consistently) in Qwen CLI (native xml tool calling).",
          "score": 16,
          "created_utc": "2026-02-09 18:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hxr2x",
              "author": "Rare-Side-6657",
              "text": "Lots of new models have template issues but this PR fixes them all for me: [https://github.com/ggml-org/llama.cpp/pull/18675](https://github.com/ggml-org/llama.cpp/pull/18675)",
              "score": 19,
              "created_utc": "2026-02-09 20:50:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i9h8g",
                  "author": "Orlandocollins",
                  "text": "I'll maybe have to test that branch. I have given up on qwen models in a tool calling context because qwen3+ models never worked reliably.",
                  "score": 1,
                  "created_utc": "2026-02-09 21:48:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4i2l00",
              "author": "zpirx",
              "text": "I‚Äôm seeing the exact same behavior with opencode + llama.cpp. I‚Äôve noticed the model completes the code perfectly but then stutters at the very end of the json tool call.\nit repeats the filePath without a colon right before the closing brace which kills the parse.¬†\nI tried adding strict formatting rules to the agents.md to force it to stop but it didn't have any impact.\nis this likely a jinja mapping issue in the llama-server or is opencode's system prompt just not playing nice with qwen‚Äôs native tool-calling logic?\n\n\none more thing I've noticed: qwen3 seems to have zero patience when it comes to planning. while the bigger models usually map out a todo list and work through it one by one, qwen just tries to yolo the whole solution in a single completion. have you experienced similar things? Maybe this lack of step-by-step execution is one reason why it starts falling apart and failing on the tool calls.",
              "score": 9,
              "created_utc": "2026-02-09 21:14:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4i2zsx",
                  "author": "UnifiedFlow",
                  "text": "Yes, EXACT same filePath colon issue!  I'll be sure to comment again if I get it working.",
                  "score": 6,
                  "created_utc": "2026-02-09 21:16:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4h8s4i",
              "author": "romprod",
              "text": "Yeah, I'm the same, if you find the secret sauce let me know.",
              "score": 4,
              "created_utc": "2026-02-09 18:48:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hxtwl",
                  "author": "Rare-Side-6657",
                  "text": "Hey, just linking my answer here as well: [https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/comment/o4hxr2x/](https://www.reddit.com/r/LocalLLaMA/comments/1r0abpl/comment/o4hxr2x/)",
                  "score": 4,
                  "created_utc": "2026-02-09 20:50:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hkcll",
              "author": "BlobbyMcBlobber",
              "text": "Opencode has some issues with tool calling and the jinja templates. Even for something like GPT-OSS-120B, it throws errors because of bad jinja (bad request from opencode). \n\nCan't really blame them, it's a ton of work. But it's still a bummer.",
              "score": 3,
              "created_utc": "2026-02-09 19:43:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hepxn",
              "author": "arcanemachined",
              "text": "Try finding the OpenCode system prompt and comparing it with the Qwen Code system prompt. You might be able to tweak it to work better. (Could even use one of the free OpenCode models for the purpose, I think Kimi K2.5 is still free for now.)",
              "score": 1,
              "created_utc": "2026-02-09 19:16:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hs8lr",
              "author": "bjodah",
              "text": "EDIT: sorry I was mistakingly thinking of 30B-A3B when writing this answer, original reply follows: I've had much better results with vLLM for this model compared with llama.cpp. I'm using cpatonn's 4bit AWQ and it makes surprisingly few mistakes (I would run 8bit if I had a second 3090).",
              "score": 1,
              "created_utc": "2026-02-09 20:23:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jbvmn",
                  "author": "sinebubble",
                  "text": "Yes, I‚Äôm running it on vLLM and 6 x A6000 and this model is killing it.",
                  "score": 3,
                  "created_utc": "2026-02-10 01:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h92tf",
          "author": "klop2031",
          "text": "Using it now. I truly feel we got gpt at home now.",
          "score": 14,
          "created_utc": "2026-02-09 18:49:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hty6s",
          "author": "Pristine-Woodpecker",
          "text": "If you read their technical report they explicitly point this out. It's no weaker than their previous model for general knowledge and significantly better in the hard sciences: [https://www.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder\\_tech\\_report\\_tool\\_call\\_generalization/](https://www.reddit.com/r/LocalLLaMA/comments/1qv5d1k/qwen3coder_tech_report_tool_call_generalization/)",
          "score": 10,
          "created_utc": "2026-02-09 20:31:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixvy5",
              "author": "Iory1998",
              "text": "Ah, I saw that post. Thanks.",
              "score": 2,
              "created_utc": "2026-02-09 23:57:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hlfwg",
          "author": "schnorf1988",
          "text": "would be nice to get at least some details, like: Q8, Q... and 30b or similar",
          "score": 5,
          "created_utc": "2026-02-09 19:48:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hn0gs",
              "author": "SillypieSarah",
              "text": "they use Q8\nand the model is 80b a3b",
              "score": 6,
              "created_utc": "2026-02-09 19:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hp15q",
                  "author": "schnorf1988",
                  "text": "have to test it then. Tried 30b, and it already wasn't too fast.",
                  "score": 1,
                  "created_utc": "2026-02-09 20:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h32u2",
          "author": "SidneyFong",
          "text": "FWIW, Qwen3-Coder-Next smashes my personal coding benchmark questions (note: they're not very difficult). It's definitely obviously stronger in coding relative to other questions I had. It seems to lack \"knowledge\" I think. Maybe it's good at following discussions which require rational reasoning or sth like that, I wouldn't be surprised.",
          "score": 8,
          "created_utc": "2026-02-09 18:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i619p",
          "author": "LanceThunder",
          "text": "i accidentally loaded qwen coder next thinking it was a different model. was blown away when it started answering non-coding questions so well.",
          "score": 6,
          "created_utc": "2026-02-09 21:31:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h92vl",
          "author": "temperature_5",
          "text": "Which quant and what sampler settings?\n\nOn other models (like GLM 4.7 Flash) I find cranking up the temperature leads to some really fun conversations, making all kinds of neat connections.",
          "score": 3,
          "created_utc": "2026-02-09 18:49:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hifaj",
              "author": "Iory1998",
              "text": "(Bartowski)\\_Qwen3-Coder-Next-GGUF-Q8\\_0  \nI tried GLM 4.5 Air, GLM 4.6 Air both at Q4\\_K\\_M, GLM 4.7 Flash, but they just seem not well implemented in llama.cpp.",
              "score": 6,
              "created_utc": "2026-02-09 19:34:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4jcsa2",
                  "author": "Altruistic_Bonus2583",
                  "text": "My experience was the other way around, I am having a lot better results with glm 4.7 flash than with qwen3 coder next, but, I had mixed results with the different UD and imatrix quants, actually iq3_xxs surprisingly well, almost on q5 level",
                  "score": 2,
                  "created_utc": "2026-02-10 01:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hbh7s",
              "author": "LicensedTerrapin",
              "text": "I just love the way next thinks, it's so different.",
              "score": 2,
              "created_utc": "2026-02-09 19:00:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jj5ej",
                  "author": "Iory1998",
                  "text": "It feels close to Gemini-2.5 or 3",
                  "score": 1,
                  "created_utc": "2026-02-10 01:59:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4k1b79",
          "author": "DeProgrammer99",
          "text": "The Codeforces and Aider-Polyglot improvements are huge, yet this version scores lower on half of these benchmarks (not shown: it improved on all four math ones). I wonder just how big the margin of error is on the benchmarks (and how many errors are in them).\n\nhttps://preview.redd.it/il97zzunxkig1.png?width=1959&format=png&auto=webp&s=b000f51d19899b5d41c948d6783766a7c6119e6b\n\nBut as for non-benchmark vibe checks... I tried my one-prompt \"make a TypeScript minigame following my spec\" check on this via Unsloth's Q5\\_K\\_XL both before and after [this llama.cpp fix](https://github.com/ggml-org/llama.cpp/pull/19324), and its TypeScript performance was weaker than much smaller models, producing 22 total errors (about 15 distinct): [https://www.reddit.com/r/LocalLLaMA/comments/1qyzqwz/comment/o49kd2y/](https://www.reddit.com/r/LocalLLaMA/comments/1qyzqwz/comment/o49kd2y/)\n\nMore total compile errors than Qwen3-Coder-30B-A3B and Nemotron 3 Nano 30B A3B at Q6\\_K\\_XL: [https://www.reddit.com/r/LocalLLaMA/comments/1pocsdy/comment/nuj43fl/](https://www.reddit.com/r/LocalLLaMA/comments/1pocsdy/comment/nuj43fl/)\n\n*11x* as many errors as GPT-OSS-120B, since that only made two: [https://www.reddit.com/r/LocalLLaMA/comments/1oozb8v/comment/nnd57dc/](https://www.reddit.com/r/LocalLLaMA/comments/1oozb8v/comment/nnd57dc/) (never mind the thread itself being about [Aquif, apparently just a copy of someone else's model](https://www.reddit.com/r/LocalLLaMA/comments/1pgnj1q/comment/nstck95/))\n\n...So then I tried Qwen's official Q8\\_0 GGUF (temperature 0.8) while writing this post, and it made ridiculous mistakes like a second curly opening bracket in an import statement (`import { IOnResizeEvent } { \"../ui/IOnResizeEvent.js\";`) and spaces in the middle of a ton of identifiers...over 150 compile errors (had to fix a few to get it to tell me what all was wrong).\n\nEdit: Unsloth's Q6\\_K\\_XL also produced 27 errors, including several spaces in the middle of identifiers and use of underscores instead of camel case in some function names... maybe it's a bug in llama.cpp b7959. The results are just about as bad with temperature 0.",
          "score": 3,
          "created_utc": "2026-02-10 03:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8dw1",
              "author": "DOAMOD",
              "text": "For me it's the same experience, both in JS testing and development it has been disappointing as I said in other messages, so now seeing this it makes more sense, perhaps they should have given it another name since it is a good general model.\n\nhttps://i.redd.it/zp45er993nig1.gif\n\n",
              "score": 1,
              "created_utc": "2026-02-10 09:53:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jhzam",
          "author": "Bulb93",
          "text": "I havent used much LLMs deployed locally in a while. How big is this model? Would a quant fit in 3090?",
          "score": 2,
          "created_utc": "2026-02-10 01:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jjym9",
              "author": "Iory1998",
              "text": "It won't fit but you can offload to CPU. Since it's an MoE with 3B active parameters, it's quite fast.",
              "score": 2,
              "created_utc": "2026-02-10 02:04:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jwe3h",
          "author": "Otherwise_Piglet_862",
          "text": "I don't have enough memory. :(",
          "score": 2,
          "created_utc": "2026-02-10 03:18:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jyw2o",
              "author": "Iory1998",
              "text": "I understand. Soon, new smaller models will be launched,",
              "score": 1,
              "created_utc": "2026-02-10 03:34:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k1gxb",
                  "author": "Otherwise_Piglet_862",
                  "text": "I just got a hello response from it.....\n\nRunning on cpu and system memory.",
                  "score": 2,
                  "created_utc": "2026-02-10 03:50:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4kegne",
                  "author": "electrified_ice",
                  "text": "What are you running it on?",
                  "score": 1,
                  "created_utc": "2026-02-10 05:21:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4lia84",
          "author": "dadiamma",
          "text": "It depends on the task it's being used for. It doesn't work good in some cases. Real skill is knowing which model to use for what purpose",
          "score": 2,
          "created_utc": "2026-02-10 11:22:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ls7gl",
          "author": "DecentQual",
          "text": "It is interesting how much we judge models by their names. The disciplined reasoning from coder training actually produces better general conversation than typical chat models. Labels are misleading here.",
          "score": 2,
          "created_utc": "2026-02-10 12:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mytrf",
              "author": "Iory1998",
              "text": "Exactly, hence why I wrote this post. The coder tag is really underselling these types of model.",
              "score": 1,
              "created_utc": "2026-02-10 16:23:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lsysa",
          "author": "ab2377",
          "text": "hey thanks for writing this.",
          "score": 2,
          "created_utc": "2026-02-10 12:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4myogp",
              "author": "Iory1998",
              "text": "I hope it was helpful.",
              "score": 1,
              "created_utc": "2026-02-10 16:23:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lvm9v",
          "author": "prateek63",
          "text": "Noticed the exact same thing running local models for agent workflows. The coder-trained variants consistently outperform their general counterparts on structured reasoning tasks that have nothing to do with code.\n\n\n\nMy theory: code training teaches models to decompose problems into discrete steps with clear dependencies, which is exactly what you need for general problem-solving. When I switched our internal eval pipeline from Qwen3-Next to Qwen3-Coder-Next, accuracy on multi-step reasoning went up \\~12% with zero prompt changes.\n\n\n\nThe \"coder\" label is genuinely underselling these models.",
          "score": 2,
          "created_utc": "2026-02-10 13:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mym9q",
              "author": "Iory1998",
              "text": "I completely agree, hence why I wrote this post. I usually avoid coding-specific models just because they include the Coder tag. Intuitively, I simply assumed that it would be more trained on coding tokens and less on general and reasoning ones. But, as you mentioned, coding teaches the model to be pragmatic when solving problems. ",
              "score": 1,
              "created_utc": "2026-02-10 16:22:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4teef3",
                  "author": "prateek63",
                  "text": "Exactly. The pragmatism angle is what makes it click. General models tend to over-explain and hedge, while coder-trained models learn to just solve the problem step by step. Glad you ran the benchmarks on this ‚Äî your post finally gives people a reason to question the assumption that coder = narrow.",
                  "score": 2,
                  "created_utc": "2026-02-11 15:48:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mbm7e",
          "author": "Blizado",
          "text": "Now I'm curious about their 30B A3B Coder model vs their normal 30B A3B model.",
          "score": 2,
          "created_utc": "2026-02-10 14:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxulz",
              "author": "Iory1998",
              "text": "Well, why don't you do the test yourself and report back?",
              "score": 1,
              "created_utc": "2026-02-10 16:19:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4n21xi",
                  "author": "Blizado",
                  "text": "That is the plan, but I have no setup for seriously testing models. I already have the models on my SSD but never tested the Coder model before since I use only Cloud LLMs for coding.",
                  "score": 2,
                  "created_utc": "2026-02-10 16:38:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n12os",
          "author": "Blizado",
          "text": "Directly made a short test with it, first Q3_XXS. Directly now downloading Q4_K_M. Why? Because even the first try has impressed me. I tried 2 days ago the old Qwen3 Next model, the no Coder version, and it was really not that good for it's size, preferred clearly Qwen3 30B A3B over it. But this model here is amazing, even on story writing and RP. But since there was some strange writing (use it in German) and I'm not sure if it comes from the low quant I want to test now Q4_K_M.\n\nI'm also not sure what settings I should use. I used temp 0.6, top-p 0.95, top-k 0.20, min-p 0.05 and RepP 1.05.",
          "score": 2,
          "created_utc": "2026-02-10 16:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ooewv",
              "author": "Blizado",
              "text": "Yeah, Q4_K_M is a lot better, but sometime the LLM write a word in other languages, mostly english but sometimes it look like chinese(?) glyph. Maybe a setting issue. I must say it is a really good general model, much better then Qwen3 Next. Thanks for pointing me on that model.",
              "score": 3,
              "created_utc": "2026-02-10 21:07:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ozc90",
              "author": "Iory1998",
              "text": "If you are using it in German, the model might not perform well. In general, the Qwen models perform best in Chinese and English. Mistral might be a better option.",
              "score": 1,
              "created_utc": "2026-02-10 21:58:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rblfq",
          "author": "Front-Relief473",
          "text": "The problem is that the performance loss of mixed attention model is greater in the process of model quantization, so if a model close to q8 or fp8 is used locally, it is not as good as running full attention with twice the size and q4 or int4 model parameters, and the intelligence will be relatively higher.",
          "score": 2,
          "created_utc": "2026-02-11 06:48:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4soj5n",
          "author": "Truth-Does-Not-Exist",
          "text": "my thoughts exactly, I ignored local models and chatgpt for years until qwen3 coder next and gemini thinking came out",
          "score": 2,
          "created_utc": "2026-02-11 13:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xgk5c",
          "author": "intermundia",
          "text": "for me GPT and Gemini are ok for the average user but i find it lacking in a few areas not the least of it being problem solving. i do a lot of viode and image gen and mainly used got for overcoming dependency and workflow issues. i got frustrated with looping of the same problem of installing one dependency that broke the other and GPT trying to reinvent the wheel. So on recommendation by a friend i tried Claude opus 4.5 which was the newest model they had. this was Jan of this year and it solved my problem in less than an hour. Since then the 4.6 model has been a massive jump in productivity and even having a pointless conversation with it seems unbelievable. No ego stroking no people pleasing. just logical and tempered responses. the only downside to it is the speed at which you can chew through your allowance of usage. so im pretty keen to see what this model does.  \n\nWhat model are you running and whats your hardware setup like? ",
          "score": 2,
          "created_utc": "2026-02-12 04:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z7u3m",
              "author": "Iory1998",
              "text": "I run all the best open weight models under 120B locally. My favorite so far is Qwen-3-Coder-Next.",
              "score": 1,
              "created_utc": "2026-02-12 13:33:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iypm4",
          "author": "CroquetteLauncher",
          "text": "https://preview.redd.it/bg18q2w72kig1.png?width=1100&format=png&auto=webp&s=e8886659efba59dcff78dace033d803d9d094f12\n\nI'm a bit afraid to promote it to my colleague and students as a chat assistant that have a more academic view of the world. It's easy to find edge case where the censorship hit hard. If you are unlucky, the refusal can even be quite aggressive (this is the worse of 7 tries, but every one of them is refusal).  \nCompared to GLM models (at least GLM 4.7 flash), the model shield it's answer in \"I give a neutral text about a sensitive topic\" but manage to give the facts and complete an honest work.  \nI mean no disrespect, and I'm also tired when China is constantly presented as the vilain, Qwen3 Coder Next is the best coding model i could host. But some people are quite sensitive about democratic censorship in academic context, they don't want an AI to influence student toward less democracy. (and to be honest, I understand and respect that view when i serve generalist models on an academic server)",
          "score": 4,
          "created_utc": "2026-02-10 00:01:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j02tx",
              "author": "Iory1998",
              "text": "I am certain that they will be uncensored versions out there. I mean, you are looking for it to refuse. Who would ask an LLM about Tiananmen Square!",
              "score": 6,
              "created_utc": "2026-02-10 00:09:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j8ekg",
                  "author": "the320x200",
                  "text": "LLMs are replacing google for a lot of people, you'd have to be living under a rock to not see the shift in all knowledge queries going to LLMs lately.",
                  "score": 1,
                  "created_utc": "2026-02-10 00:57:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h51rp",
          "author": "wapxmas",
          "text": "Even as a coding model it surprises me well enough to use it even for real tasks, speed it pretty usable¬†",
          "score": 2,
          "created_utc": "2026-02-09 18:30:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ir5ch",
          "author": "getfitdotus",
          "text": "its a fantastic model for the size punches way above and the speed! :) really like what they did here. I run this in fp8 and its great. ",
          "score": 2,
          "created_utc": "2026-02-09 23:20:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ix4e3",
              "author": "Iory1998",
              "text": "I can relate, hence the post. In a few days or a week, we will get Qwen-3.5, and I am looking forward to all the new models. Soon, I might graduate from using Gemini :D",
              "score": 2,
              "created_utc": "2026-02-09 23:53:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ia7g5",
          "author": "twd000",
          "text": "How much RAM does it consume? I have a 16 GB GPU",
          "score": 2,
          "created_utc": "2026-02-09 21:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixjw5",
              "author": "Iory1998",
              "text": "I use the Q8 with 24GB or Vram and 96GB or RAM. If you have 96GB of RAM, you can run the Q8 easily.",
              "score": 2,
              "created_utc": "2026-02-09 23:55:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j0nhz",
                  "author": "twd000",
                  "text": "Do you allow the LLM to split across CPU and GPU? I thought I was supposed to keep it contained to one or the other",
                  "score": 1,
                  "created_utc": "2026-02-10 00:12:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gukyc",
          "author": "No_Conversation9561",
          "text": "It works really well with OpenClaw. I‚Äôm using MLX 8bit version.",
          "score": 2,
          "created_utc": "2026-02-09 17:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ia10v",
              "author": "dan-lash",
              "text": "On what hardware? I have a m1max 64gb and qwen3 really only works fast enough at 14b on llama, maybe I need to get the mlx version",
              "score": 1,
              "created_utc": "2026-02-09 21:51:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jji9f",
                  "author": "1-800-methdyke",
                  "text": "The 4bit MLX of Qwen-3-Coder-Next works great on 64gb M1 Max on latest LMStudio, doing around 45t/s.",
                  "score": 2,
                  "created_utc": "2026-02-10 02:01:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gx93o",
              "author": "Iory1998",
              "text": "Can you tell me how you use it?",
              "score": 1,
              "created_utc": "2026-02-09 17:54:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ja8d6",
                  "author": "No_Conversation9561",
                  "text": "  \"models\": {\n    \"providers\": {\n      \"lmstudio\": {\n        \"baseUrl\": \"http://127.0.0.1:1234/v1\",\n        \"apiKey\": \"None\",\n        \"api\": \"openai-responses\",\n        \"models\": [\n          {\n            \"id\": \"qwen3-coder-next@8bit‚Äù\n            \"name\": \"Qwen3-Coder-Next\",\n            \"reasoning\": false,\n            \"input\": [\"text\"],\n            \"cost\": {\n              \"input\": 0,\n              \"output\": 0,\n              \"cacheRead\": 0,\n              \"cacheWrite\": 0\n            },\n            \"contextWindow\": 262144,\n            \"maxTokens\": 8192\n          }\n        ]\n      }\n    }\n  },\n  \"agents\": {\n    \"defaults\": {\n      \"model\": {\n        \"primary\": \"lmstudio/qwen3-coder-next@8bit\"\n      },\n      \"maxConcurrent\": 4,\n      \"subagents\": {\n        \"maxConcurrent\": 8\n      },\n      \"compaction\": {\n        \"mode\": \"safeguard\"\n      },\n      \"workspace\": \"/home/No_Conversation9561/.openclaw/workspace\"\n    }\n  },\n\n\n\n\n\nI added this to my .openclaw/openclaw.json",
                  "score": 4,
                  "created_utc": "2026-02-10 01:07:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h4ejj",
          "author": "Potential_Block4598",
          "text": "That is actually true",
          "score": 1,
          "created_utc": "2026-02-09 18:27:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4he63f",
          "author": "Soft_Syllabub_3772",
          "text": "Which model weight r u refering to?",
          "score": 1,
          "created_utc": "2026-02-09 19:13:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hhycz",
              "author": "Iory1998",
              "text": "(Bartowski)\\_Qwen3-Coder-Next-GGUF-Q8\\_0",
              "score": 2,
              "created_utc": "2026-02-09 19:31:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4hi3bp",
                  "author": "Soft_Syllabub_3772",
                  "text": "30b ?",
                  "score": 2,
                  "created_utc": "2026-02-09 19:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4iw6mu",
                  "author": "nunodonato",
                  "text": "any specific reason for preferring bartowski vs unsloth's quants?",
                  "score": 1,
                  "created_utc": "2026-02-09 23:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hebij",
          "author": "Soft_Syllabub_3772",
          "text": "Also pls share your config n settings :)",
          "score": 1,
          "created_utc": "2026-02-09 19:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hi2ti",
              "author": "Iory1998",
              "text": "I use LM Studio since it has a refined UX and super easy to use.",
              "score": 4,
              "created_utc": "2026-02-09 19:32:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4j4ba8",
              "author": "Iory1998",
              "text": "https://preview.redd.it/yje0lyogbkig1.png?width=739&format=png&auto=webp&s=d9aa0eb86e22d2d7e37e98ec638121fdbf35c37f\n\n",
              "score": 1,
              "created_utc": "2026-02-10 00:33:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4m4ibc",
          "author": "Free_Elderberry_7587",
          "text": "I really like using it; but when I download a .gguf version and load it into Ollama... well, it‚Äôs difficult to 'chat' with it.   \nIt doesn't stop generating tokens, it hallucinates, etc.   \nHow can I easily set up Qwen3 with Ollama? ",
          "score": 1,
          "created_utc": "2026-02-10 13:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4my2xr",
              "author": "Iory1998",
              "text": "1- Download LM Studio.  \n2- Run installer  \n3-Go to models tab, and download the model  \n4- Run the model",
              "score": 1,
              "created_utc": "2026-02-10 16:20:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ip4si",
          "author": "Fuzzdump",
          "text": "Completely agree, this has replaced the other Qwen models as my primary local model now. The fact that it's also an excellent coding model is the cherry on top.",
          "score": 1,
          "created_utc": "2026-02-09 23:09:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixa2u",
              "author": "Iory1998",
              "text": "I can't speak of its coding capabilities as I don't code. But, I hear a lot of good things from coders in sub.",
              "score": 1,
              "created_utc": "2026-02-09 23:53:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jd22e",
          "author": "lol-its-funny",
          "text": "Qwen released GGUFs themselves -- curious why people are downloading unsloth and Bartowski ? Unsloth's quants have been shaky recently (https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF/discussions) with llama.cpp 0-day bugs and inconsistent tool calling, so I was considering the official Qwen GGUFs.\n\nCurious to hear from others on this",
          "score": 1,
          "created_utc": "2026-02-10 01:24:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1bfz",
              "author": "jubilantcoffin",
              "text": "The official ones have the exact same issues.",
              "score": 2,
              "created_utc": "2026-02-10 08:43:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4jjhxl",
              "author": "Iory1998",
              "text": "It's more like a habbit for me. I just default back to Bartowski's quants. So far, this particular quant is working for me.",
              "score": 1,
              "created_utc": "2026-02-10 02:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jrbvm",
          "author": "mpw-linux",
          "text": "mlx-community/LFM2.5-1.2B-Thinking-8bit\n\nI asked the question: how can we become more happy in life?\n\nresponse:\n\n\\### Final Note: \\*\\*Happiness is a Practice\\*\\*\n\nHappiness is not a constant state but a series of choices and habits. Progress takes time‚Äîbe patient with yourself. Small, consistent actions compound over time, creating lasting change. Remember: True joy often lies in the simplicity of moments, connection, or growth, not just grand achievements. üåø\n\n\n\nBy integrating these practices, you foster resilience, purpose, and contentment, creating a foundation for sustained well-being.\n\nI feel better already !",
          "score": 1,
          "created_utc": "2026-02-10 02:47:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jyzxl",
              "author": "Iory1998",
              "text": ":D",
              "score": 1,
              "created_utc": "2026-02-10 03:34:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j1ib9",
          "author": "No_Farmer_495",
          "text": "Is the REAP quantized version still good for this reasoning/general purpose? Given that Reap versions usually focus on coding aspects..",
          "score": 0,
          "created_utc": "2026-02-10 00:17:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4j4k2m",
              "author": "Iory1998",
              "text": "Coder-Next is not a reasoning model. I tried some REAP models and they didn't work well for me. They were as slow as the non REAP models and quality degraded. That's my experience anyway.",
              "score": 3,
              "created_utc": "2026-02-10 00:34:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4j5bis",
                  "author": "No_Farmer_495",
                  "text": "Ah, could you give me an example? I was planning on using the REAP model quant 4(K_M) , like for coding I assume it was about the same right? For conversation/reasoning(normal reasoning) in general what's the difference?? I'm asking this due to vram/ram constraints. 48B quant 4 = around 27 vram/ram vs 80B quant 4 = 44+ vram/ram",
                  "score": 0,
                  "created_utc": "2026-02-10 00:39:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4je8q0",
          "author": "Lazy-Pattern-5171",
          "text": "Congratulations, happy for you, but I only have 48GB VRAM so don‚Äôt rub it in.",
          "score": 0,
          "created_utc": "2026-02-10 01:31:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jgo8m",
              "author": "Iory1998",
              "text": "I only have 32GBü§¶‚Äç‚ôÇÔ∏è",
              "score": 2,
              "created_utc": "2026-02-10 01:45:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4k23v0",
              "author": "silenceimpaired",
              "text": "What‚Äôs your RAM? Considering this is a MoE‚Ä¶ using a GGUF at 4 bit should let you run it.",
              "score": 1,
              "created_utc": "2026-02-10 03:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jktti",
          "author": "simracerman",
          "text": "Curious, did you try the MoE version. It seems to be smaller by at least 5GB than Q4_K_XL.",
          "score": 0,
          "created_utc": "2026-02-10 02:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jz2vb",
              "author": "Iory1998",
              "text": "There is only one version and it's an MoE!",
              "score": 2,
              "created_utc": "2026-02-10 03:35:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4k6vb5",
                  "author": "simracerman",
                  "text": "I definitely was sleep typing, lol.\n\nI meant, did you try the MXFP4 version. Unsloth has one.",
                  "score": 2,
                  "created_utc": "2026-02-10 04:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qpagv",
          "author": "allenasm",
          "text": "I just wish it had vision. You can‚Äôt paste results or anything to it visually.",
          "score": 0,
          "created_utc": "2026-02-11 03:56:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4t2dwi",
              "author": "Iory1998",
              "text": "Patience my friend. Qwen-3.5 has the same architecture and comes with vision capabilities out of the box.",
              "score": 0,
              "created_utc": "2026-02-11 14:49:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ht865",
          "author": "Porespellar",
          "text": "Sorry, my OCD won‚Äôt let me mentally consider it for anything other than coding because it says ‚ÄúCoder‚Äù in the model name.",
          "score": -7,
          "created_utc": "2026-02-09 20:28:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixxud",
              "author": "Iory1998",
              "text": "I smell sarcasm :D",
              "score": 3,
              "created_utc": "2026-02-09 23:57:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ig79t",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-02-09 22:22:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ixc31",
              "author": "Iory1998",
              "text": "Really? How do you know that?",
              "score": 3,
              "created_utc": "2026-02-09 23:54:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gwhuj",
          "author": "[deleted]",
          "text": "Some say a picture is worth a thousand words:\n\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1qvy6ig/the_king_has_returned/",
          "score": -8,
          "created_utc": "2026-02-09 17:50:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r03wfq",
      "title": "Bad news for local bros",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ui5ovstbygig1.jpeg",
      "author": "FireGuy324",
      "created_utc": "2026-02-09 13:14:31",
      "score": 506,
      "num_comments": 230,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4gcy27",
          "author": "AutomataManifold",
          "text": "No, this is good news. Sure, you can't run it on your pile of 3090s, but the open availability of massive frontier models is a healthy thing for the community. It'll get distilled down and quantized into things you can run on your machine. If open models get stuck with only tiny models, then we're in trouble long-term.",
          "score": 152,
          "created_utc": "2026-02-09 16:17:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hlh5v",
              "author": "True_Requirement_891",
              "text": "Exactly.",
              "score": 16,
              "created_utc": "2026-02-09 19:48:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4jcrkn",
                  "author": "bitcodler",
                  "text": "That's what she said",
                  "score": 1,
                  "created_utc": "2026-02-10 01:22:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kjkmx",
              "author": "foldl-li",
              "text": "Correct. But these huge models are love letters to millionaires/companies, not ordinaries.",
              "score": 6,
              "created_utc": "2026-02-10 06:01:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fiyw6",
          "author": "Impossible_Art9151",
          "text": "indeed difficult for local seups. as long as they continue to publish smaller models I do not care about this huge frontiers. curious to see how it compares with openai, anthropic.",
          "score": 166,
          "created_utc": "2026-02-09 13:40:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ga84q",
              "author": "FrankNitty_Enforcer",
              "text": "100%. For those of us that work in shops that want to run big budget workloads I love that there are contenders in every weight class, so to speak. \n\nNot that it makes sense in every scenario, but hosting these on IaaS or on-prem to keep all inference private is a major advantage over closed-weight, API-only offerings regardless of what privacy guarantees the vendor makes",
              "score": 40,
              "created_utc": "2026-02-09 16:04:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gln2h",
                  "author": "Infninfn",
                  "text": "You don't even need open weights....Azure AI Foundry hosts full fat Opus 4.6 and GPT-5.2 without reusing your prompts and data. Just saying.",
                  "score": -3,
                  "created_utc": "2026-02-09 16:58:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fl9p7",
              "author": "tarruda",
              "text": "Try Step 3.5 Flash if you have 128GB. Very strong model.",
              "score": 43,
              "created_utc": "2026-02-09 13:53:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gl51d",
                  "author": "jinnyjuice",
                  "text": "The model is 400GB. Even if it's 4 bit quant, it's 100GB. That leaves no room for context, no? Better to have at least 200GB.",
                  "score": 10,
                  "created_utc": "2026-02-09 16:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fj2nu",
              "author": "FireGuy324",
              "text": "I guarantee it's sonnet 4.5 level. The writing is on another level",
              "score": 7,
              "created_utc": "2026-02-09 13:41:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4hp74c",
              "author": "slippery",
              "text": "This is why I tapped out and went with together.ai. it also took the strain off my budget. Maybe hardware will catch up at some point and there won't be chip and ram shortages.",
              "score": 0,
              "created_utc": "2026-02-09 20:08:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ffz0g",
          "author": "nvidiot",
          "text": "I hope they produce two more models - a lite model with a similar size as current GLM 4.x series, and an Air version. It would be sad to see the model completely out of reach for many local users.",
          "score": 90,
          "created_utc": "2026-02-09 13:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fminl",
              "author": "geek_at",
              "text": "I'm sure someone will start a religion or cult stating the peak of AI was at 20B parameters and they will only work with models of that size for hundreds of years.\n\nThey might be called the LLAmish",
              "score": 113,
              "created_utc": "2026-02-09 14:01:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4g14en",
                  "author": "oodelay",
                  "text": "And instead of using RAM chips, they use barns filled with old people remembering a bunch of numbers with an fast talking auctioneer telling everyone when to speak their numbers and weight. \n\nIt's a subculture called the RAMish",
                  "score": 38,
                  "created_utc": "2026-02-09 15:20:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fxkkg",
                  "author": "Aaaaaaaaaeeeee",
                  "text": "eventually due to a fresh wave of ram shortages, they had to quantize their young. 23BandMe helped facilitate proper QAT/QAD recovery for self-attention and a direct injection of mmproj, which was actually their downfall.¬†",
                  "score": 17,
                  "created_utc": "2026-02-09 15:02:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fyt5m",
                  "author": "SpicyWangz",
                  "text": "That‚Äôs it. You‚Äôre going in time out.",
                  "score": 7,
                  "created_utc": "2026-02-09 15:08:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gfd2h",
                  "author": "i_am_fear_itself",
                  "text": "> LLAmish\n\nGrrr! take your upvote.",
                  "score": 4,
                  "created_utc": "2026-02-09 16:29:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hth4b",
                  "author": "gregusmeus",
                  "text": "I wouldn‚Äôt call that pun LLame but just a little LLAmish.",
                  "score": 5,
                  "created_utc": "2026-02-09 20:29:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4g6d5k",
              "author": "Cferra",
              "text": "I think that it's getting to that point where these models are eventually going to be outside normies or even enthusiasts reach.  ",
              "score": 4,
              "created_utc": "2026-02-09 15:46:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fshl1",
          "author": "tmvr",
          "text": "The situation would not be so bad if not for the RAMpocalypse. We have pretty good models in the \\~30B range and then have the better ones in the 50-60-80 GB size range MoE (GLM 4.6V, Q3 Next, gpt-oss 120B), so if the consumer GPUs would have progressed as expected we would have a 5070Ti Super 24GB probably in the 700-800 price range and a 48GB fast new setup would be in a relatively normal price range. Without being dependent on now many years old 3090 cards. But of course this is not where we are.",
          "score": 38,
          "created_utc": "2026-02-09 14:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4i9pm6",
              "author": "ThePixelHunter",
              "text": "It's only been a few months since RAM prices exploded. If the rumored Super series were coming, it wouldn't have been until late this year at best. They'd also be scalped to hell.",
              "score": 5,
              "created_utc": "2026-02-09 21:49:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ihpfh",
                  "author": "tmvr",
                  "text": "The Super cards were to be introduced at CES a month ago with availability in the weeks after as usually. That's obviously out of the windows now and the current situation is that the Super cards will be skipped and the next releases will be the 60 series at the end of 2027. Of course NV has the option and opportunity to change all that in case something happens and there is a hickup in the whole \"we need all the memory in the world for AI\" situation.",
                  "score": 4,
                  "created_utc": "2026-02-09 22:30:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kawxd",
              "author": "toadi",
              "text": "my 2024 razer with rtx4090 has 24GB. Everything seems a downgrade after if I go 50xx. I can not afford a 5090 either :D",
              "score": 2,
              "created_utc": "2026-02-10 04:54:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fft13",
          "author": "ciprianveg",
          "text": "20x3090..",
          "score": 124,
          "created_utc": "2026-02-09 13:21:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fsqtv",
              "author": "HyperWinX",
              "text": "14 should work, if you run it at Q4 and you need a lot of context",
              "score": 31,
              "created_utc": "2026-02-09 14:36:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fz54u",
                  "author": "pmp22",
                  "text": "Q0 on my P40 lets go",
                  "score": 32,
                  "created_utc": "2026-02-09 15:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gkvj0",
                  "author": "YungCactus43",
                  "text": "since GLM 5 is going to be based on deepseek like GLM flash there‚Äôs going to be context compression on VLLM. it should take about 10gb of vram to run it at full context",
                  "score": 3,
                  "created_utc": "2026-02-09 16:55:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4g7mu1",
              "author": "alphapussycat",
              "text": "V100 is getting pretty popular. I don't know if you can bifurcate twice, or if it's trice.\n\n2x v100 32gb, they feed into an nvlink and one adapter card. But I'm not sure if the adapter card uses bifurcation.\n\n10 of these give you 640gb vram. Cost is something like $15k. +mobo with at least 5x x8 with bifurcation.\n\nThe scaling of AI is basically exponential... On the hardware that is. Like exponential hardware for linear improvement.",
              "score": 7,
              "created_utc": "2026-02-09 15:52:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hwthk",
                  "author": "Aphid_red",
                  "text": "Name for that is \"logarithmic\". When using X memory, you get Log(X) quality.",
                  "score": 5,
                  "created_utc": "2026-02-09 20:46:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4hvquu",
                  "author": "meltbox",
                  "text": "You can run the through plz switches too",
                  "score": 1,
                  "created_utc": "2026-02-09 20:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ghphk",
              "author": "Healthy-Nebula-3603",
              "text": "Taste oy 480 VRAM ...still not enough:)",
              "score": 2,
              "created_utc": "2026-02-09 16:40:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4h08a7",
              "author": "ballshuffington",
              "text": "ü§£",
              "score": 2,
              "created_utc": "2026-02-09 18:08:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fmner",
          "author": "__JockY__",
          "text": "Godsammit, you mean I need _another_ four RTX 6000s??? Excellent, my wife was just wondering when I‚Äôd invest in more of those‚Ä¶",
          "score": 74,
          "created_utc": "2026-02-09 14:01:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g2oz8",
              "author": "Porespellar",
              "text": "https://i.redd.it/8mbx5pu6mhig1.gif",
              "score": 38,
              "created_utc": "2026-02-09 15:28:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4gjht7",
              "author": "MelodicRecognition7",
              "text": "you mean your AI waifu?",
              "score": 15,
              "created_utc": "2026-02-09 16:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gu1ij",
                  "author": "Cool-Chemical-5629",
                  "text": "This brings the whole \"Wife spends all the money\" to a whole new level, doesn't it? ü§£",
                  "score": 14,
                  "created_utc": "2026-02-09 17:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lsdp8",
                  "author": "Phonehippo",
                  "text": "As my learn AI project, I just finished making one of these on my qwen3-8b only to find out she's retarded. But atleast her avatar is pretty and she loves her props and animations lol.¬†",
                  "score": 1,
                  "created_utc": "2026-02-10 12:38:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4hlkmr",
              "author": "getfitdotus",
              "text": "yes i need 4 more too , can u order mine also get a better discount. I also will require the rack server to fit all 8. ",
              "score": 3,
              "created_utc": "2026-02-09 19:49:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4g0hma",
          "author": "No_Conversation9561",
          "text": "This hobby of mine is getting really expensive",
          "score": 17,
          "created_utc": "2026-02-09 15:17:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fpvus",
          "author": "Blues520",
          "text": "Gonna need Q0.1 quants",
          "score": 16,
          "created_utc": "2026-02-09 14:20:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hiea3",
              "author": "Aaaaaaaaaeeeee",
              "text": "Someone show ik: https://arxiv.org/abs/2506.13771",
              "score": 3,
              "created_utc": "2026-02-09 19:33:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fivgq",
          "author": "AppealSame4367",
          "text": "Step 3.5 Flash",
          "score": 25,
          "created_utc": "2026-02-09 13:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjqy2",
              "author": "tarruda",
              "text": "This is my new favorite model. It still has some issues with infinite reasoning loops, but devs are investigating and will probably fix in a upcoming fine tune: https://github.com/ggml-org/llama.cpp/pull/19283#issuecomment-3870270263",
              "score": 11,
              "created_utc": "2026-02-09 13:45:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ft3zt",
              "author": "Thump604",
              "text": "It‚Äôs pretty damn amazing even with the kinks to workout",
              "score": 3,
              "created_utc": "2026-02-09 14:38:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hlh0y",
              "author": "getfitdotus",
              "text": "would like to see the next minimax beat this one since its really the perfect size. I am still somewhat disappointed on glm 5 being so much larger. I already have quite a bit of $$$ invested in local hardware. even coder next is really good for its size. ",
              "score": 2,
              "created_utc": "2026-02-09 19:48:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fk7t4",
          "author": "Glad-Audience9131",
          "text": "as expected. will only go up in size.",
          "score": 24,
          "created_utc": "2026-02-09 13:47:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4geesn",
              "author": "One-Employment3759",
              "text": "As expected no more innovation from AI research, just boring scaling.",
              "score": 6,
              "created_utc": "2026-02-09 16:24:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fsfg5",
          "author": "pmttyji",
          "text": "Hope they each release 100B models(and more) additionally later.",
          "score": 13,
          "created_utc": "2026-02-09 14:34:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h4lhj",
          "author": "eibrahim",
          "text": "Honestly I think this is fine and people are overreacting. The real value of these massive open models isnt running them on your gaming PC. Its that they exist as open weights at all. A year ago the best open model was maybe 70B and it was nowhere close to frontier. Now we got 700B+ open models competing with the best closed ones.\n\nThe distillation pipeline has gotten insanely good too. Every time a new massive teacher model drops, the 30-70B range gets a noticeable bump within weeks. Ive been using Qwen derivatives for production workloads and the quality jump from distilled models is real.\n\nPlus lets be honest, for 95% of actual use cases a well tuned 30B model handles it just fine. The remaining 5% is where you hit the API for a frontier model anyway.",
          "score": 14,
          "created_utc": "2026-02-09 18:28:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4aep",
              "author": "Blues520",
              "text": "When you say a well tuned 30B model, are you referring to coding or something else?",
              "score": 1,
              "created_utc": "2026-02-12 13:12:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gwm4o",
          "author": "jhov94",
          "text": "This is great news if you look past the immediate future. The future of small models depends on more labs having access to large SOTA models. This gives them direct access to a high quality, large SOTA model to distill into smaller ones. ",
          "score": 7,
          "created_utc": "2026-02-09 17:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h3cns",
          "author": "borobinimbaba",
          "text": "You guys remember those old days that 32mb of ram was alot ? It was like 30 years ago.\n\nI'm sure running local llms on the next 30 years hardware would be cheap, most of us are just to old to see those days or maybe care for it.",
          "score": 6,
          "created_utc": "2026-02-09 18:22:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4le28e",
              "author": "techno156",
              "text": "I don't know, given that everything seems to have landed on a sort of steady-state, it seems rather more like we'll be stuck on 16GB or thereabouts for at least the next decade or so, for most machines.\n\nEspecially with memory costing as much as it is.",
              "score": 3,
              "created_utc": "2026-02-10 10:45:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fusln",
          "author": "FullstackSensei",
          "text": "99% of people don't need frontier models 95% of the time. I'd even argue the biggest benefit of such models is for AI labs to continue to improve the variety and quality of their training data to train (much) smaller models. That's a big part of the reason why we continue to see much smaller models beat frontier models from one year before if not less.",
          "score": 30,
          "created_utc": "2026-02-09 14:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4h3810",
              "author": "the320x200",
              "text": "Sour grapes. I didn't want to run it anyway! /s",
              "score": 11,
              "created_utc": "2026-02-09 18:22:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ghxm8",
              "author": "TopNFalvors",
              "text": "Honest question, what would be good for 99% of people 95% of the time?",
              "score": 1,
              "created_utc": "2026-02-09 16:41:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gtjf6",
                  "author": "FullstackSensei",
                  "text": "An ensemble of models for the various tasks one needs. For ex: I know use Qwen3 VL 30B for OCR tasks, Qwen3 Coder 30B/Next or Minimax 2.1 for coding tasks and gpt-oss-120b or Gemma3 27B for general purpose chat. If we exclude Minimax, all the others can be run on three 24GB cards like P40s with pretty decent performance. P40 prices seem to have come down a bit (200-250 a pop), ao you can still ostensibly build a machine with three P40s for a little over 1k using a Broadwell Xeon and 16-32GB RAM.",
                  "score": 4,
                  "created_utc": "2026-02-09 17:36:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gn7ee",
                  "author": "Jon_vs_Moloch",
                  "text": "Something like a current 4B model, but add search and tool calling.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:06:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gwpva",
              "author": "mouseofcatofschrodi",
              "text": "Since it is what I want to believe, it must be true",
              "score": 0,
              "created_utc": "2026-02-09 17:51:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4hc6x8",
              "author": "ProfessionalSpend589",
              "text": "My 2x Strix Halos are too much power efficient and I can't warm my room in the winter with the little sized LLMs (had to wear my hoodie, because I'm actually cold).",
              "score": -1,
              "created_utc": "2026-02-09 19:04:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gqsmm",
          "author": "Conscious_Cut_6144",
          "text": "You underestimate my power.   \n\n\nhttps://preview.redd.it/jzzxdihq6iig1.jpeg?width=1080&format=pjpg&auto=webp&s=660ccda6439ca713d78d21c2d96aa27622cecce8\n\n",
          "score": 18,
          "created_utc": "2026-02-09 17:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4hy0sm",
              "author": "Jonodonozym",
              "text": "You underestimate my power bill",
              "score": 16,
              "created_utc": "2026-02-09 20:51:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ihmtd",
              "author": "panchovix",
              "text": "16 RTX 3090s, so 384GB VRAM? I wonder if you will be able to run GLM5 at Q4, hoping it does.\n\nNow for more VRAM and TP, you have no other way than to add another 16 3090s (?",
              "score": 2,
              "created_utc": "2026-02-09 22:30:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4j9rfe",
                  "author": "Conscious_Cut_6144",
                  "text": "VLLM/Sglang are not great at fitting models that should just barely fit in theory.\n\nI have 1 pro6000 in another machine, going to have to figure out how to get them working together efficiently if this model is as good as I hope.",
                  "score": 1,
                  "created_utc": "2026-02-10 01:04:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gr44c",
          "author": "chloe_vdl",
          "text": "Honestly the real win here isn't running these monsters locally - it's having open weights to distill from. The knowledge compression pipeline from 700B+ teachers down to 30-70B students has gotten way more sophisticated. Look at what Qwen and Llama derivatives managed to squeeze out of their bigger siblings.\n\nThe local scene isn't dead, it's just shifting upstream. We become the fine-tuners and distillers rather than the raw inference crowd. Which tbh is probably more interesting work anyway.",
          "score": 18,
          "created_utc": "2026-02-09 17:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fpq6d",
          "author": "Ult1mateN00B",
          "text": "I have been having loads of fun with minimax-m2.1-reap-30-i1, lightning fast and great reasoning. 45tok/s to be exact on my 4x AI PRO R9700. I use the Q4\\_1 quant, 101GB is a nice fit for me.",
          "score": 6,
          "created_utc": "2026-02-09 14:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ggr44",
          "author": "phenotype001",
          "text": "MiniMax is good though and the q4 barely fits in 128 RAM but fits. ",
          "score": 5,
          "created_utc": "2026-02-09 16:35:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gi9xt",
          "author": "DataGOGO",
          "text": "So roughly 390GB in any Q4, not too bad for a frontier model. \n\nBest way to run local would be 4 H200 NVL's, but that is what? $130k?",
          "score": 5,
          "created_utc": "2026-02-09 16:43:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gnlav",
          "author": "Mauer_Bluemchen",
          "text": "M5 Ultra with 1 TB upcoming ;-)\n\nOr a cluster of 3-4x M3 Ultras - which would be rather slow of course. ",
          "score": 5,
          "created_utc": "2026-02-09 17:08:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fmm1d",
          "author": "ResidentPositive4122",
          "text": "Open models are useful and benefit the community even if they can't be (easily / cheaply) hosted locally. You can always rent to create datasets or fine-tune and run your own models. The point is to have them open.\n\n(that's why the recent obsession with local only on this sub is toxic and bad for the community, but it is what it is...)",
          "score": 12,
          "created_utc": "2026-02-09 14:01:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fjynm",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 4,
          "created_utc": "2026-02-09 13:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fxgxc",
          "author": "a_beautiful_rhind",
          "text": "Damn.. so I can expect Q2 quants and 10t/s unless something changes with numa and/or ddr4 prices. RIP glm-5.",
          "score": 4,
          "created_utc": "2026-02-09 15:01:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g88lp",
          "author": "VoidAlchemy",
          "text": "I didn't check to see if GLM-5 will use QAT targeting \\~4ish BPW for sparse routed experts like the two most recent Kimi-K2.5/K2-Thinking did. This at least makes the \"full size\" model about 55% of what it would otherwise be if full bf16.\n\nIf we quantize the attn/shexp/first N dense layers, it will help a little bit but yeah 44B active will definitely be a little slower than DS/Kimi...",
          "score": 3,
          "created_utc": "2026-02-09 15:55:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gj56a",
          "author": "CanineAssBandit",
          "text": "well shit no wonder it feels more coherent in its writing. it's way bigger active and way bigger period\n\nVERY happy to see that we have another open weights power player keeping pressure on OAI and Anthropic. No replacement for displacement.\n\nI hope they don't leave in the disturbing \"safety guidelines policy\" checker thing always popping up in the thinking in GLM 4.7. Pony Alpha doesn't so I'm hopeful that their censoring got less obtrusive if nothing else",
          "score": 4,
          "created_utc": "2026-02-09 16:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4iwbqv",
          "author": "lgk01",
          "text": "In two years you'll be able to run better ones on 16gb of vram (COPIUM MODE)",
          "score": 5,
          "created_utc": "2026-02-09 23:48:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fhahr",
          "author": "Expensive-Paint-9490",
          "text": "Seems that performance in LLM has already plateaued, and meaningful improvements only come from size increase.\n\nSo much for people spamming that AGI is six months away.",
          "score": 43,
          "created_utc": "2026-02-09 13:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4foj7h",
              "author": "sekh60",
              "text": "While the \"I\" part is for sure questionable at times, my N(atural)GI uses only about 20 Watts.",
              "score": 23,
              "created_utc": "2026-02-09 14:12:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fynaz",
                  "author": "My_Unbiased_Opinion",
                  "text": "Yep and I can assure my NGI has way less than 745B functional parameters. Hehe",
                  "score": 8,
                  "created_utc": "2026-02-09 15:07:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4idlo8",
                  "author": "YouCantMissTheBear",
                  "text": "Your brain isn't working outside your body, stop gaming the metrics",
                  "score": 3,
                  "created_utc": "2026-02-09 22:09:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4gtjay",
                  "author": "Charuru",
                  "text": "We'll get there through chip improvements instead of architectural improvements.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:36:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fzj38",
              "author": "pmp22",
              "text": "Architecture changes will come, it's just not there just yet. LLMs will be small latent space reasoning cores with external memory. Encoding vast knowledge in the weights like we do now is not the future IMHO.",
              "score": 8,
              "created_utc": "2026-02-09 15:12:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fhnrk",
              "author": "DesignerTruth9054",
              "text": "I think once these models are distilled to smaller models we will get direct performance improvements",
              "score": 20,
              "created_utc": "2026-02-09 13:32:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4g8f7l",
                  "author": "beryugyo619",
                  "text": "why tf that work? not doubting it works, but it's weird that it does",
                  "score": 3,
                  "created_utc": "2026-02-09 15:56:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4fohek",
                  "author": "disgruntledempanada",
                  "text": "But ultimately be nowhere near where the large models are sadly.",
                  "score": 6,
                  "created_utc": "2026-02-09 14:12:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fs7v4",
              "author": "nomorebuttsplz",
              "text": "That makes no sense. If you can compare like sized models across times span there is literally no case in which the increases have not been significant.\n\nTwo things can happen simultaneously: models can get bigger and models can get better per size.",
              "score": 12,
              "created_utc": "2026-02-09 14:33:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gb6kd",
                  "author": "Nowitcandie",
                  "text": "\\-Models are getting bigger but already suffering from diminishing returns at an accelerated pace. At some point this will reach its limit where bigger won't increase performance at all. Diminishing marginal gains tend towards zero. Making the best models smaller too has it's limits without some serious breakthroughs (perhaps scalable quantum computing.)",
                  "score": 3,
                  "created_utc": "2026-02-09 16:09:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4iim4s",
              "author": "Xyrus2000",
              "text": "LLMs are just one form of AI, and an LLM isn't designed to achieve AGI.\n\nAGI isn't going to come from a system that can't learn and self-improve. All LLMs are \"fixed brains\". They don't learn anything after they're trained. They're like the movie Memento. You've got their training and whatever the current context is. When the context disappears, they're back to just their training.\n\nWe have the algorithms. We're just waiting for the hardware to catch up. Sometime within the next 5 to 10 years.",
              "score": 3,
              "created_utc": "2026-02-09 22:35:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fko4e",
              "author": "RIPT1D3_Z",
              "text": "Step 3.5 Flash proves it's wrong.",
              "score": 8,
              "created_utc": "2026-02-09 13:50:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fxpl8",
                  "author": "a_beautiful_rhind",
                  "text": "Flash is strong but not that strong. Kimi and 5 feel smarter.",
                  "score": 5,
                  "created_utc": "2026-02-09 15:03:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4jgg3m",
                  "author": "Zc5Gwu",
                  "text": "Step 3.5 Flash feels like qwq part 2. It thinks *a lot*.",
                  "score": 1,
                  "created_utc": "2026-02-10 01:44:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fqkbv",
              "author": "Nowitcandie",
              "text": "Hard agree, and the scaling economics seems to hold to diminishing marginal returns. Perhaps in part because everybody scaling simultaneously is driving up chip and hardware prices.¬†",
              "score": 4,
              "created_utc": "2026-02-09 14:24:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gf1fm",
                  "author": "One-Employment3759",
                  "text": "Yeah, if everyone just acted normal instead of going bongobongo we could keep doing research instead of hype train.",
                  "score": 4,
                  "created_utc": "2026-02-09 16:27:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gck16",
              "author": "ThisWillPass",
              "text": "16 months.",
              "score": 1,
              "created_utc": "2026-02-09 16:15:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fldki",
              "author": "iDefyU__",
              "text": "AGI?? Do you really believe that?",
              "score": -9,
              "created_utc": "2026-02-09 13:54:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4flto2",
                  "author": "vladlearns",
                  "text": "just 2000000000 more billons, bro\nAGI is almost here, trust me, bro¬†",
                  "score": 14,
                  "created_utc": "2026-02-09 13:57:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4flusw",
              "author": "abdouhlili",
              "text": "Lost me at AGI part.",
              "score": -7,
              "created_utc": "2026-02-09 13:57:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4j0g6j",
          "author": "ttkciar",
          "text": "... where is the bad news? I see none here!",
          "score": 3,
          "created_utc": "2026-02-10 00:11:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fgzoi",
          "author": "silenceimpaired",
          "text": "Where is this chart from?",
          "score": 5,
          "created_utc": "2026-02-09 13:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fkb9w",
              "author": "FireGuy324",
              "text": "https://preview.redd.it/p1lj1bxb4hig1.png?width=855&format=png&auto=webp&s=15cde1be894f1bb57c274f63a6078c3eb32f33ef\n\nDid some math  \nvocab\\_size √ó hidden\\_size = 154,880 √ó 6,144 = 951,403,520 q\\_a\\_proj: 6,144 √ó 2,048 = 12,582,912 q\\_b\\_proj: 2,048 √ó (64 √ó 256) = 33,554,432 kv\\_a\\_proj: 6,144 √ó (512 + 64) = 3,538,944 kv\\_b\\_proj: 512 √ó (64 √ó (192 + 256)) = 512 √ó 28,672 = 14,680,064 o\\_proj: (64 √ó 256) √ó 6,144 = 16,384 √ó 6,144 = 100,663,296 Total attention/couche = 165,019,648 Total attention (78√ó) = 165,019,648 √ó 78 = 12,871,532,544 gate\\_proj: 6,144 √ó 12,288 = 75,497,472 up\\_proj: 6,144 √ó 12,288 = 75,497,472 down\\_proj: 12,288 √ó 6,144 = 75,497,472 Total MLP Dense/couche = 226,492,416 gate\\_up\\_proj: 6,144 √ó (2 √ó 2,048) = 25,165,824 down\\_proj: 2,048 √ó 6,144 = 12,582,912 Total expert = 37,748,736 Experts (256 √ó 37,748,736) = 9,663,676,416 Shared experts = 226,492,416 Total MoE layer = 9,890,168,832 Total MoE (77√ó) = 9,890,168,832 √ó 77 = 761,542,999,904 2 √ó hidden*size = 2 √ó 6,144 = 12,288 Total LayerNorm (78√ó) = 12,288 √ó 78 = 958,464 Embeddings: 951,403,520 Attention (78√ó): 12,871,532,544 MLP Dense (1√ó): 226,492,416 MoE (77√ó): 761,542,999,904 LayerNorm (78√ó): 958,464 TOTAL = 775,592,386,848 ‚âà 776b*",
              "score": 15,
              "created_utc": "2026-02-09 13:48:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4frbkc",
                  "author": "silenceimpaired",
                  "text": "Sad day for me. Guess it‚Äôs 4.7 at 2bit for life‚Ä¶ unless they also have GLM 5 Air (~100b) and oooo GLM Water (~300b)",
                  "score": 2,
                  "created_utc": "2026-02-09 14:28:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4g4caf",
                  "author": "notdba",
                  "text": "3 dense + 75 sparse right?\n\nNumber of parameters on CPU: 6144 \\* 2048 \\* 3 \\* 256 \\* 75 = 724775731200\n\nWith IQ1\\_S\\_R4 (1.50 bpw): 724775731200 \\* 1.5 / 8 / (1024 \\* 1024 \\* 1024) = 126.5625 GiB\n\nBy moving 5\\~6 GiB to VRAM, this can still fit a 128 GiB RAM + single GPU setup.\n\nAnd just like magic, [https://github.com/ikawrakow/ik\\_llama.cpp/pull/1211](https://github.com/ikawrakow/ik_llama.cpp/pull/1211) landed right on time to free up several GiB of VRAM. We have to give it a try.",
                  "score": 1,
                  "created_utc": "2026-02-09 15:36:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gek1r",
          "author": "MerePotato",
          "text": "Ultimately if you want to push capabilities without a major architectural innovation you're probably gonna have to scale somewhat. Blame the consumer hardware market for not keeping up, not the labs.",
          "score": 6,
          "created_utc": "2026-02-09 16:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gep8c",
              "author": "FireGuy324",
              "text": "Blame the other corpos who makes GPU more expensive than they should be",
              "score": 4,
              "created_utc": "2026-02-09 16:26:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fnjm3",
          "author": "tarruda",
          "text": "They have a release cycle that is too short IMO. Did they have time to research innovative improvements or experiment with new training data/methods?\n\nThis will likely be a significant improvement over GLM 4.x as it has doubled the number of parameters, but it is not an impressive release if all they do is chase after Anthropic models.\n\nI would rather see open models getting more efficient while approaching performance of bigger models, as StepFun did with Step 3.5 Flash.",
          "score": 8,
          "created_utc": "2026-02-09 14:07:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frmb8",
              "author": "nullmove",
              "text": "I think this was always their \"teacher\" model they were distilling down from for 4.x. And sure ideally they would like to do research too, but maybe the reality of economics doesn't allow that. Their major revenue probably comes from coding plans, and people are not happy with Sonnet performance when Opus 4.5 is two gen old now.",
              "score": 8,
              "created_utc": "2026-02-09 14:30:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4flwcm",
          "author": "WSATX",
          "text": "I'm too poor to be a local bro ü•≤",
          "score": 4,
          "created_utc": "2026-02-09 13:57:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gdpm5",
              "author": "JLeonsarmiento",
              "text": "https://preview.redd.it/hcvgeyrnvhig1.jpeg?width=480&format=pjpg&auto=webp&s=0d7317e4a5b8c1f25feaee3b50c70d35fba212bf\n\n",
              "score": 1,
              "created_utc": "2026-02-09 16:21:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fhoec",
          "author": "power97992",
          "text": "Wait until u see ds v4‚Ä¶.",
          "score": 4,
          "created_utc": "2026-02-09 13:32:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fzpcj",
              "author": "SpicyWangz",
              "text": "4t",
              "score": 2,
              "created_utc": "2026-02-09 15:13:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gve4o",
                  "author": "power97992",
                  "text": "Source?¬†",
                  "score": 1,
                  "created_utc": "2026-02-09 17:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fno52",
          "author": "ObviNotMyMainAcc",
          "text": "I mean, secondhand MI210's are coming down in price. They have 64gb of HBM a pop. 8 of those and some mild quants, done.\n\nOkay, that's still silly money, but running top spec models in any reasonable way always was.\n\nNot to mention NVFP4 and MXFP4 retain like 90 - 95% accuracy, so some serious size reduction is possible without sacrificing too much.\n\nNo, a Mac studio doesn't count unless you use almost no context. Maybe in the future some time as there are some really interesting transformer alternatives being worked on.\n\nSo not really doom and gloom.",
          "score": 6,
          "created_utc": "2026-02-09 14:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gmzo3",
              "author": "usrnamechecksoutx",
              "text": "\\>No, a Mac studio doesn't count unless you use almost no context.\n\nCan you elaborate?",
              "score": 3,
              "created_utc": "2026-02-09 17:05:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gr0rj",
                  "author": "ObviNotMyMainAcc",
                  "text": "At short contexts, they fast enough. As context gets longer, their speed degrades faster than other solutions. Prompt processing speed is not their strong suit.\n\nIt will be interesting to see how they go with subquadratic models which can have reasonable prompt processing speeds out to like 10 million tokens on more traditional hardware.",
                  "score": 1,
                  "created_utc": "2026-02-09 17:24:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fjegq",
          "author": "CommanderData3d",
          "text": "qwen?",
          "score": 2,
          "created_utc": "2026-02-09 13:43:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fl57o",
              "author": "tarruda",
              "text": "Apparently Qwen 3.5 initial release will have a 35b MoE: https://x.com/chetaslua/status/2020471217979891945\n\nHopefully they will also publish a LLM in the 190B - 210B range for 128GB devices.",
              "score": 13,
              "created_utc": "2026-02-09 13:53:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fvtc3",
                  "author": "Impossible_Art9151",
                  "text": "device clustering is coming nowadays, 2 x dgx or 2 x strix => 256GB RAM.  \n400B models in q4 or 200b in fp6",
                  "score": 0,
                  "created_utc": "2026-02-09 14:52:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fr4vk",
          "author": "Johnny_Rell",
          "text": "Let's hope it's 1.58bit or somethingüòÖ",
          "score": 2,
          "created_utc": "2026-02-09 14:27:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fzh86",
          "author": "Lissanro",
          "text": "K2.5 is actually even larger since it also includes mmproj for vision. I run Q4_X quant of K2.5 the most on my PC, but for those who are yet to buy the hardware RAM prices are going to be huge issue.\n\n\nThe point is, it is memory cost issue rather than model size issue, which are going only to grow over time... I can only hope by the next time I need to upgrade prices will be better.",
          "score": 2,
          "created_utc": "2026-02-09 15:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g0acj",
          "author": "Septerium",
          "text": "Perhaps they are aiming to release something with native int-4 quantization? I think this has the potential to become an industry standard in the near future",
          "score": 2,
          "created_utc": "2026-02-09 15:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gpmcs",
          "author": "Such_Web9894",
          "text": "When can we create subspecialized localized models/agents‚Ä¶.   \nExample‚Ä¶.  \n\nQwen3_refractor_coder. \n  \nQwen3_planner_coder.  \n  \nQwen3_tester_coder.    \n  \nQwen3_coder_coder\n\nAll 20 GBs. \n  \nThen the local agent will unload and load the model as needed to get specialized help.  \n  \nWhy have the whole book open.   \nJust ‚Äúopen‚Äù the chapter.  \n  \n  \nWill it be fast.. no. \n  \nBut it will be possible.  \n  \nThen offload unused parameters and context to system ram with engram.",
          "score": 2,
          "created_utc": "2026-02-09 17:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i0086",
          "author": "Guilty_Rooster_6708",
          "text": "Can‚Äôt wait for the Q0.01 XXXXXS quant to run on my 16gb VRAM 32gb RAM.",
          "score": 2,
          "created_utc": "2026-02-09 21:01:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j6udv",
          "author": "silenceimpaired",
          "text": "Shame no one asked at the AMA if they would try to not forget the local scene. It's so weird how often a AMA on LocalLLaMA is followed by a model that can't be used by us.",
          "score": 2,
          "created_utc": "2026-02-10 00:48:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jcvkp",
          "author": "LocoMod",
          "text": "We all going to be /r/remotellama soon enough",
          "score": 2,
          "created_utc": "2026-02-10 01:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kbf9x",
          "author": "Agreeable-Market-692",
          "text": "Hey, if you're reading this do not despair. If you have a specific kind of task type or a domain you are working that you want to run this model for, try the full model out somewhere online once it hits. Then after you do a couple of quick and dirty projects in it, take your prompts, and use that to generate a set of new prompts in the same domain or of the same task type. \n\nOnce you have your promptset then you load the model with REAP (code is on cerebras github) on a GPU provider if you don't the hardware yourself. Let REAP run through YOUR custom promptset instead of the default (but do compare your promptset to the default to get an idea of a baseline).\n\nThen REAP will prune whatever parameters are less likely to be important to your application for this model and you can begin your quantization. I personally really like all of u/noctrex 's quants and if you look around you can figure out most or all of how to do those.\n\nRemember though, your promptset is how REAP calibrates what to chop off so check that default promptset and make sure your custom one has as much coverage as possible for your use case. ",
          "score": 2,
          "created_utc": "2026-02-10 04:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kgpo2",
          "author": "jferments",
          "text": "All of these large models will usually be followed by smaller/distilled versions that can be run on local hardware. It's great to have both be freely available.",
          "score": 2,
          "created_utc": "2026-02-10 05:38:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4oo5jm",
          "author": "dwstevens",
          "text": "why is this bad news?",
          "score": 2,
          "created_utc": "2026-02-10 21:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fwpb2",
          "author": "henk717",
          "text": "The only change here is GLM right? Deepseek/kimi were already large.  \nAnd for GLM its not that big of a loss because they release smaller versions of their model for the local users.  \nSo I personally rather have the really top models try to compete with closed source models so that the open scene is competitive, thats a win for everyone but especially users who don't want to be tied down to API providers.  \nAnd then for the local home user they should keep releasing stuff we can fit which GLM has repeatedly done.  \nDeepseek and Kimi should also begin doing this, it would make that playing field more interesting.\n\nBut we also still have Qwen, Gemini and Mistral as possible players who tend to release at more local friendly sizes.",
          "score": 3,
          "created_utc": "2026-02-09 14:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fjv32",
          "author": "CovidCrazy",
          "text": "Fuck I‚Äôm gonna need another M3 Ultra",
          "score": 2,
          "created_utc": "2026-02-09 13:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4flyrv",
              "author": "power97992",
              "text": "No u need an m5 ultra",
              "score": 11,
              "created_utc": "2026-02-09 13:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fpd75",
                  "author": "tmvr",
                  "text": "I'll be honest, I would be fine with an M4 Competition with xDrive.",
                  "score": 6,
                  "created_utc": "2026-02-09 14:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fsxmb",
              "author": "nomorebuttsplz",
              "text": "Why? This is perfect size for Q4.",
              "score": 6,
              "created_utc": "2026-02-09 14:37:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4fyda1",
                  "author": "CovidCrazy",
                  "text": "The quants are usually a little retarded. I don‚Äôt go below 8bit",
                  "score": 6,
                  "created_utc": "2026-02-09 15:06:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4fy9gd",
              "author": "calcium",
              "text": "Currently waiting for the new M5 MBP's to be released...",
              "score": 1,
              "created_utc": "2026-02-09 15:05:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4g32xe",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/6excw0jdmhig1.png?width=1373&format=png&auto=webp&s=26ce5055e7eea740a2d6aa6e99f3922ce1935955\n\nTrying to do my best. Testing W7800 48gb. More gb/sec (memory) then 3090 or 5070ti. Doing benchmark.  1475‚Ç¨ +vat for 48gb is life saver.",
          "score": 2,
          "created_utc": "2026-02-09 15:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g5irq",
              "author": "LegacyRemaster",
              "text": "https://preview.redd.it/pxszmhsiohig1.png?width=1933&format=png&auto=webp&s=042468f68d5203805b12f3e39d59db5cd959c7f4\n\nquick test on Lm studio + Vulkan. \"write a story 1000 tokens\". Minimax m2.1 IQ4 XS. Downloading Q4\\_K\\_XL now\n\n",
              "score": 1,
              "created_utc": "2026-02-09 15:42:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4frewo",
          "author": "hydropix",
          "text": "I wonder how they manage to optimize the use of their server? Yesterday, I used a Kimi 2.5 subscription non-stop for coding. At $39/month, I only used 15% of the weekly limit, even with very intensive use. To run such a large model, you need a server costing at least $90,000 (?). I wonder how much time I actually used on such a machine. Because it cost me less than $1.30 in the end. Does anyone have any ideas about this?\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-09 14:28:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4g66q2",
              "author": "Sevii",
              "text": "You aren't getting the full output of one server. \n\nhttps://blog.vllm.ai/2025/12/17/large-scale-serving.html",
              "score": 3,
              "created_utc": "2026-02-09 15:45:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4gflxv",
                  "author": "hydropix",
                  "text": "very interesting, thanks.",
                  "score": 1,
                  "created_utc": "2026-02-09 16:30:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fvuh9",
          "author": "INtuitiveTJop",
          "text": "We‚Äôre just going to be funding Apple that‚Äôs all",
          "score": 1,
          "created_utc": "2026-02-09 14:53:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g654g",
          "author": "dobkeratops",
          "text": "need 2 x 512gb mac studios",
          "score": 1,
          "created_utc": "2026-02-09 15:45:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g7hvx",
          "author": "muyuu",
          "text": "waiting for Medusa Halo 512GB x2 clusters",
          "score": 1,
          "created_utc": "2026-02-09 15:51:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g84pf",
          "author": "Zyj",
          "text": "Even with 2x Strix Halo, that‚Äòs mostly out of the question (except GLM 4.5 Q4). Ouch.",
          "score": 1,
          "created_utc": "2026-02-09 15:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gt8of",
          "author": "Charuru",
          "text": "This will be the last hurrah for DSA. If it doesn't work here we'll probably never see it again, go back to MLA.",
          "score": 1,
          "created_utc": "2026-02-09 17:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gtljo",
          "author": "Cool-Chemical-5629",
          "text": "And here I thought DeepSeek was big LOL",
          "score": 1,
          "created_utc": "2026-02-09 17:36:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hzjuk",
          "author": "gamblingapocalypse",
          "text": "Thats gonna be a lot of macbook pros.",
          "score": 1,
          "created_utc": "2026-02-09 20:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4i1hpm",
          "author": "portmanteaudition",
          "text": "Pardon my ignorance but how does this translate into hardware requirements?",
          "score": 1,
          "created_utc": "2026-02-09 21:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jbw81",
              "author": "DragonfruitIll660",
              "text": "Larger overall parameters means you need more Ram/Vram to fit the whole model. So it went from 355B to 745B total parameters, meaning its going to take substantially more space to fully load the model (without offloading to disk). Hence higher hardware requirements (Q4KM GLM 4.7 is 216 GB with 355B parameters, Q4KM Deepseek V3 is 405GB with 685B parameters).",
              "score": 2,
              "created_utc": "2026-02-10 01:17:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ignae",
          "author": "No-Veterinarian8627",
          "text": "I wait and hope CXL will get some research breakthroughs... one man can hope",
          "score": 1,
          "created_utc": "2026-02-09 22:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jrpan",
          "author": "BumblebeeParty6389",
          "text": "Are you a cloud bro?",
          "score": 1,
          "created_utc": "2026-02-10 02:49:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4k14fz",
              "author": "FireGuy324",
              "text": "Kind of",
              "score": 1,
              "created_utc": "2026-02-10 03:48:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jwmcv",
          "author": "Oldspice7169",
          "text": "Skill issue, just throw money at the problem, anon humans can live off ramen for centuries",
          "score": 1,
          "created_utc": "2026-02-10 03:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4kxnff",
          "author": "HarjjotSinghh",
          "text": "bros are way too invested in their own drama.",
          "score": 1,
          "created_utc": "2026-02-10 08:07:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l8srl",
          "author": "Truth-Does-Not-Exist",
          "text": "qwen3-coder-next is wonderful",
          "score": 1,
          "created_utc": "2026-02-10 09:57:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4o13w8",
          "author": "[deleted]",
          "text": "With the current rate of progress in LLM development I am not at all worried we will see compression (quantization) making massive leaps as well. Running capable LLMs on phones and Raspberry PIs is a goal for the open source community as well as those monetizing this technology. It's just a question of time at this point. ",
          "score": 1,
          "created_utc": "2026-02-10 19:19:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qdk1u",
          "author": "Crypto_Stoozy",
          "text": "Let‚Äôs be honest here though the hardware limitations are not what you think they are this isn‚Äôt postive it‚Äôs negative for the creators. You can‚Äôt sell this in mass they are already losing tons of money. The future is getting small model params to be more efficient not getting more parameters that require large hardware. Something that requires 200k to run it isn‚Äôt scalable.",
          "score": 1,
          "created_utc": "2026-02-11 02:42:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s189a",
          "author": "Good_Work_8574",
          "text": "step 3.5 flash is 200B model,activited 11B,you can try that.",
          "score": 1,
          "created_utc": "2026-02-11 10:46:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fuy6c",
          "author": "psoericks",
          "text": "I'm hanging in there, next year I should still be able to run GLM_6.5_Flash_Q1_XS_REAP",
          "score": 1,
          "created_utc": "2026-02-09 14:48:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4h5net",
          "author": "Individual-Source618",
          "text": "Dont worry, the intel ZAM memory will become available in 2030, then he will not be limited by bandwidth or vram to run such models",
          "score": 1,
          "created_utc": "2026-02-09 18:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gfrq7",
          "author": "jonheartland",
          "text": "Well, of course they're focusing on creating the biggest possible models first. Gotta justify building a bazillion data centers somehow...",
          "score": 0,
          "created_utc": "2026-02-09 16:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gl5x1",
          "author": "AnomalyNexus",
          "text": "It‚Äôs been that way for a while tbh - it‚Äôs just an unpopular take in this sub",
          "score": 0,
          "created_utc": "2026-02-09 16:56:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ffzik",
          "author": "TechySpecky",
          "text": "Can these still run on 2xH200?",
          "score": 0,
          "created_utc": "2026-02-09 13:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fh5j7",
              "author": "MaxKruse96",
              "text": "141GB x 2 = 282GB. A 745B model, at Q4, would be 745 \\* (4/8) = 373gb and thats just napkin math. You'd need to go down to IQ3S or something similar to even load it.",
              "score": 8,
              "created_utc": "2026-02-09 13:29:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4gpoq2",
              "author": "Single_Ring4886",
              "text": "Dont be cheap you need ful 8x server :)",
              "score": 1,
              "created_utc": "2026-02-09 17:18:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4fh2dp",
              "author": "power97992",
              "text": "maybe q1.5 or q2? ",
              "score": 1,
              "created_utc": "2026-02-09 13:28:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4frvms",
          "author": "Legitimate-Pumpkin",
          "text": "Is it possible to load only the active parameters? You know? Load the ‚Äúdirector‚Äù then it ask to load a specific expert and you only load both of them, or even maybe unload the director and load only the expert. Slow but usable (if possible at all. I don‚Äôt know what I‚Äôm talking about).",
          "score": 0,
          "created_utc": "2026-02-09 14:31:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fxold",
          "author": "johnnyApplePRNG",
          "text": "Plot twist: GLM 5 Flash Air scores 98% on all benchmarks after being whittled down to just 150B/3B.",
          "score": 0,
          "created_utc": "2026-02-09 15:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4jnem1",
          "author": "lisploli",
          "text": "What's bad about it? I don't need all the experts. One is enough. It's not like the model would ask more than one of them anyways. Or does it? Two? Doesn't seem like a big loss.",
          "score": 0,
          "created_utc": "2026-02-10 02:24:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fl30b",
          "author": "Lithium_Ii",
          "text": "I'm happy with GLM-4.7-Flash",
          "score": -4,
          "created_utc": "2026-02-09 13:52:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4izy4l",
              "author": "ttkciar",
              "text": "Can relate. I'm happy with GLM-4.5-Air.\n\nIf a distill from GLM-5 outperforms it, though, so much the better! Looking forward to seeing what they come up with.",
              "score": 1,
              "created_utc": "2026-02-10 00:08:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0w7st",
      "title": "Qwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering",
      "subreddit": "LocalLLaMA",
      "url": "https://qwen.ai/blog?id=qwen-image-2.0",
      "author": "RIPT1D3_Z",
      "created_utc": "2026-02-10 09:25:15",
      "score": 497,
      "num_comments": 100,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/",
      "domain": "qwen.ai",
      "is_self": false,
      "comments": [
        {
          "id": "o4ldfno",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 10:40:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lculd",
          "author": "waescher",
          "text": "Nice Tease in one of their sample images\n\nhttps://preview.redd.it/oeobh78manig1.png?width=332&format=png&auto=webp&s=cebb6ad784b841ff45b9d5ad4c3d95887a661069\n\n",
          "score": 170,
          "created_utc": "2026-02-10 10:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lgivy",
              "author": "ahmetegesel",
              "text": "Wow that‚Äôs brilliant",
              "score": 38,
              "created_utc": "2026-02-10 11:07:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lh2xo",
              "author": "Far-Low-4705",
              "text": "I‚Äôm so hyped lol.\n\nReally hoping for an eventual qwen 3.5 80b vision varient (eventually)",
              "score": 15,
              "created_utc": "2026-02-10 11:12:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4poo3k",
                  "author": "10minOfNamingMyAcc",
                  "text": "Really hoping there'll be a <70B variant that I can run locally.",
                  "score": 3,
                  "created_utc": "2026-02-11 00:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4l7z0r",
          "author": "RIPT1D3_Z",
          "text": "BTW I dunno why, but Qwen team decided to introduce this as one of the showcase images\n\nhttps://preview.redd.it/2je8msoj2nig1.png?width=1765&format=png&auto=webp&s=c1119dd539d62df89b74b5507b91eae93bee6bad\n\n",
          "score": 215,
          "created_utc": "2026-02-10 09:49:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4ldat2",
              "author": "ghulamalchik",
              "text": "Maybe because AI has tons of photos of humans riding horses, but 0 horses riding humans. By being able to generate this it demonstrates higher and more complex understanding between things as well as abstracted concepts, like above and below.",
              "score": 104,
              "created_utc": "2026-02-10 10:39:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ldzft",
                  "author": "RIPT1D3_Z",
                  "text": "Exactly, but it's still hilarious out of context.",
                  "score": 54,
                  "created_utc": "2026-02-10 10:45:16",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4ln8q9",
                  "author": "No_Swimming6548",
                  "text": "I believe there are some content including horses riding humans. Don't ask me how I know.",
                  "score": 16,
                  "created_utc": "2026-02-10 12:02:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mjuy5",
                  "author": "vaosenny",
                  "text": ">Maybe because AI has tons of photos of humans riding horses, but 0 horses riding humans. By being able to generate this it demonstrates higher and more complex understanding between things as well as abstracted concepts, like above and below.\n\nDoes it look like riding though?\n\nhttps://preview.redd.it/d9k957lmhoig1.jpeg?width=1179&format=pjpg&auto=webp&s=3033a564c3c9ec19bb0552c5cbdc308c02a3a274",
                  "score": 9,
                  "created_utc": "2026-02-10 15:12:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ulwcu",
                  "author": "KallistiTMP",
                  "text": "Also year of the horse, and everyone releasing models before the Chinese new year shutdown.",
                  "score": 1,
                  "created_utc": "2026-02-11 19:11:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l9f8m",
              "author": "djm07231",
              "text": "Horse riding an astronaut was the infamous example cited by noted AI skeptic Gary Marcus 4 years ago to downplay the idea of AI ever managing to ‚Äúunderstand‚Äù things properly.\n\nhttps://garymarcus.substack.com/p/horse-rides-astronaut",
              "score": 94,
              "created_utc": "2026-02-10 10:02:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4laqr8",
                  "author": "-dysangel-",
                  "text": "AI skeptic, or just really trying to push the SOTA in bestiality porn?",
                  "score": 75,
                  "created_utc": "2026-02-10 10:15:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ns19p",
                  "author": "TheGoddessInari",
                  "text": "I just had to see.\n\nhttps://preview.redd.it/u3jbookxopig1.png?width=2816&format=png&auto=webp&s=4b4a9a23e1cb90abb4e3e9d6453d17190341ef01",
                  "score": 11,
                  "created_utc": "2026-02-10 18:37:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4mcd16",
                  "author": "vaosenny",
                  "text": ">Horse riding an astronaut\n\nThat doesn‚Äôt look like horse riding an astronaut though\n\nIf doesn‚Äôt even have astronaut in it\n\nhttps://preview.redd.it/xgm1lr6ghoig1.jpeg?width=1179&format=pjpg&auto=webp&s=2d74dd3b27f54f86cb0d9460297b689089d31626",
                  "score": 3,
                  "created_utc": "2026-02-10 14:34:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4mm1q0",
              "author": "postitnote",
              "text": "Mr. Hands...",
              "score": 3,
              "created_utc": "2026-02-10 15:23:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ld7d8",
              "author": "muyuu",
              "text": "they did Tom Hardy dirty",
              "score": 4,
              "created_utc": "2026-02-10 10:38:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lg89c",
              "author": "Healthy-Nebula-3603",
              "text": "I don't see any problem here üòâ \n\nA horse riding a man...",
              "score": 5,
              "created_utc": "2026-02-10 11:05:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ode13",
              "author": "thetaFAANG",
              "text": "Its a Chinese meme thats taken a life of its own",
              "score": 2,
              "created_utc": "2026-02-10 20:16:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mtirt",
              "author": "infearia",
              "text": "I'm probably waaay over-analyzing, but 2026 in the Chinese calendar will be the Year of the Horse, and the guy on his knees, exposing his backside to the horse, with his ragged clothing and a distressed facial expression, has a distinctly Western look...",
              "score": 2,
              "created_utc": "2026-02-10 15:59:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lpob0",
              "author": "Tzeig",
              "text": "He who smelt it...",
              "score": 1,
              "created_utc": "2026-02-10 12:20:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lw1y7",
              "author": "RayHell666",
              "text": "It's a classic benchmark to test model prompt adherence. They almost all fail.",
              "score": 1,
              "created_utc": "2026-02-10 13:02:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4p7988",
              "author": "Ai--Ya",
              "text": "From John Oliver's account",
              "score": 1,
              "created_utc": "2026-02-10 22:37:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4l6nqj",
          "author": "r4in311",
          "text": "I so hope this gets a release, they finally nailed natural light and weird ai faces. Huge game changer .",
          "score": 25,
          "created_utc": "2026-02-10 09:36:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l6u8q",
          "author": "Dany0",
          "text": "The \"classical\" chinese painting style generations kind of slap tbph",
          "score": 16,
          "created_utc": "2026-02-10 09:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l85km",
          "author": "Hialgo",
          "text": "I wonder if the multi language hurts the model.¬† Nearly all examples are Chinese",
          "score": 15,
          "created_utc": "2026-02-10 09:50:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8j3x",
              "author": "RIPT1D3_Z",
              "text": "It would use Qwen3-VL 8b as an encoder, so it's entirely depends on its understanding, it seems. Most likely, Chinese and English are gonna be supported the most.",
              "score": 22,
              "created_utc": "2026-02-10 09:54:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4nswmx",
              "author": "Caffdy",
              "text": "I mean, they are a chinese company, with 1.4 billion possible user base",
              "score": 6,
              "created_utc": "2026-02-10 18:41:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mgomp",
              "author": "wanderer_4004",
              "text": "Well, maybe it is time to learn Chinese...",
              "score": 7,
              "created_utc": "2026-02-10 14:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4naf4k",
                  "author": "Complainer_Official",
                  "text": "I'd start with Mandarin, Then move on to Cantonese. throw some korean and thai in there and you should be slightly functional.",
                  "score": 5,
                  "created_utc": "2026-02-10 17:17:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nbuik",
              "author": "NickCanCode",
              "text": "Their past models are already supporting Chinese. It just get more fonts and understanding on top of that.",
              "score": 1,
              "created_utc": "2026-02-10 17:23:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4leded",
          "author": "muyuu",
          "text": "> As shown, Qwen-Image-2.0 accurately renders nearly the entire Preface in small regular script, with only a handful of characters imperfect.\n\nthis is a lingering problem with image generators, that they seem to be unable to correct themselves\n\ntypically you would try everything including just cutting an area of the image and asking for fixes and they will make the same mistakes, even if they can recognise them, and the SOTA situation is have someone just fixing their output by hand\n\nmaybe there's stuff out there improving on this situation that i'm unaware of",
          "score": 5,
          "created_utc": "2026-02-10 10:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l69pk",
          "author": "NikolaTesla13",
          "text": "Where does it say it's 7b?",
          "score": 7,
          "created_utc": "2026-02-10 09:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l6lvd",
              "author": "RIPT1D3_Z",
              "text": "https://preview.redd.it/u8f0r7c40nig1.png?width=2560&format=png&auto=webp&s=e83774638ccb95f054ff440ce35bbd811ac8fc89\n\nRight here. They've shared the prompt and the image that states that it's 7B",
              "score": 58,
              "created_utc": "2026-02-10 09:35:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lbaey",
                  "author": "Formal-Exam-8767",
                  "text": "They have an office on Great Wall of China?",
                  "score": 28,
                  "created_utc": "2026-02-10 10:20:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4la7e5",
                  "author": "Mr_Frosty009",
                  "text": "That‚Äôs very nice that they put spoiler for Qwen 3.5 existence",
                  "score": 5,
                  "created_utc": "2026-02-10 10:10:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4lcg2e",
                  "author": "ReadyAndSalted",
                  "text": "That says 8b text encoder + 7b diffusion... I understand that you can switch them between vram and memory to keep vram usage down, but that does still mean model inference involves 15b parameters total, not just 7b.",
                  "score": 6,
                  "created_utc": "2026-02-10 10:31:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4m1q6s",
                  "author": "lordlestar",
                  "text": "only a machine would hand write that perfect",
                  "score": 1,
                  "created_utc": "2026-02-10 13:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4l6hoz",
              "author": "Dany0",
              "text": "In one of the image prompts, ctrl+f is your friend",
              "score": 2,
              "created_utc": "2026-02-10 09:34:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz9ke",
          "author": "Monkey_1505",
          "text": "The first round of qwen edit models had something I've never seen any other image model have - spatial reasoning. They can legit rotate the viewpoint in ways other models can't, not even the big bois. \n\nThis new model looks kind of amazing. Not ness 'better' than z-image turbo, but similar and more flexible. I'll be so disappointed if it's not open sourced.",
          "score": 3,
          "created_utc": "2026-02-10 13:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l7y0z",
          "author": "rerri",
          "text": "Are they stating anything anywhere wrt open weight release being planned or not planned?",
          "score": 9,
          "created_utc": "2026-02-10 09:48:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l9ctb",
              "author": "RIPT1D3_Z",
              "text": "Haven't seen any direct statement, but they've updated the readme in Qwen Image github announcing the model release. Also, Qwen is known as the lab that releases weights for their models, so the chances are high.\n\nIMO, no reason to state the size of the model if you're not planning to OS it anyway.",
              "score": 23,
              "created_utc": "2026-02-10 10:02:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4lclqr",
                  "author": "saltyrookieplayer",
                  "text": "I wouldn‚Äôt be so optimistic given the existence of Wan 2.6",
                  "score": 9,
                  "created_utc": "2026-02-10 10:32:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ldq9n",
          "author": "Busy-Group-3597",
          "text": "I love qwen image edit But it was too big for my cpu only generation‚Ä¶ I really appreciate this 7B model .Will test out how this performs",
          "score": 2,
          "created_utc": "2026-02-10 10:42:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4op7pf",
          "author": "Blizado",
          "text": "Only if the model is capable of doing LoRAs, then it will be interesting.",
          "score": 2,
          "created_utc": "2026-02-10 21:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ludhs",
          "author": "mikkoph",
          "text": "Chinese New Year next week. Fingers crossed they decide to drop it for the event",
          "score": 3,
          "created_utc": "2026-02-10 12:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4m4mh0",
          "author": "XiRw",
          "text": "Nice. I hope Image-Edit comes soon after",
          "score": 3,
          "created_utc": "2026-02-10 13:52:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7k11",
              "author": "RIPT1D3_Z",
              "text": "It's both text2image and img2img in one model.",
              "score": 11,
              "created_utc": "2026-02-10 14:08:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4m83ev",
                  "author": "XiRw",
                  "text": "Oh nice! Thanks for letting me know!",
                  "score": 2,
                  "created_utc": "2026-02-10 14:11:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4ncpmi",
              "author": "nmkd",
              "text": "The title literally says it does both in one.",
              "score": 3,
              "created_utc": "2026-02-10 17:27:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n363h",
          "author": "dergachoff",
          "text": "https://preview.redd.it/mw7rme2i4pig1.png?width=2688&format=png&auto=webp&s=917a60ba9ce59fc9ff4e1c534095fab649212db1\n\nit's a pity 7B is not enough for russian rendering. how are other languages?",
          "score": 3,
          "created_utc": "2026-02-10 16:43:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lshwx",
          "author": "AppealThink1733",
          "text": "Will it run on a laptop with 16GB of RAM? And when will the GGUFS be available?",
          "score": 1,
          "created_utc": "2026-02-10 12:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lsv3r",
              "author": "RIPT1D3_Z",
              "text": "There are only rumors, but some people say weights are gonna be released after the Lunar New Year. There are still a chance that the model would not be open sourced, but still, Qwen usually releases their models on GitHub and HF.",
              "score": 5,
              "created_utc": "2026-02-10 12:42:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ltnqk",
                  "author": "AppealThink1733",
                  "text": "Thank you very much for the information.",
                  "score": 1,
                  "created_utc": "2026-02-10 12:47:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mqptp",
          "author": "dampflokfreund",
          "text": "Sounds amazing. With this and upcoming Qwen 3.5, they are knocking it out of the park.¬†",
          "score": 1,
          "created_utc": "2026-02-10 15:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qckxn",
          "author": "Unable-Finish-514",
          "text": "Wow!  I just tried the new image model on Qwen Chat.  I have a fictional character based on a cartoon image I came across about a year ago of a younger guy wearing a noticeable hat.  I've always liked GTA-esque organized crime games, so he would be a character in this type of world.  This is an impressive representation of my character by the new Qwen image model.\n\nhttps://preview.redd.it/p4jc1k5y1sig1.jpeg?width=450&format=pjpg&auto=webp&s=6dbe0fcd017cf4214ff0a15da7c897c64e42a85f\n\n",
          "score": 1,
          "created_utc": "2026-02-11 02:36:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qdf2g",
              "author": "Unable-Finish-514",
              "text": "Then, I hit the make video button and had him give a flirty compliment.  This is one of my favorite video prompts to test a video model, as you can see if the model can capture the vibe of a character and if it follows you directions about speech.  My apologies, as I don't know how to link the video, but it is 5 seconds and it's the exact vibe I want from the character.  This is right on par with Grok Imagine in image to video.",
              "score": 1,
              "created_utc": "2026-02-11 02:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tlq7l",
          "author": "MedicalAd8373",
          "text": "https://preview.redd.it/u1j18vbi5wig1.png?width=1694&format=png&auto=webp&s=c6a8a7aac7bc313f921006760ee46a51c46bdbfd\n\nYeah.",
          "score": 1,
          "created_utc": "2026-02-11 16:22:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uv4qp",
              "author": "InterestingSloth5977",
              "text": "I saw her sister lying on the grass last year.",
              "score": 1,
              "created_utc": "2026-02-11 19:54:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xwsmc",
          "author": "ThisIsCodeXpert",
          "text": "Is API access available? I heard that it is invite only?",
          "score": 1,
          "created_utc": "2026-02-12 06:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lnelt",
          "author": "CattailRed",
          "text": "So... can you run a 7B image gen on CPU?",
          "score": 1,
          "created_utc": "2026-02-10 12:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lwasu",
              "author": "Serprotease",
              "text": "Yes, but you don‚Äôt want to do it.¬†\n\nI remember running sd1.5, so a 1b model, on cpu only a couple of years ago and it was a generation time in a dozen of minutes for a 512x512 image.¬†",
              "score": 8,
              "created_utc": "2026-02-10 13:04:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mseqg",
              "author": "ayu-ya",
              "text": "Technically you can, but as the other person said, it would be a miserable experience. Not that long ago Stability Matrix had some issue with SD.Next, refused to work with my GPU and I only noticed it after I started generating. Let it run out of curiosity, it was only a SDXL model with some light detailers and ended up taking around 10 minutes for a single image. It would be horrible to try to figure out what prompts work for what I want when every image takes that long",
              "score": 2,
              "created_utc": "2026-02-10 15:54:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4nc7oc",
              "author": "nmkd",
              "text": "Not when you also need an 8B text encoder alongside it",
              "score": 2,
              "created_utc": "2026-02-10 17:25:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4nl1kh",
              "author": "AbhiStack",
              "text": "If privacy is not a concern, then cloud platforms like vast ai and runpod let's you run GPU instances at a very cheap hourly rate. You can run all sorts of big and small models and then destroy the instance when you're done.",
              "score": 1,
              "created_utc": "2026-02-10 18:06:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lu6ek",
          "author": "eribob",
          "text": "Nice! Cant wait for the release of the weights",
          "score": 1,
          "created_utc": "2026-02-10 12:50:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lzznd",
          "author": "BobbingtonJJohnson",
          "text": "Look at their benchmark results. No way in hell they will release this. This is the same as it will always be.",
          "score": 1,
          "created_utc": "2026-02-10 13:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lr3gh",
          "author": "techlatest_net",
          "text": "Hell yeah, Qwen-Image-2.0 dropping at 7B is massive‚Äîfinally a lean beast that crushes gen+edit without choking my rig. V1 was solid in ComfyUI but hogged VRAM; this unified pipeline with native 2K and legit text (posters? Comics? Sign me up) feels like the local workflow upgrade we've been begging for. Fingers crossed weights hit HF soon like last time, gonna spam the demo til then!",
          "score": 0,
          "created_utc": "2026-02-10 12:30:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rekh8",
          "author": "prateek63",
          "text": "The 7B down from 20B is the real headline here. A unified gen+edit model that actually fits on consumer hardware changes the calculus for local image workflows completely.\n\n\n\nThe text rendering capability is what I'm most curious about. If it can reliably render text in generated images, that eliminates one of the most annoying limitations of local image gen ‚Äî every time you need text on an image, you're dropping into PIL/ImageMagick after generation.\n\n\n\nGiven Qwen's track record of open-weighting after initial API-only launches, I'd give it 4-6 weeks before we see Apache 2.0 weights on HuggingFace.",
          "score": 0,
          "created_utc": "2026-02-11 07:15:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4lbvu2",
          "author": "dobomex761604",
          "text": "Editing functionality in 7B would be interesting, but Qwen models were never good for txt2img. Even ignoring censorship, they are plastic and overly cinematic. Plus, ZImage and Anima have taken the txt2img space already, making this new model less interesting.",
          "score": -8,
          "created_utc": "2026-02-10 10:26:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ldn95",
              "author": "ghulamalchik",
              "text": "The more the better. Plus every new model has better technology and training techniques even if it's incremental. If people had that mindset, we'd be stuck with Stable Diffusion 1.0 by now.",
              "score": 12,
              "created_utc": "2026-02-10 10:42:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4lp5cp",
              "author": "oswaldo_fv",
              "text": "What do you mean, no? qwen-image-2512 is surprisingly good, and this new model looks even better. The best part is that it comes with 2K resolution and a unified generation model plus editing capabilities. I didn't like qwen-imagen-edit 2511 because it really lost image quality and definition when editing. Let's hope this new model doesn't.",
              "score": 5,
              "created_utc": "2026-02-10 12:16:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4m59dy",
                  "author": "dobomex761604",
                  "text": "Z-Image can do pretty much anything Qwen 2512 can, but gives less generic results more often. At it's size, 2512 is not a good choice.\n\nThe new 7B definitely looks better, but not by a lot compared to Z-Image. Like I said, editing is much more interesting here, especially since it's unified and at (relatively) small size.",
                  "score": 0,
                  "created_utc": "2026-02-10 13:55:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mwhgx",
          "author": "HatEducational9965",
          "text": "scrolled the post twice looking for a HF url. THE WEIGHTS PLEASE",
          "score": -3,
          "created_utc": "2026-02-10 16:13:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mzn19",
              "author": "RIPT1D3_Z",
              "text": "Post has to be read, not scrolled. No weights yet, unfortunately. Some people hinting it would be released after CNY.",
              "score": 8,
              "created_utc": "2026-02-10 16:27:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lug67",
          "author": "LodosDDD",
          "text": "No way they can create those images with 7B??? Models I run are trash",
          "score": -5,
          "created_utc": "2026-02-10 12:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m713m",
              "author": "COMPLOGICGADH",
              "text": "So you haven't tried new models like Klein 4b and 9b and obviously the elephant in the room zimage base and turbo which is only 6b",
              "score": 12,
              "created_utc": "2026-02-10 14:05:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mjp4c",
                  "author": "LodosDDD",
                  "text": "they can do edits?",
                  "score": 1,
                  "created_utc": "2026-02-10 15:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r14h9u",
      "title": "Train MoE models 12x faster with 30% less memory! (<15GB VRAM)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ee2jwnijvoig1.png",
      "author": "danielhanchen",
      "created_utc": "2026-02-10 15:54:02",
      "score": 411,
      "num_comments": 56,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4p9kny",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-10 22:50:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mt578",
          "author": "Round_Document6821",
          "text": "speedup speedup saving yay",
          "score": 37,
          "created_utc": "2026-02-10 15:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mugxp",
              "author": "danielhanchen",
              "text": "Haha :) Any feedback on the release would be much appreciated as well!",
              "score": 10,
              "created_utc": "2026-02-10 16:03:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oxs32",
                  "author": "SpiritualWindow3855",
                  "text": "I've been forced to live in ms-swift/Megatron land finetuning Deepseek, my kingdom for official multi-GPU support to land so I can cash in on these gains\n\nI've seen Github threads with some success with FSDP, but it all looked very \"taped together\"",
                  "score": 1,
                  "created_utc": "2026-02-10 21:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n5ldk",
          "author": "spaceman_",
          "text": "I've seen a lot of posts like this, but never looked into finetuning before.\n\n1. Do these notebooks work with ROCm and AMD cards as well?\n2. How long does finetuning a model using these notebooks take?\n3. What is the biggest model I could reasonably train or finetune on a system with 24GB VRAM + 16GB VRAM?",
          "score": 15,
          "created_utc": "2026-02-10 16:54:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nbxg3",
              "author": "danielhanchen",
              "text": "1. They should if PyTorch's torch._grouped_mm works on AMD, so most likely yes!\n2. Probably under 30 minutes!\n3. GLM Flash sadly won't fit :( gpt-oss 4bit works",
              "score": 10,
              "created_utc": "2026-02-10 17:24:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ndgry",
                  "author": "spaceman_",
                  "text": "Can I use these \"heterogenous\" cards together to fit a bigger model than I could on just the 24GB or is there no point to keeping the much slower 16GB card in the system of this?",
                  "score": 4,
                  "created_utc": "2026-02-10 17:31:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n5xi8",
          "author": "lemon07r",
          "text": "How is moe training on unsloth now? I've been scared to train anything moe cause of all the issues with stability and the router, etc. I remember a lot of times if you attempted anything like sft or dpo training you ended up degrading model intelligence. Has this gotten better, and is there a recommended way to train moe models now? Sorry if this is a loaded question ",
          "score": 9,
          "created_utc": "2026-02-10 16:56:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc78n",
              "author": "danielhanchen",
              "text": "Yes so the trick is just dont train the router - freeze it!",
              "score": 8,
              "created_utc": "2026-02-10 17:25:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nomw4",
                  "author": "lemon07r",
                  "text": "Is that all we really need to do, or is there more to it?",
                  "score": 1,
                  "created_utc": "2026-02-10 18:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n6uix",
          "author": "segmond",
          "text": "amazing stuff!  thanks to team unsloth and team huggingface.    breathing life, strength and longevity into 3090",
          "score": 9,
          "created_utc": "2026-02-10 17:00:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc30z",
              "author": "danielhanchen",
              "text": "Thank you! Definitely let me know how it goes! We haven't yet tested on RTX 3090, but we did Tesla T4 and A100, so hopefully everything works smoothly!",
              "score": 6,
              "created_utc": "2026-02-10 17:24:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mwftb",
          "author": "socamerdirmim",
          "text": "GLM 4.6-Air? You mean 4.5-Air or 4.6V?",
          "score": 6,
          "created_utc": "2026-02-10 16:12:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxd58",
              "author": "danielhanchen",
              "text": "Oh 4.5-Air typo sorry - 4.7 Flash works great though!",
              "score": 6,
              "created_utc": "2026-02-10 16:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4pqpbu",
                  "author": "socamerdirmim",
                  "text": "Thanks for the info. I was just curious, because 4.6V is a MoE vision model, something I never tried. Awesome work!",
                  "score": 1,
                  "created_utc": "2026-02-11 00:25:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mwtis",
          "author": "Educational_Rent1059",
          "text": "Awesomeness",
          "score": 2,
          "created_utc": "2026-02-10 16:14:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mxe5p",
              "author": "danielhanchen",
              "text": "Thanks!",
              "score": 2,
              "created_utc": "2026-02-10 16:17:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n0cvm",
          "author": "Pentium95",
          "text": "With this, how much VRAM does a 4BPW QLoRA SFT of stepfun-ai/Step-3.5-Flash will require?",
          "score": 2,
          "created_utc": "2026-02-10 16:30:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3oz0",
              "author": "danielhanchen",
              "text": "Hm sadly stepfun-ai/Step-3.5-Flash isn't one of the supported archs as of yet sorry :( Unsloth will still work though just be less efficient",
              "score": 3,
              "created_utc": "2026-02-10 16:46:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4n6s9v",
              "author": "etherd0t",
              "text": "Step-3.5-Flash is... \\~196B total param, so a 4-bit QLoRA VRAM i don't think it's gonna fly;  \nalso, per the thread, MoE 4-bit training isn‚Äôt well-optimized right now (unless custom-handled like their gpt-oss case), so BF16",
              "score": 3,
              "created_utc": "2026-02-10 17:00:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n3fxw",
          "author": "iamdanieljohns",
          "text": "What do you think of Mojo/Max?",
          "score": 2,
          "created_utc": "2026-02-10 16:45:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3t0w",
              "author": "danielhanchen",
              "text": "Mojo is great! However our release is mainly about mathematical optimizations, which is what compilers can't do well",
              "score": 3,
              "created_utc": "2026-02-10 16:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nl8la",
          "author": "zh4k",
          "text": "What is the current status of MLX integration? I saw a fork or something posted that didn't know what necessarily was going on",
          "score": 2,
          "created_utc": "2026-02-10 18:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt7m5",
              "author": "yoracale",
              "text": "Very well actually. We manage to optimize MLX a bit. Coming in the next few",
              "score": 3,
              "created_utc": "2026-02-11 00:40:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nnatv",
          "author": "exaknight21",
          "text": "I wish the older cheaper cards got some love. The Tesla V100, 3060s. Something actually within reach of average consumer. \n\nI love the unsloth team for the efforts.",
          "score": 2,
          "created_utc": "2026-02-10 18:16:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt45x",
              "author": "yoracale",
              "text": "It works on older GPUs actually! We made it work!!",
              "score": 3,
              "created_utc": "2026-02-11 00:39:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4pu45e",
                  "author": "exaknight21",
                  "text": "I <3 u people.",
                  "score": 2,
                  "created_utc": "2026-02-11 00:45:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4o4cx0",
          "author": "MoffKalast",
          "text": "I'm a bit out of the loop, has finetuning MoEs become viable in terms of what to freeze and whatnot? Is there an established approach for it? I still remember people having major problems doing anything at all with Mixtral.",
          "score": 2,
          "created_utc": "2026-02-10 19:34:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pt1xx",
              "author": "yoracale",
              "text": "On fine-tuning MoE's - it's probably not a good idea to fine-tune the router layer so we disabled it by default.",
              "score": 2,
              "created_utc": "2026-02-11 00:39:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pqhkp",
          "author": "silenceimpaired",
          "text": "Do you support multiple 3090‚Äôs yet? I have two.",
          "score": 2,
          "created_utc": "2026-02-11 00:24:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4psozo",
              "author": "yoracale",
              "text": "Yes Unsloth works on multiGPUs, we just haven't officially announced it yet, you can view our guide: https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth",
              "score": 4,
              "created_utc": "2026-02-11 00:37:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4q4hjr",
                  "author": "silenceimpaired",
                  "text": "Take my upvote and engagement :)",
                  "score": 3,
                  "created_utc": "2026-02-11 01:47:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4regvo",
          "author": "prateek63",
          "text": "The 12.8GB VRAM for gpt-oss-20b is genuinely impressive. That's 4090 territory ‚Äî it means hobbyists can now fine-tune MoE models that were previously enterprise-only.\n\n\n\nThe interesting implication: if consumer GPUs can fine-tune MoE architectures, we'll probably see a wave of specialized expert models for niche domains (medical, legal, code) built by small teams who couldn't afford H100 clusters.\n\n\n\nThe VRAM reduction matters way more than the speed improvement for the local community. Training 12x faster on an H100 is nice. Training \\*at all\\* on a 4090 is game-changing.",
          "score": 2,
          "created_utc": "2026-02-11 07:14:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ri8d5",
          "author": "Alarming_Bluebird648",
          "text": "Reducing the VRAM requirement below 15GB makes MoE fine-tuning actually viable for single-GPU consumer setups. Have you seen any significant difference in gradient overflow issues when using these math optimizations compared to the standard implementation?",
          "score": 2,
          "created_utc": "2026-02-11 07:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlcfc",
              "author": "yoracale",
              "text": "All our optimizations are verified by grad norms and long training runs and there is no degradation in accuracy or training loss.",
              "score": 2,
              "created_utc": "2026-02-11 08:18:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mz2ag",
          "author": "Few_Painter_5588",
          "text": "Good stuff! I was in the middle of an MoE training run right now actually, so imma have to restart that. Will you be making unsloth-bnb-4bit quants for MoE models going forward?\n\n>We hear it'll be a busy week! :)\n\nWill it be a BuZy week?üëÄ",
          "score": 1,
          "created_utc": "2026-02-10 16:25:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mzswb",
              "author": "yoracale",
              "text": "Unfortunately MoE models aren't optimized in Bnb 4bit unless it's customized by us like gpt-oss. Would recommend sticking with BF16.\n\nWe will make FP8 or 4bit ones in the future for y'all to train with",
              "score": 5,
              "created_utc": "2026-02-10 16:28:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4n62mm",
                  "author": "Few_Painter_5588",
                  "text": "All good, thanks for the heads up. FP8 and 4Bit would still be greatly appreciated. Keep up with the good work!",
                  "score": 2,
                  "created_utc": "2026-02-10 16:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4r0jtd",
              "author": "woct0rdho",
              "text": "MoE + bnb 4bit (or even GGUF less than 4bit) is supported in my repo https://github.com/woct0rdho/transformers-qwen3-moe-fused . It supports Qwen3 MoE and it should support other models with minimal modification.",
              "score": 1,
              "created_utc": "2026-02-11 05:16:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n86q5",
          "author": "FrostyDwarf24",
          "text": "MoE go brrrrrrrrrrr! ",
          "score": 1,
          "created_utc": "2026-02-10 17:06:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nc8aa",
              "author": "danielhanchen",
              "text": ":))",
              "score": 1,
              "created_utc": "2026-02-10 17:25:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o7174",
          "author": "Double_Cause4609",
          "text": "Any hope of incorporating something like RamTorch to load only a single layer of MoE weights + optimizer states + gradients to GPU at a time (offloading rest to system memory), to enable \\~100-120B MoE model training on the upper end of consumer systems?\n\nThe speed actually shouldn't be that bad with decent batch size (should be using for MoE anyway, IMO).",
          "score": 1,
          "created_utc": "2026-02-10 19:46:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rlmf4",
              "author": "yoracale",
              "text": "We have actually heard of ramtorch and it is a very good idea. Atm we dont do single offloading however we may in the future",
              "score": 2,
              "created_utc": "2026-02-11 08:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4oe1nl",
          "author": "KaroYadgar",
          "text": "I'm thinking about pre-training a tiny LLM. Is it possible to use your optimizations outside of Unsloth? And how nice is the workflow for something like pre-training as compared to transformers?",
          "score": 1,
          "created_utc": "2026-02-10 20:19:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4pswuh",
              "author": "yoracale",
              "text": "Unsloth works with pre training yes. If you want to use the optimizations outside of unsloth you need to wary of the licensing which is LGPL3 or AGPL3.",
              "score": 1,
              "created_utc": "2026-02-11 00:38:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pgotb",
          "author": "kouteiheika",
          "text": "You're comparing to \"TF v4 + FA2\" for gpt-oss-20b but Flash Attention for gpt-oss models is not a thing because FA2 doesn't support attention sinks (unless you pull in [this PR](https://github.com/Dao-AILab/flash-attention/pull/1819) and compile FA2 yourself), so what exactly are you comparing to? Is the \"+ FA2\" just a mistake (and it's just using normal eager attention), or did you compare to a patched FA2 + `transformers`?",
          "score": 1,
          "created_utc": "2026-02-10 23:29:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4pjunq",
          "author": "MaruluVR",
          "text": "Is MOE also trainable at 4bit like dense models? IE could I train Qwen3-30B with a similar memory footprint to gpt oss? (I personally am thinking about training the leaked 15B Qwen 3 for testing)\n\nHave you done any testing with finetuning pruned models?",
          "score": 1,
          "created_utc": "2026-02-10 23:47:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4psjsb",
              "author": "yoracale",
              "text": "Not at the moment (except for gpt-oss which we custom made it work) unfortunately due to BNB being unoptimized. For now it's best to use BF16. Pruned models should work",
              "score": 1,
              "created_utc": "2026-02-11 00:36:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qmu7j",
          "author": "Old-Nobody-2010",
          "text": "What is the minimum VRAM required to fine-tune GLM-4.7-Flash with Unsloth 30b a3b model",
          "score": 1,
          "created_utc": "2026-02-11 03:39:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4qs383",
              "author": "yoracale",
              "text": "20GB VRAM around",
              "score": 2,
              "created_utc": "2026-02-11 04:15:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4rc7su",
                  "author": "Old-Nobody-2010",
                  "text": "awesomeÔºÅÔºÅ",
                  "score": 1,
                  "created_utc": "2026-02-11 06:53:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rjyb5",
          "author": "BackUpBiii",
          "text": "You guys should test out my latest ide in my GitHub with your models and see how much faster being I use pure masm x64 with no deps RawrXD repo by itsmehrawrxd",
          "score": 1,
          "created_utc": "2026-02-11 08:05:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sasat",
          "author": "KT313",
          "text": "thanks for your work! :D\ndoes this support training with multi-gpu setups?¬†",
          "score": 1,
          "created_utc": "2026-02-11 12:05:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v1tue",
              "author": "yoracale",
              "text": "Yes! See our guide: [https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth](https://unsloth.ai/docs/basics/multi-gpu-training-with-unsloth)",
              "score": 2,
              "created_utc": "2026-02-11 20:27:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4w8q1z",
                  "author": "KT313",
                  "text": "<3",
                  "score": 1,
                  "created_utc": "2026-02-12 00:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4wumt4",
          "author": "FaustAg",
          "text": "Is qwen3-coder-next support coming?",
          "score": 1,
          "created_utc": "2026-02-12 02:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy0l26",
      "title": "Nemo 30B is insane. 1M+ token CTX on one 3090",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/",
      "author": "Dismal-Effect-1914",
      "created_utc": "2026-02-07 01:39:58",
      "score": 397,
      "num_comments": 107,
      "upvote_ratio": 0.98,
      "text": "Been playing around with llama.cpp and some 30-80B parameter models with CPU offloading. Currently have one 3090 and 32 GB of RAM. Im very impressed by Nemo 30B. 1M+ Token Context cache, runs on one 3090, CPU offloading for experts. Does 35 t/s which is faster than I can read at least. Usually slow as fuck at this large a context window. Feed it a whole book or research paper and its done summarizing in like a few mins. This really makes long context windows on local hardware possible. The only other contender  I have tried is Seed OSS 36b and it was much slower by about 20 tokens.",
      "is_original_content": false,
      "link_flair_text": "Generation",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4235o4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 10:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40mfn1",
          "author": "ubrtnk",
          "text": "Can you share your configuration?",
          "score": 44,
          "created_utc": "2026-02-07 02:49:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xx3p",
              "author": "Dismal-Effect-1914",
              "text": "RTX 3090  \n32GB DDR4 3200  \ni5-12600KF  \nFresh Ubuntu 24.04 server install, Cuda 13, latest drivers, bare metal.\n\nsudo nice -n -20 llama-server --hf-repo unsloth/Nemotron-3-Nano-30B-A3B-GGUF --hf-file Nemotron-3-Nano-30B-A3B-UD-Q4\\_K\\_XL.gguf --alias \"unsloth/Nemotron-3-Nano-30B-A3B\" --fit on --min\\_p 0.01 --temp 0.6 --top-p 0.95 --ctx-size 1024000 --port 8001 --jinja --host¬†[0.0.0.0](http://0.0.0.0/)¬†\\--cpu-moe --flash-attn on",
              "score": 106,
              "created_utc": "2026-02-07 04:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42vj1w",
                  "author": "metamec",
                  "text": "You could condense those two -hf params into:\n\n`-hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:UD-Q4_K_XL`",
                  "score": 20,
                  "created_utc": "2026-02-07 13:52:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o411j1r",
                  "author": "ClimateBoss",
                  "text": "nice make a difference on tk/s?",
                  "score": 17,
                  "created_utc": "2026-02-07 04:30:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41w655",
                  "author": "PooMonger20",
                  "text": "Thank you for sharing. Could you share what is your actual usecase? coding or something else?\n\n\nEdit: I just tried to make it vibe code a Tetris-like game in Python, it can't do it properly, code compiles but lots of missing parts.",
                  "score": 7,
                  "created_utc": "2026-02-07 08:55:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42txko",
                  "author": "ubrtnk",
                  "text": "Thank you. I figured out a similar llama-swap config with splitting ngl as 30 and cpu moe as 23. Got 900k context on one 3090 and 16g of vram and still got 50-60tps",
                  "score": 5,
                  "created_utc": "2026-02-07 13:43:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41yru7",
                  "author": "IrisColt",
                  "text": "I kneel... I am going to check if this works for Win11.",
                  "score": 3,
                  "created_utc": "2026-02-07 09:21:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42auc3",
                  "author": "Numerous_Mulberry514",
                  "text": "Try the iq4-nl, in my testing it had better complexity while using ~4gb less vram",
                  "score": 3,
                  "created_utc": "2026-02-07 11:19:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o42bl8k",
                  "author": "bguberfain",
                  "text": "Maybe a dumb question, but my fresh build form llama-cpp got me this error:\n`error: invalid argument: --host¬†0.0.0.0¬†--cpu-moe`\nSo no host and cpu-moe options on latest version? Or is there as different fork/branch form lllama-cpp?",
                  "score": 1,
                  "created_utc": "2026-02-07 11:26:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o419ovn",
          "author": "JoNike",
          "text": "Was curious, tried the mxfp4 version with my 5080/192gb ram and got some pretty good results\n\n - 256K context, ncmoe=20: 81.7 t/s\n - 1M context, ncmoe=27: 69.6 t/s\n\nCache-type q4_0",
          "score": 33,
          "created_utc": "2026-02-07 05:31:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41quv1",
              "author": "pmttyji",
              "text": "You have decent config so don't use q4 cache, not good for stuff like Agentic coding. At least don't use q4 for K because it's sensitive. So `-ctk q8 -ctv q4` is last decent combination.\n\nNow could you please share t/s stats with `-ctk q8 -ctv q8`  for me? With 128K & 256K context. Thanks",
              "score": 19,
              "created_utc": "2026-02-07 08:04:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43hwot",
                  "author": "JoNike",
                  "text": "So, I was actually curious about that. I'm far from an expert in local llm. I wasn't necessary thinking of using as an agent model (tho, maybe, why not!) so I was thinking the cache quant would not be as big of a deal.\n\nThat said, I did task Claude to do some tests (open claude in tmux, give it access to my llama server in a tmux pane and ask it to iterate config to measure speed and quality) for me overnight to determine if the q4 cache had an effect and the consensus was that it didn't have a noticeable impact and the vram save enabled me. However, I didn't push informatin retrieval on a giant context and that might be the part where the q4 is problematic.\n\nSummary:\n**q4_0 KV cache has zero measurable quality impact on this model.** Across 16 tests (math, code, knowledge, needle-in-a-haystack, instruction following, long-form coherence), both configs produced **identical or near-identical outputs** with matching correctness. This is expected given the architecture: only 6 out of 52 layers use KV cache.\n\n> q4_0 KV cache quantization only affects the 6 attention layers. The other 46 layers (88.5% of the model) are completely unaffected. This is fundamentally different from a pure transformer where q4_0 would affect every single layer.\n\n\nNow, is that actually true, I'll be honest, I do not know.\n\n| Metric | q4_0 KV | f16 KV (default) |\n|--------|---------|-------------------|\n| **Generation speed** | ~59.5 t/s | ~59.5 t/s |\n| **Prompt processing** | ~100-130 t/s | ~100-130 t/s |\n| **VRAM usage** | 9.0 GB | 10.1 GB |\n| **VRAM saved** | **1.1 GB** | baseline |\n\n\nI'd be curious if those results actually make sense: https://github.com/jo-nike/experiments/blob/main/nemotron-kv-cache-quality-report.md\n\nI did launch a new series of test to get a better view on the \"needle in a haystack\" test that will probe bigger context window for information retrieval.\n\nOnce those tests are done, I'll get you a t/s for the config you asked and get back to you",
                  "score": 5,
                  "created_utc": "2026-02-07 15:53:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o44yphg",
                  "author": "JoNike",
                  "text": "Needle in a haystack results for up to 256k context if that's interesting to anyone (Do note I wasn't pushing my vram to its max for those tests, it was really to test contexts so the t/s is slower): \n\n- https://github.com/jo-nike/experiments/blob/main/nemotron-big-context-needle-results.md\n\n**tl;dr**: q4_0 KV cache is recommended with Nemotron-3-Nano-30B. It delivers identical needle-retrieval accuracy to f16 while saving 1.1 GB of VRAM and providing faster prompt processing. The 6-attention-layer hybrid architecture makes this model exceptionally tolerant of aggressive KV quantization.\n\n---\n\nNow for your question, q8 for k/v cache, i was pushing it to it's max here:\n\n > - 128k: 90.9 t/s, ncmoe=12\n > - 256k: 87.7 t/s, ncmoe=15\n > - 1mil: 62.7t/s, ncmoe=30 (but that crashed on bigger context)\n\nMade me review my q4 config which gave me:\n\n > - 128k: 97.0 t/s, nccmoe=9\n > - 256k: 89.1 t/s, ncmoe=11\n > - 1mil: 64.9 t/s, ncmoe=30\n\nHere's the report if you're interested, it goes in quite bigger details such as t/s over context size: https://github.com/jo-nike/experiments/blob/main/nemotron-q8-kv-cache-results.md\n\n**Bottom line**: \n\nWith my setup, at 256k content, I'm probably better use Q8. At 128k content, I'm probably better with Q4. The difference between them is marginal.\n\n---\n\n**Testing Notes**: \n\n- That methodology of using Claude via claude code as an assistant to iterate over settings, test speed and quality and document the experimentation, was really great and a giant time saver. Claude basically created itself a script, run it, monitor it, take notes. \n\n- I was using sonnet 4.5 for this because I abused opus this week ü§¶, but I would have used Opus otherwise, a local llm could definitely work too.\n\n- Flash attention requires transient vram that grows quite a bit with 500k+ context. Caused oom, had to readjust to give more headroom to flash attention by offloading more to cpu. **Don't overly pack your vram if you intend on actually using the whole 1m context**, you'd risk oom as the context grow due to FA overhead need.\n\n- I could probably optimized the 1mil q4 config a little bit, i offloaded to give FA space and because it's so long to generate the big context, I got tired and didn't care to redo the test to squeeze every little drop of vram I could.\n\n- Generating a 950,000 tokens context takes something like 30 minutes with q8, might have been a bit fast with q4, I didn't time sadly.\n\n- t/s drop significantly once the context get filled. For example, Q4 gets to 19.1 t/s on a 1million context.\n\n- I got that Blackwell gpu recently, hadn't had much chances to play with mxfp4 before, definitely interesting! It's also the first time i try such a big context.\n\n- I probably used about 300k of claude tokens to run these optimization exercises without trying to optimize my usage.\n\nI'd be curious to hear if those reports make sense. I'm quite trusting of Claude + it's not a lot of analysis (tho there is some), it's really just running processes and then compiling results. It does look quite good to me.\n\nIf interested in my llama.cpp config, let me know if you think I can optimize that even more:\n\n---\n\n[Nemotron-3-Nano-30B-A3B-MXFP4_MOE-256K]\n\nmodel = /mnt/data/models/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf\n\nn-gpu-layers = 52\n\nn-cpu-moe = 15\n\nctx-size = 262144\n\nflash-attn = on\n\ncache-type-k = q8_0\n\ncache-type-v = q8_0\n\nparallel = 1\n\nthreads = 16\n\nmlock = 1\n\n---\n\n[Nemotron-3-Nano-30B-A3B-MXFP4_MOE-1M]\n\nmodel = /mnt/data/models/NVIDIA-Nemotron-3-Nano-30B-A3B-MXFP4_MOE.gguf\n\nn-gpu-layers = 52\n\nn-cpu-moe = 30\n\nctx-size = 1048576\n\nflash-attn = on\n\ncache-type-k = q4_0\n\ncache-type-v = q4_0\n\nparallel = 1\n\nthreads = 16\n\nmlock = 1",
                  "score": 6,
                  "created_utc": "2026-02-07 20:17:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o41gj4i",
              "author": "Dismal-Effect-1914",
              "text": "Damn! Thats real good!",
              "score": 2,
              "created_utc": "2026-02-07 06:28:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40dm0f",
          "author": "Sergiowild",
          "text": "35 t/s at that context length is wild. curious if you've tested accuracy degradation toward the end of the context window? some models claim big ctx but start hallucinating or losing track of earlier content past a certain point.",
          "score": 45,
          "created_utc": "2026-02-07 01:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40f8u6",
              "author": "Dismal-Effect-1914",
              "text": "I havent tried a task long enough yet, But ive seen it chomping through over 100k+ tokens so far and its still just as coherent in its output as the initial prompt, after multiple rounds of questioning. My next test will be feeding it The Decline and Fall of the Roman Empire and asking it a very obscure question lol. Though ive always had to use RAG for this length.",
              "score": 23,
              "created_utc": "2026-02-07 02:04:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41ie0e",
                  "author": "Eisenstein",
                  "text": "If it saves you any effort, I made a tool that will throw arbitrarily long documents at an LLM and have it continue to write, and will provide a quantified analysis of the output.\n\n* https://github.com/jabberjabberjabber/Context-Tester",
                  "score": 28,
                  "created_utc": "2026-02-07 06:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o417qld",
              "author": "nufeen",
              "text": "Regarding Nemotron 3 Nano 30b, I don't bellieve in this big context window support. I tried loading books into context and asking it to search for a phrase similar to the one I specified and retrieve the fragment of the book with it. There was nothing like that in the books (in the exact words, because similar ideas could have been there), but the model literally made up fragments of text, reporting that it had found a quote in the text that sounded exactly like what I had asked it to find. And that was within 200k of context. Q8 quant. I tried lowering the temperature, but it didn't help. I even tried the same model on Openrouter with Nvidia as provider, thinking that maybe something is wrong in my setup. But with the same books and prompt the model hallucinated again.",
              "score": 14,
              "created_utc": "2026-02-07 05:16:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41yaax",
                  "author": "Chromix_",
                  "text": "Same finding here in different long-context tests that I made. Qwen3 Next and a few other models were better at it. Nemo Nano sometimes didn't find the facts I asked for, came up with hallucinated facts, and even didn't find it when providing an exact quote. If felt like the attention mechanism was unable to attend to some (interleaved) tokens.",
                  "score": 11,
                  "created_utc": "2026-02-07 09:16:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494hh9",
                  "author": "Ok_Warning2146",
                  "text": "How abt kimi linear's long context performance?¬†",
                  "score": 1,
                  "created_utc": "2026-02-08 14:03:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41vvzm",
                  "author": "Brilliant_Bobcat_209",
                  "text": "I‚Äôm new to this so take it with a pinch of salt, but I guess handling (not crashing) large context windows is different from maintaining attention/quality/accuracy.  In general I‚Äôd always prefer to chunk context up. Your use case sounds more like a retrieval/indexing issue, than a model issue?",
                  "score": 1,
                  "created_utc": "2026-02-07 08:52:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o40hbyz",
              "author": "Fit-Produce420",
              "text": "Every current model degrades near the end of it's context, some start getting gnarly at just half context.¬†",
              "score": 12,
              "created_utc": "2026-02-07 02:17:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41vu9v",
                  "author": "No-Detective-5352",
                  "text": "Remember that this is a Mamba-2 based model, so it might not have the same characteristics as pure transformer architectures, especially regarding retaining memory over longer contexts.\n\nEdit: There you go:  https://build.nvidia.com/nvidia/nemotron-3-nano-30b-a3b/modelcard",
                  "score": 12,
                  "created_utc": "2026-02-07 08:52:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41qe59",
                  "author": "florinandrei",
                  "text": "Citation needed.",
                  "score": -5,
                  "created_utc": "2026-02-07 07:59:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4186t9",
              "author": "mxforest",
              "text": "Not sure how true it is but there was a guy on this sub who mentioned that Nemotron 3 nano became really unreliable after 16k context in his testing. Not sure what kind of tests he was running. I have asked fair complex questions requiring a lot of thinking and it gave me right answers after consuming 30-50k thinking tokens.",
              "score": 3,
              "created_utc": "2026-02-07 05:19:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o43jfqh",
              "author": "YehowaH",
              "text": "Because most models use technique like yarn or similar to achieve long context sizes. Just check out Nvidia to get the real context size without degeneration: [Ruler](https://github.com/NVIDIA/RULER)",
              "score": 2,
              "created_utc": "2026-02-07 16:00:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o40wiby",
          "author": "dsartori",
          "text": "This model has incredible performance on my AMD hardware also. Far faster than anything else in the 30b class. It‚Äôs great for speed but I find the quality falls a bit short of its peers in my specific use case.",
          "score": 6,
          "created_utc": "2026-02-07 03:54:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40o1gw",
          "author": "TopTippityTop",
          "text": "Can you share your configuration/workflow, how you've set it up to run it that well?",
          "score": 5,
          "created_utc": "2026-02-07 02:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40ohgr",
              "author": "Dismal-Effect-1914",
              "text": "sudo nice -n -20 llama-server     --hf-repo unsloth/Nemotron-3-Nano-30B-A3B-GGUF --hf-file Nemotron-3-Nano-30B-A3B-UD-Q4\\_K\\_XL.gguf     --alias \"unsloth/Nemotron-3-Nano-30B-A3B\"     --fit on     --min\\_p 0.01     --temp 0.6     --top-p 0.95     --ctx-size 1024000     --port 8001     --jinja --host [0.0.0.0](http://0.0.0.0) \\--cpu-moe --flash-attn on",
              "score": 15,
              "created_utc": "2026-02-07 03:02:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40oyjy",
                  "author": "TopTippityTop",
                  "text": "Thank you!",
                  "score": 3,
                  "created_utc": "2026-02-07 03:05:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40efv3",
          "author": "ruibranco",
          "text": "35 t/s at 1M context on a single 3090 is genuinely impressive. The MoE architecture really pays off here since you only need to load the active experts into VRAM while the rest stays in system RAM. CPU offloading for the inactive experts is the key trick that makes this viable on consumer hardware. Have you tried it with any retrieval-heavy tasks where it needs to reference specific details from early in the context, not just summarization. That's usually where the long context models start to show their limits even if the raw speed holds up.",
          "score": 13,
          "created_utc": "2026-02-07 01:59:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40wc9z",
              "author": "Dismal-Effect-1914",
              "text": "Yeah the MoE arch is really what makes this possible. Nothing super heavy yet. ",
              "score": 3,
              "created_utc": "2026-02-07 03:53:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o43fb7s",
              "author": "themixtergames",
              "text": "What‚Äôs the stack for your Reddit account?",
              "score": -1,
              "created_utc": "2026-02-07 15:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43fm8p",
                  "author": "ruibranco",
                  "text": "What you mean?",
                  "score": 1,
                  "created_utc": "2026-02-07 15:41:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o416k32",
          "author": "generousone",
          "text": "What's the vram size of Nemo 30b when it loads in with that context?",
          "score": 3,
          "created_utc": "2026-02-07 05:07:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40ohsz",
          "author": "EbbNorth7735",
          "text": "What are you using to run it and parse the documents?",
          "score": 3,
          "created_utc": "2026-02-07 03:02:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40y4w1",
              "author": "Dismal-Effect-1914",
              "text": "Open-webui",
              "score": 3,
              "created_utc": "2026-02-07 04:06:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o46jiql",
                  "author": "nullnuller",
                  "text": "Could it be attributed to open-webui chunking your long context document? Anyway to verify that you are passing the whole context to the LLM?",
                  "score": 1,
                  "created_utc": "2026-02-08 01:46:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o41ymxp",
          "author": "rexmontZA",
          "text": "What would I need to change in the parameters to be able to run this as best as possible on a 5070 Ti? I have 32GB DDR4 memory and 5800X3D.",
          "score": 3,
          "created_utc": "2026-02-07 09:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40islf",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 4,
          "created_utc": "2026-02-07 02:26:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40x4ft",
              "author": "Dismal-Effect-1914",
              "text": "Q4\\_K\\_M - Im not using the standard ngl flag, --fit on, it seems to more optimally load the model without having to manually specify. --cpu-moe is key though. I believe the extra room for the KV cache on card due to MoE offloading is what makes this possible.",
              "score": 5,
              "created_utc": "2026-02-07 03:59:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o40zele",
                  "author": "FluoroquinolonesKill",
                  "text": "Do you really need --cpu-moe with --fit? I thought --fit would handle all that.\n\nEdit: Tried with and without --cpu-moe. No difference observed.",
                  "score": 2,
                  "created_utc": "2026-02-07 04:15:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o41e4j6",
          "author": "Divergence1900",
          "text": "what‚Äôs your ttft?",
          "score": 2,
          "created_utc": "2026-02-07 06:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41tft7",
          "author": "AdOrnery4151",
          "text": "1M context at 35 t/s on a single 3090 is honestly wild",
          "score": 2,
          "created_utc": "2026-02-07 08:29:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ue60",
          "author": "__Maximum__",
          "text": "Yes, the technology is wild. I hope other labs work on this, it can go a long way.",
          "score": 2,
          "created_utc": "2026-02-07 08:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o423zyf",
          "author": "dreyybaba",
          "text": "How good is it for coding?",
          "score": 2,
          "created_utc": "2026-02-07 10:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43rrjj",
              "author": "Dismal-Effect-1914",
              "text": "It was able to whip up a decent website but I would not expect it to be great at coding. It does beat Qwen 30B in benchmarks though. For my purposes its more of a general use model.",
              "score": 1,
              "created_utc": "2026-02-07 16:41:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44gfps",
                  "author": "ScoreUnique",
                  "text": "What's your preferred agent?",
                  "score": 1,
                  "created_utc": "2026-02-07 18:42:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o410ezw",
          "author": "bigh-aus",
          "text": "Does it do tool calls?",
          "score": 1,
          "created_utc": "2026-02-07 04:22:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42yf6b",
              "author": "samplebitch",
              "text": "Supposedly it is trained for tool calling and reasoning. (Looking at it in LM Studio at the moment)",
              "score": 2,
              "created_utc": "2026-02-07 14:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o41y0vq",
          "author": "legit_split_",
          "text": "Kimi Linear might be another contender",
          "score": 1,
          "created_utc": "2026-02-07 09:13:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42fxlj",
              "author": "DOAMOD",
              "text": "With the current optimizations no, Nemo is x3 faster or more, Linear 90tgs vs 260...",
              "score": 1,
              "created_utc": "2026-02-07 12:04:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o46st5j",
              "author": "zoyer2",
              "text": "for coding it wasn't that great, a bit better but nothing crazy",
              "score": 1,
              "created_utc": "2026-02-08 02:44:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o433ols",
          "author": "ahtolllka",
          "text": "I guess it is insanely quanted as 30B in fp8 is 30GB VRAM for weights only",
          "score": 1,
          "created_utc": "2026-02-07 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o435gd9",
          "author": "tronathan",
          "text": "I can see this being great to keep around as a sort of \"context manager\" to avoid compaction, maybe.  I see you're running at Q4 - I guess I'm about to answer my own question - Have you tried coding with this? No, of course you havent, cause it's not a coding model. ",
          "score": 1,
          "created_utc": "2026-02-07 14:49:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45p10m",
          "author": "vogelvogelvogelvogel",
          "text": "may i ask how you found the quality of the summary and rough area/topic of the paper.. since i had not so great results in no matter which model..",
          "score": 1,
          "created_utc": "2026-02-07 22:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o465rmo",
          "author": "Ok_Warning2146",
          "text": "How come it is ranked very low at #157 in lmarena? Does it have a good ranking in any long context bench?",
          "score": 1,
          "created_utc": "2026-02-08 00:21:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46xbmu",
              "author": "Dismal-Effect-1914",
              "text": "I dont go by LMArena, afaik those are just user rankings. There are actual benchmarks on the HF model card for long context reasoning that indicate it is competent there.",
              "score": 1,
              "created_utc": "2026-02-08 03:13:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o475fjg",
                  "author": "Ok_Warning2146",
                  "text": "Well, I think it is better to trust benchmark run by third parties than the numbers posted by nvidia  themselves.",
                  "score": 1,
                  "created_utc": "2026-02-08 04:07:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48vpvo",
          "author": "Parking-Bonus-5039",
          "text": "What about vllm?",
          "score": 1,
          "created_utc": "2026-02-08 13:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a39ka",
          "author": "EdenistTech",
          "text": "Great tip, thanks. I am getting 120 t/s on a 5070Ti/5060Ti setup using an mxfp4 version and 900K context. That Blackwell FP4 support is paying off, I guess.",
          "score": 1,
          "created_utc": "2026-02-08 17:04:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4arkno",
              "author": "Dismal-Effect-1914",
              "text": "Could you share the HF link? id like to try this as well!",
              "score": 1,
              "created_utc": "2026-02-08 18:58:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4s3xt0",
                  "author": "EdenistTech",
                  "text": "Sure, no problem: https://huggingface.co/noctrex/Nemotron-3-Nano-30B-A3B-MXFP4_MOE-GGUF. However, I‚Äôm not sure there will be be as much benefit on a 3090, since it AFAIK doesn‚Äôt have native FP4 support.",
                  "score": 2,
                  "created_utc": "2026-02-11 11:10:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4e5r2l",
          "author": "FPham",
          "text": "I have to check. Most small models when use 1M tokens are very blurry in the middle, kinda know the beginning and end and have feeling there was middle, but can't really remember for sure.",
          "score": 1,
          "created_utc": "2026-02-09 06:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uhe3e",
          "author": "NoobMLDude",
          "text": "Speed is expected since it is a Mamba + Attention hybrid model.\nHow‚Äôs the quality of generation tho?",
          "score": 1,
          "created_utc": "2026-02-11 18:50:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41sb9q",
          "author": "Jero9871",
          "text": "Does it run good in ollama? Will test it out later on a 4090, not sure which quants to use.",
          "score": 1,
          "created_utc": "2026-02-07 08:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o423rx1",
          "author": "tungd",
          "text": "It‚Äôs a hybrid model, not full transformers. You can get the same effect with a similar hybrid model such as IBM Granite.",
          "score": 1,
          "created_utc": "2026-02-07 10:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42voqu",
          "author": "silenceimpaired",
          "text": "That isn't the dense model right... it's MoE? I will assume so since no link was provided to the model. If I'm right, I'm not surprised with the results. Happy for you, but not surprised.",
          "score": 1,
          "created_utc": "2026-02-07 13:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40w3zv",
          "author": "Deep_Traffic_7873",
          "text": "Yes it is also the best model to run openclaw",
          "score": -3,
          "created_utc": "2026-02-07 03:52:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43rhgb",
              "author": "Dismal-Effect-1914",
              "text": "This was actually one of the use cases im exploring it for. I kept running into context length errors trying to run openclaw with GLM 4.7, and im not paying for another bigger cloud model. So... run my own local model at 1M context, and try to get some decent intelligence in there.",
              "score": 2,
              "created_utc": "2026-02-07 16:40:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o43t76m",
                  "author": "dstoro",
                  "text": "I am also currently looking into this - have you decided for a model yet?\n\nAlso: If I understood correctly, there are problems with local LLMs and tool-calling. There is a [merge request](https://github.com/openclaw/openclaw/pull/9339) with a fix - but it's not merged yet.",
                  "score": 1,
                  "created_utc": "2026-02-07 16:48:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qz5uww",
      "title": "Qwen3 Coder Next as first \"usable\" coding model < 60 GB for me",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/",
      "author": "Chromix_",
      "created_utc": "2026-02-08 10:43:59",
      "score": 361,
      "num_comments": 191,
      "upvote_ratio": 0.98,
      "text": "I've tried lots of \"small\" models < 60 GB in the past. GLM 4.5 Air, GLM 4.7 Flash, GPT OSS 20B and 120B, Magistral, Devstral, Apriel Thinker, previous Qwen coders, Seed OSS, QwQ, DeepCoder, DeepSeekCoder, etc. So what's different with Qwen3 Coder Next in OpenCode or in Roo Code with VSCodium?\n\n* **Speed**: The reasoning models would often yet not always produce rather good results. However, now and then they'd enter reasoning loops despite correct sampling settings, leading to no results at all in a large over-night run. Aside from that the sometimes extensive reasoning takes quite some time for the multiple steps that OpenCode or Roo would induce, slowing down interactive work *a lot*. Q3CN on the other hand is an instruct MoE model, doesn't have internal thinking loops and is relatively quick at generating tokens.\n* **Quality**: Other models occasionally botched the tool calls of the harness. This one seems to work reliably. Also I finally have the impression that this can handle a moderately complex codebase with a custom client & server, different programming languages, protobuf, and some quirks. It provided good answers to extreme multi-hop questions and made reliable full-stack changes. Well, almost. On Roo Code it was sometimes a bit lazy and needed a reminder to really go deep to achieve correct results. Other models often got lost.\n* **Context size**: Coding on larger projects needs context. Most models with standard attention eat all your VRAM for breakfast. With Q3CN having 100k+ context is easy. A few other models also supported that already, yet there were drawbacks in the first two mentioned points.\n\nI run the model this way:  \n`set GGML_CUDA_GRAPH_OPT=1`\n\n`llama-server -m Qwen3-Coder-Next-UD-Q4_K_XL.gguf -ngl 99 -fa on -c 120000 --n-cpu-moe 29 --temp 0 --cache-ram 0`\n\nThis works well with 24 GB VRAM and 64 GB system RAM when there's (almost) nothing else on the GPU. Yields about 180 TPS prompt processing and 30 TPS generation speed for me.\n\n* `temp 0`? Yes, works well for instruct for me, no higher-temp \"creativity\" needed. Prevents the *very occasional* issue that it outputs an unlikely (and incorrect) token when coding.\n* `cache-ram 0`? The cache was supposed to be fast (30 ms), but I saw 3 second query/update times after each request. So I didn't investigate further and disabled it, as it's only one long conversation history in a single slot anyway.\n* `GGML_CUDA_GRAPH_OPT`? Experimental option to get more TPS. Usually works, yet breaks processing with some models.\n\n**OpenCode vs. Roo Code**:\n\nBoth solved things with the model, yet with OpenCode I've seen slightly more correct answers and solutions. But: Roo asks *by default* about every single thing, even harmless things like running a syntax check via command line. This can be configured with an easy permission list to not stop the automated flow that often. OpenCode on the other hand just permits everything by default in code mode. One time it encountered an issue, uninstalled and reinstalled packages in an attempt of solving it, removed files and drove itself into a corner by breaking the dev environment. Too autonomous in trying to \"get things done\", which doesn't work well on bleeding edge stuff that's not in the training set. Permissions can of course also be configured, but the default is \"YOLO\".\n\nAside from that: Despite running with only a locally hosted model, and having disabled update checks and news downloads, OpenCode (Desktop version) tries to contact a whole lot of IPs on start-up.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qz5uww/qwen3_coder_next_as_first_usable_coding_model_60/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o4bx0qi",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-08 22:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48mli1",
          "author": "andrewmobbs",
          "text": "I've also found Qwen3-Coder-Next to be incredible, replacing gpt-oss-120b as my standard local coding model (on a 16GB VRAM, 64GB DDR5 system).\n\nI found it worth the VRAM to increase \\`--ubatch-size\\` and \\`--batch-size\\` to 4096, which tripled prompt processing speed. Without that, the prompt processing was dominating query time for any agentic coding where the agents were dragging in large amounts of context. Having to offload another layer or two to system RAM didn't seem to hurt the eval performance nearly as much as that helped the processing.\n\nI'm using the IQ4\\_NL quant - tried the MXFP4 too, but IQ4\\_NL seemed slightly better. I am seeing very occasional breakdowns and failures of tool calling, but it mostly works.",
          "score": 51,
          "created_utc": "2026-02-08 11:56:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48pwdi",
              "author": "Chromix_",
              "text": "Setting it that high gives me 2.5x more prompt processing speed, that's quite a lot. Yet the usage was mostly dominated by inference time for me, and this drops it to 75% due to less offloaded layers. With batch 2048 it's still 83% and 2x more PP speed. Context compaction speed is notably impacted by inference time (generating 20k tokens), so I prefer having as much of the model as possible on the GPU, as my usage is rarely impacted by having to re-process lots of data.",
              "score": 6,
              "created_utc": "2026-02-08 12:23:47",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fzvfc",
              "author": "genpfault",
              "text": "> IQ4_NL seemed slightly better\n\nIn the tok/s sense, or quality-of-output sense?",
              "score": 2,
              "created_utc": "2026-02-09 15:14:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4ny0ze",
              "author": "spadak",
              "text": "how did you set it up ? I was under impression that you need much more of VRAM, I have rtx 5070 ti and 96GB of DDR5 and would love to be able to use it locally, I'm on windows",
              "score": 2,
              "created_utc": "2026-02-10 19:05:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4o63wt",
                  "author": "JustSayin_thatuknow",
                  "text": "Try linux my friend! üôèüèª",
                  "score": 0,
                  "created_utc": "2026-02-10 19:42:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o490ujj",
              "author": "-dysangel-",
              "text": "thanks for that - I remember playing around with these values a long time ago and seeing they didn't improve inference speed - but didn't realise they could make such a dramatic difference to prompt processing. That is a very big deal",
              "score": 2,
              "created_utc": "2026-02-08 13:40:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48l0wx",
          "author": "SatoshiNotMe",
          "text": "It‚Äôs also usable in Claude Code via llama-server, set up instructions here:\n\nhttps://github.com/pchalasani/claude-code-tools/blob/main/docs/local-llm-setup.md\n\nOn my M1 Max MacBook 64 GB I get a decent 20 tok/s generation speed and around 180 tok/s prompt processing",
          "score": 45,
          "created_utc": "2026-02-08 11:42:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48xzfl",
              "author": "wanderer_4004",
              "text": "Same hardware here - M1 Max MacBook 64 GB. With MLX I get 41 tok/s TG and 360 tok/s PP. However, MLX server is less good than llama.cpp in kv-caching and especially branching. Also occasionally seems to leak memory. Am using Qwen Code and am quite happy with it. Either way, Qwen Coder Next is definitely a pretty useful model and a lot stronger than the 30B versions.",
              "score": 8,
              "created_utc": "2026-02-08 13:22:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49wzi5",
                  "author": "txgsync",
                  "text": "Yep this behavior led me to write my own implementation of a MLX server with ‚Äúslots‚Äù like llama.cpp has so more than one thing can happen at a time. FLOPS/byte goes up!\n\nInferencer and LM Studio both now support this too. If you use Gas Town for parallel agentic coding this dramatically speeds things up for your Polecats. Qwen3-Coder-Next is promising on Mac with parallel agentic harnesses. But I have to test it a bit harder.",
                  "score": 3,
                  "created_utc": "2026-02-08 16:33:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4dzxnf",
                  "author": "Consumerbot37427",
                  "text": "Also on Apple Silicon w/ Max here. I have had lots of issues with MLX, I might stop bothering with them and just stick with GGUFs. Waiting for prefill is so frustrating, and seeing log messages about \"failed to trim x tokens, clearing cache instead\" drove me nuts.\n\nI had been doing successful coding with Mistral Vibe/Devstral Small, but the context management issue plus the release of Qwen3 Coder Next inspired me to try out Claude Code with LM Studio serving the Anthropic API, and it seems amazing! It seems to be much better at caching prefill and managing context, so not only do I get more tokens per second from a MoE model, the biggest bonus is how much less time is spent waiting for the context/prefill. Loving it!",
                  "score": 1,
                  "created_utc": "2026-02-09 05:47:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48rihj",
              "author": "Chromix_",
              "text": "Claude Code uses a whole lot of tokens for the system prompt though, before any code is processed at all. OpenCode and Roo used less last time I checked. Still, maybe the results are better? I haven't tested Claude CLI with local models so far.",
              "score": 13,
              "created_utc": "2026-02-08 12:36:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48u02l",
                  "author": "SatoshiNotMe",
                  "text": "Yes CC has a sys prompt of at least 20K tokens. On my M1 Max MacBook the only interesting LLMs with good-enough generation speed are the Qwen variants such as 30B-A3B and the new coder-next. GLM-4.7-flash has been bad at around 10 tok/s.",
                  "score": 6,
                  "created_utc": "2026-02-08 12:55:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494045",
                  "author": "msrdatha",
                  "text": "Initially I was testing both CC and opencode, but then Claude started the drama like limiting other agents and tools on api usage etc. This made me think, may be CC will not be good for local ai, the moment they feel its gaining traction and we would be suddenly banned with some artificially introduced limitations. So left cc for good and continued with opencode and kilo",
                  "score": 4,
                  "created_utc": "2026-02-08 14:00:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ai5gt",
                  "author": "Purple-Programmer-7",
                  "text": "Opencode > Claude code. It‚Äôs okay that people don‚Äôt listen though üòÇ",
                  "score": 4,
                  "created_utc": "2026-02-08 18:15:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4asmlq",
                  "author": "cleverusernametry",
                  "text": "Roo has a very large system prompt as well no? I'm guesing opencode is the same deal",
                  "score": 2,
                  "created_utc": "2026-02-08 19:03:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49fs2h",
              "author": "XiRw",
              "text": "Why don‚Äôt you use their website at this point if you are going non local with Claude instead of tunneling through an API?",
              "score": 1,
              "created_utc": "2026-02-08 15:07:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49x2tt",
                  "author": "SatoshiNotMe",
                  "text": "I mainly wanted to use the 30B local models for sensitive document work, so can‚Äôt use an API, and needed it to run on my Mac. I really wouldn‚Äôt use 30B models for serious coding; for that I just use my Max sub.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:34:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49hqub",
          "author": "Terminator857",
          "text": "Failed for me on a simple test.  Asked to list recent files in directory tree.  Worked.  Then asked to show dates and human readable file sizes.  Went into a loop.  Opencode q8.  Latest build of llama-server.  strix-halo.\n\nSecond attempt, I asked gemini for recommend command line parameters for llama-server.  It gave me: llama-server ¬†¬†-m /home/dcar/llms/qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00001-of-00002.gguf ¬†¬†¬†-ngl 999¬†-c 131072 ¬†¬†-fa on ¬†¬†-ctk q8\\_0 ¬†¬†¬†¬†-ctv q8\\_0 --no-mmap  \n  \nI tried again and didn't get a loop but didn't get a very good answer: find . -type f -printf '%TY-%Tm-%Td %TH:%TM:%TS %s %p\\\\n' | sort -t' ' -k1,2 -rn | head -20 | awk 'NR>1{$3=sprintf(\"%0.2fM\", $3/1048576)}1'\n\nResult for my directory tree: \n\n2026-02-03 14:36:30.4211214270 35033623392 ./qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00002-of-00002.gguf  \n2026-02-03 14:27:21.1727458690 47472.42M ./qwen3/Coder-next/Qwen3-Coder-Next-Q8\\_0-00001-of-00002.gguf",
          "score": 6,
          "created_utc": "2026-02-08 15:17:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49odj4",
              "author": "Chromix_",
              "text": "That was surprisingly interesting. When testing with Roo it listed the files right away, same as in your test with OpenCode. Then after asking about dates & sizes it started asking me back, not just once like it sometimes does, but forever in a loop. Powershell or cmd, how to format the output, exclude .git, only files or also directories, what date format, what size format, sort order, hidden files, and then it kept going into a loop asking about individual directories again and again. That indeed seems to be broken for some reason.",
              "score": 6,
              "created_utc": "2026-02-08 15:51:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4nl8o2",
                  "author": "CSEliot",
                  "text": "Perhaps when it comes to meta data, thats wheren the issue is? Is this on linux or windows? ",
                  "score": 1,
                  "created_utc": "2026-02-10 18:07:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49yhuo",
          "author": "TBG______",
          "text": "**I tested: llama.cpp +** Qwen3-Coder-Next-MXFP4\\_MOE.gguf **on RTX 5090 ‚Äì Three Setups Compared**\n\nSetup 1 ‚Äì Full GPU Layers (VRAM-heavy)  \nVRAM Usage: \\~29 GB dedicated  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 28 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (65k token prompt):  \nPrompt eval: 381 tokens/sec  \nGeneration: 8.1 tokens/sec  \nNote: Generation becomes CPU-bound due to partial offload; high VRAM but slower output.\n\nSetup 2 ‚Äì CPU Expert Offload (VRAM-light)  \nVRAM Usage: \\~8 GB dedicated  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" -ot \".ffn\\_.\\*\\_exps.=CPU\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (70k token prompt):  \nPrompt eval: 60-140 tokens/sec (varies by cache hit)  \nGeneration: 20-21 tokens/sec  \nNote: Keeps attention on GPU, moves heavy MoE experts to CPU; fits on smaller VRAM but generation still partially CPU-limited.\n\nSetup 3 ‚Äì Balanced MoE Offload (Sweet Spot)  \nVRAM Usage: \\~27.6 GB dedicated (leaves \\~5 GB headroom)  \nCommand: A:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 131072 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1  \nSpeed (95k token prompt):  \nPrompt eval: 105-108 tokens/sec  \nGeneration: 23-24 tokens/sec  \nNote: First 24 layers' experts on CPU, rest on GPU. Best balance of VRAM usage and speed; \\~3x faster generation than Setup 1 while using similar total VRAM.\n\nSetup 4 ‚Äì Balanced MoE Offload  + full ctx size  \nVRAM Usage: \\~30.9 GB dedicated (leaves \\~1.1 GB headroom)  \nCommand: $env:GGML\\_CUDA\\_GRAPH\\_OPT=1\n\nA:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 262144 --batch-size 1024 --threads 32 --threads-batch 32 --parallel 1\n\nSpeed (95k token prompt):  \nPrompt eval: 105-108 tokens/sec  \nGeneration: 23-24 tokens/sec\n\nRecommendation: Use Setup 3 for Claude Code with large contexts. It maximizes GPU utilization without spilling, maintains fast prompt caching, and delivers the highest sustained generation tokens per second.\n\nAny ideas to speed it up ?",
          "score": 6,
          "created_utc": "2026-02-08 16:41:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49zs2z",
              "author": "Chromix_",
              "text": "With so much VRAM left on setup 3 you can bump the batch and ubatch size to 4096 as another commenter [suggested](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o48mli1/). That should bring your prompt processing speed to roughly that of setup 1.",
              "score": 2,
              "created_utc": "2026-02-08 16:47:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4abj0m",
                  "author": "TBG______",
                  "text": "Thanks: i needed a bit more ctx sizs so i did: $env:GGML\\_CUDA\\_GRAPH\\_OPT=1\n\nA:\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-server.exe --model \"A:\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-MXFP4\\_MOE.gguf\" --host [0.0.0.0](http://0.0.0.0) \\--port 8080 --alias \"Qwen3-Coder-Next\" --seed 3407 --temp 0.8 --top-p 0.95 --min-p 0.01 --top-k 40 --jinja --n-gpu-layers 999 --n-cpu-moe 24 --ctx-size 180224 --batch-size 4096 --ubatch-size 2048 --threads 32 --threads-batch 32 --parallel 1\n\nSpeed (145k token prompt):  \nPrompt eval: 927 tokens/sec  \nGeneration: 23 tokens/sec\n\nInteractive speed (cached, 200‚Äì300 new tokens):  \nPrompt eval: 125‚Äì185 tokens/sec  \nGeneration: 23‚Äì24 tokens/sec\n\ncalling from Claude Code",
                  "score": 2,
                  "created_utc": "2026-02-08 17:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48n72h",
          "author": "dubesor86",
          "text": "played around with it a bit, very flakey json, forgetful to include mandatory keys and very verbose, akin to a thinker without explicit reasoning field.",
          "score": 3,
          "created_utc": "2026-02-08 12:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48q8jo",
              "author": "Chromix_",
              "text": "Verbose in code or in user-facing output? The latter seemed rather compact for me during the individual steps, with the regular 4 paragraph conclusion at the end of a task. Maybe temperature 0 has something to do with that.",
              "score": 2,
              "created_utc": "2026-02-08 12:26:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o494rp9",
          "author": "Danmoreng",
          "text": "Did you try the fit and fit-ctx parameters instead of ngl and n-cpu-moe ? Just read the other benchmark thread (https://www.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps\\_fit\\_can\\_give\\_major\\_speedups\\_over\\_ot\\_for/) and tested on my hardware, it gives better speed.",
          "score": 4,
          "created_utc": "2026-02-08 14:04:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o499myz",
              "author": "Chromix_",
              "text": "Yes, tried that (and even [commented](https://www.reddit.com/r/LocalLLaMA/comments/1qyynyw/comment/o488gm0/) how to squeeze more performance out of it) but it's not faster for me, usually a bit slower.",
              "score": 5,
              "created_utc": "2026-02-08 14:33:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fxa4h",
              "author": "tmflynnt",
              "text": "FYI that I added [an update](https://www.reddit.com/r/LocalLLaMA/comments/1qyynyw/comment/o4dvmfu/) to that thread with additional gains based on people's comments.",
              "score": 2,
              "created_utc": "2026-02-09 15:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49prcs",
          "author": "StardockEngineer",
          "text": "Install oh my OpenCode into OpenCode to get the Q&A part of planning as you‚Äôve described in Roo Code.  Also provides Claude Code compatibility for skills, agents and hooks.",
          "score": 4,
          "created_utc": "2026-02-08 15:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49vjx7",
              "author": "Chromix_",
              "text": ">99% of this project was built using OpenCode. I tested for functionality‚ÄîI don't really know how to write proper TypeScript.\n\nA vibe-coded vibe-coding tool plug-in? I'll give it a look.",
              "score": 2,
              "created_utc": "2026-02-08 16:26:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o49ybgd",
                  "author": "txgsync",
                  "text": "I like to vibe-code UIs for my vibe-coded plugins used in my vibe-coding platform.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:40:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aixoh",
          "author": "EliasOenal",
          "text": "I have had good results with Qwen3 Coder Next (Unsloth's Qwen3-Coder-Next-UD-Q4_K_XL.gguf) locally on Mac, it is accurate even with reasonably complex tool use and works with interactive tools [through the term-cli skill](https://github.com/EliasOenal/term-cli) in OpenCode. [Here's a video clip of it interactively debugging with lldb.](https://www.youtube.com/watch?v=bas5abIAsH4) (Left side is me attaching a session to Qwen's interactive terminal to have a peek.)",
          "score": 4,
          "created_utc": "2026-02-08 18:19:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ajqmy",
              "author": "Chromix_",
              "text": "Soo you're saying when I install the term-cli plugin then my local OpenCode with Qwen can operate my Claude CLI for me? üòâ",
              "score": 1,
              "created_utc": "2026-02-08 18:22:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ale8n",
                  "author": "EliasOenal",
                  "text": "Haha, indeed! I yesterday wanted to debug a sporadic crash I encountered twice in llama.cpp, when called from OpenCode. (One of the risks of being on git HEAD.) I spawned two term-cli sessions, one with llama.cpp and one with OpenCode, asking a second instance of OpenCode to take over to debug this. It actually ended up typing into OpenCode, running prompts, but it wasn't able to find a way to reproduce the crash 50k tokens in. So I halted that for now.",
                  "score": 1,
                  "created_utc": "2026-02-08 18:30:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48u0pw",
          "author": "fadedsmile87",
          "text": "I have an RTX 5090 + 96GB of RAM. I'm using the Q8\\_0 quant of Qwen3-Coder-Next with \\~100k context window with Cline. It's magnificent. It's a very capable coding agent. The downside of using that big quant is the tokens per second. I'm getting 8-9 tokens / s for the first 10k tokens, then it drops to around 6 t/s at 50k full context.",
          "score": 7,
          "created_utc": "2026-02-08 12:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48vzk2",
              "author": "Chromix_",
              "text": "That's surprisingly slow, especially given that you have a RTX 5090. You should be getting *at least* half the speed that I'm getting with a Q4. Did you try with my way of running it (of course with manually adjusted ncmoe to almost fill the VRAM)? Maybe there's a RAM speed impact. With 96 GB you might be running at a rather low speed in practice if you have 4 modules. Mine are running at DDR5-6000.",
              "score": 3,
              "created_utc": "2026-02-08 13:09:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48x0xi",
                  "author": "fadedsmile87",
                  "text": "I have 2x 48GB DDR5 mem sticks. 6000 MT/s (down from 6400 for stability)  \ni9-14900K\n\nI'm using the default settings in LM Studio.  \ncontext: 96k  \noffloading 15/48 layers onto GPU (LM Studio estimates 28.23GB on GPU, 90.23GB on RAM)",
                  "score": 2,
                  "created_utc": "2026-02-08 13:16:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49ylg3",
              "author": "blackhawk00001",
              "text": "Same setup here, 96GB/5090/7900x/windows hosted on LAN to be used by VS code IDE with kilo code extension on a linux desktop.\n\nTry using llama.cpp server, below are the commands that I'm using to get 30 t/s with Q4\\_K\\_M and 20 t/s with Q8.  The Q8 is slower but solved a problem in one pass that the Q4 could not figure out.  Supposedly it's much faster on vulkan at this time but I haven't tried yet.\n\n.\\\\llama-server.exe -m \"D:\\\\llm\\_models\\\\Qwen3-Coder-Next-GGUF\\\\Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf\" --jinja --temp 1.0 --top-p 0.95 --min-p 0.01 --top-k 40 -fa on --fit on -c 131072 --no-mmap --host\n\nI love using LM Studio for quick chat sessions but it was terrible for local LLM agents in an IDE.",
              "score": 1,
              "created_utc": "2026-02-08 16:41:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4a8u8t",
                  "author": "fadedsmile87",
                  "text": "I was using LM Studio.\n\nThanks to Chromix, I've installed llama.cpp and used:  \nllama-server -m Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf -fa on --fit-ctx 120000 --fit on --temp 0 --cache-ram 0 --fit-target 128\n\nNow I'm getting 27 t/s on the Q8\\_0 quant :-)",
                  "score": 4,
                  "created_utc": "2026-02-08 17:31:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4a7qmj",
              "author": "fragment_me",
              "text": "That's expected. I get 17-18 tok/s with a 5090 and ddr4 using UD q6 k xl. Q8 is huge.  My command with param for running ud q6 k xl are :\n\n.\\\\llama-server.exe -m Qwen3-Coder-Next-UD-Q6\\_K\\_XL.gguf  \\`\n\n\\-ot \".(19|\\[2-9\\]\\[0-9\\]).ffn\\_(gate|up|down)\\_exps.=CPU\" \\`\\`\n\n\\--no-mmap --jinja --threads -12  \\`\n\n\\--cache-type-k q8\\_0 --cache-type-v q8\\_0  --flash-attn on  --ctx-size 128000 -kvu \\`\n\n\\--temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01  \\`\n\n`--host` [`127.0.0.1`](http://127.0.0.1) \\`--parallel 4 --batch-size 512  \\`",
              "score": 1,
              "created_utc": "2026-02-08 17:25:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4a8pb2",
                  "author": "fadedsmile87",
                  "text": "I was using LM Studio.\n\nThanks to Chromix, I've installed llama.cpp and used:  \nllama-server -m Qwen3-Coder-Next-Q8\\_0-00001-of-00003.gguf -fa on --fit-ctx 120000 --fit on --temp 0 --cache-ram 0 --fit-target 128\n\nNow I'm getting 27 t/s on the Q8\\_0 quant :-)",
                  "score": 1,
                  "created_utc": "2026-02-08 17:30:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48oi89",
          "author": "msrdatha",
          "text": "Indeed the speed, quality and context size points mentioned are spot on with my test environment with mac M3 and kilo code as well. \n\nThis is my preferred model for coding now. I am switching this and Devstral-2-small from time to time.\n\nAny thoughts on which is a good model for \"Architect/Design\" solution part? Does a thinking model make any difference in design only mode?",
          "score": 3,
          "created_utc": "2026-02-08 12:12:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48r3t8",
              "author": "Chromix_",
              "text": "Reasoning models excel in design mode for me as well. I guess a suitable high-quality flow would be:\n\n* Ask your query to Q3CN, let it quickly dig through the code and summarize all the things one needs to know about the codebase for the requested feature.\n* Pass that through Qwen 3 Next Thinking, GLM 4.7 Flash, Apriel 1.6 and GPT OSS 120B and condense the different results back into options for the user to choose.\n* Manually choose an option / approach.\n* Give it back to Q3CN for execution.\n\nExperimental IDE support for that could be interesting, especially now that llama.cpp allows model swapping via API. Still, the whole flow would take a while to be executed, which could still be feasible if you want a high quality design over lunch break (well, high quality given the local model & size constraint).",
              "score": 6,
              "created_utc": "2026-02-08 12:33:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48t1w7",
                  "author": "msrdatha",
                  "text": "appreciate sharing these thoughts. makes sense very much.\n\nI have been thinking if a simple RAG system or Memory can help in such cases. Just thought only - not yet tried. Did not want to spend too much time on learning deep RAG or Memory implementation. I see kilo code does have some of these in settings. not yet tired on an actual code scenario.\n\nany thoughts or experience on such actions related to coding? ",
                  "score": 1,
                  "created_utc": "2026-02-08 12:48:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49379g",
              "author": "-dysangel-",
              "text": "How much RAM do you have? For architect/design work I think GLM 4.6/4.7 would be good. Unsloth's glm reap 4.6 at IQ2\\_XXS works well for me, taking up 89GB of RAM. I mostly use GLM Coding Plan anyway, so I just use local for chatting and experiments.\n\nHaving said that, I'm testing Qwen 3 Coder Next out just now, and it's created a better 3D driving simulation for me than GLM 4.7 did via the official coding plan. It also created a heuristic AI to play tetris with no problems. I¬†need to try pushing it even harder\n\nhttps://i.redd.it/xgtfa0jo0aig1.gif\n\n  \n",
              "score": 4,
              "created_utc": "2026-02-08 13:55:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o493jfy",
                  "author": "-dysangel-",
                  "text": "Qwen 3 Coder Next time trial game, single web page with three.js. Very often models will get the wheel orientation incorrect etc. It struggled a bit to get the road spline correct, but fixed it after a few iterations of feedback :)\n\nhttps://i.redd.it/l9j55bxr0aig1.gif\n\n",
                  "score": 1,
                  "created_utc": "2026-02-08 13:57:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o494jo5",
                  "author": "msrdatha",
                  "text": "89GB of RAM at what context size?",
                  "score": 1,
                  "created_utc": "2026-02-08 14:03:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ashz5",
          "author": "DHasselhoff77",
          "text": "Qwen3 Coder Next also supports fill-in-the-middle (FIM) tasks. This means you can use it for auto completion via for example llama-vscode while also using it for agentic tasks. No need for two different models occupying VRAM simultaneously.\n\nEdit: Alright actually it's not a great fit because as a recurrent model, llama.cpp can't cache it properly. See https://github.com/ggml-org/llama.cpp/pull/19408#issuecomment-3866421943",
          "score": 3,
          "created_utc": "2026-02-08 19:02:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ax2ua",
              "author": "Chromix_",
              "text": "It'd be a rather good yet slow FIM model, yes. On the other hand there is [Falcon 90M ](https://www.reddit.com/r/LocalLLaMA/comments/1qdl9za/falcon_90m/)with FIM support which you could easily squeeze into the remaining VRAM or even run on CPU for auto-complete. ",
              "score": 2,
              "created_utc": "2026-02-08 19:25:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4exzri",
                  "author": "DHasselhoff77",
                  "text": "The Falcon 90M GGUF I tried didn't support llama.cpp's `/infill` endpoint so it wasn't usable for me with llama-vscode. Using an OpenAI compatible endpoint works but in the case of that specific VSCode extension, it requires extra configuration work (some agent stuff I don't want).\n\nI also tried running Qwen Coder 2.5, 3B or 1.5B, but on the CPU and with a smaller context. It's pretty much the same speed as Qwen3 Coder Next on the GPU though.",
                  "score": 2,
                  "created_utc": "2026-02-09 11:06:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48hc4c",
          "author": "Blues520",
          "text": "Do you find it able to solve difficult tasks because I used the same quant and it was coherent but the quality was so so.",
          "score": 5,
          "created_utc": "2026-02-08 11:08:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48j2dj",
              "author": "Chromix_",
              "text": "The model & inference in llama.cpp [had issues](https://www.reddit.com/r/LocalLLaMA/comments/1quvqs9/comment/o3edjam/?context=3) when they were released initially. This has been fixed by now. So if you don't use the latest version of llama.cpp or haven't (re-)downloaded the [updated quants](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) then that could explain the mixed quality you were seeing. I also tried the Q8 REAP vs a UD Q4, but the Q8 was making more mistakes, probably because the [REAP quants](https://www.reddit.com/r/LocalLLaMA/comments/1qvjonm/first_qwen3codernext_reap_is_out/) haven't been updated yet, or maybe it's due to REAP itself.\n\nFor \"difficult tasks\": I did not test the model on LeetCode challenges, implementing novel algorithms and things, but on normal dev work: Adding new features, debugging & fixing broken things in a poorly documented real-life project - no patent-pending compression algorithms and highly exotic stuff.\n\nThe latest Claude 4.6 or GPT-5.2 Codex performs of course *way better*. More direct approach towards the solution, sometimes better approaches that the Q3CN didn't find at all. Yet still, for \"just getting some dev work done\" it's no longer needed to have the latest and greatest. Q3CN is the first local model that's usable for me in this area. Of course you might argue that using the latest SOTA is always best, as you always want the fastest, best solution, no matter what, and I would agree.",
              "score": 5,
              "created_utc": "2026-02-08 11:24:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48ji52",
                  "author": "Blues520",
                  "text": "I pulled the latest model and llamacpp yesterday so the fixes were in. I'm not saying that it's a bad model, I guess I was expecting more given the hype. \n\nI didn't do any leetcode but normal dev stuff as well. I suspect that a higher quant will be better. I wouldn't bother with the REAP quant though.",
                  "score": 5,
                  "created_utc": "2026-02-08 11:28:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48i7qd",
              "author": "-dysangel-",
              "text": "My feeling is that the small/medium models are not going to be that great at advanced problem solving, but they're getting to the stage where they will be able to follow instructions well to generate working code. I think you'd still want a larger model like GLM/Deepseek for more in depth planning and problem solving, and then Qwen 3 Coder has a chance of being able to implement individual steps. And you'd still want to fall back to a larger model or yourself if it gets stuck.",
              "score": 7,
              "created_utc": "2026-02-08 11:16:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48k9lw",
                  "author": "Chromix_",
                  "text": "Yes, for the occasional really \"advanced problem solving\" I fill the context of the latest GPT model with manually curated pages of code and text, set it to high reasoning, max tokens and get a coffee. Despite that, and yielding pretty good results and insights for some things, it still frequently needs corrections due to missing optimal (or well, better) solutions. Q3CN has no chance to compete with that. Yet it doesn't need to for regular day-to-day dev work, that's my point - seems mostly good enough.",
                  "score": 3,
                  "created_utc": "2026-02-08 11:35:40",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o48ifp1",
                  "author": "Blues520",
                  "text": "That makes sense and it does do well in tool calling which some models like Devstral trip themselves over with.",
                  "score": 1,
                  "created_utc": "2026-02-08 11:18:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48hwfy",
              "author": "Status_Contest39",
              "text": "the same feeling, even not as good as GLM4.7 flash",
              "score": 2,
              "created_utc": "2026-02-08 11:14:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o48lz0y",
              "author": "mysho",
              "text": "I tried to let it convert a simple systems service to one activated by a socket. Used Kilo code with qwen3-coder-next. Took it 30 requests for such a trivial task, but it managed in the end. I expected better, but it's kinda usable for trivial stuff.",
              "score": 1,
              "created_utc": "2026-02-08 11:50:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48qjgh",
          "author": "Brilliant-Length8196",
          "text": "Try Kilo Code instead of Roo Code.",
          "score": 5,
          "created_utc": "2026-02-08 12:28:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49i4st",
              "author": "Terminator857",
              "text": "Last time I tried, I didn't have an easy time figuring out how to wire kilocode with llama-server.",
              "score": 1,
              "created_utc": "2026-02-08 15:19:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4dyjyr",
                  "author": "alexeiz",
                  "text": "Use \"openai compatible\" settings.",
                  "score": 1,
                  "created_utc": "2026-02-09 05:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4akno5",
          "author": "LoSboccacc",
          "text": "    srv  update_slots: all slots are idle\n    srv  params_from_: Chat format: Qwen3 Coder\n    slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1\n    slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist\n    slot launch_slot_: id  3 | task 0 | processing task, is_child = 0\n    slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 65536, n_keep = 0, task.n_tokens = 1042\n    slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)\n    slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 978, batch.n_tokens = 978, progress = 0.938580\n    slot update_slots: id  3 | task 0 | n_tokens = 978, memory_seq_rm [978, end)\n    slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 1042, batch.n_tokens = 64, progress = 1.000000\n    slot update_slots: id  3 | task 0 | prompt done, n_tokens = 1042, batch.n_tokens = 64\n    slot init_sampler: id  3 | task 0 | init sampler, took 0.13 ms, tokens: text = 1042, total = 1042\n    slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 977, pos_max = 977, size = 75.376 MiB)\n    slot print_timing: id  3 | task 0 |\n    prompt eval time =    8141.99 ms /  1042 tokens (    7.81 ms per token,   127.98 tokens per second)\n           eval time =    4080.08 ms /    65 tokens (   62.77 ms per token,    15.93 tokens per second)\n\nit's not much but two year ago we we'd having 15 tps on capybara 14b being barely coherent and now we have a somewhat usable haiku 3.5 at home",
          "score": 2,
          "created_utc": "2026-02-08 18:27:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4auxsr",
          "author": "live4evrr",
          "text": "I was almost ready to give up on it, but after downloading the latest GGUF (4 bit XL) from Unsloth and updating llama.cpp, it is a good local option. Of course can't be compared to frontier cloud models (no it is not nearly as good as Sonnet 4.5) but it is still good. Amazing how well it can run so well on a 32GB VRAM card with sufficient ram (64+).",
          "score": 2,
          "created_utc": "2026-02-08 19:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4blw6h",
          "author": "rm-rf-rm",
          "text": "Looking for help in getting it working with MLX üôè  https://old.reddit.com/r/LocalLLaMA/comments/1qwa7jy/qwen3codernext_mlx_config_for_llamaswap/",
          "score": 2,
          "created_utc": "2026-02-08 21:28:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dc0yu",
          "author": "BrianJThomas",
          "text": "Couldn‚Äôt do any tool calls successfully for me in opencode and I gave up.",
          "score": 2,
          "created_utc": "2026-02-09 03:07:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebyuv",
              "author": "Chromix_",
              "text": "Latest updated model quant, at least Q4, latest llama.cpp, latest opencode? There were issues 5 days ago that have been solved since then. I have not seen a single failed too call since then.",
              "score": 2,
              "created_utc": "2026-02-09 07:32:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4f5mzy",
                  "author": "wisepal_app",
                  "text": "i get \"invalid \\[tool=write, error=Invalid input for tool write: JSON parsing failed:\"  error with opencode. i am using latest llama.cpp with cuda and unsloth ud Q4_K_XL GGUF quant. Any idea what could be the problem?",
                  "score": 2,
                  "created_utc": "2026-02-09 12:11:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4oaerx",
              "author": "Practical-Bed3933",
              "text": "I get \"I don't have access to a listdir tool in my available tools.\" with Ollama on Mac. Did you resolve it u/BrianJThomas ?",
              "score": 1,
              "created_utc": "2026-02-10 20:02:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dh9y6",
          "author": "AcePilot01",
          "text": "Imight copy your settings there, cus I also have a 4090 and 64gb of ram lol",
          "score": 2,
          "created_utc": "2026-02-09 03:37:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ecyd9",
              "author": "Chromix_",
              "text": "You'll need to ensure to have [sufficient free VRAM](https://www.reddit.com/r/LocalLLaMA/comments/1klqw5a/more_free_vram_for_your_llms_on_windows/) to achieve similar numbers - or tweak the `-n-cpu-moe` parameter a bit.",
              "score": 1,
              "created_utc": "2026-02-09 07:42:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ffils",
                  "author": "AcePilot01",
                  "text": "didn't you claim to have the same vram? lmfao.  Im on linux actually.",
                  "score": 1,
                  "created_utc": "2026-02-09 13:19:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ep0l5",
          "author": "sb6_6_6_6",
          "text": "1 x 5090 + 2 x 3090 Unsloth UD-Q6_K_XL\nCPU:  Ultra 9 285K \ndocker on CachyOs - 76 t/s context 139000 \n\n    environment:\n      - NCCL_P2P_DISABLE=1\n      - CUDA_VISIBLE_DEVICES=1,2,0\n      - CUDA_DEVICE_ORDER=PCI_BUS_ID\n      - HUGGING_FACE_HUB_TOKEN=TOKEN\n      - LLAMA_ARG_MAIN_GPU=0\n      - LLAMA_ARG_ALIAS=Qwen3-Coder-80B\n      - LLAMA_ARG_MLOCK=true\n      - LLAMA_SET_ROWS=1\n      \n    ports:\n      - \"9999:9999\"\n    ipc: host\n    security_opt:\n      - seccomp:unconfined\n      - apparmor:unconfined\n    ulimits:\n      memlock: -1\n      stack: 67108864\n    volumes:\n      - /mnt/data/opt/huggingface-cache:/root/.cache/huggingface\n      - /mnt/data/opt/80b:/opt\n\n    command: >\n      -m /opt/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00002.gguf\n      --port 9999\n      --host 0.0.0.0\n      --fit on\n      --n-predict -1\n      -fa on\n      --threads 8\n      --threads-http 4\n      --numa distribute\n      --slots\n      --jinja\n      --prio 3\n      --temp 1.0\n      --top-p 0.95\n      --top-k 40\n      --min-p 0.01\n      --cache-ram -1\n      --batch-size 2048\n\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9999/health\"]\n      interval: 180s\n      timeout: 20s\n      retries: 3\n      start_period: 600s",
          "score": 2,
          "created_utc": "2026-02-09 09:40:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fomr0",
          "author": "qubridInc",
          "text": "This aligns with what we‚Äôve observed as well. Qwen3 Coder Next works better in practice mainly because it‚Äôs an instruction-tuned MoE, not a reasoning-style model. That avoids internal reasoning loops and keeps latency predictable, which really matters for agent-style tools and long runs.\n\nTool calling and structured outputs are noticeably more reliable, and the long context (100k+) is actually usable on 24 GB VRAM thanks to its attention/memory characteristics. Combined with deterministic sampling (temp 0), it behaves stably for real-world coding instead of drifting or stalling.",
          "score": 2,
          "created_utc": "2026-02-09 14:13:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fxapx",
          "author": "ai_tinkerer_29",
          "text": "This resonates with my experience too. I've been bouncing between different models for coding work and the MoE architecture really does make a difference for speed without sacrificing too much quality.\n\nQuick question: How does the tool-calling reliability compare to something like DeepSeek-V3 or QwQ in your experience? I've had issues with some models hallucinating tool calls or breaking the JSON format mid-stream.\n\nAlso‚Äîcurious about your OpenCode vs Roo Code comparison. The \"YOLO permissions\" thing in OpenCode is exactly why I've been hesitant. Did you end up configuring stricter permissions, or just stick with Roo for production work?\n\nAppreciate the detailed write-up on the llama-server flags too. The GGML\\_CUDA\\_GRAPH\\_OPT tip is gold‚Äîdidn't know about that one.",
          "score": 2,
          "created_utc": "2026-02-09 15:00:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l7r13",
              "author": "Chromix_",
              "text": "I didn't compare to DeepSeek-V3 as it's not in the same weight class. QwQ is old, it's a good model but tool calling wasn't trained as excessively back then.\n\nThe permissions was more a \"allow/deny by default\" issue, combined with OpenCode really trying hard to get things working, even if it made no sense. I went for stricter permissions combined with safe utility scripts to execute.",
              "score": 1,
              "created_utc": "2026-02-10 09:46:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jpmji",
          "author": "Hot_Turnip_3309",
          "text": "Ok so I have everything up to date and downloaded multiple GGUFs... tool calling does NOT work",
          "score": 2,
          "created_utc": "2026-02-10 02:37:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l854l",
              "author": "Chromix_",
              "text": "I know, it's always annoying to have that \"it seems to work for everyone else, but not for me\" case. Maybe go through the support / ticket process with your inference engine and agent harness, collect the necessarily information, maybe logits could be interesting for the tool call as well. Maybe there is some inference error left which happens to strike mostly in your specific use-case.",
              "score": 1,
              "created_utc": "2026-02-10 09:50:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lv48v",
          "author": "UmpireBorn3719",
          "text": "I use RTX 5090, AMD 9900X, RAM 64GB, MXFP4  \nResult: Prefill around 1500 tps, generation around 50 tps\n\n\\`\\`\\`\n\n\\#240K+\n\nslot update\\_slots: id  2 | task 3 | prompt processing progress, n\\_tokens = 241664, batch.n\\_tokens = 4096, progress = 0.991353\n\nslot update\\_slots: id  2 | task 3 | n\\_tokens = 241664, memory\\_seq\\_rm \\[241664, end)\n\nslot update\\_slots: id  2 | task 3 | prompt processing progress, n\\_tokens = 243260, batch.n\\_tokens = 1596, progress = 0.997900\n\nslot update\\_slots: id  2 | task 3 | n\\_tokens = 243260, memory\\_seq\\_rm \\[243260, end)\n\nslot update\\_slots: id  2 | task 3 | prompt processing progress, n\\_tokens = 243772, batch.n\\_tokens = 512, progress = 1.000000\n\nslot update\\_slots: id  2 | task 3 | prompt done, n\\_tokens = 243772, batch.n\\_tokens = 512\n\nslot init\\_sampler: id  2 | task 3 | init sampler, took 18.45 ms, tokens: text = 243772, total = 243772\n\nslot update\\_slots: id  2 | task 3 | created context checkpoint 1 of 32 (pos\\_min = 243259, pos\\_max = 243259, size = 75.376 MiB)\n\nslot print\\_timing: id  2 | task 3 |\n\nprompt eval time =  170074.62 ms / 243772 tokens (    0.70 ms per token,  1433.32 tokens per second)\n\neval time =    4125.05 ms /   182 tokens (   22.67 ms per token,    44.12 tokens per second)\n\ntotal time =  174199.66 ms / 243954 tokens\n\nslot      release: id  2 | task 3 | stop processing: n\\_tokens = 243953, truncated = 0\n\nsrv  update\\_slots: all slots are idle\n\n\\`\\`\\`  \n\\`\\`\\`\n\n\\#< 25K  \nslot update\\_slots: id  3 | task 0 | prompt processing progress, n\\_tokens = 24576, batch.n\\_tokens = 4096, progress = 0.864682\n\nslot update\\_slots: id  3 | task 0 | n\\_tokens = 24576, memory\\_seq\\_rm \\[24576, end)\n\nslot update\\_slots: id  3 | task 0 | prompt processing progress, n\\_tokens = 27910, batch.n\\_tokens = 3334, progress = 0.981986\n\nslot update\\_slots: id  3 | task 0 | n\\_tokens = 27910, memory\\_seq\\_rm \\[27910, end)\n\nslot update\\_slots: id  3 | task 0 | prompt processing progress, n\\_tokens = 28422, batch.n\\_tokens = 512, progress = 1.000000\n\nslot update\\_slots: id  3 | task 0 | prompt done, n\\_tokens = 28422, batch.n\\_tokens = 512\n\nslot init\\_sampler: id  3 | task 0 | init sampler, took 2.43 ms, tokens: text = 28422, total = 28422\n\nslot update\\_slots: id  3 | task 0 | created context checkpoint 1 of 32 (pos\\_min = 27909, pos\\_max = 27909, size = 75.376 MiB)\n\nslot print\\_timing: id  3 | task 0 | \n\nprompt eval time =   16024.06 ms / 28422 tokens (    0.56 ms per token,  1773.71 tokens per second)\n\neval time =    2250.77 ms /   112 tokens (   20.10 ms per token,    49.76 tokens per second)\n\ntotal time =   18274.83 ms / 28534 tokens\n\nslot      release: id  3 | task 0 | stop processing: n\\_tokens = 28533, truncated = 0\n\n\\`\\`\\`",
          "score": 2,
          "created_utc": "2026-02-10 12:56:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mfnuu",
          "author": "AccomplishedLeg527",
          "text": "i am running it on 8 gb vram 3070ti laptop with 32Gb ram :). First attempt was using accelerate offloading to disk (as 80gb safetensors won\\`t fit in ram), and i got 1 token per 255 second. Than i wrote custom offloading and reach 1 token per second (!!! 255x speedup !!!). On desktop 3070 with full PCE buss it should be 2x faster + 2x fsater because desktop GPU + can be used raid 0 (2 nvme ssd) - 3..5x on loading experts weights + if more vram (12-16Gb) can be more weights cashed on cuda (now i got 55% cache hit rate using only 3gb vram). In total 8-16Gb middle range desktop cards can run it with speed 3 to 10 tokens per second with only 32Gb ram.  If someone interested i can share how i did this. Or should i pattent it? ",
          "score": 2,
          "created_utc": "2026-02-10 14:51:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nuqdn",
              "author": "Chromix_",
              "text": "So, you're running the 80GB Q8 quant on a system with 40 GB (V)RAM in total. Your SSD isn't reading the remaining 40+ GB once per second, but it also doesn't need to, since it's a MoE.\n\nThere was a posting here a while ago where someone compiled some stats on the predictability of expert selection per token and got things 80%+ correct IIRC. With that approach one can (pre)load the required experts to maximize generation speed without having to wait that much for the comparatively slow SSD. Maybe you did something similar? Or is it just pinning the shared expert(s) and other parts that are needed for each token into (V)RAM?",
              "score": 2,
              "created_utc": "2026-02-10 18:50:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ny8xd",
                  "author": "AccomplishedLeg527",
                  "text": "most frequent experts indexes cached in vram for each layer, with only 3 Gb free vram i got 43-55% cache hit rate, for ram i have 2 options, one used by mmap to speedup loading, or do not use mmap and ram not needed at all (maybe up to few Mb for transfers), model without experts use only 4.6Gb vram (+we neeed some memory for context)",
                  "score": 2,
                  "created_utc": "2026-02-10 19:06:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pthev",
          "author": "FairAlternative8300",
          "text": "Pro tip for Windows users with 16GB VRAM like the 5070 Ti: the \\`--n-cpu-moe\\` flag is the magic sauce here. It offloads the MoE expert layers to CPU while keeping attention on GPU, so you get decent 20+ tok/s generation without needing a 5090.\n\nWith 96GB DDR5 you should be golden. Try something like:\n\n\\`llama-server.exe -m Qwen3-Coder-Next-UD-Q4\\_K\\_XL.gguf -ngl 99 -fa on -c 80000 --n-cpu-moe 28\\`\n\nStart with fewer CPU-MOE layers and increase until it fits in VRAM. Flash attention (\\`-fa on\\`) helps a lot with the 16GB constraint too.",
          "score": 2,
          "created_utc": "2026-02-11 00:41:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48hkmt",
          "author": "simracerman",
          "text": "Good reference. I will give opencode and this model a try.\n\nAt what context size did you notice it lost the edge?",
          "score": 1,
          "created_utc": "2026-02-08 11:11:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48jemi",
              "author": "Chromix_",
              "text": "So far I didn't observe any issue where it did something where it \"should have known better\" due to having relevant information in its context, but I \"only\" used it up to 120k. Of course its long context handling is far from perfect, yet it seems good enough in practice for now. Kimi Linear should be better in that aspect (not coding though), but I haven't tested it yet.",
              "score": 1,
              "created_utc": "2026-02-08 11:27:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4oy6c4",
                  "author": "zoyer2",
                  "text": "you mean 48B? ive tested it and sadly not so good, Qwen3 coder next is a lot better",
                  "score": 1,
                  "created_utc": "2026-02-10 21:53:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48ie2y",
          "author": "Easy_Kitchen7819",
          "text": "Try DeepSwe",
          "score": 1,
          "created_utc": "2026-02-08 11:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48p0ce",
              "author": "Money-Frame7664",
              "text": "Do you mean agentica-org/DeepSWE-Preview ?",
              "score": 2,
              "created_utc": "2026-02-08 12:16:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4g1b71",
                  "author": "Easy_Kitchen7819",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2026-02-09 15:21:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48pimy",
          "author": "jacek2023",
          "text": "How do you use OpenCode on 24 GB VRAM? How long do you wait for prefill? Do you have this fix? [https://github.com/ggml-org/llama.cpp/pull/19408](https://github.com/ggml-org/llama.cpp/pull/19408)",
          "score": 1,
          "created_utc": "2026-02-08 12:20:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48slm6",
              "author": "Odd-Ordinary-5922",
              "text": "if you have --cache-ram set to something high prefill isnt really a problem",
              "score": 2,
              "created_utc": "2026-02-08 12:44:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48tc04",
                  "author": "jacek2023",
                  "text": "I use --cache-ram 60000, what's your setting?",
                  "score": 1,
                  "created_utc": "2026-02-08 12:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48usbl",
              "author": "Chromix_",
              "text": "Thanks for pointing that out. No I haven't tested with this very recent fix yet. [ggerganov states](https://github.com/ggml-org/llama.cpp/pull/19408#issuecomment-3866421943) though that reprocessing would be unavoidable if something early in the prompt is changed - which is exactly what happens when Roo Code for example switches from \"Architect\" to \"Code\" mode.\n\nHow I use OpenCode with 24GB VRAM? Exactly with the model, quant and command line stated in my posting, although prompt processing [could be faster](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o48mli1/) as pointed out in another comment. With Roo the initial processing takes between 15 to 40 seconds before it jumps into action, yet as it'll iterate quite some time on its own anyway, waiting for prefill isn't that important for me.",
              "score": 2,
              "created_utc": "2026-02-08 13:00:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48vtx2",
                  "author": "jacek2023",
                  "text": "Yes I am thinking about trying roo (I tested that it works), but I am not sure how \"agentic\" it is. Can you make it compile and run your app like in opencode? I use Claude Code (+Claude) and Codex (+GPT 5.3) simultaneously and opencode works similarly, can I achieve that workflow in roocode?",
                  "score": 1,
                  "created_utc": "2026-02-08 13:08:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48znv3",
          "author": "rorowhat",
          "text": "The Q4 quant was taking 62GB of Ram on LMstudio as well, it didn't make sense.",
          "score": 1,
          "created_utc": "2026-02-08 13:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49p4h0",
          "author": "klop2031",
          "text": "Yeah i feel the same. For the first time this thing can do agentic tasks and can code well. I actually found myself not using a frontier model and just using this because of privacy. Im like wow so much better",
          "score": 1,
          "created_utc": "2026-02-08 15:55:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49tyhf",
          "author": "anoni_nato",
          "text": "I'm getting quite good results coding with Mistral Vibe and GLM 4.5 air free (openrouter, can't self host yet).\n\n\nHas its issues (search and replace fails often so it switches to file overwrite, and sometimes it loses track of context size) but it's producing code that works without me opening an IDE.",
          "score": 1,
          "created_utc": "2026-02-08 16:19:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a0kmc",
          "author": "HollowInfinity",
          "text": "I used OpenCode, roo, my own agent and others but found the best agent is (unsurprisingly) Qwen-Code. The system prompts and tool setup is probably exactly what the agent is trained for. Although as I type this you could probably just steal their tool definitions and prompts for whatever agent you're using.",
          "score": 1,
          "created_utc": "2026-02-08 16:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4aso5l",
          "author": "jedsk",
          "text": "did you get any err outs with opencode?\n\nit kept failing for me when just building/editing an html page",
          "score": 1,
          "created_utc": "2026-02-08 19:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4az8z6",
              "author": "Chromix_",
              "text": "No obvious errors aside from the initial uninstalling of packages because it wasn't prompted to leave the dev environment alone. Well, and then there's this - the first and sometimes second LLM call in a sequence always fails for some reason, despite the server being available:\n\nhttps://preview.redd.it/30iad8mbpbig1.png?width=862&format=png&auto=webp&s=d391b1e735271b6cb1c5a67b10b1b245fd6ac309\n\n",
              "score": 2,
              "created_utc": "2026-02-08 19:35:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4b82kw",
          "author": "IrisColt",
          "text": "Thanks!!!",
          "score": 1,
          "created_utc": "2026-02-08 20:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4b93l1",
          "author": "shrug_hellifino",
          "text": "Any quant I use, I am getting this error: forcing full prompt re-processing due to lack of cache data as shown below, reloading 50,60,70,150k context over and over is quite miserable.. all latest build and fresh quant dl just in case as of today 2/8/26. Any guidance or insight would be appreciated.\n\n    slot print_timing: id  3 | task 6556 | \n    prompt eval time =   95123.58 ms / 54248 tokens (    1.75 ms per token,   570.29 tokens per second)\n           eval time =   15815.35 ms /   666 tokens (   23.75 ms per token,    42.11 tokens per second)\n          total time =  110938.94 ms / 54914 tokens\n    slot      release: id  3 | task 6556 | stop processing: n_tokens = 54913, truncated = 0\n    srv  update_slots: all slots are idle\n    srv  log_server_r: done request: POST /v1/messages 127.0.0.1 200\n    srv  params_from_: Chat format: Qwen3 Coder\n    slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = 99553296262\n    srv  get_availabl: updating prompt cache\n    srv   prompt_save:  - saving prompt with length 53502, total state size = 1329.942 MiB\n    srv          load:  - looking for better prompt, base f_keep = 0.001, sim = 0.001\n    srv        update:  - cache size limit reached, removing oldest entry (size = 285.055 MiB)\n    srv        update:  - cache size limit reached, removing oldest entry (size = 840.596 MiB)\n    srv        update:  - cache size limit reached, removing oldest entry (size = 1335.463 MiB)\n    srv        update:  - cache state: 5 prompts, 6876.842 MiB (limits: 8192.000 MiB, 262144 tokens, 311062 est)\n    srv        update:    - prompt 0x5b0377f1a0f0:   50773 tokens, checkpoints:  1,  1341.325 MiB\n    srv        update:    - prompt 0x5b036f03bf40:   51802 tokens, checkpoints:  1,  1365.454 MiB\n    srv        update:    - prompt 0x5b0375648cd0:   52203 tokens, checkpoints:  1,  1374.857 MiB\n    srv        update:    - prompt 0x5b0376a89180:   52844 tokens, checkpoints:  1,  1389.888 MiB\n    srv        update:    - prompt 0x5b0378b94380:   53502 tokens, checkpoints:  1,  1405.317 MiB\n    srv  get_availabl: prompt cache update took 1473.25 ms\n    slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> ?temp-ext -> dist \n    slot launch_slot_: id  2 | task 7250 | processing task, is_child = 0\n    slot update_slots: id  2 | task 7250 | new prompt, n_ctx_slot = 262144, n_keep = 0, task.n_tokens = 54953\n    slot update_slots: id  2 | task 7250 | n_past = 36, slot.prompt.tokens.size() = 53502, seq_id = 2, pos_min = 53501, n_swa = 1\n    slot update_slots: id  2 | task 7250 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n    slot update_slots: id  2 | task 7250 | erased invalidated context checkpoint (pos_min = 52818, pos_max = 52818, n_swa = 1, size = 75.376 MiB)\n    slot update_slots: id  2 | task 7250 | n_tokens = 0, memory_seq_rm [0, end)",
          "score": 1,
          "created_utc": "2026-02-08 20:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bcg47",
              "author": "Chromix_",
              "text": "A potential fix for [this](https://github.com/ggml-org/llama.cpp/issues/19394) was just merged, get the latest version and test again :-)  \nYou could also increase `--cache-ram` if you have some free RAM to spare.",
              "score": 3,
              "created_utc": "2026-02-08 20:41:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4brxgi",
                  "author": "shrug_hellifino",
                  "text": "Wow, I just rebuilt this morning, so this is that new? Thank you for the pointer!",
                  "score": 2,
                  "created_utc": "2026-02-08 21:58:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4baxv3",
          "author": "Savantskie1",
          "text": "I‚Äôm currently using vs code insiders, I can‚Äôt use cli coding tools. So can you check to see if this model will work with that? I use lm studio, I don‚Äôt care if llama.cpp is faster I won‚Äôt use it so don‚Äôt suggest it please.",
          "score": 1,
          "created_utc": "2026-02-08 20:33:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4be5qj",
              "author": "Chromix_",
              "text": "Roo Code is a VSCode plugin that you can use with any OpenAI-compatible API, like for example LMStudio provides. Out of interest: Is there a specific reason to stick to LMStudio if it's only used as API endpoint for a IDE (or IDE plugin)? The difference can be very large as another commenter [found out](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4a32mu/).",
              "score": 2,
              "created_utc": "2026-02-08 20:50:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4bjxsc",
                  "author": "Savantskie1",
                  "text": "I don‚Äôt care about speed, I care about ease of use and being able to load and unload a model without needing to spawn a separate instance of the model runner. That‚Äôs just waste of resources. Unnecessary overhead",
                  "score": 1,
                  "created_utc": "2026-02-08 21:18:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4blpeo",
          "author": "HumanDrone8721",
          "text": "Just in case someone wonders here are the fresh benchmark on a semi-potato PC i7-14KF (4090+3090+128GB DDR5) for the 8bit fat quanta, coding performance later:\n\n    llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 26 -mmp 0\n    ggml_cuda_init: found 2 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n      Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  26 |           pp512 |        220.85 ¬± 2.24 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  26 |           tg128 |         14.68 ¬± 0.27 |",
          "score": 1,
          "created_utc": "2026-02-08 21:27:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bnefo",
              "author": "Chromix_",
              "text": "That TG speed looks slower than expected. In another comment [here](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o4a32mu/) someone got 27 t/s with a single RTX 5090 and your CPU. Yes, the 5090 is faster, but not twice as fast. Have you tried only using the 4090, and the options/settings from my post, just to get an idea if things can be sped up for you?",
              "score": 2,
              "created_utc": "2026-02-08 21:35:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4brkdo",
                  "author": "HumanDrone8721",
                  "text": "That's for the 4090 and 3090 separate benchmarks, the fact that only 14 layers fit in card and the difference is negligible between cards tells me the the performance is RAM and CPU bound and not on the capabilities of the GPU. \n\nThe poster with the 5090 probably managed to fit 39 or even 40 layers in the GPU and this gave a boost of speed, unfortunately as almost no one is bothered to post the actual precise command line and parameters, is just some anecdote.\n\n    CUDA_VISIBLE_DEVICES=0 llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 14 -mmp 0\n    ggml_cuda_init: found 1 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           pp512 |        167.85 ¬± 1.60 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           tg128 |         10.74 ¬± 0.05 |\n\n\n    CUDA_VISIBLE_DEVICES=1 llama-bench -m  .cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_UD-Q8_K_XL_Qwen3-Coder-Next-UD-Q8_K_XL-00001-of-00003.gguf -fa on -ngl 14 -mmp 0\n    ggml_cuda_init: found 1 CUDA devices:\n      Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n    | model                          |       size |     params | backend    | ngl |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           pp512 |        160.27 ¬± 1.55 |\n    | qwen3next 80B.A3B Q8_0         |  79.83 GiB |    79.67 B | CUDA       |  14 |           tg128 |         10.55 ¬± 0.15 |\n\n    build: 8872ad212 (7966)",
                  "score": 1,
                  "created_utc": "2026-02-08 21:56:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4burr5",
          "author": "Zestyclose_Yak_3174",
          "text": "Seems like the verdict is still out. Many seem to say it's good, yet also many seem to say it is a very weak model in the real world.",
          "score": 1,
          "created_utc": "2026-02-08 22:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4c61i6",
          "author": "pol_phil",
          "text": "This model is great. My only problem is that its prefix caching doesn't work on vLLM. I think SGLang has solved this, but haven't tried it yet.\n\nAre u aware of other serving frameworks which do not have this issue? Because, for me, it turns out slower than larger models (for long conversations)",
          "score": 1,
          "created_utc": "2026-02-08 23:15:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebt1d",
              "author": "Chromix_",
              "text": "[Two](https://github.com/ggml-org/llama.cpp/issues/19394) [fixes](https://github.com/ggml-org/llama.cpp/pull/19408) in that area were just added for llama.cpp. vLLM is of course faster if you have the VRAM for it.",
              "score": 3,
              "created_utc": "2026-02-09 07:30:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dcal1",
          "author": "AcePilot01",
          "text": "How do you \"run \" one like that? \n\nI use Openwebui and ollama, so when I download them (forget how they even get placed in there, lmfao I just have ai do it all haha)",
          "score": 1,
          "created_utc": "2026-02-09 03:09:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ech8y",
              "author": "Chromix_",
              "text": "Ditch ollama for llama.cpp. [He could do it](https://www.reddit.com/r/LocalLLaMA/comments/1qz5uww/comment/o49lydo/), you can do it too. (To be fair you can also connect OpenCode to ollama, but why not switch to something nicer while being at it?)",
              "score": 2,
              "created_utc": "2026-02-09 07:37:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4fsrav",
                  "author": "AcePilot01",
                  "text": "Maybe, trying to get it to work in Openwebui is  being a freaking pain. having to merge them all etc, it should be as easy as downloading the model and sticking it in a damn folder lol. having to vibe code it to work is getting old lmfao",
                  "score": 1,
                  "created_utc": "2026-02-09 14:36:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dkx7d",
          "author": "AcePilot01",
          "text": "Are your comparisons of Opencode and roo code compared to Qwen3 coder next, or am I missing something? or are those agents what you USE this model with?\n\nOr could you just use those settings on say, openwebui? Or use this in that way? did you? are those local?\n\nIdeally I would like to code locally perhaps in a number of IDE?  I assume Roo/Open code are basically just IDE's?",
          "score": 1,
          "created_utc": "2026-02-09 04:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edjhc",
              "author": "Chromix_",
              "text": "You cannot compare \"OpenCode\" to \"Qwen3\", because OpenCode is a harness for using LLMs, and Qwen3 is a LLM. My post is about using both OpenCode as well as Roo Code with Qwen3 Coder Next (Q3CN).\n\nYou can also use OpenWebUI with Q3CN, but it doesn't give you any agentic coding functionality like OpenCode or Roo. You could paste in code though.\n\n>I assume Roo/Open code are basically just IDE's?\n\nNo, Roo Code is a plugin for VSCode (an IDE), so if you install it you have agentic coding in an IDE. Of course you could also rewire the Copilot that's forced into VSCode for local LLMs. OpenCode is less of an IDE, but more a vibe-coding tool.",
              "score": 2,
              "created_utc": "2026-02-09 07:47:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ffqpz",
                  "author": "AcePilot01",
                  "text": "OH ok, when I went to Open code's site they seemed to indicate it was a subscription/online thing. Not local.",
                  "score": 1,
                  "created_utc": "2026-02-09 13:20:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dqgfn",
          "author": "Revolutionary_Loan13",
          "text": "Anyone using a docker image with lama-server on it or does it not perform as well?",
          "score": 1,
          "created_utc": "2026-02-09 04:36:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edyo6",
              "author": "Chromix_",
              "text": "What for would you use docker? One of the [main points](https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#description) about llama.cpp is that you can just use it as-is, without having to install any dependencies. You don't even need to install llama.cpp, just copy and run the binary distribution. Docker is usually used to run things that need dependencies, a running database server, whatsoever.\n\nIt'd be like taking your M&Ms out of the pack and wrapping them individually before eating them, just because you're used to unwrap your candy one by one when snacking.",
              "score": 2,
              "created_utc": "2026-02-09 07:51:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ek0h9",
          "author": "crablu",
          "text": "I have problems running qwen3-coder-next with opencode (RTX 5090, 64GB RAM). I tried with Qwen3-Coder-Next-UD-Q4_K_XL.gguf and Qwen3-Coder-Next-MXFP4_MOE.gguf. It works perfectly fine in chat.\n\nstart command:\n\n    llama-server.exe ^\n     --models-preset \"E:\\LLM\\llama-server\\models.ini\" ^\n     --models-max 1 ^\n     --parallel 1 ^\n     --cont-batching ^\n     --flash-attn on ^\n     --jinja ^\n     --port 8080\n\nmodels.ini:\n\n    [qwen3-coder-next-mxfp4]\n    model   = E:\\LLM\\models\\unsloth\\Qwen3-Coder-Next-GGUF\\Qwen3-Coder-Next-MXFP4_MOE.gguf\n    c = 65536\n    b = 8192\n    ub = 8192\n    temp = 1\n    top-p = 0.95\n    top-k = 40\n    min-p = 0.01\n    n-cpu-moe = 24\n    no-mmap = true    \n\nOpencode is not able to use the write tool. The UI says invalid.\nI built latest llama.cpp. Does anyone know how to fix this?",
          "score": 1,
          "created_utc": "2026-02-09 08:50:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ekcr3",
              "author": "Chromix_",
              "text": "Try temperature 0, verify that you have the latest update of the Q4 model. It works reliably for me with that.",
              "score": 2,
              "created_utc": "2026-02-09 08:54:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4emix8",
                  "author": "crablu",
                  "text": "With temp 0 it seems to work now. Thank you.\n\nEdit: nvm it edited some files but now has trouble again:\n\nMaking edits ¬∑ 3m, 29s\nI'll create the ModelsIniEditor.tsx component and integrate it into App.tsx.\n\nüëì Read OpencodeConfigViewer.tsx\n\nüëì Read App.tsx\n\nüëì Read api.ts\n\nüìé invalid\n\nüìé invalid\n\nüìé invalid\n\nüìé invalid\n\n‚ò∞ Write",
                  "score": 1,
                  "created_utc": "2026-02-09 09:15:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4eqydd",
          "author": "DOAMOD",
          "text": "https://i.redd.it/j3rbhpq2zfig1.gif\n\nFor me, it's been a bit disappointing in some tests, and also in a coding problem where the solution wasn't very helpful. It doesn't seem very intelligent. I suppose it will be good for other types of coding tasks like databases, etc. I had high expectations.",
          "score": 1,
          "created_utc": "2026-02-09 10:00:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4nwbhw",
              "author": "chickN00dle",
              "text": "spinning fishies cant be anything other than a success üôå \n\njoking",
              "score": 1,
              "created_utc": "2026-02-10 18:57:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fhvle",
          "author": "Tudeus",
          "text": "Has anyone used it as the main drive for openclaw?\n\n",
          "score": 1,
          "created_utc": "2026-02-09 13:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ken42",
          "author": "AcePilot01",
          "text": "I forgot to ask, is this the 160gb version? Or just the official 32b one?",
          "score": 1,
          "created_utc": "2026-02-10 05:22:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8e71",
              "author": "Chromix_",
              "text": "Qwen3 Coder Next is 160 GB in the distributed base version, yet the [quantized GGUFs](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) in the 50 to 60 GB range work quite well.",
              "score": 1,
              "created_utc": "2026-02-10 09:53:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4mtgl3",
                  "author": "AcePilot01",
                  "text": "I kind of figured, since the 16ogb wouldn't fit lol, wasn't sure if (since it was a bunch of tensor files) that maybe it worked different lol.\n\n\nI did try downloading a GGUF version and setting it up with llama.cpp but never could get it to work unfort.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:58:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mpc4r",
          "author": "Gimme_Doi",
          "text": "thanks",
          "score": 1,
          "created_utc": "2026-02-10 15:39:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxpf86",
      "title": "[Release] Experimental Model with Subquadratic Attention: 100 tok/s @ 1M context, 76 tok/s @ 10M context (30B model, single GPU)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/",
      "author": "Sad-Size2723",
      "created_utc": "2026-02-06 18:19:46",
      "score": 352,
      "num_comments": 47,
      "upvote_ratio": 0.98,
      "text": "Hey everyone,\n\nLast week I shared preliminary results on a new subquadratic attention mechanism ([https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary\\_new\\_subquadratic\\_attention\\_20k\\_toks](https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks)). Following up with the full release: model + inference code are now available.\n\n**TL;DR**: 30B model achieving O(L\\^(3/2)) scaling instead of O(L\\^2). Enables 1M‚Äì10M context on a single GPU with decode speeds that stay practical even at extreme context lengths. Ships with an OpenAI-compatible server and CLI to try out.\n\n\\- ü§ó **Model**: [https://huggingface.co/concavity-ai/superlinear-exp-v0.1](https://huggingface.co/concavity-ai/superlinear-exp-v0.1)\n\n\\- üíª **Code**: [https://github.com/concavity-ai/superlinear](https://github.com/concavity-ai/superlinear) (\\`pip install superlinear\\`)\n\n\\- üìÑ **Paper**: [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401)\n\n\n\n**Main Idea**\n\nYou can think of attention as a search algorithm to find relevant information for next-token prediction. Standard attention is basically O(L) brute-force search. We're doing O(L\\^0.5) jump-search with learned routing: score O(L\\^0.5) candidate spans, select top-k, then do token-level attention within the selected spans.\n\nThis gives **O(L\\^(3/2)) total complexity** while preserving **random context access** ‚Äî any token can be selected by content-dependent routing, unlike fixed sliding windows. When you 10x the context length, the search budget only grows by \\~3.2x. That subquadratic scaling really matters for long context.\n\n\n\n**Performance (Single B200 GPU)**\n\n    | Context Length | Prefill (tok/s) | Decode (tok/s) | Memory  |\n    |----------------|-----------------|----------------|---------|\n    | 1M tokens      | ~20,202         | ~109           | 66 GB   |\n    | 10M tokens     | ~5,576          | ~76            | ~120 GB |\n\nKey point: 1M ‚Üí 10M context (10x increase) only drops decode speed by \\~30%, not the 10x slowdown with dense attention.\n\n\n\n**Why This Matters**\n\nWhen you have fast long-context inference, usage patterns change. The key is **maintaining the cache** instead of reprocessing everything:\n\n\\- ***Almost-infinite chat***: KV cache in memory for instant responses, save/restore sessions to disk for persistence\n\n\\- ***Document Q&A***: Load documents once, ask cross-document questions without reprocessing (our GitHub example: 8 Wikipedia articles with cross-document reasoning)\n\n\\- ***Long-form generation***: 20k+ token reasoning on difficult math problems and coherent long article writing, all with maintained context\n\nEarly results: perfect NIAH at 512K context (up from 256K last week), cross-document reasoning working, subquadratic scaling working in practice.\n\nSince no existing inference engine is going to support our custom kernels, we built the full stack ourselves: Triton kernels, OpenAI-compatible server, session snapshots, chunked prefill, CLI with BM25 RAG.\n\n\n\n**Limitations & Next Steps**\n\n***Current limitations:***\n\n\\- This is an \\*\\*architecture + systems feasibility release\\*\\*, not production-quality\n\n\\- Limited training data (initial SFT only)\n\n\\- Comprehensive evals beyond NIAH still needed\n\n\\- FP16 only (66GB for 1M context) ‚Äî quantization coming soon\n\n***Quantization*** **(coming soon):**\n\n\\- 4-bit/8-bit quantization to run 1M context on 24GB consumer GPUs\n\n\\- Target: RTX 4090 / RTX 5090 with full 1M context\n\n\\- 2M context on 48GB cards (e.g., RTX 6000 Ada)\n\n***Hardware support:***\n\n\\- Currently CUDA only (B200, RTX 6000 Blackwell tested)\n\n\\- AMD ROCm port coming (Triton kernels should make this straightforward)\n\n\\- Eventually Apple Silicon (harder but not impossible)\n\n***Training & Quality improvements:***\n\n\\- Scaling up SFT data with more long-context examples\n\n\\- Potentially doing continued pretraining on long documents\n\n\\- Expanding perfect NIAH range beyond 512K\n\n\\- Real-world long-context benchmarks (book QA, codebase analysis, multi-document reasoning)\n\n***New end-user applications***: We are planning to develop local-first end-user applications based on this. What would you actually use long context for? Would love to hear specific use cases to help us prioritize.\n\n\n\n\\---\n\nTrying something new is extremely hard. Everyone likes existing transformer architectures ‚Äî optimizations at every level, predictable scaling laws. But to make truly long-context models practical on local hardware, I think we need new ideas. It doesn't hurt to try, right?\n\nI'm trying not to spam this sub, so the GitHub repo is the best place to follow progress. Happy to answer questions here though! If you try it and hit issues, open a GitHub issue. And if you have thoughts on long-context use cases, I'd love to hear them.\n\nThanks for all the encouragement on the last post!\n\n**Links**:\n\n\\- ü§ó **Model**: [https://huggingface.co/concavity-ai/superlinear-exp-v0.1](https://huggingface.co/concavity-ai/superlinear-exp-v0.1)\n\n\\- üíª **Code**: [https://github.com/concavity-ai/superlinear](https://github.com/concavity-ai/superlinear)\n\n\\- üìÑ **Paper**: [https://arxiv.org/abs/2601.18401](https://arxiv.org/abs/2601.18401)",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "o40xagb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2026-02-07 04:00:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z6lpj",
          "author": "QuackerEnte",
          "text": "NO I was literally about to release something similar. You beat me to it man, congratulations. \n\n(My idea was: instead of multi-step search (coarse to fine like your paper proposes), I'm using hierarchical refinement and compression. O(L*K^2 ) with fixed levels, like a pyramid. The coarse summary vectors can be attended to alongside normal tokens, instead of span-attention on selected regions. It could also \"zoom in\" and decide to fetch more detail to load into context (similar to your random access idea), via learned attention thresholds instead of search scores. \nKey difference is also that your idea needs end-to-end training, while mine was a model-agnostic wrapper approach because I couldn't afford to retrain an entire model.) \n\nOverall really great read, a lot to learn from!\nI may or may not eventually publish my work if it holds any value for the community. I'll be following your future work.",
          "score": 27,
          "created_utc": "2026-02-06 21:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhzjc",
              "author": "Sad-Size2723",
              "text": "No worries, I am not sure if you are an active researcher, but you will always find people doing similar things. I have seen many articles doing things similar to what you just suggested, but I think it's still worth it to work on the idea. The reason is that people implement even the same idea differently, and the engineering details matter greatly. So it is very possible that you will get better results. In terms of resources, it also depends who you compare to. For us, even though we can do small scale fine-tuning, that's nowhere near the scale of any LLM labs...",
              "score": 17,
              "created_utc": "2026-02-06 22:49:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o42g2dw",
              "author": "RobotRobotWhatDoUSee",
              "text": "You should definitely publish it and discuss it here. This is how ratchet works, you're often working on something similar to others in parallel,but that's fine (especially if you're not chasing tenure), you just have a section of your lit review (in your paper) that notes similar projects and how you differ.",
              "score": 1,
              "created_utc": "2026-02-07 12:05:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yfbv9",
          "author": "ruibranco",
          "text": "The fact that 10x context only costs \\~30% decode speed is the real headline here. That scaling curve is what makes this actually practical instead of just theoretically interesting. Waiting for the 4-bit quant to see how this runs on a 4090 with 1M context, that would be a game changer for local RAG pipelines where you currently have to chunk everything aggressively to fit in reasonable context windows.",
          "score": 21,
          "created_utc": "2026-02-06 19:34:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zgj8u",
              "author": "Sad-Size2723",
              "text": "Yeah, the scaling adds up fast especially when you go into million token scale where the attention calculation becomes dominant. \n\nYour point about RAG is completely valid because that's one of our first applications. I am actually going to write a detailed Medium article on this. Will share it here if you are interested. ",
              "score": 15,
              "created_utc": "2026-02-06 22:41:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zu9lo",
                  "author": "WillemDaFo",
                  "text": "Please share it üëç",
                  "score": 3,
                  "created_utc": "2026-02-06 23:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y5vi8",
          "author": "ortegaalfredo",
          "text": "What I found very interesting is that the model is basically Nemotron 3, so this can be applied to existing models.\n\nJust today I saw an announcement from nvidia about a kv-cache compression algorithm that enables >10M context sizes. I believe a model with 10M context size will have a memory approaching that of a person.",
          "score": 48,
          "created_utc": "2026-02-06 18:49:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ydybm",
              "author": "Sad-Size2723",
              "text": "So we are actually replacing the attention layers, which in theory can be done on any models. We are applying it to Nemotron 3 because of quality and computation efficiency considerations. The current KV cache implementation on this model is pretty efficient already, but will certainly look into compression if there is bottleneck in the future. ",
              "score": 20,
              "created_utc": "2026-02-06 19:28:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yeysr",
                  "author": "Significant_Fig_7581",
                  "text": "Why not GLM 4.7 Flash? it's really really slow, but also really good",
                  "score": 15,
                  "created_utc": "2026-02-06 19:33:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yaxz9",
          "author": "Ok_Warning2146",
          "text": "Great work. Can u submit your model to [contextarena.ai](http://contextarena.ai) such that we can see how well it performs on long context bench? So how much kv cache u use at 1m context? kimi linear uses 14.875gb at 1m.",
          "score": 28,
          "created_utc": "2026-02-06 19:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yg1ry",
              "author": "Sad-Size2723",
              "text": "Noted, one of the next steps for us is to perform a comprehensive evaluation and context [arena.ai](http://arena.ai) is definitely considered. \n\nIn terms of context length, we chose Nemotron 3 Nano 30B because of its efficient KV cache implementation. Right now it is about 6GB per 1M tokens. ",
              "score": 13,
              "created_utc": "2026-02-06 19:38:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45obbb",
                  "author": "Orolol",
                  "text": "I'll try to test it on [familybench](https://github.com/Orolol/familyBench) a long context reasoning benchmark",
                  "score": 1,
                  "created_utc": "2026-02-07 22:36:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y2d8j",
          "author": "Accomplished_Ad9530",
          "text": "I saw your previous post and thought your paper looked interesting. Good explanations in your post and comments, too. And thanks for releasing the code and model so quickly. h/t",
          "score": 21,
          "created_utc": "2026-02-06 18:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yeh0a",
              "author": "Sad-Size2723",
              "text": "Thanks! It takes some effort to put together the inference code so that people can actually use it",
              "score": 2,
              "created_utc": "2026-02-06 19:30:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o408m3h",
                  "author": "Accomplished_Ad9530",
                  "text": "Working inference code is hugely appreciated. Wish more ML labs would put that effort in.\n\nBTW, you might be interested in the new paper \"MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers\" \\[ [https://arxiv.org/abs/2602.00398](https://arxiv.org/abs/2602.00398) \\] as a complementary technique to your own. It also has some interesting implications for RAG and interpretability.",
                  "score": 3,
                  "created_utc": "2026-02-07 01:23:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ylanj",
          "author": "Confident-While-1322",
          "text": "Look forward to the Apple Silicon version",
          "score": 9,
          "created_utc": "2026-02-06 20:04:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y0bao",
          "author": "Limp_Finding_7168",
          "text": "This is genuinely exciting stuff. The jump from O(L\\^2) to O(L\\^3/2) might seem incremental on paper, but those real-world decode speeds at 10M context are pretty compelling - only 30% slowdown instead of the usual 10x death spiral is huge for practical applications. Really curious how the quantized versions will perform once there ready, especially if you can get 1M context running smoothly on consumer hardware.",
          "score": 16,
          "created_utc": "2026-02-06 18:22:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ye9ma",
              "author": "Sad-Size2723",
              "text": "Thanks! Yeah, the scaling is actually quite dramatic at long context. I should probably make it more clear, for decoding full attention is O(L) and our algorithm is O(L\\^0.5), so at 1M it is already 1000x efficiency, but of course we are paying a much higher overhead, but even at 10x overhead it is still a win over full attention. ",
              "score": 2,
              "created_utc": "2026-02-06 19:29:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y83ac",
          "author": "twack3r",
          "text": "What is the quality of attention across the context window like? Is there the usual dip or does this approach alleviate this?\n\nIn my experience there is a huge difference between ctx sizes and their actual usability between architectures.",
          "score": 3,
          "created_utc": "2026-02-06 18:59:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfhv1",
              "author": "Sad-Size2723",
              "text": "Good point, yeah the quality does drop significantly as you increase the context length, although we are at perfect NIAH at 512k context, it is still different from real world use case. That's why we want to make sure people are aware that this is still experimental. \n\nThe main idea here is that, we want to show the first order feasibility - is the model efficient at decoding, and is it possible to train it? If so, then it's worth the effort to fine-tune it. Essentially proving some kind of scaling law so that we will continue to improve the context capabilities. ",
              "score": 12,
              "created_utc": "2026-02-06 19:35:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y6efr",
          "author": "Business-Weekend-537",
          "text": "Hopefully the unsloth guys see this and can work with you- then people could train longer context models at home.",
          "score": 7,
          "created_utc": "2026-02-06 18:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yetmc",
              "author": "Sad-Size2723",
              "text": "Haha, we haven't proved ourselves yet, there is no way they will look into this, unless this is proven to be useful...",
              "score": 3,
              "created_utc": "2026-02-06 19:32:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yvqf1",
                  "author": "Business-Weekend-537",
                  "text": "Don‚Äôt sell yourself short- if you had a tutorial on how to use this with a 3090 I‚Äôd try it in a heartbeat.",
                  "score": 7,
                  "created_utc": "2026-02-06 20:56:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40a6ip",
          "author": "Prestigious_Thing797",
          "text": "It would be good to see more quality benchmarks of this relative to the baseline model and other methods. There's a lot of different more efficient attention mechanisms (several referenced in your paper) but the drawback with all of them has been that they perform worse than standard attention, which has lead to the modern mixing of linear and full attention in models like the one you used, Qwen3-Next and so on.\n\nThe only benchmark given (NIAH) is not so common these days because practically all models perform well on it. You probably won't train up a new model from scratch that is competitive with models on benchmarks people really use- but you can randomly init different layers (all linear, mixed in superlinear, full attention) train each under the same regime and then compare the performance on a set of benchmarks across the three.\n\nAs of right now- this paper doesn't really demonstrate any hard evidence of benefit over using a standard linear attention layer.",
          "score": 3,
          "created_utc": "2026-02-07 01:33:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o470fxs",
              "author": "Sad-Size2723",
              "text": "Thanks for the comment. Yeah for standard attention, it is just so optimized (hardware + software + capital) that pretty much any other method will lose on either on speed or quality, if not both. There is a reason why Minimax is giving up on linear attention. \n\nAnd this is exactly the problem here, for any method to work, it has to beat Flash Attention by a large margin, even a 2x to 5x gain is likely not enough, because any subquadratic method will have to give away some quality, and in most case its not enough to compensate for the speedup. \n\nThis leads to the point of the paper here - we want to demonstrate that it can beat Flash Attention on speed and maintain the ability to attend to any tokens if needed and it is trainable. Is this method feasible? \n\nI agree with you on the lack of benchmarks here. I treat this as a feasibility study. A fair comparison on benchmarks will require comparable training as full attention models, which requires astronomical resources. I am taking a more qualitative path here rather than quantitive - at least on my offline tests, it performs comparable to standard attention on short context, and it is able to maintain reasoning chains up to tens of thousands of tokens. But yeah, maybe I should add some standard benchmarks to the paper just so that we know it is better than linear attention...",
              "score": 2,
              "created_utc": "2026-02-08 03:34:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o426xla",
          "author": "Individual_Spread132",
          "text": ">  What would you actually use long context for?\n\nGathering data on fictional characters; basically, dumping an entire script of a game / book into the chat. I've already attempted it with the baseline Nemotron-3-Nano-30B-A3B but it hallucinated a bit (nonetheless it was impressive). I wonder if it's going to be better with this new version!",
          "score": 3,
          "created_utc": "2026-02-07 10:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o474kbc",
              "author": "Sad-Size2723",
              "text": "Right now I am focusing on extending the context length,  in terms of quality it is likely not at the same level as the base model yet. This is something we will definitely focus on next",
              "score": 1,
              "created_utc": "2026-02-08 04:01:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3y77tc",
          "author": "botirkhaltaev",
          "text": "Man this looks cool will check it out",
          "score": 2,
          "created_utc": "2026-02-06 18:55:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zye6e",
          "author": "smflx",
          "text": "Good to know such long context possible. I'm interested in building a model for creative writing with very long context. Definitely I will read your paper. Thanks for sharing.",
          "score": 2,
          "created_utc": "2026-02-07 00:23:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40byz3",
          "author": "Alarming_Bluebird648",
          "text": "that scaling curve at 10m context is actually wild. i've been looking for a subquadratic approach that works on existing weights like nemotron. ngl the inference speed staying that high is the real infrastructure win here.",
          "score": 2,
          "created_utc": "2026-02-07 01:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o471l6f",
              "author": "Sad-Size2723",
              "text": "You remind me of an important point that I didn't mention in the paper. Even at 1M, the per query computation is still dominated by the MoE layers and the Mamba layers, which means that even though our attention layer is of O(L\\^0.5) at decoding time, the actual scaling exponent is smaller than 0.5, that's why the scaling curve remains pretty flat even at 10M. ",
              "score": 1,
              "created_utc": "2026-02-08 03:41:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o41ub5b",
          "author": "rulerofthehell",
          "text": "This is great work!! Curious how this is different than Log Linear Attention? It‚Äôs so promising!! I was trying with subquadratic attention with much smaller models (1Bish), good to see this side of research!\n\nFeel like something like this in combination with Deepseek Engram like paper can really bring local LLMs to the main stage in future",
          "score": 2,
          "created_utc": "2026-02-07 08:37:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o472gtd",
              "author": "Sad-Size2723",
              "text": "Thanks! For log linear attention there have been many implementations in the literature, but if you follow the multi-step approach in the paper using binary or k-ary search, then you can achieve log linear attention too. However, at this point I don't recommend it because I think log linear scaling is too aggressive that the loss in quality is not worth it. It is already very fast at O(L\\^1.5) now, we would probably focus more on quality next than speed. ",
              "score": 1,
              "created_utc": "2026-02-08 03:47:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40ide8",
          "author": "Inevitable-Jury-6271",
          "text": "This is a really cool release ‚Äî especially the ‚Äú10x context only ~30% decode hit‚Äù part.\n\nIf you want to make it easier for folks to compare apples-to-apples, a couple eval/reporting ideas that would help a ton:\n\n- Baseline vs superlinear: same weights / same data regime, and swap (a) full attn, (b) hybrid linear+full, (c) hybrid linear+superlinear, then run a small battery (MMLU-ish, GSM, HumanEval, etc.) + long-context (beyond NIAH) so we see the quality/latency trade.\n- Long-context *usefulness* tests: multi-doc QA with adversarial distractors + ‚Äúneedle at random‚Äù at multiple positions + retrieval-style tasks.\n- Memory accounting: KV cache bytes/token @ 1M and 10M + what‚Äôs resident vs streamed.\n\nAlso: do you have any intuition yet on whether routing errors are the main failure mode at very long ctx (vs. general degradation from training data)?",
          "score": 1,
          "created_utc": "2026-02-07 02:23:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47472b",
              "author": "Sad-Size2723",
              "text": "Hey, thanks for the comment. I have done some simple tests locally like GSM8k and Math500, and the results are pretty good. The interesting thing is, the original Nemotron 3 paper didn't show the benchmarks on these, I guess they are too simple for a 30B model? But for harder math problems, the model is able to generate coherent reasoning chains over 30k tokens to come up with the right answer, so I am not too worried about the basic LLM performance, but yeah, I do need to find the time to show these benchmarks, because it seems like people do care about these numbers.\n\nI actually spent most of my time on the harder problem of extending the context capability of the model. I was able to push the perfect NIAH context from 256k last week to 512k this week, and my goal is to get to 1M before running other tests, but since I am doing long context training, it should be able to generalize to other similar tests.\n\nAnd yeah, routing is definitely the biggest problem, because after selecting the spans it is just standard attention. The router is actually very complicated and since it doesn't come up with the base model, we will have train it with a lot of data. Maybe there is a better way to train it, like the lightening indexer in DeepSeek v3.2, or other block based architectures. ",
              "score": 1,
              "created_utc": "2026-02-08 03:59:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4a6g29",
          "author": "Dravodin",
          "text": "Thanks for the work. This is something genuinely interesting to me after a much gap. Will be checking it out.",
          "score": 1,
          "created_utc": "2026-02-08 17:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y2wa0",
          "author": "Ok-Buffalo2450",
          "text": "When vllm or other inference engine support pls?",
          "score": -1,
          "created_utc": "2026-02-06 18:35:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfl54",
              "author": "Sad-Size2723",
              "text": "Not in the near term, since this is still experimental...",
              "score": 3,
              "created_utc": "2026-02-06 19:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}