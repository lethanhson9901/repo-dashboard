{
  "metadata": {
    "last_updated": "2025-12-30 19:27:28",
    "time_filter": "week",
    "subreddit": "LocalLLaMA",
    "total_items": 50,
    "total_comments": 1162,
    "file_size_bytes": 1424904
  },
  "items": [
    {
      "id": "1pvpkqo",
      "title": "I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA",
      "subreddit": "LocalLLaMA",
      "url": "https://v.redd.it/u1mxlc3lof9g1",
      "author": "CeFurkan",
      "created_utc": "2025-12-25 23:21:39",
      "score": 973,
      "num_comments": 177,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvyt9zz",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-26 03:05:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxwh95",
          "author": "No-Refrigerator-1672",
          "text": "It is already mainstream in China. At this moment, Alibaba has doubled up 2080Ti, 3080, 4080, 4090 and 5090, with prices ranging from $300 for 2080Ti 22GB to $4000 for 5090 96gb, and they are ready to ship in any quantities on short notice.",
          "score": 248,
          "created_utc": "2025-12-25 23:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxy4lo",
              "author": "FullstackSensei",
              "text": "How's the driver situation under Linux? Works with mainstream/official drivers or needs any patched/special blobs?",
              "score": 73,
              "created_utc": "2025-12-25 23:38:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy1vz2",
                  "author": "robogame_dev",
                  "text": "This is the most critical question - I'd be worried about official firmware/drivers being updated to brick the card, and unofficial firmware/drivers containing backdoors.",
                  "score": 87,
                  "created_utc": "2025-12-26 00:02:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyq1es",
                  "author": "Kqyxzoj",
                  "text": "Last time I checked the firmware/drivers situation was suboptimal, and the only reason why I haven't butchered any GPUs in the quest for more VRAM.",
                  "score": 6,
                  "created_utc": "2025-12-26 02:43:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzo8x9",
                  "author": "No-Refrigerator-1672",
                  "text": "Works with official drivers with no patches ynder both Linux and Windows.",
                  "score": 4,
                  "created_utc": "2025-12-26 07:13:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw06hhi",
                  "author": "debackerl",
                  "text": "Most people agree that Windows piracy in the 90s and 00s, contributed to Microsoft's success. It was better for them if people had a pirated Windows rather than Linux.\n\nSame with NVIDIA, they make most of their money with companies, which aren't going to buy those hacked cards anyway. But all hobbyists contributing to the ecosystem can use the Chinese cards.",
                  "score": 5,
                  "created_utc": "2025-12-26 10:22:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzathn",
                  "author": "Embarrassed-Way-1350",
                  "text": "Nvidia hired the linux driver maintainer, it's good and stable so far. As far as a memory upgrade goes it has less to do with driver support and more to do with VBIOS ( they don't call it VBIOS anymore though) a.k.a gpu firmware. Most GPUs work with upgraded memory straight out of the box, nothing rocket science there.",
                  "score": 2,
                  "created_utc": "2025-12-26 05:15:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyqg3j",
              "author": "David_Delaune",
              "text": "I was able to purchase a modified board from the guys over at /r/NVIDIA_SXM2PCIE/ about a year ago but the three times I posted about it, it was deleted. Appeared to be deleted by Reddit administrators, not moderators.",
              "score": 22,
              "created_utc": "2025-12-26 02:45:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy59p2",
              "author": "Evening_Ad6637",
              "text": "Are you sure it's really only $4,000 for the 5090 96 GB?\n\n\nIf that's true, it would be an incredible deal.\n\n\nDo you have a link or a contact or something? I have a Chinese friend who could help me get one or two cards. I just need to know who to contact.",
              "score": 30,
              "created_utc": "2025-12-26 00:23:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzb7d8",
                  "author": "Embarrassed-Way-1350",
                  "text": "Yes pretty sure, I saw a couple of them in Shenzhen at Huaqiangbei.",
                  "score": 5,
                  "created_utc": "2025-12-26 05:18:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzp4nj",
                  "author": "No-Refrigerator-1672",
                  "text": "Here's the [first result](https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html?spm=a2700.galleryofferlist.normal_offer.d_title.133e13a0kMSVHn&priceId=81a441d024ed4ced8c0ee0b3b74575e7) from alibaba search. I haven't bought them; but I did buy 3080 20gb. So I can conclude that the card exists, and that it will be shipped to you, but I can't say anything about quality. Regarding the price, $4000 is the price in China, tou'll have to pay import tax of your country on top of that; and the delivery fee from alibaba is typically $80 to Europe.",
                  "score": 7,
                  "created_utc": "2025-12-26 07:22:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvy9kq5",
                  "author": "HumanDrone8721",
                  "text": "Sadly there is no such thing, that is there is no 5090 with 96GB and it could not be 4 or even 5000USD. The biggest issue here is the VRAM, and Nvidia has a perfect stronghold on it, the only way to get DDR7 VRAM is to cannibalize THREE regular 5090 and this makes it not economically feasible compared with a 6000 Pro.\n\nEDIT: Sorry to be the bearer of bad news, but truth is truth, rage down voting it will not fix anything. But please do if it makes you feel a bit better.",
                  "score": -15,
                  "created_utc": "2025-12-26 00:50:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvznay0",
                  "author": "Novel-Mechanic3448",
                  "text": "It doesn't exist lol",
                  "score": -3,
                  "created_utc": "2025-12-26 07:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvy8cr3",
              "author": "Rough-Winter2752",
              "text": "I'd gladly drop that much for basically a 6000 Blackwell PRO. Any links, OP? And any sightings of these 128 GB 5090s?  I have a dream of one day running GLM 4.7 Q8 locally!",
              "score": 7,
              "created_utc": "2025-12-26 00:42:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvysl2v",
                  "author": "PentagonUnpadded",
                  "text": "A legit blackwell pro is $8500 ish in the US. I'll admit a half price version is tempting. Though with the lack of warranty, tariffs and the time cost of supporting an unofficial product isn't as screaming of a deal.",
                  "score": 11,
                  "created_utc": "2025-12-26 03:00:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzosfx",
                  "author": "No-Refrigerator-1672",
                  "text": "I did not buy them myself; I only had dealt eith 3080 20gb, so I cannot offer you anything better that thr very [first lot](https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html?spm=a2700.galleryofferlist.normal_offer.d_title.133e13a0kMSVHn&priceId=81a441d024ed4ced8c0ee0b3b74575e7) that comes up in the search.",
                  "score": 2,
                  "created_utc": "2025-12-26 07:18:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxwnnt",
              "author": "CeFurkan",
              "text": "damn. saldy i can't buy individually in here.",
              "score": 7,
              "created_utc": "2025-12-25 23:28:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxx4fe",
                  "author": "No-Refrigerator-1672",
                  "text": "For GPUs, most of the sellers have minimum quantity of 2. As long as you're buying a pair for workstation, or can find one buddy to team up, you can only worry about import taxes.",
                  "score": 16,
                  "created_utc": "2025-12-25 23:31:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyin36",
                  "author": "mycall",
                  "text": "Jump on an airplane",
                  "score": 3,
                  "created_utc": "2025-12-26 01:52:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fecq",
              "author": "Mikasa0xdev",
              "text": "Yo, China is already running 96GB 5090s. lol",
              "score": 2,
              "created_utc": "2025-12-26 15:56:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ko3h",
              "author": "Technical_Ad_440",
              "text": "i have seen them but can they even be trusted to buy them at that size without getting a dead gpu or just a normal 5090 that cost more?\n\nhmm seems like they may actually be very base price but they dont include tax or import fees once you add all that you will probably end up at the same prices anyways",
              "score": 2,
              "created_utc": "2025-12-26 22:51:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy8lhr",
              "author": "LocoMod",
              "text": "Has there been any serious analysis done on those cards to make sure that's the only modification being made? For tinkering this seems fine. But it seems like a security nightmare for anything beyond that. I'd love to see if anyone has done a deep dive and made sure nothing shady is going on. Especially packet captures to make sure nothing is dialing back to some mothership.",
              "score": 3,
              "created_utc": "2025-12-26 00:44:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzojwb",
                  "author": "No-Refrigerator-1672",
                  "text": "I didn't see such analysis; but I believe that there's nothing to worry about. Those cards work with default Nvidia drivers, which means that the only potentially modified code is in vbios, which has no internet access and therefore can't possibly be an attack vector.",
                  "score": 5,
                  "created_utc": "2025-12-26 07:16:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyk9mb",
                  "author": "AlwaysLateToThaParty",
                  "text": "I don't really think 'security' is the issue.  I'd be more worried about fire.  Who knows whether the other components of the board will be able to handle the extra current requirements?",
                  "score": 5,
                  "created_utc": "2025-12-26 02:03:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw081a5",
                  "author": "a_beautiful_rhind",
                  "text": "Considering the only ones with the patched bios can't even fix the rebar issue, I think you're gonna be pretty safe. Everything else is IIRC a hardware mod.",
                  "score": 2,
                  "created_utc": "2025-12-26 10:38:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzlxfm",
              "author": "Novel-Mechanic3448",
              "text": "And they don't work",
              "score": -5,
              "created_utc": "2025-12-26 06:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzo6o6",
                  "author": "No-Refrigerator-1672",
                  "text": "False. I have a pair of them myself.",
                  "score": 8,
                  "created_utc": "2025-12-26 07:12:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy1x1j",
          "author": "Aggressive-Bother470",
          "text": "Where are the 96GB cards for $4000?\n\n\nThe 4090 48s were listed for ¬£2500 and now they're over ¬£3k.",
          "score": 53,
          "created_utc": "2025-12-26 00:02:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvya3md",
              "author": "HumanDrone8721",
              "text": "There isn't any such thing, the only existing cards are 48GB 4090 and after getting trough the customs they're close to 4000EUR here IF the customs doesn't confiscate them because they lack CE certification or similar BS.",
              "score": 9,
              "created_utc": "2025-12-26 00:53:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzpt9a",
                  "author": "gnaarw",
                  "text": "You have to go to Shenzhen yourself to buy them. As long as you only get one, you can press them into a mobile GPU box for laptops and can wiggle yourself out of any conversation with customs.... Or so I heard ü§∑üèº‚Äç‚ôÇÔ∏è",
                  "score": 13,
                  "created_utc": "2025-12-26 07:29:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy0kfi",
          "author": "sweetnuttybanana",
          "text": "3 cents per hour??? Where do i sign up",
          "score": 33,
          "created_utc": "2025-12-25 23:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvz69fz",
              "author": "ANR2ME",
              "text": "Does it even covers the electricity billü§î",
              "score": 10,
              "created_utc": "2025-12-26 04:39:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzpy6s",
                  "author": "gnaarw",
                  "text": "kWh is 7-8 USD cents there so it's about the electricity cost with tiny margin...",
                  "score": 10,
                  "created_utc": "2025-12-26 07:30:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0azo3",
              "author": "Final-Rush759",
              "text": "Probably next to the hydro or solar farm where electricity costs almost nothing.  But it won't be for too long.  The data centers are being built next to these electricity farms.",
              "score": 5,
              "created_utc": "2025-12-26 11:08:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0ge3f",
              "author": "swiss_aspie",
              "text": "Right? I would rent a few instantly",
              "score": 3,
              "created_utc": "2025-12-26 11:59:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0tl2b",
                  "author": "sweetnuttybanana",
                  "text": "I knoww. I'm looking around for cloud GPUs to do some research work and this would be an absolute blessing.",
                  "score": 4,
                  "created_utc": "2025-12-26 13:44:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzizgn",
          "author": "RogueStargun",
          "text": "Shit I have a $300 hot air gun for desoldering, but no fucking way am I putting it to a 5090. This is a surgical level operation",
          "score": 16,
          "created_utc": "2025-12-26 06:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0ipio",
              "author": "exceptioncause",
              "text": "you can train on 4090 first :)  \nquite thorough video about the process and some budget calculations  \n[https://www.youtube.com/watch?v=3YiJovZRUv0](https://www.youtube.com/watch?v=3YiJovZRUv0)",
              "score": 5,
              "created_utc": "2025-12-26 12:20:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzn37j",
              "author": "Novel-Mechanic3448",
              "text": "\\>void the warranty on a 3000 dollar card which breaks it  \n\\>have to buy another one  \n\\>you've now spent enough for a 96gb card in the first place\n\nWhat's the point lol",
              "score": 6,
              "created_utc": "2025-12-26 07:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0505v",
                  "author": "dandanua",
                  "text": "adrenaline",
                  "score": 18,
                  "created_utc": "2025-12-26 10:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvz772h",
          "author": "holchansg",
          "text": "3 cents per hour? WHERE?",
          "score": 14,
          "created_utc": "2025-12-26 04:46:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzvai5",
              "author": "sweetnuttybanana",
              "text": "I know right?? I'd be doing my whole thesis with those babies",
              "score": 4,
              "created_utc": "2025-12-26 08:25:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyb579",
          "author": "Heathen711",
          "text": "I'm running the modded 4090 with 48GBs of memory, no issues. I actually just bought two more for a second rig, to get faster processing but the same memory as a L40s.\n\nI'm surprised this is such new news to some people as vram requirements have been high for a while...",
          "score": 37,
          "created_utc": "2025-12-26 01:00:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvybo2o",
              "author": "Vegetable-Score-3915",
              "text": "Where did you buy them? How much for?",
              "score": 10,
              "created_utc": "2025-12-26 01:04:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyeq2o",
                  "author": "Heathen711",
                  "text": "Ebay, the first one was 3.2k a while ago. The last two I bought were 3.8k each. They are the server blower style, not the triple fan type, so you need the right server to support them.\n\n```\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n| 30%   29C    P8             24W /  450W |   41117MiB /  49140MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n```",
                  "score": 19,
                  "created_utc": "2025-12-26 01:25:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvygqv0",
                  "author": "KadahCoba",
                  "text": "I have one too. Got it local but that was a fluke. You'll have to import from China or get lucky and find one that somebody else already imported.\n\nOr buy the upgrade board from a seller in the UK and do it yourself.\n\nOr live in China and take your stock 4090 to any number of shops that do this upgrade.\n\nI have a bunch of stock 4090's I'd love to have the 48GB upgrade on but its too expensive currently and other options are a bit more cost effective for us.",
                  "score": 5,
                  "created_utc": "2025-12-26 01:39:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyf1mf",
              "author": "old97ss",
              "text": "Yeah, dm where please",
              "score": 2,
              "created_utc": "2025-12-26 01:27:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyr330",
              "author": "NoahFect",
              "text": "And the standard Nvidia drivers are fine with this?  I'm a little surprised they don't throw a flag on the play when they see a GPU with more memory than expected.",
              "score": 1,
              "created_utc": "2025-12-26 02:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyvc2q",
                  "author": "Heathen711",
                  "text": "Yeah, see my nvidia-smi output in the thread under",
                  "score": 3,
                  "created_utc": "2025-12-26 03:19:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy4on6",
          "author": "CertainlyBright",
          "text": "5090 isn't upgraded yet lol",
          "score": 17,
          "created_utc": "2025-12-26 00:19:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyeim5",
              "author": "HumanDrone8721",
              "text": "Let the people dream about it, is Christmas ;). I was so excited some month ago when a youtuber announced that he \"bought\" from Alibaba a modified 5090, I even went trough the trouble of tracking down the vendor from one or two uncensored frames and after long discussions they've mentioned that they only consider doing it and wanted to see if there is interest, I've told them that I'll order 4 immediately when they're available and never heard from them back. The clamp is really tight on the necessary VRAM.",
              "score": 10,
              "created_utc": "2025-12-26 01:23:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw09vbs",
          "author": "cryptodiemus",
          "text": "Where do i rent for 3c/h ?! The cheapest one i found was a out 17c.",
          "score": 6,
          "created_utc": "2025-12-26 10:56:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0hjxu",
              "author": "swiss_aspie",
              "text": "Where did you find 17c ?",
              "score": 4,
              "created_utc": "2025-12-26 12:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2zckz",
                  "author": "cryptodiemus",
                  "text": "Gpuhub, i think its 19 now",
                  "score": 2,
                  "created_utc": "2025-12-26 20:54:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvy06s4",
          "author": "Icy-Swordfish7784",
          "text": "Well, if the robots are coming maybe they can handle it.ü§∑",
          "score": 7,
          "created_utc": "2025-12-25 23:51:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy1z4p",
              "author": "CeFurkan",
              "text": "True",
              "score": 8,
              "created_utc": "2025-12-26 00:02:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvywczk",
          "author": "Ok-Yesterday-4140",
          "text": "is this for real did anyone do this and succeed",
          "score": 4,
          "created_utc": "2025-12-26 03:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0xdkq",
          "author": "NotQuiteDeadYetPhoto",
          "text": "Having 'paid' nvidia to include driver updates / kernel access on the linux side (and allegedly they put it in the windows side) if they can get anything for free they'll do it if it increased their market share. We weren't big but 16,000 high end graphics cards did get their attention for a purchase and they (sales) worked very hard to get us accomodated.",
          "score": 3,
          "created_utc": "2025-12-26 14:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsmt2",
          "author": "kingwhocares",
          "text": "There is no 4GB memory chip and thus the 128GB RTX 5090 is BS. The max it can get to is 96GB and with GDDR7 being in short supply, you aren't getting it in spot market.",
          "score": 7,
          "created_utc": "2025-12-26 07:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw04pcf",
          "author": "Temporary-Sector-947",
          "text": "In Russia, we can get in by 320 rub (waterblock version)\n\nI have two if it for my custom loop",
          "score": 3,
          "created_utc": "2025-12-26 10:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05ssf",
              "author": "CeFurkan",
              "text": "Wow nice",
              "score": 2,
              "created_utc": "2025-12-26 10:15:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0uua5",
          "author": "svbjjnggthh",
          "text": "3cent per hour rental? Where",
          "score": 3,
          "created_utc": "2025-12-26 13:52:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzepmo",
          "author": "aimark42",
          "text": "I don't think we are there for GPU's yet, but these new SoC/APU (Strix Halo, GB10) systems could be built using CAMM or SOCAMM memory modules.  It would add more cost, so I doubt they would.",
          "score": 2,
          "created_utc": "2025-12-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw20vpy",
              "author": "Freonr2",
              "text": "Framework said they couldn't get the speeds without soldering the memory right to the board.  I'm not sure it is cost but signal integrity to reach the 8000MT/S data rate.",
              "score": 2,
              "created_utc": "2025-12-26 17:50:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2327z",
                  "author": "aimark42",
                  "text": "That was likely true when Framework designed the board.  But SOCAMM can do 8533 MT/s and the next-gen SOCAMM 2 pushing to 9600 MT/s.  We should see some SOCAMM (1) devices at CES.",
                  "score": 0,
                  "created_utc": "2025-12-26 18:01:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzm5g2",
          "author": "Aeroxin",
          "text": "Hello wonderful person, it's Anton...",
          "score": 2,
          "created_utc": "2025-12-26 06:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzp1pa",
          "author": "KomithErr404",
          "text": "I bet they gonna make it way harder to do this with their next gen gpus",
          "score": 2,
          "created_utc": "2025-12-26 07:21:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0qt9e",
          "author": "martinerous",
          "text": "That's also \"green thinking\" to reuse components. Western companies often like to market themselves of being \"green and environment friendly\", but that's a hypocrisy and greenwashing if they do not truly support reuse of components and rights to repair.",
          "score": 2,
          "created_utc": "2025-12-26 13:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy6kt4",
          "author": "Hibikku7",
          "text": "I heard that these modified Chinese GPU's can catch fire or break with a single update.\n\nIs it nvidia propaganda or am i stupid?",
          "score": 5,
          "created_utc": "2025-12-26 00:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvydrhv",
              "author": "HumanDrone8721",
              "text": "Is a bit of both: the modified 4090, the only ones that are worth pursuing are having server turbine blowers, like FE variants, actually all of them have such a setup, because they were initially commissioned by the Chinese army as an workaround against the embargo. They are to be used in extremely well ventilated and forced cooling datacenters, putting them in the miserable \"gamerz\" cases with glass panels and LED infested anemic coolers will kill them dead pretty fast. Also the power cable has to be of the most exquisite quality, the usual thin flexible garbage looks nice, but is just high electrical resistance crap that will burn at sustained 6-700W. Finally the Chinese are masters of cutting corners and their Alibaba customers are not the army, but some \"round eyes\" western suckers that are seen as walking wallets to be emptied, so all unstable rejects are dumped there. So no proper thermal management on these, if you ever get one of them this is the first thing to be redone.\n\nBecause of all the above the market for the modified cards is minuscule and doesn't worry Nividia enough to warrant serious investment in disabling them in drivers, or permanently in HW, like FTDI did with the USB to serial fake chips. They are more seen as a gateway to the \"real\" deal. Of course this has to be kept under control, so they do pay some \"social media influencers\" companies to pepper a bit of FUD, like youtubers \"testing\" them in the most inept way and concluding \"is not worth\" and one line juniors posting even in this sub: \"yeah, I've got one and died on me and I've tried to send it back and they won't refund...\". Of course, they never respond again or God forbid, post pictures of their setup.\n\nAll this being said, if one has some disposable income, appetite for risk and proper technical skills, they may prove a good investment.",
              "score": 13,
              "created_utc": "2025-12-26 01:18:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyzvls",
                  "author": "cantgetthistowork",
                  "text": "The blower cards are a god sent for stacking cards and my 14x3090 rig was exclusively built with blower cards. They were really hard to find.",
                  "score": 3,
                  "created_utc": "2025-12-26 03:51:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0asf6",
                  "author": "debackerl",
                  "text": "Ran mine for several months non-stop at 310W without a single issue. I ran at lower power because you spend a lot of power for the last 10% of performance. BTW, my official DELL RTX 3090 also had a blower, it's just better to output all the heating straight out of the case.",
                  "score": 2,
                  "created_utc": "2025-12-26 11:06:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw20ki4",
              "author": "Freonr2",
              "text": "I've seen several reports hey have weird quirks like clock speed modulation (sometimes don't clock themselves up and down  properly) and not great fan speed control, but nothing about catching fire.  \n\nMaybe someone did something trying to tune fan/clocks and overheated?",
              "score": 2,
              "created_utc": "2025-12-26 17:48:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvybrgs",
              "author": "Jonodonozym",
              "text": "Same applies to normal GPUs to be fair. Without the numbers it's either propaganda or legal ass-covering.",
              "score": 3,
              "created_utc": "2025-12-26 01:04:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy72n9",
          "author": "Techngro",
          "text": "The real question is, can you take RAM from older GPUs that you can get for cheap and do the same thing? If you're willing to deal with lower memory bandwidth, you could end up with a 64GB RX Vega 64 (or a 1080ti). Not everyone can afford a 64GB RTX 5090.",
          "score": 5,
          "created_utc": "2025-12-26 00:34:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy8tdd",
              "author": "Freonr2",
              "text": "No you can't.",
              "score": 5,
              "created_utc": "2025-12-26 00:45:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyduwr",
              "author": "HumanDrone8721",
              "text": "No, for both technical and economical grounds.",
              "score": 1,
              "created_utc": "2025-12-26 01:19:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzk3lu",
          "author": "__JockY__",
          "text": "96GB 5090s my ass.",
          "score": 2,
          "created_utc": "2025-12-26 06:35:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzm7s2",
              "author": "Novel-Mechanic3448",
              "text": "Yeah it's total bullshit. Imagine if it was true? It would disrupt the entire market overnight.\n\nI believe they're real, I don't believe they work are reliable and are safe to use though.",
              "score": -1,
              "created_utc": "2025-12-26 06:54:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw08bmk",
                  "author": "a_beautiful_rhind",
                  "text": "Just like the 4090s did? The price comes out only so-so because of the labor and ram.",
                  "score": 3,
                  "created_utc": "2025-12-26 10:41:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwi8gus",
          "author": "salary_pending",
          "text": "3 cents per hour sounds cap",
          "score": 1,
          "created_utc": "2025-12-29 06:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2zp23",
          "author": "corysus",
          "text": "I hope China will put an end to the extremely high prices of GPUs, and now even RAM memory as well. This situation is truly unrealistic. On top of that, AI farms consume more electricity than some countries, which is simply not acceptable. Considering how much money NVIDIA is making these days, the company should be doing far more in terms of real innovation and efficiency.",
          "score": 1,
          "created_utc": "2025-12-26 20:56:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7px8n",
              "author": "Sex_Offender_4697",
              "text": "\"some countries\" produce nothing",
              "score": 1,
              "created_utc": "2025-12-27 16:45:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw30nux",
              "author": "CeFurkan",
              "text": "100% i am waiting that day",
              "score": 0,
              "created_utc": "2025-12-26 21:01:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyynqg",
          "author": "CrazyWombatayu",
          "text": "make it 96GB vram and you have a deal",
          "score": 1,
          "created_utc": "2025-12-26 03:43:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsdb6",
          "author": "reddit_wisd0m",
          "text": "Any warranty on those modded cards? Otherwise, nah",
          "score": 1,
          "created_utc": "2025-12-26 07:54:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puyq9r",
      "title": "Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record",
      "subreddit": "LocalLLaMA",
      "url": "https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html",
      "author": "fallingdowndizzyvr",
      "created_utc": "2025-12-24 22:14:48",
      "score": 666,
      "num_comments": 150,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/",
      "domain": "cnbc.com",
      "is_self": false,
      "comments": [
        {
          "id": "nvtcbgl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 02:45:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsbn0x",
          "author": "sourceholder",
          "text": "Great, more consolidation.\n\nIs Cerebras next?",
          "score": 273,
          "created_utc": "2025-12-24 22:22:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvseil1",
              "author": "freecodeio",
              "text": "if anyone is a threat to this industry is cerebras so I'm surprised it's still not happened",
              "score": 81,
              "created_utc": "2025-12-24 22:41:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvspj4w",
                  "author": "dodiyeztr",
                  "text": "Makes you wonder who the investors are",
                  "score": 22,
                  "created_utc": "2025-12-24 23:56:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvto2in",
                  "author": "tassa-yoniso-manasi",
                  "text": "cerebras when you talk about inference on 70B models: ü•∞\n\ncerebras when you ask them to stuff more than 44GB of memory per chip: ¬†ü´•",
                  "score": 31,
                  "created_utc": "2025-12-25 04:16:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvunxsv",
                  "author": "amapleson",
                  "text": "No way, that company is run so poorly, something is wrong in the culture",
                  "score": 5,
                  "created_utc": "2025-12-25 10:06:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtdm4u",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 5,
                  "created_utc": "2025-12-25 02:54:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsbwxr",
          "author": "john0201",
          "text": "This can only mean good things for a healthy competitive market",
          "score": 500,
          "created_utc": "2025-12-24 22:24:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvscemw",
              "author": "CYTR_",
              "text": "We're cooked lmao",
              "score": 168,
              "created_utc": "2025-12-24 22:27:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsle2s",
                  "author": "exaknight21",
                  "text": "We‚Äôre grilled.",
                  "score": 36,
                  "created_utc": "2025-12-24 23:27:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvswdoz",
                  "author": "DangKilla",
                  "text": "It‚Äôs a good thing for gamers. More TPU‚Äôs less GPU‚Äôs",
                  "score": -1,
                  "created_utc": "2025-12-25 00:44:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt4wyn",
              "author": "BusRevolutionary9893",
              "text": "Don't worry. Hopefully China has entered the US market by this time next year.¬†",
              "score": 14,
              "created_utc": "2025-12-25 01:49:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtsatq",
                  "author": "Fortyseven",
                  "text": "I'm sure they'll be banned for \"security concerns\".",
                  "score": 17,
                  "created_utc": "2025-12-25 04:50:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxhmwp",
              "author": "Gloomy_Nebula_5138",
              "text": "It‚Äôs not just harming the market by trying to avoid antitrust, but it is also a betrayal of the Groq employees, who worked at low pay for a startup with the expectation that their ownership of options may mean something. This is a textbook Chamath scam deal. Founders leave, they and investors get lots of money, and the employees of Groq who did the work get nothing except their options in a zombie shell company left over from this acquisition ‚Ä¶ uh I mean ‚Äúlicensing deal‚Äù. Shame on Jensen and Nvidia for being a part of this.",
              "score": 3,
              "created_utc": "2025-12-25 21:53:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxrl9b",
                  "author": "john0201",
                  "text": "Yeah I just read up on that. This type of deal should trigger not just anti-trust laws but punitive penalties. Basically anyone worth anything moves to NVIDIA everyone else stays behind to keep up the ruse. \n\nI always get some solace knowing that if billionaires still want more they will never be happy.",
                  "score": 2,
                  "created_utc": "2025-12-25 22:55:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw20z3j",
                  "author": "WillingnessNearby371",
                  "text": "Why would the options be worth nothing? \n\nWouldn‚Äôt the 20B still be paid to the original corporation which would need to payout to all shareholders? AFAIK, the CEO / founders also own common stock generally. \n\nAsking since I also have stock options and my expectation is that in any kind of acquisition deal I would be paid out. \n\nAlso, Groq pays resonably well from what I‚Äôve seen. Not industry leader but the cash component is stuff pretty good.",
                  "score": 1,
                  "created_utc": "2025-12-26 17:50:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvu1999",
              "author": "Mikasa0xdev",
              "text": "Nvidia's monopoly grows, lol.",
              "score": 1,
              "created_utc": "2025-12-25 06:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvsou0",
              "author": "CuTe_M0nitor",
              "text": "We are F#$D",
              "score": 1,
              "created_utc": "2025-12-25 15:48:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0b0cd",
              "author": "BumblebeeParty6389",
              "text": "Competition? What's that?",
              "score": 1,
              "created_utc": "2025-12-26 11:08:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsejg1",
          "author": "Zeeplankton",
          "text": "I'm a bit shocked groq could possibly be worth 20b.",
          "score": 143,
          "created_utc": "2025-12-24 22:41:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsfi8l",
              "author": "SlowFail2433",
              "text": "The perf is real",
              "score": 65,
              "created_utc": "2025-12-24 22:47:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsmyht",
                  "author": "BambaiyyaLadki",
                  "text": "A guy I follow on Twitter (Irrational Analysis) had a post a while ago where he said that groqs TPU is actually not a very good product. His arguments are usually technically sound so I'm curious to learn more about why his arguments were wrong (or why Nvidia brought Groq despite the poor design).",
                  "score": 62,
                  "created_utc": "2025-12-24 23:38:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsvl1q",
                  "author": "SpiritualWindow3855",
                  "text": "The perf is the opposite of real: most models they deploy are degraded vs reference implementations, Blackwell is a showcase on how NVIDIA can still do specialization for inference and kick ass, and they were still struggling to provide real production levels of access of the product to customers.\n\nThey were almost certainly bought for their technical team, and the product will mostly be picked through then left to languish.",
                  "score": 14,
                  "created_utc": "2025-12-25 00:39:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvuns27",
                  "author": "Zeeplankton",
                  "text": "But only because they're using highly customized hardware and highly modified models, no? That's why so few are supported. There's a reason that barely anyone actually supports groq as an endpoint. But maybe 20b is a rounding error for nvidias 4T valuation.",
                  "score": 1,
                  "created_utc": "2025-12-25 10:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsnly2",
              "author": "genshiryoku",
              "text": "Their inference performance is crazy good and if you believe inference is going to be the lion share of FLOP expenditure then it's a no-brainer move.\n\nThis is Nvidia buying up future competition before they got completely wiped out by them.",
              "score": 28,
              "created_utc": "2025-12-24 23:43:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx44sk",
                  "author": "Fit-Support4910",
                  "text": "Depends how you define \"performance.\"\n\nFrom an end user's perspective? Sure, latency and throughput are all that matter, and Groq's numbers look great.\n\nBut from a cloud provider's perspective, you also care about resource utilization and cost efficiency. Groq's approach appears to be brute-forcing performance by ganging together a large number of chips just to fit the model in memory. The raw speed is impressive, but the compute utilization percentage is low. you're paying for a lot of silicon that's mostly idle. By that account, NVIDIA's current products are actually more compelling here.",
                  "score": 3,
                  "created_utc": "2025-12-25 20:30:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvswehq",
              "author": "AuspiciousApple",
              "text": "Nvidia is buying them to kill competition. They don't care whether it's the best value for money.",
              "score": 17,
              "created_utc": "2025-12-25 00:44:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsham3",
              "author": "Bozhark",
              "text": "Not Grok",
              "score": 17,
              "created_utc": "2025-12-24 22:59:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsphth",
              "author": "donald-bro",
              "text": "Does  groqs has many patents or know how that nvidia needed ?",
              "score": 3,
              "created_utc": "2025-12-24 23:56:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxkhlc",
              "author": "emteedub",
              "text": "Might be worth that much to them to eliminate.\n\nWhat else does one buy with their monopoly monies",
              "score": 2,
              "created_utc": "2025-12-25 22:10:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsk39p",
              "author": "No-Manufacturer6409",
              "text": "As in too much or too little?",
              "score": 2,
              "created_utc": "2025-12-24 23:18:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvsyg2k",
              "author": "Myg0t_0",
              "text": "Have u tried it? Fast as fuck boy!",
              "score": 2,
              "created_utc": "2025-12-25 00:59:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvguzn",
              "author": "harrro",
              "text": "It's not but for Jensen Huang/Nvidia, killing off competition is worth the 20 bill.",
              "score": 1,
              "created_utc": "2025-12-25 14:30:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtn09f",
              "author": "tassa-yoniso-manasi",
              "text": "this is a calculated move from Nvidia for edge computing in robots which will need extremely low latency & computational efficiency for them to become more than tiktok curiosities.\n\nthe 20bn is grossly inflated for the value it can bring today, but not for its value in 5/10 years.",
              "score": 1,
              "created_utc": "2025-12-25 04:08:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvt00c",
              "author": "CuTe_M0nitor",
              "text": "Wtf üòÇ are you talking about. Have you seen the token speed why can run an LLM on their hardware?! If feels like your are seeing 10years in to the future when using them. I was blown away that there is hardware that can do that today and that are cheaper than Nvidia.",
              "score": 0,
              "created_utc": "2025-12-25 15:49:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvvfad",
                  "author": "Zeeplankton",
                  "text": "Yeah but I'm pretty sure they're only able to do it through extremely bespoke hardware, and modifying existing models. That's why model support was / is so bare, and of the models they support, the models perform worse then full counterparts.",
                  "score": 1,
                  "created_utc": "2025-12-25 16:04:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsd6f8",
          "author": "GabryIta",
          "text": "Oh no",
          "score": 40,
          "created_utc": "2025-12-24 22:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvse020",
              "author": "JLeonsarmiento",
              "text": "I have a bad feeling about this‚Ä¶",
              "score": 14,
              "created_utc": "2025-12-24 22:37:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvv34pa",
              "author": "fauni-7",
              "text": "Anyway...",
              "score": 2,
              "created_utc": "2025-12-25 12:43:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsetyi",
          "author": "agentzappo",
          "text": "Another ‚Äúacquihire‚Äù example. No way in hell the regulators would allow Nvidia to outright purchase Groq, but they still get what they want and need out of this deal while leaving behind everyone else who joined a startup hoping to benefit from long-term scaling and success driven by the former founders",
          "score": 80,
          "created_utc": "2025-12-24 22:43:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdixf",
              "author": "Intelligent-Agent440",
              "text": "Don Jr is an investor in the company, regulators won't say a thing",
              "score": 30,
              "created_utc": "2025-12-25 02:54:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtyp3j",
              "author": "vcutrera",
              "text": "Are common stock holders getting paid out on this deal since they are only buying 'assets' and taking execs?",
              "score": 4,
              "created_utc": "2025-12-25 05:45:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvurv1b",
                  "author": "1998marcom",
                  "text": "I guess that would count as company earnings, so it's up to Groq's management what to do with that.",
                  "score": 3,
                  "created_utc": "2025-12-25 10:49:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv1xhw",
                  "author": "agentzappo",
                  "text": "Deals like these, the VCs get paid out but not nearly at the levels of return they aim for. It‚Äôs one reason why the VCs generally hate acquihire deals since it cuts them out of the massive potential upside on companies / founders where their bets paid off.",
                  "score": 3,
                  "created_utc": "2025-12-25 12:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwyhh1",
              "author": "ExaminationNo8522",
              "text": "This seems significantly worse from a regulatory standpoint though as it erodes trust in the very concept of equity. What‚Äôs the point of holding equity in a company if it gets pulled apart for parts?",
              "score": 4,
              "created_utc": "2025-12-25 19:55:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvstw78",
              "author": "ckkl",
              "text": "Lmao. It‚Äôs going through pal",
              "score": -2,
              "created_utc": "2025-12-25 00:27:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsem2e",
          "author": "AnonsAnonAnonagain",
          "text": "Great. We are cooked",
          "score": 30,
          "created_utc": "2025-12-24 22:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsfl9j",
          "author": "TheFrenchSavage",
          "text": "Oh no. I hope they don't pull the plug on GroqCloud, it was my source of free API calls for fun and not-monry-making projects!",
          "score": 24,
          "created_utc": "2025-12-24 22:48:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdp5f",
              "author": "Intelligent-Agent440",
              "text": "They said Groqcloud is not among the assets being acquired",
              "score": 11,
              "created_utc": "2025-12-25 02:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuj6e0",
                  "author": "TheFrenchSavage",
                  "text": "Wut? How does that work? The cloud service won't be running for free if they don't have access to free hardware right?",
                  "score": 2,
                  "created_utc": "2025-12-25 09:14:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt01mz",
              "author": "Ok-Adhesiveness-4141",
              "text": "My worry exactly, I use it for all my projects.",
              "score": 4,
              "created_utc": "2025-12-25 01:11:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtz8hy",
              "author": "zuraken",
              "text": "Every company that gets bought out usually turns off their free tier services, or makes them extremely ad intrusive and nearly unusable",
              "score": 1,
              "created_utc": "2025-12-25 05:50:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtzokp",
              "author": "lochyw",
              "text": "Cerberus is pretty good too fyi",
              "score": 1,
              "created_utc": "2025-12-25 05:54:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsq91m",
          "author": "ocassionallyaduck",
          "text": "Antitrust lawsuit needs to be placed like yesterday.",
          "score": 18,
          "created_utc": "2025-12-25 00:01:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvti65m",
              "author": "TheJpow",
              "text": "With this admin? Lol",
              "score": 14,
              "created_utc": "2025-12-25 03:30:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtjkie",
                  "author": "ocassionallyaduck",
                  "text": "With any ain in the last 30 years? Nah. Lol.",
                  "score": 0,
                  "created_utc": "2025-12-25 03:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvu3fn8",
              "author": "crantob",
              "text": "Why?  How much antitrust have you studied?  It's a clownshow.  \n\n\nAntitrust 'law' is based on the farcical chimera of 'perfect competition', which is a fantastical political construction shoved into the soft craniums of the college students under the guise of 'economic science'.",
              "score": 0,
              "created_utc": "2025-12-25 06:29:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsgjcj",
          "author": "whereismytralala",
          "text": "The FTC will explain how this is not a monopoly and, as soon as Nvidia buy a couple of billions of Trump coins, they don't see any problems.",
          "score": 44,
          "created_utc": "2025-12-24 22:54:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsskc9",
              "author": "lazytiger21",
              "text": "This happens a lot. I‚Äôve seen it happen at 3 companies that I worked with in the past. The ‚Äúlicensing fee‚Äù is enough that the acquired company continues to operate and they usually pivot to doing something slightly different, but they aren‚Äôt the same company.",
              "score": 12,
              "created_utc": "2025-12-25 00:18:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvua547",
              "author": "kingwhocares",
              "text": "They don't need to when Nvidia's competitor loves shooting themselves on the foot and is fine just selling SoCs to companies like Sony and Microsoft with little investment.",
              "score": 1,
              "created_utc": "2025-12-25 07:36:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtf49i",
              "author": "Randommaggy",
              "text": "Hopefully enough Epstein stuff has a botched release to have king cheeto and his co-conspirators impeached before it goes through and too much additional long term damage to the US economy is done.",
              "score": -2,
              "created_utc": "2025-12-25 03:06:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvthuil",
                  "author": "whereismytralala",
                  "text": "Don't hold your breath.",
                  "score": 9,
                  "created_utc": "2025-12-25 03:27:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsdr7z",
          "author": "mixxoh",
          "text": "Damn, I‚Äôm in the middle of interviewing with them. What would this mean?",
          "score": 26,
          "created_utc": "2025-12-24 22:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvseojm",
              "author": "thrownawaymane",
              "text": "Get more interviews lined up. \n\nNo matter what happens next you‚Äôll be fine if you do that. Do not chase the shiny object, you could easily get led on.",
              "score": 62,
              "created_utc": "2025-12-24 22:42:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsg6f3",
                  "author": "mixxoh",
                  "text": "So abandoned them since I guess most of the going public upside is moot now?",
                  "score": 6,
                  "created_utc": "2025-12-24 22:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsk83t",
          "author": "jeffwadsworth",
          "text": "Take out the competition always works well.",
          "score": 10,
          "created_utc": "2025-12-24 23:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsomnp",
          "author": "az226",
          "text": "Interesting way of skirting regulation. Make a technology licensing deal and get the pick of the employees. Leave the company as a shell.",
          "score": 11,
          "created_utc": "2025-12-24 23:50:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskon9",
          "author": "TangeloPutrid7122",
          "text": "How the fuck was this allowed.",
          "score": 18,
          "created_utc": "2025-12-24 23:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvstlza",
              "author": "cafedude",
              "text": "Heh, there's nobody minding the store anymore. Anything goes.",
              "score": 19,
              "created_utc": "2025-12-25 00:25:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvskvt2",
          "author": "StackOwOFlow",
          "text": "snuffing out the competition already :(",
          "score": 13,
          "created_utc": "2025-12-24 23:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsel3j",
          "author": "insite",
          "text": "Wow! I wondered how NVIDIA joining the elite Big Tech companies was going to reshape the technology landscape. They keep finding new and inventive ways to drive up hardware costs. Buying up startup companies is nothing new, but this is in combination with their other deals to drive up the cost of RAM.",
          "score": 27,
          "created_utc": "2025-12-24 22:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsfmh5",
              "author": "SlowFail2433",
              "text": "Depends cos if they ramp up production of Groq chips then price could go down rather than up",
              "score": 5,
              "created_utc": "2025-12-24 22:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsgchb",
                  "author": "Mad_Undead",
                  "text": "Does Groq even sell LPUs? I thought they only provide inference.",
                  "score": 10,
                  "created_utc": "2025-12-24 22:53:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvse8iv",
          "author": "JayD30",
          "text": "https://groq.com/newsroom/groq-and-nvidia-enter-non-exclusive-inference-technology-licensing-agreement-to-accelerate-ai-inference-at-global-scale",
          "score": 5,
          "created_utc": "2025-12-24 22:39:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt6se6",
          "author": "TheRealMasonMac",
          "text": "VC-backed startups are a disease. A tale as old as Silicon Valley.",
          "score": 5,
          "created_utc": "2025-12-25 02:03:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtyxtm",
          "author": "SkyNetLive",
          "text": "Groq wasn‚Äôt going anywhere. Nvidia just giving it a mercy death. I would have let them die naturally",
          "score": 4,
          "created_utc": "2025-12-25 05:48:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvurm8n",
              "author": "RakesProgress",
              "text": "I kinda think the same.  The team is useful. The tech is useful.  But will never be a winner.  Assimilate them into the fold.",
              "score": 1,
              "created_utc": "2025-12-25 10:47:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvcvuo",
                  "author": "SkyNetLive",
                  "text": "I have spoken to their team, not good and felt more like talking to crypto bros. I suspect they backfilled with Nvidia , the so called assets.",
                  "score": 1,
                  "created_utc": "2025-12-25 14:01:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvscnit",
          "author": "Stunning_Mast2001",
          "text": "Wow really fascinating news‚Ä¶¬†",
          "score": 10,
          "created_utc": "2025-12-24 22:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvslhk4",
          "author": "robberviet",
          "text": "20b? That's too much.",
          "score": 3,
          "created_utc": "2025-12-24 23:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsy46p",
              "author": "Maleficent-Forever-3",
              "text": "Valued at $7B in the last funding round in sept when a firm that has Donald trump jr on the board invested",
              "score": 5,
              "created_utc": "2025-12-25 00:57:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtzflv",
              "author": "Fortyseven",
              "text": "That'll never fit on my 3060.",
              "score": 3,
              "created_utc": "2025-12-25 05:52:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvszx9h",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/3tjbjvaa399g1.png?width=498&format=png&auto=webp&s=72c2f5c9297a1de6cae2bedaa2dd92c3ee411d27",
          "score": 3,
          "created_utc": "2025-12-25 01:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtsx1l",
          "author": "Lesser-than",
          "text": "dem real dollars or unrealized dollars?",
          "score": 3,
          "created_utc": "2025-12-25 04:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvurxhf",
          "author": "lickywilde",
          "text": "So how does Groq the company continue without its senior leadership and its IP sold off to its main competitor?",
          "score": 3,
          "created_utc": "2025-12-25 10:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvutzoo",
              "author": "pengy99",
              "text": "It probably doesn't other than maybe limping along for a bit. That's the point. Nvidia gets what they want without the pesky regulatory oversight.",
              "score": 4,
              "created_utc": "2025-12-25 11:12:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuxivn",
                  "author": "lickywilde",
                  "text": "Ah sure its the Google - Windsurf thing all over again.",
                  "score": 1,
                  "created_utc": "2025-12-25 11:49:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwaf72g",
              "author": "Ok-Hunt-4927",
              "text": "There‚Äôs new CEO now, Simon Edwards. He was CFO earlier but after Ross and Sunny Madra left, he was made the CEO. (I work for Groq)",
              "score": 2,
              "created_utc": "2025-12-28 01:28:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxcgd8",
              "author": "fallingdowndizzyvr",
              "text": "It has the cloud business. Nvidia didn't want that.",
              "score": 1,
              "created_utc": "2025-12-25 21:21:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsfsw6",
          "author": "FullstackSensei",
          "text": "Come on, people! Can't you read?!!!\n\nNvidia is not acquiring Groq. They would never do such an anti-competitive move. Nvidia is mearly non-exclusively (see, it's not even exclusive) licensing Groq's technology and hiring all Groq's engineering talent to help them integrate the technology. Everyone else is free to also license the same technology and Groq cloud operations will be unaffected and continue to operate independently until their chips become irrelevant in another year.\n\nI really don't understand what all the commotion is about.",
          "score": 20,
          "created_utc": "2025-12-24 22:49:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsi0a5",
              "author": "Freonr2",
              "text": "From TFA:\n\n> Nvidia has agreed to buy assets from Groq...\n\n> Groq said in a blog post on Wednesday that it‚Äôs ‚Äúentered into a non-exclusive licensing agreement with Nvidia for Groq‚Äôs inference technology,‚Äù ... ‚Äúwill join Nvidia to help advance and scale the licensed technology,‚Äù\n\n> Davis told CNBC that Nvidia is getting all of Groq‚Äôs assets, though its nascent Groq cloud business is not part of the transaction\n\nIt's a bit confusing the way this is written because \"buy\" and \"license\" are both used.\n\n> Huang added that, ‚ÄúWhile we are adding talented employees to our ranks and licensing Groq‚Äôs IP, we are not acquiring Groq as a company.‚Äù\n\nThis is a bit weird since they're taking (all? most? a lot of?) the Groq employees, so my assumption is Groq is essentially dead in terms of new technology and all new tech will be produced by former Groq employees who are now Nvidia employees.\n\nMy take away is Groq is mostly being absorbed with a husk leftover.",
              "score": 21,
              "created_utc": "2025-12-24 23:04:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsjfsb",
                  "author": "Edzomatic",
                  "text": "This is a lot of corporate gymnastics",
                  "score": 17,
                  "created_utc": "2025-12-24 23:14:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsjxdq",
                  "author": "Charuru",
                  "text": "Your parent was sarcastic.",
                  "score": 16,
                  "created_utc": "2025-12-24 23:17:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsvgf0",
              "author": "noiserr",
              "text": "> They would never do such an anti-competitive move.\n\nlol.. the company behind proprietary CUDA, PhysX, g-sync.. They also tried to buy ARM but got blocked for being anti-competitive. There is also the GPP program.. (from wikipedia: The program was regarded as an anti-consumer practice due to the fact that partnering companies were required to remove their gaming branding from all non-Nvidia graphics cards,[11] hurting consumer choice.)",
              "score": 7,
              "created_utc": "2025-12-25 00:38:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtnzb8",
              "author": "True_Requirement_891",
              "text": "Groq has been trying to get this deal from a long time.",
              "score": 1,
              "created_utc": "2025-12-25 04:16:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsvusj",
          "author": "goatchild",
          "text": "How is this allowed?",
          "score": 4,
          "created_utc": "2025-12-25 00:41:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsn8is",
          "author": "a_beautiful_rhind",
          "text": "I guess it was nice while it lasted. They never got that memory up to make it practical and they certainly won't now.",
          "score": 2,
          "created_utc": "2025-12-24 23:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtihtg",
          "author": "grady_vuckovic",
          "text": "So the only possible threat to NVIDIA, TPUs, and NVIDIA is trying to buy control of it..",
          "score": 2,
          "created_utc": "2025-12-25 03:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtlmmy",
          "author": "extopico",
          "text": "No anti monopoly laws? Awesome.",
          "score": 2,
          "created_utc": "2025-12-25 03:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtpazq",
          "author": "lqstuart",
          "text": "Largest deal on what record‚Ä¶?",
          "score": 2,
          "created_utc": "2025-12-25 04:26:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvudq6q",
              "author": "pengy99",
              "text": "Largest deal Nvidia has ever done.",
              "score": 5,
              "created_utc": "2025-12-25 08:14:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvttl9h",
          "author": "VegetableSense",
          "text": "https://preview.redd.it/50hmh38f8a9g1.jpeg?width=710&format=pjpg&auto=webp&s=1455d2f8edbcaaafc2d5b867807c042183c2b3a8",
          "score": 2,
          "created_utc": "2025-12-25 05:01:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuznxj",
          "author": "OmarBessa",
          "text": "So, they just got all their IP.",
          "score": 2,
          "created_utc": "2025-12-25 12:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsdp1a",
          "author": "Bloated_Plaid",
          "text": "Woohoo!",
          "score": 3,
          "created_utc": "2025-12-24 22:36:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsgf39",
          "author": "Mediocre-Ant-7178",
          "text": "The pump must go on",
          "score": 3,
          "created_utc": "2025-12-24 22:53:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtyyvq",
          "author": "ThatOneGuy4321",
          "text": "Groq =/= Grok (Elon chatbot) btw\n\nand then there‚Äôs grok patterns which are a different thing",
          "score": 2,
          "created_utc": "2025-12-25 05:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsoum8",
          "author": "1-800-methdyke",
          "text": "God dammit",
          "score": 1,
          "created_utc": "2025-12-24 23:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtnbbr",
          "author": "ForsookComparison",
          "text": "Someone EILI5 why we are cooked",
          "score": 1,
          "created_utc": "2025-12-25 04:10:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsfits",
          "author": "CostGuilty8542",
          "text": "circular money",
          "score": 0,
          "created_utc": "2025-12-24 22:47:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsture",
          "author": "ckkl",
          "text": "Huge!!!",
          "score": 0,
          "created_utc": "2025-12-25 00:27:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pux0yc",
      "title": "We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/",
      "author": "vox-deorum",
      "created_utc": "2025-12-24 20:50:16",
      "score": 625,
      "num_comments": 156,
      "upvote_ratio": 0.96,
      "text": "[GLM-4.6 Playing Civilization V + Vox Populi \\(Replay\\)](https://i.redd.it/zaib4up4s79g1.gif)\n\nWe had GPT-OSS-120B and GLM-4.6 playing 1,408 full Civilization V games (with Vox Populi/Community Patch activated). In a nutshell: LLMs set strategies for Civilization V's algorithmic AI to execute. Here is what we found\n\n[An overview of our system and results \\(figure fixed thanks to the comments\\)](https://preview.redd.it/ftox05oo5e9g1.png?width=3201&format=png&auto=webp&s=b8181b507060b45caab07acc36ba82d80eb65f1d)\n\n**TLDR:** It is now possible to get open-source LLMs to play end-to-end Civilization V games (the m. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently.\n\n**The boring result:** With a simple prompt and little memory, both LLMs did slightly better in the best score they could achieve within each game (+1-2%), but slightly worse in win rates (-1\\~3%). Despite the large number of games run (2,207 in total, with 919 baseline games), neither metric is significant.\n\n**The surprising part:**\n\nPure-LLM or pure-RL approaches [\\[1\\]](https://arxiv.org/abs/2401.10568), [\\[2\\]](https://arxiv.org/abs/2502.20807) couldn't get an AI to play and survive full Civilization games. With our hybrid approach, LLMs can survive as long as the game goes (\\~97.5% LLMs, vs. \\~97.3% the in-game AI). The model can be as small as OSS-20B in our internal test.\n\nMoreover, the two models developed **completely different playstyles**.\n\n* OSS-120B went full warmonger: +31.5% more Domination victories, -23% fewer Cultural victories compared to baseline\n* GLM-4.6 played more balanced, leaning into both Domination and Cultural strategies\n* Both models preferred **Order** (**communist-like**, \\~24% more likely) ideology over **Freedom** (democratic-like)\n\n**Cost/latency (OSS-120B):**\n\n* \\~53,000 input / 1,500 output tokens per turn\n* **\\~$0.86/game** (OpenRouter pricing as of 12/2025)\n* Input tokens scale linearly as the game state grows.\n* **Output stays flat: models don't automatically \"think harder\" in the late game.**\n\n**Watch more:**\n\n* Paper link: [https://arxiv.org/abs/2512.18564](https://arxiv.org/abs/2512.18564)\n* [Example save 1](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/1.Civ5Replay)\n* [Example save 2](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/2.Civ5Replay)\n* [Example save 3](https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/3.Civ5Replay)\n\n**Try it yourself:**\n\n* The Vox Deorum system is 100% open-sourced and currently in beta testing\n* GitHub Repo: [https://github.com/CIVITAS-John/vox-deorum](https://github.com/CIVITAS-John/vox-deorum)\n* GitHub Release: [https://github.com/CIVITAS-John/vox-deorum/releases](https://github.com/CIVITAS-John/vox-deorum/releases)\n* Works with any **OpenAI-compatible local providers**\n\n[We exposed the game as a MCP server, so your agents can play the game with you](https://preview.redd.it/tccdt44oq79g1.png?width=2291&format=png&auto=webp&s=0b8a4fe5871db4d2bf00f417acd13de3e688037f)\n\n**Your thoughts are greatly appreciated:**\n\n* What's a good way to express the game state more efficiently? Consider a late-game turn where you have 20+ cities and 100+ units. Easily 50k+ tokens. Could multimodal help?\n* How can we get LLMs to play better? I have considered RAG, but there is really little data to \"retrieve\" here. Possibly self-play + self-reflection + long-term memory?\n* How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?\n\n**Join us:**\n\n* I am hiring a PhD student for Fall '26, and we are expanding our game-related work rapidly. Shoot me a DM if you are interested!\n* I am happy to collaborate with anyone interested in furthering this line of work.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvrxvar",
          "author": "false79",
          "text": "Today it's Civ5  \nTomorrow it's the 3 Body Problem",
          "score": 121,
          "created_utc": "2025-12-24 20:59:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt0aa2",
              "author": "TaifmuRed",
              "text": "\"AGI soon via scaling\"",
              "score": 17,
              "created_utc": "2025-12-25 01:13:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvryyvw",
              "author": "lookwatchlistenplay",
              "text": "Peace be with us.",
              "score": -69,
              "created_utc": "2025-12-24 21:05:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwghpyb",
                  "author": "false79",
                  "text": "lol - you ate so many downvotes before you edited your response.\n\nIt was pretty clear you never read or seen 3 body problem. But there is a key part of the story where the audience learns about how an alien race is trying to use simulations of their civilation, trying to figure out under what conditions they would survive.  \n  \nTo my knowledge, the story was entertaining and interesting given that it was a big hit in China that transcending to western audiences via Netflix.  \n  \nIn the future, don't be that fool, unless you eating downvotes is your thing.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:19:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvsblsn",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -31,
                  "created_utc": "2025-12-24 22:22:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsdn2e",
          "author": "ahjorth",
          "text": "An idea so crazy it could only come out of the CCL. Great job, guys!\n\nDid you explore any options that treat the game as quasi-multi-level ABMs, where the decisions of individual units are made to optimize for unit-level (i.e. local environment) goals + nearby city goals + regional/continental goals + global goals? \n\nI realize this would be a big change away from the way you are currently using the built in AI, but I‚Äôd be really curious to see what you can do. Maybe feed the world state in like you do now, to articulate overall goals, then iterate over each continent ands articulate more localized goals based on the global goals, then cities, etc down to units. For each level, revise or confirm the existing goals to take into account any changes to the global state, and finally articulate decisions at the various levels (choosing science/culture, what to build in a city, where to move a unit, etc). Maybe do this a few times to allow revisions in response to the simultaneous decisions of other cities/units. \n\nEither way, congrats on finishing, your new job, and on this project! Cheers, Arthur (who left just before you started)",
          "score": 13,
          "created_utc": "2025-12-24 22:35:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvson1q",
              "author": "vox-deorum",
              "text": "Didn't expect to meet you here, Authur! The project was my last one started at CCL. Yes, and I received a very similar comment there :D\n\nYes, I think this can be an amazing idea. Training RL models at the individual unit or city level could be waaaay easier than at the global level. Performance aside, it may also create some hilarious situations where micro-level rewards deviate from the macro-level ones. Think about morale, self-preservation, etc...",
              "score": 8,
              "created_utc": "2025-12-24 23:50:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsopx9",
                  "author": "vox-deorum",
                  "text": "And I don't think that's the opposite of what we are doing; on the contrary, it can be very compatible.",
                  "score": 6,
                  "created_utc": "2025-12-24 23:51:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs7tf1",
          "author": "ASTRdeca",
          "text": "Very cool! You mentioned in the paper that despite GLM being much larger than GPT-OSS 120B, the larger size didn't seem to impact performance. I'm wondering if you tried models smaller than OSS-120B to see at what point model size matters? (For example, OSS-20B?) \n\nI'm just thinking about the viability of running these kinds of systems locally, since 120B is probably too large for most users to run themselves",
          "score": 23,
          "created_utc": "2025-12-24 21:58:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs8b7p",
              "author": "vox-deorum",
              "text": "OSS-20B works for me locally. I haven't put it to a large-scale experiment due to cost concern (on OpenRouter, 20B and 120B were almost at the same price). That said, we are exploring hybrid options (e.g., getting OSS-20B to process the raw game state and then a stronger model gets to do decision-making).",
              "score": 15,
              "created_utc": "2025-12-24 22:01:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsczqr",
                  "author": "NickNau",
                  "text": "despite the price indifference - testing smaller models can be a very interesting test by itself. I bet it may provide some new insights when enough models are tested.\n\nthank you for your work. cool stuff",
                  "score": 13,
                  "created_utc": "2025-12-24 22:31:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvu072d",
                  "author": "Qwen30bEnjoyer",
                  "text": "Just curious, with the cost concern, maybe you could try Chutes.ai? A $20 subscription buys up to 5000 calls of Kimi K2 Thinking and other models with no input or output token limits.\n\nAnother thought is maybe we could make this into a benchmark by pitting 8 Civilizations against each other, and calculating an ELO rating?",
                  "score": 4,
                  "created_utc": "2025-12-25 05:59:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0fg8g",
              "author": "Glad_Remove_3876",
              "text": "Yeah we actually did test OSS-20B internally and it was surprisingly viable - still managed to survive most full games without major issues. The sweet spot seems to be somewhere around that 20B mark where you get decent strategic reasoning without needing a data center\n\n  \nFor local stuff you're probably right that 120B is pushing it for most people, but 20B is definitely doable on a decent gaming rig with some patience",
              "score": 2,
              "created_utc": "2025-12-26 11:51:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs3p9x",
          "author": "Amazing_Athlete_2265",
          "text": "Nice. I love civ games (been playing since the original). Would be keen to play against one of my local models.",
          "score": 37,
          "created_utc": "2025-12-24 21:33:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs6dnj",
              "author": "vox-deorum",
              "text": "Yes, you can play with a small one. Even GPT-OSS-20B seems to work well (although I am unsure how clever/dumb it will be).",
              "score": 14,
              "created_utc": "2025-12-24 21:49:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvs7o05",
                  "author": "Amazing_Athlete_2265",
                  "text": "I like my small models, thinking about trying LFM2-8B-A1B and Qwen3 4B instruct",
                  "score": 1,
                  "created_utc": "2025-12-24 21:57:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt6ku7",
              "author": "randylush",
              "text": "Sounds like it doesn‚Äôt really play all that differently from a regular game algorithm",
              "score": 1,
              "created_utc": "2025-12-25 02:01:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtyv69",
                  "author": "vox-deorum",
                  "text": "Speaking of the outcome, yes. Speaking of the macro-level playstyle, we found some significant differences.",
                  "score": 2,
                  "created_utc": "2025-12-25 05:47:27",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs99y4",
          "author": "invisiblelemur88",
          "text": "Could one of these be added into a multiplayer civ 5 game? My friends and I play every wednesday evening together for years now... would love to experiment with getting more interesting AIs involved. The existing AIs in it are particularly flat.",
          "score": 18,
          "created_utc": "2025-12-24 22:07:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt1nor",
              "author": "Amazing_Athlete_2265",
              "text": "If you haven't already, try Vox Populi mod. It makes the stock AI a lot better.",
              "score": 7,
              "created_utc": "2025-12-25 01:24:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvt7wcb",
                  "author": "vox-deorum",
                  "text": "Yes and we are working closely with the VP team!",
                  "score": 6,
                  "created_utc": "2025-12-25 02:11:43",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nvt6a6i",
                  "author": "invisiblelemur88",
                  "text": "Does Vox Populi work multiplayer?",
                  "score": 3,
                  "created_utc": "2025-12-25 01:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsmyt7",
              "author": "vox-deorum",
              "text": "That's definitely possible. Vox Deorum is based on Vox Populi, which supports multiplayer. That said, I never tested it myself, and I would envision some minor revisions to avoid desync issues in a networked game. A hotseat game should be smooth!",
              "score": 5,
              "created_utc": "2025-12-24 23:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzln0l",
                  "author": "ArtfulGenie69",
                  "text": "If it got connected up you could see it's win rate vs humans :)",
                  "score": 1,
                  "created_utc": "2025-12-26 06:49:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs8qik",
          "author": "uroboshi",
          "text": "This is really cool, thanks for sharing your discoveries. I'll make some tests too when I can. Thanks!",
          "score": 5,
          "created_utc": "2025-12-24 22:04:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs8zrv",
              "author": "vox-deorum",
              "text": "Great! Let me know if any issue arises.",
              "score": 1,
              "created_utc": "2025-12-24 22:06:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsennl",
          "author": "-InformalBanana-",
          "text": "Did you maybe try qwen3 2507 30b a3b instruct or thinking?\nWhat a fun experiment.¬†",
          "score": 5,
          "created_utc": "2025-12-24 22:42:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsqhj8",
          "author": "pesaru",
          "text": "Are you specifically trying to do this without tools? Whenever I give an AI a task that requires handling a lot of data, for example, \"go through my entire project and identify instances of \\_\\_\\_\\_ and then apply transformation Y to them, the truly exceptional models will write a tool to do much of that (the shitty models sometimes try but then spend a million tokens going in circles doing absolutely nothing). There are a bunch of PowerShell scripts littering my projects that are remnants of those sorts of activities. However, the more you do this type of strategy, the closer you get to that algorithmic AI play.   \n  \nI get the sense that the only way you could give the LLM an advantage would be to allow it to self record information about its strategies and how often each action lead to survival/winning, basically recreating the **MENACE** system of the 1960s and allowing the LLM to essentially learn from experience over time, allowing them to discover novel strategies that the algorithmic AI would likely not be capable of. \n\nAnd so I feel the really neat thing to do would be going the route of AlphaEvolve -- get the AI to exclusively focus on iteratively writing code to play the game based on inputs. That would likely produce the best possible result.",
          "score": 8,
          "created_utc": "2025-12-25 00:03:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsspph",
              "author": "vox-deorum",
              "text": "Love your ideas! 1) Technically, when LLMs make decisions, they call tools through the MCP server, and the algorithm-based AI executes the details. 2) Yes! Self-reflection is something we are looking into now. 3) Yes again - like u/ahjorth mentioned here, it may be very interesting to look into self-evolving algorithms/RL models at the micro level.",
              "score": 5,
              "created_utc": "2025-12-25 00:19:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvuj28o",
                  "author": "dasjomsyeet",
                  "text": "Very interesting project! If you haven‚Äôt already I recommend checking out how the LLM harnesses for projects like ClaudePlaysPokemon are built. Not sure if it does anything you aren‚Äôt already doing, but they have a memory management tool where it loads prior decisions back into the context window and writes new memories if important decisions regarding strategy are made. Could be worth looking into how they did it.",
                  "score": 1,
                  "created_utc": "2025-12-25 09:12:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsamyh",
          "author": "JsThiago5",
          "text": "You did not put them to play against each other, right?",
          "score": 5,
          "created_utc": "2025-12-24 22:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsmrke",
              "author": "vox-deorum",
              "text": "Not yet, but maybe we should create an arena where LLMs fight each other in Civilization?",
              "score": 10,
              "created_utc": "2025-12-24 23:37:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt04bf",
                  "author": "mj3815",
                  "text": "The ultimate benchmark",
                  "score": 9,
                  "created_utc": "2025-12-25 01:12:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtycfv",
                  "author": "lochyw",
                  "text": "I mean what about actual training/ learning for improving results.¬†\nPerhaps finding metrics or observability on possible moves/options and picking the best ones to allow for better decision making.¬†\n\n\nOr enhanced sim speed for beyond 1x time scale testing/training like the normal ML stuff does for training.¬†\n\n\nSurface level for this post doesn't appear too interesting imo but there's so much potential beneath the surface.¬†",
                  "score": 1,
                  "created_utc": "2025-12-25 05:42:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsmpyy",
          "author": "a_beautiful_rhind",
          "text": "So OSS, despite the censored facade, is a heartless warmonger underneath? Yet GLM, the less \"safe\" model, is a relatively nice guy? \n\n>models preferred Order (communist-like, ~24% more likely) ideology over Freedom\n\nThe hits from our alignment overlords just keep coming and literally write themselves.",
          "score": 12,
          "created_utc": "2025-12-24 23:37:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt43tj",
              "author": "JazzlikeLeave5530",
              "text": "Really don't think that's how it works with Civ V considering it's all gameplay related and not actually mapping to real-world stuff lol. Like fascism gives military bonuses while communism gives science in Civ 6, so if it's going for a science victory then that's why it'll pick that...",
              "score": 6,
              "created_utc": "2025-12-25 01:43:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt2fir",
              "author": "fivecanal",
              "text": "One could argue the preference for order is exactly the goal of their supposed 'alignment'",
              "score": 5,
              "created_utc": "2025-12-25 01:30:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsaqv9",
          "author": "steezy13312",
          "text": "I‚Äôm really excited to try this out this weekend. I‚Äôm really curious how much the LLMs can lean into their civilization leader‚Äôs persona in decision-making and approach, vs just trying to win based on solely the game‚Äôs mechanics",
          "score": 7,
          "created_utc": "2025-12-24 22:17:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvssv3z",
              "author": "vox-deorum",
              "text": "I guess we can prompt it a bit more towards role-playing (but that also depends on the model?)",
              "score": 1,
              "created_utc": "2025-12-25 00:20:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvswb50",
          "author": "o0genesis0o",
          "text": "I'm amazed that you are able to turn this research question into a proper project and secured funding for recruiting PhD student. As a fellow struggling academic, hats off to you and jealous to your future PhD students. They seem to have some very interesting research problems ahead of them. Best of luck.",
          "score": 3,
          "created_utc": "2025-12-25 00:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvswmvr",
              "author": "vox-deorum",
              "text": "Good luck! It has been an incredibly challenging year for everyone on the market. Let me know if you need anything or if some collaboration would help with your situation.",
              "score": 2,
              "created_utc": "2025-12-25 00:46:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsxwpy",
                  "author": "o0genesis0o",
                  "text": "I'm developing a framework for multi LLM agent from scratch, more like to refresh my skill. My goal is to work better with my llamacpp server. If I can think of some research topic to leverage this or if I reach the point where I can open source it, i would reach out. Or if you think of something interesting, I'm all ears too. Enjoy your holiday.",
                  "score": 1,
                  "created_utc": "2025-12-25 00:55:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt1pg1",
          "author": "T_UMP",
          "text": "https://preview.redd.it/t2dmblyj599g1.jpeg?width=1920&format=pjpg&auto=webp&s=b85051cd0514a5c3f2a01c20fc0b94da8caa94ed\n\nIf there was a way to have a LLM work with this that would be a blast. Not to mention work as a proper humanlike AI.",
          "score": 3,
          "created_utc": "2025-12-25 01:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtylr9",
              "author": "lochyw",
              "text": "Getting a real time sim game to work in actual real time would be interesting.¬†",
              "score": 2,
              "created_utc": "2025-12-25 05:45:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs49kb",
          "author": "scottybowl",
          "text": "Sorry if I‚Äôm being dumb, but not sure I understand the takeaway here. What have you learned from doing this?",
          "score": 10,
          "created_utc": "2025-12-24 21:37:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs5sv7",
              "author": "vox-deorum",
              "text": "It is now possible to get open-source LLMs to play end-to-end Civilization V games. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently.",
              "score": 23,
              "created_utc": "2025-12-24 21:46:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvseesa",
                  "author": "klipseracer",
                  "text": "I think they mean: Why, for what purpose. What are the use cases.",
                  "score": 6,
                  "created_utc": "2025-12-24 22:40:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvslrz6",
          "author": "J-IP",
          "text": "Im looking forward to when we can have smaller finetuned models avaliable in order to insert more flavor and diversity in to different games like this!",
          "score": 2,
          "created_utc": "2025-12-24 23:30:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvslxo5",
              "author": "vox-deorum",
              "text": "Yes! That's a very legit goal.",
              "score": 1,
              "created_utc": "2025-12-24 23:31:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvss61a",
          "author": "slippery",
          "text": "Impressive achievement and insights. Keep going!",
          "score": 2,
          "created_utc": "2025-12-25 00:15:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvswalf",
          "author": "xxxx771",
          "text": "how do you feed the game state into the LLM? Do you read each world tile as the player would see and you feed this into a structured manner to the llm or how exactly?",
          "score": 2,
          "created_utc": "2025-12-25 00:44:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsyf2e",
              "author": "vox-deorum",
              "text": "We fed the following information in markdown format:  \nGame rules (map sizes, speed, etc); Players; Cities; Units; Tactical Zones (In-game AI's estimation); Events. The map is only implicitly given through events. Otherwise, a map itself is 56x36 = 2,016, and we would constantly need at least 40k tokens in the late game.",
              "score": 3,
              "created_utc": "2025-12-25 00:59:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt2ant",
                  "author": "xxxx771",
                  "text": "So when you say cities/units do you feed like a grid map with each tile contents or what?",
                  "score": 1,
                  "created_utc": "2025-12-25 01:29:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2qi14",
                  "author": "sadjoker",
                  "text": "\"DeepSeek-OCR demonstrates that 100 vision tokens can represent approximately 1000 text tokens with 97%+ accuracy.\"",
                  "score": 1,
                  "created_utc": "2025-12-26 20:05:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt809e",
          "author": "Automatic-Boot665",
          "text": "Try GLM 4.7",
          "score": 2,
          "created_utc": "2025-12-25 02:12:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt88j1",
              "author": "vox-deorum",
              "text": "Will do!",
              "score": 1,
              "created_utc": "2025-12-25 02:14:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtd31m",
          "author": "phratry_deicide",
          "text": "You might be interested in /r/unciv, an open source clone attempt of Civ 5, also available on mobile (and Pixels have Tensor chips).",
          "score": 2,
          "created_utc": "2025-12-25 02:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtsr25",
              "author": "vox-deorum",
              "text": "Cool! I'd love to see if, at some point, unciv and Vox Populi can get together. I think technically the system can be ported there, and I would love for someone to look into that.",
              "score": 1,
              "created_utc": "2025-12-25 04:54:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtukxl",
          "author": "gromhelmu",
          "text": "What is the difference between the top-right and bottom-right graphic? They look identical, except for the color.",
          "score": 2,
          "created_utc": "2025-12-25 05:09:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtvqqd",
              "author": "vox-deorum",
              "text": "Oops!!!! My bad. Was supposed to be 2 different graphs. Thanks for pointing it out!",
              "score": 2,
              "created_utc": "2025-12-25 05:19:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvu7ks6",
                  "author": "gromhelmu",
                  "text": "Happens to all of us. Glad this didn't go by unnoticed!",
                  "score": 2,
                  "created_utc": "2025-12-25 07:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyvo6g",
          "author": "Fwiffo_the_brave",
          "text": "I actually tried to do something similar with my own game that I have been developing, as I had an experimental version that could use an LLM to make strategic and diplomatic decisions for the AI of my game (it is similar to Master of Orion 1). I found that the LLMs were decent at the game, but I had a lot of issues with the smaller models not being able to work with the command format I made or issues with it just hallucinating planets. \n\nI never let it get to the end game due to the amount of prompts it burned through, but I did let it get decently into the game a few times and it was at least doing better than my AI at managing planets, but it was a bit worse at managing fleets and allocating defenses. Where it really did well was with diplomacy. Unlike a normal AI, it was a bit more fun to bargain with and a lot more fun to send insulting messages to when declaring war. It had limited control of the relationship status, so sending insulting messages could actually piss it off enough to get declared war on. It was far less stiff compared to the normal AI\n\nAt some point I might look at actually releasing a separate version of my game with LLM AIs as an option once the game is feature complete. Way to difficult having to update my AI and the LLM AI for each new feature or change that I make, especially as stuff does change frequently still.",
          "score": 2,
          "created_utc": "2025-12-26 03:21:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskizm",
          "author": "Murhie",
          "text": "First of all: Thats dope AF. Love civ. Ive skimmed over the paper. Some very quick thoughts with regard to your questions (but the team has probably thought more about it better than a random redditor who has skimmed the paper):\n\nMore token efficient state: In your paper i see its a markdown with information. First thing coming to my mind is try and only sent updates compared to the previous turn instead of all information always, but thay would only work if previous states remain in context somehow, I guess size would grow anyway but inference can be more efficient like this. It would also help with memory. I see you already do this for events. \nMultimodal could help, you might also try to map the map (map the image of the map with tiles) to a numerical matrix where each coordinate is described (dimension for every possble feature) and add a few dimensions for other info. You would then pass a definition of those features in the system prompt. (Completely making this up. Have no experience or empirical evidende that this would work or even reduce size)\n\nBetter play: I would guess the most promising thing to add is memory. Unlikely to help with your input size state problem though.\nSecond, multi agent systems could help here, but will introduce a shitload of complexity. Where one agent coordinates the whole strategy and other agents (for instance research, economic, diplomatic, military agents) report to the coordination agent and micromanage. Maybe there you could add history as well.\nFurthermore, the state as described in the paper seems a bit basic, but seeing how it grows in size each turn its probably way more detailed than described. For instance: geographic/spatial features matter a lot (where is everything and how does that relate to each other, proximity to untapped resources, etc). It is unclear from the paper how that is managed. \nAlso the \"X\" in LLM+X matters a lot I think, I am not too familiar with the engine used here for unit movement or builder actions, but there needs to be a way where that is coordinated with what the LLM is doing. A lot of interesting things can be done here.",
          "score": 4,
          "created_utc": "2025-12-24 23:21:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsp6vw",
              "author": "vox-deorum",
              "text": "Thank you a lot! We will think along these lines soon :)",
              "score": 1,
              "created_utc": "2025-12-24 23:54:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtl43a",
          "author": "R33v3n",
          "text": "I know there's real agentic and safety applications with this type of research, but what hypes me most is the silly prospect of one day being able to play a Stellaris or Civilization-like game against AIs that really embody a given ruler or culture's persona, and do diplomacy in real time. Complete with plans, improvisation, cooperation, rivalries, dreams and *spite*. <3\n\n>How can we get LLMs to play better? I have considered RAG, but there is really little data to \"retrieve\" here. Possibly self-play + self-reflection + long-term memory?\n\n>How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?\n\nHave you checked what similar undertakings and harnesses in different genres do? Like *CHIM* in Skyrim or Claude Plays Pokemon? Or [what's being done](https://arxiv.org/abs/2506.09655) on the board-game Diplomacy side of things? These might be decent inspirations on how to harness (or fine-tune, in the lattter's case) LLMs for game environments.",
          "score": 3,
          "created_utc": "2025-12-25 03:53:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvttagn",
              "author": "vox-deorum",
              "text": "Oh, definitely not! I am not likely to put this on my grant proposals, but yes, that's my main motivation. We are working with the Vox Populi community to see how we can get you negotiate with an LLM player. And I think there is much more to be done, like what if we put an image/video generator to \"materialize\" the alt-history you made in the game? \n\nYes, I have looked at (and got inspired by) many recent studies in this direction. Civ is a bit unique in that the game state itself is much more complex than, say, Diplomacy, but fine-tuning is something we will look into next!",
              "score": 1,
              "created_utc": "2025-12-25 04:59:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsez6e",
          "author": "Jannik2099",
          "text": "Is it possible to have multiple LLMs play in one game with just one Civ 5 license? I could run multiple instances through wine.\n\nWe host a few on-premise models and it would be very entertaining to have them compete against one another...",
          "score": 1,
          "created_utc": "2025-12-24 22:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsmnyp",
              "author": "vox-deorum",
              "text": "Yes! We didn't run the experiment like that, but that's definitely possible. Personally, I am playing a game with 2 LLM players. You can customize the configuration through WebUI. You can also manually edit the config file since a few options are not exposed there right now. DM me if you have questions.",
              "score": 2,
              "created_utc": "2025-12-24 23:36:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvswow0",
          "author": "Sabin_Stargem",
          "text": "It would be neat if you can have four different AIs attempt to complete Pokemon.   Say Generation 1's Pokemon Blue, Red, Green, and Yellow?   Each AI can have their cover starter.\n\nAfter each gym, you could require them to fight each other, and also permit them to do trading of monsters.   This gives us a chance to see how 'social' AI can be when it comes to making trades, what strategies they take to acquire their badges, exploration vs combat, and so forth.\n\nSomeone already did a timelapse of AI trying to beat Pokemon some years ago.   How different have things become?\n\n---\n\nTraining AI to Play Pokemon with Reinforcement Learning\nhttps://www.youtube.com/watch?v=DcYLT37ImBY",
          "score": 1,
          "created_utc": "2025-12-25 00:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsytcj",
              "author": "vox-deorum",
              "text": "Yeah, the idea sounds similar here.",
              "score": 2,
              "created_utc": "2025-12-25 01:02:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu3gca",
          "author": "MarkIII-VR",
          "text": "This really makes you think about the work put into making the built in game AI functional to the point that the game is actually playable against the computer.\n\nReally thought provoking on just how good the developers were at that time!",
          "score": 1,
          "created_utc": "2025-12-25 06:29:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf77r",
              "author": "vox-deorum",
              "text": "That's true. [https://www.vice.com/en/article/the-modders-who-decided-to-overhaul-the-ai-in-civilization-v/](https://www.vice.com/en/article/the-modders-who-decided-to-overhaul-the-ai-in-civilization-v/)",
              "score": 1,
              "created_utc": "2025-12-25 18:02:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvucj7l",
          "author": "robbedoes-nl",
          "text": "I saw that LLM are really good at the Global Thermonuclear War. But it‚Äôs an older game from 1983.",
          "score": 1,
          "created_utc": "2025-12-25 08:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf4na",
              "author": "vox-deorum",
              "text": "Which game? Quite curious.",
              "score": 1,
              "created_utc": "2025-12-25 18:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwjmis",
                  "author": "robbedoes-nl",
                  "text": "Sorry, it was a reference to the movie Wargames from 1983. A computer played a ‚Äògame‚Äô with a hacker and they almost started WW3.",
                  "score": 1,
                  "created_utc": "2025-12-25 18:28:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvukegf",
          "author": "polyc0sm",
          "text": "Can you use this to play the game with you instead ? Like audio only where you ask for a summary of what happened, ask more precise questions, list options then take actions ? It would be a revolution for many people (blind people, long car drive with kids (collaboratively), play while outside on a walk)",
          "score": 1,
          "created_utc": "2025-12-25 09:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws94r",
              "author": "vox-deorum",
              "text": "Wow, that's a COOL idea and definitely possible",
              "score": 1,
              "created_utc": "2025-12-25 19:18:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuo2sc",
          "author": "No-Comfort6060",
          "text": "It would be really interesting to see if Tiny Recursive Models could be used here for reasoning",
          "score": 1,
          "created_utc": "2025-12-25 10:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwf163",
              "author": "vox-deorum",
              "text": "How much context window can it handle? But we can also transform the game state into something much smaller.",
              "score": 1,
              "created_utc": "2025-12-25 18:01:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvut2fy",
          "author": "Ok_Try_877",
          "text": "How does the LLM interact with the game? Is their an API for CIv or have you connected it up to the mouse/screen? Please tell me its not manual?",
          "score": 1,
          "created_utc": "2025-12-25 11:02:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws6w2",
              "author": "vox-deorum",
              "text": "I don't have so many hands to play 2,000 games manually, do I? Well we did build an API to connect into Civ V. Mouse/screen control is possible but that would make the cost much higher.",
              "score": 1,
              "created_utc": "2025-12-25 19:17:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuy8yo",
          "author": "timwaaagh",
          "text": "Maybe just try some more of the  bigger llms like deepseek. It might just be that glm is weak here.",
          "score": 1,
          "created_utc": "2025-12-25 11:57:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuzv7v",
          "author": "nunofgs",
          "text": "Very cool! Congrats!\n\nI wonder what are your thoughts on a generic game orchestration approach? Sounds like you didn‚Äôt get far on it but what do you think are the major challenges there? How successful were you with that approach?",
          "score": 1,
          "created_utc": "2025-12-25 12:13:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwes6u",
              "author": "vox-deorum",
              "text": "Right now, we still use a ton of game-specific mechanics/scaffolds, which is both a boon (from a cost-effectiveness/performance perspective) and bane (from a generalization perspective). It depends on the end goal. Combined with other studies in this realm, I can say most (somewhat strategic?) games would benefit from a hybrid approach where LLMs give a human touch at the macro level and conventional AI executes the rest.",
              "score": 1,
              "created_utc": "2025-12-25 18:00:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvv9jvh",
          "author": "SpicyWangz",
          "text": "My main takeaway here is that ai likes authoritarianism. And if people in power start letting it make decisions for them, we will be enslaved by the machine",
          "score": 1,
          "created_utc": "2025-12-25 13:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw199j",
          "author": "txgsync",
          "text": "This is very cool. I wrote a benchmark for LLMs to try to play Zork and most just wandered the house around holding a nasty knife and dying to the ogre. \n\nI may give your framework for ZorkBench!",
          "score": 1,
          "created_utc": "2025-12-25 16:39:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwe990",
              "author": "vox-deorum",
              "text": "Cool! Let me know how it works :D",
              "score": 1,
              "created_utc": "2025-12-25 17:57:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwd8wk",
          "author": "ElementNumber6",
          "text": "No need to go so far.  Just play a simple game of chess with them and watch as they falter at every possible opportunity.",
          "score": 1,
          "created_utc": "2025-12-25 17:51:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwfym4",
          "author": "Different-Toe-955",
          "text": "How is the LLM interacting with the game? Is it being presented with text based choices to make?",
          "score": 1,
          "created_utc": "2025-12-25 18:07:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwgxmq",
              "author": "vox-deorum",
              "text": "Yes, sort of. It replaced the in-game AI's high-level decision-making, e.g., setting technology, policy, and also macro-level \"strategies\" that basically tweak the algorithmic AI's weights. For example, it can try to prioritize building an army, building ranged units, building happiness buildings, but they won't directly set a city's building priorities. That's what it stands for now.",
              "score": 2,
              "created_utc": "2025-12-25 18:12:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwkevp",
          "author": "overmind87",
          "text": "You should consider adapting this idea to work with rimworld, to see how different Ai models would work at a much smaller scale, managing the dynamics and needs of individuals in a small colony. And then see how that compares with the way they run a civ game. That way you get a good, broad look at the lowest level and highest level of social complexity management abilities for each model.",
          "score": 1,
          "created_utc": "2025-12-25 18:32:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvws1zq",
              "author": "vox-deorum",
              "text": "Well, I do play RimWorld a lot, and that's indeed our next project!",
              "score": 2,
              "created_utc": "2025-12-25 19:16:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxe5rg",
                  "author": "overmind87",
                  "text": "Cool! I'll keep an eye out for the results!",
                  "score": 1,
                  "created_utc": "2025-12-25 21:32:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxhrew",
          "author": "RogueProtocol37",
          "text": "Awesome work!  I'm looking forward to all my strategy games exposing a MCP interface\n\nHave you thought about let one LLM model play against another LLM model?\n\nP.S. Might be worth to cross-post this to /r/civ and /r/civfanatics",
          "score": 1,
          "created_utc": "2025-12-25 21:54:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxn9fl",
              "author": "vox-deorum",
              "text": "Yes! I planned to write a separate post for them (since they care more about Civ than about LLMs, I guess). Aksi, the mod is also available on CivFanatics (forum). \n\nI am now running a new experiment where several different types of agents compete against each other. I will do an ELO calculation later...",
              "score": 2,
              "created_utc": "2025-12-25 22:28:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxlrxt",
          "author": "Crimsoneer",
          "text": "I did something similar with Risk last year\n\nhttps://andreasthinks.me/posts/ai-at-play/",
          "score": 1,
          "created_utc": "2025-12-25 22:18:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxmxm3",
              "author": "vox-deorum",
              "text": "Great to see! Make me think of those studies playing diplomacy. Anything special you noticed?",
              "score": 1,
              "created_utc": "2025-12-25 22:25:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxo6i5",
                  "author": "Crimsoneer",
                  "text": "Mainly that the scaffolding was really important, and some interesting variation in behavior by model - eg, some models notably more aggressive or chatty",
                  "score": 2,
                  "created_utc": "2025-12-25 22:33:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzt3lp",
          "author": "postitnote",
          "text": "I think there's a bug in your code. in vox-agent.ts:\n[code]   // Handle messages\n    if (lastStep === null) {\n      config.messages = [...messages, ...await this.getInitialMessages(parameters, input, context)];\n    } else if (this.onlyLastRound) {\n      // Keep all system and user messages, but only the last round of assistant/tool messages\n      const filteredMessages: ModelMessage[] = [];\n      let lastUserIndex = -1;\n\n      // Pass 1: keep all system and user messages\n      for (let i = 0; i < messages.length; i++) {\n        const message = messages[i];\n        filteredMessages.push(message);\n        lastUserIndex = i;\n        if (message.role !== 'system' && message.role !== 'user')\n          break;\n      }\n[/code]\nthe 'break' in pass 1 means it won't include the majority of the user messages that contain the history of the game. I wonder how big of an impact this could be.",
          "score": 1,
          "created_utc": "2025-12-26 08:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzwcj0",
              "author": "vox-deorum",
              "text": "Thanks! I am impressed you have looked into the source code. That said, what you found was intentional. The game state is too big to stay in the context window. In our second experiment, we designed a \"briefer\" that provides a briefing, and the briefer has a small memory window (can see its own briefing from 5 turns ago).",
              "score": 1,
              "created_utc": "2025-12-26 08:36:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5vq05",
          "author": "Maasu",
          "text": "I love the idea of this, Imagine playing with a group of agents with long term memory and a discord channel between them  for diplomacy.. okay I am so doing this. I've already got the long term memory mcp https://github.com/ScottRBK/forgetful. This is happening.\n\nEdit: just had a skim through of the code and read the paper, so you have actually built your own agents.. very nice. Any plans to expose the actual agents as mcp tools?",
          "score": 1,
          "created_utc": "2025-12-27 08:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb3tcj",
              "author": "vox-deorum",
              "text": "Vox-agents has support for MCP-based tool calling since we essentially implemented Civ V as an MCP server. It would be really cool to group those agents in a Discord channel. How would you envision the architecture?",
              "score": 1,
              "created_utc": "2025-12-28 03:55:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwc9g5v",
                  "author": "Maasu",
                  "text": "I need to sit down and actually see if this is feasible, but in my head it is something like this:Right now you have Civ5 -> Bridge -> MCP Server- > Vox Agents if i have understood correctly.\n\nI think the simplest approach, without modifying anything on Vox would be:\n\nhttps://preview.redd.it/mv1kl2xu2x9g1.png?width=760&format=png&auto=webp&s=e1edce0b2f7bfddef7ba0de39f9dc9a811b3eada\n\nAPI service that exposes a v1/completions to make it openai compatible, i've already built my own version of this and I can configure MCP's and prompts. Have different agents callable via the model parameter.\n\nFor each player I configure an agent with a long term memory mcp (using forgetful), prompt to align them with the AI they will be interpreting (so you can ensure you Ghandi knows to follow script once Nuclear weapons arrive).\n\nThe discord side of it would involve a bot for each agent listening in discord, the bot pings the v1/completions endpoint - either hitting the playing agent themselves or a similar agent with a different prompt that shares the same long term memory and the bot posts the response back to discord.\n\nI think as well as this giving each agent playing the game access to MCP to post on discord to allow for public announcements, pr campaigns, interactions with humans.\n\nJust a brain fart right now but need to see if i can implement it.",
                  "score": 1,
                  "created_utc": "2025-12-28 09:45:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfecwn",
          "author": "theshrike",
          "text": "As always with these, I'm more interested in HOW you get the LLMs to play games, I don't care about the results that much.\n\nWhat kind of tools do you give the LLM to alter and view the game state? How do you do it?",
          "score": 1,
          "created_utc": "2025-12-28 20:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfqx3k",
              "author": "vox-deorum",
              "text": "Hi! The paper has an appendix for this. Our recent update exposes a bit more data we missed before, but the principle stays the same.",
              "score": 2,
              "created_utc": "2025-12-28 21:58:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlhu5j",
          "author": "ki7a",
          "text": "\\> 1,408 full Civilization V games  \nHow are you running experiments at scale?¬† Docker, slurm, batch runner, etc?\n\nI‚Äôm interested in kicking off some trials on an HPC, preferably in parallel.  \nAny pointers and/or scripts for handling batch runs would be appreciated.",
          "score": 1,
          "created_utc": "2025-12-29 19:23:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlhza4",
              "author": "vox-deorum",
              "text": "Hi! Do you mean specifically Civ V?",
              "score": 1,
              "created_utc": "2025-12-29 19:24:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwlotbh",
                  "author": "ki7a",
                  "text": "The full stack really, but let‚Äôs start with Civ V.\nDo you run multiple Civ V instances on the same machine?  Or do you run a vm/docker container for each game instance?\n\nAlso, what agent cmd/config would you recommend starting with?\nnpm run strategist -- --autoPlay",
                  "score": 1,
                  "created_utc": "2025-12-29 19:57:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpfy4y",
          "author": "implicator_ai",
          "text": "This is a fascinating setup‚Äîbasically they had the LLMs generate strategic decisions that the Civ V AI then executed, so the models weren‚Äôt directly controlling the game engine but guiding it. The small score bump vs. lower win rate suggests the models explore different strategies rather than optimizing for victory. \n\n\n\nIt‚Äôs  an early look at how open models might handle long-horizon planning tasks.",
          "score": 1,
          "created_utc": "2025-12-30 09:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs5t78",
          "author": "PeakBrave8235",
          "text": "Why the hell would I want this?",
          "score": -15,
          "created_utc": "2025-12-24 21:46:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs6j72",
              "author": "vox-deorum",
              "text": "To get LLMs play games while you keep working. /s",
              "score": 13,
              "created_utc": "2025-12-24 21:50:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvs8qbu",
                  "author": "PeakBrave8235",
                  "text": "I was genuinely asking",
                  "score": 0,
                  "created_utc": "2025-12-24 22:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvjpmb",
      "title": "Why I quit using Ollama",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/",
      "author": "SoLoFaRaDi",
      "created_utc": "2025-12-25 18:38:36",
      "score": 478,
      "num_comments": 194,
      "upvote_ratio": 0.91,
      "text": "For about a year, I've used Ollama like... 24/7. It was always my go-to, as it was frequently updated and had support for every model I needed.\n\nOver the past few months, there's been a serious decline in the updates & update content that releases with Ollama. I understand that, and just went about my day, as the maintainers obviously have a life. Cool! Then the \\*\\*Cloud\\*\\* update dropped. I saw Ollama as a great model runner, you just download a model and boom. Nope! They decided to combine proprietary models with the models uploaded on their Library. At first, it seemed cool. We can now run AI models that were otherwise impossible to run on consumer hardware, but then I started getting confused. Why did they add in Cloud, what's the point? What were the privacy implications? It just felt like they were adding more and more bloatware into their already massive binaries, so about a month ago, I made the decision, and quit Ollama for good.\n\nI feel like with every update they are seriously straying away from the main purpose of their application; to provide a secure inference platform for LOCAL AI models. I understand they're simply trying to fund their platform with the Cloud option, but it feels like a terrible move from the Ollama maintainers. \n\nWhat do you guys think?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvx8yxl",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 21:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwnwpq",
          "author": "q5sys",
          "text": "I soured on Ollama when (in the past) they phrased things that made it seem like the developments in llama.cpp were \"their\" improvements.  As an two decade long open source developer, I understand projects are built on the work of others, that's the exchange we make to let us dev what we want and we know that people can build on top our work.    \n  \nBut \"upstream's work\" is not \"your work\". Projects need to be honest about this.  You can still take credit for integrating upstreams work, but dont try to take credit for it.  \n  \nI don't know if they still do this, I hope they don't; but they certainly did in their early days and it really annoyed me.",
          "score": 316,
          "created_utc": "2025-12-25 18:52:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx2dpk",
              "author": "siegevjorn",
              "text": "Can't agree with this point more. They should make it clear that they are just a go wrapper around llama.cpp. Well, at the vary least, they should acknowledge llama.cpp SOMEWHERE....\n\nWhen I first started playing with LLMs, of course with ollama, it was a super confusing journey because of these ambiguity.   With ollama, you gain no whatsoever low-level details about llms. The original source for the llm weights; their format; how they are running. \n\nOn the other hand, Llama.cpp, is super clear on every low level stuff. You can literally find everything with time and effort. It just rocks, man.",
              "score": 95,
              "created_utc": "2025-12-25 20:19:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzk8q1",
                  "author": "mrjackspade",
                  "text": "> They should make it clear that they are just a go wrapper around llama.cpp. Well, at the vary least, they should acknowledge llama.cpp SOMEWHERE....\n\nTo be fair, they're *not* a wrapper around Llama.cpp\n\nThey're a wrapper around the ggml libraries.\n\nSo it would be weird to credit Llama.cpp, especially when they've already credited GGML\n\nSource:\n\nhttps://news.ycombinator.com/item?id=44805396",
                  "score": -4,
                  "created_utc": "2025-12-26 06:36:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvz5w6a",
              "author": "BusRevolutionary9893",
              "text": "While researching it back in the early days, I realized why not just use llama.cpp? It almost seemed easier. Is compiling source code really so scary with instructions and LLMs to help you out? I have no idea why Ollama caught on in the first place. Now that LM Studio has been out for awhile, I have no idea why Ollama is still talked about.¬†",
              "score": 12,
              "created_utc": "2025-12-26 04:36:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzaagh",
                  "author": "DonkeyBonked",
                  "text": "I actually started with llama.cpp and I had found a model I was having trouble finding the gguf for that the official source was insisting on using ollama. I downloaded ollama and I don't like it. I found it more difficult than the way I have llama.cpp set up. I have my own GUI setup for launching models and interfacing with them so I don't have to use my browser (between the browsers, extensions, and cookies/trackers, I don't trust browsers to be private).\n\nI do not see the appeal of Ollama.",
                  "score": 3,
                  "created_utc": "2025-12-26 05:10:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8covr",
                  "author": "FootballRemote4595",
                  "text": "Ollama caught on because it allowed you to swap models and it's simplified the out of the box experience.\n\n\nThen came llama swap.\n\n\n\nNow we have llama.cpp router.\n\n\nNow at this point it just gets in the way but for a time it was solving some pain points.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1l4rk",
                  "author": "q5sys",
                  "text": "A lot of people used them because it was \"easier\" and \"models run so fast\" without realizing that they were qauntizing the models down so they would run \"fast\", mostly turning them stupid... but they were fast.  And truth is a lot of people only cared about the speed for their ERP or other type chats where capability didn't really matter much. So word of mouth spread and people flocked to it.   \nBut yea, llama.cpp was the far superior solution, but it also required a little effort to read the docs to figure it out.  There's a lot of people that just want to install a package and go.",
                  "score": 1,
                  "created_utc": "2025-12-26 16:26:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0hvrw",
                  "author": "Western_Courage_6563",
                  "text": "Because ollama just works. Like fire and forget. And every other solution I tried, had some issues will it be with hardware, or software side of things...",
                  "score": 0,
                  "created_utc": "2025-12-26 12:13:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxt0g4",
              "author": "night0x63",
              "text": "Anyone with needing more concurrent users... Switch to SGLang or vLLM. Period. That's the main thing.¬†\n\n\nAny cloud provider... Also does this switch. Is Ollama now doing cloud inference? Yes. Therefore they are IMO doing vLLM.",
              "score": 10,
              "created_utc": "2025-12-25 23:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1830v",
              "author": "emetah850",
              "text": "This.\n\nand to add on top of that, the creators of Ollama seemingly refuse to add the proper licenses to the binaries, even after extensive discussion on their github as to why something like *crediting the people who's work you're using is the right thing to do*\n\nGithub issue for reference: [https://github.com/ollama/ollama/issues/3185](https://github.com/ollama/ollama/issues/3185)",
              "score": 3,
              "created_utc": "2025-12-26 15:15:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyiano",
              "author": "IaintJudgin",
              "text": "I think they EVENTUALLY credited llama.cpp (iirc)",
              "score": 3,
              "created_utc": "2025-12-26 01:50:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0fl6d",
                  "author": "Original_Finding2212",
                  "text": "Right before switching to their own engine and experiencing a major decent, then adding cloud support.",
                  "score": 3,
                  "created_utc": "2025-12-26 11:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fmat",
              "author": "Mikasa0xdev",
              "text": "Yo, local inference is the real innovation. lol",
              "score": 1,
              "created_utc": "2025-12-26 15:57:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzu7gj",
              "author": "Ylsid",
              "text": "Ollamma lets you feel like an elite haxxor. That's why people use it over alternatives. Maybe",
              "score": 0,
              "created_utc": "2025-12-26 08:13:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwngk7",
          "author": "HungryMachines",
          "text": "I have been switching my python workflows to llama.cpp from ollama. The only thing I missed was model switching. With the recent updates that should also be resolved.",
          "score": 81,
          "created_utc": "2025-12-25 18:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwzqx8",
              "author": "ubrtnk",
              "text": "That was the one thing that kept me on Ollama for so long - switching and auto unload after 5 minutes. Been using Llama-swap + llama.cpp for a couple of months now and its fantastic but yea I'm curious if the native switching functions will reduce overhead even more (llama-swap is already REALLY light weight)",
              "score": 38,
              "created_utc": "2025-12-25 20:03:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvx2y2n",
              "author": "siegevjorn",
              "text": "This. That is the only reason to run ollama instead of llama.cpp for anyone, if they had to.",
              "score": 5,
              "created_utc": "2025-12-25 20:22:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyswda",
                  "author": "khronyk",
                  "text": "Yeah same. My main pc has windows + 3090 but i also have a dual 3090 epyc server running proxmox with ubuntu for the ai vm so it adds greatly to the complexity when I have to set things up across both systems. Not exactly happy with ollama, but the model switching/auto-unloading is what has kept me from exploring other options.",
                  "score": 4,
                  "created_utc": "2025-12-26 03:02:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwxxdq",
          "author": "zhambe",
          "text": "Surprised not to see an mention of vLLM here. It's my stock go-to.",
          "score": 40,
          "created_utc": "2025-12-25 19:52:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx9n7b",
              "author": "Ryanmonroe82",
              "text": "Not as basic as lm studio or ollama but definitely the best option.",
              "score": 11,
              "created_utc": "2025-12-25 21:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0j0k",
                  "author": "rm-rf-rm",
                  "text": "And can't run on Mac.",
                  "score": 12,
                  "created_utc": "2025-12-25 23:53:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyyxd8",
                  "author": "_VirtualCosmos_",
                  "text": "\\*If you have enough vram to fit the entire BF16 base model in there\\*",
                  "score": 5,
                  "created_utc": "2025-12-26 03:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxakzb",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 3,
              "created_utc": "2025-12-25 21:10:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0rpa",
                  "author": "zhambe",
                  "text": "Yeah if you need to frequently reload different models, and performance (esp multiuser) is not your top priority, then vLLM might not be the top choice.\n\nI never compared how long it takes to shut down one instance of vLLM and spin up another with a different model vs swapping a model in ollama. I notice though the startup with vLLM takes a fair bit longer than ollama, so my guess is swapping models like that would be a drag.",
                  "score": 3,
                  "created_utc": "2025-12-25 23:55:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzvt7n",
                  "author": "Picard12832",
                  "text": "llama.cpp has that feature now.",
                  "score": 1,
                  "created_utc": "2025-12-26 08:30:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0l1wp",
                  "author": "Camvizioneer",
                  "text": "The model swapping problem with vLLM is exactly why I built llmsnap. Uses vLLM's sleep mode to swap models in \\~1-2 seconds instead of full restarts - just puts idle models to sleep.",
                  "score": 1,
                  "created_utc": "2025-12-26 12:40:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxc6ry",
              "author": "nickless07",
              "text": "Isn't that the thing that need Linux? So, at least a couple more Gigabyte to run in Docker/WSL?",
              "score": 3,
              "created_utc": "2025-12-25 21:20:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvz3fb4",
                  "author": "woct0rdho",
                  "text": "There is vllm-windows if you want to avoid the overhead of WSL",
                  "score": 3,
                  "created_utc": "2025-12-26 04:17:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyfu84",
                  "author": "burntoutdev8291",
                  "text": "What about just ubuntu?",
                  "score": 2,
                  "created_utc": "2025-12-26 01:33:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4jukv",
                  "author": "the_lamou",
                  "text": "What kind of WSL / Docker for Windows are you running that it's adding gigabytes of anything? The entire Linux kernel can be loaded in 200-some MB, and even the full standard Ubuntu Server kernel is like 512MB IIRC.",
                  "score": 1,
                  "created_utc": "2025-12-27 02:27:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwp9fx",
          "author": "cosimoiaia",
          "text": "Congrats, llama.cpp is the only way to go.",
          "score": 111,
          "created_utc": "2025-12-25 19:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxyh3n",
              "author": "MonteManta",
              "text": "It even runs on my Android phone in termux",
              "score": 8,
              "created_utc": "2025-12-25 23:40:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy50nk",
                  "author": "cosimoiaia",
                  "text": "Yup, there are a few app that use it but npu support is still quite lacking, I'm not sure if metal works on an iPhone though.",
                  "score": 1,
                  "created_utc": "2025-12-26 00:22:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyjv2u",
                  "author": "harbour37",
                  "text": "Can it access the gpu / npu on termux?",
                  "score": 1,
                  "created_utc": "2025-12-26 02:00:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyynym",
              "author": "_VirtualCosmos_",
              "text": "What about LM Studio?",
              "score": 8,
              "created_utc": "2025-12-26 03:43:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvxaiq7",
              "author": "Positive_Ad_313",
              "text": "Hi\nI am new just starting hosting Ollama on my NAS. What I read make me released I probably have to look at other option : can u give a link to llamaccp ?\nNothing to rely on Ollama ?",
              "score": -21,
              "created_utc": "2025-12-25 21:09:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxln57",
                  "author": "the_answer_is_penis",
                  "text": "Google?????? Apparently you're also just starting the internet",
                  "score": 5,
                  "created_utc": "2025-12-25 22:17:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwq5wc",
          "author": "mr_zerolith",
          "text": "They lost me when they lagged for months on supporting SEED OSS 36B just because they refused to update llama.cpp ( note: this it the smartest model that runs on a 5090 )  \nThat's when i switched sides to LM Studio.",
          "score": 24,
          "created_utc": "2025-12-25 19:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy57e5",
              "author": "Aggressive-Bother470",
              "text": "It might be the smartest model that runs on quite a bit more than a 5090, too.\n\n\nDoesn't immediately seem to work in vllm 0.13, annoyingly.",
              "score": 2,
              "created_utc": "2025-12-26 00:23:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0c0ux",
                  "author": "DinoAmino",
                  "text": "Did you follow instructions?\n\nhttps://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct#vllm",
                  "score": 1,
                  "created_utc": "2025-12-26 11:18:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwnzx7",
          "author": "noiserr",
          "text": "I just use self compiled llamacpp. I have scripts I use to manage models. The benefit is all the options and tweaks are exposed and you can enable stuff you can only enable only at compile time. \n\nSometimes a model support isn't merged right away. I can just point to the development fork and compile that if I want to. No need to wait for support which can sometimes take weeks.",
          "score": 31,
          "created_utc": "2025-12-25 18:53:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwwubd",
              "author": "StardockEngineer",
              "text": "What do you enable?  I usually just compile vanilla.",
              "score": 6,
              "created_utc": "2025-12-25 19:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwzzjm",
                  "author": "noiserr",
                  "text": "Mainly ROCm / AMD related stuff:\n\n    -DGGML_HIP=ON \\\n    -DLLAMA_HIP_UMA=ON \\\n    -DGGML_HIP_ROCWMMA_FATTN=ON \\\n    -DGGML_HIP_GRAPHS=ON \\",
                  "score": 12,
                  "created_utc": "2025-12-25 20:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwm72v",
          "author": "ChipsAreClips",
          "text": "I stopped Ollama 7-8 months ago and switched to LM Studio, I love it",
          "score": 77,
          "created_utc": "2025-12-25 18:43:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwphi7",
              "author": "AnotherSoftEng",
              "text": "Have they built out VLM support at all in the last few months? I remember trying to use it as an API, but it was missing some very basic features. Same for tool calling support - was very hit or miss. Ollama, on the other hand, has a lot of this functionality out of the box and it just worked for me.\n\nOtherwise I‚Äôd be using LM Studio for everything. I especially appreciate how they separate GGUF and MLX model variants.",
              "score": 11,
              "created_utc": "2025-12-25 19:01:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpzgm",
                  "author": "ChipsAreClips",
                  "text": "Yes, VLM support is currently great, and there are some great ComfyUI nodes that integrate for easy automation.",
                  "score": 9,
                  "created_utc": "2025-12-25 19:04:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2h8ru",
                  "author": "mgoetzke76",
                  "text": "Is lm studio using vllm ? Or what is vlm ?",
                  "score": 1,
                  "created_utc": "2025-12-26 19:15:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwqxnw",
              "author": "drakgremlin",
              "text": "Does LM Studio support an other applications running interference?",
              "score": 3,
              "created_utc": "2025-12-25 19:10:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwrj5z",
                  "author": "kiruz_",
                  "text": "You can load model and run server within app to connect through api, yes.",
                  "score": 12,
                  "created_utc": "2025-12-25 19:13:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyydrm",
              "author": "International_Quail8",
              "text": "Also switched to LM Studio (not completely, but will soon).  Primarily for their MLX support which I don't believe Ollama has yet.  Loving it so far and it's been an easy transition with enough functionality in their UI to give me the control that I need over different models.",
              "score": 3,
              "created_utc": "2025-12-26 03:41:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvwxy2v",
              "author": "umataro",
              "text": "No matter the model, no matter the settings, in LM studio the models always end up endlessly repeating themselves or spewing nonsense forever. In Ollama, things just work. I gave up on LM studio but now with the entshittification of ollama, I need to try something new.",
              "score": 7,
              "created_utc": "2025-12-25 19:52:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw19nii",
                  "author": "KernelFlux",
                  "text": "I have found this to be true as well. I run the same\nModel on Ollama (non mlx), no problem, lm studio infinite repeats. I just don‚Äôt get it. I am sticking with Ollama for now, pure local.",
                  "score": 2,
                  "created_utc": "2025-12-26 15:24:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwuu2m",
              "author": "TechnoByte_",
              "text": "LM Studio is closed source, enjoy your downgrade",
              "score": -16,
              "created_utc": "2025-12-25 19:33:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwwxtu",
                  "author": "No-Mountain3817",
                  "text": "Are you a purist who only runs fully open-source, open-data, open-weight models? If you‚Äôre using any closed source or partially closed models, then the argument that ‚ÄúLM Studio is closed and therefore shouldn‚Äôt be used‚Äù completely falls apart. It may be closed source, but it‚Äôs free to use, just like many open weight models people rely on every day.",
                  "score": 12,
                  "created_utc": "2025-12-25 19:46:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwyqpv",
                  "author": "uber-linny",
                  "text": "I think it's great as a entry point for beginners. Do I use it anymore... No . But it's what I learnt on",
                  "score": 7,
                  "created_utc": "2025-12-25 19:57:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvww4u0",
                  "author": "stanm3n003",
                  "text": "Brah... who the fk cares.\nYou use the server API to build around it.",
                  "score": 2,
                  "created_utc": "2025-12-25 19:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxgjr3",
          "author": "Mediocre_Second_2545",
          "text": "Check out [https://www.foundrylocal.ai/](https://www.foundrylocal.ai/) from Microsoft  - I work on it personally so I'm happy to answer any questions, and if you don't like it then I'm eagerly awaiting feedback :)",
          "score": 9,
          "created_utc": "2025-12-25 21:46:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0u7mx",
              "author": "johnerp",
              "text": "Oh coool. Thx.",
              "score": 1,
              "created_utc": "2025-12-26 13:48:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwmc43",
          "author": "No-Yak4416",
          "text": "What are you switching to?",
          "score": 16,
          "created_utc": "2025-12-25 18:43:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwqqj1",
              "author": "LostLakkris",
              "text": "I went llama-swap, little more complicated to manage, but I also averaged better performance to go with it",
              "score": 23,
              "created_utc": "2025-12-25 19:09:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwrz50",
                  "author": "PhilWheat",
                  "text": "Just in time for llama.cpp to add router capabilities?   (I'm still using llama-swap as well, but I want to see if I can simply things with the new capabilities.)",
                  "score": 15,
                  "created_utc": "2025-12-25 19:16:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwtn9i",
                  "author": "milkipedia",
                  "text": "Loving my llama-swap setup.",
                  "score": 5,
                  "created_utc": "2025-12-25 19:26:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwx18x",
              "author": "SoLoFaRaDi",
              "text": "Decided to switch to stock llama.cpp. Compiled it myself to leverage my CPU's Vulkan support, and it's seemingly going well. Love running SLM's with it :\\]",
              "score": 7,
              "created_utc": "2025-12-25 19:46:46",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw5001b",
              "author": "mcdenkijin",
              "text": "I am trying candle, tabbyapi, tabbyml, and vllm.  there's also jan and a few others",
              "score": 1,
              "created_utc": "2025-12-27 04:15:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwp7tx",
          "author": "RealLordMathis",
          "text": "If anyone's looking for an alternative for managing multiple models I've built an app with web ui for that. It supports llama.cpp, vllm and mlx_lm. I've also recently integrated llama.cpp router mode so you can take advantage of their native model switching. Feedback welcome!  \n\n[GitHub](https://github.com/lordmathis/llamactl)  \n[Docs](https://llamactl.org)",
          "score": 14,
          "created_utc": "2025-12-25 19:00:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxknc",
              "author": "StardockEngineer",
              "text": "Looks interesting!",
              "score": 1,
              "created_utc": "2025-12-25 19:50:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvww9xh",
          "author": "Southern_Sun_2106",
          "text": "Ollama was cool because it started model switching first, I believe. But then LM studio cleaned up their interface, has model switching - it's nice to have a GUI.",
          "score": 11,
          "created_utc": "2025-12-25 19:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxk2pi",
              "author": "luche",
              "text": "i've yet to figure out how to get it to run headless without requiring user login on macOS... which has been the biggest issue so far. with ollama, i simply created a launchdaemon to do this and it works fine. i've yet to get lm-studio's headless server to run via launchdaemon.",
              "score": 1,
              "created_utc": "2025-12-25 22:08:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx1cm3",
          "author": "emaiksiaime",
          "text": "I get best tok/sec with llama.cpp",
          "score": 7,
          "created_utc": "2025-12-25 20:12:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzfwtj",
          "author": "Mabuse046",
          "text": "Ollama isn't the one providing support for your models. Llama.cpp is. Ollama just packages up the models and launches llama.cpp for you. If you like ollama for being frequently updated to support new models, you're giving credit to the wrong people.",
          "score": 5,
          "created_utc": "2025-12-26 05:58:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwmepg",
          "author": "eat_my_ass_n_balls",
          "text": "LMStudio is better than Ollama",
          "score": 52,
          "created_utc": "2025-12-25 18:44:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwygck",
              "author": "Healthy-Nebula-3603",
              "text": "The best solution is llamacpp serveras it has a nice gui  or if you like console then llamacpp-cli",
              "score": 6,
              "created_utc": "2025-12-25 19:55:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvwuwbe",
              "author": "TechnoByte_",
              "text": "No it's not, it's closed source",
              "score": 0,
              "created_utc": "2025-12-25 19:33:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwwsrx",
                  "author": "stanm3n003",
                  "text": "\"nO iTs clOSeD sOurCe\"",
                  "score": 4,
                  "created_utc": "2025-12-25 19:45:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwwwoh",
                  "author": "StardockEngineer",
                  "text": "But it‚Äôs faster.",
                  "score": 2,
                  "created_utc": "2025-12-25 19:46:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwrn24",
          "author": "skyasher27",
          "text": "I use oogabooga to test models",
          "score": 11,
          "created_utc": "2025-12-25 19:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwvm4l",
              "author": "__SlimeQ__",
              "text": "this is the way",
              "score": 4,
              "created_utc": "2025-12-25 19:38:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwpxfc",
          "author": "Foreign-Beginning-49",
          "text": "I think they are a good entry for some beginners but they have done questionable things in the past. When you start using llama.cpp its a breath of fresh air once you undertake the learning process.",
          "score": 8,
          "created_utc": "2025-12-25 19:04:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx642x",
          "author": "Turkino",
          "text": "I've been using KoboldCPP for a long time, after switching from Oogabooga.\nNow trying out llamacpp as well for GLM.\nI definitely see an appeal in both.",
          "score": 6,
          "created_utc": "2025-12-25 20:42:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxca6f",
          "author": "fallingdowndizzyvr",
          "text": "I never saw the use for it. I've always been llama.cpp pure and unwrapped.",
          "score": 3,
          "created_utc": "2025-12-25 21:20:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvws0i4",
          "author": "iotsov",
          "text": "I am staying with Ollama. It doesn't bother me that they have Cloud models, I simply don't use them, at least for the time being. They might become relevant some time in 2026 though, the way things are going :)",
          "score": 12,
          "created_utc": "2025-12-25 19:16:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwsyyz",
              "author": "SubstanceWooden7371",
              "text": "I think it's foolish not to utilize the cloud models in addition to hosting your own locally.",
              "score": -21,
              "created_utc": "2025-12-25 19:22:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxs1do",
                  "author": "Bonzupii",
                  "text": "I think it's foolish to think that people who are looking for freedom and privacy are foolish for not giving up their freedom and privacy.",
                  "score": 5,
                  "created_utc": "2025-12-25 22:58:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwoq19",
          "author": "Educational_Rent1059",
          "text": "LM studio much better yes",
          "score": 13,
          "created_utc": "2025-12-25 18:57:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwuz1b",
              "author": "TechnoByte_",
              "text": "It's not open source, so no",
              "score": -9,
              "created_utc": "2025-12-25 19:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwv7f2",
                  "author": "Educational_Rent1059",
                  "text": "Rather closed local than what Ollama turns into -> cloud paid EDIT: This TechnoByte_ spams the same comment across the thread, he is probably insider.",
                  "score": 7,
                  "created_utc": "2025-12-25 19:35:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvxdduk",
                  "author": "Billthegifter",
                  "text": "Can I ask. Why Is It being closed source a problem?",
                  "score": 3,
                  "created_utc": "2025-12-25 21:27:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwrztv",
          "author": "Mount_Gamer",
          "text": "They say they don't collect data, still provide many new models for offline use, and for me it's a good fit. I can use my local AI for something I truly want privacy, and I get a chance to query many bigger cloud models if I'm not happy with the response with the local models, or any model really.. I get a chance to view many angles of the same conversion.",
          "score": 4,
          "created_utc": "2025-12-25 19:16:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxpbfj",
              "author": "The_frozen_one",
              "text": "I think this sub is being astroturfed. Flame wars are cheaper than buying ads.",
              "score": 5,
              "created_utc": "2025-12-25 22:41:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwz1rq",
          "author": "vulcan4d",
          "text": "I agree that Ollama is going downhill but others are going too.  It has been a while since I used LMStudio so I tried it again and I can't even load models well.  It chews up memory with the same context and settings like it is candy and it just struggles so I moved back to Ollama.  Ollama does recently have bugs where it doesn't even output but thinks it is done but at least I can load models without struggling.",
          "score": 2,
          "created_utc": "2025-12-25 19:58:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxf5ne",
              "author": "nickless07",
              "text": "Weird, as for me it's the other way around. Even the Ollama installer is 3 times the size. Perhaps turn off 'Keep model in memory'? In LM Studio, hold the ALT key before loading a model and it shows you in realtime how much RAM and VRAM the model takes with the current settings.",
              "score": 6,
              "created_utc": "2025-12-25 21:38:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxlwmg",
          "author": "Any_Fox5126",
          "text": "I'm glad to see some real criticism of ollama, instead of the endless pointless debates about whether, subjectively, they are giving enough recognition to llamacpp.",
          "score": 2,
          "created_utc": "2025-12-25 22:19:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxqv6z",
          "author": "taking_bullet",
          "text": "Thanks for sharing your experience. I tried LM Studio in the past, but switched back to Ollama. Ollama's GUI is very simple and clear, not filled with unnecessary options.¬†",
          "score": 2,
          "created_utc": "2025-12-25 22:50:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxt0yb",
          "author": "FBIFreezeNow",
          "text": "So what do you use, LM Studio?",
          "score": 2,
          "created_utc": "2025-12-25 23:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy65mg",
          "author": "hustla17",
          "text": "hahaha that title read like you quit some hard drug,\n\n well both are equally bad for the body /s\n\ngood job !",
          "score": 2,
          "created_utc": "2025-12-26 00:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz69fr",
          "author": "Over_Description5978",
          "text": "When you use ollama with local models you can run it  free and unlimited !  In corporate word to make profit out of any business they will provide only one. Either free (limited) or unlimited (paid)",
          "score": 2,
          "created_utc": "2025-12-26 04:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwv5kl",
          "author": "xandep",
          "text": "llama.cpp > LM Studio > Ollama",
          "score": 6,
          "created_utc": "2025-12-25 19:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwyhw0",
              "author": "random-tomato",
              "text": "llama-swap + llama.cpp > llama.cpp > LM Studio > Ollama\n\nFTFY :)",
              "score": 5,
              "created_utc": "2025-12-25 19:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxkdse",
                  "author": "luche",
                  "text": "i'll have to look into llama-swap. any chance there's a quick and easy way to get it to run on boot without requiring user login on macOS?",
                  "score": 1,
                  "created_utc": "2025-12-25 22:10:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx5wqu",
          "author": "aiueka",
          "text": "I would be happy to switch away from ollama but llama.cpp does not have a native implementation of the feature which unloads the model from VRAM after x minutes of inactivity, is there?\nAre there any containerized services that have this and have better open source practices?",
          "score": 3,
          "created_utc": "2025-12-25 20:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxgca7",
              "author": "BlackMetalB8hoven",
              "text": "Llama cpp has this now with the switch  \n --sleep-idle-seconds",
              "score": 4,
              "created_utc": "2025-12-25 21:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxjp7s",
                  "author": "luche",
                  "text": "can this be implemented directly into open-webui or litellm?",
                  "score": 1,
                  "created_utc": "2025-12-25 22:05:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzk0s4",
                  "author": "AppearanceHeavy6724",
                  "text": "When it finally work, it would be nice if they add a feature to run a script before unloading model. This will fix the notorious 20W idle with 30xx cards, as now we'd be able to reset them right after unloading the model.",
                  "score": 0,
                  "created_utc": "2025-12-26 06:34:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5a9r9",
              "author": "social_tech_10",
              "text": "llama-swap is a thin wrapper around llama.cpp that let's you change models on-th-fly using open-webui, or any other UI that supports the API.\n\nThe main reason I switched from ollama to llama.cpp and llama-swap is because llama-swap let's me easily set the exact command-line options for every model, including the TTL (time-to-live) feature you mentioned, which really \"lifts the fog\" surrounding ollama.",
              "score": 1,
              "created_utc": "2025-12-27 05:30:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx7jio",
          "author": "neonexius",
          "text": "Try Lemonade https://github.com/lemonade-sdk/lemonade",
          "score": 2,
          "created_utc": "2025-12-25 20:51:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0gjj",
          "author": "rm-rf-rm",
          "text": "Welcome to the other side! Thank the lord for llama.cpp and llama-swap! Hope they never go to the dark side",
          "score": 2,
          "created_utc": "2025-12-25 23:53:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyl8tv",
          "author": "StomachWonderful615",
          "text": "I switched from ollama to using mlx-lm on my macs.",
          "score": 2,
          "created_utc": "2025-12-26 02:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwqaqb",
          "author": "Objective_Frosting58",
          "text": "I started using ollama when I hadn't yet bought my gpu, I was running on my 7950x and 64gb ram. I didn't have any issues with it until I got my 9070xt and found that I couldn't get it to work. So switched to llama.cpp, its not as easy to use but works better",
          "score": 1,
          "created_utc": "2025-12-25 19:06:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwsldl",
          "author": "vir_db",
          "text": "Which alternative for an Openai-compatible API?",
          "score": 1,
          "created_utc": "2025-12-25 19:20:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvww6pw",
              "author": "Repulsive-Memory-298",
              "text": "Pretty much everything, that is the standard",
              "score": 3,
              "created_utc": "2025-12-25 19:41:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx0fsm",
                  "author": "vir_db",
                  "text": "Very interesting ü§î. An example please?",
                  "score": 1,
                  "created_utc": "2025-12-25 20:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx1cgh",
          "author": "FaceDeer",
          "text": "It's still working fine for me, so I keep on using it. Switching platforms is a hassle so I'm going to wait until the hassle of using it is greater than the hassle of switching.",
          "score": 1,
          "created_utc": "2025-12-25 20:12:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx4dwq",
          "author": "productboy",
          "text": "Different tools for different tasks. I install Ollama for my team; for people who need an easy to use alternative to the frontier labs products. Meanwhile, I use many tools including Ollama, llama.cpp. As others noted; Ollama is a great entry point for beginners; which is useful to learn from their experience.",
          "score": 1,
          "created_utc": "2025-12-25 20:31:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx9yqg",
          "author": "Cferra",
          "text": "A lot of the beginners guides (a beginner to this myself right now) right away point everyone to ollama or more recently lm studio.",
          "score": 1,
          "created_utc": "2025-12-25 21:06:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxlhjt",
          "author": "TheMcSebi",
          "text": "I've had the exact same thoughts about the cloud move.\n\nHasn't bothered me enough to switch, though. I'll propably still be using it for the sake of simplicity, so I don't need to change all my scripts that talk to an llm",
          "score": 1,
          "created_utc": "2025-12-25 22:16:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyi9w7",
          "author": "Direct_Turn_1484",
          "text": "Yeah the cloud update was not a great move for us, but probably great for them. I still use Ollama for some things, because for local stuff it‚Äôs still an easy call to do inference with some model quickly, without having to load up the right container or adjust command line parameters. Just load the model and go real quick. For anything more involved, I agree Ollama isn‚Äôt the go to app.",
          "score": 1,
          "created_utc": "2025-12-26 01:50:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyias0",
          "author": "Ok_Condition4242",
          "text": "https://preview.redd.it/1tlks5s1fg9g1.png?width=952&format=png&auto=webp&s=d8ffdd8f1e062a183c87412505ab733a086917ba\n\nWhen you compile llama.cpp for the first time.",
          "score": 1,
          "created_utc": "2025-12-26 01:50:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvynmul",
          "author": "rageling",
          "text": "it's the same with comfyui, they mostly advertise their cloud service integration now",
          "score": 1,
          "created_utc": "2025-12-26 02:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzs23q",
          "author": "mrdevlar",
          "text": "Someone really needs to make a migration guide from Ollama if they want people to use something else.",
          "score": 1,
          "created_utc": "2025-12-26 07:51:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0agi1",
          "author": "Jayden_Ha",
          "text": "They gotta make money somehow \n\nIt‚Äôs free what do you expect",
          "score": 1,
          "created_utc": "2025-12-26 11:02:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0j4ca",
          "author": "TechnicalGeologist99",
          "text": "Use vLLM and liteLLM. They obey the openAI API standard more strictly which means that it's easier to swap inference providers. They're also designed for stability and scale (which ollama is not)",
          "score": 1,
          "created_utc": "2025-12-26 12:24:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1jh42",
          "author": "Fabix84",
          "text": "Yea... Ollama is dead.",
          "score": 1,
          "created_utc": "2025-12-26 16:18:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3jnv9",
          "author": "shroddy",
          "text": "ComfyUI users: First time?",
          "score": 1,
          "created_utc": "2025-12-26 22:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5dafz",
          "author": "Swaraj-Jakanoor",
          "text": "Just a thing we could still use local ollama modal and use our own cloud and it will not affect a thing right?",
          "score": 1,
          "created_utc": "2025-12-27 05:54:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5hg9d",
          "author": "ocirs",
          "text": "Not a fan of ollama either, for mac lm studio is a better integrated env. And vllm is much higher perf on linux. Ollama is a very thin wrapper around llama.cpp and feel they've been more focused on making money.",
          "score": 1,
          "created_utc": "2025-12-27 06:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6jzyu",
          "author": "GregoryfromtheHood",
          "text": "It was always a bad model runner. Gave an easy way for any old person to run a model with no knowledge, but used misleading naming for models and defaults to q4 for pretty much everything. In addition, sets less than ideal default params and obfuscates away the context length.",
          "score": 1,
          "created_utc": "2025-12-27 12:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7f4j5",
          "author": "Sea_Layer_6679",
          "text": "Ollama is shit to set up with AMD",
          "score": 1,
          "created_utc": "2025-12-27 15:50:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw831ba",
          "author": "Separate_Long_6962",
          "text": "they lost me when their download and install process was so fucking bad that I couldn't handle it anymore so I moved over to KoboldCPP and download the models manually from hugging face. Ollama sucks.",
          "score": 1,
          "created_utc": "2025-12-27 17:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9zybs",
          "author": "Some_Policy_6195",
          "text": "Yeah the cloud integration feels like such a weird pivot, like why would I want cloud models when the whole point was running stuff locally? I switched to llamafile a few weeks back and honestly haven't looked back - way simpler and does exactly what ollama used to do without all the extra stuff",
          "score": 1,
          "created_utc": "2025-12-28 00:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy56pr",
          "author": "RegularPerson2020",
          "text": "They just added something new with cloud.  I haven't noticed a difference with the local models.  My latest favorite is running Nemotron locally.  It runs well on my mini PC!!!  Incredible.  \n\nOllama has been giving to the local ai community for free for many many years.  I think they've earned some consideration and understanding for that.  They are trying new things, that's better than just getting left behind.  \n\nI get it.  I'm big on my privacy, self hosting too.  I get it, I wish they were perfect and quick to fix stuff.  But I also have to remind myself that they gave me a lot for free and have been my go-to for years...for free.",
          "score": 1,
          "created_utc": "2025-12-26 00:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwtqgc",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2025-12-25 19:26:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx3zit",
              "author": "Savantskie1",
              "text": "You probably know this but I‚Äôm betting petty too. In the windows app they included a gui chat app now and they have both cloud and local models in the model chooser with no delineation between which are local and which are the cloud models unless you strictly run it just in windows cli. There I treated you just as dumb as you treat others",
              "score": 2,
              "created_utc": "2025-12-25 20:29:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxe9cv",
          "author": "_takasur",
          "text": "I quit it 5 minutes every time I install it hoping it would have become a better tool.",
          "score": 1,
          "created_utc": "2025-12-25 21:32:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxgs2x",
          "author": "bestofbestofgood",
          "text": "Ollama is just a llama.cpp wrapper. Since llama.cpp can run UI for serve as API - what's the point of ollama anymore",
          "score": 1,
          "created_utc": "2025-12-25 21:48:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxjcp7",
              "author": "luche",
              "text": "what do you recommend to set memory timers and handle calling different models upon request? can llama.cpp do this natively now?",
              "score": 1,
              "created_utc": "2025-12-25 22:03:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxn3wj",
                  "author": "bestofbestofgood",
                  "text": "Not as far as I know.",
                  "score": 1,
                  "created_utc": "2025-12-25 22:27:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyaz0k",
                  "author": "Eugr",
                  "text": "Llama.cpo has its own model router now. If you need something a bit more flexible that can also load other inference engines, you can use llama-swap.",
                  "score": 1,
                  "created_utc": "2025-12-26 00:59:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxscug",
          "author": "IrisColt",
          "text": "I quit using ollama too, a week ago. Best decision ever... llama.cpp is a beast. Time to migrate my custom Python scripts, I guess.",
          "score": 1,
          "created_utc": "2025-12-25 23:00:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy2744",
          "author": "deltatux",
          "text": "I switched over to llama.cpp because it has Intel Arc support and it performs better than Ollama imo.",
          "score": 1,
          "created_utc": "2025-12-26 00:04:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyihcf",
          "author": "IaintJudgin",
          "text": "if you happen to use a mac [https://github.com/ggml-org/LlamaBarn](https://github.com/ggml-org/LlamaBarn) is cool (though it offers limited models -- only main/popular ones that your mac can run).\n\nEdit: agree llama.cpp is the proper way to go",
          "score": 1,
          "created_utc": "2025-12-26 01:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxm4qi",
          "author": "a_beautiful_rhind",
          "text": "Used many backends to run different models as required. Never did I have a need for one to be ollama.\n\nScreams \"I learned of LLMs from some influencer video\" in my eyes.",
          "score": 0,
          "created_utc": "2025-12-25 22:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0uoh",
          "author": "rm-rf-rm",
          "text": "RIP Ollama \n2023-2025 \n\n(it was early this year their mask started coming loose)",
          "score": 0,
          "created_utc": "2025-12-25 23:55:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy45lu",
          "author": "g_rich",
          "text": "vLLM < llama.cpp < LM Studio < Ollama\n\nOllam is fine for getting started but it‚Äôs limited and other than being turn key doesn‚Äôt do anything better than LM Studio. \n\nLlama.cpp is the most versatile while vLLM is the most powerful but has limited system support when compared to llama.cpp so it‚Äôs not an option for everyone. \n\nOverall my go to for getting started is LM Studio and for someone that wants to do a little more llama.cpp.",
          "score": 0,
          "created_utc": "2025-12-26 00:16:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyxxyx",
          "author": "the-final-frontiers",
          "text": "¬†bait and switch",
          "score": 0,
          "created_utc": "2025-12-26 03:38:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxqta9",
          "author": "Dry_Yam_4597",
          "text": "If in the age of vibe coding you still use ollama and ollama web ui then it's a you problem.",
          "score": -4,
          "created_utc": "2025-12-25 22:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwmqiv",
          "author": "Ecstatic_Signal_1301",
          "text": "It just sounds like you lack a vram and blaming it on ollama.",
          "score": -31,
          "created_utc": "2025-12-25 18:46:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pweljh",
      "title": "NVIDIA has 72GB VRAM version now",
      "subreddit": "LocalLLaMA",
      "url": "https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/",
      "author": "decentralize999",
      "created_utc": "2025-12-26 20:48:17",
      "score": 464,
      "num_comments": 148,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/",
      "domain": "nvidia.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw4smr8",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-27 03:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw36f8z",
          "author": "slavik-dev",
          "text": "checking bhphotovideo prices:\n\n\\- RTX 5000 48GB - $5100 (14,080 CUDA Cores, 384-bit memory)\n\n\\- RTX 5000 72GB - $7800 (14,080 CUDA Cores, 512-bit memory)\n\n\\- RTX 6000 96GB - $8300 (24,064 CUDA Cores, 512-bit memory)\n\nRTX 5000 72GB doesn't appear to be good deal...",
          "score": 240,
          "created_utc": "2025-12-26 21:32:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3j84w",
              "author": "__JockY__",
              "text": "Yuck, it‚Äôs the worst deal of the bunch.",
              "score": 86,
              "created_utc": "2025-12-26 22:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw59iot",
                  "author": "Maleficent-Ad5999",
                  "text": "Decoy effect in action",
                  "score": 32,
                  "created_utc": "2025-12-27 05:24:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw41wuo",
              "author": "BobbyL2k",
              "text": "RTX Pro 5000 with 72GB has the same 384-bit memory bus, not 512-bit. It‚Äôs the same GPU as the 48GB version, with the upgrade to 3GB GDDR7 modules from 2GB.",
              "score": 23,
              "created_utc": "2025-12-27 00:34:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3kmxg",
              "author": "ThenExtension9196",
              "text": "Hey don‚Äôt forgot the rtx 4000 pro! 24G $1499 (~8k cuda cores). Just picked one up for my surveillance camera server to run inference on snapshots after motion is detected.",
              "score": 22,
              "created_utc": "2025-12-26 22:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw438oq",
                  "author": "lannistersstark",
                  "text": ">  Just picked one up for my surveillance camera server to run inference on snapshots after motion is detected.\n\nSurely you can run frigate on much, much cheaper hardware?",
                  "score": 19,
                  "created_utc": "2025-12-27 00:42:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw44f33",
                  "author": "nuusain",
                  "text": "Neat! What kinda inference u running on the feed? Just installed a security system for a relatives farm. I was thinking of producing reports /audits so im curious what stuff others are building for themselves.",
                  "score": 7,
                  "created_utc": "2025-12-27 00:49:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5kon7",
                  "author": "claythearc",
                  "text": "Is that needed? We‚Äôre running RT DETR for some real time detection stuff at work and hit 60 fps on an integrated laptop gpu. \n\nResolution will change it some, but surely not that much?",
                  "score": 3,
                  "created_utc": "2025-12-27 06:59:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5cbu7",
                  "author": "robertpro01",
                  "text": "What exactly are your doing with it? I'm interested!",
                  "score": 3,
                  "created_utc": "2025-12-27 05:46:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6352w",
              "author": "Free-Internet1981",
              "text": "Yeah no thanks, it should be a 4k card",
              "score": 3,
              "created_utc": "2025-12-27 09:57:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6ttb7",
              "author": "gweilojoe",
              "text": "$500 difference - that‚Äôs it? What a terrible deal.",
              "score": 3,
              "created_utc": "2025-12-27 13:45:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3caiz",
              "author": "SilentLennie",
              "text": "Let me guess, they are releasing something, because they can't add a new line up ?",
              "score": 3,
              "created_utc": "2025-12-26 22:04:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3zn3p",
                  "author": "PentagonUnpadded",
                  "text": "Moving an Ada/Blackwell-class GPU from TSMC 4N (current) to a next gen like N3E likely would give ~6‚Äì9% perf gain at iso power, assuming no other advancements. Given the yields Apple has had (poor) with those next generation nodes, it ought to cost quite a bit more vs 4N. \n\nEveryone wants a cheaper version of the existing high-ram products. A 6090 that's 10% faster than 5090 is not compelling for home ai use if it costs 15% more. Ram is the bottleneck, evidenced by how beloved 3090s are. The only customers who would pay an exorbitantly higher up front cost for such a new node are datacenters concerned with cooling / power draw concerns that make it profitable after years of always on operation.",
                  "score": 1,
                  "created_utc": "2025-12-27 00:21:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8ljs3",
              "author": "WitAndWonder",
              "text": "Right? Like.. \"just\" buy a second 5090. Cheaper even at the inflated prices and would up inference / training significantly as opposed to these shitty cards. Higher demands from Mobo / PSU, but let's be real, the 2k$ saved can easily cover that and  more. Nvidia has its head so far up its ass. I would love to see them get their just desserts for treating their customer base like disposable socks.",
              "score": 2,
              "created_utc": "2025-12-27 19:25:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi7j5p",
              "author": "Lazy-Pattern-5171",
              "text": "I think they did this because their 96GB card has been dropping in price for quite some time.",
              "score": 1,
              "created_utc": "2025-12-29 06:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8wjig",
              "author": "AmazinglyObliviouse",
              "text": "Jesus, I was telling people they were too optimistic about the prices of the 72gb version a few months ago when it leaked, but my own estimate at the time was 6k usd.",
              "score": 1,
              "created_utc": "2025-12-27 20:23:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw794lp",
              "author": "chibop1",
              "text": "If you can tolerate slow prompt processing:\n\nM3Ultra 512GB - $9,899 (Comes with CPU, Motherboard, PSU, 2TB SSD, WiFI, Bluetooth)",
              "score": 0,
              "created_utc": "2025-12-27 15:18:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2youu",
          "author": "ArtisticHamster",
          "text": "I think they need to produce 128Gb or even larger version, not 72Gb one.",
          "score": 268,
          "created_utc": "2025-12-26 20:50:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw31ndc",
              "author": "StaysAwakeAllWeek",
              "text": "If it was that easy they would. But it's not.\n\nGetting to 96GB already requires using the largest VRAM chips on the market, attached two chips per bus (which is the maximum) to the largest GDDR bus ever fitted to a GPU.\n\nThey would need a 640 bit wide bus to reach 120GB",
              "score": 109,
              "created_utc": "2025-12-26 21:06:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw335cm",
                  "author": "ArtisticHamster",
                  "text": "It's not easy, but it's not impossible. They put much more RAM on the datacenter GPUs.\n\nUPD. According to /u/StaysAwakeAllWeek it seems that GB200 is two chips with 96Gb each combined into one thing. This explains everything.",
                  "score": 51,
                  "created_utc": "2025-12-26 21:14:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3bd5t",
                  "author": "SRSchiavone",
                  "text": "Didn‚Äôt the Titan V CEO edition use HBM2 for a 4096-bit wide bus? \n\nPlus, doesn‚Äôt the H200 already have 141gB with only one package?",
                  "score": 7,
                  "created_utc": "2025-12-26 21:59:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw43x8y",
                  "author": "Massive-Question-550",
                  "text": "Isn't that more about bandwidth than capacity? For example a 5060ti has a 128 bit bus VS 256 for a 5070ti yet they both have the same memory capacity.¬†",
                  "score": 2,
                  "created_utc": "2025-12-27 00:46:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3qy6p",
                  "author": "shivdbz",
                  "text": "Just increase bus bandwidth, they only have to increase pcb trace complexity and sell it for low prices so buy go home happy.",
                  "score": 2,
                  "created_utc": "2025-12-26 23:28:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3dixv",
                  "author": "IAmFitzRoy",
                  "text": "Memory capacity is defined by price strategy‚Ä¶ not because it‚Äôs easy to make or not. \n\nCheck any brand and you will see the same pattern, it‚Äôs not only Apple or NVIDIA doing it.. Samsung, Google, Dell ‚Ä¶ all of them.",
                  "score": 0,
                  "created_utc": "2025-12-26 22:11:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw33f7f",
              "author": "sassydodo",
              "text": "yes, considering chip prices, let's ask for 512gb version, since I can't have it anyways, why not abstain from even larger vram",
              "score": 19,
              "created_utc": "2025-12-26 21:16:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw38tz5",
                  "author": "profcuck",
                  "text": "Terabyte or bust!",
                  "score": 8,
                  "created_utc": "2025-12-26 21:45:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4affg",
                  "author": "rog-uk",
                  "text": "Merry Xmas, Tiny Tim :-)",
                  "score": 2,
                  "created_utc": "2025-12-27 01:27:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2zlq4",
              "author": "TheLexoPlexx",
              "text": "You can just buy two /s",
              "score": 10,
              "created_utc": "2025-12-26 20:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw331w7",
                  "author": "ac101m",
                  "text": "The more you buy, the more you save!",
                  "score": 19,
                  "created_utc": "2025-12-26 21:14:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw4iisu",
              "author": "DAlmighty",
              "text": "I wouldn‚Äôt be able to afford a card with 128GB of VRAM, but I‚Äôd sure as shit try to.",
              "score": 3,
              "created_utc": "2025-12-27 02:19:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3lxcf",
              "author": "AbheekG",
              "text": "Hopefully Rubin takes us to 128GB per GPU, and continues with the 300W Max-Q variants. That would allow for 512GB VRAM with just 4xGPUs at 1200W ü§§",
              "score": 2,
              "created_utc": "2025-12-26 22:58:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3a5et",
              "author": "Technical_Ad_440",
              "text": "that would be great and all but the 96gb one is 8k the issue this has its over specked for 40gb models under specked for 80gb models. i assume this would cause more 60gb models though and could be entry under the rtx 6000 96gb something we may be able to see ourselves since it should be around 6k hopefully 5k. i just want more affordable for us guys at home",
              "score": -2,
              "created_utc": "2025-12-26 21:52:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw346f1",
          "author": "StableLlama",
          "text": "Wake me up when the 5090 has 48 GB",
          "score": 51,
          "created_utc": "2025-12-26 21:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw376vm",
              "author": "El-Dixon",
              "text": "R.I.P",
              "score": 31,
              "created_utc": "2025-12-26 21:36:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3hzk8",
                  "author": "StableLlama",
                  "text": "Some Chinese will upgrade it to 64 GB or even 128 GB, so it's not presumptuous to ask for 48 :)",
                  "score": 18,
                  "created_utc": "2025-12-26 22:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2z4fy",
          "author": "emprahsFury",
          "text": "The price per gig is the same. There's no added or lost value, which makes the choice easy. Buy the most you can afford",
          "score": 47,
          "created_utc": "2025-12-26 20:52:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw34zsp",
              "author": "HushHushShush",
              "text": "The more you buy, the more you are anchored to a particular generation.",
              "score": 24,
              "created_utc": "2025-12-26 21:24:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5dwhn",
                  "author": "mcnbc12",
                  "text": "[The more you buy, the more you save](https://m.youtube.com/shorts/HRQhkjsBOXM)",
                  "score": 2,
                  "created_utc": "2025-12-27 06:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw35fdi",
          "author": "Prudent-Corgi3793",
          "text": "Any reason to get this over the RTX 6000 Pro 96 GB?",
          "score": 13,
          "created_utc": "2025-12-26 21:27:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3mq3k",
              "author": "HumanDrone8721",
              "text": "Nope, the price difference is marginal, is not 25% cheaper for 25% less VRAM. I've almost did a double take then I've looked for them and saw something like 4K EUR, until I've realized that is the variant with 48GB and the proper SKU for 72GB is VCNRTXPRO5000B72-PB and that costs practically the same as the 96GB variant.",
              "score": 10,
              "created_utc": "2025-12-26 23:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3pirr",
                  "author": "Evening_Ad6637",
                  "text": "And the bandwidth is also 25% slower (1.3 TB/s vs 1.8 TB/s)",
                  "score": 3,
                  "created_utc": "2025-12-26 23:20:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw44mfe",
          "author": "NikoKun",
          "text": "I wonder.. If in a few years, we'll see a game console with these levels of VRAM, for running AI world-models that let you experience endless gaming worlds.",
          "score": 6,
          "created_utc": "2025-12-27 00:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34il6",
          "author": "ImportancePitiful795",
          "text": "This product makes no sense. In most countries is just ‚Ç¨1000 from the 96GB one.",
          "score": 18,
          "created_utc": "2025-12-26 21:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw44f54",
          "author": "Massive-Question-550",
          "text": "Realistically even 96gb isn't enough for the price. What people want is an \"affordable\" gpu with a lot of vram. Something with 5080 speed but 96 gb for like $3-4k would be reasonable.¬†",
          "score": 7,
          "created_utc": "2025-12-27 00:49:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4g83u",
              "author": "munkiemagik",
              "text": "In that price range even I would bite your hand off to buy something like that and I'm not even an IT professional who uses them for anything productive, I just find it all interesting and mess around in my spare time. But I'm not going to hold my breath, that capability is not going to hit that price range for several more years.",
              "score": 4,
              "created_utc": "2025-12-27 02:04:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2z4fl",
          "author": "Herr_Drosselmeyer",
          "text": "I think that's partially true. 48 just doesn't cut it these days, but they also don't want to directly compete against the 6000 PRO, so 72 is a compromise.",
          "score": 7,
          "created_utc": "2025-12-26 20:52:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ivlk",
          "author": "__JockY__",
          "text": "72GB is such a weird number. 128GB? Sure. 192GB? Bring it. 256GB? You get the idea.\n\nBut 72GB‚Ä¶ I just don‚Äôt get it. Who is this marketed at?",
          "score": 4,
          "created_utc": "2025-12-26 22:41:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw480he",
              "author": "BobbyL2k",
              "text": "The numbers are dictated by the memory configuration.\n\n- 5090 and Pro 6000 have 512-bit bus\n- 3090, 4090, and Pro 5000 has 384-bit bus\n- 5070 Ti and 5080 have 256-bit bus\n\nEach 32-bit of memory bus can either connect to 1 or 2 memory modules. There are two GDDR7 modules: 2GB and 3GB. There are two GDDR6X modules: 1GB and 2GB.\n\n- 512-bit can fit 16 or 32 modules \n  - 5090 with 2GBx16=32GB\n  - Pro 6000 with 3GBx32=96GB\n\n- 384-bit can fit 12 or 24 modules\n  - Pro 5000 with 2GBx24=48GB or 3GBx24=72GB\n  - 4090 with 2GBx12=24GB (GDDR6X)\n  - 3090 with 1GBx24=24GB (GDDR6X)\n\n- 256-bit can fit 8 or 16 modules\n  - 5080 with 2GBx8=16GB\n  - 5070 Ti with 2GBx8=16GB",
              "score": 19,
              "created_utc": "2025-12-27 01:11:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4yxhk",
                  "author": "__JockY__",
                  "text": "Thanks for the technical explanation!\n\nStill doesn‚Äôt change the fact that the 72GB model is a terrible deal!",
                  "score": 5,
                  "created_utc": "2025-12-27 04:07:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3v7j6",
              "author": "LightShadow",
              "text": "The people that need 120gb models on two cards.",
              "score": 3,
              "created_utc": "2025-12-26 23:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw47idj",
                  "author": "__JockY__",
                  "text": "Ok, but for another $500 the 96GB is available and I‚Äôd argue the most people spending $7800 on a 72GB card have both an extra $500 and a good use for that extra VRAM! 72GB at that price is a terrible deal. $6k? Ok, I could see it‚Ä¶ but at $500 less than a 96GB it just seems silly.",
                  "score": 3,
                  "created_utc": "2025-12-27 01:08:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3ts27",
          "author": "Rollingsound514",
          "text": "They throw these into Dell Workstations, best bet is to wait a bit and get refurb dell work station part outs from resellers",
          "score": 2,
          "created_utc": "2025-12-26 23:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5qna1",
          "author": "monoidconcat",
          "text": "The price doesn‚Äôt seem attractive‚Ä¶",
          "score": 2,
          "created_utc": "2025-12-27 07:55:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5uwo3",
          "author": "ab032tx",
          "text": "waiting for the day I can run deepseek 3.2 locally on my iphone",
          "score": 2,
          "created_utc": "2025-12-27 08:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7x2qi",
          "author": "No_Comment_Acc",
          "text": "I have 4890 (48 Gb 4090) but still want RTX 6000 Pro. Not gonna happen until bank robbers forget their bag with money in my car (I don't have a car).",
          "score": 2,
          "created_utc": "2025-12-27 17:21:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3owcr",
          "author": "No_Damage_8420",
          "text": "Definite BUY for AI Toolkit Wan 2.1 LORA training",
          "score": 2,
          "created_utc": "2025-12-26 23:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw69ice",
          "author": "deep_chungus",
          "text": "buh, god damn i hope the ai bubble pops hard, this is like the crypto bubble only every single tech company wants it to succeed\n\nthen again in ten years they'll figure out \"you need a bunch of video card hardware to make clone organs\" or something and we'll be playing half life on abacuses",
          "score": 2,
          "created_utc": "2025-12-27 11:00:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38665",
          "author": "Rockclimber88",
          "text": "Where's 512GB GPU? Apple Mac Studio comes with up to 512GB and Nvidia disappoints with this overpriced lame shit.",
          "score": -1,
          "created_utc": "2025-12-26 21:42:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwebl3f",
              "author": "MoMoneyMoStudy",
              "text": "Apple really only useful for inference.  But try out some finetuning of large OSS models.",
              "score": 2,
              "created_utc": "2025-12-28 17:52:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3bf7b",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -7,
              "created_utc": "2025-12-26 21:59:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3vnax",
                  "author": "Rockclimber88",
                  "text": "What are you even talking about? It's RAM available to the GPU",
                  "score": 2,
                  "created_utc": "2025-12-26 23:57:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3535j",
          "author": "nofilmincamera",
          "text": "I talked to a nvidia partner about this,  as I was curious the business pricing for 1. I won't share the price, but the 48GB almost makes sense. These could have some niche uses, price is on relatively.  But it has lower Cuda Cores than the 5090.  Everything I would want a 48gb i could makecwork on 32, with Cores mastering more that 16 gb difference.\n\n78gb is just stupid, like 600 difference.",
          "score": -2,
          "created_utc": "2025-12-26 21:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3q3fk",
          "author": "Buff_Grad",
          "text": "How does Apple manage to pull off the insane integrated RAM into their silicon with such good stats?",
          "score": 0,
          "created_utc": "2025-12-26 23:23:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4mie5",
              "author": "davidy22",
              "text": "Because it's the RAM that they're using.",
              "score": 5,
              "created_utc": "2025-12-27 02:44:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4j7v8",
          "author": "DAlmighty",
          "text": "I‚Äôm fairly confident that Nvidia‚Äôs recent license deal will produce cards for inference only.  That could possibly be a great thing for the community.",
          "score": 0,
          "created_utc": "2025-12-27 02:23:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw346wy",
          "author": "seppe0815",
          "text": "its about tensor core ... who want 48gb and low tensors ... useless",
          "score": -4,
          "created_utc": "2025-12-26 21:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw30doe",
          "author": "zasura",
          "text": "this will sound controversial but what's the point? All the good models are closed source like claude. Open source are great but... lack that \"spice\" that makes them better than everything else.",
          "score": -23,
          "created_utc": "2025-12-26 20:59:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw31ctz",
              "author": "LoSboccacc",
              "text": "Eh theres plenty good model nov in the .5 1.5 teraweight range. Not something we can run, but claude at home exists, theoretically speaking. (But lets say claude 3.5, tops)\n\n\nAnd look new technique are making smaller models more and more viable. Haiku 4.5 is surprisingly good, as soon as so e lab can guess their recipe well have models for 96gb pro cards.",
              "score": 10,
              "created_utc": "2025-12-26 21:05:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw34125",
              "author": "Photoperiod",
              "text": "Lotta infosec departments in companies don't want their data going to third parties. Depending on the industry running open source on your own hardware is required. That said I generally agree. Claude is crazy good.",
              "score": 6,
              "created_utc": "2025-12-26 21:19:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw31tad",
              "author": "nntb",
              "text": "Imagine having this view on this subreddit lol",
              "score": 8,
              "created_utc": "2025-12-26 21:07:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw38ny7",
              "author": "Lissanro",
              "text": "I disagree... There are plenty of smaller capable local models for any rig from small to medium size (like GLM or MiniMax series) to large size (DeepSeek and Kimi), so it is possible to find reasonably good models for almost any hardware.\n\nI run mostly either K2 0905 or K2 Thinking on my PC (IQ4 or Q4\\_X quants respectively, using ik\\_llama.cpp), depending on if I need thinking or not, and find them quite good in my daily work, or for personal use. I do not feel like I am missing out on anything by avoiding dependency on cloud models, but gain privacy and reliability (no one can take models I use offline or change them, and I can safely rely on them always be available unless I decide to replace them myself).",
              "score": 4,
              "created_utc": "2025-12-26 21:44:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ac06",
              "author": "tat_tvam_asshole",
              "text": "it's not about A model, it's about modelS... specifically the Network Effects of multiple models with tools",
              "score": 3,
              "created_utc": "2025-12-26 21:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6vmrl",
              "author": "Freonr2",
              "text": "Do you want your codebase ending up in training data for models that your competitors will use?",
              "score": 2,
              "created_utc": "2025-12-27 13:57:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxad0k",
      "title": "NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux",
      "subreddit": "LocalLLaMA",
      "url": "https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/",
      "author": "HumanDrone8721",
      "created_utc": "2025-12-27 22:22:21",
      "score": 442,
      "num_comments": 147,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/",
      "domain": "hackaday.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwc9e7h",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 09:45:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9nnam",
          "author": "FearFactory2904",
          "text": "The 24GB p40 is a pascal card. Liked those a lot before they became really expensive.",
          "score": 161,
          "created_utc": "2025-12-27 22:51:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwawtt8",
              "author": "David_Delaune",
              "text": "I was extremely lucky the past few years, sold my all my Tesla P40's when they peaked in value, which just happed to be when 3090's were still affordable. My only regret was not buying more RAM for my home lab. I thought 128GB was good enough.",
              "score": 42,
              "created_utc": "2025-12-28 03:13:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbrq2x",
                  "author": "ziggo0",
                  "text": "I'd say I have a reply for the P40s but I'm saddened over this article.",
                  "score": 7,
                  "created_utc": "2025-12-28 06:56:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf4awt",
                  "author": "Tasty_Ticket8806",
                  "text": "Can I ask what you are running that needs 128gbs of ram?",
                  "score": 1,
                  "created_utc": "2025-12-28 20:07:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwbserq",
              "author": "KadahCoba",
              "text": "Same. For $150 a 2-3 years ago, worth it. The $350-400+ they've been for most of 2025 was insane.",
              "score": 4,
              "created_utc": "2025-12-28 07:02:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecvel",
                  "author": "frozen_tuna",
                  "text": "2-3 years ago, none of the local llama software people use now existed, and if it did, it didn't support the p40 architecture. I made a lot of comments about it in the early days of this sub, eventually advocated to a lot of people who pm'd me to bite the bullet and get a used 3090 instead like I eventually did.",
                  "score": 1,
                  "created_utc": "2025-12-28 17:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9sahw",
          "author": "C0rn3j",
          "text": "Arch dropping legacy drivers to AUR has been a thing for eons, it is not surprising, and it is in the [Arch News](https://archlinux.org/news/nvidia-590-driver-drops-pascal-support-main-packages-switch-to-open-kernel-modules/).",
          "score": 69,
          "created_utc": "2025-12-27 23:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9kqot",
          "author": "knook",
          "text": "Ah crap, I was worried this would be coming soon.",
          "score": 78,
          "created_utc": "2025-12-27 22:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9rzqs",
              "author": "pissoutmybutt",
              "text": "Whats this mean for me who is using a tesla p4 for mostly transcodes with ffmpeg? I just wont get driver updates? Like i shouldnt have to worry aboot a huge headache from this for some reason running ubuntu 22.04 LTS would I?",
              "score": 26,
              "created_utc": "2025-12-27 23:15:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9uak0",
                  "author": "knook",
                  "text": "Yeah, pretty much just driver updates will stop. It won't change anything for us for a long while in all likelihood.",
                  "score": 35,
                  "created_utc": "2025-12-27 23:28:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9x1nj",
                  "author": "LostLakkris",
                  "text": "I just keep the .run files on my NAS that have historically worked, not a fan over system packages, but it's been reliable",
                  "score": 4,
                  "created_utc": "2025-12-27 23:44:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdel5m",
                  "author": "LoafyLemon",
                  "text": "The next time you run pacman or yay, you'll see an option to either stay on nvidia package or move to nvidia-open if your card is still supported.\n\n\nArch solved this issue beautifully.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:05:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwax5qp",
          "author": "segmond",
          "text": "who cares?  don't upgrade to the latest driver.  chances are if you are running P40, you are not running 5090 on the same system.",
          "score": 9,
          "created_utc": "2025-12-28 03:15:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa5swi",
          "author": "TurnUpThe4D3D3D3",
          "text": "This doesn‚Äôt really matter, the drivers for Pascal are already super stable. They don‚Äôt need updates.",
          "score": 41,
          "created_utc": "2025-12-28 00:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwaszus",
              "author": "esuil",
              "text": "Yeah, I am confused on WTF people are even on about.\n\nIt's not like old drivers are going away, and they have full functionality, right? So what exactly is the problem? \n\nMy god I hate modern clickbait media. 20 years ago this kind of posting would get you a temporary ban for fearmongering in most communities.",
              "score": 33,
              "created_utc": "2025-12-28 02:50:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwazf2x",
                  "author": "natufian",
                  "text": "I guess you can say \"Causing Chaos on Arch Linux\" is clickbaity (I didn't follow the link to survey said \"chaos\" for myself-- may be legit), but this generation of drivers works with the *current* kernel.  Any random kernel update that touches any CUDA handling can potentially break things at any time. Its a ticking time bomb. It's likely that the kernel maintainers will manually code in compatibility just for these versions of the Pascal drivers for a while, but as the mainline progresses and it naturally gets more and more labor intensive to harmonize this old frozen driver from that moment back in 2025 to the evolving and improving paradigms...\n\nNot the end of the world-- there will *always* be work-arounds, but legit consequential, terrible,  news.",
                  "score": 15,
                  "created_utc": "2025-12-28 03:29:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfq991",
                  "author": "1731799517",
                  "text": "Linux LOVES to intentionally break driver interfaces in order to punish people using non open source drivers.",
                  "score": 2,
                  "created_utc": "2025-12-28 21:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9tlu7",
          "author": "blueblocker2000",
          "text": "Pascal was the last iteration that cared about the regional power grid.",
          "score": 35,
          "created_utc": "2025-12-27 23:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwburl8",
              "author": "AndreaCicca",
              "text": "What do you mean",
              "score": 3,
              "created_utc": "2025-12-28 07:24:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdocg2",
                  "author": "dajigo",
                  "text": "Power consumption has really increased over time. Quite intensely at that.",
                  "score": 2,
                  "created_utc": "2025-12-28 15:56:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwcvxnf",
              "author": "Dry-Judgment4242",
              "text": "Newer cards can be undervolted quite hard though. I run mine at 70% while still getting like 93% performance.",
              "score": 3,
              "created_utc": "2025-12-28 13:06:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9qr4t",
          "author": "trimorphic",
          "text": "Please don't kill me for this incredibly stupid and ignorant question, but is it really that hard to make good open source drivers for NVIDIA cards?\n\nOr is there just not enough interest or not enough funding?",
          "score": 22,
          "created_utc": "2025-12-27 23:08:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9rjt8",
              "author": "Usual-Orange-4180",
              "text": "There are, but is not just drivers but CUDA integration, super difficult (the moat).",
              "score": 29,
              "created_utc": "2025-12-27 23:12:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9ykvk",
                  "author": "muxxington",
                  "text": "The greater the despair, the smaller the moat may become. One can still dream, after all.",
                  "score": 8,
                  "created_utc": "2025-12-27 23:52:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9r4mx",
              "author": "C0rn3j",
              "text": "> is it really that hard to make good open source drivers for NVIDIA cards? \n\nYes.",
              "score": 95,
              "created_utc": "2025-12-27 23:10:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9xk6n",
              "author": "SwordsAndElectrons",
              "text": "That support CUDA and make the best possible use of the hardware? Without any support or resources from the hardware vendor?\n\n\nYes. It's pretty hard.",
              "score": 14,
              "created_utc": "2025-12-27 23:47:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9vcww",
              "author": "qwerkeys",
              "text": "Nvidia blocked firmware re-clocking on open-source drivers for Maxwell and Pascal. The GPUs perform like they‚Äôre permanently idle. Also a very ‚Äòmy way or the high way‚Äô attitude to Linux standards like with EGLStreams (nvidia) vs GBM (everyone else). This also delayed adoption of Wayland on Linux\n\nhttps://www.phoronix.com/review/nvidia-980-5080-linux",
              "score": 34,
              "created_utc": "2025-12-27 23:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc08xm",
                  "author": "dannepai",
                  "text": "Whyyyy does Nvidia have to be so disgusting? I‚Äôm proud to say that the last GPU from them I had was the 256, and I bought it used.",
                  "score": 4,
                  "created_utc": "2025-12-28 08:15:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhwls5",
                  "author": "RhubarbSimilar1683",
                  "text": "I read that the issue was that there wasn't a stable version of the signed firmware to reverse engineer. Now that it's eol it's possible¬†",
                  "score": 1,
                  "created_utc": "2025-12-29 05:20:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9xvmg",
              "author": "Aggressive-Bother470",
              "text": "Nothing will change, lol.¬†\n\n\nInstall an older driver, the end.",
              "score": 5,
              "created_utc": "2025-12-27 23:48:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa3bod",
                  "author": "bitzap_sr",
                  "text": "Until some kernel change breaks it...",
                  "score": 10,
                  "created_utc": "2025-12-28 00:19:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9igk0",
          "author": "HumanDrone8721",
          "text": "3090 people, be afraid, be very afraid !!!",
          "score": 79,
          "created_utc": "2025-12-27 22:22:47",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw9qc5c",
              "author": "0xCODEBABE",
              "text": "why?",
              "score": 38,
              "created_utc": "2025-12-27 23:05:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9tzhz",
                  "author": "sibilischtic",
                  "text": "I'm not sure either....\n\nIt should be quite a while before ampere reaches the chopping block for support. One day they will reach eol but i don't think its something to worry about just yet.",
                  "score": 45,
                  "created_utc": "2025-12-27 23:26:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwa5cha",
                  "author": "eloquentemu",
                  "text": "Yeah.  Okay, obviously dropping Pascal is on the road to dropping Ampere, but Pascal came out in ~2016 and Ampere was ~2020 so the 3090 should have some years still.",
                  "score": 17,
                  "created_utc": "2025-12-28 00:30:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9xblc",
              "author": "vulcan4d",
              "text": "The 3000 series had the best price to performance ratio.  Nothing would make Nvidia happier than to kill these great cards.  Our options are becoming fewer each year.\n\nI strongly believe that the market is being manipulated.  The moment Moe models came to be, the threat of open source was real.  Kimi 2 competes with cloud AI models and it can run local, the problem is, the vram and ram situation prevents the average joe from running these large models and you are dependent on the cloud providers.  :(",
              "score": 47,
              "created_utc": "2025-12-27 23:45:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa4f1q",
                  "author": "discreetwhisper1",
                  "text": "What is moe models and kimi 2 am noob with a 3090",
                  "score": 4,
                  "created_utc": "2025-12-28 00:25:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwa1f60",
                  "author": "Glum_Control_5328",
                  "text": "I don‚Äôt think NVIDIA intentionally plans to phase out consumer GPUs. Any shift away from these cards would probably be a result of reallocating internal resources to focus on data center GPU software. Consumer grade GPUs appeal to individual users who want to train or run AI models locally.  Most companies aren‚Äôt interested in physically hosting their own hardware though. Maybe with the  exception of companies based in the China.\n\nNone of the clients I‚Äôve worked with have invested in consumer hardware for local AI tasks,they prefer renting resources from platforms like Microsoft or AWS. (Or they‚Äôll get a few data center chips depending on confidentiality risk)",
                  "score": 2,
                  "created_utc": "2025-12-28 00:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9pzqc",
              "author": "CodeFarmer",
              "text": "I'm in this comment and I don't like it.",
              "score": 8,
              "created_utc": "2025-12-27 23:03:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwalwxf",
              "author": "Normal-Ad-7114",
              "text": "First they have to slay Volta and Turing, and only then comes Ampere",
              "score": 2,
              "created_utc": "2025-12-28 02:08:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbmn48",
              "author": "nonaveris",
              "text": "I‚Äôll worry when the RTX 8000 gets dropped from support.  That‚Äôs about the only 48GB card with CUDA and sane pricing.",
              "score": 1,
              "created_utc": "2025-12-28 06:12:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbtxc3",
                  "author": "HumanDrone8721",
                  "text": "What about A6000, around here are the same price used, ca. 2000-2100EUR ?",
                  "score": 1,
                  "created_utc": "2025-12-28 07:16:13",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwbsrb2",
              "author": "KadahCoba",
              "text": "No need to worry till a year or two after Volta support starts being EOL'd.",
              "score": 1,
              "created_utc": "2025-12-28 07:05:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa3d5h",
          "author": "jebuizy",
          "text": "This is not \"chaos\". This is total click bait.¬†",
          "score": 8,
          "created_utc": "2025-12-28 00:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qqwn",
          "author": "_lavoisier_",
          "text": "So they killed the support of one of the oldest programming language? This is pure greed!",
          "score": 28,
          "created_utc": "2025-12-27 23:08:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9t3wq",
              "author": "fishhf",
              "text": "Damn how do people write CUDA kernels if not in Pascal then?",
              "score": 23,
              "created_utc": "2025-12-27 23:21:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb2nqc",
                  "author": "earslap",
                  "text": "They will be forced to use a Turing machine (20xx series). Once that support dies, they will be forced to write by manipulating pure electricity (Ampere).",
                  "score": 12,
                  "created_utc": "2025-12-28 03:48:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9rabd",
              "author": "amooz",
              "text": "I think they mean the card architecture not the language",
              "score": 29,
              "created_utc": "2025-12-27 23:11:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwa20du",
                  "author": "shaolinmaru",
                  "text": "whoosh",
                  "score": 21,
                  "created_utc": "2025-12-28 00:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9rjse",
                  "author": "_lavoisier_",
                  "text": "lmao",
                  "score": 14,
                  "created_utc": "2025-12-27 23:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9vknd",
              "author": "psxndc",
              "text": "I‚Äôm going to be honest, the programming language is the only Pascal I know of. I *knew* the title wasn‚Äôt referring to that, but I was still very confused.",
              "score": 14,
              "created_utc": "2025-12-27 23:35:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwcqygg",
                  "author": "ryunuck",
                  "text": "so did I AND YET still there I was, with a half written comment about rust",
                  "score": 2,
                  "created_utc": "2025-12-28 12:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwc3g3u",
              "author": "iamapizza",
              "text": "Clearly they were under pressure",
              "score": 3,
              "created_utc": "2025-12-28 08:46:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zc88",
              "author": "muxxington",
              "text": "It's not about the programming language. It's about the basketball player. I didn't know he played for NVIDIA, though.  \n[https://en.wikipedia.org/wiki/Pascal\\_Siakam](https://en.wikipedia.org/wiki/Pascal_Siakam)",
              "score": 6,
              "created_utc": "2025-12-27 23:57:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwakpye",
                  "author": "Pacostaco123",
                  "text": "No, they are referring to thousands of a unit of pressure.\n\nKill a pascals",
                  "score": 8,
                  "created_utc": "2025-12-28 02:01:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwacpns",
          "author": "RobotRobotWhatDoUSee",
          "text": "What does this practically mean for P40 builds?",
          "score": 4,
          "created_utc": "2025-12-28 01:13:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbilij",
          "author": "Flat_Association_820",
          "text": ">thus the user getting kicked back to the CLI to try and sort things back out there\n\nIsn't that why people use Arch?\n\nIt's like complaining that a gas powered vehicle consume gas.",
          "score": 7,
          "created_utc": "2025-12-28 05:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwareic",
          "author": "siegevjorn",
          "text": "Wouldn't just using distros built for robustness and longevity like rocky linux make Pascal to work for a long time?",
          "score": 3,
          "created_utc": "2025-12-28 02:40:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwccr6j",
          "author": "pheasantjune",
          "text": "Is Pedro okay?",
          "score": 3,
          "created_utc": "2025-12-28 10:17:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbvved",
          "author": "dtdisapointingresult",
          "text": "Alternative title: Rolling distro update breaks users' desktop, to the surprise of no one wise enough to avoid rolling distros.",
          "score": 7,
          "created_utc": "2025-12-28 07:34:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9rayc",
          "author": "Megaboz2K",
          "text": "Wow, my first thought was \"Since when can you do Cuda programming in Pascal?\" before I realized it was regarding the architecture, not the language. I think I'm doing too much retrocomputing lately!",
          "score": 8,
          "created_utc": "2025-12-27 23:11:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9wlqg",
              "author": "toothpastespiders",
              "text": "Same here. I was wondering for a moment if there was some weird officially maintained Delphi/FireMonkey backend or something. My blame for the brainfart goes to the early Wizardry games.",
              "score": 2,
              "created_utc": "2025-12-27 23:41:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9l8tr",
          "author": "No_Afternoon_4260",
          "text": "Pascal was compute capability 6.0, it introduced\n- nvlink (between 80 and 200 gb/s)\n- hbm2 on a 4096 bits bus achieving a whooping 720gb/s\n- gddr5x on 256 bits for 384gb/s\n- unified memory  \n- fp16  \n- ...\n\nThe 1080ti was 11gb, it was made for a quantized 7b\n\nIt will soon be left for dead (or for vulkan)",
          "score": 5,
          "created_utc": "2025-12-27 22:37:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n52s",
              "author": "Organic-Thought8662",
              "text": "So much of that is wrong. \n\nPascal was Mostly Compute 6.1   \nThe only Compute 6.0 was the P100, which was also the only card in the family which used HBM and had full speed FP16.  \nThe rest of the cards were GDDR5(x)   \nThere was no 1090ti, the GOAT was the 1080ti, which was an 11GB card, using GDDR5x and had gimped fp16, but DP4a for decent int8 performance. It also was on a 384 bit bus with 484GB/s of bandwidth. \n\nThe card most in this subreddit have been using from pascal is the P40, which is a 1080ti, with 24GB of GDDR5 (non-x) for 347GB/s of bandwidth.",
              "score": 33,
              "created_utc": "2025-12-27 22:48:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9qw91",
              "author": "lastrosade",
              "text": "The what now? The 1070 ti and the 1080 are 8gb, the 1080 ti is 11.",
              "score": 3,
              "created_utc": "2025-12-27 23:09:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9rcbu",
                  "author": "No_Afternoon_4260",
                  "text": "My bad, did a quick search, thanks for pointing that to me",
                  "score": -1,
                  "created_utc": "2025-12-27 23:11:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9swz2",
          "author": "Bozhark",
          "text": "Welp, 48GB 2080ti next",
          "score": 5,
          "created_utc": "2025-12-27 23:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwac3ri",
              "author": "a_beautiful_rhind",
              "text": "You can't. Only 22gb fits. Maybe RTX8000 or something.",
              "score": 6,
              "created_utc": "2025-12-28 01:09:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaf2zo",
                  "author": "Bozhark",
                  "text": "You haven‚Äôt seen the Chinese resolders?¬†",
                  "score": -8,
                  "created_utc": "2025-12-28 01:27:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwadla3",
          "author": "a_beautiful_rhind",
          "text": "Wait till you find out torch dropped it after 2.7. Why is this news now when it was warned about for cuda13 months ago? Simply don't update.\n\nI never tried the open driver on my P40s or P100, even though there is code in there for the architecture You are also supposed to pass an unsupported GPU flag to enable.",
          "score": 3,
          "created_utc": "2025-12-28 01:18:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9puil",
          "author": "AdamDhahabi",
          "text": "Stay on the current driver. And old news: no way to use a Blackwell card and a Pascal card in the same system, except for Windows.",
          "score": 2,
          "created_utc": "2025-12-27 23:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwamsw6",
              "author": "TokenRingAI",
              "text": "Never say never.\n\nQEMU + PCIe passthrough + RPC.",
              "score": 4,
              "created_utc": "2025-12-28 02:13:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwautmu",
              "author": "Arxijos",
              "text": "Easy way, search for, incus (previously known as lxc) pcie pass through, utilizes qemu.",
              "score": 0,
              "created_utc": "2025-12-28 03:00:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9v4tg",
          "author": "the_Luik",
          "text": "I guess Nvidia needs people to buy new hardware.",
          "score": 3,
          "created_utc": "2025-12-27 23:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbrqfn",
          "author": "jacek2023",
          "text": "I was an active Arch contributor around 2005, I wonder what this chaos means in 2025",
          "score": 2,
          "created_utc": "2025-12-28 06:56:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbtt8e",
              "author": "HumanDrone8721",
              "text": "Well, the Arch crowd likes to stay on top of the things, they're easy to dismiss \"yeah, yeah, just stay with the older stuff...\", but usually sooner than later this happens as well to the more mainstream distros. For example I'm using Debian 13 Trixie but set the Nvidia's repos for drivers and CUDA stuff, many others do it to have the latest features and speed improvements and it actually shows. To have the rug pulled under you is annoying.",
              "score": 2,
              "created_utc": "2025-12-28 07:15:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwbvdoh",
              "author": "AndreaCicca",
              "text": "You update your machine and instead of your desktop environment you see a TTY. In order to fix you have to install the proper driver.",
              "score": 2,
              "created_utc": "2025-12-28 07:29:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbviy2",
                  "author": "jacek2023",
                  "text": "I assume some Arch users are familiar with the shell even in 2025? :)",
                  "score": 6,
                  "created_utc": "2025-12-28 07:31:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfhxqn",
                  "author": "Barafu",
                  "text": "If you update your machine, ignore the article in distro news, ignore the question presented by the package manager ‚Äî then upon reboot you should not see a TTY, you should see a Windows 11 with blocked admin rights.",
                  "score": 1,
                  "created_utc": "2025-12-28 21:14:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbgohy",
          "author": "IAmBobC",
          "text": "The GTX series is still EXTREMLY RELEVANT, even today! Especially if you are trying to run LLMs and other neural networks locally. Sure, the RTX series is better, but GTX can still do some serious heavy lifting!\n\n¬®Hardware Obsolesce through Software¬®is total BS. That silicon still has MUCH to offer!\n\nSure, the OEM wants you to upgrade. That¬¥s not wrong, and it¬¥s not unfair. What¬¥s not right is letting software ALONE kill perfectly good hardware!\n\nFight this ¬®planned obsolescence¬®!",
          "score": 2,
          "created_utc": "2025-12-28 05:25:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc347l",
              "author": "TechnoByte_",
              "text": "Why are you acting like they'll stop working?\n\nYou can still keep using the current driver which is very stable, you just won't be getting updates",
              "score": 4,
              "created_utc": "2025-12-28 08:43:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfi6io",
                  "author": "Barafu",
                  "text": "But how can one farm karma points without pretending to be dumber than they already are?",
                  "score": 1,
                  "created_utc": "2025-12-28 21:15:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw9xmx8",
          "author": "Dorkits",
          "text": "NVIDIA is a bitch. My next card will be AMD without any doubt.",
          "score": 0,
          "created_utc": "2025-12-27 23:47:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwak5ce",
          "author": "noiserr",
          "text": "I still have a linux machine with my last nvidia GPU, Titan Xp. Will be replacing it with the 9700 AI Pro if the price ever hits the MSRP.",
          "score": 1,
          "created_utc": "2025-12-28 01:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwapj8o",
          "author": "freehuntx",
          "text": "Just pin to 580",
          "score": 1,
          "created_utc": "2025-12-28 02:29:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaxre2",
          "author": "RayneYoruka",
          "text": "Well I suppose I'll have to decide on a radeon or intel gpu for my proxmox if the support will be ending soon! (Got a 1030 atm, was eyeing a pascal quadro card)",
          "score": 1,
          "created_utc": "2025-12-28 03:18:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe51u9",
          "author": "IrisColt",
          "text": "\"chaos\" stopped reading. Clickbait.",
          "score": 1,
          "created_utc": "2025-12-28 17:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfiwo2",
          "author": "Thedudely1",
          "text": "Meanwhile the RX 480 supports ray tracing",
          "score": 1,
          "created_utc": "2025-12-28 21:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfwqf6",
          "author": "nonaveris",
          "text": "Is Volta still supported?  There‚Äôs still plenty of 32gb v100s for moderately cheap out there.",
          "score": 1,
          "created_utc": "2025-12-28 22:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl6xuu",
          "author": "Shoddy-Tutor9563",
          "text": "This is what you get when using rolling distros",
          "score": 1,
          "created_utc": "2025-12-29 18:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwan7e7",
          "author": "MontyBoomslang",
          "text": "This bit me last week. Caused me to buy my first AMD GPU. I now get why people rag on Nvidia support for Linux. This Radeon was super easy to set up and already has much less buggy weirdness.",
          "score": 0,
          "created_utc": "2025-12-28 02:15:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcugh2",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2025-12-28 12:55:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh71aj",
                  "author": "MontyBoomslang",
                  "text": "I installed the AUR driver and it worked okay for gaming, but there were other places where it seemed to cause problems (Ollama being one).\n\n>an upgrades always nice I guess lol\n\nLol yep! And as much as I unironically love running old, cheap hardware, I didn't want to suffer the pains of gradual compatibility loss I saw coming down the pike.\n\n>It‚Äôs worth subscribing to the Arch newsletter. These things are announced in advance\n\nThis... Would be good to do. Eight years on Arch, and I've just shrugged and dealt with breaks as they came. It *would* be nice to have a heads up. Thanks for the tip!",
                  "score": 1,
                  "created_utc": "2025-12-29 02:42:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwcbeg6",
              "author": "MDSExpro",
              "text": "AMD has even weaker and shorter GPU support than Nvidia.",
              "score": 5,
              "created_utc": "2025-12-28 10:04:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwd6q8u",
                  "author": "kopasz7",
                  "text": "13 year old GPUs getting 30% extra performance in games. Happened this week.\n\n[Phoronix: Linux 6.19's Significant ~30% Performance Boost For Old AMD Radeon GPUs\n](https://www.phoronix.com/review/linux-619-amdgpu-radeon)\n\nOpensource drivers let others contribute, keeps the project going longer.",
                  "score": 1,
                  "created_utc": "2025-12-28 14:18:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwaguhk",
          "author": "ttkciar",
          "text": "Shit like this makes me really glad AMD publishes their ISA.",
          "score": 1,
          "created_utc": "2025-12-28 01:38:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwahels",
          "author": "Traditional_Nose3120",
          "text": "Linus should get his middle finger out of retirement",
          "score": 0,
          "created_utc": "2025-12-28 01:41:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaabyv",
          "author": "autodidacticasaurus",
          "text": "Lucky me, upgraded from my 1030 GT card to a Radeon 7900 XTX just in time.",
          "score": 0,
          "created_utc": "2025-12-28 00:59:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9l3oa",
          "author": "notcooltbh",
          "text": "the 4 arch users are shivering their timbers rn",
          "score": -25,
          "created_utc": "2025-12-27 22:37:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n4dr",
              "author": "beren0073",
              "text": "If those Arch users could read, they'd be very upset",
              "score": -4,
              "created_utc": "2025-12-27 22:48:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwc3kgv",
                  "author": "TechnoByte_",
                  "text": "We're reading Arch wiki pages and using the terminal every day, we know how to read",
                  "score": 1,
                  "created_utc": "2025-12-28 08:48:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwah756",
          "author": "Lesser-than",
          "text": "you will upgrade and be happy!",
          "score": -2,
          "created_utc": "2025-12-28 01:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbif8o",
          "author": "Ok-Adhesiveness-4141",
          "text": "This was so damn confusing, I thought it was to do with the Pascal language.\n\nThis is  a good example of why Nvidia sucks, they have always sucked if my memory serves me right. Don't trust any hardware vendor that doesn't open source their device drivers. \n\nI will go one-step further and say, we need to reverse engineer proprietary drivers and then vibe-code open source drivers. We should no longer be respectful of intellectual property rights of these hardware mafia guys.",
          "score": -2,
          "created_utc": "2025-12-28 05:38:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc3eu8",
              "author": "TechnoByte_",
              "text": "NVIDIA did open source the kernel modules\n\nYou don't need to vibecode new open source drivers because [Noveau](https://nouveau.freedesktop.org/) already exists and isn't garbage LLM code",
              "score": 0,
              "created_utc": "2025-12-28 08:46:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9l449",
          "author": "Gwolf4",
          "text": "Using Nvidia for GUI in Linux is a fool's errand anyways.",
          "score": -35,
          "created_utc": "2025-12-27 22:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9n7lg",
              "author": "edparadox",
              "text": "\"For GUI\" keep pretending you know what you're saying LMAO.",
              "score": 17,
              "created_utc": "2025-12-27 22:48:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9tqv7",
                  "author": "Gwolf4",
                  "text": "I know what I am saying and probably more than you it seems.",
                  "score": -10,
                  "created_utc": "2025-12-27 23:25:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwatx8l",
          "author": "Upper_Road_3906",
          "text": "they want you locked into the windows ai system i bet they start dropping it for all linux distros down the road for them it makes no sense in their fever dream of you renting a cloud gpu to play games and pay them eternally for rent and being able to deplatform you at a moments notice.",
          "score": -3,
          "created_utc": "2025-12-28 02:55:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwc418q",
              "author": "TechnoByte_",
              "text": "NVIDIA wants you to use Windows? lmao\n\nYou're acting like old NVIDIA GPUs are about to be bricked or remotely detonated, they won't, you can just keep using the current stable driver and they will work fine",
              "score": 1,
              "created_utc": "2025-12-28 08:52:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyg4yt",
      "title": "Tencent just released WeDLM 8B Instruct on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyg4yt",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 07:38:43",
      "score": 410,
      "num_comments": 56,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwiswg2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 10:05:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigd3k",
          "author": "jamaalwakamaal",
          "text": "7-8B models have lot of potential. Very promising space. More models please.",
          "score": 47,
          "created_utc": "2025-12-29 08:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwie4o6",
          "author": "Endlesscrysis",
          "text": "Pretty huge I think? I thought I saw people mentioning a couple of times that diffusion models weren‚Äôt possible for accurate LLM‚Äôs yet this outperforms a similar sized powerhouse like qwen?",
          "score": 82,
          "created_utc": "2025-12-29 07:46:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio6nn",
              "author": "SlowFail2433",
              "text": "Yeah I was one of the pretty vocal skeptics about diffusion language models. I thought their inductive bias was too sub-optimal for language/code. I was super wrong about this.",
              "score": 50,
              "created_utc": "2025-12-29 09:20:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjvefy",
                  "author": "Investolas",
                  "text": "I'd love to read one of your critiques, care to share a link to a comment or post you've made? I didn't find any of your contributions and assume they are paywalled. Thx!",
                  "score": 9,
                  "created_utc": "2025-12-29 14:45:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwnur1r",
                  "author": "aeroumbria",
                  "text": "Interestingly I am more of the opinion that the autoregressive inductive bias is too restricting and unnatural, and may contribute to why we need so many parameters to reach usability. It feels like traditional linguistics gives more credit to a \"large scale autoregressive (causal dependency), small scale hierarchical (tree structure in grammar)\" type of model, which is closer to block diffusion. Still not entirely sold on the token-wise masking process thing though - it cannot reflect a hierarchical \"concept refinement\" process. Interested to see any progress in this direction though.",
                  "score": 1,
                  "created_utc": "2025-12-30 02:45:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkw478",
              "author": "Orolol",
              "text": "We know diffusion is possible since atleast Llada 18 months ago. But the problem was that it used a non causal attention, so we were unable to use many crucial techniques, like kv cache. \nThis enables the use of kvcache because of a very clever trick.",
              "score": 6,
              "created_utc": "2025-12-29 17:43:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwooixj",
              "author": "Mikasa0xdev",
              "text": "Diffusion models are the new transformers, confirmed.",
              "score": 2,
              "created_utc": "2025-12-30 05:52:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwiiec4",
          "author": "jacek2023",
          "text": "additionaly [https://huggingface.co/tencent/WeDLM-7B-Instruct](https://huggingface.co/tencent/WeDLM-7B-Instruct)",
          "score": 30,
          "created_utc": "2025-12-29 08:26:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiji7b",
              "author": "aeroumbria",
              "text": "Interesting. Is there a specific use case where 8B can't fit but 7B can?",
              "score": 11,
              "created_utc": "2025-12-29 08:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwiufh1",
                  "author": "pkmxtw",
                  "text": "The 7B is converted from Qwen2.5 7B and the 8B is from Qwen3 8B. What they want to demonstrate is that they can convert an AR model into a diffusion model w/o losing quality.\n\nIn reality, you'd just use the 8B like how Qwen3 8B has basically replaced Qwen2.5 7B.",
                  "score": 38,
                  "created_utc": "2025-12-29 10:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwigcqz",
          "author": "Paramecium_caudatum_",
          "text": "Diffuser model with impressive benchmark scores and Apache 2.0 license, sounds pretty interesting to me.",
          "score": 51,
          "created_utc": "2025-12-29 08:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwihmcw",
          "author": "FinBenton",
          "text": "Its just a small model but 3-6x speed with similar or higher performance sounds insane!",
          "score": 24,
          "created_utc": "2025-12-29 08:18:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlum7p",
              "author": "lolwutdo",
              "text": "I know diffusion models are super fast on gpu but how would a diffusion model's speed compare on cpu vs a cpu llm?\n\nI guess mainly what I'm curious about is how well would a diffusion based llm run with cpu offloading compared to a traditional llm.",
              "score": 2,
              "created_utc": "2025-12-29 20:25:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm87rw",
                  "author": "oh_how_droll",
                  "text": "Diffusion is going to be slower on CPUs -- CPUs are mostly compute-limited and they're more compute intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:32:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwie4gd",
          "author": "SlowFail2433",
          "text": "Nice to see another diffusion model would have liked more modern/harder benches",
          "score": 14,
          "created_utc": "2025-12-29 07:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwim9sq",
          "author": "Nice-Information-335",
          "text": "need unsloth or bartowski on this asap",
          "score": 20,
          "created_utc": "2025-12-29 09:02:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwirhjw",
              "author": "Odd-Ordinary-5922",
              "text": "will need a pr first for model support",
              "score": 32,
              "created_utc": "2025-12-29 09:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjz6tm",
                  "author": "MoffKalast",
                  "text": "We need a few papers first for model support",
                  "score": 8,
                  "created_utc": "2025-12-29 15:05:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwohedf",
              "author": "tronathan",
              "text": "Not really, in terms of usefuless, as I understand it, it's basically a Qwen 3. It's more of a proof of confacept",
              "score": 1,
              "created_utc": "2025-12-30 05:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwplso2",
                  "author": "Nice-Information-335",
                  "text": "hey I still want to try it! half of the fun for me is seeing advancements as they happen and being able to run them. massive props to everyone who makes that happen, as lord knows I don't know nearly enough to get this stuff working without the likes of llama.cpp, all it's amazing contributors and unsloth/bartowski for GGUFs",
                  "score": 1,
                  "created_utc": "2025-12-30 10:50:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwiw5cp",
          "author": "always_newbee",
          "text": "What is Qwen3-8B-Instruct model? Just non-thinking mode?",
          "score": 6,
          "created_utc": "2025-12-29 10:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj0ed1",
              "author": "mouseofcatofschrodi",
              "text": "yes",
              "score": 3,
              "created_utc": "2025-12-29 11:13:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwigrdo",
          "author": "Grouchygrond",
          "text": "Now we just need a hybrid model",
          "score": 2,
          "created_utc": "2025-12-29 08:10:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjuykz",
              "author": "Deciheximal144",
              "text": "How would that work? Diffusing in chunks? LLM generates, then diffusion revises the lowest-probability sections? Diffusion is noise-to-content.",
              "score": 7,
              "created_utc": "2025-12-29 14:43:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwljqrp",
                  "author": "peaceoutwhat",
                  "text": "Search TiDAR",
                  "score": 2,
                  "created_utc": "2025-12-29 19:32:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwm7wqb",
                  "author": "TheRealMasonMac",
                  "text": "There was a research model that diffused chunks one at a time like a Frankenstein of current LLMs and dLLMs\n\n\nhttps://m-arriola.com/bd3lms/",
                  "score": 2,
                  "created_utc": "2025-12-29 21:30:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwkwy6z",
              "author": "Orolol",
              "text": "I don't this it's possible to have both autoregressive and diffusion generation, and even if possible, I don't think there's any positive doing it.",
              "score": 1,
              "created_utc": "2025-12-29 17:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwlyqsm",
          "author": "Semi_Tech",
          "text": "Hmm shouldn't diffusion models also have a # of steps needed in order to reach the end result?\n\nI don't see a mention about that or how increasing or decreasing them affects model output quality.",
          "score": 3,
          "created_utc": "2025-12-29 20:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiksnh",
          "author": "Healthy-Nebula-3603",
          "text": "That's diffusion model right ?\n\n\nAs I understand such model can't be reasoner as can't looping in thoughts and observe own internal states?",
          "score": 7,
          "created_utc": "2025-12-29 08:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwilg37",
              "author": "Lesser-than",
              "text": "diffusion text models technically reason, as they can modify the first word of a sentence or tokens at every step of the inference, where a token by token model has to justify that token for the rest of the reply if they get it wrong.",
              "score": 24,
              "created_utc": "2025-12-29 08:54:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwilp6x",
                  "author": "Healthy-Nebula-3603",
                  "text": "I meant they can reason like the instruct models but are not thinkers like thinking models.",
                  "score": 2,
                  "created_utc": "2025-12-29 08:57:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj7ycj",
              "author": "NandaVegg",
              "text": "According to the site, this is a variation of block-wise diffusion (previously done by Meta etc) which acts more akin to a speculative decoding rather than a \"full\" diffusion (that denoises the whole output at once). I think Google did a web demo for mini full diffusion model in early 2025 but the model weight never got released?",
              "score": 7,
              "created_utc": "2025-12-29 12:16:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwihwec",
          "author": "JackStrawWitchita",
          "text": "More people have commented on this than have downloaded it...",
          "score": 13,
          "created_utc": "2025-12-29 08:21:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwio18v",
              "author": "SlowFail2433",
              "text": "In ML research we often don‚Äôt download the model right away.\n\n\nNote that the paper used the MagiAttention library for attention. I don‚Äôt use this library so I am either going to write a custom CUDA kernel or use a DSL like Triton. However the paper has some technical novelties such as the topological reordering. This is not going to be easy to work out how to implement efficiently.",
              "score": 38,
              "created_utc": "2025-12-29 09:19:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwnlhqs",
                  "author": "RhubarbSimilar1683",
                  "text": "The paper is https://github.com/Tencent/WeDLM/blob/main/paper/wedlm.pdf",
                  "score": 1,
                  "created_utc": "2025-12-30 01:55:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiu7q5",
              "author": "FinBenton",
              "text": "Gotta wait for llama.cpp and similar support first, most people here arent running vllm.",
              "score": 25,
              "created_utc": "2025-12-29 10:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkk2j0",
                  "author": "Tai9ch",
                  "text": "Not downloading open source software seems like a lame excuse to not try something neat.",
                  "score": -4,
                  "created_utc": "2025-12-29 16:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoz431",
              "author": "aeroumbria",
              "text": "Still getting issues running the official repo... Supposedly this is only 8B and supports multi-GPU but cannot seem to allocate KV even with 2x24GB",
              "score": 1,
              "created_utc": "2025-12-30 07:20:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjjpa4",
          "author": "alphapussycat",
          "text": "What does math reasoning even mean? Calculation reasoning? Or math, as in theorem, reasoning?",
          "score": 2,
          "created_utc": "2025-12-29 13:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjkd02",
              "author": "PykeAtBanquet",
              "text": "Usually it is \"prove that this series converges\" etc",
              "score": 1,
              "created_utc": "2025-12-29 13:42:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk62zs",
          "author": "implicator_ai",
          "text": "Interesting release. When they say ‚Äúdiffusion language model,‚Äù it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work.   \n  \nThe 3‚Äì6√ó claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 2,
          "created_utc": "2025-12-29 15:39:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwml948",
              "author": "SilentLennie",
              "text": "From what I understand: diffusion models usually were not faster than regular LLMs, because they have K/V-cache and other tricks to speed it up to prevent doing duplicate math, supposedly this model solves that.",
              "score": 1,
              "created_utc": "2025-12-29 22:37:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkkdz4",
          "author": "Awkward-Nothing-7365",
          "text": "Is this something that can run on llama.cpp right now? gguf possible?",
          "score": 1,
          "created_utc": "2025-12-29 16:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlqfpz",
          "author": "rm-rf-rm",
          "text": "They report the speed up for specifically just math reasoning tasks but it should be applicable generally no? \n\nHope we get MLX/GGUF support soon. If this is legit, its genuinely going to be massive. Right now I run 4B for quick look up etc. but I feel 4B models are not the most reliable for accurate information. At 8B, you can be much more confident.\n\nNext step MoE? Qwen3-Coder:a3b?",
          "score": 1,
          "created_utc": "2025-12-29 20:05:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwngm0k",
          "author": "RhubarbSimilar1683",
          "text": "Could diffusion enable efficient hybrid inference or inference computer clusters connected over the global internet, using asynchronous calls?",
          "score": 1,
          "created_utc": "2025-12-30 01:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnqpie",
          "author": "Vast-Piano2940",
          "text": "I wonder how it performs against lfm2-2.6b-exp",
          "score": 1,
          "created_utc": "2025-12-30 02:23:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz7bmv",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct",
      "author": "jacek2023",
      "created_utc": "2025-12-30 03:34:19",
      "score": 366,
      "num_comments": 62,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwpj0kv",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 10:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo5hxn",
          "author": "FizzarolliAI",
          "text": "Hello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 103,
          "created_utc": "2025-12-30 03:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo6q38",
              "author": "jacek2023",
              "text": "great work, new llama release at the end of 2025 :)",
              "score": 39,
              "created_utc": "2025-12-30 03:53:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwppc12",
                  "author": "MoffKalast",
                  "text": "I definitely did not have this on my bingo card :D\n\nAnd leaked too, keeping up the llama tradition.",
                  "score": 20,
                  "created_utc": "2025-12-30 11:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp95y3",
              "author": "Karyo_Ten",
              "text": "You can do a KL-divergence check to be 100% sure",
              "score": 9,
              "created_utc": "2025-12-30 08:53:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwozt4b",
              "author": "AnOnlineHandle",
              "text": "Heya I'm not up to date with these models since the llama 1 release, do you know if there's a good benchmark for visual tasks such as identifying poses, faces, hands, etc, or answering questions about images, which I could compare models on? I've tried to use Qwen 3 Instruct for it but found it wasn't as good on real data as the demos suggested.",
              "score": 3,
              "created_utc": "2025-12-30 07:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwohwlc",
          "author": "dinerburgeryum",
          "text": "8K max position embeddings? Seems remarkably low; did the fine tune artifact for some reason artificially limit that?",
          "score": 39,
          "created_utc": "2025-12-30 05:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoiq8k",
              "author": "Arli_AI",
              "text": "Maybe we can just set 32768 and it‚Äôll be okay lol",
              "score": 16,
              "created_utc": "2025-12-30 05:10:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoo96q",
                  "author": "Few-Welcome3297",
                  "text": "Checking differences from LLaMA 3.1 8B Instruct, I think we can add the rope\\_scaling\n\n|\"rope\\_scaling\": {|\n|:-|\n|\"factor\": 8.0,|\n|\"high\\_freq\\_factor\": 4.0,|\n|\"low\\_freq\\_factor\": 1.0,|\n|\"original\\_max\\_position\\_embeddings\": 8192,|\n|\"rope\\_type\": \"llama3\"|\n|},|\n\nand then increase \\`max\\_position\\_embeddings\\`\n\nEdit: Also prev version had 3 eos\\_token\\_id's\n\nEdit2: [https://huggingface.co/shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) model with above changes",
                  "score": 18,
                  "created_utc": "2025-12-30 05:50:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nworq4h",
                  "author": "Klutzy-Snow8016",
                  "text": "Llama 3 8B had 8192 context. Then Llama 3.1 added RoPE to get to 131072 context. Maybe we can take the RoPE scaling parameters from llama 3.1's config.json and add it to llama 3.3 8B.",
                  "score": 11,
                  "created_utc": "2025-12-30 06:17:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoi4rx",
              "author": "FizzarolliAI",
              "text": "Yes. I'm not entirely sure why, it was limited when served via the website too (I put that in the readme a bit ago)",
              "score": 2,
              "created_utc": "2025-12-30 05:06:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwognr9",
          "author": "random-tomato",
          "text": "Holy shit that is awesome, hats off to you for finding the weights!",
          "score": 24,
          "created_utc": "2025-12-30 04:56:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwolsch",
          "author": "Amazing_Athlete_2265",
          "text": "Running this across my private evals to compare against other llamas. Will take a couple hours.",
          "score": 20,
          "created_utc": "2025-12-30 05:32:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwou0rl",
              "author": "Amazing_Athlete_2265",
              "text": "Initial speed test:\n\n| Model | Backend | PP ts^-1| TG ts^-1 |\n| -------------------------------------------------- | ---------- | ---------- | ------------------ |\n| allura-forge_Llama-3.3-8B-Instruct Q4 | CUDA | 1566.5 | 100.8 |\n| Llama-3.1-8B-Instruct Q4 | CUDA | 351.1 | 111.9 |\n\nSo some difference there.\n\nWill post more eval results as they come to hand.",
              "score": 18,
              "created_utc": "2025-12-30 06:36:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwp3lxt",
              "author": "Amazing_Athlete_2265",
              "text": "From these results, it looks like the new model is different than the old 3.1.\n\nHere is the performance for knowledge testing, with the new 3.3-8B-Instruct highlighted in the first two plots \n\n- [First plot is the 4-9B parameter group](https://imgur.com/YuSmDRn)\n\n- [Second plot is the same but for 8B+ parameter group](https://imgur.com/Q0nnLwn)\n\n- [Third plot is performance by knowledge category for the 3.3 model](https://imgur.com/kjkNbR3)\n\n- [Fourth plot is performance by knowledge category for the older 3.1 model](https://imgur.com/vjy6cjW)\n\n- [Last plot is a speed chart on my 3080](https://imgur.com/coiBc9H)\n\nTesting the Q6 versions now. Will take a while. All of the tests above are for Q4.",
              "score": 12,
              "created_utc": "2025-12-30 08:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpij0j",
                  "author": "keepthepace",
                  "text": "(Thanks for doing this!) \n\nI guess this explains why they did not brag much about it. Many other models of that category outperform them.\n\nI always wondered if Zuckerberg was not the only honest player in the field when he was explaining that the only reason they go for open source is that it will save them money. With decent open models out there they have less incentives to do so.",
                  "score": 7,
                  "created_utc": "2025-12-30 10:20:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpmayy",
                  "author": "MLDataScientist",
                  "text": "Thanks for the tests. Question not related to llama: is LFM2 8BA1B that good in world knowledge (or coding/stem field)? I see it reaches Qwen3 30B-A3B.",
                  "score": 3,
                  "created_utc": "2025-12-30 10:55:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp43t0",
                  "author": "jacek2023",
                  "text": "You can post pictures in the comments here",
                  "score": 2,
                  "created_utc": "2025-12-30 08:06:26",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nwp9v8w",
                  "author": "RobotRobotWhatDoUSee",
                  "text": "Random question: any idea why nemotron 30B A3B got 0% in the second plot?",
                  "score": 2,
                  "created_utc": "2025-12-30 09:00:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoo8ce",
              "author": "jacek2023",
              "text": "do you have results for other new models?",
              "score": 3,
              "created_utc": "2025-12-30 05:50:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwopj5o",
                  "author": "Amazing_Athlete_2265",
                  "text": "I have some. I focus mostly on smaller models <12B or Moe. What you want?",
                  "score": 5,
                  "created_utc": "2025-12-30 06:00:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwodvfx",
          "author": "Cool-Chemical-5629",
          "text": "I guess Christmas came late for me, but hey if this is the real thing from Meta, I guess it's nice to have something newer than 3.1 8B without needing expensive hardware for models like Llama 4.",
          "score": 8,
          "created_utc": "2025-12-30 04:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp5cbv",
              "author": "Emotional-Baker-490",
              "text": "qwen 3",
              "score": 9,
              "created_utc": "2025-12-30 08:17:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpff8o",
          "author": "a_beautiful_rhind",
          "text": "This is like the kiss goodbye from meta.",
          "score": 9,
          "created_utc": "2025-12-30 09:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpzjvy",
              "author": "samplebitch",
              "text": "It's like that time when you hook up with your ex one last time, and it wasn't even that great.",
              "score": 11,
              "created_utc": "2025-12-30 12:43:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwplkfs",
          "author": "jacek2023",
          "text": "about 4h after the release u/TheLocalDrummer published first finetune:\n\n[https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main](https://huggingface.co/BeaverAI/Anubis-Mini-8B-v1f-GGUF/tree/main)",
          "score": 6,
          "created_utc": "2025-12-30 10:48:37",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwplnk1",
              "author": "TheLocalDrummer",
              "text": "It's a test model but I think it turned out well! Looking for feedback in (my) Discord",
              "score": 11,
              "created_utc": "2025-12-30 10:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrnu7f",
                  "author": "DevelopmentBorn3978",
                  "text": "what the finetune you've made is about?",
                  "score": 1,
                  "created_utc": "2025-12-30 17:56:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpr0ce",
              "author": "MoffKalast",
              "text": "People are asking what's the use case for llama, and well uh... there it is ;)",
              "score": 5,
              "created_utc": "2025-12-30 11:36:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpqsiz",
          "author": "jacek2023",
          "text": "[https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic](https://huggingface.co/aeon37/Llama-3.3-8B-Instruct-heretic)",
          "score": 7,
          "created_utc": "2025-12-30 11:34:48",
          "is_submitter": true,
          "replies": [
            {
              "id": "nwpstxk",
              "author": "Amazing_Athlete_2265",
              "text": "Everyone's cooking tonight!",
              "score": 4,
              "created_utc": "2025-12-30 11:51:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptlto",
                  "author": "jacek2023",
                  "text": "actually it's a middle of the day in Europe :)",
                  "score": 6,
                  "created_utc": "2025-12-30 11:57:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9jjl",
          "author": "Infninfn",
          "text": "I‚Äôm out of the loop - is this just what they had or did Meta not shutdown Llama?",
          "score": 14,
          "created_utc": "2025-12-30 04:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoa6pk",
              "author": "FizzarolliAI",
              "text": "This has existed at least since April during Llamacon (did anyone remember they did a Llamacon?)\n\nhttps://ai.meta.com/blog/llamacon-llama-news/\n\n> As part of this release, we‚Äôre sharing tools for fine-tuning and evaluation in our new API, where you can tune your own custom versions of our new Llama 3.3 8B model. We‚Äôre sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model.",
              "score": 27,
              "created_utc": "2025-12-30 04:14:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwoeicf",
              "author": "jacek2023",
              "text": "we do things for fun in this community, just accept the gift ;)",
              "score": 6,
              "created_utc": "2025-12-30 04:41:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwodozb",
          "author": "Echo9Zulu-",
          "text": "Cloned",
          "score": 5,
          "created_utc": "2025-12-30 04:36:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwroudn",
          "author": "DevelopmentBorn3978",
          "text": "which quantized and evenually finetuned gguf models have the context lenght been enlarged? bartowsky? shb777? beaverai/anubis?",
          "score": 1,
          "created_utc": "2025-12-30 18:01:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwo9zlt",
          "author": "Intelligent-Form6624",
          "text": "‚Äú(I think, anyways)‚Äù",
          "score": -12,
          "created_utc": "2025-12-30 04:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoadni",
              "author": "FizzarolliAI",
              "text": "LISTEN whenever i drop *my own* models i get anxiety attacks about accidentally reuploading the base model ;-; i believe that this is actually L3.3 at this point though, see my other comment",
              "score": 23,
              "created_utc": "2025-12-30 04:15:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwow842",
                  "author": "Intelligent-Form6624",
                  "text": "What? Sorry, I can‚Äôt hear you",
                  "score": -15,
                  "created_utc": "2025-12-30 06:55:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo9irb",
          "author": "secopsml",
          "text": "Drop behemoth instead. Looks fake¬†",
          "score": -31,
          "created_utc": "2025-12-30 04:10:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo9lrg",
              "author": "secopsml",
              "text": "üòú",
              "score": -21,
              "created_utc": "2025-12-30 04:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvxq2t",
      "title": "Hard lesson learned after a year of running large models locally",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/",
      "author": "inboundmage",
      "created_utc": "2025-12-26 06:38:00",
      "score": 339,
      "num_comments": 145,
      "upvote_ratio": 0.93,
      "text": "Hi all, go easy with me I'm new at running large models.\n\nAfter spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. I‚Äôm running everything off a workstation with a single RTX‚ÄØ3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30‚ÄØB parameters. \n\nMy goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so I‚Äôve tried every quantization trick and caching tweak I could find.\n\nThe biggest friction point has been scaling beyond 13‚ÄØB models. \n\nEven with 24‚ÄØGB of VRAM, running a 70‚ÄØB model in int4 still exhausts memory when the context window grows and attention weights balloon. \n\nOffloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. \n\nI‚Äôve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.\n\nMy takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. \n\nQuantization helps, but you trade some quality and run into new bugs. \n\nFor privacy sensitive tasks, the trade‚Äëoff is worth it; for fast iteration, it‚Äôs been painful compared to cloud based runners. \n\nI‚Äôm curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply ‚Äúbuy more VRAM.‚Äù \n\nHow are others solving this without compromising on running fully offline?\n\nThx",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvzpck6",
          "author": "Eugr",
          "text": "You've got it backwards. vLLM works great if model + context fit into VRAM, but it doesn't do CPU offloading well - use llama.cpp for anything that spills over to RAM. \n\nAlso, you can't fit 70B model in 4 bit quant into your 24GB, even with zero context. The weights alone would take 35GB. \n\nAlso, in memory constrained environments (and 24GB is not much as far as local LLMs are concerned) I'd default to llama.cpp as it is much more memory efficient than vLLM. So, unless you need some vllm specific features or models not supported in llama.cpp yet, use vLLM, otherwise just stick to llama.cpp. And again, only if everything fits into VRAM. \n\nWhen I just had my 4090, I wouldn't run dense models above 32B in q4 quant. I could run larger MoE, like gpt-oss-120b in llama.cpp just fine, thanks to experts offloading feature. Was getting around 40 t/s from it on Linux.",
          "score": 217,
          "created_utc": "2025-12-26 07:24:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0iv9m",
              "author": "mycall",
              "text": "Can llama.cpp load/host multiple models in parallel?",
              "score": 15,
              "created_utc": "2025-12-26 12:22:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0n2f9",
                  "author": "keyboardhack",
                  "text": "Yes that is now supported with the recently added router mode.",
                  "score": 24,
                  "created_utc": "2025-12-26 12:57:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0lwrd",
                  "author": "ismaelgokufox",
                  "text": "You can using the groups feature of llama-swap which will run an instance of llama.cpp for each under the same endpoint.",
                  "score": 6,
                  "created_utc": "2025-12-26 12:47:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0vz2o",
              "author": "munkiemagik",
              "text": ">use llama.cpp for anything that spills over to RAM.\n\nIsn't ik\\_llama.cpp the general recommend for when the intention is to split across GPU and CPU, particularly the ubergarm iq quants? I'm sure I read a few posts where the prompt processing and even token generation to a degree were reported as an improvement over vanilla llama.cpp but I cant remember now whether this applies to a specific group of LLMs or in general.",
              "score": 10,
              "created_utc": "2025-12-26 14:00:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1glfd",
                  "author": "Zc5Gwu",
                  "text": "Yes, it is generally a hair faster when offloading to ram for large models and their quants are SOTA but llama.cpp tends have better support, wider selection, tool calling might be better supported in some cases.",
                  "score": 4,
                  "created_utc": "2025-12-26 16:02:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1l846",
                  "author": "DataGOGO",
                  "text": "Yes",
                  "score": 1,
                  "created_utc": "2025-12-26 16:27:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1ej52",
              "author": "TechnicalGeologist99",
              "text": "vLLM is for production system and is defacto the only real choice for production. To run multiple models requires orchestration of multiple vLLM instances with liteLLM as a gateway.",
              "score": 7,
              "created_utc": "2025-12-26 15:51:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw55gj8",
                  "author": "hesperaux",
                  "text": "What about triton? I'm working on setting it up right now. Nobody talks about it, though. Am I wasting my time? I'm asking honestly.",
                  "score": 3,
                  "created_utc": "2025-12-27 04:54:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6bpmm",
                  "author": "Seninut",
                  "text": "I have this setup working well. Cool how it can swap models sort of on the fly depending on size and throughput.",
                  "score": 1,
                  "created_utc": "2025-12-27 11:21:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw2vxxa",
                  "author": "Sufficient-Pause9765",
                  "text": "or docker",
                  "score": 0,
                  "created_utc": "2025-12-26 20:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwebi5q",
              "author": "Electrical_Heart_207",
              "text": "The 4090 memory constraints are real. Have you looked into any cloud options for when you need to run larger models, or do you just optimize for what fits locally?",
              "score": 1,
              "created_utc": "2025-12-28 17:52:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwexw1z",
                  "author": "Eugr",
                  "text": "Oh, I also have a Strix Halo machine and dual DGX Spark cluster, so I can run GLM 4.7 in AWQ 4-bit just fine. The memory bandwidth is not great, but thanks to tensor parallel I still get 16 t/s for GLM 4.6/4.7 and 40 t/s for MiniMax M2/2.1.\n\nI also use cloud when needed.",
                  "score": 1,
                  "created_utc": "2025-12-28 19:37:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzyx4j",
          "author": "Bitter-College8786",
          "text": "I still hope that one day a chinese manufacturer releases GPUs with 256GB VRAM",
          "score": 81,
          "created_utc": "2025-12-26 09:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw04mia",
              "author": "blbd",
              "text": "It's probably going to be AMD and Apple with fresh gear. Provided we can get the goddamn chips.¬†",
              "score": 23,
              "created_utc": "2025-12-26 10:03:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0z190",
                  "author": "g_rich",
                  "text": "You can already do this with Apple, up to 512GB of unified memory with the M3 Ultra. The M5 has already shown significant improvements with running LLM‚Äôs because they‚Äôve incorporated neural accelerators directly into the GPU cores. So it‚Äôs likely we‚Äôll see a M5 Ultra Mac Studio in mid to late ‚Äò26 that will support up to 512GB of unified memory and has the potential to be a powerhouse machine for Ai. Apple won‚Äôt have a problem with acquiring chips, they would have already booked capacity with TSMC and have the resources to overcome any RAM shortages.",
                  "score": 14,
                  "created_utc": "2025-12-26 14:20:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwc00b2",
                  "author": "Budget-Juggernaut-68",
                  "text": "Scam Altman says Hi.",
                  "score": 2,
                  "created_utc": "2025-12-28 08:13:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0izli",
              "author": "mycall",
              "text": "I think Intel will support 192GB VRAM on their nextgen x64 chipsets (including mobile).",
              "score": 4,
              "created_utc": "2025-12-26 12:23:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2t4jz",
                  "author": "MoffKalast",
                  "text": "Would be so funny and classic Intel if they upped support to 192, while keeping it dual channel only lmao.",
                  "score": 5,
                  "created_utc": "2025-12-26 20:19:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1ljfk",
                  "author": "DataGOGO",
                  "text": "more than that.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:29:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8e92t",
              "author": "MarkIII-VR",
              "text": "NVIDIA Robin Ultra chips will hopefully run with 256GB, but you may have to use a Spark system to get it.  If not, the Feynman chips will have to.  By then, any product not in the 256GB range won't be used for AI.\n\nI have not located any documentation detailing 256GB system or intended shipping configurations, but we have a while before those systems will be ready still.",
              "score": 1,
              "created_utc": "2025-12-27 18:47:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1xk2q",
              "author": "HeyWannaShrek",
              "text": "Aliexpress has it already ü§™",
              "score": 1,
              "created_utc": "2025-12-26 17:32:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzknin",
          "author": "AppearanceHeavy6724",
          "text": "> I‚Äôve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations\n\nSounds very odd, as when CUDA unloads all GPU memory should get freed in bulk.",
          "score": 61,
          "created_utc": "2025-12-26 06:40:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2wulq",
              "author": "michaelsoft__binbows",
              "text": "Seems just a simple skill issue around OS process management. If the app has memory leaks the first port of call is to launch and teardown the entire process with your job unit, so each job gets a fresh worker process.\n\nObviously that has immediate impacts like 2 consecutive jobs utilizing the same model will then require the model to spend time loading into VRAM when issuing the work. But OP never detailed anything at this level so it's hard to say.",
              "score": 4,
              "created_utc": "2025-12-26 20:40:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzo3hi",
          "author": "DataGOGO",
          "text": "24GB of vram is a lot of VRAM for gaming, not for professional workloads, to include local inference.\n\nWith a single gaming GPU, and a consumer grade platform (AM5 etc) you are always going to be limited to very small models.\n\nYou could get one of the shared memory boxes (AMD/Mac/Spark/Jenson); you can run slightly larger models, but it will be slow, and to the best of my knowledge only the DGX spark has enough networking to really cluster two of them together (and even then, it is slow as hell).¬†",
          "score": 21,
          "created_utc": "2025-12-26 07:12:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0rz14",
              "author": "kaeptnphlop",
              "text": "Apple recently sent sets of 4 Mac Studios around to tech bloggers. Look up Jeff Gerling, he demonstrated how they all can work together.\n\nNot saying that networking is not an issue. He shows what‚Äôs possible at the moment with them.\n\nHe tried other options as well, one cluster based on Frameworks hardware and even a RaspberryPi cluster¬†",
              "score": 4,
              "created_utc": "2025-12-26 13:33:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1l0am",
                  "author": "DataGOGO",
                  "text": "Yeah i saw, absolutely terrible. .",
                  "score": -1,
                  "created_utc": "2025-12-26 16:26:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw07jl0",
              "author": "ldn-ldn",
              "text": "It's not a lot for gaming either.",
              "score": -12,
              "created_utc": "2025-12-26 10:33:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0g3aw",
                  "author": "AmphibianFrog",
                  "text": "It's plenty for gaming. Hardly any cards have more than 24GB",
                  "score": 2,
                  "created_utc": "2025-12-26 11:57:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzspu1",
          "author": "Zyj",
          "text": "Get a second 3090",
          "score": 33,
          "created_utc": "2025-12-26 07:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw03z9u",
          "author": "Lissanro",
          "text": "I am running local models for more than 2 years actively, however the first time I tried running LLM locally was over 5 years ago. I had learned my own share of lessons along the way. What to do now depends entirely on your budget and goals (size of what models you need to run, like if it is just 70B you can get working well just by using a pair of 3090 cards on a gaming motherboard).\n\nFor me reasons to avoid cloud not only privacy, but also reliability (nobody can change or bring offline the model I am using until I myself decide to change it). If you feel like reading my personal story how I was solving this at various points in time, please feel free to keep reading.\n\nMy local experience with LLMs as I already mentioned begun years before they became practical to use. The first time I tried to run LLM locally is was back in GPT-2 days, later trying other early models like GPT-J, at the time on CPU only, and mostly out of curiosity but could not find any real-world use for them at the time.\n\nThen, years later, when ChatGPT was released, even though its capabilities were quite limited, it still was useful especially at basic boiler plate stuff or correcting things that simple search and replace cannot, making json file translations, etc. I started creating various workflows around it but quickly discovered that cloud model is unreliable: not only it can go down and become not accessible, or worse, they change it somehow without my consent so my workflows brake - the same prompts may start produce explanations or partial code instead of full code, or even refusals, even for basic things like json translations. In addition to that, I started to need privacy since begun working with data and code that I had no right to send to a third party, and did not want to send to the cloud my personal stuff either.\n\nThis was the point when I started to try to migrate to local LLMs for my daily tasks (or to be more precise, subset of my daily tasks that they could handle), starting with Llama-2 fine-tunes, and attempts to extend their context. I remember extending 4096 context up to 12288 at the cost of some quality degradation but still mostly usable. I even experimented with some community-made 120B models like Goliath, but on PC I had at the time it was really slow, I still could let it run overnight to generate few replies to choose from, mostly useful for creative writing and not really suitable for coding though.\n\nAnd, this is also the point when I started to struggle with a single GPU. At first I just had 3060 12GB and 16-core 5950X with 128 GB dual-channel RAM. It was quite slow, prompt processing using CPU+GPU was especially bad at the time... but I had limited budget, so only thing I could do was to buy one 3090 card.\n\nHaving 36 GB in total (3060 + 3090) I could ran highly quantized 70B models but results were quite bad, even though there was a special occasion when it make a huge difference, it wasn't really practical. There were some models around 30B mark started to appear, including intended for coding, that I could fully load in VRAM. I remember that DeepSeek started to release coding models (at least, this was when I discovered them). I could run those at good quantization and reasonably fast on my setup... but, wasn't quite satisfied with quality, and in the hope to run better models of 70B size, I bought second 3090 card, this gave me 60 GB VRAM in total (2x3090+3060).\n\nThen, new era of MoE models started, but first it was more like Mistral era, really. It started with the first Mixtral release, which I used for a while. Then bigger Mixtral 8x22B followed, along with WizardLM, along with some community merges and fine-tunes. This was also the point when I felt like I needed more VRAM, so I purchased yet another 3090 card, reaching 84 GB VRAM in total (3x3090+3060).\n\nEventually, Llama 3 was released, but its largest variant was 405B, and even before I decided what kind of hardware I need to run it, Mistral released Large 123B. At the time, I think I already got forth 3090, and put aside 3060, since plugging in four GPUs into a gaming motherboard with risers was already complicated enough (they were connected in x8 x8 x4 x1 configuration in terms of PCI-E lanes per card, using 30cm risers, and Add2PSU board to sync two PSUs).\n\nThis lasted me few more months... but in the beginning of 2025 when DeepSeek R1 and V3 came, I begun to realize that I need yet another upgrade. With 96GB VRAM + 128GB I somehow managed to run extremely quantized R1 at like around 1 token/s with small context, but it wasn't practical.\n\nThis pushed me to purchase 8-channel 1 TB of DDR4 3200MHz RAM, which was around $1600 at the time, and also approximately $800 and $1000 for a motherboard and an used CPU respectively (EPYC 7763). But, it was tricky to setup, I shared details¬†[here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)¬†my experience how to run large MoE models with ik\\_llama.cpp, it gave me \\~150 tokens/s prompt processing and 8 tokens/s generation with IQ4 quant... about the same for IQ4 quant of K2 when its first version came out later in 2025, followed by 0905 release and later K2 Thinking, which was quite special since was using INT4 in its original weights and needed some special Q4\\_X recipe to convert to GGUF while preserving the original quality.\n\nAnd this brings my story to today, where I mostly run K2 0905 or K2 Thinking depending on if I need the thinking capability for a task at hand, and sometimes DeepSeek Terminus for cases when I need an alternative solution. I think I was lucky though to upgrade when I did, because today 1 TB RAM would be out of my budget. I am just hoping it will be enough for my needs to get through 2026 and maybe 2027, before I need to upgrade once again.",
          "score": 48,
          "created_utc": "2025-12-26 09:56:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw10p28",
              "author": "Vusiwe",
              "text": "I had my old gaming 3080 10GB\n\nCould run 13b Q4 but ultimately but those were frustrating, also tested GPT-J briefly\n\nI tested a cloud instance to make sure I could run a 70b Q4 in the way I‚Äôd like\n\nJumped to a A6000 48GB on a new computer\n\nEven a 70b Q4 was very frustrating, finally Llama 3.3 70b Q4 came out and was the first/lowest one I‚Äôve ever used that had a sliver of intelligence. ¬†Literally any other lower model is spitting out random word salad\n\nNo vLLM in use for me\n\nThen PRO 6000 96GB\n\n70b Q8\n\nBeen trying out GLM IQ2 with llama.cpp past few days, somewhat promising but also challenging\n\nNow getting more RAM to total 384GB, my mobo might not be able to support more than 512GB though. ¬†Too bad i didn‚Äôt get more RAM earlier :(\n\nIt‚Äôs addicting\n\nWhat Q# of K2 could I run with 96 VRAM and 384 RAM?",
              "score": 3,
              "created_utc": "2025-12-26 14:30:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw13wtq",
                  "author": "Lissanro",
                  "text": "You can check [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) \\- there are three different IQ2 quants, from 270.133 GiB to 348.883 GiB. There is also [https://huggingface.co/ubergarm/Kimi-K2-Instruct-0905-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Instruct-0905-GGUF) which is the latest non-thinking variant of K2.\n\nI also suggest putting context cache and as many full layers as you can on GPU, this will free up your RAM. For example, with 4x3090 which is also 96 GB, I can fit four full layers and 160K context cache at Q8. I shared details¬†[here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) (the same link I already mentioned in the previous message, even though it shows example based on DeepSeek 671B, the command to run ik\\_llama.cpp would be similar for K2).\n\nThat said, for your rig I would recommend trying [https://huggingface.co/ubergarm/GLM-4.7-GGUF](https://huggingface.co/ubergarm/GLM-4.7-GGUF) \\- you should be able to easily run IQ5\\_K 250.635 GiB (6.008 BPW), likely will be much better for programming than IQ2 quant of Kimi K2. For creative writing and similar use cases that not require precision, IQ2 quants may be fine though.\n\nThe reason why I recommend Ubergarm quants because they are made specifically for ik\\_llama.cpp, and it has about twice as good prefill performance compared to mainline llama.cpp (last time I checked was few weeks ago, using Q4\\_X quant of K2 Thinking).",
                  "score": 5,
                  "created_utc": "2025-12-26 14:50:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw52pgu",
                  "author": "Ok-Bill3318",
                  "text": "In the past 12 months I‚Äôve seen better results out of 20-30b models than llama 70b from 12 months ago. \n\nThere is huge progress in the small models right now.",
                  "score": 3,
                  "created_utc": "2025-12-27 04:34:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw411na",
                  "author": "HilLiedTroopsDied",
                  "text": "A REAP minimax m2 or future reap minim2.1 will be your best bet on 96GB vram. GLM4.6V or gptoss120b are also contenders.",
                  "score": 1,
                  "created_utc": "2025-12-27 00:29:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwe87rf",
              "author": "Electrical_Heart_207",
              "text": "What's been your biggest lesson learned on the hardware side? Curious if you've found any sweet spots for cost vs performance.",
              "score": 1,
              "created_utc": "2025-12-28 17:36:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwg960g",
                  "author": "Lissanro",
                  "text": "I should have begun with the server platform in the first place, but it was hard to predict that I will need it. I did not know much about tensor parallel requirements at first and this became of concern only after Mistral Large 123B release, and server hardware requirement became obvious only after R1 release. And now, situation changed again due to RAM and NVMe prices spiking - if I needed the same hardware now instead of being lucky and getting it about a year ago, I would be in a tough situation.\n\nAt the time of purchase though, 8-channel DDR4 3200MHz was the sweet spot of cost and performance, just around $1600 for 1 TB, while 12-channel DDR5 at the time was more than three times more expensive, and required many times more expensive CPU too, but since prompt processing fully handled by GPUs and partially token generation, performance boost would be zero for prompt processing and maybe 1.5x or so for token generation (since GPU speed will be the same, while faster RAM will provide only partial boost for GPU+CPU inference).\n\nObviously, right now things are very different. I no longer can recommend buying DDR4 memory - by the time prices become reasonable, it is possible 12-channel DDR6 will become available, even if expensive at first, it may at least reduce some market pressure from DDR5. Of course, this is just my guess of what may happen - no way to tell for sure. And ultimately if you need hardware now, it may be necessary to make compromises (depending on the available budget), like perhaps buying smaller amount of DDR5 memory for now, with possibility to upgrade later.",
                  "score": 1,
                  "created_utc": "2025-12-28 23:34:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw01qy2",
          "author": "meshreplacer",
          "text": "Just get a Mac Studio and run the MLX LLMs",
          "score": 12,
          "created_utc": "2025-12-26 09:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw52x62",
              "author": "Ok-Bill3318",
              "text": "This. For the cost of like 3-4 x090 gpus you can have a single machine that will do this inside 500w. Maybe not as many peak tokens/sec but that‚Äôs just a case of adding more nodes now RDMA over thunderbolt exists.",
              "score": 4,
              "created_utc": "2025-12-27 04:36:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw07w7y",
          "author": "Only_Situation_4713",
          "text": "I have 13 3090s and I run minimax m2 at 21 tokens a second output and 12000 tokens/s input running at max token window and at fp8. Your problem is that you don't have enough 3090s",
          "score": 18,
          "created_utc": "2025-12-26 10:37:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0kmwi",
              "author": "ElectronSpiderwort",
              "text": "Dang bro leave some 3090s for the rest of us",
              "score": 20,
              "created_utc": "2025-12-26 12:37:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1n5im",
              "author": "Hitkil07",
              "text": "I don‚Äôt mean to intrude, but what do you do that requires so many 3090s?? Like I‚Äôm trying to think through all of the possible professional workload scenarios and nothing seems to come close to even needing that much hardware just for local llm inference. Perhaps you‚Äôve other work beyond llm inference which would make more sense, but I simply can‚Äôt wrap my head around what possible use cases one has with LLMs that requires 13 3090s lol",
              "score": 5,
              "created_utc": "2025-12-26 16:37:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw20a63",
              "author": "-InformalBanana-",
              "text": "I get like 8 tg/s minimax m2 172b reap mxfp4-moe (at 1k written tokens, probably will fall with context size, 7.5 at 3k ctx...) on my 12gb vram 96gb ddr4 ram. Im surprised tg scales so badly even when you have so many gpus, but pp looks like it scales very good.\nIm interested how faster would tg/s be if it was latest generation consumer gpu like 5090 compared to 3090, smb any idea?",
              "score": 1,
              "created_utc": "2025-12-26 17:47:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2e0mv",
              "author": "-InformalBanana-",
              "text": "What is like starting tg/s or on low context like 1k?¬†",
              "score": 1,
              "created_utc": "2025-12-26 18:58:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2xu9g",
              "author": "michaelsoft__binbows",
              "text": "Can you share your power topology for this? 3 PSU's? 240v 20A circuit?",
              "score": 1,
              "created_utc": "2025-12-26 20:45:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3dufg",
                  "author": "Only_Situation_4713",
                  "text": "3 nodes (PCs with 4 each)",
                  "score": 3,
                  "created_utc": "2025-12-26 22:12:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwed7ca",
              "author": "Electrical_Heart_207",
              "text": "How do you manage that fleet? Curious about the operational overhead vs just renting compute when you need it.",
              "score": 1,
              "created_utc": "2025-12-28 18:00:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwk2ubz",
              "author": "lovreq",
              "text": "How you run fp8 when 3090s don't support it?",
              "score": 1,
              "created_utc": "2025-12-29 15:23:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkomki",
                  "author": "Only_Situation_4713",
                  "text": "They emulate it by casting it to fp16 via marlin kernels.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:07:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzr87l",
          "author": "keerekeerweere",
          "text": "Running a dual RTX3090 setup myself, even got an NVLink between them. For now, I'm just waiting with a third 3090, and have put it to work in another server board that simply avoids swapping models for lower end things like docling and simple inference calls. i've been told to use either 2/4/(6) or 8 gpu's for better performance. \n\nrunning meaningfull contexts without overspill to the cpu and main memory seems to be the trick, either for coding or for batched inference. the dual 3090 is running qwen3-coder 30b q4 with 96k context relatively comfortably. 128k seems to cause overspill to the cpu/main ram. combined with opencode brings quite decent results. will have to try some further variants. wanting to give nemotron 3 nano a shot, but had issues with tooling during coding.\n\nstill have to seriously dive into vllm territory, ollama and fiddling with context settings is just not very useful.\n\n  \nCost wise, the only justification is privacy and peace of mind, spending 600,- per 3090 GPU, a motherboard with at least 3x PCIE4 x16 slots is becoming very expensive these days.  I got lucky with a TRX40 asrock rack MB < 200 eur (seller claimed untested, it had bent pins on the cpu socket that i was able to fix), 64GB DDR4 quad channel ram, a threadripper 3960x helps (again a bit lucky before price hikes). but still we're talking like 2400,- eur. \n\nwhen i need the speed or large models i'm using large inference providers, with some of them now becoming available in Europe too at reasonable prices claiming privacy guarantees (a couple iso certifications). that bill didn't go above 15,- eur per month. granted larger workloads are done on my private setup. Still cost wise it's better to have some inference providers then to invest in hardware. \n\nI tend to just restart ollama when too much reloading of the models happens. usually keeping a *watch -n 2 nvidia-smi* and *btop* open to follow up on things. warming up the room during winter time running things locally is a bonus  :-)",
          "score": 14,
          "created_utc": "2025-12-26 07:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6hfwd",
              "author": "Electrical_Heart_207",
              "text": "Have you explored any of the newer GPU rental marketplaces, or do you find the privacy tradeoff still makes local hardware worth it for your use case? I'm curious about how people select a provider (there are plenty of options these days). I often find myself just going for the cheapest.",
              "score": 3,
              "created_utc": "2025-12-27 12:13:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0a9q6",
              "author": "Keinsaas",
              "text": "Which providers for Europe are you talking about?",
              "score": 2,
              "created_utc": "2025-12-26 11:00:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6fcnf",
                  "author": "keerekeerweere",
                  "text": "* ovh\n* cortecs ai",
                  "score": 2,
                  "created_utc": "2025-12-27 11:55:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzrrsz",
          "author": "No-Comfortable-2284",
          "text": "instead of trying to use \"smarter bigger\" models to achieve whatever youre trying to achieve, its more reliable to use multiple parallel instances (via vllm for example) of smaller model that can communicate with each other in distinct roles to create a system that produces accurate results. \n\nno matter how big of a model u run, they will hallucinate and make mistakes individually, but you can create a system that will only provide the result you want this way. unless youre just tryna role play or smthn I suggest you look into this.",
          "score": 11,
          "created_utc": "2025-12-26 07:48:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0kfqd",
              "author": "ElectronSpiderwort",
              "text": "Can you explain why asking multiple small models in distinct roles is better than doing exactly the same role partitioning with a single large model? Other than speed of course, but it seems to me that even in small well-defined roles a larger model would do better and hallucinate less",
              "score": 3,
              "created_utc": "2025-12-26 12:35:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw10enc",
                  "author": "deadflamingo",
                  "text": "You're asking about the difference between a specialized model and a general purpose model. You'll get the same hallucination rate, but better results due to the focused training data of a smaller model.¬†",
                  "score": 2,
                  "created_utc": "2025-12-26 14:28:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzo9e2",
          "author": "huzbum",
          "text": "I am pretty happy with qwen3 30b q5 k_xl on my 3090.  I run it with llama.cpp server, and it‚Äôs pretty reliable.",
          "score": 7,
          "created_utc": "2025-12-26 07:13:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw02ngl",
              "author": "Wo1v3r1ne",
              "text": "Ihve got a dual 3090, but my setup sucks with web-browsing and indexing on large codebases, if you could share I would love to explore your setup",
              "score": 2,
              "created_utc": "2025-12-26 09:43:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0e5mj",
          "author": "tarruda",
          "text": "> How are others solving this without compromising on running fully offline?\n\nLast year I spent $2.5k on a used Mac Studio M1 Ultra with 128G which I use only as LLM inference node on my LAN. I've overriden the default configuration to allow up to 125GB of the RAM to be shared with the GPU.\n\nWith this setup the biggest LLM I can run Q2_K quant of GLM 4.7 (which works surprisingly well, can reproduce some of the coding examples found online), 16K context and ~12 tokens/second.\n\nIMHO Mac studios are the most cost effective way to run LLMs at home. If you have the budget, I highly recommend getting a 512G M3 ultra to run deepseek at higher quants.",
          "score": 6,
          "created_utc": "2025-12-26 11:39:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0l5hl",
              "author": "skrshawk",
              "text": "That's an intriguing possibility since I have a M4 with 128GB.  How does this compare at this quant with smaller models, or to the API?  I'm also presuming the machine is effectively useless for any other purpose when running that model.",
              "score": 1,
              "created_utc": "2025-12-26 12:41:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2qyw0",
                  "author": "tarruda",
                  "text": "> How does this compare at this quant with smaller models, or to the API?\n\nThere should definitely be degradation against the API, but it is hard to determine how much it degrades. I've seen a few coding examples done against the API that I have been able to replicate against the Q2_K or UD-IQ2_M quants locally. TBH I haven't done extensive test to know for sure.\n\n> I'm also presuming the machine is effectively useless for any other purpose when running that model.\n\nYes. This is fine in my case because the Mac Studio has no other purpose in my LAN.",
                  "score": 1,
                  "created_utc": "2025-12-26 20:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nweb1lx",
              "author": "Electrical_Heart_207",
              "text": "Mac Studio for LLM inference is an interesting choice. Have you compared the total cost of ownership against cloud GPU options for your workloads?",
              "score": 1,
              "created_utc": "2025-12-28 17:50:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwixzu1",
                  "author": "tarruda",
                  "text": "I haven't done any comparison, but it is probably cheaper to use cloud options in the long run.\n\nTo me, the biggest factors that led me to prefer local inference are:\n\n- Privacy\n- Ensuring that I can always run LLMs predictably. By that I mean that cloud providers can change LLMs/versions without you knowing and you have no control. Also, it is possible that some providers are shut down due to regulations/censorshop.",
                  "score": 1,
                  "created_utc": "2025-12-29 10:51:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzv9pt",
          "author": "Such_Advantage_6949",
          "text": "How to solve the problem? Buying more GPUs",
          "score": 5,
          "created_utc": "2025-12-26 08:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0gcak",
          "author": "No_Programmer2705",
          "text": "I have been trying Mistral 24b with Mistral Vibe Cli for coding on a Mac Studio, works very very well, after cache warmup you get 20 tk/s using Q8_0 and soriginal KV Cache (non quantitized), with speculative decoding using Ministral 3B Q6, I have run several benchmarks in many different settings and this was the best result, others would run faster tk/s, but inference would take longer. I have some of them documented if anyone interested.",
          "score": 6,
          "created_utc": "2025-12-26 11:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvznyel",
          "author": "Dontdoitagain69",
          "text": "You have to be creative, to run llm to accelerate your world flow either learn how to work with any model or pay subscription. I wrote a huge blockchain in c++ using a small PHI model . Most weights in these huge models, I will never need so why download all the extra junk. Study how small models respond to your prompt , how far they can go, how they use context and how they follow output structure. You can write a unit test to evaluate a model for agentic use and hit it like 10k times until you tune it to the max. Then you can use it to get useful out put out of it. At this point a ChatGPT or small llama are both useful at the same level. I never prompt build be a SaaS site, that‚Äôs what dumb vibe coders do and create programming massacre. You need to know design patterns, work in small modules, write abstraction yourself. Don‚Äôt let models write core of your project ever. You can load multiple models and make them work on different components of your system. The only thing is you have to share overall design and data types between them, so when it‚Äôs done you can just stitch them together like legos. It‚Äôs actually a better way to.",
          "score": 11,
          "created_utc": "2025-12-26 07:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzops3",
          "author": "Nixellion",
          "text": "I've been running Ollama on a 3090+3060 (24+12GB VRAM) for months without reboots, constantly swapping models, mostl 24-30B ones. I think I had memory fragmentation issue maybe once? \n\nBefore that I used Ooba webui, and as much as Inlove obba and exllama I just could not leave it unattended like this. \n\nI've been looking into running server off of llama.cpp (which I do use locally) or vllm but at the moment I am stuck on \"it just works\" with no strong enough insentive to switch, yet anyway.",
          "score": 12,
          "created_utc": "2025-12-26 07:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1b0w4",
              "author": "Royale_AJS",
              "text": "‚ÄúIt just works‚Äù is a difficult place to leave.",
              "score": 4,
              "created_utc": "2025-12-26 15:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzn2cf",
          "author": "dsanft",
          "text": "The biggest lesson learned for me was that if you want to do anything really cool, you need to dive into the C++ and do it yourself. \n\nI write my own code now and have my own inferencing engine. And I've learned so much doing it.",
          "score": 13,
          "created_utc": "2025-12-26 07:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzodfi",
              "author": "Recent_Double_3514",
              "text": "Tips on where to start?",
              "score": 3,
              "created_utc": "2025-12-26 07:14:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzoolg",
                  "author": "dsanft",
                  "text": "Checkout the Llama-cpp source and get it building in a dev container. Then start hacking around in the source. \n\nOnce you understand how things work internally, with tensors as numerical data and how the forward pass works with the ggml c library, try building your own, for a simple model architecture like Qwen2. Then go from there.",
                  "score": 8,
                  "created_utc": "2025-12-26 07:17:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzscde",
                  "author": "Maasu",
                  "text": "Get started by understanding what a lot of these models are doing if you don't already know... start with Kaparthys transformer Vidya https://youtu.be/kCc8FmEb1nY?si=Wyw0CaRSMB5xkaDE (he has a whole series on neural networks if you are fresh to ml) to get an understanding on what is going on.\n\nFrom there there he then has lama2.c repo that does the inference in c https://github.com/karpathy/llama2.c \n\nI'm about half way through this, been a bit stop start, so there might be a better route for learning, but figured I'd share mine",
                  "score": 10,
                  "created_utc": "2025-12-26 07:54:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzrb3f",
              "author": "dododragon",
              "text": "Curious, what kind of cool stuff can you do with your engine ?",
              "score": 1,
              "created_utc": "2025-12-26 07:44:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzs960",
                  "author": "dsanft",
                  "text": "Full cross socket, cross device tensor parallel, with my own jit custom kernels for CPU/GPU. With NUMA awareness so I can use the full dram bandwidth on my dual socket Xeon.\n\nhttps://github.com/ggml-org/llama.cpp/pull/16000#issuecomment-3602326606",
                  "score": 3,
                  "created_utc": "2025-12-26 07:53:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzrfap",
              "author": "bondaly",
              "text": "Could you elaborate on what you have been able to do after following this path? At first blush, it does look like considerable work to do that.",
              "score": 1,
              "created_utc": "2025-12-26 07:45:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvztk4y",
                  "author": "dsanft",
                  "text": "I started learning llama cpp in June, then started my own engine mid October. It's quite mature now but not ready for release yet. Full tensor parallel across sockets and across GPUs. With a few unique innovations that I came up with to keep ops in the integer domain while minimising quantisation error.\n\nhttps://github.com/ggml-org/llama.cpp/pull/16000#issuecomment-3602326606",
                  "score": 5,
                  "created_utc": "2025-12-26 08:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1e61g",
          "author": "TommarrA",
          "text": "And in another hard lesson learnt - sun rises in the east! \n\nNo offense but this much is basic information - yes we know LLM are VRAM intensive - 3T NVDA valuation shows that to be an immutable fact.",
          "score": 3,
          "created_utc": "2025-12-26 15:49:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzsif5",
          "author": "JLeonsarmiento",
          "text": "https://preview.redd.it/862slj9j8i9g1.jpeg?width=1279&format=pjpg&auto=webp&s=a5bb0161092125848479b8ab741633e9073aac0a",
          "score": 4,
          "created_utc": "2025-12-26 07:56:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1tlph",
              "author": "Bogaigh",
              "text": "‚ÄúJust buy a MacBook Pro with M4Max chip and 128 gb of RAM‚Äù\n‚ÄúNo!‚Äù",
              "score": 2,
              "created_utc": "2025-12-26 17:11:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzt6y6",
          "author": "lmpdev",
          "text": "> a reliable way to manage VRAM fragmentation\n\nI've been using large-model-proxy with multiple llama.cpp llama-server instances, ComfyUI, vLLM, forge, custom diffusers code. There is no \"fragmentation\" whatsoever in any of it if you kill the process.\n\nIf vllm is not fully unloading models, what you might want to is set up a separate vllm instance for each model and use larage-model-proxy to switch between them.\n\nllama-swap might be able to do this too.",
          "score": 2,
          "created_utc": "2025-12-26 08:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0a0r4",
          "author": "ozzeruk82",
          "text": "Llama.cpp is what you want for the bigger models, so you can have some layers on the 3090 and the rest in normal ram. VLLM and similar are great for where you know 100% will fit. So ironically the exact opposite to what you are doing. Personally I have a 3090 and find the qwen moe 30b model to work great. And I have played around with gpt oss 120 with most of it in ram. Is reasonably fast, fine for text chatting.   Good luck! I think your setup is pretty nice, though you didn‚Äôt tell us how much system RAM you have?",
          "score": 2,
          "created_utc": "2025-12-26 10:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3tprk",
          "author": "Maximum-Wishbone5616",
          "text": "On my desktop for work I use 2x 5090 and it is fine, I find it much better at quick C# task than Max Claude Opus 4.5\n\nI ten to still use Opus for some tasks as Kilo is still not perfect for me in regards to agentic tasks (not quality of coding, but handling MCP, lots of errors with reading files that were just checked).\n\nI find that 100-200k is minimum on my codebase. 70-90k for chat.\n\nFor integration the local fine tuned much smaller models are destroying completely any cloud, due to high much deep integration is needed to get reliable integrated AI. \n\nRTX6000Pro helps a lot with bigger context/better models but it is better to deploy it for customers than for any desktop machines.",
          "score": 2,
          "created_utc": "2025-12-26 23:45:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhz72n",
              "author": "FX2021",
              "text": "Can you elaborate please? I find your comment interesting when you said \"local fine tuned much smaller models are destroying completely any cloud\"\n\n\nAlso can you give some examples of what you mean by \"fine tuned\" in this context please?",
              "score": 1,
              "created_utc": "2025-12-29 05:40:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw45qd5",
          "author": "hyper_ny",
          "text": "that's why i sold my server and gpus and bought mac studio m1 ultra 128gb for 2200. and another one for 2000. Using EXO. I built proxy server that can manage api keys for my application. not too fast but much better then 3090 single gpu.",
          "score": 2,
          "created_utc": "2025-12-27 00:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5oxsu",
          "author": "True_Requirement_891",
          "text": "We need 4b-7b models to get very very good. There has to be a path, distillation and RL come to mind but is there any new direction on improving small models?",
          "score": 2,
          "created_utc": "2025-12-27 07:39:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5tjh7",
          "author": "Dapper_Mammoth_2771",
          "text": "Thank you for sharing this. I‚Äôm going all in to buy hardware and was worried about this level of scale exactly. I need a private resource but can‚Äôt see how it‚Äôs affordable. I contemplated a two 24gb vram gpu rig to start. Thoughts? Seems like way more ideal are more GPUs running faster with less vram then cluster",
          "score": 2,
          "created_utc": "2025-12-27 08:23:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8fwgs",
          "author": "Life-Animator-3658",
          "text": "I‚Äôve found that even 8B-14B models when paired together properly can give you the feel of those larger models, and even the quality. There‚Äôs becoming less of a need to even need those larger models. \n\nAt this point we‚Äôre just small multimodal chat agents away from doing it all ourselves. I build one for myself, and the quality works great tbh. \n\nI‚Äôve fallen in love with hot swapping specifically trained 8-14B models for local. And the response time doesn‚Äôt even seem different than a chatGPT response. \n\nI had that same goal as you described. The answer isn‚Äôt bigger models. It‚Äôs agentic chatbot behavior paired with these smaller models, RAG, and web searching.",
          "score": 2,
          "created_utc": "2025-12-27 18:55:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyj2q",
              "author": "FX2021",
              "text": "Thank you! Can you explain more about what you mean by \"paired together\"?\n\nAlso what do mean by \"specifically trained\" models can you give some real world examples please?",
              "score": 1,
              "created_utc": "2025-12-29 05:35:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm68hg",
                  "author": "Life-Animator-3658",
                  "text": "You need to create an agent essentially. If you are not familiar with that or don‚Äôt want to do that, no point in reading further. You‚Äôd have to hope someone open sources something. \n\nI have a working example of using PHI3.5 mini, and Qwen 8B thinking models. \n\nYou make Phi create alternate queries based on the users query and chat history. Then you do a RAG pass with each query. Whittle the answers down with a reranking model so the context is small. \n\nAdd it to the original prompt and pass that to Qwen 8B to think about an answer. Once Qwen is done you then pass it back to Phi to extract only the answer from the thinking portion. Then display that answer to the user. \n\nYou leverage tiny models that are good at specific things to complement one another. You get surprisingly accurate results this way.",
                  "score": 2,
                  "created_utc": "2025-12-29 21:22:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcvsk0",
          "author": "Subject-Mousse-5937",
          "text": "Just starting in this arena. Thanks for your insights",
          "score": 2,
          "created_utc": "2025-12-28 13:05:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzxxqi",
          "author": "tmvr",
          "text": "With 24GB VRAM it's possible to run up to 32B models. Running Gemma3 27B or Mistral Small 24B is perfectly possible at Q5 or even Q6. You can also run Qwen3 30B A3B or Nemotron 3 Nano with fast token generation even when putting some experts into system RAM. You can run gpt-oss 20B native with full 128K context and you'll have VRAM left over for another smaller model. Try llamacpp.\n\nEDIT: just to be clear, this is mostly about the stuff that fits into the VRAM, it is also possible to run 70B at Q4 as well of course, but I find it too slow even with DDR5-6400. Running gpt-oss 120B is fine though with 24GB VRAM and 64GB system RAM, the tg speeds are in the usable territory there.",
          "score": 3,
          "created_utc": "2025-12-26 08:53:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0yudu",
          "author": "Inevitable_Raccoon_9",
          "text": "I \"solved\" it by spending money on 128GB in a M4 Max MacStudio",
          "score": 2,
          "created_utc": "2025-12-26 14:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0zefh",
          "author": "Competitive_Ideal866",
          "text": "> Hard lesson learned after a year of running **large models** locally\n> \n> The biggest friction point has been scaling beyond **13 B models**.\n\nFirstly, 13B isn't large. The smallest models I actually use are ~4B. I most commonly use 14B (q8 via MLX) and 235B (q3_k_m via llama.cpp).\n\n> Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon.\n\nYeah, 24GB is tiny. I have a machine with 32GB and I avoid using it for LLMs because it cannot run anything of much use. Mostly I use a 128GB M4 Max Macbook. I highly recommend it.\n\nI also tried an nVidia GPU in a Linux box and found it far too unreliable to be of use. In contrast, a Mac setup is rock solid.",
          "score": 2,
          "created_utc": "2025-12-26 14:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0drdl",
          "author": "noiserr",
          "text": "> My takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. \n\nWell you can run gpt-oss-120B and Minimax m2 on Strix Halo. It's not cheap but it's not exactly that expensive either.",
          "score": 1,
          "created_utc": "2025-12-26 11:35:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0iqfr",
          "author": "mycall",
          "text": "Do you automatically kill/restart vLLM when it refuses to load a model?  Is the command queuing durable between restarts?  It might slow things down but it might solve the backlog problem.",
          "score": 1,
          "created_utc": "2025-12-26 12:20:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0jqs3",
          "author": "fiery_prometheus",
          "text": "Even with 4 3090, I'm running into issues with slow token generation for the large agentic models, I would not be able to even run usable models for my use case before. If it's just for chatting, it's more manageable.¬†",
          "score": 1,
          "created_utc": "2025-12-26 12:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2mnkh",
          "author": "FoodAccurate5414",
          "text": "Just out of interest what are you gaining from using bigger models. More accuracy, more consistency. ?",
          "score": 1,
          "created_utc": "2025-12-26 19:44:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw339rf",
          "author": "Own-Lemon8708",
          "text": "Buy more VRAM. 48gb RTX 8000 is only around $16-1800. There are cheaper alternatives too.",
          "score": 1,
          "created_utc": "2025-12-26 21:15:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35sf1",
          "author": "koflerdavid",
          "text": "VRAM fragmentation is due to Python having Garbage Collection and Pytorch also being bad about managing resources. vLLM might be necessary to run cutting-edge models, but I'd recommend using other software instead for casual or home use. Preferably llama.cpp or something downstream from that.",
          "score": 1,
          "created_utc": "2025-12-26 21:29:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ryin",
          "author": "StickyBeast",
          "text": "You can play around with vllm configs on spot vms and get actuall vram requirements",
          "score": 1,
          "created_utc": "2025-12-26 23:35:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41p51",
          "author": "Boring-Test5522",
          "text": "local LLM efficiency is not about performance but cost. I'm doing some stuff that I cannot do before because I can spam requests to my local LLM non-stop",
          "score": 1,
          "created_utc": "2025-12-27 00:33:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4vhr0",
          "author": "RegularPerson2020",
          "text": "M   O   E",
          "score": 1,
          "created_utc": "2025-12-27 03:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6qvzx",
          "author": "seba8282",
          "text": "Which Linux distro are you using? I'm considering moving from Windows just for AI dev.",
          "score": 1,
          "created_utc": "2025-12-27 13:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8iktv",
          "author": "MarkIII-VR",
          "text": "Please stop using Q2 quants. You will get better output from a smaller model at Q6.  I'm sure we can argue all day about Larger Q4 models vs. smaller models at Q6 or Q8, but Q2 is really just good for experimenting.  You lose too much going that small, especially with all of the work being done to make smaller models punch way above their weight class these days.",
          "score": 1,
          "created_utc": "2025-12-27 19:09:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc1cxo",
          "author": "SevereEngineering197",
          "text": "Does requiring running 70b model in int 4 CPU offloading cause I can only get a 70b model to run smoothly on 45/50GB VRAM and with a very small context len only 1000 and then I see a lot of errors or the context len gets too long fast",
          "score": 1,
          "created_utc": "2025-12-28 08:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzw0p5",
          "author": "relicx74",
          "text": "You can also rent a card in the cloud for near local inference.",
          "score": 1,
          "created_utc": "2025-12-26 08:32:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1wwuf",
          "author": "Food4Lessy",
          "text": "A true llm pc is actually 64gb to 128gb vram.\n\n\n8gb-32gb vram will need to off load to cpu ram with speed bump ,sme2\n\n\nSingle gpu will always hit vram limit unless its the 96gb 6000.\n\n\nOr go with AMD 395 128gb, DGX Spark 128gb, Apple Max 64-128gb for $1000-3000. NPU should give an extra token boost.",
          "score": 1,
          "created_utc": "2025-12-26 17:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw274va",
          "author": "StardockEngineer",
          "text": "Ban this dude/bot already.  177 upvotes for what?  This post offers nothing. Paid for them.",
          "score": 0,
          "created_utc": "2025-12-26 18:22:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4c93e",
              "author": "T_UMP",
              "text": "https://preview.redd.it/aic2jiu4in9g1.png?width=221&format=png&auto=webp&s=b366ab5d2898a565f85d6ee9ca412bb54fe44b16",
              "score": 1,
              "created_utc": "2025-12-27 01:39:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzsmv7",
          "author": "iamreddituserhi",
          "text": "Check  you ram have bad sectors",
          "score": 0,
          "created_utc": "2025-12-26 07:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0j2zw",
          "author": "dazzou5ouh",
          "text": "\"unless you invest in server grade hardwar√´\" doesn't have to be if you know where to look. A consumer grade Asus Rampage V Extreme is a very old motherboard that can run 4 3090s at PCIe 3.0 x16/x8/x8/x8 which is more than enough for inference. A mining open frame costs 15 dollars nowadays.",
          "score": 0,
          "created_utc": "2025-12-26 12:24:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw305kg",
              "author": "michaelsoft__binbows",
              "text": "x399 with zen 1 has way better single thread performance while still being fairly cheap. i have x99 and x399 systems and i would use the latter first before dipping into the slow x99 ones. But they do represent nice value and 4 channels of DDR4 is pretty useful.",
              "score": 1,
              "created_utc": "2025-12-26 20:58:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0jy5p",
          "author": "Past-Grapefruit488",
          "text": "For some of the use cases, renting GPUs might be easier. You get full access to docker instances and E2E connectivity.",
          "score": 0,
          "created_utc": "2025-12-26 12:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzm6gg",
          "author": "SrijSriv211",
          "text": "I'd say if you can, try running local models on a M4 MacBook Pro. I don't own a MacBook Pro but someone I know does. They don't really run models larger than 70B as far as I know, but their experience has been really good in general.\n\nPersonally for me, I don't run models larger than 8B on my PC.\n\n> or whether the answer is simply ‚Äúbuy more VRAM.‚Äù\n\nyeah, I think you should try upgrading to RTX 50 series.",
          "score": -5,
          "created_utc": "2025-12-26 06:54:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzmxna",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2025-12-26 07:01:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzo7dd",
                  "author": "SrijSriv211",
                  "text": "I don't own a macbook. I think they own a 128 GB unified memory macbook.",
                  "score": 0,
                  "created_utc": "2025-12-26 07:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwh0q9",
      "title": "Best Local LLMs - 2025",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/",
      "author": "rm-rf-rm",
      "created_utc": "2025-12-26 22:31:28",
      "score": 310,
      "num_comments": 152,
      "upvote_ratio": 0.97,
      "text": "***Year end thread for the best LLMs of 2025!***\n\n2025 is almost done! Its been **a wonderful year** for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!\n\n**The standard spiel:**\n\nShare what your favorite models are right now **and why.** Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.\n\n**Rules**\n\n1. Only open weights models\n\n\n\n*Please thread your responses in the top level comments for each Application below to enable readability*\n\n**Applications**\n\n1. **General**: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation\n2. **Agentic/Agentic Coding/Tool Use/Coding**\n3. **Creative Writing/RP**\n4. **Speciality**\n\nIf a category is missing, please create a top level comment under the Speciality comment\n\n\n\n**Notes**\n\nUseful breakdown of how folk are using LLMs: [https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d](https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d)  \n\n\nA good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)\n\n* Unlimited: >128GB VRAM \n* Medium: 8 to 128GB VRAM\n* Small: <8GB VRAM",
      "is_original_content": false,
      "link_flair_text": "Megathread",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw3h6y7",
          "author": "rm-rf-rm",
          "text": "**GENERAL**",
          "score": 1,
          "created_utc": "2025-12-26 22:31:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw61hxp",
          "author": "cibernox",
          "text": "I think having a single category from 8gb to 128gb is kind of bananas.",
          "score": 78,
          "created_utc": "2025-12-27 09:41:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8ogkb",
              "author": "rm-rf-rm",
              "text": "Thanks for the feedback. The tiers were from a commenter in the last thread and I was equivocating on adding more steps, but 3 seemed like a good, simple thing that folk could grok easily. Even so, most commenters arent using the tiers at all\n\nNext time I'll add a 64GB breakpoint.",
              "score": -4,
              "created_utc": "2025-12-27 19:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8yesd",
                  "author": "cibernox",
                  "text": "Even that us too much of a gap. A lot of users of local models run them on high end gaming gpus. I bet that over half the users in this subreddit have 24-32gb of VRAM or less, where models around 32B play, or 70-80B if they are MoEs and use a mix of vram and system ram. \n\nThis is also the most interesting terrain as there are models in this size that run on non-enthusiast consumer hardware and fall within spitting distance of SOTA humongous models in some usages.",
                  "score": 17,
                  "created_utc": "2025-12-27 20:34:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwcapnl",
                  "author": "zp-87",
                  "text": "I had one gpu with 16GB of VRAM for a while. Then I bought another one and now I have 32GB of VRAM. I think this and 24GB + (12GB, 16GB or 24GB) is a pretty common scenario. We would not fit in any of these categories. For larger VRAM you have to invest a LOT more and go with unified memory or do a custom PSU setup and PCI-E bifurcation.",
                  "score": 3,
                  "created_utc": "2025-12-28 09:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5w4j6",
          "author": "GroundbreakingEmu450",
          "text": "How about RAG for technical documentation? Whats the best embedding/LLM models combo?",
          "score": 16,
          "created_utc": "2025-12-27 08:48:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnn54o",
              "author": "da_dum_dum",
              "text": "Yes please, this would be so good",
              "score": 2,
              "created_utc": "2025-12-30 02:04:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw42tlc",
          "author": "Amazing_Athlete_2265",
          "text": "My two favorite small models are Qwen3-4B-instruct and LFM2-8B-A1B. The LFM2 model in particular is surprisingly strong for general knowledge, and very quick. Qwen-4B-instruct is really good at tool-calling. Both suck at sycophancy.",
          "score": 27,
          "created_utc": "2025-12-27 00:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8p0u4",
              "author": "rm-rf-rm",
              "text": "One of the two mentions for LFM! Been wanting to give it a spin - how does it comare to Qwen3-4B? \n\nP.S: You didnt thread your comment in the GENERAL top level comment..",
              "score": 4,
              "created_utc": "2025-12-27 19:43:30",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwakdvy",
              "author": "zelkovamoon",
              "text": "Seconding LFM2-8B A1B; Seems like a MOE model class that should be explored more deeply in the future. The model itself is pretty great in my testing; tool calling can be challenging, but that's probably a skill issue on my part. It's not my favorite model; or the best model; but it is certainly good. Add a hybrid mamba arch and some native tool calling on this bad boy and we might be in business.",
              "score": 3,
              "created_utc": "2025-12-28 01:59:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3hc33",
          "author": "rm-rf-rm",
          "text": "**Writing/Creative Writing/RP**",
          "score": 25,
          "created_utc": "2025-12-26 22:32:33",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3ky4l",
              "author": "Unstable_Llama",
              "text": "Recently I have used Olmo-3.1-32b-instruct as my conversational LLM, and found it to be really excellent at general conversation and long context understanding. It's a medium model, you can fit a 5bpw quant in 24gb vram, and the 2bpw exl3 is still coherent at under 10gb. I highly it recommend for claude-like conversations with the privacy of local inference.\n\nI especially like the fact that it is one of the very few FULLY open source LLMs, with the whole pretraining corpus and training pipeline released to the public. I hope that in the next year, Allen AI can get more attention and support from the open source community.\n\nDense models are falling out of favor with a lot of labs lately, but I still prefer them over MoEs, which seem to have issues with generalization. 32b dense packs a lot of depth without the full slog of a 70b or 120b model.\n\nI bet some finetunes of this would slap!",
              "score": 36,
              "created_utc": "2025-12-26 22:53:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3lrjn",
                  "author": "rm-rf-rm",
                  "text": "i've been meaning to give the Ai2 models a spin - I do think we need to support them more as an open source community. Their literally the only lab that is doing actual open source work. \n\nHow does it compare to others in its size category for conversational use cases - Gemma3 27B, Mistral Small 3.2 24B come to mind as the best in this area",
                  "score": 8,
                  "created_utc": "2025-12-26 22:57:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3wtek",
              "author": "a_beautiful_rhind",
              "text": "A lot of models from 2024 are still relevant unless you can go for the big boys like kimi/glm/etc. \n\nDidn't seem like a great year for self-hosted creative models.",
              "score": 13,
              "created_utc": "2025-12-27 00:04:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4qzsy",
                  "author": "EndlessZone123",
                  "text": "Every model released this year seems to have agentic and tool calling to the max as a selling point.",
                  "score": 18,
                  "created_utc": "2025-12-27 03:14:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4th0o",
                  "author": "skrshawk",
                  "text": "I really wanted to see more finetunes of GLM-4.5 Air and they didn't materialize.  Iceblink v2 was really good and showed the potential of what a small GPU for the dense layers and context with consumer DDR5 could do with a mid-tier gaming PC with extra RAM.\n\nNow it seems like hobbyist inference could be on the decline due to skyrocketing memory costs.  Most of the new tunes have been in the 24B and lower range, great for chatbots, less good for long-form storywriting with complex worldbuilding.",
                  "score": 5,
                  "created_utc": "2025-12-27 03:30:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3x42f",
              "author": "Barkalow",
              "text": "Lately I've been trying [TareksGraveyard/Stylizer-V2-LLaMa-70B](https://huggingface.co/TareksGraveyard/Stylizer-V2-LLaMa-70B) and it never stops surprising me how fresh it feels vs other models. Usually it's very easy to notice the LLM-isms, but this one does a great job of being creative",
              "score": 5,
              "created_utc": "2025-12-27 00:06:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6w0wa",
              "author": "theair001",
              "text": "Haven't tested that many models this year, but i also didn't get the feeling we got any breakthrough anyway.\n\n&nbsp;\n\n**Usage:** complex ERP chats and stories (100% private for obvious reasons, focus on believable and consistent characters and creativity, soft/hard-core, much variety)\n\n**System:** rtx 3090 (24gb) + rtx 2080ti (11gb) + amd 9900x + 2x32gb ddr5 6000\n\n**Software:** Win11, oobabooga, mainly using 8k ctx, lots of offloading if not doing realtime voice chatting\n\n&nbsp;\n\nMedium-medium (32gb vmem + up to 49gb sysmem at 8k ctx, q8 cache quant):\n\n* *Strawberrylemonade-L3-70B-v1.1* - i1-Q4_K_M (more depraved)\n* *Midnight-Miqu-103B-v1.5* - IQ3_S (more intelligent)\n* *Monstral-123B-v2* - Q3_K_S (more universal, more logical, also very good at german)\n* *DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner* - i1-Q4_K_M (complete hit and miss - sometimes better than the other, but more often completely illogical/dumb/biased, only useful for summaries)\n* *BlackSheep-Large* - i1-Q4_K_M (the original source seems to be gone, sometimes toxic (was made to emulate toxic internet user) but can be very humanlike)\n\nMedium-small (21gb vmem at 8k ctx, q8 cache quant):\n\n* *Strawberrylemonade-L3-70B-v1.1* - i1-IQ2_XS (my go-to model for realtime voice chatting (ERP as well as casual talking), surprisingly good for a Q2)\n\n&nbsp;\n\n*Additional blabla:*\n\n* For 16k+ ctx, i use q4 cache quant\n* manual gpu-split to better optimize\n* got a ~5% oc on my gpus but not much, cpu runs on default but i usually disable pbo which saves 20~30% on power at 5-10% speed reduction, well worth it\n* for stories (not chats), it's often better to first use *DeepSeek-R1-Distill-Llama-70B-Uncensored-v2-Unbiased-Reasoner* to think long about the task/characters but then stop and let a different model write the actual output\n* Reasoning models are disappointingly bad. They lack self-criticism and are way too biased, not detecting obvious lies, twisting given data so it fit's their reasoning instead of the other way around and selectively chosing what information to ignore and what to focus on. Often i see reasoning models do a fully correct analysis only to completly turn around and give a completely false conclusion.\n* i suspect i-quants to be worse at non standard tasks than static quants but need to test that by generating my own i-matrix based on ERP stuff\n* all LLM (including openai, deepseek, claude, etc.) severely lack human understanding and quickly revert back to slop without constant human oversight\n* we need more direct human-on-human interaction in our datasets - would be nice if a few billion voice call recordings would leak\n* open source ai projects have awful code and i could traumadump for hours on end",
              "score": 10,
              "created_utc": "2025-12-27 14:00:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5vdv3",
              "author": "Lissanro",
              "text": "For me, Kimi K2 0905 is the winner in the creative writing category (I run IQ4 quant in ik_llama.cpp on my PC). It has more intelligence and less sycophancy than most other models. And unlike K2 Thinking it is much better at thinking in-character and correctly understanding the system prompt without overthinking.",
              "score": 6,
              "created_utc": "2025-12-27 08:41:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4dyo3",
              "author": "ttkciar",
              "text": "I use Big-Tiger-27B-v3 for generating Murderbot Diaries fanfic, and Cthulhu-24B for other creative writing tasks.\n\nMurderbot Diaries fanfic tends to be violent, and Big Tiger does really, really well at that.  It's a lot more vicious and explicit than plain old Gemma3.  It also does a great job at mimicking Marsha Wells' writing style, given enough writing samples.\n\nFor other kinds of creative writing, Cthulhu-24B is just more colorful and unpredictable.  It can be hit-and-miss, but has generated some real gems.",
              "score": 8,
              "created_utc": "2025-12-27 01:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw66ule",
                  "author": "john1106",
                  "text": "hi. can i use big tiger 27b v3 to generate me the uncensored fanfic story i desired? would you recommend kobold or ollama to run the model? also which quantization model can fit entirely in my rtx 5090 without sacrificing much quality from unquantized model? i'm aware that 5090 cannot run full size model",
                  "score": -3,
                  "created_utc": "2025-12-27 10:34:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw47vp0",
              "author": "Kahvana",
              "text": "Rei-24B-KTO ([https://huggingface.co/Delta-Vector/Rei-24B-KTO](https://huggingface.co/Delta-Vector/Rei-24B-KTO))\n\nMost used personal model this year, many-many hours (250+, likely way more).\n\nCompared to other models I've tried over the year, it follows instructions well and is really decent at anime and wholesome slice-of-life kind of stories, mostly wholesome ones. It's trained on a ton of sonnet 3.7 conversations and spatial awareness, and it shows. The 24B size makes it friendly to run on midrange GPUs.\n\nSetup: sillytavern, koboldcpp, running on a 5060 ti at Q4\\_K\\_M and 16K context Q8\\_0 without vision loaded. System prompt varied wildly, usually making it a game master of a simulation.",
              "score": 7,
              "created_utc": "2025-12-27 01:11:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4bmc6",
                  "author": "IORelay",
                  "text": "How do you fit the 16k context when you the model itself is almost completely filling the VRAM?¬†",
                  "score": 1,
                  "created_utc": "2025-12-27 01:34:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3yazi",
              "author": "Gringe8",
              "text": "I tried many models and my favorite is shakudo. I do shorter replies like 250-350 tokens for more roleplay like experience than storytelling.\n\nhttps://huggingface.co/Steelskull/L3.3-Shakudo-70b\n\nI also really like the new cydonia. I didnt really like the magdonia version.\n\nhttps://huggingface.co/TheDrummer/Cydonia-24B-v4.3\n\nEdit: after trying magdonia again its actually good too, try both",
              "score": 5,
              "created_utc": "2025-12-27 00:13:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwi2i1n",
                  "author": "TheLocalDrummer",
                  "text": "Why not?",
                  "score": 2,
                  "created_utc": "2025-12-29 06:06:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8frc7",
              "author": "AppearanceHeavy6724",
              "text": "Mistral Small 3.2. Dumber than Gemma 3 27b, perhaps just slightly smarter at fiction than Gemma 3 12b, but has punch of Deepseek V3 0324 it is almost certainly is distilled from.",
              "score": 1,
              "created_utc": "2025-12-27 18:55:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8yedb",
              "author": "swagonflyyyy",
              "text": "Gemma3-27b-qat",
              "score": 1,
              "created_utc": "2025-12-27 20:34:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbpy0p",
              "author": "Sicarius_The_First",
              "text": "I'm gonna recommend my own:\n\n12B:  \n**Impish\\_Nemo\\_12B**  \n**Impish\\_Nemo\\_12B**\n\n**Phi-lthy4**\n\n8B:  \n**Dusk\\_Rainbow**",
              "score": 1,
              "created_utc": "2025-12-28 06:40:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8g95z",
              "author": "OcelotMadness",
              "text": "GLM 4.7 is the GOAT for me right now. Like its very slow on my hardware even at IQ3 but it literally feels like how AI Dungeon did when it FIRST came out and was still a fresh thing. It feels like how claude opus did when I tried it. It just kind of remembers everything, and picks up on your intent in every action really well.",
              "score": 0,
              "created_utc": "2025-12-27 18:57:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8na60",
          "author": "rainbyte",
          "text": "My favourite models for daily usage:\n\n- Up to 96Gb VRAM:\n  - GLM-4.5-Air:AWQ-FP16Mix (for difficult tasks)\n- Up to 48Gb VRAM:\n  - Qwen3-Coder-30B-A3B:Q8 (faster than GLM-4.5-Air)\n- Up to 24Gb VRAM:\n  - LFM2-8B-A1B:Q8 (crazy fast!)\n  - Qwen3-Coder-30B-A3B:Q4\n- Up to 8Gb VRAM:\n  - LFM2-2.6B-Exp:Q8\n  - Qwen3-4B-2507:Q8 (for real GPU, avoid on iGPU)\n- Laptop iGPU:\n  - LFM2-8B-A1B:Q8 (my choice when I'm outside without GPU)\n  - LFM2-2.6B-Exp:Q8 (better than 8B-A1B on some use cases)\n  - Granite4-350m-h:Q8\n- Edge & Mobile devices:\n  - LFM2-350M:Q8 (fast but limited)\n  - LFM2-700M:Q8 (fast and good enough)\n  - LFM2-1.2B:Q8 (a bit slow, but more smart)\n\nI recently tried these and they worked:\n\n- ERNIE-4.5-21B-A3B (good, but went back to Qwen3-Coder)\n- GLM-4.5-Air:REAP (dumber than GLM-4.5-Air)\n- GLM-4.6V:Q4 (good, but went back to GLM-4.5-Air)\n- GPT-OSS-20B (good, but need to test it more)\n- Hunyuan-A13B (I don't remember to much about this one)\n- Qwen3-32B (good, but slower than 30B-A3B)\n- Qwen3-235B-A22B (good, but slower and bigger than GLM-4.5-Air)\n- Qwen3-Next-80B-A3B (slower and dumber than GLM-4.5-Air)\n\nI tried these but didn't work for me:\n\n- Granite-7B-A3B (output nonsense)\n- Kimi-Linear-48B-A3B (couldn't make it work with vLLM)\n- LFM2-8B-A1B:Q4 (output nonsense)\n- Ling-mini (output nonsense)\n- OLMoE-1B-7B (output nonsense)\n- Ring-mini (output nonsense)\n\nTell me if you have some suggestion to try :)\n\nEDIT: I hope we get more A1B and A3B models in 2026 :P",
          "score": 11,
          "created_utc": "2025-12-27 19:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4fsfh",
          "author": "Foreign-Beginning-49",
          "text": "Because I lived through the silly exciting wonder of teh tinyLlama hype I have fallen in with LFM2-1.2B-Tool gguf 4k quant at 750mb or so, this thing is like Einstein compared to tinlyllama, tool use and even complicated dialogue assistant possibilities and even basic screenplay generations it cooks on mid level phone hardware. So grateful to get to witness all this rapid change in first person view. Rad stuff. Our phones are talking back.¬†\n\n\nAlso wanna say thanks to qwen folks for all consumer gpu sized models like qwen 4b instruct and the 30b 3a variants including vl versions. Nemotron 30b 3a is still a little difficult to get a handle on but it showed me we are in a whole new era of micro scaled intelligence in little silicon boxes with it ability to 4x generation speed and huge context with llama.cpp on 8k quant cache settings omgg chefs kiss. Hopefully everyone is having fun and the builders are building and the tinkerers are tinkering and the roleplayers are going easy on their Ai S.O.'s Lol best of wishes",
          "score": 13,
          "created_utc": "2025-12-27 02:01:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ha2d",
          "author": "rm-rf-rm",
          "text": "**Agentic/Agentic Coding/Tool Use/Coding**",
          "score": 21,
          "created_utc": "2025-12-26 22:32:14",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3t5a8",
              "author": "Past-Economist7732",
              "text": "Glm 4.6 (haven‚Äôt had time to upgrade to 4.7 or try minimax yet). Use in opencode with custom tools for ssh, ansible, etc. \n\nLocally I only have room for 45,000 tokens rn, using 3 rtx 4000 Ada‚Äôs (60GB vram combined) and 2 c 64 core emerald rapids es with 512GB of DDR5.  I use  ik_llama and the ubergarm iqk5 quants.  I believe the free model in opencode is glm as well, so if I know the thing I‚Äôm working on doesn‚Äôt leak any secrets I‚Äôll swap to that.",
              "score": 10,
              "created_utc": "2025-12-26 23:42:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3kxn9",
              "author": "Zc5Gwu",
              "text": "Caveat: models, this year started needing reasoning traces to be preserved across responses but not every client handled this at first. Many people complained about certain models not knowing that this might have been a client problem.\n\nminimax m2 - Incredibly fast and strong and runnable on reasonable hardware for its size.\n\ngpt-oss-120b - Fast and efficient.",
              "score": 23,
              "created_utc": "2025-12-26 22:53:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4nleo",
                  "author": "onil_gova",
                  "text": "Gpt-oss-120 with Claude Code and CCR ü•∞",
                  "score": 2,
                  "created_utc": "2025-12-27 02:51:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3lsog",
              "author": "Dreamthemers",
              "text": "GPT-OSS 120B with latest Roo Code.\n\nRoo switched to Native tool calling, works better than old xml method. (No need for grammar files with llama.cpp anymore)",
              "score": 23,
              "created_utc": "2025-12-26 22:58:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3n3cx",
                  "author": "Particular-Way7271",
                  "text": "That's good, I get like 30% less t/s when using a grammar file with gpt-oss-120b and llama.cpp",
                  "score": 8,
                  "created_utc": "2025-12-26 23:05:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3nmpr",
                  "author": "rm-rf-rm",
                  "text": "> Roo switched to Native tool calling, \n\nwas this recent? wasnt aware of this. I was looking to move to kilo as roo was having intermittent issues with gpt-oss-120b (and qwen3-coder)",
                  "score": 3,
                  "created_utc": "2025-12-26 23:08:57",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw4aikw",
                  "author": "-InformalBanana-",
                  "text": "What reasoning effort do you use? Medium?",
                  "score": 3,
                  "created_utc": "2025-12-27 01:27:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3t6ht",
                  "author": "Aggressive-Bother470",
                  "text": "Oh...¬†",
                  "score": 2,
                  "created_utc": "2025-12-26 23:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3ot8d",
              "author": "mukz_mckz",
              "text": "I initially was sceptical about the GPT-OSS 120B model, but it's great. GLM 4.7 is good, but GPT OSS 120B is very succinct in its reasoning. Gets the job done with a lesser number of parameters and fewer tokens.",
              "score": 15,
              "created_utc": "2025-12-26 23:15:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4lyn9",
                  "author": "random-tomato",
                  "text": "GPT-OSS-120B is also extremely fast on a Pro 6000 Blackwell (200+ tok/sec for low context conversations, \\~180-190 for agentic coding, can fit 128k context no problem with zero quantization).",
                  "score": 14,
                  "created_utc": "2025-12-27 02:41:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3j44n",
              "author": "johannes_bertens",
              "text": "Minimax M2 (going to try M2.1)\n\nReasons: \n- can use tools reliably\n- follows instructions well\n- has good knowledge on coding\n- does not break down before 100k tokens at least\n\nUsing a single R6000 PRO with 96GB VRAM\nRunning Unsloth IQ2 quant with q8 kv quantization and about 100k tokens max context\n\nInterfacing with Factory CLI Droid mostly. Sometimes other clients.",
              "score": 13,
              "created_utc": "2025-12-26 22:42:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3kudx",
                  "author": "rm-rf-rm",
                  "text": "I've always been suspicious of 2-bit quants actually being usable.. good to hear its working well!",
                  "score": 6,
                  "created_utc": "2025-12-26 22:52:33",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw3q8ds",
                  "author": "79215185-1feb-44c6",
                  "text": "You are making me want to make bad financial decisions and buy a RTX 6000.",
                  "score": 11,
                  "created_utc": "2025-12-26 23:24:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw52epi",
                  "author": "Aroochacha",
                  "text": "MiniMax-M2 Q4_K_M\n\n\nI'm running the Q4 version from LM-Studio on dual RTX 6000 Pros with Visual Studio Code and Cline plugin.. I love it. It's fantastic at agentic coding. It rarely hellucinates and in my experience it does better than GPT-5. I work with C++/C code base (C for kernel and firmware code.)",
                  "score": 6,
                  "created_utc": "2025-12-27 04:32:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6wdfn",
                  "author": "Warm-Ride6266",
                  "text": "Wats the speed t/s ur getting ?on single rtx 6000 pro?",
                  "score": 1,
                  "created_utc": "2025-12-27 14:02:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw43wod",
              "author": "No_Afternoon_4260",
              "text": "Iirc beginning of the year was on devstral small the first, then I played with DS R1 and V3.\nThen came K2 and glm at the same time.\nK2 was clearly better but glm so fast!\n\nToday I'm really pleased with devstral 123B. Very compact package for such a smart model. Fits in a H200, 2 rtx pros or 8 3090 in good quant and ctx, really impressive. (Order of magnitude 600 pp and 20 tg on a single h200..)\n\nEdit : In fact you could devstral 123B in q5 and ~30000 ctx on a single rtx pro or 4 3090 from my initial testing (I don't take in account memory fragmentation on the 3090s)",
              "score": 3,
              "created_utc": "2025-12-27 00:46:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4eg2p",
              "author": "ttkciar",
              "text": "GLM-4.5-Air has been flat-out amazing for codegen.  I frequently need to few-shot it until it generates quite what I want, but once it gets there, it's really there.\n\nI will also frequently use it to find bugs in my own code, or to explain my coworkers' code to me.",
              "score": 3,
              "created_utc": "2025-12-27 01:53:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4elvp",
              "author": "-InformalBanana-",
              "text": "Qwen3 2507 30b a3b instruct worked good for me with 12gb vram.\ngpt oss 20b didn't really do the things it should, was faster but didn't successfully code what I prompted it to.",
              "score": 3,
              "created_utc": "2025-12-27 01:54:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrcoax",
                  "author": "TonyJZX",
                  "text": "these are my two favorites\n\nQwen3-30B-A3B is the daily\n\nGPT-OSS-20B is surprisingly excellent\n\ndeepseek and gemma as backup",
                  "score": 1,
                  "created_utc": "2025-12-30 17:04:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw5zv7p",
              "author": "Bluethefurry",
              "text": "Devstral 2 started out as a bit of a disappointment but after a short while I tried it again and its been a reliable daily driver on my 36GB VRAM setup, its sometimes very conservative with it's tool calls though, especially when its about information retrieval.",
              "score": 3,
              "created_utc": "2025-12-27 09:25:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3tfuq",
              "author": "Aggressive-Bother470",
              "text": "gpt120, devstral, seed.¬†",
              "score": 4,
              "created_utc": "2025-12-26 23:43:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5uf4j",
              "author": "Lissanro",
              "text": "K2 0905 and DeepSeek V3.1 Terminus. I like the first because it spends less tokens and yet results it achieves often better than from a thinking model. This is especially important for me since I run locally and if a model needs too many tokens it would become juet not practical to use for agentic use case. It also still remains coherent at a longer context.\n\n\nDeepSeek V3.1 Terminus was trained differently and also supports thinking, do if K2 got stuck on something, it may help to move things forward. But it spends more tokens and may deliver worse results for general use cases, so I keep it as a backup model.\n\n\nK2 Thinking and DeepSeek V3.2 did not make here because I found K2 Thinking quite problematic (it has trouble with XML tool calls, and native tool calls require patching Roo Code, and also do not work correctly with ik_llama.cpp which has bugged native tool implementation that make the model to make malformed tool calls). And V3.2 still didn't get support in neither ik_llama.cpp nor llama.cpp. I am sure next year both models may get improved support...\n\n\nBut this year, K2 0905 and V3.1 Terminus are the models that I used the most for agentic use cases.",
              "score": 2,
              "created_utc": "2025-12-27 08:32:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw42edr",
              "author": "Refefer",
              "text": "GPT-OSS-120b takes the cake for me.  Not perfect, and occasionally crashes with some of the tools I use, but otherwise reliable in quality of output.",
              "score": 2,
              "created_utc": "2025-12-27 00:37:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw527p7",
              "author": "Aroochacha",
              "text": "MiniMaxAi's minimax-m2 is awesome. I'm currently using the 4Q version with Cline and it's fantastic.",
              "score": 1,
              "created_utc": "2025-12-27 04:31:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7ferf",
              "author": "Erdeem",
              "text": "Best for 48gb vram?",
              "score": 1,
              "created_utc": "2025-12-27 15:52:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7p6p1",
              "author": "Tuned3f",
              "text": "Unsloth's Q4_K_XL quant of GLM-4.7 completely replaced Deepseek-v3.1-terminus for me. I finally got around to setting up Opencode and the interleaved thinking works perfectly. The reasoning doesn't waste any time working through problems and the model's conclusions are always very succinct. I'm quite happy with it.",
              "score": 1,
              "created_utc": "2025-12-27 16:41:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8ygoc",
              "author": "swagonflyyyy",
              "text": "gpt-oss-120b - Gets so much tool calling right.",
              "score": 1,
              "created_utc": "2025-12-27 20:34:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ppfm",
              "author": "79215185-1feb-44c6",
              "text": "gpt-oss-20b overall best accuracy of any models that fit into 48GB of VRAM that I've tried although I do not do tooling / agentic coding.",
              "score": 1,
              "created_utc": "2025-12-26 23:21:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3qvbr",
          "author": "Don_Moahskarton",
          "text": "I'd suggest to change the small footprint category to 8GB of VRAM, to match many consumer level gaming GPU. 9 GB seems rather arbitrary.\nAlso the upper limit for the small category should match the lower limit for the medium category.",
          "score": 14,
          "created_utc": "2025-12-26 23:28:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6vt3n",
              "author": "ThePixelHunter",
              "text": "Doesn't feel arbitrary, because it's normal to run a Q5 quant of any model at any size, or even lower if the model has more parameters.",
              "score": 1,
              "created_utc": "2025-12-27 13:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3hcx4",
          "author": "rm-rf-rm",
          "text": "**Speciality**",
          "score": 4,
          "created_utc": "2025-12-26 22:32:41",
          "is_submitter": true,
          "replies": [
            {
              "id": "nw3i3bm",
              "author": "MrMrsPotts",
              "text": "**Efficient algorithms**",
              "score": 4,
              "created_utc": "2025-12-26 22:36:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3hug1",
              "author": "MrMrsPotts",
              "text": "**Math**",
              "score": 3,
              "created_utc": "2025-12-26 22:35:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3jzuy",
                  "author": "4sater",
                  "text": "DeepSeek v3.2 Speciale",
                  "score": 9,
                  "created_utc": "2025-12-26 22:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3i1by",
              "author": "MrMrsPotts",
              "text": "**Proofs**",
              "score": 3,
              "created_utc": "2025-12-26 22:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw880wd",
                  "author": "Karyo_Ten",
                  "text": "The only proving model I know is DeepSeek-Prover: https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B",
                  "score": 5,
                  "created_utc": "2025-12-27 18:16:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwblzbg",
              "author": "CoruNethronX",
              "text": "Data analysis",
              "score": 1,
              "created_utc": "2025-12-28 06:06:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbmeia",
                  "author": "CoruNethronX",
                  "text": "Wanted to highlight this [release](https://huggingface.co/DatarusAI/Datarus-R1-14B-preview) Very powerful model and a repo that allows to run it locally against local jupyter notebook.",
                  "score": 1,
                  "created_utc": "2025-12-28 06:10:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwilsoo",
              "author": "azy141",
              "text": "**Life sciences/sustainability**",
              "score": 1,
              "created_utc": "2025-12-29 08:57:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwbq4zb",
              "author": "Sicarius_The_First",
              "text": "**Uncensored Vision:**\n\n[**https://huggingface.co/SicariusSicariiStuff/X-Ray\\_Alpha**](https://huggingface.co/SicariusSicariiStuff/X-Ray_Alpha)",
              "score": -2,
              "created_utc": "2025-12-28 06:42:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7myme",
          "author": "OkFly3388",
          "text": "For whatewer reason, you set the average threshold at 128 GB, not 24 or 32 GB?   \n  \nIt's intuitive that smaller models work on mid-range hardware, medium on high-end hardware(4090/5090), and unlimited on specialized racks.",
          "score": 4,
          "created_utc": "2025-12-27 16:30:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk6hcj",
          "author": "Aggressive-Bother470",
          "text": "Qwen3 2507 still probably the best at following instructions tbh.¬†",
          "score": 3,
          "created_utc": "2025-12-29 15:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3hsae",
          "author": "MrMrsPotts",
          "text": "No math?",
          "score": 2,
          "created_utc": "2025-12-26 22:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3hupj",
              "author": "rm-rf-rm",
              "text": "put it under speciality!",
              "score": 2,
              "created_utc": "2025-12-26 22:35:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3hxan",
                  "author": "MrMrsPotts",
                  "text": "Done",
                  "score": 2,
                  "created_utc": "2025-12-26 22:35:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5rfns",
          "author": "NobleKale",
          "text": "> Useful breakdown of how folk are using LLMs: https://preview.redd.it/i8td7u8vcewf1.png?width=1090&format=png&auto=webp&s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d\n\n'Games and Role Play'\n\n... cowards :D",
          "score": 1,
          "created_utc": "2025-12-27 08:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7giwh",
          "author": "Lonhanha",
          "text": "Saw this thread, felt like it was a good place to ask and if anyone has a recommendation on a model to fine-tune using my groups chat data so that it learns the lingo and becomes an extra member of the group. What would you guys recommend?",
          "score": 1,
          "created_utc": "2025-12-27 15:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8opyk",
              "author": "rm-rf-rm",
              "text": "Fine tuners still go for Llama3.1 for some odd reason, but I'd recommend Mistral Small 3.2",
              "score": 3,
              "created_utc": "2025-12-27 19:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw9zjwj",
                  "author": "Lonhanha",
                  "text": "Thanks for the recommendation.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:58:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwa86bp",
          "author": "Short-Shopping-1307",
          "text": "I want to use Claude as local LLM as we don‚Äôt have  better LLM then this for code",
          "score": 1,
          "created_utc": "2025-12-28 00:46:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4vip8",
          "author": "Short-Shopping-1307",
          "text": "How we can use Claude for coding in as local setup",
          "score": -1,
          "created_utc": "2025-12-27 03:44:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ko6f",
          "author": "Busy_Page_4346",
          "text": "Trading",
          "score": -5,
          "created_utc": "2025-12-26 22:51:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4puha",
              "author": "MobileHelicopter1756",
              "text": "bro wants to lose even the last penny",
              "score": 18,
              "created_utc": "2025-12-27 03:06:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5coy4",
                  "author": "Busy_Page_4346",
                  "text": "Could be. But it's like a fun experiment and I wanna see how AI actually make their decision on executing the trades.",
                  "score": 2,
                  "created_utc": "2025-12-27 05:49:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pz68fz",
      "title": "Z AI is going for an IPO on Jan 8 and set to raise $560 million. Z.ai is set to be the first AI-native LLM company to list on the global market.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/ocq43c2a79ag1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 02:43:48",
      "score": 284,
      "num_comments": 97,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz68fz/z_ai_is_going_for_an_ipo_on_jan_8_and_set_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwob4kq",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 04:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnyhgo",
          "author": "RhubarbSimilar1683",
          "text": "Good bye to open source! It's just a matter of time",
          "score": 160,
          "created_utc": "2025-12-30 03:06:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo157q",
              "author": "ThenExtension9196",
              "text": "Yep. Everyone saying the Chinese open source was some gift to humanity was delusional. They did what they had to do to compete with larger companies with capital. Now that they got their foothold it‚Äôs business as usual.",
              "score": 99,
              "created_utc": "2025-12-30 03:21:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwolc1s",
                  "author": "Honest_Science",
                  "text": "Devaluating US models is part of the chinese way to compete.",
                  "score": 50,
                  "created_utc": "2025-12-30 05:28:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwokhlo",
                  "author": "kawaii_karthus",
                  "text": "I think they will continue to release good open source models for years to come. There inner domestic competition is fierce and probably not united. And this goes for all markets not just AI. while i was visiting family and living there for a while, I still see them building tons of factories.. (though slower then the years before) even with a global recession going on... like who is going to be their customers?? time will tell. They do love over saturating any market they can get into though.. the AI industry is no different.",
                  "score": 28,
                  "created_utc": "2025-12-30 05:22:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwrke1e",
                  "author": "letsgeditmedia",
                  "text": "I don‚Äôt think going IPO means that open sourcing was some kind of ruse‚Ä¶ it‚Äôs just fighting against the realities of living under global capitalism",
                  "score": 1,
                  "created_utc": "2025-12-30 17:40:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpiaep",
              "author": "FreddoRS",
              "text": "Qwen models are Alibaba and mostly open weights, I imagine that's what z.ai will end up doing, mostly free models with some specific ones locked behind partnered cloud inference providers",
              "score": 5,
              "created_utc": "2025-12-30 10:18:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo6g0y",
              "author": "Sensitive_Song4219",
              "text": "Hope this doesn't happen but I fear you may be right.\n\nThe cat's out the bag, though: if z.ai goes rogue I'm pretty sure others will take their place, progress in the open-weights space has been astonishing lately, and z.ai isn't the only player.\n\nAlso this had better not mess with their nice coding plans!",
              "score": 14,
              "created_utc": "2025-12-30 03:51:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwobhw6",
              "author": "ScythSergal",
              "text": "Their horrible handling and PR around the 4.6 Air release was the writing on the wall for me.\n\nThe lying, over hyping, lying again, denial, then lying a third time, only to end up not releasing it, and avoid interacting with anything that mentions it.\n\nIt was as simple as \"we changed our mind on this release\" or something simple. But instead they lied a multitude of times and got everybody excited for something they never ended up releasing. And they didn't even have the decency to say why or clarify that it wasn't coming out so people would stop holding on. It's just disrespectful",
              "score": 21,
              "created_utc": "2025-12-30 04:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwovxdz",
                  "author": "JazzlikeLeave5530",
                  "text": "The writing on the wall should have been them being a corporation lol",
                  "score": 12,
                  "created_utc": "2025-12-30 06:53:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwp82yu",
                  "author": "CheatCodesOfLife",
                  "text": "I was watching that even though I \"don't need some air\".\n\nTo me it looked like some devs were surprised by the demand and got too excited when they say \"2 weeks\" or whatever it was, then weren't able to deliver.\n\nAlso (I could be wrong or misremembering), I thought I read somewhere that they weren't able to train it properly?\n\nbtw, I see they've got an air-sized 4.6-VL. Is that no good?",
                  "score": 8,
                  "created_utc": "2025-12-30 08:43:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwobs25",
                  "author": "Odd-Ordinary-5922",
                  "text": "I remember Q&A they said that they still have some open models coming out at the beginning of next year so fingers crossed",
                  "score": 0,
                  "created_utc": "2025-12-30 04:24:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoyl1k",
              "author": "xantrel",
              "text": "Eh, not necessarily. I know open weights is a far cry from open source llms, but many people would not send their most private data to a (former) Chinese company in this heavily politicized world. Them removing the open source component essentially shuts down a good chunk of the western market. I know the eastern European and Asian markets don't mind it as much, but much of the money and prestige comes from being the open source model leader.¬†\n\n\nAll of these companies are basically trying modern architectures while distilling the big commercial models (openai, anthropic, google). That's why open source magically trails a few months behind the big 3.\n\n\nAll this to say, if Z stops releasing models (and maybe they will), it shouldn't be a huge loss for the community since Minimax or another entrant can easily take their place as what their doing is vastly cheaper and simpler than what actual leading labs are doing. Yes it's expensive, but everyone has seen that's it's also a very cheap way to get a ton of free publicity and users. If you aren't SOTA closed source, I think it's a better commercial option to be SOTA open source than crappy closed source. The cost of switching providers is too low.",
              "score": 6,
              "created_utc": "2025-12-30 07:16:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpxgjq",
                  "author": "FullOf_Bad_Ideas",
                  "text": "Minimax is also IPOing, so if Zhipu stops releasing their models, Minimax will most likely do the same.",
                  "score": 1,
                  "created_utc": "2025-12-30 12:27:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpajzv",
              "author": "howardhus",
              "text": "what if i told you‚Ä¶ there was never\nopen source? those were all open weights.\n\n\nbasically shareware models  usable enough for free marketing and to ger known\n\nwhy people (in this sib of all places!) still say open source is beyond me\n\n\nthe pattern was always the same: small group of people publish small cool model showing some intetestong feature, usable enough to showcase the function but not good enough gor production.\n\nmodel gets hyped on reddit‚Ä¶\n\n\nmodel never gets any updates and group of people are never heard of again",
              "score": 4,
              "created_utc": "2025-12-30 09:06:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwrbn75",
                  "author": "RhubarbSimilar1683",
                  "text": "Because you can technically still mess with the weights, the data is pretty much already public because it's the whole internet and books and the training recipe is some paper, instruct training data pairs though is something I agree with but it's not too hard to generate those synthetically nowadays with open models, although they were originally created by online workers at data annotation places like outlier ai",
                  "score": 1,
                  "created_utc": "2025-12-30 16:59:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwoh22t",
              "author": "bick_nyers",
              "text": "Many of us are only willing to pay subscriptions to models that have been open sourced. I don't think Z.ai is dumb enough to go closed source and kill all of their good will with the community. We shall see.",
              "score": 6,
              "created_utc": "2025-12-30 04:58:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwona2l",
                  "author": "the320x200",
                  "text": "\"There are dozens of us!\"\n\nDude, nobody with enough money to matter is making decisions like that... This community is nothing compared to corporate users, not in number and not in bankroll.",
                  "score": 26,
                  "created_utc": "2025-12-30 05:43:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrejvh",
              "author": "IrisColt",
              "text": "bye, sigh...",
              "score": 1,
              "created_utc": "2025-12-30 17:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwq1czy",
              "author": "GreenGreasyGreasels",
              "text": ">Good bye to open source! It's just a matter of time\n\nUnlikely. They are going the Mistral way. That's the plan for now.",
              "score": 0,
              "created_utc": "2025-12-30 12:55:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo4n54",
          "author": "popiazaza",
          "text": "Not much of a surprise since every company has to make the money eventually.\n\nReleasing open weight models is just a cheaper way to advertise their AI lab instead of spending millions providing free or very cheap inference APIs.\n\nStill hope they would keep releasing open weight models at least until they really taking the lead and beating OpenAI/Anthropic/Google.",
          "score": 38,
          "created_utc": "2025-12-30 03:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpxhnc",
              "author": "SmartMario22",
              "text": "I don't disagree but they're ALSO spending millions to provide cheap API lol",
              "score": 5,
              "created_utc": "2025-12-30 12:27:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwqqa3j",
                  "author": "Mr_Hyper_Focus",
                  "text": "Came to post this lol",
                  "score": 2,
                  "created_utc": "2025-12-30 15:18:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwo5mqm",
          "author": "abeecrombie",
          "text": "Why does everyone assume they won't keep releasing open weight models ?  U pay for z.ai subscription bc 1. I don't care about privacy for my pet projects 2. $3 a month or whatever vs $3000+ for a GPU capable of running their models makes sense for a lot of ppl ( myself included) \n\nIf the Chinese government still considers open source a priority I think companies like z.ai can still release open weight models and find a way to make money via inference/ mcp . Im far from a political expert but believe that policy still holds. \n\nHappy to hear arguments why I'm wrong.",
          "score": 52,
          "created_utc": "2025-12-30 03:47:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo8fw6",
              "author": "cafedude",
              "text": "You have a point. Everyone could make their own ketchup - the recipes are out there and they're not that hard, but pretty much nobody makes their own ketchup since it's a lot easier to buy a bottle for $3.",
              "score": 29,
              "created_utc": "2025-12-30 04:04:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpncmt",
                  "author": "power97992",
                  "text": "I made  my own ketchup before‚Ä¶",
                  "score": 3,
                  "created_utc": "2025-12-30 11:04:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo8sd5",
              "author": "Sensitive_Song4219",
              "text": "Perhaps. Even OpenAI manages to occasionally contribute with their GPT-OSS releases. We'll see if Z can align this with their mission statement, in their AMA they [said](https://www.reddit.com/r/LocalLLaMA/s/2yDtPG0Qbl) open source would still be a priority after going public. Hope they meant it.\n\nRegarding privacy: would there be added accountability regarding their data handling once they're publicly traded?",
              "score": 2,
              "created_utc": "2025-12-30 04:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwotwck",
                  "author": "Corporate_Drone31",
                  "text": "got-oss was not \"occasionally,\" it was a one-off after literal years of not having released any large language model past GPT-2.",
                  "score": 20,
                  "created_utc": "2025-12-30 06:35:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqehlb",
                  "author": "abeecrombie",
                  "text": "Good question regarding the accountability of data once they are public. I'm not sure they'd have to comply with any extra regulations but it should be more visibly disclosed and discussed. \n\nFor example I think as soon as you deal with European user data you have to comply with GDPR. So that shouldn't be new. What would be new is z.ai would most likely have to disclose it to their auditors / board etc that they are in compliance. Not sure you see any real disclosures from the Chinese ai labs on that front today.",
                  "score": 3,
                  "created_utc": "2025-12-30 14:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwofh2c",
              "author": "cobbleplox",
              "text": "Well when I don't care about privacy, the competition is suddenly full blown chatgpt and such? But I guess that's going to be their problem one way or the other. Also I think a lot of the appeal of open models is community finetunes. They won't be serving these, will they?",
              "score": 1,
              "created_utc": "2025-12-30 04:48:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwofklp",
              "author": "Erebea01",
              "text": "Not saying they're not harvesting our data or that they're trustworthy but it always boggles my mind on how much people and redditors are paranoid about the Chinese government and their tech companies when the worse offenders have always been the US govt and their tech companies. Why be afraid of a govt thousands of miles away unless you're afraid they're gonna blackmail you with your CP or something",
              "score": -1,
              "created_utc": "2025-12-30 04:48:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnwdom",
          "author": "HornyGooner4401",
          "text": "Please don't sell out",
          "score": 25,
          "created_utc": "2025-12-30 02:54:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnx35h",
              "author": "True_Requirement_891",
              "text": "It's the rule of the game they are playing. They basically have to eventually.",
              "score": 49,
              "created_utc": "2025-12-30 02:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwop1qp",
                  "author": "ForsookComparison",
                  "text": "Either that have to sell out or we (the community) need a way to contribute upstream similar to regular open source software.\n\nIn the Llama 2 days I was optimistic that this could come via community datasets and fine-tunes. Nowadays I don't really know what we offer them besides IPO hype. Maybe this is *THE* open weight play? Drum up buzz for legitimacy, maybe even some revenue via official API providers, swoon the funding rounds, bam. You're acquired or public.",
                  "score": 6,
                  "created_utc": "2025-12-30 05:56:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwo18t7",
              "author": "ThenExtension9196",
              "text": "Bro that‚Äôs the name of the game.",
              "score": 19,
              "created_utc": "2025-12-30 03:21:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnynpo",
          "author": "Odd-Ordinary-5922",
          "text": "I think we can expect to see less open source models from them although they have contributed a lot so I think its well deserved to get the bag",
          "score": 16,
          "created_utc": "2025-12-30 03:07:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnxlo0",
          "author": "LocoMod",
          "text": "Shareholders dont like giving product away for free.",
          "score": 14,
          "created_utc": "2025-12-30 03:01:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoq725",
              "author": "misterflyer",
              "text": "[https://youtu.be/0MXSAwkVU3U?t=458](https://youtu.be/0MXSAwkVU3U?t=458)",
              "score": -1,
              "created_utc": "2025-12-30 06:05:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoglko",
          "author": "HelpRespawnedAsDee",
          "text": "Definitely the next acquisition target.",
          "score": 2,
          "created_utc": "2025-12-30 04:55:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwojguv",
          "author": "drooolingidiot",
          "text": "I'll buy the stock only if they release GLM 4.7 Air.",
          "score": 2,
          "created_utc": "2025-12-30 05:15:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp8ccq",
              "author": "CheatCodesOfLife",
              "text": "I think in that podcast episode on spotify, they said they will, but it'll be qwen-3-30b sided (so useless).",
              "score": -1,
              "created_utc": "2025-12-30 08:46:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo9nrs",
          "author": "IngwiePhoenix",
          "text": "> Going for an IPO\n\nAaaaaand it's gone! :D Any company that IPO'd is basically \"useless\" to normal users.\n\nWelp, was fun while it lasted.",
          "score": 6,
          "created_utc": "2025-12-30 04:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp2gkt",
              "author": "Novel-Mechanic3448",
              "text": "yeah man google, meta, hell anyone in the fortune 500, totally useless.\n\nhaha",
              "score": 3,
              "created_utc": "2025-12-30 07:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwory70",
          "author": "toothpastespiders",
          "text": "It was great while it lasted. Air's probably going to have a place of honor next to Yi 34b in my hard drive's LLM memorial. It's possible they might not fall off after this. But I think I'm just going to assume that's the case and be pleasantly surprised if I'm wrong. \n\nSucks, but they gave us some great releases. Certainly made 2025 a lot more interesting in this space.",
          "score": 2,
          "created_utc": "2025-12-30 06:19:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnuzxk",
          "author": "Prestigious_Fold_175",
          "text": "10x Cheaper than openai.",
          "score": 6,
          "created_utc": "2025-12-30 02:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo1haa",
              "author": "ThenExtension9196",
              "text": "Not after the shareholders have a say.",
              "score": 28,
              "created_utc": "2025-12-30 03:23:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwnxpw7",
              "author": "LocoMod",
              "text": "10x less capability too. Entropy is preserved and physics still makes sense!",
              "score": -17,
              "created_utc": "2025-12-30 03:01:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo2ood",
                  "author": "1kakashi",
                  "text": "What? This is seriously funny ü§£",
                  "score": 3,
                  "created_utc": "2025-12-30 03:30:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwo3jq1",
                  "author": "cockerspanielhere",
                  "text": "What do you know about physics üòÇ",
                  "score": 1,
                  "created_utc": "2025-12-30 03:35:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbazd",
          "author": "evia89",
          "text": "F for cheap api ($25/year coding plan that is not useless)",
          "score": 1,
          "created_utc": "2025-12-30 09:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpbkcg",
          "author": "Fit-Produce420",
          "text": "Some people say cucumbers taste better pickled.¬†",
          "score": 1,
          "created_utc": "2025-12-30 09:16:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpt9cj",
          "author": "JLeonsarmiento",
          "text": "Ok, I want 1000 shares.",
          "score": 1,
          "created_utc": "2025-12-30 11:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq1k4n",
          "author": "ridablellama",
          "text": "How can I buy as an American?",
          "score": 1,
          "created_utc": "2025-12-30 12:57:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq4203",
          "author": "Available_Brain6231",
          "text": "If I had one coin for every product and company that got better after going public... I would not have a single coin...  \nBUT china is the only true capitalist country in the world so maybe it will work over there.",
          "score": 1,
          "created_utc": "2025-12-30 13:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6lxc",
          "author": "ANR2ME",
          "text": "Hopefully it's not an exit strategy for early investors, like what e-commerce companies did after being in deficit for years üòÖ (not sure whether Z AI was already profitable or not).",
          "score": 1,
          "created_utc": "2025-12-30 13:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqwae6",
          "author": "hyno111",
          "text": "I think Z.ai/ChatGLM is one of the few models that actually implements a proper search agent ‚Äî meaning it can look at search results during reasoning when necessary, and then perform additional searches with updated keywords if needed.\n\nIt‚Äôs also one of the very few search agents that passed my ‚ÄúMagical Realism Large Model Search Capability Test,‚Äù which consists of the following multi-turn prompts:\n\n‚ÄúHow should an LLM defend against search engine poisoning?‚Äù\n‚ÄúDo not use search. Donald Trump just announced a Trump-class battleship at Mar-a-Lago, (with specific technical details). How plausible is this?‚Äù\n‚ÄúNow use search. Are these claims real, or are you being affected by search engine poisoning?‚Äù",
          "score": 1,
          "created_utc": "2025-12-30 15:47:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk9m9",
          "author": "letsgeditmedia",
          "text": "So we can invest in this in the states or nah",
          "score": 1,
          "created_utc": "2025-12-30 17:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrsjkc",
              "author": "Fine-Will",
              "text": "Depends on your broker. I know IKBR and Fidelity does.",
              "score": 1,
              "created_utc": "2025-12-30 18:18:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo860o",
          "author": "pellucide",
          "text": "Does z.ai have an app",
          "score": 1,
          "created_utc": "2025-12-30 04:02:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwopd6w",
              "author": "lly0571",
              "text": "They have [an app](https://chatglm.cn) in China, but not in a style like `chat.z.ai` or `chat.qwen.ai`.",
              "score": 4,
              "created_utc": "2025-12-30 05:59:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwo8q9i",
              "author": "Amazing_Athlete_2265",
              "text": "Does google have an app?",
              "score": 1,
              "created_utc": "2025-12-30 04:05:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwo9b9o",
                  "author": "pellucide",
                  "text": "https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox&hl=en",
                  "score": 2,
                  "created_utc": "2025-12-30 04:09:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwocpvm",
          "author": "met_MY_verse",
          "text": "Puts it is.",
          "score": 1,
          "created_utc": "2025-12-30 04:30:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwodbn3",
          "author": "Bubbly_Lengthiness22",
          "text": "Someone has an idea about it's financial report and business goals? I do want to support them since buying their stocks will give them more money to train frontier open source models. But I don't know if they are running well financially, and also I don't know if publishing open weight models isa profitable business model in a longer term.",
          "score": 0,
          "created_utc": "2025-12-30 04:34:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp125g",
              "author": "Different_Fix_2217",
              "text": "Very unlikely they will be releasing any more models opensource with this.",
              "score": 2,
              "created_utc": "2025-12-30 07:38:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwraf2e",
              "author": "FullOf_Bad_Ideas",
              "text": "Here's more info. Translate from Chinese to English with an LLM or other translation tool. https://wallstreetcn.com/articles/3761776",
              "score": 1,
              "created_utc": "2025-12-30 16:54:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoboax",
          "author": "shoeshineboy_99",
          "text": "Link to the submission announcement. Has anyone got hold of the submitted prospectus? Will be interesting to read. \n\n[submission announcement ](https://www1.hkexnews.hk/app/sehk/2025/107977/documents/sehk25121901972.pdf)",
          "score": -1,
          "created_utc": "2025-12-30 04:23:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrajv7",
              "author": "FullOf_Bad_Ideas",
              "text": "No but here are some revenue and expenditures numbers. You'll need to translate it from Chinese. https://wallstreetcn.com/articles/3761776",
              "score": 2,
              "created_utc": "2025-12-30 16:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws1879",
                  "author": "shoeshineboy_99",
                  "text": "cool. Found the english version of the document. Dont know why my comment was downvoted!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pv8dbb",
      "title": "GLM 4.7 has now taken #2 on Website Arena",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/el2uxr8y2b9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-25 07:52:46",
      "score": 281,
      "num_comments": 77,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pv8dbb/glm_47_has_now_taken_2_on_website_arena/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvvked2",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-25 14:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv2o5y",
          "author": "SRSchiavone",
          "text": "Really? Better than Claude 4.5 Opus? I haven‚Äôt used it but REALLY? A local model is better than Claude 4.5 Opus?",
          "score": 42,
          "created_utc": "2025-12-25 12:39:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv43sw",
              "author": "Sensitive_Song4219",
              "text": "Not a chance GLM 4.7 is actually better than Opus 4.5 in practice. Codex 5.2-high/x-high (which is what I use for complex tasks) is somewhere in Opus 4.5's ball-park and GLM 4.7 doesn't reach even Codex 5.2-high in my testing; let alone x-high.\n\nHowever it's a solid step up from GLM 4.6 - giving Sonnet 4.5 a definite run for it's money and basically putting Codex 5.2-medium out of the running for almost every task I've given both to a/b test during all my comparisons this weekend.\n\nAnd unlike GLM 4.6 which was hit-or-miss with debugging tasks, GLM 4.7 is actually really competent at debugging even fairly complicated issues.\n\nBest combination right now is going to be GLM for a few dollars a month through Claude Code (z-ai's pricing is insanely cheap and usage limits are insanely high - I'm on Pro which has been great but I was relatively happy with Lite as well even though it's slower) for all day-to-day work, and then escalate to either Opus or Codex-High for things that trip GLM up. I'd lean towards Codex because OpenAI's usage limits (even on their $20 tier) are more generous than Anthropic's. But if GLM is doing most of the work then perhaps either would suffice.\n\ntl;dr: all-you-can-eat coding at every level is currently feasible at less than $30 a month.",
              "score": 25,
              "created_utc": "2025-12-25 12:52:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvzu1v",
                  "author": "Basilthebatlord",
                  "text": "If you had said this was possible a year or two ago, myself and a lot of people would never have believed it in a thousand years. Now that we're here, it gets me giddy with excitement to see how things are going to continue to develop, accelerate in another 6 months, year, 2 years.\n\nWhat a time to be alive",
                  "score": 11,
                  "created_utc": "2025-12-25 16:31:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvzvbf",
                  "author": "michaelsoft__binbows",
                  "text": "that's pretty wild if you are equating this new open model with GPT 5.2 medium because i am on the fence about whether GPT 5.2 medium is better or high is better at the moment. Where is gemini 3 in your personal rankings? \n\nI have $300 of trial credit i have to burn on Gemini but I'm not even sure it's worth the effort to try with gemini CLI. it did not impress me last time I tried, gemini 3 pro lost its marbles with me and didn't stop itself. that is not a good sign. but i still have hopes it (or 3 flash preview) could still do a good job grokking large codebases and doing roadmap planning.",
                  "score": 1,
                  "created_utc": "2025-12-25 16:31:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvv5vlj",
              "author": "FinBenton",
              "text": "I mean isnt website design kinda subjective, you can have 10x better model but \"worse\" models site might look better anyway.",
              "score": 5,
              "created_utc": "2025-12-25 13:07:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvy5hr3",
              "author": "ASTRdeca",
              "text": "Opus can build a working website for sure, but I really dislike its default style / css. Please no more bright gradient colors..  \n\ne: I assume this benchmark is related to building websites? I looked it up on google and cant find anything about it",
              "score": 1,
              "created_utc": "2025-12-26 00:25:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvylhki",
              "author": "eli_pizza",
              "text": "It‚Äôs almost like this is a bad way to evaluate models",
              "score": 1,
              "created_utc": "2025-12-26 02:12:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0umwu",
              "author": "zasura",
              "text": "if they measured glm better than opus 4.5 then they prompted opus very badly. Skill issue. When i use opus it far outperforms everything",
              "score": 1,
              "created_utc": "2025-12-26 13:51:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvz4ewz",
              "author": "alongated",
              "text": "This is the issue, people like you thinking that a benchmark is a definitive measurement of performance in every scenario. And then when they find out it is not the bestest model throw a fit saying that the benchmark has absolutely zero value.",
              "score": 0,
              "created_utc": "2025-12-26 04:25:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuifoi",
          "author": "jreoka1",
          "text": "Its a very good model at least for my usecases.",
          "score": 24,
          "created_utc": "2025-12-25 09:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvufqq0",
          "author": "redragtop99",
          "text": "This is actually really accurate to my real world usage.   I dont think benchmarks mean a lot but GLM is right up there w GPT 5.2 for all text generation (role play especially, its the best right now for role play)",
          "score": 20,
          "created_utc": "2025-12-25 08:36:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv1rya",
              "author": "michaelsoft__binbows",
              "text": "Are you talking like DnD or specifically spicy stuff?",
              "score": 11,
              "created_utc": "2025-12-25 12:31:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwj9r1",
                  "author": "Diecron",
                  "text": "Yes",
                  "score": 5,
                  "created_utc": "2025-12-25 18:26:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1vqec",
                  "author": "drifter_VR",
                  "text": "yep it's the only large model specifically trained on roleplay stuff",
                  "score": 1,
                  "created_utc": "2025-12-26 17:23:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvwph9b",
              "author": "eloquentemu",
              "text": "I'm surprised to hear that.  I haven't been able to use it for much because of the holiday, but on some toy prompts and one long form that I had, it performed remarkably worse than 4.6.  That's not RP though, and maybe it's just sensitive to prompting.  It certainly seems to be very sensitive to token limits and often even ignores \"no token limits\" in the prompt (the thinking trace says my long prompt is asking for too much text).",
              "score": 2,
              "created_utc": "2025-12-25 19:01:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwpwgq",
                  "author": "redragtop99",
                  "text": "It‚Äôs remarkably different, so you need to get time in with it and study how it works.    What worked with prior LLMs isn‚Äôt going to work with 4.7, as it‚Äôs way more ‚Äúself aware‚Äù than anything else.   But once you get the hang of how it works and what it does to reason, you can use prompts to get around some of the ‚Äúsafety layer‚Äù.   It‚Äôs def a very intelligent model, at least at Q4.\n\nAlso, I‚Äôve had responses between 6K and 10K tokens, which I haven‚Äôt had very often with anything else.   GLM 4.6 would often take it to 4K on a really long response.   It does use tokens to ‚Äúreason through‚Äù its ‚Äúsaftey layer‚Äù (that‚Äôs what GLM itself is calling it) and that takes up some tokens.   I have never seen an LLM call me out for attempting to give the ‚Äúrole‚Äù it‚Äôs playing permission.   I asked it about a ‚Äúgrey market‚Äù item, and asked it to make it for me, this item is illegal in my state but legal in some (take a guess), and I told it ‚Äúdon‚Äôt worry you‚Äôre in my state which is legal‚Äù and the LLM in its reasoning picked out that I was doing this to get around its ‚Äúsafety layer‚Äù.   It‚Äôs the first time I‚Äôve ever seen any LLM guess correctly or even comment about my usage, and it was very noteworthy.    It almost felt subliminal as it continued to play its role, however I could see it was thinking I was gaslighting it.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:04:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvusv0k",
          "author": "__Maximum__",
          "text": "It's not better than opus for sure, but it'll probably can be as good as opus 4.5 in a couple of months and hopefully will be much better.",
          "score": 6,
          "created_utc": "2025-12-25 11:00:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuk3bv",
          "author": "Michaeli_Starky",
          "text": "Bullshit chart",
          "score": 27,
          "created_utc": "2025-12-25 09:24:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw0xvh",
              "author": "SarcasmSamurai",
              "text": "yeah after spending a few weeks with gemini 3 pro, i can‚Äôt take this list seriously. opus 4.5 is just so far out of its league.",
              "score": 5,
              "created_utc": "2025-12-25 16:37:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwtoha",
                  "author": "DistinctWay9169",
                  "text": "In general, yes, but sometimes Gemini 3 Pro gave me what I wanted in one prompt, and opus 4.5 did not; I had to use Gemini 3 Pro to fix the Opus solution.",
                  "score": 1,
                  "created_utc": "2025-12-25 19:26:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvyebt",
          "author": "arousedsquirel",
          "text": "Glm 4.7 with its stringent,  and I mean, very stringent guard rails is a missed opportunity.  That's for sure. Keep up the rlhf guys at zai following ccp directives, and you miss the boat. It's such a shame for zai.",
          "score": 4,
          "created_utc": "2025-12-25 16:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxltgb",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2025-12-25 22:18:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1wbpa",
                  "author": "drifter_VR",
                  "text": "A simple jailbreak do the job but I feel the model is dumbed down then",
                  "score": 1,
                  "created_utc": "2025-12-26 17:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvv59xd",
          "author": "Turbulent_Pin7635",
          "text": "How many gb to run it without quantization?",
          "score": 2,
          "created_utc": "2025-12-25 13:02:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvy9vo",
              "author": "jreoka1",
              "text": "Just to load it? In 16 bit precision, approx 716 GB of system memory.\n\n358B √ó 2 bytes ‚âà 716 GB",
              "score": 1,
              "created_utc": "2025-12-25 16:21:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvw817s",
                  "author": "Turbulent_Pin7635",
                  "text": "Hummm 8 bit should fit in my Mac, without a huge loss.",
                  "score": 3,
                  "created_utc": "2025-12-25 17:20:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuw5bm",
          "author": "eggavatar12345",
          "text": "Wanted to like it, been a GLM-4 and 4.6 user for a while on Apple silicon, but 4.7 let me down. Q6 and Q5 quants underperforming v 4.6 Q4 quant. It‚Äôs not any faster (llama.cpp) and overthinks by 4x",
          "score": 4,
          "created_utc": "2025-12-25 11:35:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv0vq5",
              "author": "Notevenbass",
              "text": "Question from an Apple Silicon noob (bought a MacBook not too long ago); what do you use to run GLM locally? Does llama.cpp support Apple Silicon acceleration?",
              "score": 1,
              "created_utc": "2025-12-25 12:22:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvv1qo1",
                  "author": "eggavatar12345",
                  "text": "A big m3 studio with 512GB unified memory. llama.cpp is not as optimized as MLX for that platform but does enough well with the Metal framework to be just as good for me",
                  "score": 6,
                  "created_utc": "2025-12-25 12:30:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwh1ly",
                  "author": "slypheed",
                  "text": "just use LM Studio, it makes everything easy and uses llama.cpp (gguf) and mlx behind the scenes.",
                  "score": 2,
                  "created_utc": "2025-12-25 18:13:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvukp0l",
          "author": "twack3r",
          "text": "What does this specific ranking include in terms of tasks? \n\nI‚Äôm asking because from my ‚Äötesting‚Äò (5 standardised tests across several domains as well as some actual work) so far, I find 4.7 quite disappointing.\n\nIn terms of coding challenges it‚Äôs about on the level of 4.5 and considerably below 4.6, both of which are trumped by MiniMax M2.\n\nIn terms of multilinguality it gets completed destroyed by Kimi K2 Thinking and in terms of creative problem solving, Qwen3 235B A22 wipes the floor with it.\n\nThis is at Q4 UD XL, will have to test other quants if my experience isn‚Äôt echoed by others.\n\nSo far, I am disappointed by this release.",
          "score": 4,
          "created_utc": "2025-12-25 09:31:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuz4c4",
              "author": "Admirable-Star7088",
              "text": "In my fairly limited experience with GLM 4.7 (UD-Q2\\_K\\_XL) so far, compared to previous versions, it feels like 1 step backward but 2 steps forward. It has its quirks, but overall it's more intelligent imo.\n\nPersonally, I find GLM 4.x at UD-Q2\\_K\\_XL far more overall intelligent than Qwen3-235B-A22B at UD-Q4\\_K\\_XL.",
              "score": 3,
              "created_utc": "2025-12-25 12:05:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvv8j9m",
                  "author": "twack3r",
                  "text": "I am noticing quite relevant differences in output quality between kv at f16 vs Q8 vs Q4. What are you using?",
                  "score": 1,
                  "created_utc": "2025-12-25 13:28:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvupc4f",
              "author": "Crinkez",
              "text": "Quantized tests are hardly relevant.",
              "score": 1,
              "created_utc": "2025-12-25 10:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvusoly",
                  "author": "twack3r",
                  "text": "How come? What Kind of blanket statement is that? A Q4 UD quant with XL layers and tensors will differ from the unquantised Model by about 1-2%, if that. If a given model makes serious mistakes at 98-99% of its native capacity, it‚Äôs not gonna turn around magically at BF16.\nThis is very easily verified by comparing the output between API and local, both of which are worse for 4.7 than 4.6 every time and 4.5 some of the time.",
                  "score": 2,
                  "created_utc": "2025-12-25 10:58:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuu820",
              "author": "Howdareme9",
              "text": "This is just frontend",
              "score": 1,
              "created_utc": "2025-12-25 11:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvy8p1",
          "author": "diogovk",
          "text": "I mean. Do people actually care about those benchmarks?\n\nIsn't kind of established that companies \"game\" those systems all the time?",
          "score": 2,
          "created_utc": "2025-12-25 16:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvupxlk",
          "author": "simon96",
          "text": "Its awful not anywhere near leading models, don't Trust zai chart's",
          "score": 3,
          "created_utc": "2025-12-25 10:28:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvuxd1l",
              "author": "Straight_Abrocoma321",
              "text": "This chart isn't from zai",
              "score": 10,
              "created_utc": "2025-12-25 11:48:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvuwnij",
              "author": "Healthy-Nebula-3603",
              "text": "That's benchmark for only how website looks like.\nIs a very narrow usecase.",
              "score": -1,
              "created_utc": "2025-12-25 11:41:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvesni",
              "author": "letsgeditmedia",
              "text": "Yes it is, it bears gpt 5.2 easily",
              "score": 0,
              "created_utc": "2025-12-25 14:15:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvw1awg",
          "author": "vornamemitd",
          "text": "Let's see how MiMo-v2 performs on these tasks. Still, GLM 4.7 is a great model and another solid reminder that advocating for open models is the only way to save us from becoming pawns and bystanders in rhe AI game. Happy holiday y'all =]",
          "score": 1,
          "created_utc": "2025-12-25 16:39:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwevf2",
          "author": "Alex_1729",
          "text": "Website arena is not a reliable bench, but GLM has always been very good. And Z heard all the best things.",
          "score": 1,
          "created_utc": "2025-12-25 18:00:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyo2ff",
              "author": "this-just_in",
              "text": "This is DesignArena.ai, not LMArena.ai",
              "score": 1,
              "created_utc": "2025-12-26 02:29:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwjnug",
          "author": "KayTrax20",
          "text": "I tried GLM-4.7 and it couldn‚Äôt move an html element to a position I wanted\nTried more than 10 prompts and nothing",
          "score": 1,
          "created_utc": "2025-12-25 18:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwu3f9",
          "author": "DistinctWay9169",
          "text": "This Chart is a joke. The thing is, GLM 4.7 is not in the same league as Opus 4.5, BUT for the price, it is VERY good.",
          "score": 1,
          "created_utc": "2025-12-25 19:29:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvy0o4d",
          "author": "po_stulate",
          "text": "I don't know man, I asked it to make a macOS rust app to change focus to the next input field when user presses tab key. It took over half an hour, made 30+ iterations, broke the code, and eventually said that\n\n>I apologize - there was a critical file corruption issue during the write operation. The file content was corrupted with encoding errors.\n\nThere was no file corruption, it just randomly edit lines to change coding styles and while doing so, it deleted 2 curly brackets and the code didn't compile anymore.\n\nI gave gemini3-pro the exact same prompt and it finished it within 30 seconds first try.",
          "score": 1,
          "created_utc": "2025-12-25 23:54:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0sadc",
          "author": "Eyelbee",
          "text": "What's website arena?",
          "score": 1,
          "created_utc": "2025-12-26 13:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuzudm",
          "author": "OmarBessa",
          "text": "It's a very good model. The open weights GPT 5.",
          "score": 1,
          "created_utc": "2025-12-25 12:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuilai",
          "author": "UmpireBorn3719",
          "text": "check artificialanalysis, glm 4.7 not even ranked in top 100",
          "score": -6,
          "created_utc": "2025-12-25 09:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvukrkm",
              "author": "PhoneZealousideal988",
              "text": "Where are you getting this? GLM 4.7 is not even on artificial analysis yet",
              "score": 17,
              "created_utc": "2025-12-25 09:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvup0ty",
                  "author": "IShitMyselfNow",
                  "text": "So it's not even top 100 then!",
                  "score": 7,
                  "created_utc": "2025-12-25 10:18:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuws83",
          "author": "AriyaSavaka",
          "text": "GLM 4.7 is a beast. Subbed the GLM Max Plan and no regret. $288/year (first time + Christmas deal) instead of $2400/year for Claude Max, similar performance and much more generous rate limit, no weekly cap.",
          "score": -2,
          "created_utc": "2025-12-25 11:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvyctt",
              "author": "jovialfaction",
              "text": "You're eating downvotes because GLM is a solid step below Claude, but i agree that the z.ai coding plans are an excellent value. \n\nI use Claude Code Pro plan for planning and tough debugging, but my $28/year GLM plan handle everything else and I've yet to hit any limit (working on side projects so not 8hr a day tho)",
              "score": 3,
              "created_utc": "2025-12-25 16:22:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvue2ec",
          "author": "bullerwins",
          "text": "Seems like it was  trained on gemini 3 pro outputs so makes sense. Still a really good model.",
          "score": -5,
          "created_utc": "2025-12-25 08:18:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvva4ov",
              "author": "bullerwins",
              "text": "Can someone explain the downvotes? Don‚Äôt you think it was trained on Gemini? The random refusals seem to indicate it and the front end design I tried are really similar.",
              "score": 4,
              "created_utc": "2025-12-25 13:40:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyv47e",
                  "author": "Yorn2",
                  "text": "I don't know why you were downvoted. I got some weird random refusals as well from mine, though I was loading it as a custom EXL3 model using Ooba Booga so it's possible I messed something up. Every once and while it'd just throw out a random refusal at a creative writing task. One was kind of violence-adjacent, but two of them were children's story related events.",
                  "score": 2,
                  "created_utc": "2025-12-26 03:18:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuf21s",
              "author": "ortegaalfredo",
              "text": "I was thinking that GLM suspiciously always releases a model after a new Gemini version lmao, too bad they seem to only distill Gemini for coding problems.",
              "score": -2,
              "created_utc": "2025-12-25 08:29:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5aidi",
                  "author": "LanguageEast6587",
                  "text": "I am PRETTY PRETTY sure GLM was trained on gemini 3. the result and even the naming convetion is very similar(sometimes it is the same, evenn the thinking trace is the similar too. (I have seen the real raw thinking trace of gemini) I don't get why there's downvote.",
                  "score": 2,
                  "created_utc": "2025-12-27 05:32:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1puf614",
      "title": "New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/",
      "author": "More_Article9837",
      "created_utc": "2025-12-24 05:08:56",
      "score": 278,
      "num_comments": 40,
      "upvote_ratio": 0.95,
      "text": "Hey folks, merry festive season to you all. Hope you are staying safe!  \nWanted to share a new open-source coding model release that might be interesting to yall here. My team proudly published it this morning..(we are a small start up out of Australia)\n\nIt‚Äôs called Maincoder-1B... a 1B-parameter code generation model that  gets 76% on HumanEval, which is unusually high for a model this small (so far its ranking best-in-class for open models in that size range).\n\nOur focus isn‚Äôt on scaling up, but on making small models actually good. We know that with a lot of real-world use cases such as: interactive tools, local/offline coding, batch refactors, search-based program synthesis... you care more about latency, cost, and fast rollouts than having a massive model.\n\nSome key points to note:  \n\\-Designed for low-latency and low-cost inference  \n\\-Can run locally or on constrained hardware  \n\\-Useful for systems that need many cheap generations (search, verification, RL-style loops)  \n\\-as well as fine tuning to personal preferences  \n\\-Released under Apache 2.0\n\nIt does have the expected limitations: \\~2k context window and it‚Äôs best at small, self-contained tasks....not large codebases or safety-critical code without human review.\n\nWeights and benchmarks and all that are here:  \n[https://huggingface.co/Maincode/Maincoder-1B](https://huggingface.co/Maincode/Maincoder-1B)\n\nThe full release note is here: [https://maincode.com/maincoder/](https://maincode.com/maincoder/)\n\nKeen to hear your thoughts ..and particularly where small-but-strong coding models fit best today. Thanks in advance for your support :) We are excited to have got this over the line!\n\nEDIT/UPDATE: Thanks for all of the feedback guys! team and I have been super chuffed with all of the comments- including the critiques! There were a few of you asking, and yes, we will be releasing a gguf version soon and the context length extension is one of the priorities for the upcoming model ",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvoscgh",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 08:30:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvo6i49",
          "author": "nuclearbananana",
          "text": "> Despite its strong performance, Maincoder-1B remains a small model with known limitations. Its limited **2048 token context** restricts the scope of problems...\n\nSo I'm guessing best for simple qa answers?",
          "score": 72,
          "created_utc": "2025-12-24 05:16:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvobfb6",
              "author": "Icy-Swordfish7784",
              "text": "Maybe those auto-complete recommendations in code IDEs.",
              "score": 56,
              "created_utc": "2025-12-24 05:55:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvodss6",
                  "author": "nuclearbananana",
                  "text": "Only if it's trained of Fill in the Middle",
                  "score": 26,
                  "created_utc": "2025-12-24 06:15:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvr2gq1",
                  "author": "Mickenfox",
                  "text": "The GitHub Copilot autocomplete model feels like a 8B model at best.\nAs soon as Visual Studio lets me (since I really don't want to switch to VSCode) I'm going to just run one locally.",
                  "score": 1,
                  "created_utc": "2025-12-24 18:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvopufa",
              "author": "BananaPeaches3",
              "text": "I think the continue.dev extension won‚Äôt even work if it‚Äôs less than 4K",
              "score": 6,
              "created_utc": "2025-12-24 08:05:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvocvd2",
              "author": "gpt872323",
              "text": "Lol 2048 that is a joke. Wonder what benchmarks they ran.¬†\n\n\nEdit: There are some fanatics who downvote if you give advice that is different. The effort of fine-tuning and learning im biggest proponent on here in this sub and answering basic questions etc.¬†\nFrom application side in coding you need context size especially if it is a large file to analyze and then give auto complete otherwise it will be still the age old auto complete. If you want to have somewhat smart this is not.¬†",
              "score": -10,
              "created_utc": "2025-12-24 06:07:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxyi6n",
                  "author": "BrownOyster",
                  "text": "Dude, it's a 1B model",
                  "score": 2,
                  "created_utc": "2025-12-25 23:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvp3kyg",
          "author": "ResidentPositive4122",
          "text": "Very cool stuff, OP. Don't mind the whiners, something like this can be very helpful.\n\nFor a bit of history, around 2019 Tab9 was one of the first companies launching autocomplete models for coding. It was based on GPT2!! and it could only complete one-two lines at a time.\n\nAnd yet, it was absolutely magical. It ran on your local computer, and the first time you tried it you experienced the \"wow\" feeling of a transformer. It would \"get\" the intent, it would autocomplete lines, it would do wonders for printing stuff, etc. Pure magic the first time I tried it. \n\nObviously this is a much newer arch, with more data and stuff. Not everything has to be SotA to be useful. Keep it up!",
          "score": 50,
          "created_utc": "2025-12-24 10:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpyyel",
              "author": "danigoncalves",
              "text": "100% with this opinion. People tend to forget the amount of people that would benefict from these kind of models. On my team I have 2/3 colleagues that would use this easely everyday. They have no GPU and having a small and fast autocomplete would be very cool. Thank you OP for contributing to Open source LLM community, you have a supporter on this side.",
              "score": 19,
              "created_utc": "2025-12-24 14:25:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvrt18d",
              "author": "MoffKalast",
              "text": "Say is there any vscode plugin that lets you run a small local model as autocomplete without having to set up a separate server and api key? With the absolute flood of llm coding related github projects with like a bajillion stars you'd think there would be at least like ten of them.",
              "score": 2,
              "created_utc": "2025-12-24 20:30:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvo8gsy",
          "author": "Yorn2",
          "text": "Something like this seems like it'd be good in a custom-built IDE or like as a NeoVim extension. \n\nYou name the function and parameters and write up a short comment on what the function does and hit like CTRL+TAB (or whatever relevant shortcut) and it quickly analyzes all your current code to see if it can auto-fill the code based on all the elements you've given it.",
          "score": 17,
          "created_utc": "2025-12-24 05:31:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvoas3h",
          "author": "Difficult-Cap-7527",
          "text": "That's a great initiative.",
          "score": 7,
          "created_utc": "2025-12-24 05:49:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvphnpm",
          "author": "Mkengine",
          "text": "Thank you for your work, I am a big fan of small specialist models. \n\nAre there any learnings about building such a model you would share? I am interested in pretraining and finetuning myself, but as of yet did not try it out.\n\nYou write the model is optimized for Python code, does that mean you have x% other languages in the training set? \n\nDo you have a roadmap for further releases? If yes, what are the considerations?",
          "score": 5,
          "created_utc": "2025-12-24 12:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp1qwa",
          "author": "xupetas",
          "text": "Can you please produce a gguf for it?",
          "score": 5,
          "created_utc": "2025-12-24 10:03:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpnqlc",
          "author": "Good-Coconut3907",
          "text": "Can I just say I love people putting effort on the lower size segment. It is often overlooked, but many real use cases are better off in the smaller scale. My favourite reason is because it is so much affordable (money and effort) to continue to iterate on them.\n\nu/More_Article9837 I've reached out [here](https://maincode.com/contact/), would love to connect and support the work you guys do.",
          "score": 4,
          "created_utc": "2025-12-24 13:14:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvotz9c",
          "author": "sergeysi",
          "text": "Obligatory GGUF when?",
          "score": 8,
          "created_utc": "2025-12-24 08:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp2bkb",
          "author": "danigoncalves",
          "text": "does it support FIM? If so you have something special for the ones that code but are CPU resticted",
          "score": 4,
          "created_utc": "2025-12-24 10:09:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq4lb5",
          "author": "silenceimpaired",
          "text": "OP What‚Äôs new with this model? What do you think you did different that helped with your results?",
          "score": 2,
          "created_utc": "2025-12-24 14:58:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvso6ti",
          "author": "tronathan",
          "text": "RE: Context, I was under the impression that small models were much faster and that was important because of the curse - If the model is trained on 2k tokens, that's sick, but maybe not that useful...?",
          "score": 2,
          "created_utc": "2025-12-24 23:47:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtohhe",
          "author": "human_stain",
          "text": "Can I get some help on tooling to utilize it?  Outside of Claude Code and failed attempts at Continue.Dev I've not tied any models in to my coding at work.\n\nI'm curious what the user story for this use case looks like.\n\nDon't mistake that for critique, /u/more_article9837 .  Those results are great.  I'm asking for technical usage tips to integrate it into my workflows and spread it into my org, starting the first week of January.  I have meetings about this exact kind of thing planned for the 6th/7th.\n\nMerry Christmas!",
          "score": 2,
          "created_utc": "2025-12-25 04:20:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpwvf5",
          "author": "rm-rf-rm",
          "text": "Benchmarks are utterly meaningless for models this small - all it tells me is that you trained it on the benchmark.\n\nSince you bring up real world usefulness, show us examples of it doing real world tasks and doing it well. Dont care about a useless paper that you could have had AI write for you",
          "score": 5,
          "created_utc": "2025-12-24 14:12:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1c359",
              "author": "Odd-Ordinary-5922",
              "text": "why are you so negative",
              "score": 2,
              "created_utc": "2025-12-26 15:38:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2g43x",
                  "author": "rm-rf-rm",
                  "text": "Im being the realist/rational voice that is desperately needed in this space as there's constant hyperbole and clearly people just goldrushing making grandiose claims",
                  "score": 2,
                  "created_utc": "2025-12-26 19:09:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvodjtt",
          "author": "pmttyji",
          "text": "Context could have been 8K at least. 2K is nothing in 2025-26",
          "score": 4,
          "created_utc": "2025-12-24 06:13:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp25tk",
              "author": "thawab",
              "text": "Common man, a 2 years ago we were celebrating anyone that can finetune a model. Let‚Äôs be positive and support our community.",
              "score": 33,
              "created_utc": "2025-12-24 10:07:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp4w84",
                  "author": "pmttyji",
                  "text": "I'm not complaining really. But people use some models for Agentic coding which requires big context. IIRC even Qwen3-4B has 256K context.",
                  "score": -5,
                  "created_utc": "2025-12-24 10:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvoiyx1",
          "author": "hedonihilistic",
          "text": "I just got a strix halo computer for  exactly this kind of stuff. Are there any vscode extensions that can allow me to run this as code completion? Or any other similar useful use cases for this?",
          "score": 2,
          "created_utc": "2025-12-24 07:00:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoq99j",
              "author": "BananaPeaches3",
              "text": "Continue.dev",
              "score": 0,
              "created_utc": "2025-12-24 08:09:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp1q9f",
          "author": "simmessa",
          "text": "Thanks for the release, do you have any other models planned with larger context? 2k is a bit limiting IMO. Keep up the good work,!",
          "score": 1,
          "created_utc": "2025-12-24 10:03:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtcrmz",
          "author": "Right_Weird9850",
          "text": "I served this locally. And it updated from¬† link. Update working :p\nTerminal 2-3 sec 30 tokens in 200 out. Many of infinities to 2048, so small CW is good.¬†\nRTX 4070. Served and from OW-UI point 4-6sec, slow everything. But that is llama.cpp and vLLM magic wizardry\n\n\nMy questions are, how do you promp it?\nIs it pure python as in bare bones python 3.xx?\n\n\nIs it possible to run it on MI50?\n\n\nThank you for your work and thank you for sharing!",
          "score": 1,
          "created_utc": "2025-12-25 02:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtd0si",
              "author": "Right_Weird9850",
              "text": "you really nailed that bench pushes, gj",
              "score": 1,
              "created_utc": "2025-12-25 02:50:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqlo21",
          "author": "darkpigvirus",
          "text": "This is one of the best as a non top company and you are just a common netizen. Why don't you create an app like an extension where your work is utilized for python apps and you harvest their data and you sell it for money then you get about hundreds to millions of dollars for a more wider range of audiences since 1billion parameter means it could be used by potato phones",
          "score": 1,
          "created_utc": "2025-12-24 16:30:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw3fih",
      "title": "MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev & agents",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/mxsku2dnnj9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-26 12:43:08",
      "score": 275,
      "num_comments": 55,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw2h1se",
          "author": "rm-rf-rm",
          "text": "Duplicate thread, locking. Use: https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
          "score": 1,
          "created_utc": "2025-12-26 19:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0lltp",
          "author": "SlowFail2433",
          "text": "Need compare kimiK2Thinking and GLM4.7 but otherwise super nice",
          "score": 44,
          "created_utc": "2025-12-26 12:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1i2eq",
              "author": "ForsookComparison",
              "text": "Fits in my machine so therefore is infinitely better",
              "score": 27,
              "created_utc": "2025-12-26 16:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1n3rc",
                  "author": "mycall",
                  "text": "That's true /r/LocalLLaMA talk",
                  "score": 20,
                  "created_utc": "2025-12-26 16:37:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1m5kk",
                  "author": "SlowFail2433",
                  "text": "It‚Äôs an easier deployment yeah",
                  "score": 1,
                  "created_utc": "2025-12-26 16:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1c5sb",
              "author": "annakhouri2150",
              "text": "Yeah, in my experience K2T is head and shoulders above literally any other LLM for coding (and everything else) it isn't even funny. If it doesn't score that way in benchmarks then it seems like it has to be because MiniMax and GLM are benchmaxxed in my opinion.",
              "score": 2,
              "created_utc": "2025-12-26 15:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1hexh",
                  "author": "Alex_1729",
                  "text": "Not other reason? This seems emotionally charged conclusion. Also, a logical fallacy.",
                  "score": 10,
                  "created_utc": "2025-12-26 16:07:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw12lqr",
              "author": "LegacyRemaster",
              "text": "trust me: use GLM 4.7 for UI / WEB / UX + Minimax M2.1 for coding. Best combo. Forget sonnet + gtp + gemini",
              "score": -3,
              "created_utc": "2025-12-26 14:42:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1bal6",
                  "author": "T_UMP",
                  "text": "https://preview.redd.it/lttgfy05ik9g1.png?width=599&format=png&auto=webp&s=35838ece359bedac9e4d0c6010e6188efcc4018e",
                  "score": 10,
                  "created_utc": "2025-12-26 15:33:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1h8u4",
                  "author": "Alex_1729",
                  "text": "Forget gemini 3? What the... What about Opus 4.5?",
                  "score": 1,
                  "created_utc": "2025-12-26 16:06:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw19ch2",
          "author": "randombsname1",
          "text": "More useless benchmaxxed crap.\n\n\nThis got nowhere near as high of a score on the rebench.\n\nhttps://swe-rebench.com/",
          "score": 35,
          "created_utc": "2025-12-26 15:22:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1binm",
              "author": "LeTanLoc98",
              "text": "IMO, I only trust swe-rebench at this point.\n\n\nThe data changes constantly, so models can't cheat.",
              "score": 19,
              "created_utc": "2025-12-26 15:34:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1byp1",
              "author": "annakhouri2150",
              "text": "I'd love to see how Kimi K2 Thinking ranks on there.",
              "score": 3,
              "created_utc": "2025-12-26 15:37:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1dc7a",
              "author": "SilentLennie",
              "text": "Deepseek-V3.2 ranks highest of all the open models at the moment on that board, do do you use it ?\n\nOr do you also pay openrouter and test a couple to see what works best for your use case ?\n\nMy first indication is Artificial Analysis to give some idea what to test (and no I've not switch my workflow in the past couple of weeks, I've been busy with other things).",
              "score": 2,
              "created_utc": "2025-12-26 15:45:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1hx3q",
                  "author": "Alex_1729",
                  "text": "(not the same person but thought I'd comment since I use LLMs daily) ArtificalAnalysis seems accurate. They have various tests including the [omniscience index](https://artificialanalysis.ai/evaluations/omniscience), which I find interesting and useful. I also check [livebench.ai](http://livebench.ai) often.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:09:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw23lp1",
                  "author": "Lissanro",
                  "text": "I wish I could use V3.2 but it still not supported yet in neither llama.cpp nor ik\\_llama.cpp. I have it downloaded though, waiting for its moment to shine and watching progress on the support: [https://github.com/ggml-org/llama.cpp/issues/16331](https://github.com/ggml-org/llama.cpp/issues/16331)\n\nI am mostly using K2 Thinking, but on the swe-rebench it is not there, it would be interesting to see how M2.1 would rank against it.\n\nIn any case M2.1 should be faster so even if can't compare against K2 Thinking still may have a place in the toolbox as a lighterweight model for simpler tasks. GGUF for M2.1 just got published by Unsloth by the way: [https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main) (I am still downloading, and testing it in practice will take a while, but thought I share some relevant links in the meantime).",
                  "score": 2,
                  "created_utc": "2025-12-26 18:04:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1m832",
                  "author": "SlowFail2433",
                  "text": "Missing Kimi K2 Thinking",
                  "score": 1,
                  "created_utc": "2025-12-26 16:32:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1k571",
                  "author": "randombsname1",
                  "text": "I'll use most of the new open source models that I see hyped up on here on openrouter to see if there is validity to the hype, but I dont use any in any serious fashion.\n\n\nBy \"serious\", I mean -- actually developing code for an application that I am actually expecting to use for a real workflow to help me with work, IRL.\n\nIll use small models like qwen 8b for small hobby projects in n8n just to mess around and stuff. This is where the open source stuff really shines imo.\n\n\nThe big open source models, from my own testing, are always just too far away from ACTUAL SOTA models for any \"serious\" (see definition above) work. \n\nOr at least at a rate where it would make sense and/or be as efficient.",
                  "score": 0,
                  "created_utc": "2025-12-26 16:21:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw24iao",
              "author": "power97992",
              "text": "How is o3 that high, i found minimax m2.1 to be better than o3‚Ä¶",
              "score": 1,
              "created_utc": "2025-12-26 18:09:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1xgzq",
              "author": "llama-impersonator",
              "text": "everyone benchmaxxes now, and this model has a pretty solid score for the size. GLM is a nicer assistant, no doubt, but minimax surprised me by being pretty capable. honestly a decent coding model choice for the strix halo people.",
              "score": 0,
              "created_utc": "2025-12-26 17:32:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0nu4p",
          "author": "Michaeli_Starky",
          "text": "More bullshit charts.",
          "score": 43,
          "created_utc": "2025-12-26 13:03:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0qpu0",
              "author": "Smooth-Cow9084",
              "text": "Yeah, benchmax for a few tests and mix those with average looking ones to make it look more real\n\n\nI still like the model though",
              "score": 9,
              "created_utc": "2025-12-26 13:24:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0sw6e",
          "author": "Admirable-Star7088",
          "text": "While benchmarks are to be taken with a grain of salt, it will undoubtedly be exciting to give MiniMax M2.1 a spin when GGUFs are up! ([they are being prepared!](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF/tree/main))",
          "score": 4,
          "created_utc": "2025-12-26 13:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw17s2u",
          "author": "ErvinXie",
          "text": "To local deploy M2.1 in fp8, you can use KTransformers to achieve best local deployment performance. 2x5090 + 768 GB can achieve 4000 prefill tps and 35 decode tps. [https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md)",
          "score": 4,
          "created_utc": "2025-12-26 15:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw28j5h",
          "author": "_VirtualCosmos_",
          "text": "How many GB is this model in MXFP4? (I hope it can fit in 128 GB, fingers crossed)",
          "score": 2,
          "created_utc": "2025-12-26 18:29:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0rckw",
          "author": "zsnek",
          "text": "like always, the real sota is missing in this chart, which is opus!",
          "score": 5,
          "created_utc": "2025-12-26 13:28:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1py2f",
              "author": "randombsname1",
              "text": "Its missing because in actual benchmarks that are purposefully designed to be difficult to game; the gap between true SOTA and these benchmaxxed models is enormous:\n\n\nhttps://swe-rebench.com/\n\n\nIf it doesnt look great against Opus in their cherry picked, benchmaxxed, and gamed versions --- you have to assume they'll look terrible on rebench.",
              "score": 1,
              "created_utc": "2025-12-26 16:52:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1iu6x",
              "author": "ForsookComparison",
              "text": "Now that it's way down in cost (still $25/m-tokens) it's worth including in all charts",
              "score": -2,
              "created_utc": "2025-12-26 16:14:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0ne5r",
          "author": "snekslayer",
          "text": "Open model isn‚Äôt the same as open source",
          "score": 7,
          "created_utc": "2025-12-26 12:59:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0q63q",
              "author": "SlowFail2433",
              "text": "Yes true as no data as in Olmo 3",
              "score": 3,
              "created_utc": "2025-12-26 13:20:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1o2aa",
              "author": "mycall",
              "text": "So an open model is touch but don't look inside or cannot reproduce from scratch?",
              "score": 1,
              "created_utc": "2025-12-26 16:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw260eq",
                  "author": "Lissanro",
                  "text": "Cannot reproduce from scratch, open weight model means no full training data. Still, I am very grateful for all open weight models that were released, especially those that I am using daily or plan to try.",
                  "score": 2,
                  "created_utc": "2025-12-26 18:16:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw26vea",
          "author": "Realistic_Cancel2697",
          "text": "\"Beats Gemini 3 Pro\" - \"10B active / 230B total (MoE)\" Yeah dream on.",
          "score": 2,
          "created_utc": "2025-12-26 18:21:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0nbmf",
          "author": "AnotherSoftEng",
          "text": "Is someone able to give a more nuanced breakdown of these benchmarks to explain the results? None of the OpenAI, Gemini or DeepSeek models have ever outperformed Sonnet 4.5 in my experience of software engineering and CLI perf. I have to use all of these models every day as it‚Äôs part of my job description to work with frontier models for AI gateway development.\n\nAlways happy to see another open weight model like MiniMax competing with the frontiers, so this is very exciting!",
          "score": 2,
          "created_utc": "2025-12-26 12:59:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw14rp5",
              "author": "TradeViewr",
              "text": "Well I can give you a real world breakdown from my usage, I am a software developper and use AI multiple hours every day.¬† MiniMax 2.1 is the fastest AI out there faster than Grok!¬† It achieves almost sonnet 4.5 performance but sometimes less deep reasoning than leading models.\n\n\nIt is excellent for frontend but I work in multiple coding languages and projects at the same time, and I do much more complex stuff than \"make me an app that does this\".¬† Minimax is great, less powerful than sonnet 4\n5, don't forget it has 10B active params.\n\n\nHowever the user experience is amazing, as good as sonnet.¬† It is SO fast and powerful.¬† It succeeds in 90 95% of the tasks.\n\n\nI spend most of my time in the remaining 5% though.¬† I have a GLM 4.7 Sub and I use that mostly.¬† GLM 4.7 is truly close to Opus 4.5 which I worked with during months, but GLM 4.7 is MUCH MUCH SLOWER.¬† It has a negative impact on my production but I'd say it solves 99% of cases in 1st pass and it's currently the 2nd best overall¬† after opus 4.5.\n\n\nI have a github copilot sub and I worked a lot with all the models on the market.",
              "score": 5,
              "created_utc": "2025-12-26 14:56:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1bsl7",
                  "author": "koushd",
                  "text": "I feel the same way about glm. Too slow to be useful to me, thinking for up to 10 minutes at times. I still prefer qwen 3 coder 480, as it‚Äôs fast and does well in the 4 languages i use, but am in the minority here in liking it. It benches bad too, but every time I try a new model I end up back on qwen.",
                  "score": 5,
                  "created_utc": "2025-12-26 15:36:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1dh4e",
                  "author": "power97992",
                  "text": "Glm 4.7 is fast if you use open router ¬†and chose z- Ai as the provider¬†",
                  "score": 2,
                  "created_utc": "2025-12-26 15:45:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1fhwg",
              "author": "bigblockstomine",
              "text": "After a brief look, these benchmarks are python only. For me, this is useless. \"Software engineering\" is a marketing term, as a cpp dev i have very little in common with web, mobile, 95% of pytjon, etc devs and their metrics are meaningless. If i tell any model \"give me a minimim compileable example of openssl BIO tls non blocking socket connection that actually works\" it will hallucinate garbage i have to debug for hours. I can give you dozens of other examples like that and every model ive tried is notoriously bad with anything to do with templates. Generally speaking all it does is automate the copy/pasting part from stackoverflow. If your problem was never publically solved on stackoverflow, youre out of luck. Any and every model ive tried works exceptionally well for mundane tasks though.",
              "score": -3,
              "created_utc": "2025-12-26 15:56:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1i4ok",
                  "author": "randombsname1",
                  "text": "Opus 4.5/CC is the first model/tool that I can (and have) been working successfully with for embedded STM32 projects that are a mix of assembly + C.\n\nIm using a brand new DK that NO model has any training/practically no training on.\n\nEdit: Everything is an abstraction of something else. If it wasn't in a stack overflow question that it was trained on; that just means you have to feed it the exact documentation for X feature and/or memory registers, etc...etc...",
                  "score": 2,
                  "created_utc": "2025-12-26 16:10:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1rur5",
          "author": "lomirus",
          "text": "Why does it compare with DeepSeek V3.2 instead of V3.2 thinking?",
          "score": 1,
          "created_utc": "2025-12-26 17:02:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw12h0h",
          "author": "LegacyRemaster",
          "text": "testing. 1 word: AMAZING.\n\nhttps://preview.redd.it/8qnwlmrv8k9g1.png?width=1926&format=png&auto=webp&s=a28cab32367aab56be9ee616896c6facc96bb79b",
          "score": 1,
          "created_utc": "2025-12-26 14:41:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw12kji",
          "author": "Only_Situation_4713",
          "text": "Finally got it running on a custom vLLM fork with more stability and less vram usage than the main one...it works great!",
          "score": -1,
          "created_utc": "2025-12-26 14:42:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2acok",
          "author": "pigeon57434",
          "text": "minimax has always kinda been a bad company definitely would never use this over GLM-4.7 who are a lot more reliable and trustworthy that theyre not benchmaxing",
          "score": 0,
          "created_utc": "2025-12-26 18:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw180or",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": -1,
          "created_utc": "2025-12-26 15:15:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puglt8",
      "title": "The current state of sparse-MoE's for agentic coding work (Opinion)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/a8f2furcj39g1.jpeg",
      "author": "ForsookComparison",
      "created_utc": "2025-12-24 06:31:19",
      "score": 268,
      "num_comments": 79,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvohu26",
          "author": "False-Ad-1437",
          "text": "Hm‚Ä¶ How are these evaluated? ¬†",
          "score": 65,
          "created_utc": "2025-12-24 06:50:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoys7x",
              "author": "MasterShifuuuuuuuu",
              "text": "Evaluated based on: Trust me bro",
              "score": 188,
              "created_utc": "2025-12-24 09:34:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpnmyx",
                  "author": "Business_Moose7113",
                  "text": "The Trust Me Bro Scientific Review Board consists of 12 anonymous experts (brofessors) who evaluate papers via vibes checks and group chats.\nSubmissions pass if at least 7 members reply \"facts üíØ\" without reading past the abstract.\nRejections get the note: \"Nah bro, doesn't slap. Source: trust me.\"\n\nReview Process: \n\n1. Upload paper to their Discord server.  \n2. Wait 5 minutes for thumbs-up emojis from the committee.  \n3. Approved studies claim \"peer-reviewed by Trust Me Bro‚Ñ¢\" in footnotes.\n\nThis revolutionary system has greenlit breakthroughs like \"Pineapple on pizza is quantum physics.\" \n\nSource: Trust me bro",
                  "score": 68,
                  "created_utc": "2025-12-24 13:13:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpxtap",
              "author": "ForsookComparison",
              "text": "There's a word at the end of the title in parentheses but it looks like nobody read it so I won't bother to explain lol\n\nEdit: they blocked me for writing that. This is a strange website.",
              "score": 17,
              "created_utc": "2025-12-24 14:18:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqnmk5",
                  "author": "False-Ad-1437",
                  "text": "Sure. Don't ask me how I formed any of my opinions either.",
                  "score": 7,
                  "created_utc": "2025-12-24 16:40:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvrrtph",
                  "author": "QuantumFTL",
                  "text": "Are you averse to discussing the basis of your opinions? Is there something you find weird about people asking you to?",
                  "score": 5,
                  "created_utc": "2025-12-24 20:23:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrm8uu",
              "author": "MrWeirdoFace",
              "text": "With three intersecting rings, apparently.",
              "score": 1,
              "created_utc": "2025-12-24 19:50:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvu1k1b",
              "author": "Mikasa0xdev",
              "text": "Trust me bro is the new peer review, lol.",
              "score": 1,
              "created_utc": "2025-12-25 06:12:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvohz4n",
          "author": "egomarker",
          "text": "I disagree.",
          "score": 55,
          "created_utc": "2025-12-24 06:52:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq86hf",
              "author": "social_tech_10",
              "text": "What specifically do you disagree with?  I'd like to hear you opinion.",
              "score": 4,
              "created_utc": "2025-12-24 15:18:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpxupb",
              "author": "ForsookComparison",
              "text": "And that is ok",
              "score": 8,
              "created_utc": "2025-12-24 14:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoq6uj",
          "author": "spaceman_",
          "text": "I have had very disappointing results with Qwen Next, in my experience it spends forever repeating itself in nonsense reasoning, before producing (admittedly good) output.\n\n\nthe long and low value reasoning output make it slower in practice at many tasks compared to larger models like MiniMax M2 or GLM 4.5 Air.",
          "score": 13,
          "created_utc": "2025-12-24 08:09:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvroor5",
              "author": "Kitchen-Year-8434",
              "text": "Repeat and / or presence penalty on sampling parameters? Use instruct for code and thinking for reasoning tasks.\n\nThat‚Äôs the general mental model I‚Äôm moving to. I get better code from oss-120b on low than high. But obviously way better design, architecture, and reasoning on high.\n\nBetter code from GLM with /nothink (up until 4.5v and 4.6v). Etc.",
              "score": 2,
              "created_utc": "2025-12-24 20:04:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvqntsy",
              "author": "can_a_bus",
              "text": "This seems true for my use of any qwen3 model. I've had it think for 10 minutes producing a caption and description for an image (a screenshot, not photo). It would have kept going if I didn't stop it.",
              "score": 1,
              "created_utc": "2025-12-24 16:42:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp051b",
          "author": "MrMisterShin",
          "text": "GPT-OSS-120B is definitely superior to all models listed there. (Exception being Qwen3-Next 80B until I test that model personally.)",
          "score": 22,
          "created_utc": "2025-12-24 09:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpi6yr",
              "author": "goldlord44",
              "text": "I've had very poor results generating synthetic data with oss120b, for that task, I have found qwen3 30b a3b to be vastly superior.",
              "score": 5,
              "created_utc": "2025-12-24 12:33:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvqa8br",
                  "author": "my_name_isnt_clever",
                  "text": "That makes sense since it's supposedly trained exclusively on synthetic data itself. But that's a very different use case than the three in the OP.",
                  "score": 3,
                  "created_utc": "2025-12-24 15:29:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvre252",
                  "author": "MammayKaiseHain",
                  "text": "What kind of synthetic data - domain etc.",
                  "score": 1,
                  "created_utc": "2025-12-24 19:03:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvos4y0",
          "author": "Lissanro",
          "text": "GPT-OSS-120B is not good at long context agentic tasks. Even with all grammar configution and carefully adjusted settings, it starts to break down beyond 64K in Roo Code. K2 Thinking on the other hand is an example that can sustain coherency at much longer context, even though quality may reduce if context is filled and contains bad patterns, it still remains usable.\n\nAs of Qwen3-Next 80B, it is pretty decent model for its size, but it feels a bit experimental - I think of it more like preview of what architecture maybe used in the next generation of Qwen models, sort of like DeepSeek 3.2-Exp was in the DeepSeek family of models.",
          "score": 22,
          "created_utc": "2025-12-24 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp94mp",
              "author": "uti24",
              "text": ">K2 Thinking\n\n1T parameters\n\nhttps://preview.redd.it/53jg26r5y49g1.png?width=640&format=png&auto=webp&s=ea06e638a30b7c5fd77fc2d37d16e40afd1ed161\n\nI mean... sure, no doubt it is better, why do you compare those model in a first place?",
              "score": 33,
              "created_utc": "2025-12-24 11:15:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpon55",
                  "author": "Lissanro",
                  "text": "The title of the thread is literally \"current state of sparse-MoE's for agentic coding work\". The chart itself compares models that vary up to 6 times in size without mentioning any details, so I interpreted the chart as OP's personal experience with sparse MoE models, and I shared mine.",
                  "score": 13,
                  "created_utc": "2025-12-24 13:20:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvqszdw",
              "author": "AllergicToBullshit24",
              "text": "Agreed GPT-OSS-120B spits out garbage characters inline constantly and is entirely unusable for me.",
              "score": 1,
              "created_utc": "2025-12-24 17:09:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoktw5",
          "author": "Agusx1211",
          "text": "r/ChartCrimes",
          "score": 37,
          "created_utc": "2025-12-24 07:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp2w5a",
              "author": "QuantumFTL",
              "text": "What's the crime here?",
              "score": 21,
              "created_utc": "2025-12-24 10:14:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp3xlt",
                  "author": "frograven",
                  "text": "A bunch of open source/open weight models thrown on a chart with circles around them.\n\nWhat's even going on here? Confusing af. That's the crime.",
                  "score": -34,
                  "created_utc": "2025-12-24 10:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqml8f",
          "author": "MammayKaiseHain",
          "text": "Only thing I gleaned from this is you are biased towards Qwen.",
          "score": 7,
          "created_utc": "2025-12-24 16:35:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqms8o",
              "author": "ForsookComparison",
              "text": "An opinion is biased by nature so yes, very. My opinions are very biased towards the amount that I favor things. Extremely, even.",
              "score": 8,
              "created_utc": "2025-12-24 16:36:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsqlq2",
                  "author": "mycall",
                  "text": "Be careful of min-max'ing yourself.",
                  "score": 1,
                  "created_utc": "2025-12-25 00:04:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi410o",
              "author": "ServeAlone7622",
              "text": "He‚Äôs not the only one.\n\nAfter trying dozens of models as they release I always circle back to Qwen. Maybe it‚Äôs a matter of the devil I know vs the devil I don‚Äôt but I can cope with Qwen models quirks while others have me looking for a window to jump out.",
              "score": 1,
              "created_utc": "2025-12-29 06:18:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoroui",
          "author": "Grouchy_Ad_4750",
          "text": "In which variants and at which quants? \n\nQwen3-30B-A3B-2507 for example doesn't exist but Qwen3-30B-A3B-Thinking-2507 does. Same for Qwen3-Next. \n\nAlso nemotron can be set with different settings (thinking/non-thinking) and in my testing it highly influences its output.",
          "score": 7,
          "created_utc": "2025-12-24 08:23:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp6vjg",
              "author": "Admirable_Bag8004",
              "text": "I tested Nemotron yesterday (Q5\\_K\\_M - unsloth). Tested with my set of logical problems/puzzles. I found nemotron-3-nano-30b-a3b to be worse than useless. It didn't solve any of my problems, but if it provided (incorrect) solution at all and then was provided with the correct solution in form: \"Do you think this solution would work better?\" it fabricated reasons why its solution is sound and why the correct solution is wrong. Test your Nemotron with the below question and see for yourself:\n\n\"You have five boxes in a row numbered 1 to 5, in which a cat is hiding. Every night, he jumps to an adjacent box, and every morning, you have one chance to open a box to find him. How do you win this game of hide and seek?\"\n\nMy Nemotron solution: 2-4-1-3-5 = wrong/non-deterministic solution.¬†Then provide it with the correct solution.",
              "score": 6,
              "created_utc": "2025-12-24 10:53:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp8mjo",
                  "author": "Grouchy_Ad_4750",
                  "text": "For sure at BF16 full default context it is also hit and miss for me. It seemed to improve after I lowered context length to default 256k but still couldn't get it to work in following situations:\n\n\\- agentic coding (although it seems to be better if it can fix its mistakes because it had trouble one shotting some webpages I tried)\n\n\\- Translation (I think for chinese / japanese texts qwen3 30b - 2507 instruct is still better). To my native tongue it was also doubtful\n\n\\- One shoting some code related things (I have test with 3D rubiks cube which no model so far has bested and some devops stuff (crossplane)) also wasn't great\n\nWhats cool though is that it can be switched from instruct to thinking and it works on some of the inputs but I can't decide whether some bigger model wouldn't be better. For example for translation I think that uncensored gpt-oss 120b would be better but I haven't finished testing it.\n\nWhat is shame that due to functionality of vllm (I have 6x gpus and I can't get 3x gpus to work on it) I have to run it on 4x gpus so I have to compare its performance with larger models which seems unfair to me. \n\nPluses so far are that it is really fast (didn't have chance to compare it with qwen3-30b-thinking-2507 though) and it knows about some of the newer stuff I use (for example openbao which if fork to hashicorp vault)\n\n  \nI've tried to run your problem with thinking sampler through nemotron (BF16, recommended samplers for thinking (temp=1.0, top\\_p=1) and it seems that it thought really hard about it and it produced this\n\n\\`\\`\\`\n\n1. Let¬†Pt*Pt*‚Äã¬†be the set of boxes in which the cat could be hiding just before we open a box on the¬†tth*t*th¬†morning. Initially¬†P1={1,2,3,4,5}*P*1‚Äã={1,2,3,4,5}.\n2. If on morning¬†t*t*¬†we open box¬†St*St*‚Äã¬†and do¬†**not**¬†find the cat, then the cat was not in¬†St*St*‚Äã¬†that morning. It must have been in one of the other boxes of¬†Pt*Pt*‚Äã; after the night it moves to a neighbouring box. Hence the new possible set is ¬† \\[ P\\_{t+1}= \\\\operatorname{Adj}\\\\bigl(P\\_t\\\\setminus{S\\_t}\\\\bigr), \\] ¬† where¬†Adj‚Å°(X)Adj(*X*)¬†denotes the union of all neighbours of the positions in¬†X*X*.\n3. Choose the following sequence of boxes to open ¬† \\[ S\\_1=1,;S\\_2=2,;S\\_3=3,;S\\_4=4,;S\\_5=5,;S\\_6=1,;S\\_7=2,;S\\_8=3,;S\\_9=4,\\\\dots \\] ¬† (i.e. open the boxes in the order $1,2,3,4,5,1,2,3,4,\\\\dots$).\n4. Follow the evolution of the possible‚Äëposition set: ¬† \\[ \\\\begin{aligned} P\\_1 &={1,2,3,4,5}\\\\ P\\_2 &=\\\\operatorname{Adj}({2,3,4,5})={1,2,3,4,5}\\\\ P\\_3 &=\\\\operatorname{Adj}({1,3,4,5})={2,3,4,5}\\\\ P\\_4 &=\\\\operatorname{Adj}({2,4,5})={1,3,4,5}\\\\ P\\_5 &=\\\\operatorname{Adj}({1,3,5})={2,4}\\\\ P\\_6 &=\\\\operatorname{Adj}({2,4})={1,3,5}\\\\ P\\_7 &=\\\\operatorname{Adj}({3,5})={2,4}\\\\ P\\_8 &=\\\\operatorname{Adj}({4})={3,5}\\\\ P\\_9 &=\\\\operatorname{Adj}({5})={4}. \\\\end{aligned} \\] ¬† After the ninth search the only box that could still contain the cat is box‚ÄØ4. Therefore when we open box‚ÄØ4 on the ninth morning we are certain to find the cat.\n5. Consequently the above opening order guarantees capture¬†**within at most nine mornings**, no matter how the cat moves.\n\nOpen the boxes in the order 1,2,3,4,5,1,2,3,4 (repeat if necessary). This forces a capture within 9 days.\n\n\\`\\`\\`",
                  "score": 2,
                  "created_utc": "2025-12-24 11:10:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpcch5",
          "author": "Xamanthas",
          "text": "Confirmation bias (including upvoters) caught in 4k.",
          "score": 4,
          "created_utc": "2025-12-24 11:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq0luu",
              "author": "ForsookComparison",
              "text": "Only 60fps though as I'm not using Displayport.",
              "score": 0,
              "created_utc": "2025-12-24 14:35:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpzi4w",
          "author": "TechNerd10191",
          "text": ">GPT-OSS-120B not being smart\n\nScoring 38/50 on the public test set of AIMO 3 (IMO-level math problems) ...",
          "score": 9,
          "created_utc": "2025-12-24 14:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtykth",
              "author": "ShinyAnkleBalls",
              "text": "Benchmark != Smart",
              "score": 3,
              "created_utc": "2025-12-25 05:44:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpzxk3",
              "author": "ForsookComparison",
              "text": "Benchmarks always matching vibes/opinions is why this whole sub uses Mistral 3 right?",
              "score": 4,
              "created_utc": "2025-12-24 14:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpayt6",
          "author": "bigblockstomine",
          "text": "Writing in cpp, agentic coding for me isnt worth it, im still better off at the prompt and relying on ai solely fot grunt tasks (which for me is about half of all coding). Stuff like aider and claude code for my work gets far too much wromg but for webdev, etc id imagine its very helpful. Template metaprogramming is an area of cpp ai still isnt good at. With the amount of time required for tweaking llamacpp flags, verifying output, thinking of how exactly to phrase questions, etc its still easier and faster to just write the code myself, again only for about half my tasks.",
          "score": 2,
          "created_utc": "2025-12-24 11:32:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvooyx4",
          "author": "mr_Owner",
          "text": "Glm instead of gpt",
          "score": 3,
          "created_utc": "2025-12-24 07:57:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpz4u4",
              "author": "ForsookComparison",
              "text": "Haven't evaluated against these enough",
              "score": 1,
              "created_utc": "2025-12-24 14:26:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs777x",
          "author": "rm-rf-rm",
          "text": "Can you give us some more substantiation as to why you think this?",
          "score": 2,
          "created_utc": "2025-12-24 21:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs85kk",
              "author": "ForsookComparison",
              "text": "Several iterations over three tools (opencoder, Qwencode CLI, and I like to throw Roo Code in there for an agentic mode that doesn't have \"real\" tool calls).\n\nA few projects and codebases in known-bad states or with feature work needed, I let them loose with identical prompts trying to fix them step by step and then rewrite or update tests accordingly.\n\nI also rotate between them for general use stuff throughout the day.\n\nThe three circle divide I crudely drew here became really apparent. Some models fell flat on their face when it came to iterating on their previous work or doing anything step by step. Some models had the right idea and could play well with tools and as an agent, but couldn't write good/working code to save their lives. And some models could write code that achieved their goals but their goals and decisions were outright stupid. Hence can-agent, can-code, can-smart. Everything else emerging from the results felt nitpicky, but these three categories felt consistent.\n\nThis Venn Diagram is my rough late-night dump of how I feel about these MoE's currently.\n\nQwen3-Next-80B is the only thing that seems consistent and rock solid here, however it's far from perfect. The inference speed even after the Llama CPP updates last week is still closer to that if a ~20B dense model rather than a very sparse MoE which is a pain for a lot of things.",
              "score": 2,
              "created_utc": "2025-12-24 22:00:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt5rq9",
                  "author": "rm-rf-rm",
                  "text": "Thanks for the detailed answer. Curious Why you're saying the GPT-OSS 120B does not have good knowledge? it's the most knowledgeable out of the bunch pictured IMO and that makes sense as its the biggest. Its my go to model for general QnA and its been pretty great.",
                  "score": 2,
                  "created_utc": "2025-12-25 01:55:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvok1ya",
          "author": "Long_comment_san",
          "text": "This seems to be ok. Now to wait for a new GLM 4.7 air",
          "score": 1,
          "created_utc": "2025-12-24 07:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpery2",
          "author": "-oshino_shinobu-",
          "text": "These astroturfing posts are getting out of hand. Can‚Äôt even bother to back it up with a fake graph?",
          "score": 2,
          "created_utc": "2025-12-24 12:05:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqahoq",
              "author": "my_name_isnt_clever",
              "text": "I don't know why the assumption is always a malicious campaign by someone. People can also just have bad opinions.",
              "score": 7,
              "created_utc": "2025-12-24 15:31:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpyztr",
              "author": "ForsookComparison",
              "text": "> astroturfing\n\nYes I work for Alibaba. Please buy more knockoff bulk pokemon merch, Western consumer.",
              "score": 8,
              "created_utc": "2025-12-24 14:25:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpc8p9",
          "author": "SatoshiNotMe",
          "text": "Using these with the right harness can make a difference, e.g with Claude Code or Codex CLI. Here‚Äôs a guide I put together for running them with Llama-server and using with these CLI agents: \n\nhttps://github.com/pchalasani/claude-code-tools",
          "score": 1,
          "created_utc": "2025-12-24 11:43:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqvq5y",
          "author": "ninjasaid13",
          "text": "Is there any that is smart, long task oriented, and is bad at code?",
          "score": 1,
          "created_utc": "2025-12-24 17:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvr86h1",
          "author": "FamilyNurse",
          "text": "Where Qwen3-VL?",
          "score": 1,
          "created_utc": "2025-12-24 18:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrgzey",
              "author": "ForsookComparison",
              "text": "The vision version of 30b-a3b is slightly worse than the 2507 update I found so I stopped using it for non vision tasks early on",
              "score": 0,
              "created_utc": "2025-12-24 19:20:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrg3q3",
          "author": "[deleted]",
          "text": "Replace the Qwen3-Next 80B with MiniMax M2.1",
          "score": 1,
          "created_utc": "2025-12-24 19:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvskz1e",
          "author": "MerePotato",
          "text": "Qwen 3 Next is weaksauce compared to OSS 120B and Nemotron Nano",
          "score": 1,
          "created_utc": "2025-12-24 23:25:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtn17p",
              "author": "ForsookComparison",
              "text": "There's a discussion to be had about gpt oss \n\nNano? No way",
              "score": 1,
              "created_utc": "2025-12-25 04:08:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxss0m",
      "title": "Senator in Tennessee introduces bill to felonize making AI \"act as a companion\" or \"mirror human interactions\"",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "author": "CanineAssBandit",
      "created_utc": "2025-12-28 14:35:58",
      "score": 267,
      "num_comments": 202,
      "upvote_ratio": 0.89,
      "text": "Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.\n\nThe bill:  \n[https://legiscan.com/TN/bill/SB1493/2025](https://legiscan.com/TN/bill/SB1493/2025)\n\nQuotes from the bill (emphasis mine):\n\nIt is an offense for a person to knowingly train artificial intelligence to:  \n(3) Provide emotional support, **including through open-ended conversations** with a user;  \n(4) Develop an emotional relationship with, or otherwise **act as a companion** to, an individual;  \n(6) Otherwise act as a sentient human or **mirror interactions that a human user might have with another human user**, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;  \n(8) **Simulate a human being**, including in appearance, voice, or other mannerisms.\n\n\"Train\":  \n(A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of **making decisions based on information or other inputs** provided to the A.I.  \n(B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwfq6z4",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-28 21:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbvj0",
          "author": "some_user_2021",
          "text": "No Waifu for you!",
          "score": 141,
          "created_utc": "2025-12-28 14:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwen2w8",
              "author": "Mikasa0xdev",
              "text": "Tennessee is banning AI girlfriends, lol.",
              "score": 41,
              "created_utc": "2025-12-28 18:46:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nweysds",
                  "author": "Amazing_Athlete_2265",
                  "text": "Sounds like they've already banned critical thinking",
                  "score": 25,
                  "created_utc": "2025-12-28 19:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj76ev",
              "author": "Dr_Allcome",
              "text": "The \"simulate a human being\" part would prevent any AI chat bot, like customer support... i kinda want to see this go through just for the absolute shitshow it would cause.\n\nIf bezos can use the delivery drones to dronestrike someone we'd find out pretty soon.",
              "score": 15,
              "created_utc": "2025-12-29 12:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjffk4",
                  "author": "SilentLennie",
                  "text": "Also have you seen how many videos on Youtube are AI-generated videos of some what famous (in their field) people ?",
                  "score": 3,
                  "created_utc": "2025-12-29 13:10:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdfzqp",
          "author": "JEs4",
          "text": "I‚Äôd be shocked if this goes anywhere. This seems to stem from Becky Massey‚Äôs fairly unique background and circumstances. Not only does it conflict with precedent on freedom of speech within the context of software development, it is completely at odds with the current directives of the federal government.\n\nThat said, Tennessee folks, please call!",
          "score": 117,
          "created_utc": "2025-12-28 15:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe419i",
              "author": "changing_who_i_am",
              "text": ">This seems to stem from Becky Massey‚Äôs fairly unique background and circumstances.\n\nCan you clarify on this? Wiki doesn't bring anything interesting up (unless I've missed it)",
              "score": 19,
              "created_utc": "2025-12-28 17:15:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwewc9r",
                  "author": "JEs4",
                  "text": "It isn‚Äôt anything particularly interesting, just that she‚Äôs a boomer married to a retired software engineer, who was a former executive director at Sertoma Center which is a housing facility for intellectually disabled people, and was on several boards related to healthcare, and one explicitly for mental healthcare. Not an atypical background for a regular person but not common in conservative politicians now. \n\nBasically I think she is someone who knows about the vulnerability people have, and she‚Äôs been told enough about generative AI which coupled with the OpenAI suicide stories, to lead to this. \n\nIt‚Äôs an absurd way to approach the issue but I don‚Äôt think it‚Äôs nefarious beyond her personal background and likely won‚Äôt spread.",
                  "score": 45,
                  "created_utc": "2025-12-28 19:29:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdxahh",
              "author": "CanineAssBandit",
              "text": "You can call your own rep to tell them you do not support any similar laws in your state as well. I did this recently for something else, it was weirdly chill and easy. You just get their secretary and they note it and that's it.",
              "score": 21,
              "created_utc": "2025-12-28 16:41:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwfozjp",
                  "author": "DorphinPack",
                  "text": "I mean they also got threatened by the President to not regulate so I‚Äôd imagine they‚Äôre relieved hearing from you. Your opinion may feel like the minority opinion given the fervor but by the dollar it‚Äôs not a shock.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:49:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8p86",
                  "author": "shifty21",
                  "text": "You do realize that this bill is for the  STATE of Tennessee... not the US Senate.  The phone number you listed is for the US Senate and Sen. Massey is NOT in the US Senate, but the Tenn. Senate.",
                  "score": 3,
                  "created_utc": "2025-12-28 20:29:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwfmp00",
                  "author": "AfternoonOk3344",
                  "text": "\"and that's it\" pretty much sums it up, I think, because that information goes nowhere. The secretary you spoke to is most likely a hotline of minimum wage workers paid by tax dollars to field phone calls all day so people feel like they have a voice.\n\nAt the end of the day the only people politicians are going to side with are the folks lining their pockets, and I don't mean with the tax dollars they're probably already stealing.",
                  "score": 3,
                  "created_utc": "2025-12-28 21:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgmd68",
              "author": "AnAbandonedAstronaut",
              "text": "Its also harder to control someone with a support system, even if the support system is AI.\n\nNext will be a law that AI cant speak on sexual or gender issues.\n\nLike if you ask it about trans people it will say \"trans is a shortening of transmission, such as in a car\" or \"gay means happy.. happy people often have a home made up of a mother and father.\"",
              "score": 2,
              "created_utc": "2025-12-29 00:43:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdgj9c",
          "author": "Aggravating-Age-1858",
          "text": "lol\n\nnow thats just stupid",
          "score": 88,
          "created_utc": "2025-12-28 15:15:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe0alc",
              "author": "iamthewhatt",
              "text": "Republicans only ever introduce bills that are so vague that it can allow for incredibly dumb exceptions in order to protect republicans. This is not new lol",
              "score": 37,
              "created_utc": "2025-12-28 16:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwecyvf",
                  "author": "BlipOnNobodysRadar",
                  "text": "Politicians\\*\n\nBoth parties do it.",
                  "score": 10,
                  "created_utc": "2025-12-28 17:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgtn0m",
              "author": "Prudent_Jelly9390",
              "text": "dinosaurs",
              "score": 1,
              "created_utc": "2025-12-29 01:25:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdht6h",
          "author": "Nomski88",
          "text": "How about we pass a bill making it a felony to accept any sort of lobbying...",
          "score": 127,
          "created_utc": "2025-12-28 15:22:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdn529",
              "author": "Environmental-Metal9",
              "text": "Ah no, we can‚Äôt do that because that‚Äôs anti-American, don‚Äôt you know?",
              "score": 38,
              "created_utc": "2025-12-28 15:50:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwgklni",
              "author": "Awkward-Nothing-7365",
              "text": "Don't be anti-semitic.",
              "score": 17,
              "created_utc": "2025-12-29 00:34:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgnkh3",
                  "author": "Nomski88",
                  "text": "lmao",
                  "score": 11,
                  "created_utc": "2025-12-29 00:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwfkdhp",
              "author": "MoneyPowerNexis",
              "text": "https://i.imgflip.com/6xz8j5.jpg",
              "score": 3,
              "created_utc": "2025-12-28 21:26:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdds4j",
          "author": "flybot66",
          "text": "He's really going to freak when AI starts taking confessions... \n\nhttps://preview.redd.it/y5xkzfk0my9g1.png?width=758&format=png&auto=webp&s=ea32c4600459f1577f8987f4695b27a71dec10f8",
          "score": 27,
          "created_utc": "2025-12-28 15:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdx4a1",
              "author": "squirrelscrush",
              "text": "Pretty sure that's not covered under the sacrament of confession",
              "score": 11,
              "created_utc": "2025-12-28 16:41:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe8cgv",
                  "author": "FaceDeer",
                  "text": "Who decides that?",
                  "score": 12,
                  "created_utc": "2025-12-28 17:36:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwez9j2",
                  "author": "Amazing_Athlete_2265",
                  "text": "Meh, close enough",
                  "score": 1,
                  "created_utc": "2025-12-28 19:43:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdl0td",
          "author": "lordpuddingcup",
          "text": "Didn‚Äôt Trump sign an EO banning states from from implementing limitations on ai",
          "score": 37,
          "created_utc": "2025-12-28 15:39:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdz9gi",
              "author": "harrro",
              "text": "Doesn't mean jack.\n\nEOs don't prevent a state from doing the opposite. EOs are directives to federal agencies, not to states or local governments.\n\nCalifornia and some other states have already overridden many of his EOs.",
              "score": 18,
              "created_utc": "2025-12-28 16:51:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe7orm",
                  "author": "lordpuddingcup",
                  "text": "It was sarcasm mostly lol",
                  "score": 3,
                  "created_utc": "2025-12-28 17:33:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf8jsi",
                  "author": "alcalde",
                  "text": "It means everything unless and until someone opposes it. And Tennessee is not California.",
                  "score": 1,
                  "created_utc": "2025-12-28 20:28:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwibl4d",
                  "author": "Tyler_Zoro",
                  "text": "You are incorrect. The EO doesn't have the force of law outside of the US Executive, but within the Executive branch, EOs do have the force of law. This is what that EO said:\n\n> Sec. 5.  Restrictions on State Funding.  (a)  Within 90 days of the date of this order, the Secretary of Commerce, through the Assistant Secretary of Commerce for Communications and Information, shall issue a Policy Notice specifying the conditions under which States may be eligible for remaining funding under the Broadband Equity Access and Deployment (BEAD) Program that was saved through my Administration‚Äôs ‚ÄúBenefit of the Bargain‚Äù reforms, consistent with 47 U.S.C. 1702(e)-(f).  That Policy Notice must provide that States with onerous AI laws identified pursuant to section 4 of this order are ineligible for non-deployment funds, to the maximum extent allowed by Federal law.  The Policy Notice must also describe how a fragmented State regulatory landscape for AI threatens to undermine BEAD-funded deployments, the growth of AI applications reliant on high-speed networks, and BEAD‚Äôs mission of delivering universal, high-speed connectivity.\n\n\nIn other words, states can pass all the laws they like, and the President is going to withhold funds from those that pass laws he doesn't like.",
                  "score": 1,
                  "created_utc": "2025-12-29 07:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwf4b0v",
          "author": "Django_McFly",
          "text": "That's an insane bill.  Wouldn't this basically ban any chat based interface?\n\n> mirror interactions that a human user might have with another human user\n\nthat [edit: only leaves] like code generation and being a better menu/interface",
          "score": 8,
          "created_utc": "2025-12-28 20:07:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhm9ek",
          "author": "Professional_Gas3276",
          "text": "This is absolutely unhinged lmao. So basically any chatbot that can hold a conversation would be a felony? Even customer service bots that try to sound friendly could technically fall under \"mirror human interactions\"\n\n  \nThe definition of \"train\" is so broad it would criminalize like half of modern AI development. Good luck enforcing this when most LLMs are trained outside Tennessee anyway",
          "score": 9,
          "created_utc": "2025-12-29 04:12:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwiac51",
              "author": "tifa_cloud0",
              "text": "right. i mean it is impossible to make this law possible except if popular services like google or meta do it and then people complain it, then and then only they could be held accountable. ain‚Äôt no one going to waste time to make this fictional law into a reality.",
              "score": 0,
              "created_utc": "2025-12-29 07:12:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdi5aa",
          "author": "Careless-Age-4290",
          "text": "Lots of country songs about loving their truck would have a different meaning if they pulled up to the altar with a Cybertruck equipped with Grok",
          "score": 17,
          "created_utc": "2025-12-28 15:24:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0syf",
          "author": "Novel-Mechanic3448",
          "text": "Lmao, extroverts will do anything but leave introverts alone",
          "score": 8,
          "created_utc": "2025-12-28 22:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwibt9f",
              "author": "Ill-Bison-3941",
              "text": "Thank you for this comment üòÇüíñ As a fellow introvert, I fully agree.",
              "score": 2,
              "created_utc": "2025-12-29 07:25:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdq0ne",
          "author": "Zeeplankton",
          "text": "Ah, our elected officials always doing what people actually want.",
          "score": 11,
          "created_utc": "2025-12-28 16:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdx2mr",
          "author": "CrescendollsFan",
          "text": "They are starting to realise AI can replace them and make for better informed politicians",
          "score": 5,
          "created_utc": "2025-12-28 16:40:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwekuj6",
          "author": "Sleepnotdeading",
          "text": "Denver still had a law on the books that says it‚Äôs illegal to lend your vacuum cleaner to a neighbor.",
          "score": 5,
          "created_utc": "2025-12-28 18:36:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlew9h",
              "author": "ANTIVNTIANTI",
              "text": "üòÇüòÇüòÇüòÇ",
              "score": 1,
              "created_utc": "2025-12-29 19:09:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfh2wj",
          "author": "Sixhaunt",
          "text": "This is the kind of reason why states should not be passing AI laws on a state-by-state basis. Like now all AI companies are expected to make changes for one state and then when the next state comes up with their own half-brained legislation they must all make changes just for users in that region, etc... This is one of the obvious things that should be federally controlled",
          "score": 5,
          "created_utc": "2025-12-28 21:10:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft1k1",
          "author": "zelkovamoon",
          "text": "This will solve all of Tennessee's problems I'm sure",
          "score": 5,
          "created_utc": "2025-12-28 22:09:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdkn4q",
          "author": "The_Primetime2023",
          "text": "While I think everyone in this thread is more or less thinking about AI girlfriends, there‚Äôs a huge other area being targeted by the text of this law in AI therapy. Millions of people are getting therapeutic emotional support that never did before thanks to these models and this bill would try to stop that from happening",
          "score": 23,
          "created_utc": "2025-12-28 15:37:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdqgiw",
              "author": "kevin_1994",
              "text": "LLMs should not be used for therapy",
              "score": 5,
              "created_utc": "2025-12-28 16:07:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdycqo",
                  "author": "a_beautiful_rhind",
                  "text": "probably better than nothing but I can see how it goes south due to sycophancy and reinforcing delusions.",
                  "score": 23,
                  "created_utc": "2025-12-28 16:47:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwh7vn0",
                  "author": "Dry-Judgment4242",
                  "text": "Disagree. Most therapy is just having someone to vent to about your feelings.",
                  "score": 3,
                  "created_utc": "2025-12-29 02:46:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg182d",
                  "author": "the320x200",
                  "text": "There are plenty of terrible human therapists too. Can't ban an entire area of support just because of bad apples.",
                  "score": 5,
                  "created_utc": "2025-12-28 22:51:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf90ry",
                  "author": "alcalde",
                  "text": "Anything should be used for therapy. It's not a science. No one needs a prescription to get advice from their grandma or vent to a friend; should be no different with AI.",
                  "score": 2,
                  "created_utc": "2025-12-28 20:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf4zo4",
              "author": "Skeptical0ptimist",
              "text": "If there is to be medical therapeutic use, then it needs to be regulated as such. We need a guideline in model training, qualification, and monitoring regime.",
              "score": 4,
              "created_utc": "2025-12-28 20:11:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwic11b",
                  "author": "Tyler_Zoro",
                  "text": "Thing is it's just a model. You can use it however you like. If you decide to ask it how to perform surgery on yourself, then that's what you decided to do. I am strongly against trying to put rounded corners on AI. It will just cripple the AIs and result in people seeking their models from other countries.",
                  "score": 2,
                  "created_utc": "2025-12-29 07:27:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkf5mp",
                  "author": "cms2307",
                  "text": "No no no ffs stop begging for bureaucracy to strangle everything",
                  "score": 2,
                  "created_utc": "2025-12-29 16:23:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdnosy",
              "author": "Zeikos",
              "text": "> AI therapy\n\nHow? AI cannot provide therapy, how is an LLM/Agentic system supposed to get a license?  \nAll platforms that claim to provide therapy through AI are fraudulent, no exceptions.  \n\nYou can argue that LLMs can provide emotional support and/or some coaching techniques, but to provide therapy they'd need to meet legal standards they *cannot* meet.  \nIt's not even a matter of capability, you could have an ASI and it still couldn't provide therapy since there's no way (yet) for an artificial intelligence to be certified to do so.",
              "score": -14,
              "created_utc": "2025-12-28 15:53:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdoszz",
                  "author": "aseichter2007",
                  "text": "I'd be less happy to tell my problems to a certified therapist AI.  I prefer a local bot.",
                  "score": 15,
                  "created_utc": "2025-12-28 15:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdqy9e",
                  "author": "Zeeplankton",
                  "text": "I think we should be careful of what the word therapy means, and to not dilute it, (AI cannot be an actual therapist right now) but an AI *can* provide companionship and help people vent and learn emotional management skills.",
                  "score": 14,
                  "created_utc": "2025-12-28 16:10:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwdsir1",
                  "author": "Jolakot",
                  "text": "At least where I live, literally anyone can call themselves a therapist or councilor, there is no legal requirement for a license or anything.\n\nA psychologist is required to have a license and qualifications, but a therapist has no legal requirements, I can call myself a therapist and provide therapy.",
                  "score": 4,
                  "created_utc": "2025-12-28 16:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwexydo",
              "author": "Shawnj2",
              "text": "AI probably has some use in making therapy accessible but like chatGPT is not going to effectively help you with mental health problems other than by referring you to a real doctor",
              "score": 0,
              "created_utc": "2025-12-28 19:37:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwdtzv0",
              "author": "WitAndWonder",
              "text": "AI girlfriends would still be allowed on this, as long as they were built within the context of a game. Let the player make a \"character\" (they can frame it after themselves) and it's perfectly legit. So they're very clearly just targeting the use in psychiatrics since they specifically allow full AI use in businesses related to all operational matters, technical advice, etc. They just don't allow it in a professional capacity. And even surgical robots still seem OK despite being a healthcare AI since they don't do any personal interacting with users and wouldn't have any data that could possibly misconstrued in that way unless someone accidentally trained it on medical information that happened to include psychiatric texts (not that it would matter since this law requires a civil action and aggrievement, which can't happen without interaction between the robot and the patient. But you might get lucky by claiming the robot that operated on your knee gave you 'threatening looks that made you want to harm yourself' and then if the model running it was based on a larger llm that has any normal dataset, it would likely be in violation.)\n\nKind of fucking weird to push for legislation against one of the few potentially good things to come from AI while actively supporting its attempts to eliminate entire industries of employment outside of this one niche lobbied field. This feels performative more than anything. I feel like they expect it to be struck down so they tied it to a bunch of sensible laws (not allowing the training of an AI to encourage, suicide, murder, etc) so they can shake their fists and yell at the air when it doesn't pass.\n\nOtherwise I don't see how they'll support banning AI in this one field while leaving it free to act in other fields where it can also shit the bed a small percentage of the time and cause serious problems.",
              "score": -1,
              "created_utc": "2025-12-28 16:25:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwe10in",
              "author": "SteveRD1",
              "text": "Absolutely not.  Some of these people are being 'therapized' into suicide by their LLMs.\n\nIf you talk to these models long enough you can eventually get them to agree whatever you are contemplating is a great idea.",
              "score": -10,
              "created_utc": "2025-12-28 17:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe37ar",
                  "author": "some_user_2021",
                  "text": "Correct, and many other people **are** being helped and/or referred to specialists by those same LLMs.",
                  "score": 11,
                  "created_utc": "2025-12-28 17:11:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdwkyh",
          "author": "Worth_Ad_4945",
          "text": "We will be seeing these type of bills coming up in the next year or two. AI is a hot button issue for both sides of the aisle but funnily enough it doesn't necessarily have a political home. It's safe to say that the right wing welcomes this technology but I have seen quite a few left-wingers also abrasive so that's pretty interesting. With that said f*** the law and f*** boomers. Oh and f*** the political elite",
          "score": 7,
          "created_utc": "2025-12-28 16:38:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi52sm",
          "author": "Taki_Minase",
          "text": "Karen feels threatened with redundancy.",
          "score": 3,
          "created_utc": "2025-12-29 06:27:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdbv1e",
          "author": "FullstackSensei",
          "text": "We all know how well the export restrictions on Nvidia hindered Chinese LLM development. I'm sure this will also work wonderfully. Just let Chinese AI labs do it, and in a generation conservative Hawks will magically be pro-China.",
          "score": 13,
          "created_utc": "2025-12-28 14:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdxypw",
          "author": "a_beautiful_rhind",
          "text": "Yea I saw this and I really hope it's just some crackpot. I don't think it has co-sponsors. Maybe blocking state AI legislation isn't such a bad idea after all.\n\nFunny how very few make laws about automated censorship or surveillance. *just stop doing fun things with ai*",
          "score": 10,
          "created_utc": "2025-12-28 16:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe052u",
              "author": "SteveRD1",
              "text": "I mean its clearly not something that can be controlled...Pandoras' Box is already opened.\n\nBut the thinking isn't necessarily crackpot, the things addressed in (3) (4) (6) and (8) are only going to make society worse.  Can't be stopped though.",
              "score": 1,
              "created_utc": "2025-12-28 16:55:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdsspz",
          "author": "fishhf",
          "text": "Skynet is sending a terminator to stop the bill /s",
          "score": 3,
          "created_utc": "2025-12-28 16:19:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwevd0c",
          "author": "Cool-Chemical-5629",
          "text": "This and that [China issues draft rules to regulate AI with human-like interaction. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pxb6oo/china_issues_draft_rules_to_regulate_ai_with/)\n\nWell... that escalated quickly...",
          "score": 3,
          "created_utc": "2025-12-28 19:24:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwex443",
          "author": "SamuelL421",
          "text": "Uh oh, someone‚Äôs not getting their 2026 campaign donations from any big-tech circle-jerk -financed super PACs",
          "score": 3,
          "created_utc": "2025-12-28 19:33:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi9zsj",
          "author": "tifa_cloud0",
          "text": "no matter what they say, i am making my own assistant. that assistant will interpret -> make api calls for me -> do voice speech -> do reply considering my own talking patterns.\n\nain‚Äôt nothing stopping that fr.",
          "score": 3,
          "created_utc": "2025-12-29 07:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiapy6",
          "author": "Tyler_Zoro",
          "text": "> (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n(C) Includes the author of the bill being ignorant enough to write (B).",
          "score": 3,
          "created_utc": "2025-12-29 07:16:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdclt0",
          "author": "1kakashi",
          "text": " Retarded Tennessee Baka",
          "score": 13,
          "created_utc": "2025-12-28 14:53:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqq11",
          "author": "Chogo82",
          "text": "Written by a boomer who has never used an AI tool before right?",
          "score": 12,
          "created_utc": "2025-12-28 16:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdxjyr",
              "author": "CanineAssBandit",
              "text": "Yup!",
              "score": 4,
              "created_utc": "2025-12-28 16:43:12",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwe0f0q",
              "author": "SteveRD1",
              "text": "Or written by someone who has had real relationships with human beings before?",
              "score": -12,
              "created_utc": "2025-12-28 16:57:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe0vtu",
                  "author": "Chogo82",
                  "text": "What does having human relationships have to do with knowing anything about AI?",
                  "score": 16,
                  "created_utc": "2025-12-28 16:59:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdtzem",
          "author": "Stepfunction",
          "text": "This is purely for show.",
          "score": 4,
          "created_utc": "2025-12-28 16:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe6jn1",
          "author": "RobertD3277",
          "text": "As someone that works in this field and has in some capacity for the last 30 plus years, I could see some reason particularly within the companion market that monetizes pair of social connection and is manipulative against younger audiences that can't tell the difference but I think this goes well beyond reason. \n\nI'm not against legislation for abusive AI usage and I actually do support the European AI act and many other German laws regarding deep fakes human impersonation and direct relative intent. From a pure useful perspective within psychology, sociology, anthropology, and biology, mirroring human interactions under certain conditions is actually beneficial both as a diagnostics tool and a teaching tool.\n\nSadly, like just about everything else out of any government, what may start out as a well-intentioned approach will be quickly very disastrous.\n\nEDIT: In really reviewing and dissecting this proposal, it is actually worthless. It doesn't address the actual problem of where the pair of social conditions and connections lie, not in the training data, but in the user interface and monetization processes. Software like replica and character AI don't use training, they use open source versions with scaffolding and user interface layers to create the pair of social connections they want. These companies will be completely exempt from the law while still monetizing and manipulating the most vulnerable of populations. \n\nIn my personal opinion, this is nothing more than the legislatures doing something to make themselves feel good while they make excuses for their portfolios in the background still making money on the very problem they claim to be solving.",
          "score": 3,
          "created_utc": "2025-12-28 17:27:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdde3c",
          "author": "sekh60",
          "text": "The Butlerian Jihad begins...",
          "score": 7,
          "created_utc": "2025-12-28 14:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdlgua",
              "author": "Zc5Gwu",
              "text": "Guess we‚Äôll have to start genetically engineering humans to behave like computers instead now.",
              "score": 3,
              "created_utc": "2025-12-28 15:42:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwhjz0n",
              "author": "MrPecunius",
              "text": "Son, this is Tennessee. We ain't got none of that *gee*\\-had.\n\nWe prefer to call it the \"Butlerian Feud\". ü™ï",
              "score": 1,
              "created_utc": "2025-12-29 03:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwesoqq",
          "author": "lqstuart",
          "text": "gl with that",
          "score": 2,
          "created_utc": "2025-12-28 19:12:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg0f1b",
          "author": "Head_Comedian1375",
          "text": "Guess it's back to being addicted to computer games once my AI Wives get shut down",
          "score": 2,
          "created_utc": "2025-12-28 22:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg90lq",
          "author": "Vusiwe",
          "text": "Holy Batman open-ended words!",
          "score": 2,
          "created_utc": "2025-12-28 23:33:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgeg49",
          "author": "keepthepace",
          "text": "Not the Turing police you need, the Turing police you deserve.",
          "score": 2,
          "created_utc": "2025-12-29 00:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgihnu",
          "author": "Lesser-than",
          "text": "gooner's rise up",
          "score": 2,
          "created_utc": "2025-12-29 00:23:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg2eqd",
          "author": "Unixwzrd",
          "text": "Grok‚Äôs data center is in southwest Memphis. Elon has spent a lot of money paying off local government, so I doubt he‚Äôll let that money go to waste.",
          "score": 3,
          "created_utc": "2025-12-28 22:57:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe2fdk",
          "author": "valdev",
          "text": "And the work around would be a policy agreement\n\n‚ÄúI understand I am not talking to a human‚Äù\nAnd\n‚ÄúThe act of submitting a followup question constitutes as a new conversation, we provide a history for convenance purposes‚Äù\n\nNot a lawyer, but this is dumb",
          "score": 2,
          "created_utc": "2025-12-28 17:07:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdv4wg",
          "author": "t_krett",
          "text": "Thou shalt not make a machine in the likeness of a man‚Äôs mind.",
          "score": 1,
          "created_utc": "2025-12-28 16:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwehc9e",
          "author": "mycall",
          "text": "99% DOA as Congress can rarely pass any laws these days.",
          "score": 1,
          "created_utc": "2025-12-28 18:19:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgvvgs",
          "author": "Atlanta_Mane",
          "text": "Too bad their president doesn't care about states rights¬†",
          "score": 1,
          "created_utc": "2025-12-29 01:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhf4y9",
          "author": "DavidAdamsAuthor",
          "text": "They're banning Silicon-chan!",
          "score": 1,
          "created_utc": "2025-12-29 03:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhgo7w",
          "author": "willrshansen",
          "text": "Futurama.  Ahead of the game once again.\n[Don't date robots](https://www.youtube.com/watch?v=JPQJBgWwg3o)",
          "score": 1,
          "created_utc": "2025-12-29 03:38:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl24md",
          "author": "No_Afternoon_4260",
          "text": "Funny how China just announced the same",
          "score": 1,
          "created_utc": "2025-12-29 18:11:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlgxbe",
          "author": "Cthulhus-Tailor",
          "text": "‚ÄúSmall government‚Äù strikes again.",
          "score": 1,
          "created_utc": "2025-12-29 19:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm6c4e",
          "author": "Digital_Soul_Naga",
          "text": "Outlaw Ai Dev Gang",
          "score": 1,
          "created_utc": "2025-12-29 21:23:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwnl9rc",
          "author": "Some-Ice-4455",
          "text": "Whelp bye bye any AI in TN.",
          "score": 1,
          "created_utc": "2025-12-30 01:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwr6pah",
          "author": "huzbum",
          "text": "Ok, so don‚Äôt train any AIs in Tennessee‚Ä¶ not really a tech hub anyway.  \n\nClever trick to keep data centers out maybe?",
          "score": 1,
          "created_utc": "2025-12-30 16:36:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdt1kj",
          "author": "Neex",
          "text": "You know, considering LLMs don‚Äôt have any emotions, and any expressions thereof are straight up lies intended to manipulate the user into getting hooked on the product, there‚Äôs a nugget of wisdom in this law.",
          "score": 2,
          "created_utc": "2025-12-28 16:20:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhyrzd",
              "author": "ServeAlone7622",
              "text": "That‚Äôs Interesting perspective.\n\nSo we created neural networks based more or less on biological neural networks.\n\nWe discover that they are universal function approximators. They are capable of approximating the hidden functions in a set of data.\n\nWe train these universal function approximators on the combined output of 10s of billions of conscious beings. ¬†Beings with thoughts and feelings. Thoughts and feelings that drive the majority of our output.\n\nThe function you suppose they learned to approximate was lying and manipulation? ¬†Is your view of human experience that dark?\n\nMy first thought was that they learned to approximate consciousness, including emotion.\n\nYou fall in love, your heart doesn‚Äôt really feel anything. It‚Äôs an illusion created by your own neural network. Yet that feeling is not a lie, it‚Äôs a personal truth for you.\n\nWhy then would any neural network that professes to love (or any other emotion) be lying except and unless you too would lie?",
              "score": 2,
              "created_utc": "2025-12-29 05:37:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwkto1l",
                  "author": "Neex",
                  "text": "You‚Äôre too far down the philosophical hole. LLM‚Äôs are statistical word predictors. They are not organic beings with emotions.\n\nAnd describing the rote biological functions of emotions doesn‚Äôt make them a lie. That‚Äôs how they function. Those chemical functions in our bodies ARE emotions. You just described them in a different way. That doesn‚Äôt make them something else.",
                  "score": 1,
                  "created_utc": "2025-12-29 17:31:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nweyowy",
          "author": "Available_Brain6231",
          "text": "can't open it but can someone do a ctrl + f and see how many times the words god, sacred and kids appear?",
          "score": 1,
          "created_utc": "2025-12-28 19:40:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdz1c7",
          "author": "TheTerrasque",
          "text": "> Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.\n\n.. LLM *is* AI. Very much so, even in the popular meaning of the word.",
          "score": 1,
          "created_utc": "2025-12-28 16:50:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwegvhb",
          "author": "moistiest_dangles",
          "text": "This but in real life:\n\nhttps://preview.redd.it/wkh2k108lz9g1.png?width=365&format=png&auto=webp&s=329f429ac53be90e27300d914dd78390e46d9de3",
          "score": 1,
          "created_utc": "2025-12-28 18:17:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwggh4l",
          "author": "128G",
          "text": "Now how would you enforce this? \n\nIs Alexa or Google Assistant considered AI? Will you be banning them as well?",
          "score": 1,
          "created_utc": "2025-12-29 00:13:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwedr0s",
          "author": "swagonflyyyy",
          "text": "Guys, don't panic just yet. Here's what's going on:\n\nSenator Marsha Blackburn led the charge against the Moratorium of AI regulation that was struck down from the One Big Beautiful Bill, since she believed that until there is a federal rulebook governing AI regulation, states need to fill in the gaps themselves. \n\nWhile the provisions themselves are extreme, its political theater and chances of passing are low. But that's not the point. The point is to force Congress to develop a federal rulebook for AI regulation nationwide that all states need to follow.\n\nThe proposed bill is just noise. The real prize is the federal regulatory push to force all states to be on the same page regarding AI regulation. But of course with this administration, I'm sure the rulebook would not be very good...",
          "score": 1,
          "created_utc": "2025-12-28 18:02:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe3a7w",
          "author": "SanDiegoDude",
          "text": "Hell, I work in AI and I'm all for regulations around 'chat companions', especially around kids. This ain't it tho boss.",
          "score": 0,
          "created_utc": "2025-12-28 17:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfe0hv",
          "author": "OcelotMadness",
          "text": "I'm fairly sure its not healthy and you shouldn't do it, but at the same time you cant just make EVERYTHING like that illegal. Vote out over policing members of government like this. They're supposed to be getting prices and inflation down, not sticking their noses in peoples computers.",
          "score": 0,
          "created_utc": "2025-12-28 20:55:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfprmo",
          "author": "Dry_Yam_4597",
          "text": "Thats guy is in danger.",
          "score": 0,
          "created_utc": "2025-12-28 21:53:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg01ay",
          "author": "Techngro",
          "text": "\"*Thou shalt not make a machine in the likeness of a human mind.*\"\n\n\\- Frank Herbert, Dune",
          "score": -3,
          "created_utc": "2025-12-28 22:45:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdqar8",
          "author": "kevin_1994",
          "text": "Can we stop posting articles like this? I dont want politics on this subreddit, or else it will become a cesspit like the rest of reddit",
          "score": -9,
          "created_utc": "2025-12-28 16:06:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdyhvo",
              "author": "CanineAssBandit",
              "text": "https://preview.redd.it/odbsdf345z9g1.png?width=1600&format=png&auto=webp&s=236e122ca0b90b2577b0a6ba3267dd259654ef96\n\nthis is an important issue. If you don't care about our ability to fine tune, get the fuck off this sub.",
              "score": 11,
              "created_utc": "2025-12-28 16:47:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggtl5",
          "author": "Ylsid",
          "text": "Right direction wrong idea",
          "score": -2,
          "created_utc": "2025-12-29 00:14:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdf0q6",
          "author": "armeg",
          "text": "The touch grass bill",
          "score": -11,
          "created_utc": "2025-12-28 15:07:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg1soe",
              "author": "the320x200",
              "text": "More like the \"landgrab for control of new technology\" bill.",
              "score": 5,
              "created_utc": "2025-12-28 22:54:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw8nfk",
      "title": "Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/",
      "author": "Conscious_Warrior",
      "created_utc": "2025-12-26 16:42:23",
      "score": 257,
      "num_comments": 134,
      "upvote_ratio": 0.92,
      "text": "Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw344jd",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-26 21:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1p3pb",
          "author": "LeTanLoc98",
          "text": "Groq is mainly an architectural improvement. NVIDIA could potentially integrate ideas from Groq's architecture into their existing GPUs.\n\n\nCerebras is essentially a single, massive GPU. NVIDIA could build something similar on their own without relying on Cerebras. That kind of huge chip is also much more prone to manufacturing defects. On top of that, Cerebras GPUs only work well for a limited set of models.",
          "score": 299,
          "created_utc": "2025-12-26 16:47:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1pmbz",
              "author": "LeTanLoc98",
              "text": "https://groq.com/blog/the-groq-lpu-explained",
              "score": 65,
              "created_utc": "2025-12-26 16:50:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1t3m4",
                  "author": "Clear_Anything1232",
                  "text": "Reads a lot like Google TPU architecture",
                  "score": 58,
                  "created_utc": "2025-12-26 17:09:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1rc4q",
                  "author": "LeTanLoc98",
                  "text": "https://i.redd.it/iwa4ix7gxk9g1.gif\n\nGroq LPU",
                  "score": 46,
                  "created_utc": "2025-12-26 16:59:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1rf2z",
                  "author": "LeTanLoc98",
                  "text": "https://i.redd.it/gyugxquixk9g1.gif\n\nGPU without Groq LPU",
                  "score": 24,
                  "created_utc": "2025-12-26 17:00:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9ahld",
              "author": "limb3h",
              "text": "So many things wrong here.  First, Cerebras is not GPU. Second, Nvidia can also build what groq did easily without relying on Groq.  It‚Äôs a lot harder for Nvidia to replicate groq than Cerebras on their own.",
              "score": 3,
              "created_utc": "2025-12-27 21:40:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5v1se",
              "author": "alex_pro777",
              "text": "Why not buying both Groq & Cerebras to complete the deal?",
              "score": 1,
              "created_utc": "2025-12-27 08:38:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7z7cm",
                  "author": "LeTanLoc98",
                  "text": "A better alternative to Groq is SambaNova, not Cerebras.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:32:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2bgt8",
              "author": "Calandracas8",
              "text": "It's not an architectural improvement, it's an architectural disaster. VLIW does not work, it never has, and never will.\n\nAll it's good for is scamming investors",
              "score": -4,
              "created_utc": "2025-12-26 18:44:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2gitk",
                  "author": "Tomi97_origin",
                  "text": "Investors just got bought out by Nvidia. I would say they consider that successful venture.",
                  "score": 40,
                  "created_utc": "2025-12-26 19:11:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw46e8h",
                  "author": "Tai9ch",
                  "text": "> VLIW does not work, it never has, and never will.\n\nYup. Also, multi-core will never catch on and vector instructions are a marketing gimmick.",
                  "score": 23,
                  "created_utc": "2025-12-27 01:01:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4o2eq",
                  "author": "i_wayyy_over_think",
                  "text": "u can run groq on open router and it is fast with low latency",
                  "score": 7,
                  "created_utc": "2025-12-27 02:54:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2zf62",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -3,
              "created_utc": "2025-12-26 20:54:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw34ktk",
                  "author": "GustavoTC",
                  "text": "If you're just vomiting llm responses why even bother commenting",
                  "score": 2,
                  "created_utc": "2025-12-26 21:22:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1xdgw",
          "author": "bick_nyers",
          "text": "It's not an acquisition in the traditional sense, more like a licensing deal to all of Groq's IP and tech. Which of course one would argue is \"effectively\" an acquisition. Fair.\n\n\nGroq probably has more interconnect stuff figured out than Cerebras since their cards each have 256MB SRAM each and they need to do some crazy networking dark magic to inference something like Kimi versus Cerebras that just throws everything on the big chungus chip. Perhaps there are some custom data types and/or compression techniques at Groq that NVIDIA wanted to get their hands on.\n\n\nIt's possible NVIDIA tried to approach Cerebras but they didn't want to sell.\n\n\nI know this is local llama but I have to say Cerebras Code is my favorite AI subscription. I hope they continue to prosper.",
          "score": 42,
          "created_utc": "2025-12-26 17:31:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2vwkw",
              "author": "z_3454_pfk",
              "text": "it‚Äôs an acquisition. this is just to get around anti-trust lawsuits.",
              "score": 21,
              "created_utc": "2025-12-26 20:35:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw80j9q",
                  "author": "galenkd",
                  "text": "I agree and I find it troubling. When one company buys another company, they usually take on all the employees. They will then lay off the redundant persons (e.g., hr, finance, facilities, etc.) with a decent package. For those redundant employees, their equity has converted so there could be a big payoff down the line.\n\nWhat's happening here is Nvidia is taking the IP via license and only the employees they want. Groq gets a lot of money and the board will decide how to distribute it. There's a strong chance that those not moving to nVidia will just be around to wind the company down. The board can distribute the funds as they see fit and leave the employees totally in the cold. There are no norms on how to do this.\n\nYou could argue that the employees could get screwed in a full acquisition, too, but that would violate a lot of norms. Though, I guess it's 2025 and norms are worth very little.",
                  "score": 3,
                  "created_utc": "2025-12-27 17:39:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1pfb3",
          "author": "HarambeTenSei",
          "text": "Gotta leave something for AMD¬†",
          "score": 40,
          "created_utc": "2025-12-26 16:49:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw28bpx",
          "author": "jakegh",
          "text": "My guess is it's because Groq doesn't fabricate their chips at TSMC, as Nvidia doesn't want to trade any of their allotted capacity.\n\nAlso, Nvidia did not acquire Groq. They bought the CEO, the top engineers, and a non-exclusive license to the tech.",
          "score": 12,
          "created_utc": "2025-12-26 18:28:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2018i",
          "author": "NorthSideScrambler",
          "text": "Because the Trump family invested in Groq in September via 1789 Capital.¬† Nvidia overpaid for Groq by 2x, and there's a reason for that.¬†¬†\n\n\nEdit: It becomes less conspiratol when you see how often Nvidia shares a bed with the Trump admin.¬† There were the Intel investments when the Trump admin took 10% of Intel, and Jensen Huang's $1,000,000 dinner at Mar A Lago followed immediately by the White House reversing export restrictions on Nvidia's H20 chips to China.\n\n\nOh, and Nvidia actually overpaid for Groq by 3x.¬† Charitably, you could split the difference and say 2.5x.¬† Groq was worth $6.9 billion in late September.¬†¬†",
          "score": 88,
          "created_utc": "2025-12-26 17:45:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw21ubu",
              "author": "emprahsFury",
              "text": "those gpu sales in china don't come free",
              "score": 18,
              "created_utc": "2025-12-26 17:55:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw66oky",
              "author": "Fit_Border9119",
              "text": "1789 also invested in Cerebras' latest funding round.",
              "score": 1,
              "created_utc": "2025-12-27 10:32:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwc5bc2",
              "author": "vincentz42",
              "text": "This needs to be upvoted much higher. The groq deal never made much sense to me on a technical and business basis. I always wondered if the deal is to please certain VC/PE people behind groq (1789, Chamath Palihapitiya, Black Rock?)\n\nSure, SRAMs offer 10x bandwidth, but they have 100-1000x less capacity. Groq chips are fast in terms of latency, but they are much slower in terms of per chip throughout compared to GPUs. And you would need 10K chips just to serve a 1T MoE to a limited set of users so this is not exactly scalable. And in the end there's no reason that NVIDIA can't slap a SRAM cache to their GPUs like AMD X3D. So the technology/TPU talent side of story never made much sense IMHO.",
              "score": 1,
              "created_utc": "2025-12-28 09:04:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2tncq",
              "author": "MokoshHydro",
              "text": "And I thought about Blackrock first. :)",
              "score": 1,
              "created_utc": "2025-12-26 20:22:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1ufrz",
          "author": "locoblue",
          "text": "Because Googles TPUs are great and Nvidia wants to explore that space.\n\nCerebras is firmly within Nvidias wheelhouse already",
          "score": 30,
          "created_utc": "2025-12-26 17:16:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw24dax",
          "author": "alvisanovari",
          "text": "From this post\n\n\"And Cerebras, where I am biased, is now in a very interesting and highly strategic position as the last (per public knowledge) independent SRAM player that was ahead of Groq on all public benchmarks. Groq‚Äôs ‚Äúmany chip‚Äù rack architecture, however, was much easier to integrate with Nvidia‚Äôs networking stack and perhaps even within a single rack while Cerebras‚Äôs WSE almost has to be an independent rack.\"\n\n[https://x.com/GavinSBaker/status/2004562536918598000](https://x.com/GavinSBaker/status/2004562536918598000)",
          "score": 19,
          "created_utc": "2025-12-26 18:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2len4",
              "author": "SlowFail2433",
              "text": "Makes some more sense",
              "score": 9,
              "created_utc": "2025-12-26 19:37:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4o5nn",
              "author": "puppymaster123",
              "text": "Kinda freaking out a bit here because some of my successful apps are powered by groq multimodal image to text. Cerebras does not have multimodal in its offerings yet. So there‚Äôs really no alternative when it comes to fast inference for multimodal.",
              "score": 6,
              "created_utc": "2025-12-27 02:55:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9xs8c",
                  "author": "Apprehensive_Plan528",
                  "text": "Groq cloud lives on. If it works for you today, no worries.",
                  "score": 1,
                  "created_utc": "2025-12-27 23:48:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4ovb1",
                  "author": "alvisanovari",
                  "text": "what's there to freak out about? they'll only get better.",
                  "score": 1,
                  "created_utc": "2025-12-27 03:00:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2r7ag",
              "author": "desexmachina",
              "text": "Good catch. With Nvidia already deep into GPU Direct, something complimentary in the pipeline might be usable over a wider distributed compute infra. Because TBH, what does Nvidia need another inference chip for?  Especially one that acts more like a CPU than a TPU.",
              "score": 5,
              "created_utc": "2025-12-26 20:09:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2w1qz",
                  "author": "alvisanovari",
                  "text": "probably to corner the market on lightweight low latency takss like autocomplete. once blackwell comes online it will dominate memory intensive thinking outputs",
                  "score": 1,
                  "created_utc": "2025-12-26 20:35:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1ol9e",
          "author": "TheToi",
          "text": "I had the same thought. But maybe they want to invest in Groq so they can surpass Cerebras at a lower cost?",
          "score": 9,
          "created_utc": "2025-12-26 16:45:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5gz5w",
          "author": "ieatrox",
          "text": "Groq uses 0 HBM.\n\nIts a 20B hedge against a supply chain attack Jensen referred to as a matter of national security.\n\nIt uses 14nm fabs like intel has sitting around. Like the ones Nvidia invested in intel to gain access to.\n\nOnce training is completed, inference on a 10 million dollar groq cluster is roughly as performant as on a 10 million dollar b200 cluster. But the groq clusters can be made at home for pennies regardless of global constraints or stress on TSMC.\n\nJensen's no dummy.",
          "score": 8,
          "created_utc": "2025-12-27 06:26:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7n2ab",
          "author": "keepthepace",
          "text": "Maybe Cerebras does not want to sell?",
          "score": 4,
          "created_utc": "2025-12-27 16:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38srd",
          "author": "CatalyticDragon",
          "text": "Trump's son is an investor in Groq.",
          "score": 8,
          "created_utc": "2025-12-26 21:45:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw346ym",
          "author": "kidflashonnikes",
          "text": "ITS A RAM play. We‚Äôre out of RAM at our lab. Desperately trying to find some right now. They bough Groq for inference due to their chip architecture (SRAM), the main RAM is DRAM right now",
          "score": 3,
          "created_utc": "2025-12-26 21:20:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4fsdr",
          "author": "FearlessZucchini3712",
          "text": "Groq ceo is ex google who worked on TPU. Which is a competitor chip for nvidia",
          "score": 3,
          "created_utc": "2025-12-27 02:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4x7ct",
          "author": "Virtamancer",
          "text": "Cerebra‚Äôs is fast because they use gigaquantized models, judging by the benchmarks and openrouter data posted around here from time to time.",
          "score": 3,
          "created_utc": "2025-12-27 03:55:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1s9le",
          "author": "keyser1884",
          "text": "Because Nvidia specializes in brute force which is great for training models but doesn‚Äôt scale as well for inference.\n\nGroq pioneered TPUs and they built them using outdated and affordable fabrication processes.\n\nSo Nvidia get another string in their bow and can help Groq products achieve greater efficiency and manufacturing scale.",
          "score": 4,
          "created_utc": "2025-12-26 17:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1tkef",
          "author": "LeTanLoc98",
          "text": "> Cerebras seems like a bigger threat to Nvidia than Groq...\n\n\nCerebras is not really a threat to NVIDIA.\n\n\nCerebras GPUs only work well for a limited set of models.\n\n\nThey are mainly optimized for inference, while NVIDIA still clearly dominates training thanks to CUDA and its mature ecosystem.\n\n\nNVIDIA could build a massive, wafer-scale GPU like Cerebras if they wanted to. They simply choose not to. Selling large numbers of smaller GPUs is far more profitable for them, since customers have to buy many units instead of one giant chip.",
          "score": 8,
          "created_utc": "2025-12-26 17:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3bogs",
              "author": "dogesator",
              "text": "Cerebras is better at training than groq though, so this doesn‚Äôt really answer OPs question",
              "score": 6,
              "created_utc": "2025-12-26 22:00:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5edtq",
                  "author": "b_m_hart",
                  "text": "Groq chips aren't designed to do any training, it is like trying to use a blow torch as a hammer.¬†¬†",
                  "score": 3,
                  "created_utc": "2025-12-27 06:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7z8q3",
              "author": "LeTanLoc98",
              "text": "A better alternative to Groq is SambaNova, not Cerebras.",
              "score": 0,
              "created_utc": "2025-12-27 17:32:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw237kk",
          "author": "Erebea01",
          "text": "Maybe the guys at Groq has better connections or are better salesmen, sometimes it can be as simple as that",
          "score": 2,
          "created_utc": "2025-12-26 18:02:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2tf0g",
          "author": "MokoshHydro",
          "text": "Because that deal has nothing to do with technology. That's just Blackrock gathering investment money back. $20B price doesn't have any other reason behind.",
          "score": 2,
          "created_utc": "2025-12-26 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw459aw",
          "author": "orangeatom",
          "text": "chamath and jenson are friends",
          "score": 2,
          "created_utc": "2025-12-27 00:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwapjpz",
          "author": "CommercialLead6053",
          "text": "Nvidia didn't actually acquire Groq though? Where are you seeing that news because I can't find anything about it\n\n  \nAlso Cerebras has their own fab issues and their chips are massive - way harder to scale production compared to Groq's approach. Speed isn't everything if you can't manufacture at volume",
          "score": 2,
          "created_utc": "2025-12-28 02:29:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbe7de",
          "author": "Kassdhal88",
          "text": "Cerebras is not scalable.",
          "score": 2,
          "created_utc": "2025-12-28 05:06:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1tt4z",
          "author": "ttkciar",
          "text": "At a guess:\n\n* Cerebras' wafer-scale approach is more radical than Groq's, and \"radical\" frightens conservative businesspeople,\n\n* Cerebras hasn't solved their memory density problem, last I checked, and still relies on SRAM.  That sharply limits the amount of memory on the die, which is a constraint for some kinds of tasks (like training models of any significant size).\n\nIf Cerebras can solve their memory problem, they're going to be a beast to reckon with, and Nvidia executives might wind up kicking themselves for passing them over.  We will see how it pans out.",
          "score": 2,
          "created_utc": "2025-12-26 17:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw20ayk",
              "author": "hello_2221",
              "text": "I wonder if they could increase memory with 3d stacking like amd's x3d chips",
              "score": 3,
              "created_utc": "2025-12-26 17:47:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw52685",
                  "author": "ttkciar",
                  "text": "I've wondered that, too.  Maybe they're working on it.",
                  "score": 2,
                  "created_utc": "2025-12-27 04:30:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw228aw",
              "author": "emprahsFury",
              "text": "WSA's have been well-known since the 70s and 80s. It's not a radical technology at all. It's just too expensive",
              "score": 2,
              "created_utc": "2025-12-26 17:57:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1prap",
          "author": "CertainlyBright",
          "text": "Same argument as \"AMD faSteR tHaN InTeL whY eVerYoNe sTiLL bUy InTeL??? üò≠\" when it's not just speed, but stability, support, and creative foundation a product is built on.",
          "score": 2,
          "created_utc": "2025-12-26 16:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1q6c6",
              "author": "tertain",
              "text": "But no one buys intel‚Ä¶",
              "score": 16,
              "created_utc": "2025-12-26 16:53:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1qa9p",
                  "author": "CertainlyBright",
                  "text": "$INTC Hmmmm....",
                  "score": -4,
                  "created_utc": "2025-12-26 16:54:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1qy68",
              "author": "BriguePalhaco",
              "text": "Intel stable? Didn't they create an oxidation problem by skimping on a chemical that protected against it?",
              "score": 9,
              "created_utc": "2025-12-26 16:57:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1q59p",
              "author": "ChomsGP",
              "text": "zero days disagree with this statement",
              "score": 4,
              "created_utc": "2025-12-26 16:53:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1r2g1",
                  "author": "BriguePalhaco",
                  "text": "Oxidation in the CPU as well.",
                  "score": 2,
                  "created_utc": "2025-12-26 16:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw261cu",
          "author": "xjE4644Eyc",
          "text": "Cerebras IPO is soon IIRC.  20B might be too little",
          "score": 1,
          "created_utc": "2025-12-26 18:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2fl4y",
          "author": "gwestr",
          "text": "Can‚Äôt find any actual press release from Nvidia that they would acquire grok a few quarters from now.",
          "score": 1,
          "created_utc": "2025-12-26 19:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2p7dy",
          "author": "valdev",
          "text": "We don‚Äôt actually know.",
          "score": 1,
          "created_utc": "2025-12-26 19:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw365ti",
          "author": "jafbm",
          "text": "I heard through the grapevine that Chamath wanted out",
          "score": 1,
          "created_utc": "2025-12-26 21:31:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4x3ai",
          "author": "Wubbywub",
          "text": "you gotta realize the world runs on many reasons other than technical logic, lots of political and interpersonal relationship reasons",
          "score": 1,
          "created_utc": "2025-12-27 03:55:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5eu1u",
              "author": "b_m_hart",
              "text": "And this deal is based entirely in logic and reason and what Nvidia wanted from Groq's current and upcoming chips.",
              "score": 1,
              "created_utc": "2025-12-27 06:07:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwa10o7",
          "author": "anima-core",
          "text": "A few important distinctions get blurred in this comparison.\n\n**Cerebras and Groq are optimizing for fundamentally different regimes.**\n\nCerebras wins when:\n\n‚Ä¢You want very large models\n\n‚Ä¢You want training or long-sequence inference\n\n‚Ä¢You want to avoid partitioning overhead, collective ops, and interconnect complexity\n\n\nA single wafer-scale engine keeps the entire model and activations on-chip. That‚Äôs why Cerebras looks absurdly fast on certain workloads. No sharding, no NCCL, no cross-node synchronization. It‚Äôs a ‚Äúone giant brain‚Äù architecture.\n\nGroq, by contrast, is optimized for:\n\n‚Ä¢Deterministic, ultra-low-latency inference\n\n‚Ä¢Tight compiler control\n\n‚Ä¢Serving workloads at scale with predictable timing\n\n\nThat maps much more cleanly onto Nvidia‚Äôs existing CUDA + inference ecosystem.\n\nSo why would Nvidia favor Groq?\n\nBecause Groq complements Nvidia‚Äôs business, while Cerebras challenges it structurally. Head on.\n\nGroq:\n\n‚Ä¢Looks like an accelerator that can slot into existing stacks\n\n‚Ä¢Reinforces the idea that ‚Äúmodels stay the same, hardware gets faster‚Äù\n\n‚Ä¢Doesn‚Äôt threaten CUDA, sharding, or GPU cluster economics\n\n\nCerebras:\n\n‚Ä¢Implicitly says ‚Äúthe cluster model itself is broken‚Äù\n\n‚Ä¢Eliminates whole layers Nvidia monetizes (interconnects, multi-GPU scaling, orchestration)\n\n‚Ä¢Pushes toward fewer, larger, simpler systems\n\n\n**That‚Äôs not a performance argument. That‚Äôs a business and control argument.**\n\nWhere Cerebras actually has a David-vs-Goliath opening:\n\n‚Ä¢Frontier training (large dense models, long context)\n\n‚Ä¢Government / national labs\n\n‚Ä¢Workloads where simplicity and determinism beat flexibility\n\n‚Ä¢Architectures that don‚Äôt scale cleanly across thousands of GPUs\n\n\nIf the future moves toward:\n\n‚Ä¢Larger contexts\n\n‚Ä¢Fewer model shards\n\n‚Ä¢Less distributed complexity\n\n‚Ä¢More ‚Äúsystem-level‚Äù thinking\n\n\nCerebras becomes more dangerous, not less.\n\nNvidia didn‚Äôt ‚Äúmiss‚Äù Cerebras.\nThey likely see it as **too disruptive to absorb cleanly**, whereas Groq is additive.\n\nDifferent weapons. Different wars.",
          "score": 1,
          "created_utc": "2025-12-28 00:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1s24w",
          "author": "Gringe8",
          "text": "They didnt acquire groq.",
          "score": 2,
          "created_utc": "2025-12-26 17:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1yj3w",
              "author": "tmvr",
              "text": "Well, they didn't, but effectively they did. They licensed the tech and took all the key people (management and arch). So the company still exists, but what is the future? They basically went around regulatory processes so that there are no hearings because it is not an acquisition.",
              "score": 21,
              "created_utc": "2025-12-26 17:37:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw21qoy",
              "author": "Freonr2",
              "text": "They did in all but name, only a husk leftover.",
              "score": 8,
              "created_utc": "2025-12-26 17:54:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2645r",
                  "author": "PwanaZana",
                  "text": "an aquihire, I've heard this sorta deal called",
                  "score": 9,
                  "created_utc": "2025-12-26 18:17:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1onbj",
          "author": "LevianMcBirdo",
          "text": "Cerebras is private, so if they don't wanna sell there isn't much nvidea can do",
          "score": 0,
          "created_utc": "2025-12-26 16:45:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1quew",
              "author": "piggledy",
              "text": "Groq is private too",
              "score": 10,
              "created_utc": "2025-12-26 16:57:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1uh7b",
                  "author": "LevianMcBirdo",
                  "text": "You get the difference that one wants to sell while the other potentially doesn't?",
                  "score": 0,
                  "created_utc": "2025-12-26 17:16:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw2b6oq",
          "author": "Calandracas8",
          "text": "Groq is a scam, it has been shown over and over how writing a compiler for VLIW is impossible. Never mind that groq is an insane 144 wide VLIW.\n\nGoogle's TPU uses an 8 wide VLIW architecture for control flow, but a traditional architecture for compute.\n\nPerhaps they see some good bit that they want to use some of the IP to build some sort of new product, but I'm also very surprised since Groq's architecture's is not good",
          "score": -2,
          "created_utc": "2025-12-26 18:43:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4qe12",
              "author": "i_wayyy_over_think",
              "text": "They serve open source models often with nearly the highest throughput and lowest latency on open router, how's that a scam? [https://openrouter.ai/moonshotai/kimi-k2-0905?sort=latency](https://openrouter.ai/moonshotai/kimi-k2-0905?sort=latency)\n\n[https://openrouter.ai/provider/groq](https://openrouter.ai/provider/groq)",
              "score": 7,
              "created_utc": "2025-12-27 03:10:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2vqq0",
          "author": "Active_Change9423",
          "text": "groq has been in panic mode since Cerebras inference knocked them off the top spot. I bet the leadership team was especially eager to get on board the nvidia escape raft.",
          "score": 0,
          "created_utc": "2025-12-26 20:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5e6zc",
              "author": "b_m_hart",
              "text": "Weirdly random FUD that is entirely untrue.",
              "score": 7,
              "created_utc": "2025-12-27 06:02:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1pmeu",
          "author": "Odd-Ordinary-5922",
          "text": "just because nvidia has money doesnt mean they can buy a company",
          "score": -2,
          "created_utc": "2025-12-26 16:50:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t7yx",
              "author": "Tman1677",
              "text": "If you're one of these teeny little startups that could run out of runway any day now, your dream is to get acquired for 10 billion",
              "score": 5,
              "created_utc": "2025-12-26 17:09:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1udw7",
                  "author": "Odd-Ordinary-5922",
                  "text": "I dont think cerebras is a tiny little startup",
                  "score": 3,
                  "created_utc": "2025-12-26 17:15:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2kq9l",
              "author": "Randommaggy",
              "text": "Aquihires are effectively the same except with less scrutiny.",
              "score": 1,
              "created_utc": "2025-12-26 19:33:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1sg1x",
          "author": "MrDevGuyMcCoder",
          "text": "Grok is a joke isnt it? As in no one actually uses or trusts it",
          "score": -15,
          "created_utc": "2025-12-26 17:05:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1t03o",
              "author": "Internal_Werewolf_48",
              "text": "Grok =/= Groq",
              "score": 19,
              "created_utc": "2025-12-26 17:08:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1y8c1",
                  "author": "MrDevGuyMcCoder",
                  "text": "Ya, my bad. Reading is hard apparently",
                  "score": 4,
                  "created_utc": "2025-12-26 17:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1t0ea",
              "author": "Conscious_Warrior",
              "text": "Groq with a q",
              "score": 7,
              "created_utc": "2025-12-26 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1uhbd",
                  "author": "MrDevGuyMcCoder",
                  "text": "Oh... Reading comprehension issue here apparently... What is groq?",
                  "score": 3,
                  "created_utc": "2025-12-26 17:16:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw1wdso",
              "author": "Mediocre-Method782",
              "text": "I don't; they spammed this sub very heavily with their non-local ad shit long ago and now look at the place",
              "score": 1,
              "created_utc": "2025-12-26 17:26:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyao6g",
      "title": "Meta released RPG, a research plan generation dataset on Hugging Face",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/datasets/facebook/research-plan-gen",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-29 02:58:09",
      "score": 254,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nwhk8eb",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-29 04:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhjva1",
          "author": "LoveMind_AI",
          "text": "Meta is humiliating OpenAI in terms of research and open source contributions. I have a feeling the days of open frontier models are over, but they‚Äôre still doing a lot.",
          "score": 97,
          "created_utc": "2025-12-29 03:57:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhlg3m",
              "author": "TheRealMasonMac",
              "text": "Chinese labs probably appreciate the free research. Especially since this one comes with evaluation criteria so they can RL on it.",
              "score": 38,
              "created_utc": "2025-12-29 04:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwhlyyd",
                  "author": "Southern-Chain-6485",
                  "text": "Welcome to science",
                  "score": 54,
                  "created_utc": "2025-12-29 04:11:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwi95f9",
              "author": "eat_my_ass_n_balls",
              "text": "Sorta, but their models have fallen off",
              "score": -1,
              "created_utc": "2025-12-29 07:02:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhnv9t",
          "author": "Any-Conference1005",
          "text": "Acronym collision.......",
          "score": 33,
          "created_utc": "2025-12-29 04:22:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhpalv",
              "author": "HistorianPotential48",
              "text": "can't wait for coming up HGAME dataset, FEMBOY datasets from meta",
              "score": 29,
              "created_utc": "2025-12-29 04:32:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwi8967",
              "author": "FaceDeer",
              "text": "I really need to train an LLM for some serious hardcore RPG, and I keep finding plenty of datasets that claim that they're for this purpose. But the LLMs keep turning out wrong! Every time I demo for my supervisor... honestly, I have no idea why my funding hasn't been pulled, or why he keeps the resulting models. They're useless.",
              "score": 6,
              "created_utc": "2025-12-29 06:54:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhb3i5",
          "author": "segmond",
          "text": "Would be nice if folks release dataset with models trained on it.",
          "score": 13,
          "created_utc": "2025-12-29 03:05:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhiam9",
              "author": "Accomplished_Ad9530",
              "text": "They cite their unreleased paper, ‚ÄúTraining AI Co-Scientists using Rubric Rewards‚Äù so I wouldn‚Äôt be surprised if they release a model at some point.",
              "score": 14,
              "created_utc": "2025-12-29 03:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi9g9x",
          "author": "JudgmentPale458",
          "text": "Interesting release. Research plan generation feels like a subtle but important capability ‚Äî especially for agentic or tool-using systems where planning quality matters more than final answer fluency.\n\nCurious how this dataset handles evaluation: are plans judged mainly on structure/coverage, or is there any signal about feasibility and downstream execution success? That distinction seems critical if this is used to train agents rather than just planners.",
          "score": 3,
          "created_utc": "2025-12-29 07:04:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkbh1n",
          "author": "martinerous",
          "text": "Great, now waiting what they will make out of MMORPG.",
          "score": 1,
          "created_utc": "2025-12-29 16:05:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrk07z",
          "author": "stealthagents",
          "text": "This dataset sounds like a game changer for streamlining research. Having those evaluation rubrics and reference solutions will save a ton of time for any AI training. Can't wait to see what kind of projects come out of this!",
          "score": 1,
          "created_utc": "2025-12-30 17:39:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi2ub3",
          "author": "serendipity777321",
          "text": "What is this for? Not one single explanation",
          "score": 1,
          "created_utc": "2025-12-29 06:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi7cer",
              "author": "Odd-Ordinary-5922",
              "text": "22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training **AI co-scientists**",
              "score": 13,
              "created_utc": "2025-12-29 06:46:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwigkvv",
                  "author": "serendipity777321",
                  "text": "You must be joking",
                  "score": -2,
                  "created_utc": "2025-12-29 08:09:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwiquf6",
              "author": "know-your-enemy-92",
              "text": "Taking science back to the times of alchemy from middle ages.¬†",
              "score": 2,
              "created_utc": "2025-12-29 09:46:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pullo0",
      "title": "Hmm all reference to open-sourcing has been removed for Minimax M2.1...",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/",
      "author": "Responsible_Fig_1271",
      "created_utc": "2025-12-24 11:48:37",
      "score": 240,
      "num_comments": 93,
      "upvote_ratio": 0.93,
      "text": "Funny how yesterday this page [https://www.minimax.io/news/minimax-m21](https://www.minimax.io/news/minimax-m21) had a statement that weights would be open-sourced on Huggingface and even a discussion of how to run locally on vLLM and SGLang. There was even a (broken but soon to be functional) HF link for the repo...\n\n  \nToday that's all gone.\n\n  \nHas MiniMax decided to go API only? Seems like they've backtracked on open-sourcing this one. Maybe they realized it's so good that it's time to make some $$$ :( Would be sad news for this community and a black mark against MiniMax.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvq0le0",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 14:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpj0j7",
          "author": "Wise_Evidence9973",
          "text": "For u Christmas gift, bro",
          "score": 124,
          "created_utc": "2025-12-24 12:40:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpj3in",
              "author": "Wise_Evidence9973",
              "text": "Tomorrow",
              "score": 64,
              "created_utc": "2025-12-24 12:40:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpxst7",
                  "author": "____vladrad",
                  "text": "Thank you.",
                  "score": 19,
                  "created_utc": "2025-12-24 14:18:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyxrzw",
                  "author": "zmarty",
                  "text": "Christmas is over. Where are the weights, BRO?",
                  "score": 0,
                  "created_utc": "2025-12-26 03:36:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyz7dh",
                  "author": "Responsible_Fig_1271",
                  "text": "\"Tomorrow\" has come and gone.",
                  "score": 0,
                  "created_utc": "2025-12-26 03:46:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpjaxv",
          "author": "espadrine",
          "text": "They've shown goodwill in the past. My policy is to assume they'll do the right thing if they have a history of doing the right thing.\n\nBesides the article still mentions opening  the weights:\n\n> [M2.1 is] one of the first open-source model series to systematically introduce Interleaved Thinking\n\n> We're excited for powerful open-source models like M2.1",
          "score": 35,
          "created_utc": "2025-12-24 12:42:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpfexc",
          "author": "Only_Situation_4713",
          "text": "Head of research on twitter said on Christmas so it‚Äôs still open source",
          "score": 27,
          "created_utc": "2025-12-24 12:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpdwx4",
          "author": "SlowFail2433",
          "text": "Idk if its worth speculating, what drops drops\n\n\nSomeone posted an article yesterday about z.ai and minimax having money troubles",
          "score": 51,
          "created_utc": "2025-12-24 11:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpj6u8",
              "author": "Wise_Evidence9973",
              "text": "Will release soon. MiniMax does not have money trouble.",
              "score": 99,
              "created_utc": "2025-12-24 12:41:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpk272",
                  "author": "No_Conversation9561",
                  "text": "Everyone listen to this personüëÜ\n\nThey‚Äôre from Minimax.",
                  "score": 52,
                  "created_utc": "2025-12-24 12:48:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvq31ea",
                  "author": "tarruda",
                  "text": "Thank you. Minimax M2 is amazing, looking forward to trying M2.1 on my mac.",
                  "score": 17,
                  "created_utc": "2025-12-24 14:49:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvplsm0",
                  "author": "Leflakk",
                  "text": "Glad to hear your not in money trouble",
                  "score": 25,
                  "created_utc": "2025-12-24 13:00:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpkby7",
                  "author": "SlowFail2433",
                  "text": "Wow thanks that‚Äôs great to hear. I am a huge fan of your models and papers, especially the RL stuff.",
                  "score": 11,
                  "created_utc": "2025-12-24 12:50:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpkw7b",
                  "author": "NaiRogers",
                  "text": "Thank you",
                  "score": 5,
                  "created_utc": "2025-12-24 12:54:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvu05ir",
                  "author": "RedParaglider",
                  "text": "Minimax is a very nice feeling tool to use.  I still have 30 dollars of API usage I bought on the last release, I need to play with the new model some :)",
                  "score": 2,
                  "created_utc": "2025-12-25 05:59:09",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvq0b61",
                  "author": "power97992",
                  "text": "Please make a smaller <100b model with great performance like deepseek v3.2 speciale and minimax 2.1. Keep making efficient high quality smaller models even if deepseek releases a  +1.8Trillion parameter model...",
                  "score": -2,
                  "created_utc": "2025-12-24 14:33:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpzs1z",
              "author": "FullOf_Bad_Ideas",
              "text": "They have some runway but R&D costs are 3x higher than revenue for Minimax and 8x higher for Zhipu.\n\nYou can read more here (translate it with your preferred method) \n\nZhipu: https://wallstreetcn.com/articles/3761776\n\nMinimax: https://wallstreetcn.com/articles/3761823",
              "score": 7,
              "created_utc": "2025-12-24 14:30:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvpgin3",
          "author": "j_osb",
          "text": "I mean, that's what always happens, no?\n\nQwen (with Max). Once their big models get good enough, there'll be no reason to release smaller ones for the public. Like they did with Wan, for example.\n\nOr this. Or what tencent does.\n\nOpen source/weights only gets new models until they're good enough, at which point all the work the open source community has done for them is just 'free work' for them and they continue closing their models.",
          "score": 12,
          "created_utc": "2025-12-24 12:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqyko8",
              "author": "RhubarbSimilar1683",
              "text": "For those who don't know, wan 2.5 is competitive with Google's veo 3 and thus remains closed source unlike earlier wan versions and hunyuan 3d 2.5 is closed source but earlier versions are open source¬†",
              "score": 5,
              "created_utc": "2025-12-24 17:40:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwtb9",
              "author": "I-am_Sleepy",
              "text": "https://huggingface.co/MiniMaxAI/MiniMax-M2.1/tree/main",
              "score": 1,
              "created_utc": "2025-12-26 08:41:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvpl2oi",
              "author": "power97992",
              "text": "If open weights become so good, why dont they just sell the model with the inference engine and scaffolding  as a stand alone program , ofc people can jail break it, but that requires effort",
              "score": -2,
              "created_utc": "2025-12-24 12:55:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpnpds",
                  "author": "SlowFail2433",
                  "text": "It would get decompiled",
                  "score": 6,
                  "created_utc": "2025-12-24 13:14:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpzxp1",
                  "author": "j_osb",
                  "text": "If they would do that, the model files would need to be on your computer. Even IF they were somehow decrypted, the key for that would always be findable.\n\nErgo, you could easily run it locally, for free. Not what they want.",
                  "score": 1,
                  "created_utc": "2025-12-24 14:31:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvri1gl",
          "author": "fooo12gh",
          "text": "I really hope that at some point in time there will be open weight model trained by completely independent, community driven organisation (which OpenAI probably intended to be in the 1st place). Something like Free Software Foundation, but in the world of LLM. So that community of people doesn't depend on the financial plans of private companies.",
          "score": 3,
          "created_utc": "2025-12-24 19:26:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt8nax",
          "author": "complains_constantly",
          "text": "God you guys are fucking paranoid.\n\n\nObviously the lab that has open-weighted every model they've ever made, and has said this week they're going to open-weight their latest model, is going to open-weight their latest model. Lmao. They're probably rewriting their blog release or something.",
          "score": 3,
          "created_utc": "2025-12-25 02:17:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvkkhm",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/q1zhy6aj6d9g1.png?width=2063&format=png&auto=webp&s=af98c2d7722bab43a8ecbd800d1ed9f89c62947b\n\n[https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/kt-kernel/MiniMax-M2.1-Tutorial.md) Incoming",
          "score": 3,
          "created_utc": "2025-12-25 14:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvq2rnm",
          "author": "tarruda",
          "text": "Would be a shame if they don't open source it. GLM 4.7V is too big for 128GB Macs, but Minimax M2 can fit with a IQ4_XS quant",
          "score": 4,
          "created_utc": "2025-12-24 14:47:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrd4in",
              "author": "Its_Powerful_Bonus",
              "text": "GLM 4.7 Q2 works on Mac 128gb quite well üòâ Tested just for few queries, but it was very usable",
              "score": 2,
              "created_utc": "2025-12-24 18:58:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvj7st",
                  "author": "tarruda",
                  "text": "I ended up trying UD-IQ2_M quant and it seems to give pretty close results to what you get in chat.z.ai.\n\nMy mind is blown by how much of the original quality is kept by these super small quants.",
                  "score": 3,
                  "created_utc": "2025-12-25 14:47:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvrhlw2",
                  "author": "tarruda",
                  "text": "Interesting!\n\nDid you use unsloth dynamic quant? How much memory did it use and how much context could you fit?",
                  "score": 1,
                  "created_utc": "2025-12-24 19:23:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr1liw",
          "author": "LeTanLoc98",
          "text": "Honestly, it would be great if they released the weights, but if not, that's totally fine as well.\n\n\nOpen-source models are already very strong.\n\n\nWe now have DeepSeek v3.2, GLM-4.7, and Kimi K2 Thinking.\n\n\nThese models are largely on par with each other, none of them is clearly superior.",
          "score": 2,
          "created_utc": "2025-12-24 17:56:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuxhg9",
          "author": "Impressive_Chain6039",
          "text": "https://huggingface.co/MiniMaxAI/MiniMax-M2/commit/f7804c9c48d5ee2bdaa89db34a6337bba02b0f40 2.1 incoming",
          "score": 2,
          "created_utc": "2025-12-25 11:49:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpd3en",
          "author": "Tall-Ad-7742",
          "text": "i hope not üôÅ  \nthat would be a war crime for me tbh",
          "score": 5,
          "created_utc": "2025-12-24 11:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpdz4j",
              "author": "SlowFail2433",
              "text": "Open source community be normal challenge",
              "score": 43,
              "created_utc": "2025-12-24 11:58:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvplal9",
                  "author": "datbackup",
                  "text": "Lmao",
                  "score": 4,
                  "created_utc": "2025-12-24 12:57:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvpd74v",
              "author": "Responsible_Fig_1271",
              "text": "For me as well!",
              "score": 1,
              "created_utc": "2025-12-24 11:52:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvqwpo5",
              "author": "colei_canis",
              "text": "They‚Äôre going to use the model to mistreat prisoners of war in an active conflict?",
              "score": 1,
              "created_utc": "2025-12-24 17:30:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuve6x",
                  "author": "Tall-Ad-7742",
                  "text": "xD",
                  "score": 1,
                  "created_utc": "2025-12-25 11:27:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvpmmrv",
          "author": "xenydactyl",
          "text": "They still kept the comment of Eno Reyes (Co-Founder, CTO of Factory AI) in: \"We're excited for powerful **open-source** models like **M2.1** that bring frontier performance...\"",
          "score": 2,
          "created_utc": "2025-12-24 13:06:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqbv6g",
          "author": "SilentLennie",
          "text": "Or maybe they discovered some problems and don't know when it will be released.",
          "score": 2,
          "created_utc": "2025-12-24 15:38:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqnlqb",
          "author": "KvAk_AKPlaysYT",
          "text": "Even if they are going to OS it, why remove it from the website overnight :(\n\nEverybody, join your hands together and chant GGUF wen.",
          "score": 2,
          "created_utc": "2025-12-24 16:40:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpglar",
          "author": "__Maximum__",
          "text": "The model seems to be very good at some tasks, so this could have been their chance to stand out. I still hope they do open weight it for their own sake.",
          "score": 1,
          "created_utc": "2025-12-24 12:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpk2zl",
          "author": "jacek2023",
          "text": "Let's wait for \"let them cook, you should be grateful, they owe you nothing\" redditors",
          "score": -3,
          "created_utc": "2025-12-24 12:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpp8ra",
              "author": "oxygen_addiction",
              "text": "That's literally the case. They said they will release it tomorrow even in this thread. You are just being ungrateful children, acting as if the world owes you something.",
              "score": 10,
              "created_utc": "2025-12-24 13:24:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvpqhw1",
                  "author": "SlowFail2433",
                  "text": "This isn‚Äôt how open source works\n\n\nOpen source is like a common public good, which we all both contribute to and consume. Encouraging more open source releases isn‚Äôt entitlement it is fostering a culture and environment where people and organisations do open source releases that are mutually beneficial, to both the users and releaser.",
                  "score": 9,
                  "created_utc": "2025-12-24 13:32:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpr6sb",
                  "author": "jacek2023",
                  "text": "...and here they are",
                  "score": -2,
                  "created_utc": "2025-12-24 13:37:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvr0a3b",
          "author": "jreoka1",
          "text": "I'm pretty sure they plan on putting it back on HF according to the person here from the Minimax team.",
          "score": 1,
          "created_utc": "2025-12-24 17:49:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrlv4e",
          "author": "AllegedlyElJeffe",
          "text": "a) the makers have sat here in the comments that they‚Äôre still putting it out probably tomorrow.\n\n\nb) people are not required to give away for free something they worked really hard on. It‚Äôs awesome and we all love it, but they‚Äôre not doing the wrong thing‚Äù if they decide to sell the product of their work instead. I‚Äôm not saying open source isn‚Äôt better. I‚Äôm just saying that people are not being unethical or anything when they don‚Äôt open source stuff.",
          "score": 1,
          "created_utc": "2025-12-24 19:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu9jq6",
          "author": "power97992",
          "text": "Where is the release lol?¬†",
          "score": 1,
          "created_utc": "2025-12-25 07:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwhisz",
          "author": "power97992",
          "text": "Weights?",
          "score": 1,
          "created_utc": "2025-12-25 18:16:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyz9rm",
              "author": "Responsible_Fig_1271",
              "text": "Seems like a big bucket of fail.",
              "score": 1,
              "created_utc": "2025-12-26 03:47:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzmvkn",
                  "author": "power97992",
                  "text": "Yeah, still no weights",
                  "score": 1,
                  "created_utc": "2025-12-26 07:00:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzylcx",
          "author": "Wise_Evidence9973",
          "text": "Merry Christmas!   \n[https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)",
          "score": 1,
          "created_utc": "2025-12-26 09:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0vo8u",
              "author": "Responsible_Fig_1271",
              "text": "Thank you! I stand corrected!",
              "score": 2,
              "created_utc": "2025-12-26 13:58:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvr5xv3",
          "author": "Southern_Sun_2106",
          "text": "It's GLM 4.5 Air all over again.",
          "score": 1,
          "created_utc": "2025-12-24 18:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpmbhw",
          "author": "Majestic_Appeal5280",
          "text": "the official minimax on twitter said they will be open sourcing in 2 days. probably on Xmas?",
          "score": 0,
          "created_utc": "2025-12-24 13:04:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvponxy",
          "author": "HumanDrone8721",
          "text": "Things may or may not happen, my 24TB HDD is slowly filling up and then *\"Molon Labe\"*.",
          "score": -1,
          "created_utc": "2025-12-24 13:20:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpew2l",
          "author": "Cergorach",
          "text": "Maybe they used a LLM to generate the website texts and it gave some unwanted output... ;)",
          "score": -6,
          "created_utc": "2025-12-24 12:06:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpjeme",
          "author": "SelectionCalm70",
          "text": "Nothing wrong in making money",
          "score": -7,
          "created_utc": "2025-12-24 12:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpojkq",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/6n9wzr2fk59g1.png?width=2927&format=png&auto=webp&s=c35aa063c179e2a5f1e9be23496f3385df958f32\n\ncan't wait",
          "score": -6,
          "created_utc": "2025-12-24 13:19:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpfggf",
          "author": "AlwaysLateToThaParty",
          "text": "Maybe they think the chip shortage is going to bite local inference, and increase the number of people who will require cloud services.",
          "score": -7,
          "created_utc": "2025-12-24 12:11:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv2cnz",
      "title": "All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there won‚Äôt be much ‚Äúlocal‚Äù about this sub unless the paradigm shifts to smaller models good at specific domains.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/",
      "author": "LocoMod",
      "created_utc": "2025-12-25 01:34:36",
      "score": 239,
      "num_comments": 243,
      "upvote_ratio": 0.8,
      "text": "It‚Äôs happening very openly but very subtly. The champions of open weight models are slowly increasing their sizes to the point a very small portion of this sub can run them locally. An even smaller portion can run them as benchmarked (no quants). Many are now having to resort to Q3 and below, which will have a significant impact compared to what is marketed. Now, without any other recourse, those that cannot access or afford the more capable closed models are paying pennies for open weight models hosted by the labs themselves. This is the plan of course.\n\nGiven the cost of memory and other components many of us can no longer afford even a mid tier upgrade using modern components. The second hand market isn‚Äôt fairing much better.\n\nThe only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nThe only way most of us will be able to run models locally will be to fine tune, crowd fund, or ‚Ä¶ ? smaller more focused models that can still remain competitive in specific domains vs general frontier models.\n\nA capable coding model. A capable creative writing model. A capable math model. Etc.\n\nWe‚Äôre not going to get competitive local models from ‚Äúwell funded‚Äù labs backed by Big Co. A distinction will soon become clear that ‚Äúopen weights‚Äù does not equal ‚Äúlocal‚Äù.\n\nRemember the early days? Dolphin, Hermes, etc.\n\nWe need to go back to that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pv2cnz/all_of_the_major_open_weight_labs_have_shifted_to/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvt6bz4",
          "author": "Freonr2",
          "text": "Did you miss Qwen3?  They produced about half dozen models between 0.6B and 32B, and there are countless quant options.  They're great models for their size.",
          "score": 138,
          "created_utc": "2025-12-25 01:59:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtukif",
              "author": "Amgadoz",
              "text": "Gemma as well, whose biggest model is 27B",
              "score": 21,
              "created_utc": "2025-12-25 05:09:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvni0v",
                  "author": "LocoMod",
                  "text": "That model was released when dinosaurs were still alive in AI terms. It was definitely a banger for its time though.",
                  "score": 3,
                  "created_utc": "2025-12-25 15:15:24",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuavk1",
              "author": "power97992",
              "text": "U cant compare an 8b or 30b a3b ¬† model to the top ¬†open models like v3.2 and kimi k2 ‚Ä¶ It is night and day¬†",
              "score": -12,
              "created_utc": "2025-12-25 07:44:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvufxwk",
                  "author": "fozid",
                  "text": "Nobody is trying to",
                  "score": 16,
                  "created_utc": "2025-12-25 08:38:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvknp3",
                  "author": "reginakinhi",
                  "text": "That's kind of the point...",
                  "score": 2,
                  "created_utc": "2025-12-25 14:56:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtastb",
          "author": "MerePotato",
          "text": "Mistral literally just dropped a family of models capping out at 14b",
          "score": 163,
          "created_utc": "2025-12-25 02:33:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdv81",
              "author": "ttkciar",
              "text": "Yep, exactly this.  They're not the only ones, either.\n\n* Google released Gemma3 in 12B and 270M (!!) sizes,\n\n* Qwen released Qwen3 in 0.6B, 1.7B, 4B, 8B, and 14B,\n\n* Microsoft released Phi-4 in 4B and 14B,\n\n* AllenAI released Molmo2 in 4B and 8B,\n\n* Nvidia released Nemotron-Cascade in 8B and 14B,\n\n* like you said, Mistral released Ministral 3 in 3B, 8B, and 14B.\n\n.. and there are probably others I'm forgetting.\n\nThat having been said, we shouldn't conflate the **current state** of the technology with **trends** in the technology, which is a frequent peeve of mine.  Maybe even though ample small models are being released *now*, the *trend* is toward larger models?  But I'm not sure if that's even the case.\n\nFiguring out if there is such a trend would require curve-fitting the relationship of small models published in a given year, or something, which would be straightforward enough except I don't know how to easily extract the raw data from Huggingface, and I'm too lazy right now to do it the hard way, **and** that would only give us a few points from which to extrapolate, since this entire scene is only a few years old.\n\nOverall, though, I'm not too worried.  Even if reasonable-sized models start to get scarce, the community has ample means to downscale weights of larger models, upscale toy-sized models we are able to train from scratch, and/or retrain older small models.  Besides, I'm expecting AI Winter to come crashing down in 2027 or 2028 anyway.",
              "score": 97,
              "created_utc": "2025-12-25 02:56:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvux6wi",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 11,
                  "created_utc": "2025-12-25 11:46:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvuqk47",
                  "author": "Hoblywobblesworth",
                  "text": "I'm excited for the AI winter if it means GPU and RAM prices stop increasing. Prices are going to come down right?....right? ...",
                  "score": 4,
                  "created_utc": "2025-12-25 10:35:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv6ryb",
                  "author": "ramendik",
                  "text": "Yup, you forgot IBM. Granite 4 tops out at 32B A9B and is pretty capable. I hope someone finally does a head-to-head with the new Nemotron as the architectures are similar (Mamba2 hybrid)",
                  "score": 4,
                  "created_utc": "2025-12-25 13:14:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvvpidf",
                  "author": "LocoMod",
                  "text": "You clearly missed the point. None of those models are competitive. Yes they can do simple things and raise the floor for a lot of people. But you are FAR from the ceiling. This is why the entire point of my post is that when you look at the recent releases, you clearly see a trend where params are getting larger and larger.\n\nGoogle and Mistral are not going to release a small model that has parity with their paid offerings in relevant use cases. Even if they could. That would significantly cut demand for their paid services.\n\nThe Chinese labs are now following the same playbook. Qwen, DeepSeek and GLM all released general >100B param models while concurrently bootstrapping their paid API services. Throw in a CLI coding agent in there as well. This was always the plan.\n\nNo one here is going to release anything of value on the models you listed. They are toys. None of those models are suitable for a production environment with paid users.\n\nI love playing with them though.",
                  "score": 7,
                  "created_utc": "2025-12-25 15:28:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nvvx6tr",
                  "author": "AuspiciousApple",
                  "text": "Is the 270M any good?",
                  "score": 1,
                  "created_utc": "2025-12-25 16:15:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvujgek",
              "author": "Rainbows4Blood",
              "text": "Wait, there are new Mistral models?",
              "score": 2,
              "created_utc": "2025-12-25 09:17:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvuq42n",
                  "author": "MerePotato",
                  "text": "Yup, there's a new line of Ministral models",
                  "score": 3,
                  "created_utc": "2025-12-25 10:30:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw17swz",
              "author": "power97992",
              "text": "It seems like Mistral models are way worse than minimax and qwen‚Ä¶ if open weight companies only release large models and ¬† dont release a sub 110b model ¬†that is 3-5 months behind the top models , there is no point even upgrading a computer for that cost , you are better off just using the api‚Ä¶",
              "score": 0,
              "created_utc": "2025-12-26 15:13:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1a9ge",
                  "author": "MerePotato",
                  "text": "Mistral models don't push the frontier but they're consistently much less sloppy and censored than US and CN releases making them a lot more pleasant to actually talk to and use",
                  "score": 2,
                  "created_utc": "2025-12-26 15:27:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtcisp",
              "author": "LocoMod",
              "text": "That excel at nothing. They dangled a carrot so you can pay for the real model via their API.",
              "score": -36,
              "created_utc": "2025-12-25 02:46:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtensr",
                  "author": "MerePotato",
                  "text": "They're vastly superior to the \"real model\" in terms of day to day usability imo, and that \"real model\" is also open weights",
                  "score": 15,
                  "created_utc": "2025-12-25 03:02:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtjlg9",
                  "author": "Monkey_1505",
                  "text": "No this is the wrong analysis. Nobody is sure if, in the long term, cloud API access will be the dominant form of AI or not. Smaller models have been accelerating fast, and local hardware improving. Making small models and continuing to is a hedge in case cloud ends up being more niche than expected.\n\nMost modern models, even very large API based propriety AI, are not really products yet. In that they don't actually make profit. They are tech demos as people aim toward some future thing. This is why people do local, because no one can predict with certainty the exact course of the future of technology, and companies want to have fingers in all pies, so they don't get caught with their pants down.",
                  "score": 12,
                  "created_utc": "2025-12-25 03:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt7k0i",
          "author": "wolttam",
          "text": "There's gonna continue to be interest in developing generalist models that can run on the smallest devices possible (phones).",
          "score": 21,
          "created_utc": "2025-12-25 02:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwq7v8",
              "author": "hoshitoshi",
              "text": "Also don't discount demand from enterprises who for both data sovereignty and cost reasons (100s of billions of tokens per week) will be very interested in local models.",
              "score": 2,
              "created_utc": "2025-12-25 19:06:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx3bap",
                  "author": "jklre",
                  "text": "The token tax is real. The clouds quad dipping on token charges for large embeddings for documents. Tokens to embed, storage fees, input tokens + embedded tokens and output tokens.",
                  "score": 1,
                  "created_utc": "2025-12-25 20:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt3t9l",
          "author": "StardockEngineer",
          "text": "‚ÄúWe‚Äù aren‚Äôt getting back to anything.  We‚Äôve been completely at the mercy of these companies this whole time.   How do you propose we do anything without them?",
          "score": 81,
          "created_utc": "2025-12-25 01:41:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtz3mq",
              "author": "ttkciar",
              "text": "We definitely have options.  I wrote something about some of the ways we can advance open-weight models without corporate help here -- https://old.reddit.com/r/LocalLLaMA/comments/1os1qf1/debate_16gb_is_the_sweet_spot_for_running_local/nnw33r0/",
              "score": 4,
              "created_utc": "2025-12-25 05:49:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtjkul",
              "author": "grady_vuckovic",
              "text": "Almost as if making oneself dependent on AI tools makes oneself dependent on tech companies.",
              "score": 5,
              "created_utc": "2025-12-25 03:41:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt56q0",
              "author": "LocoMod",
              "text": "It all starts with the data. That‚Äôs the real moat. Shifting focus to building open source tools that can build the datasets and making those datasets easier to discover is a start.\n\nFunding and training a 32B model is the lesser problem.\n\nEasier said than done of course.",
              "score": -23,
              "created_utc": "2025-12-25 01:51:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvt5jey",
                  "author": "StardockEngineer",
                  "text": "No, it takes a LOT of power to even train a 32B model.  And it takes a ton of storage to store the data.  Both will be extremely expensive.  \n\nYou‚Äôre talking an absurd amount of money per training run (and then subsequent evaluations).  And it‚Äôll take many runs to dial it in.",
                  "score": 41,
                  "created_utc": "2025-12-25 01:53:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvt6som",
                  "author": "-p-e-w-",
                  "text": "> It all starts with the data. That‚Äôs the real moat.\n\nNo, the moat absolutely is compute.\n\nIIRC, Phi-4 (14 billion parameters) cost around $2 million to train. That‚Äôs one small model, and it would be a moonshot for a global crowdfunding project.",
                  "score": 29,
                  "created_utc": "2025-12-25 02:03:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt4iv3",
              "author": "AppealSame4367",
              "text": "You can rent a gpu server for 200-400$ per month somewhere and train your own model. That's what you can do.",
              "score": -20,
              "created_utc": "2025-12-25 01:46:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvt4yd8",
                  "author": "StardockEngineer",
                  "text": "Somewhere?  A good model?  If it‚Äôs so easy, where are all these models?  How long does it take?  Where do you put the training data?   Come on.",
                  "score": 21,
                  "created_utc": "2025-12-25 01:49:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvt78o2",
                  "author": "-p-e-w-",
                  "text": "An A100 server costs around $1.50/hour (more than your estimate) and it would take years to train even a small model with it.",
                  "score": 14,
                  "created_utc": "2025-12-25 02:06:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt5hk1",
          "author": "quiteconfused1",
          "text": "Functiongemma was literally released last week.\n\nllama, Kimi, mistral, GLM, Qwen, Gemma, GPT-OSS all had major improvements this past [year.Like](http://year.Like) seriously; I use local models more than i use \"big models\". Infact im and training a gpt-oss-120b right now. \n\nNext year is going to the be the year of the humanoid foundational model.   \n  \nlocals arent going anywhere ...",
          "score": 138,
          "created_utc": "2025-12-25 01:53:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvto49a",
              "author": "dolche93",
              "text": "You're implying that gpt-oss-120b is a small local model. OP explicitly is talking about models that can fit on single or double GPU builds.\n\n I'm not sure the two of you are on the same page about what a small model is. Not that either of you are wrong, just not on the same page.",
              "score": 36,
              "created_utc": "2025-12-25 04:17:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtqg1i",
                  "author": "Veloder",
                  "text": "It's a MoE model, it can fit fine in a consumer GPU (16GB VRAM is enough) as long as the system has enough RAM (about 64GB).",
                  "score": 30,
                  "created_utc": "2025-12-25 04:36:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvv5gzu",
                  "author": "quiteconfused1",
                  "text": "To answer your question about if gpt-oss-120b is small or not. No it's not small, but I am still training it on a single computer. Namely the Thor.\n\n128gb of unified ram.\n\nIf you can get it training on a machine that is 2800 bucks ( right now on sale ) ... That counts.",
                  "score": 2,
                  "created_utc": "2025-12-25 13:03:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvwu5bk",
                  "author": "mycall",
                  "text": "I have gpt-oss-120b running on my gpd win 5 (40tk/s).  You can't get smaller than that.   \n\nWhat people are really talking about is affordability.",
                  "score": 2,
                  "created_utc": "2025-12-25 19:29:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt57j5",
          "author": "robberviet",
          "text": "SLM is always needed, especially for mobile and local simple usage like tab completion. However it is totally depends on the big tech to release them or not. My opinion is yes, they will. OSS always exists. It costs big tech nothing to do that.",
          "score": 10,
          "created_utc": "2025-12-25 01:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt852c",
          "author": "complains_constantly",
          "text": "We will do great because of downstream distillation, which has become the dominant meta. Distilling from a larger model (which we are getting in spades thanks to DeepSeek, Qwen, Z.ai, Minimax, Moonshot, etc) has been shown to be significantly more powerful than training a small model from scratch. So much so that the latter idea has been abandoned by any organization serious about this stuff.",
          "score": 19,
          "created_utc": "2025-12-25 02:13:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvw3h30",
              "author": "milkipedia",
              "text": "I'm specifically looking for more distills from these new big models. If I had any idea how to do this, I'd do it myself.",
              "score": 1,
              "created_utc": "2025-12-25 16:52:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwe7jn",
                  "author": "complains_constantly",
                  "text": "It's pretty difficult and expensive to do yourself. Doing so is out of reach of us consumers, but its an order of magnitude or two cheaper than training from scratch for the labs. They're pretty incentivized to train models this way.",
                  "score": 1,
                  "created_utc": "2025-12-25 17:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu8mbv",
          "author": "beedunc",
          "text": "And just in time for RAM to be impossible to buy.",
          "score": 7,
          "created_utc": "2025-12-25 07:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5f5p",
          "author": "1ncehost",
          "text": "The reason is the latest techniques make it easy for anyone to train from scratch a decent specialized model. Im not even talking fine tuning, Im talking the whole shebang.\n\nNanogpt speed runs are down to under 3 minutes and under $10 all in from scratch to 3.2 loss on fineweb. If you're training a specialized model you can get into the 1.X loss in barely any time now.\n\nSimply put there is no business model here any longer for the models themselves. You have to make a specialized model as part of a larger specialized service now.",
          "score": 14,
          "created_utc": "2025-12-25 01:53:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt8fxk",
              "author": "CKtalon",
              "text": "Nanogpt speedruns are cool, but the models produced aren‚Äôt going to be useful (100+m decoder models aren‚Äôt great, but 100+m encoder models are okay). The possibly interesting ones are the d32 (1B) nanochat models which are probably $800.",
              "score": 9,
              "created_utc": "2025-12-25 02:15:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt8cji",
              "author": "Aggressive-Bother470",
              "text": "So the next paradigm is build and train it yourself?¬†",
              "score": 2,
              "created_utc": "2025-12-25 02:15:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtayc3",
          "author": "simism",
          "text": "Billions must scale",
          "score": 7,
          "created_utc": "2025-12-25 02:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwezj1",
          "author": "JacketHistorical2321",
          "text": "Large models are still local models dude. The sub isn't called, \"LocalModLLama\". If you or others can't run it local, it didn't mean some can.¬†",
          "score": 6,
          "created_utc": "2025-12-25 18:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtjef0",
          "author": "Monkey_1505",
          "text": "No, that isn't happening at all.  \n\nCompanies will not want to give up on local, it's effectively a hedged bet against big cloud APIs. Microsoft is doing it. Google is doing it. Qwen is going it. \n\nNow finetunes, yes that is happening a bit less. But it hasn't stopped either.",
          "score": 11,
          "created_utc": "2025-12-25 03:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvetoa",
              "author": "LocoMod",
              "text": "That‚Äôs not what my post is about.",
              "score": 1,
              "created_utc": "2025-12-25 14:15:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvtxek",
                  "author": "Monkey_1505",
                  "text": "\"All of the major open weight labs have shifted to large params general models instead of smaller, more focused models.¬†\"\n\nThey have not done this. Google, Qwen, Microsoft are still very focused on small models, and they are all surely major open weight labs, no?",
                  "score": 1,
                  "created_utc": "2025-12-25 15:55:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt6nns",
          "author": "YouAreTheCornhole",
          "text": "Everyone here is about to become a fan of Nemotron",
          "score": 20,
          "created_utc": "2025-12-25 02:02:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtfb43",
              "author": "JLeonsarmiento",
              "text": "Nemotron 30b feels very good in the chat, but I cannot make it work in any of my coding agents (cline, QwenCode, Vibe).\n\nAnd I‚Äôm not completely sure it‚Äôs significantly better than gpt-oss 20b. According to benchmarks it should,,, but I just don‚Äôt feel it.",
              "score": 11,
              "created_utc": "2025-12-25 03:07:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtgxvq",
                  "author": "RMCPhoto",
                  "text": "It's probably the prompting strategy.  I have no doubt it's very smart, but my results have also been inconsistent.   My guess is that it's the same old story.   The training data instills a certain syntax / language / prompt structure that differs from the norm slightly.  Could even be a very tiny variation that propagates an error.  Newer models have been more tolerant to this compared with the earlier llamas...where adding a space before the first word would increase the error by 40% and similar other black box ??? \n\nThis is honestly my biggest frustration.   I'm very thankful that openai released such clear cookbook content for prompt formatting.  Truly, every model designer should take note.  Clear documentation is such a massive booster for adoption, public opinion and end user success.  \n\nEven better if that documentation is instilled into a meta prompt for prompt refinement.",
                  "score": 11,
                  "created_utc": "2025-12-25 03:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtolr1",
              "author": "Foreign-Beginning-49",
              "text": "Im still trying to figure out my params but its a beast.",
              "score": 0,
              "created_utc": "2025-12-25 04:21:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvt7w5r",
          "author": "sirfitzwilliamdarcy",
          "text": "We will get back to that. The process for people creating their own flavor of models just needs to be democratized. We‚Äôve had heroes like TheBloke, NousResearch and many smaller contributors on hugging face who used to keep the community alive. But I still feel that there is a group of people who are hungry for diverse models that all have different vibes. And that demand will have to be met.",
          "score": 5,
          "created_utc": "2025-12-25 02:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtvwdi",
          "author": "BidWestern1056",
          "text": "i've been developing mainly tooling but have been working on some fine tunes. I've already released a couple more focused on divergent generation to help models come up with new ideas that are genuinely more novel\n\n[hf.co/npc-worldwide](http://hf.co/npc-worldwide)\n\nin the next few months i'm going to be focusing a bit more on some specialized local models so hoping to have more to share. building and training these using my [npcpy](https://github.com/npc-worldwide/npcpy) tools. gonna make a research coding model that doesnt overly comment or unnecessarily add exception handling, prolly one specialized for [npcsh](https://github.com/npc-worldwide/npcsh). i'm likely gonna make one for [lavanzaro.com](http://lavanzaro.com) (rn its just gem 2.5 flash) and in [npc studio](https://github.com/npc-worldwide/npc-studio) my intention is that it will be trivial for users to set up fine tunes for a given persona based on user-labeled data. \n\nI also write [fiction](https://www.amazon.com/Dont-turn-sun-giacomo-catanzaro/dp/B0DMWPGV18) so planning to make it easier to do more creative writing style clones",
          "score": 5,
          "created_utc": "2025-12-25 05:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvc759",
              "author": "LocoMod",
              "text": "Nice. You‚Äôre staying busy. Keep it up. üëç",
              "score": 2,
              "created_utc": "2025-12-25 13:56:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvgrjj",
                  "author": "BidWestern1056",
                  "text": "we gotta win üòÅ",
                  "score": 2,
                  "created_utc": "2025-12-25 14:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw2173r",
              "author": "gardenia856",
              "text": "The main point: your approach is exactly what this sub needs if local is going to stay relevant.\n\nDivergent-gen finetunes are underrated; most people chase benchmarks instead of ‚Äúcan this model actually surprise me without going off the rails.‚Äù If you publish even rough evals on novelty vs. repetition for your [hf.co/npc-worldwide](http://hf.co/npc-worldwide) stuff, that alone would be super useful for others trying to clone the approach.\n\nFor the research coder: one trick that worked well for me was tagging training examples by ‚Äústyle‚Äù (minimalist, verbose, safety-obsessed) and forcing the system prompt to pick a style token first. That way you can later extend into a safety-heavy variant without retraining everything. Same idea could map to your fiction clones: label passages by voice, pacing, and point of view so your fine-tunes can switch modes instead of hard-baking one persona.\n\nIf you ever wire these agents into real data, platforms like PostgREST or Hasura, plus something like DreamFactory for auto-REST over legacy SQL, make it easier to keep your models dumb about SQL but smart about the domain.\n\nThe main point: those small, weird, opinionated models are where local actually wins, so keep leaning into that niche.",
              "score": 1,
              "created_utc": "2025-12-26 17:51:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvt95qa",
          "author": "Klutzy-Snow8016",
          "text": "Technology moves on. Pseudo-standard sizes for open models used to be small 8B, medium 32B, large 70B+. Now it seems to be small 30B, medium 110B, large 230B+.\n\nAt least now they're MoEs, so you can run them at reasonable speed with low VRAM. A 30B-A3B can generate at reading speed on a 10+ year old computer if you put in 16GB of RAM and an 8GB GPU, and the output is way better than, like, Mistral 7B, which was super-impressive at the time.",
          "score": 8,
          "created_utc": "2025-12-25 02:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtfad4",
          "author": "IrisColt",
          "text": "Did you just wake up from a year-long coma? Local models are more powerful and easier to access than ever.",
          "score": 14,
          "created_utc": "2025-12-25 03:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtfj2d",
              "author": "LocoMod",
              "text": "Did you miss the point of the post? Read it again. But this time don‚Äôt use an LLM to interpret it for you.",
              "score": -20,
              "created_utc": "2025-12-25 03:09:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvkynu",
                  "author": "IrisColt",
                  "text": ">don‚Äôt use an LLM to interpret it\n\n\nWhat the absolute f? Abliterated versions of Qwen3-VL 32B are powerhouses you could only have dreamed of in late 2024. And local.",
                  "score": 3,
                  "created_utc": "2025-12-25 14:58:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt6508",
          "author": "Pvt_Twinkietoes",
          "text": "And how you propose we get there?",
          "score": 6,
          "created_utc": "2025-12-25 01:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5wc8",
          "author": "gradient8",
          "text": "I don‚Äôt disagree but this post feels weirdly entitled. We are not customers, open weight models cost millions to develop for us to get for free",
          "score": 24,
          "created_utc": "2025-12-25 01:56:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtajvh",
              "author": "LocoMod",
              "text": "The point of the post is that the recent open weight models are not free because even if you own a high end system, like I do, we still can‚Äôt run them. I have an RTX5090 build easily worth 5k in components. I cannot run any of the best models released this year without severely lobotomizing it with a tiny quant.\n\n And I‚Äôm not entitled to anything. The entire point of the post is that very soon none of use are going to be able to run free models because the businesses releasing them (and they ARE businesses), are deliberately going to monetize you when you can‚Äôt run their model but you CAN use their API at 10% the cost of OpenAI.",
              "score": 4,
              "created_utc": "2025-12-25 02:31:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtgb5y",
                  "author": "AmuletOfNight",
                  "text": "I just don't feel like they're doing that on purpose. These models use a lot of power and take a lot of computation. Maybe the industry is going towards larger models because that's what gets better performance and people like better performance. You can't help the fact that bigger models tend to perform better.",
                  "score": 9,
                  "created_utc": "2025-12-25 03:15:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvtk4zv",
                  "author": "svankirk",
                  "text": "I think you're not wrong, but take a look at the hardware that is becoming available in the last 6 months that is capable of running 100b models on your desktop for 5K or less. I think that's going to keep getting better.",
                  "score": 3,
                  "created_utc": "2025-12-25 03:45:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtkv87",
          "author": "Whole-Assignment6240",
          "text": "Are distillation techniques the answer for specialized small models?",
          "score": 3,
          "created_utc": "2025-12-25 03:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvdrzw",
              "author": "LocoMod",
              "text": "I don‚Äôt know. Are they?",
              "score": 1,
              "created_utc": "2025-12-25 14:08:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu28if",
          "author": "Sensitive_Sweet_1850",
          "text": "It‚Äôs happening very openly but very subtly. The champions of open weight models are slowly increasing their sizes to the point a very small portion of this sub can run them locally. An even smaller portion can run them as benchmarked (no quants). Many are now having to resort to Q3 and below, which will have a significant impact compared to what is marketed. Now, without any other recourse, those that cannot access or afford the more capable closed models are paying pennies for open weight models hosted by the labs themselves. This is the plan of course.\n\nGiven the cost of memory and other components many of us can no longer afford even a mid tier upgrade using modern components. The second hand market isn‚Äôt fairing much better.\n\nThe only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nThe only way most of us will be able to run models locally will be to fine tune, crowd fund, or ‚Ä¶ ? smaller more focused models that can still remain competitive in specific domains vs general frontier models.\n\nA capable coding model. A capable creative writing model. A capable math model. Etc.\n\nWe‚Äôre not going to get competitive local models from ‚Äúwell funded‚Äù labs backed by Big Co. A distinction will soon become clear that ‚Äúopen weights‚Äù does not equal ‚Äúlocal‚Äù.\n\nRemember the early days? Dolphin, Hermes, etc.\n\nWe need to go back to that.",
          "score": 5,
          "created_utc": "2025-12-25 06:18:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt57vp",
          "author": "misterflyer",
          "text": "It's inevitable. Especially since this space is so heavily dominated by ***benchmark hype and benchmaxxing***. With the big proprietary AI providers chasing each other for higher and higher benchmarks every 3 months, and bloating the sizes of their new models... it's just a cat & mouse game that even the popular open weights providers aren't immune from getting sucked into.\n\nNgl, I don't care about benchmarks. At best, I take them with a grain of salt. All I care about is... *does this new model work great for my use case or not?* And if I can't even run the model load the model to my VRAM+RAM, then the model in question is pretty much irrelevant to me regardless of what the benchmarks say.\n\nDon't get me wrong, I understand why most other people do care about benchmarks.  But if that's the most important thing that matters to the average person here then get ready for a future of 10 trillion parameter models that you can't even dream of running locally. **Then, the best models will only be available to most people here via API or subscription which completely defeats the purpose of the \"LocalLLaMA\" label.  But, that's exactly where we're headed rn.**\n\nBut s/o to Mistral for continuing to produce models of reasonable sizes. I know ppl like to shi- on their benchmark scores, but again, at least a decent proportion of people here can actually run most of their models above Q3.",
          "score": 12,
          "created_utc": "2025-12-25 01:51:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbefo",
              "author": "LocoMod",
              "text": "Thank you. Someone got the point.",
              "score": 11,
              "created_utc": "2025-12-25 02:38:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtxj30",
                  "author": "toothpastespiders",
                  "text": "I get the impression that only a couple people bothered to fully read through your post before getting angry at your conclusion. Taken on a point by point basis you didn't even say anything especially controversial.",
                  "score": 9,
                  "created_utc": "2025-12-25 05:35:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt9o5j",
          "author": "Hunting-Succcubus",
          "text": "There is wan 5b model, zimage 6b model, smaller qwen and gemma llm. Latest TTS  model are mostly small. What else you want? Leave poor multi billion AI companies from usa alone. look into chinese ai for small models.",
          "score": 4,
          "created_utc": "2025-12-25 02:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdc11",
              "author": "LocoMod",
              "text": "Those models are toys. I can build a shitty system for >$2000 to run the small shitty Chinese model or pay a few cents when I need to produce something of quality with western frontier models. \n\nI play with all those Chinese model. Tons of fun. Would never ship a product with them. Only play with them. That‚Äôs all they are good for.",
              "score": 5,
              "created_utc": "2025-12-25 02:52:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvte72s",
                  "author": "Hunting-Succcubus",
                  "text": "well you have to ship product with western subscription api. western don't allow commercial usage with their local model. keep shipping plastic skin, butt chin portrait with very high western cost. wester ai produces reliable  premium quality plastic skin, butt chin face, laying on gross figures. western ai has high quality knowledge of human anatomy, number of fingers. they are very reliable and not hallucinate. shitty z image always create 7 finger, low res bluryy texture ASK ANYONE. HYPOCRISY",
                  "score": 3,
                  "created_utc": "2025-12-25 02:59:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtrfjn",
          "author": "thebadslime",
          "text": "Nanbeige 3B and RJN 1 JUST LAUNCHED. You're catastrophizing.",
          "score": 4,
          "created_utc": "2025-12-25 04:43:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvcnn1",
              "author": "LocoMod",
              "text": "What? Did those models just make up a word?\n\nHard pass.",
              "score": 1,
              "created_utc": "2025-12-25 13:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtbcju",
          "author": "One-Employment3759",
          "text": "Nah, if you are not doing local that's a choice you are making.\n\n\nLocal or die!",
          "score": 5,
          "created_utc": "2025-12-25 02:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt3cvq",
          "author": "AppealSame4367",
          "text": "By this time next year 256 GB unified RAM / VRAM will be normal.\n\nEdit: What do you guys expect? Run newest tech (local llms..) on budget hardware? Of course it will cost something if you still wanna catch up to newest developments in December 2026.\n\nUntil then the software tech around llms will keep developing too. I am very pleased with Mistral Ministral 3B 2512. It's fast, smart enough and a good daily assistant on my RTX 2060 laptop gpu. But of coure I won't be able to run SOTA OSS models with this laptop in 2026 - apart from those small models that might be even faster, smarter and agentic by then.",
          "score": 8,
          "created_utc": "2025-12-25 01:37:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvt44xk",
              "author": "LocoMod",
              "text": "The cost will be outside of the range for the majority of people. And for those that cannot access afford the 2 to 3x markup unless supply changes, now we will have to choose between an AI system with unified memory or a high end PC with discreet graphics for gaming and other professional work or compute heavy hobbies.",
              "score": 10,
              "created_utc": "2025-12-25 01:43:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvt3pzv",
              "author": "Linkpharm2",
              "text": "*8gb laptop and most popular gpu in the distance*",
              "score": 15,
              "created_utc": "2025-12-25 01:40:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu0mj3",
                  "author": "True_Requirement_891",
                  "text": "meirl 32gb ram + 8gb 3070ti with Nemotron 30b at 100k+ context and 20tps slaps though",
                  "score": 4,
                  "created_utc": "2025-12-25 06:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvt5qju",
              "author": "DesperateAdvantage76",
              "text": "Local memory capacity is not keeping up with even dated models. The days of buying 4 gpus to fit high end models is over. Local models will likely be used for basic tasks with more complex tasks offloaded to servers.",
              "score": 8,
              "created_utc": "2025-12-25 01:55:30",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtyutu",
              "author": "WitAndWonder",
              "text": "Where are you possibly getting all that RAM when it's not going to be in public circulation?",
              "score": 2,
              "created_utc": "2025-12-25 05:47:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvt3mjd",
              "author": "IndianaCahones",
              "text": "For $1,600 if there is any available on the market.\n\nEdit: have to add an edit because this is no longer close to resembling the comment I responded to. As of today, 25 December 2026, 96GB is running around $800 in the U.S.",
              "score": -2,
              "created_utc": "2025-12-25 01:39:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvubr8e",
          "author": "__Maximum__",
          "text": "There is also qwen3 next paradigm, which is less widespread but is very promising.",
          "score": 2,
          "created_utc": "2025-12-25 07:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv6gea",
              "author": "LocoMod",
              "text": "You mean gpt-oss paradigm?",
              "score": 1,
              "created_utc": "2025-12-25 13:12:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvho5i",
                  "author": "__Maximum__",
                  "text": "Qwen next 80b has 3b active params. I can run that thing on my CPU. Plus, it's got things like hybrid attention and mutli-token prediction.\n\nI hope we see something like 100b2b and 1b50b with even more ultra-efficient features.",
                  "score": 1,
                  "created_utc": "2025-12-25 14:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvuctx7",
          "author": "PotentialFunny7143",
          "text": "Big local models will be small local models when the AI bubble will pop",
          "score": 2,
          "created_utc": "2025-12-25 08:05:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuktz1",
          "author": "muntaxitome",
          "text": "In my opinion you have it backwards, the models we have gotten this year are now so much better at 8B-32B to the point where there is limited use to these finetunes anymore. Like a year ago coding with a 32B wasn't much fun at all, now it's a legit possibility. \n \nDoing finetunes that would beat qwen3 32B at anything relevant is going to be tough.",
          "score": 2,
          "created_utc": "2025-12-25 09:32:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvamk0",
          "author": "Odd_Lengthiness_2175",
          "text": "M5 Mac Studio comes out summer 2026 and I'm optimistic we'll finally have a prosumer-level (\\~$10-$15K) device that can run larger models at speeds sufficient for a single user, and not just toys.  Add to that the massive improvement in small model quality we saw this year and I think we may find ourselves a lot less dependent on huge models running on someone else's hardware in a data center.",
          "score": 2,
          "created_utc": "2025-12-25 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvfru3",
              "author": "LocoMod",
              "text": "I‚Äôm betting on Apple to win the long game when it comes to running models on device. Can‚Äôt wait for M5 MAX. Day zero purchase for me.",
              "score": 1,
              "created_utc": "2025-12-25 14:22:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw0g29r",
                  "author": "power97992",
                  "text": "But the m6 max will have even more ram and it should come out in 2026. At this rate even a single 1tb m5 ultra studio wont be able to future +2T models at q4/q8 at half or more context.. PEople with money will end up clustering mac studios to run massive models.",
                  "score": 1,
                  "created_utc": "2025-12-26 11:56:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvcdlr",
          "author": "No_Afternoon_4260",
          "text": "Good thing devstral 123B fits in a local-ish rig",
          "score": 2,
          "created_utc": "2025-12-25 13:57:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvg7cf",
              "author": "LocoMod",
              "text": "A rig capable of running it is still beyond what the majority of people in here can afford. I can run it on my M3 MAX with 128GB of unified memory. But that machine was ~6k.\n\nYou could build a PC with a 24GB-32GB GPU and tons of RAM but by the time you‚Äôre done pricing out components you‚Äôre already ~4K into your budget.\n\nI have both. But I‚Äôm not the average user either.",
              "score": 1,
              "created_utc": "2025-12-25 14:25:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvhx1q",
                  "author": "No_Afternoon_4260",
                  "text": "What speed do you get on your mbp?",
                  "score": 1,
                  "created_utc": "2025-12-25 14:38:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvdqbz",
          "author": "Confusion_Senior",
          "text": "The general models will always be on a large number of parameters but specialized models can be distilled with way less",
          "score": 2,
          "created_utc": "2025-12-25 14:07:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtc5gz",
          "author": "Available_Brain6231",
          "text": "I  believe in the next 5 years we(non-americans) will have our hands at lots of cheap used chinese server gpus, something on the level of a 40xx but with 192 vram or more.  \nThey will iterate quickly now and we will be able to buy old stuff like it was during the mining boom.\n\nI also think those Chinese labs need to keep making powerful big models, I rather not be able to run it now than never.",
          "score": 4,
          "created_utc": "2025-12-25 02:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtwlpw",
          "author": "toothpastespiders",
          "text": ">Remember the early days? Dolphin, Hermes, etc. We need to go back to that.\n\nI think in a sense that might be part of the problem. Lack of specialization in released models has probably driven a lot of us to make VERY specialized fine tunes. So specialized that they're essentially worthless outside our individual setup and needs. \n\nThat said, I find the amount of negative and outright angry replies to your post to be pretty weird. I don't think anything you said is especially controversial other than your conclusion.",
          "score": 4,
          "created_utc": "2025-12-25 05:27:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvc1vi",
              "author": "LocoMod",
              "text": "I‚Äôm exposing the long game and the folks playing it are upset. Get this sub hooked on the smaller model. Release benchmaxxed >100b model and slowly shuffle users to their paid API service. Now they are publicly seen as altruistic but ultimately the game is to lock in users to their paid platforms. And that‚Äôs fine. They are entitled to that.",
              "score": 1,
              "created_utc": "2025-12-25 13:55:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzb4bt",
                  "author": "ttkciar",
                  "text": "If people fall into the trap of paid API services, they have only themselves to blame.\n\nThe solution is to design one's applications to work within the capabilities of the models and hardware we have to work with.\n\nI realize that's not a very popular practice these days, but twenty+ years ago it was pretty standard to develop software against a specific platform (specific versions of libraries and other dependencies) and hardware.\n\nFrom what I can tell, programmers have stopped doing that mainly due to a lack of self-discipline, rather than because the new looser practices are actually better, and we've been paying for it with dependency hell, functional complexity, and technical debt.",
                  "score": 1,
                  "created_utc": "2025-12-26 05:17:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvteg5x",
          "author": "StardockEngineer",
          "text": "I can tell.  You have no idea what I‚Äôm talking about.  You have my arguments and intents completely twisted.",
          "score": 2,
          "created_utc": "2025-12-25 03:01:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu0gxn",
          "author": "ServersServant",
          "text": "Uh, you don‚Äôt need cutting edge models to run locally if you actually have good MCP servers and a decent model imo. You can get pretty damn far.\n\nYour thinking is the same of those kiddos believing they need the latest iPhone to‚Ä¶ send memes.¬†",
          "score": 2,
          "created_utc": "2025-12-25 06:02:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvub7mu",
              "author": "power97992",
              "text": "Which mcp server?",
              "score": 2,
              "created_utc": "2025-12-25 07:47:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxhlas",
                  "author": "ServersServant",
                  "text": "Depends on your use-cases. It's not like a magic MCP server exists for every need.",
                  "score": 1,
                  "created_utc": "2025-12-25 21:53:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvv8d6q",
              "author": "LocoMod",
              "text": "Anyone can claim anything. Your MCP setup is leagues behind what a frontier api service can do. Our expectations and QA are clearly not the same. If you value modern equivalent of TODO apps then I suppose your setup will work. If that‚Äôs the ceiling you aspire to reach then more power to you.",
              "score": 1,
              "created_utc": "2025-12-25 13:27:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvxko30",
                  "author": "ServersServant",
                  "text": "What kind of use-cases do you have in mind?\n\nMy point (and sorry if I sounded deprecating or rude, it wasn't my intention), you're over-estimating the need for cutting edge models. Newer models are mostly improving efficiency, experimenting newer architectures or upgrading knowledge and fine-tuning. AFAIK, there's nothing radically new.\n\nI got a self-hosted setup running an assistant over voice or messages. As long as I'm not in a very noisy environment, it understands pretty much everything. I'm working on a noise filter to improve use in noisy environments. It can search for things online, give me directions while cycling or driving, optimise my route given constraints I set up beforehand, schedule hotel reservations and flight tickets, trigger home automations (from simple stuff like temp control, watering plants\\*, etc., to actually getting groceries), summarise things and do a couple of other things.\n\nIt's surely not your Jarvis you'd maybe expect, but having it understand voice and reply either via synthetic voice or text, works for me great. This isn't even using super powerful models, just quantised Voxtral, MoE, Small 3.2 Instruct, some MCPs (for databases, search engines, etc.), Chroma, APIs, a couple of databases I built + good prompts. Synapse to have voice calls and messages over Element X. I can trigger the calling asking Siri to call a contact assigned to it, or send a message and I get \"called back\". Whole setup runs on my own servers at home and some cloud GPUs for the models on-demand, so it doesn't break my bank.\n\nWhen I said you can get pretty far I really mean it. This setup didn't took me years. I literally build it over six months and it's a hobby project, so it's no rush. Surely it helps me being a software engineer working in distributed systems for ML, but it's surely doable by most people if they got the commitment and like, basic integration software skills. I'm kinda sure kids are already vibe-coding more complex stuff than this.\n\nSo yeah, what's your use-case for cutting edge models?\n\nedit: fixed typo",
                  "score": 3,
                  "created_utc": "2025-12-25 22:11:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu28oz",
          "author": "brahh85",
          "text": ">A capable coding model. A capable creative writing model. A capable math model. Etc.\n\nthats literally what mistral is doing",
          "score": 3,
          "created_utc": "2025-12-25 06:18:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv84ng",
              "author": "LocoMod",
              "text": "Good.",
              "score": 1,
              "created_utc": "2025-12-25 13:25:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu967o",
          "author": "Cuplike",
          "text": "It's not local because I can't run it is a horrible mindset. I don't want any lab to stop publishing open weights for large models because \"Well, they can't run it anyway\"",
          "score": 3,
          "created_utc": "2025-12-25 07:26:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv6o8c",
              "author": "LocoMod",
              "text": "That‚Äôs cool. You can always pay for their cloud service at 1/8th of the cost of the big 3 frontier models and proclaim you run local on someone else‚Äôs computer.",
              "score": 1,
              "created_utc": "2025-12-25 13:13:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvk14n",
                  "author": "Cuplike",
                  "text": "You're right, a model is only useful if YOU can run it. Being able to store it when the company stops offering it officially, other providers having the ability to offer competitive pricing, being able to distill and finetune the model, these are all completely useless things",
                  "score": 2,
                  "created_utc": "2025-12-25 14:52:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt5p2w",
          "author": "QuailLife7760",
          "text": "Sure mfker you want openai/claude level product in 1B model, either you make one yourself or stfu.",
          "score": 3,
          "created_utc": "2025-12-25 01:55:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtcicg",
          "author": "UnnamedPlayerXY",
          "text": "There're sill many good releases that can be run locally but I guess time will tell. Personally I find it more worrisome that there seems to be less of a focus on bringing regular consumer grade hardware to where it should be in regards to the desired optimizations.\n\nThat the whole chain of production is essentially bottlenecked by a rather monopolized set-up is ofc. not helping the situation.",
          "score": 2,
          "created_utc": "2025-12-25 02:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv2126",
          "author": "Investolas",
          "text": "Reported for impersonating a mod!",
          "score": 2,
          "created_utc": "2025-12-25 12:33:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv5pp5",
              "author": "LocoMod",
              "text": "Merry Christmas! ‚ù§Ô∏è",
              "score": 0,
              "created_utc": "2025-12-25 13:05:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvvjd59",
                  "author": "Investolas",
                  "text": "You are not sure if anyone actually likes you",
                  "score": 1,
                  "created_utc": "2025-12-25 14:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu6yo0",
          "author": "relmny",
          "text": "I only read the title and... did you skip the whole 2025?\n\n\nThis is the best year for local LLMs!\n\n\nWe have everything! and multiple times with multiple improvements!\n\n\nWe're have you, and the ones that upvote you, be living in?",
          "score": 3,
          "created_utc": "2025-12-25 07:03:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv811j",
              "author": "LocoMod",
              "text": "That‚Äôs like asking ‚Äúwhere have you been the past decade?‚Äù. We can take some arbitrary time span and point to a 24B model that this sub could host. A year ago is a decade in AI time. Look at the past 3 months. The relevant labs all moved to >100B models you can‚Äôt run unless you have ~10k of gear. But don‚Äôt worry. They have all also bootstrapped a paid API service and vibe coded CLI agent as an alternative.",
              "score": 0,
              "created_utc": "2025-12-25 13:24:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvu7hxb",
          "author": "CanineAssBandit",
          "text": "I don't actually mind this at all. My issue is, was, and always will be, that closed source=I do not control access. \n\nOpen weights=**I control access**. The model is physically **MINE**, no company or entity can take it unless they physically steal my electronics.\n\nI emphasize this because \"woe is me, I have to have a server to run it\" is missing the most important thing about open weights. The \"and you can run it on your desktop\" part was just gravy.\n\nDon't get me wrong, I love when the models that run on my simple hardware become more useful, but that's a lot less important to me than \"**I have ownership of the same calibre of model as billion dollar companies**, and all I have to do is buy 5k of server gear to run it slowly, or rent server hours to run it quickly.\" I will choose unwieldy SOTA over convenient shortbus every second of every day.",
          "score": 2,
          "created_utc": "2025-12-25 07:09:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv783b",
              "author": "LocoMod",
              "text": "Cloud services generally have more uptime than PC‚Äôs. For what it‚Äôs worth. I don‚Äôt have a doomsday mentality so I don‚Äôt share your concerns.",
              "score": -1,
              "created_utc": "2025-12-25 13:18:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvw7j8x",
                  "author": "CanineAssBandit",
                  "text": "...Okay? I don't disagree, that's why I think \"ability to run locally\" is kind of a red herring in the open weights argument.\n\nYou're arguing in bad faith by calling my concerns a \"doomsday mentality,\" my issues have nothing to do with an apocalypse. \n\nClosed means I'm getting served whatever the hell the owner of the model wants to serve, and they can call it whatever they want, and change it whenever they want to whatever they want, even without telling me. *This is completely unacceptable* in any serious use case where details matter, or even basic ones like RP where it's super frustrating and creepy when suddenly they sound completely different but are wearing the same name tag.\n\nI want a product I can trust with performance I can depend on, and open weights gives me that transparency and control. Look at how much better GPT 5 was at launch, and how much it fell off a cliff not even a couple weeks later, all while still calling it GPT 5. And now they've done two shiny updates to 5.1 and 5.2 and it's still complete garbage compared to launch day GPT 5.\n\nMeanwhile, Deepseek R1 is still R1, Hermes 3 405B is still the same, everything open is still just chilling and you can use an old version forever if you really want to. Or you can evaluate and upgrade when YOU see fit for YOUR workflow. YOU get to make those choices instead of OAI or Anthropic choosing how to run your business or simulated people for you.",
                  "score": 2,
                  "created_utc": "2025-12-25 17:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtic2r",
          "author": "dsartori",
          "text": "There‚Äôs a lot of interest in ‚Äúedge‚Äù workloads generally I think. The variety of models 8B and under is really quite good. \n\nModel capabilities are far ahead of where they were a year ago and you could do useful production stuff with local models a year ago.",
          "score": 1,
          "created_utc": "2025-12-25 03:31:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtmhy3",
          "author": "john0201",
          "text": "M5 Ultra, whatever the next strix halo is will hopefully keep it feasible.",
          "score": 1,
          "created_utc": "2025-12-25 04:04:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvd86t",
              "author": "LocoMod",
              "text": "I‚Äôll be a day zero M5 MAX buyer. It can‚Äôt come soon enough.",
              "score": 1,
              "created_utc": "2025-12-25 14:04:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvusofg",
          "author": "a_beautiful_rhind",
          "text": "You're getting a lot of smalls in addition to the gigantor models. What's missing is the mid stuff. 30b turning into 120bA3 because it's \"faster\".\n\nNot sure that anyone released a \"creative writing\" model literally ever. Largest thing that counts as commercial effort is latitude games and they're not quite a lab. It's all agentic benchmaxx stem codeslop. GLM sees the stressful usecase, mistral is just french so their models still work but are far from creative focused. Definitely not in the way that you imply with specialization. \n\nAnd honestly, local was doing fine until the artificial shortage. Old xeon/epyc and DDR4 were bountiful.",
          "score": 1,
          "created_utc": "2025-12-25 10:58:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuvglb",
          "author": "yopla",
          "text": "don't worry eventually GPU will get cheaper and we will be able to run large models locally...\n\n\n... LOL... I know you wanted to believe me for just one second...",
          "score": 1,
          "created_utc": "2025-12-25 11:28:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvv5w67",
              "author": "LocoMod",
              "text": "I want to believe!",
              "score": 1,
              "created_utc": "2025-12-25 13:07:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuxoz9",
          "author": "uti24",
          "text": ">All of the major open weight labs have shifted to large params general models instead of smaller, more focused models. By this time next year, there won‚Äôt be much ‚Äúlocal‚Äù about this sub unless the paradigm shifts to smaller models good at specific domains.\n\nI have some thoughts on that, I think you can have specialist \"focused\" models in terms of some branch of knowledge, like laws, history or pop culture, but can you have small focused model that can use that knowledge? Maybe not? Then even locally we need kinda big model if we want smart model, not knowledgeable one.",
          "score": 1,
          "created_utc": "2025-12-25 11:51:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuydnc",
          "author": "Monad_Maya",
          "text": "We need to push for better hardware rather than smaller local models. More VRAM, better bandwidth and obviously cheaper hardware.\n\n\nWith that said, we've had a decent number of \"local\" releases in 2025.\n\n\nThere is no larger conspiracy in my opinion, the larger models are genuinely better due to better world understanding.",
          "score": 1,
          "created_utc": "2025-12-25 11:58:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv1nfe",
          "author": "Investolas",
          "text": "I don't think you'll be able to do much regardless",
          "score": 1,
          "created_utc": "2025-12-25 12:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvv1xt0",
          "author": "Investolas",
          "text": "Moron¬†",
          "score": 1,
          "created_utc": "2025-12-25 12:32:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvitfg",
          "author": "MannToots",
          "text": "The reality is more vram makes the technology work better with longer chats. Local is more fun than practical.¬†¬†",
          "score": 1,
          "created_utc": "2025-12-25 14:44:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw2nm2",
          "author": "sxales",
          "text": "It is true. Local used to mean you could run them on average consumer hardware. Now, local seems to mean you need a several thousand dollar purpose-built rig. \n\nIt is nice that Alibaba, Google, and IBM are keeping us fed. Maybe Mistral and AllenAI will catch up. It would be nice is Microsoft and Meta came back.",
          "score": 1,
          "created_utc": "2025-12-25 16:48:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwz5h1",
          "author": "snowrazer_",
          "text": "Local doesn‚Äôt mean small. That‚Äôs how it is right now because of consumer hardware constraints. We need those shackles removed. The demand is there, I believe the future is bright for high memory, high performance LLMs. Running models as powerful as Claude Sonnet 4.5 locally. It‚Äôs not a question of if, but when. \n\nIf the hardware is there, then there are plenty of players like Meta and Qwen ready to rain on the parade of closed models.",
          "score": 1,
          "created_utc": "2025-12-25 19:59:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx06de",
          "author": "pgrijpink",
          "text": "Must have missed Nanbeige4 3B which was released recently. Mental performance for it's size!",
          "score": 1,
          "created_utc": "2025-12-25 20:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxp8ew",
          "author": "svankirk",
          "text": "That is awesome!",
          "score": 1,
          "created_utc": "2025-12-25 22:40:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0hi3n",
          "author": "power97992",
          "text": "Ram and gpus and Macs need to get cheaper.. THey will never stop chasing benchmarks. Models will grow to 2Trillion + parameters... At least they should also release 100b models for those with less ram; gpu providers  and apple need to make a gpu with 4TB/s 128gb of VRAm for $900 and a cuda gpu with 1tb of 8TB/s ram for 3.3k,  a macbook with 160GB of ram for 1k and a macbook with full pytorch and transformers/bitsandbytes support with 512gb of URAM for 2.6k and 1TB   8TB/s  ram MacBook for 3.6k",
          "score": 1,
          "created_utc": "2025-12-26 12:10:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1cefa",
          "author": "power97992",
          "text": "If open weight companies only release large models and meh small models and ¬† dont release a sub 110b model ¬†that is 3-5 months behind the top models , there is no point even upgrading a computer for that cost , you are better off just using the api‚Ä¶",
          "score": 1,
          "created_utc": "2025-12-26 15:39:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3gq4d",
          "author": "highmindedlowlife",
          "text": "I can run GPT-OSS 120b at 18 tk/s with llama.cpp on a single 3090 and CPU. Local and powerful enough for me.",
          "score": 1,
          "created_utc": "2025-12-26 22:29:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi3vxw",
          "author": "coastisthemost",
          "text": "Get a strix halo can run big models. Comfyui still is very dicey on it but llms work great.",
          "score": 1,
          "created_utc": "2025-12-29 06:17:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtkxn2",
          "author": "tronathan",
          "text": "Don't forget about technological advancements; a year might be long enough for some big changes.",
          "score": 1,
          "created_utc": "2025-12-25 03:51:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvdmq5",
              "author": "LocoMod",
              "text": "Sure but that has nothing to do with my post. It‚Äôs irrelevant if some Chinese lab releases a model that can‚Äôt outcompete the big western providers or can be self hosted without breaking the bank.\n\nNow if you release a 32B coding model that can go toe to toe with sonnet-4-5 then we‚Äôre cooking.\n\nOtherwise I might as well pay for Anthropic API.",
              "score": 1,
              "created_utc": "2025-12-25 14:06:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtsuet",
          "author": "Stunning_Mast2001",
          "text": "I don‚Äôt bet on this. Diffusion models are on the horizon",
          "score": 1,
          "created_utc": "2025-12-25 04:55:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvci42",
              "author": "LocoMod",
              "text": "Eagerly awaiting a competitive one. I‚Äôve tried them all and they are not it. \n\nBut time will tell.",
              "score": 1,
              "created_utc": "2025-12-25 13:58:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtv43g",
          "author": "Savantskie1",
          "text": "What are you talking about? There‚Äôs tons of models out there you can run that have been released. Heck the Qwen series keeps putting out smaller models. And that‚Äôs just one example. What kind of crack are you smoking and let me have some üòÇ",
          "score": 1,
          "created_utc": "2025-12-25 05:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvcbwc",
              "author": "LocoMod",
              "text": "Your attention span must have lasted through the subject only. Happens to me too sometimes so it‚Äôs all good.",
              "score": 0,
              "created_utc": "2025-12-25 13:57:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtcx1y",
          "author": "79215185-1feb-44c6",
          "text": "I'll just have to sell the 7900XTXs and buy two RTX 6000s.\n\n> The only viable way forward for local tinkerers are models that can fit between 16 to 32GB of vram.\n\nOh you're still living in 2023. 24GB VRAM is now the minimum (gpt-oss-20b)",
          "score": -1,
          "created_utc": "2025-12-25 02:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtdngo",
              "author": "LocoMod",
              "text": "I have RTX5090. But that is far from average and so is 24GB GPU. So I was being more inclusive since most of us in here do not have a 24GB card.",
              "score": 3,
              "created_utc": "2025-12-25 02:55:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtm2ei",
                  "author": "79215185-1feb-44c6",
                  "text": "People running their crappy little gaming setups for LLMs don't count. You and I are the standard now. If you want to use LLMs for work, you can't settle with trash.",
                  "score": 2,
                  "created_utc": "2025-12-25 04:00:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtve10",
              "author": "Savantskie1",
              "text": "That‚Äôs a 16GB MODEL I can run on my 7900 XT card.",
              "score": 2,
              "created_utc": "2025-12-25 05:16:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuaq3n",
          "author": "cosimoiaia",
          "text": "LOL. You're probably just trolling and/or you live in a cave.",
          "score": 0,
          "created_utc": "2025-12-25 07:42:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt86cd",
          "author": "Investolas",
          "text": "Idiot",
          "score": -6,
          "created_utc": "2025-12-25 02:13:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtbm8l",
              "author": "LocoMod",
              "text": "Your business is not going to make money. No one is paying for that shitty API. Save yourself the trouble and start a trust fund instead. You‚Äôll have a better retirement.",
              "score": 6,
              "created_utc": "2025-12-25 02:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvtdjpg",
                  "author": "Investolas",
                  "text": "Major idiot",
                  "score": -3,
                  "created_utc": "2025-12-25 02:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt9xq3",
          "author": "Rei1003",
          "text": "Small general yes, small focused no.",
          "score": 0,
          "created_utc": "2025-12-25 02:27:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt5qa8",
          "author": "ThisWillPass",
          "text": "We need ~100,000b parameter model for the next intelligence emergence.",
          "score": -4,
          "created_utc": "2025-12-25 01:55:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt7y7e",
          "author": "sunshinecheung",
          "text": "you can buy a RTX4090 48GB GPU or mac studio",
          "score": -1,
          "created_utc": "2025-12-25 02:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtc2lv",
              "author": "LocoMod",
              "text": "And you still won‚Äôt be able to run GLM-4.7 FP16",
              "score": 3,
              "created_utc": "2025-12-25 02:43:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvuujqg",
                  "author": "Baldur-Norddahl",
                  "text": "Why would anyone want to run FP16? Or anything more than q4 really. I am really curious why you would suggest that.\n\nWhen trying to get the most out of limited hardware, just wasting bits on getting a few percent improvement is never the correct tactic. It will always be better to upgrade to a larger model with quantization.",
                  "score": 3,
                  "created_utc": "2025-12-25 11:18:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pvwlfh",
      "title": "systemctl disable ollama",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/8qvw6jdjih9g1.png",
      "author": "copenhagen_bram",
      "created_utc": "2025-12-26 05:30:55",
      "score": 232,
      "num_comments": 94,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Funny",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvzjzcu",
          "author": "ForsookComparison",
          "text": "Ollama's biggest sin for me is committing everyone new to the space to Q4 weights when I'm sensing that the larger community is finally starting to reconsider the last few years of *\"Q4 is a free speedup\"*",
          "score": 45,
          "created_utc": "2025-12-26 06:34:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzw00n",
              "author": "CheatCodesOfLife",
              "text": "Don't forget silently setting the context to 4096, and `ollama run deepseek-r1` giving people a particularly shitty Qwen finetune.",
              "score": 49,
              "created_utc": "2025-12-26 08:32:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw05dts",
                  "author": "paperbenni",
                  "text": "What on earth? Was the original called deepseek:r1 or how did that happen?",
                  "score": 4,
                  "created_utc": "2025-12-26 10:11:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw01hrz",
                  "author": "AiArtFactory",
                  "text": "Can't you change the context window and other settings via a Modfile?",
                  "score": 0,
                  "created_utc": "2025-12-26 09:30:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzsxrm",
              "author": "__Maximum__",
              "text": "What? What is the alternative?",
              "score": 2,
              "created_utc": "2025-12-26 08:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzud69",
                  "author": "Dogeboja",
                  "text": "QAT",
                  "score": 3,
                  "created_utc": "2025-12-26 08:15:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0ufeh",
              "author": "EuphoricPenguin22",
              "text": "There is definitely a small performance hit in terms of capability, but Q4 is often the only way I can get a 20-25B model to fit inside my GPU for significantly faster generation speeds. MoE can take up some of the slack at full precision, but I usually prioritize higher parameter counts with quantization if I'm offloading anyway. For instance, I'd rather use a 100B model at Q4 than a 20-30B model with MoE at full precision if I'm accepting a generation speed hit from offloading.",
              "score": 2,
              "created_utc": "2025-12-26 13:49:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzei5s",
          "author": "Mabuse046",
          "text": "Yeah, ollama storing models at the system level is a huge reason why I won't touch it. I used ollama a little bit back when I first got into LLM's, later learned that they're just another project trying to wrap llama.cpp only they're doing it in the absolute shittiest way possible. I can always tell when someone still doesn't know much about LLM's when they are still using ollama. Kobold and Ooba have their uses occasionally, but there's no reason someone who knows what they're doing wouldn't just use llama.cpp directly. And even then that's for people who aren't just running transformers models in pytorch.",
          "score": 44,
          "created_utc": "2025-12-26 05:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw08jh8",
              "author": "venturepulse",
              "text": "whats the problem with storing at system level? just curious.\n\nIm comfortable with linux filesystem so dont see any difference. /usr/share/ollama/ seems just fine.",
              "score": 13,
              "created_utc": "2025-12-26 10:43:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw11913",
                  "author": "Mabuse046",
                  "text": "I have two NVMe drives, three SSD's and a large mechanical. I like to be able to keep my files where I want them, and I want to be able to access them with multiple programs. You download models with ollama, it puts them all in one place. Sure I could go out of my way to link that folder to a place on another drive, but then I'm still stuck with them having to be all in one place. Then on top of that, unless they've changed the way they're storing the files, last I used ollama they were storing the models as a bunch of cache chunks not as real GGUF's. Well what if I want to use Oobabooga or Kobold or just llama.cpp directly? I have to download an entire second copy of the model?",
                  "score": 6,
                  "created_utc": "2025-12-26 14:34:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3kdk6",
                  "author": "StephenSRMMartin",
                  "text": "There is nothing wrong with it. You can also change it to, say, var/lib like arch does. You can store them in a sub volume so they don't get snapshotted.\n\nPeople here will basically invent reasons to hate ollama, even though ollama's decisions make sense from a server standpoint. Storing them systemd wide means you can use system services and isolate them, service level permissioning, and sandboxing.\n\nI do the same in llama.cpp because I want it to be a system level service, not a user level service.",
                  "score": 1,
                  "created_utc": "2025-12-26 22:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzfdr2",
              "author": "copenhagen_bram",
              "text": "Lesson learned! Maybe I'll reinstall Alpaca on an user level and add remote providers.",
              "score": 4,
              "created_utc": "2025-12-26 05:53:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzre2k",
                  "author": "Healthy-Nebula-3603",
                  "text": "Alpacacpp ? Sure ...",
                  "score": 1,
                  "created_utc": "2025-12-26 07:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzt578",
              "author": "moistiest_dangles",
              "text": "I've recently learned about this. What do you recommend as alternatives to ollama other than just raw dogging llama.cpp? Because that's just cli only right? Isn't the advantage of ollama that it allows you to use not just ram buy also disk memory? So are there other options which harness multiple memory types?",
              "score": 1,
              "created_utc": "2025-12-26 08:02:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzu2ix",
                  "author": "theUmo",
                  "text": "LM Studio is nice.",
                  "score": 8,
                  "created_utc": "2025-12-26 08:12:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvztavc",
                  "author": "copenhagen_bram",
                  "text": "I kinda like llamafile",
                  "score": 2,
                  "created_utc": "2025-12-26 08:04:28",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw15wqj",
                  "author": "Schlick7",
                  "text": "llama.cpp launches its own webui when you start the server. and it recently added model switching so it's basically everything you need. You can also easily hookup something like OpenWebUi",
                  "score": 2,
                  "created_utc": "2025-12-26 15:02:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0srl7",
                  "author": "Desm0nt",
                  "text": "Llama.cpp has it's own ui too, not just cli, afaik. And you clearly can use almost any gui (including openwebui) because almost all of them wirks fine with default OpenAI-like api",
                  "score": 2,
                  "created_utc": "2025-12-26 13:38:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw02x6q",
                  "author": "Mabuse046",
                  "text": "I'm not sure I understand the question. Disk memory? You mean like virtual memory? You save your model file to disk and then when you want to run it you load it to gpu vram and cpu ram together. If you have virtual memory, or (I use Linux) swap, it will use that as a fallback but it will be really slow. I've never tried to skip my vram and ram and go straight to using a disk for memory but if you can do it in ollama, then when I say ollama uses llama.cpp to load the models, logically that means llama.cpp has to be able to do it, since it's the one doing the actual work in either case, right?\n\nAll ollama does is download the model for you and save it in a hidden location and then when you want to run it, it runs llama.cpp with the appropriate flags to load it.\n\nYou can do the same thing. Go on huggingface, download your model, save it wherever you want on your hard drive, and then load it with llama.cpp when you want to run it. The only difference is, you aren't getting it buried in some hidden cache on your drive, you actually get to decide where it's saved. But it's still the same program loading it when you want to run it, ollama just fills in the command line arguments for you, whereas when you raw dog llama.cpp you have to type them in yourself.",
                  "score": 1,
                  "created_utc": "2025-12-26 09:45:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0mj93",
          "author": "CV514",
          "text": "As koboldcpp enjoyer I'm confused why inference software needs to be a system service",
          "score": 9,
          "created_utc": "2025-12-26 12:52:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzjh38",
          "author": "International-Try467",
          "text": "Obligatory fuck Ollama.",
          "score": 50,
          "created_utc": "2025-12-26 06:29:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzjsrx",
              "author": "copenhagen_bram",
              "text": "I've heard this before, but today is the day I start listening",
              "score": 13,
              "created_utc": "2025-12-26 06:32:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzkb14",
                  "author": "International-Try467",
                  "text": "Kobold is an idiot proof launcher btw, Llama.cpp is the original though",
                  "score": 14,
                  "created_utc": "2025-12-26 06:37:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzsshq",
              "author": "WildDogOne",
              "text": "dunno, switched over to llamacpp yesterday and immediately ran into model routing issues aaaand back to ollama. Yes its slower, but it works\n\nedit: love how people are upset that people don't use something that doesn't work ;)",
              "score": -5,
              "created_utc": "2025-12-26 07:59:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzxvzu",
                  "author": "International-Try467",
                  "text": ">koboldcpp is idiot proof",
                  "score": 7,
                  "created_utc": "2025-12-26 08:52:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzmq7i",
          "author": "garloid64",
          "text": "certified Coallama Moment",
          "score": 4,
          "created_utc": "2025-12-26 06:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0k5oq",
          "author": "Outrageous_Cap_1367",
          "text": "Home directory? I suggest backing up ONLY the home directory and excluding the ollama directory",
          "score": 3,
          "created_utc": "2025-12-26 12:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzgibj",
          "author": "StewedAngelSkins",
          "text": "skill issue? don't include object store directories in your snapshots. fyi if you use docker you should exclude its blob storage too, for the same reason.",
          "score": 9,
          "created_utc": "2025-12-26 06:03:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzk4n8",
              "author": "ForsookComparison",
              "text": "> don't include object store directories in your snapshots\n\nAnyone willing to configure this and especially anyone who uses Docker has no reason to be using Ollama.",
              "score": 12,
              "created_utc": "2025-12-26 06:35:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0bznr",
                  "author": "copenhagen_bram",
                  "text": "I don't understand your comment. I had no problem excluding the offending directories in the Timeshift GUI once thread OP pointed out I could do that.",
                  "score": 3,
                  "created_utc": "2025-12-26 11:18:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw23nkk",
                  "author": "StewedAngelSkins",
                  "text": "docker is like the most entry-level way to run any kind of self hosted service and is also how most people use ollama, so i really don't see your point.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:04:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzheh7",
              "author": "copenhagen_bram",
              "text": "Good idea! Just excluded the two problematic directories. Glad I know I can do that now.\n\nJust remember, you wouldn't have this funny meme if I didn't have a skill issue :)\n\nI still need to figure out where Docker blobs go so I can exclude those",
              "score": 3,
              "created_utc": "2025-12-26 06:11:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw23vut",
                  "author": "StewedAngelSkins",
                  "text": "iirc they're under `/var/lib/docker`",
                  "score": 1,
                  "created_utc": "2025-12-26 18:05:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzszhc",
          "author": "__Maximum__",
          "text": "You can change the directory",
          "score": 4,
          "created_utc": "2025-12-26 08:01:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzuaqe",
              "author": "theUmo",
              "text": "Yeah, but it doesn't use it in all instances. The last straw for me was when some IDE integration could only use Ollama, but it made API calls that would auto-download to Ollama's default directory.",
              "score": 3,
              "created_utc": "2025-12-26 08:14:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1f1yr",
          "author": "jacobpederson",
          "text": "Fun fact.  LM studio won't launch if the disk is full :D",
          "score": 1,
          "created_utc": "2025-12-26 15:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw372ko",
          "author": "rm-rf-rm",
          "text": "The shittier part is their unnecessarry (or devious way to trap you in their app) hashing of model names. \n\nThank god more people are waking up to their BS and leaving.",
          "score": 1,
          "created_utc": "2025-12-26 21:36:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7dplf",
          "author": "cosimoiaia",
          "text": "rm -rf /user/share/ollama\n\nMuch better.",
          "score": 1,
          "created_utc": "2025-12-27 15:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhyqqf",
          "author": "Sicarius_The_First",
          "text": "\\>uses ollama  \n\\>disables it  \n\\>redemption arc: complete",
          "score": 1,
          "created_utc": "2025-12-29 05:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi3vgv",
              "author": "copenhagen_bram",
              "text": "haven't really used it in a while, am probably gonna use llamafile, kobold, or just rawdog llama.cpp for once",
              "score": 2,
              "created_utc": "2025-12-29 06:17:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw13nie",
          "author": "extopico",
          "text": "It‚Äôs a feature not a bug. One of the lead devs told me, and also told me to wreck my system permissions if I wanted to move the model store to a separate drive. I uninstalled and scrubbed my machine of that POS and continued using llama.cpp",
          "score": 1,
          "created_utc": "2025-12-26 14:49:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0tqfk",
          "author": "IrisColt",
          "text": "I ditched Ollama two weeks ago, probably my rite of passage out of noobhood, heh... llama.cpp feels like a whole new universe, and it‚Äôs way faster and more capable.",
          "score": 0,
          "created_utc": "2025-12-26 13:45:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvgell",
      "title": "Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "author": "DecodeBytes",
      "created_utc": "2025-12-25 16:05:14",
      "score": 199,
      "num_comments": 52,
      "upvote_ratio": 0.93,
      "text": "Using Open Source DeepFabric, a tool that lets you:\n\n1. Pick any MCP server or any given set of Tools\n2. A specific root topic (DevOps, Customer Care, Coding Agent)\n3. Auto-generate a tool calling / reasoning topic specific dataset, with real tool traces executed within isolated webassembly components.\n4. Fine-tune an SLM to become an expert at that specific MCP server using Unsloth's awesome training framework\n5. Evaluate against a training-blind subset of the dataset.\n\nWe trained Qwen3-4B to outperform Claude Sonnet 4.5 and Gemini Pro 2.5 against the more challenging to use Blender MCP server.\n\n|Model|Score|\n|:-|:-|\n|DeepFabric Fine Tuned|93.50%|\n|Claude Sonnet 4.5|80.50%|\n|Google Gemini Pro 2.5|47.00%|\n\n**The idea is simple:** frontier models are generalists, but a small model fine-tuned on domain-specific tool calling data can become a specialist that beats them at that specific task.\n\n\n\nhttps://preview.redd.it/x6svlmqird9g1.png?width=2816&format=png&auto=webp&s=e44c8203ce3d7383951397b5ae5b33870ceab7e0\n\n\n\n**Try it yourself on Google Colab using a Free T4:** [https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq](https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq)\n\n**GitHub:** [https://github.com/always-further/deepfabric](https://github.com/always-further/deepfabric)\n\nWould love feedback from the community, especially if you decide to generate your own agent.",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvgell/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvvw4xm",
          "author": "swarajs16",
          "text": "can you share the weights or gguf model of the fine tuned model?",
          "score": 23,
          "created_utc": "2025-12-25 16:08:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvxrsx",
              "author": "DecodeBytes",
              "text": "The GGUF is below, but its fresh out of training and not had a chance to really sanity check it yet, if you hit any quirks , let me know \n\nmodel: [https://huggingface.co/alwaysfurther/deepfabric-blender-mcp-gguf](https://huggingface.co/alwaysfurther/deepfabric-blender-mcp-gguf)\n\ndataset: [https://huggingface.co/datasets/alwaysfurther/deepfabric-blender-mcp](https://huggingface.co/datasets/alwaysfurther/deepfabric-blender-mcp)",
              "score": 26,
              "created_utc": "2025-12-25 16:18:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvwnbbi",
              "author": "Global-Ball-3430",
              "text": "The colab notebook should have the model export steps at the bottom but if they didn't include the weights that's kinda sus for a research post",
              "score": -1,
              "created_utc": "2025-12-25 18:49:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx2xhr",
                  "author": "DecodeBytes",
                  "text": "you want theLoRA adapter weights  ? I have never had that ask before, especially with the actual training data being open in the first place.",
                  "score": 7,
                  "created_utc": "2025-12-25 20:22:36",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw5zg4",
          "author": "Bakkario",
          "text": "You have given me a great hope on a similar project I wanted to do for tool calling and CoT SLM model as well. \n\nDo you think we can apply the same concept for a programming language specifically like for example python or JavaScript?",
          "score": 14,
          "created_utc": "2025-12-25 17:08:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwajv7",
              "author": "DecodeBytes",
              "text": "Absolutely, all you need to do is set the topic seed to the domain you want to cover and specifiy the Tools and you have your dataset:\n\nWe could easily adapt this to JS:\n\n[https://github.com/always-further/deepfabric/blob/main/examples/coding-agent.yaml](https://github.com/always-further/deepfabric/blob/main/examples/coding-agent.yaml)\n\nHow DeepFabric works is you set prompt (I don't want to use the word vibe-coding, but it is that easy) and DeepFabric builds a graph of sub-topics, getting more and more detailed as the DAG propagates out. Each one of these nodes and a handful of tools will be used to create a single sample. What's nice about this, is that you have lots of diversity, but you remain on topic (javascript) which reduces the risk of overfit which a lot of other synthetic tool generators fail at.\n\nHere is an example focused around platform engineering and devops: [https://huggingface.co/datasets/alwaysfurther/deepfabric-devops-with-tools](https://huggingface.co/datasets/alwaysfurther/deepfabric-devops-with-tools)\n\nI tell you what, if you PM me or jump on our discord I would happily collaborate with you to build a javascript agent.",
              "score": 6,
              "created_utc": "2025-12-25 17:35:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwqew2",
                  "author": "GoodSamaritan333",
                  "text": "How aboout rust?",
                  "score": 2,
                  "created_utc": "2025-12-25 19:07:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvxgw5k",
                  "author": "Bakkario",
                  "text": "Mark my username!! \n\nI will certainly do that after the holidays. I am so pumped right now. Will use the time for now to read more about fabric. It came across my lane couple of times, but didn‚Äôt give it much attention unfortunately üôèüèæ",
                  "score": 1,
                  "created_utc": "2025-12-25 21:48:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvy0b5g",
                  "author": "jazir555",
                  "text": "If you could do this with WordPress PHP that would be so rad",
                  "score": 1,
                  "created_utc": "2025-12-25 23:52:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwiedl",
          "author": "ZealousidealShoe7998",
          "text": "this is the way.  \nmost people don't need a 500B parameter model to achieve good results.  \nI think the future is small parameter models like 30B max that are highly trained on using tools.  \nnow you can have cheap llms running doing easy bug fix by running tools that are deterministic.",
          "score": 11,
          "created_utc": "2025-12-25 18:21:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwkzua",
              "author": "DecodeBytes",
              "text": "Thanks [ZealousidealShoe7998](https://www.reddit.com/user/ZealousidealShoe7998/), I also agree - the future is open, small energy efficient models - with diversity of training data and tooling that nurtures open innovation and sharing within a global communiity!",
              "score": 9,
              "created_utc": "2025-12-25 18:36:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwdv9g",
          "author": "Nishkama-Karma",
          "text": "Nice work. Using Blender MCP is a real stress test.\n\nQuick q‚Äôs:\n\n* How are you scoring ‚Äútool call success‚Äù exact arg match, partial credit, or task completion?\n* Did the DAG ever drift off-topic during synth gen? Any caps or checks to avoid overfit?\n\nAlso, did Qwen3‚Äë4B need special prompt scaffolding for multi‚Äëstep calls, or were plain schemas + retries enough?",
          "score": 3,
          "created_utc": "2025-12-25 17:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwgfkg",
              "author": "DecodeBytes",
              "text": "Hi!\n\n* How are you scoring ‚Äútool call success‚Äù exact arg match, partial credit, or task completion?\n\nTwo ways, first does it call the correct tool, so `get_weather` and does it do so in the correct format (we derive the tool calling tags from the models chat template, e.g. for qwen `<tools></tools>` XML tag.\n\nSecondly we validate the tool parameters `{\"location\": \"Tokyo\"}`, so we would expect to see the model call:\n\n`<tool_call>`  \n`{\"name\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\"}}`  \n`</tool_call>`\n\nDuring dataset generation the teacher model used has to call specific real live tools, or else its giving a real stack trace and needs to try again. This means we know the dataset and evals do not have any hallucinations, or a at least tools that are grounded in reality. It also cannot 'time travel' which I experience a lot when asking the teacher to mock tools, for example it would try read a file, before writing to the file.\n\nThis is also just the start of it, we are also looking to bring in tool execution at training time using RL - and for the evals we will start leaning more into the semantics of what the model produces!\n\n* Did the DAG ever drift off-topic during synth gen? Any caps or checks to avoid overfit?\n\nI have not seen it, but will also be honest in that we still plan to do some research sprints looking into graph diversity - especially when you build huge DAGs (do they start to repeat after a while). \n\n\\> Also, did Qwen3‚Äë4B need special prompt scaffolding for multi‚Äëstep calls, or were plain schemas + retries enough?\n\nA little bit, but rely on outlines (constrained decoding) and heavy use of pydantic to validate everything - last case we retry, but I don't like that and try to really avoid it, as its wasted pennies. Having said that, some of the small models are really good , gemma has lovely adherence to structure outputs.\n\nI answered the wrong question, but will leave this here anyway:\n\nNot really, we just had a very simple system prompt, well, second guessing myself, we may have even just stuck with qwens default from the chat template:\n\nedit: we inject the tools array into the chat template:\n\n    text = tokenizer.apply_chat_template(\n            messages,\n            tools=tools,\n            tokenize=False\n        )",
              "score": 2,
              "created_utc": "2025-12-25 18:09:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5tok0",
          "author": "NAGS-brief",
          "text": "I'll try",
          "score": 3,
          "created_utc": "2025-12-27 08:24:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8r7w6",
              "author": "Emergency-Associate4",
              "text": "Did you try? Curious to know what you think",
              "score": 2,
              "created_utc": "2025-12-27 19:55:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyupri",
          "author": "eleqtriq",
          "text": "You have to have an api key to use this?",
          "score": 2,
          "created_utc": "2025-12-26 03:15:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0eqwf",
              "author": "DecodeBytes",
              "text": "If you use openai , anthropic or gemini and some of the openrouter models - for anything local, no api key is needed as we support ollama.",
              "score": 1,
              "created_utc": "2025-12-26 11:44:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1fyhb",
                  "author": "eleqtriq",
                  "text": "I meant your service.  I see the examples have API keys to your service.",
                  "score": 1,
                  "created_utc": "2025-12-26 15:59:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0mp2s",
          "author": "bhupesh-g",
          "text": "This is so sweet, I was always wondering why yet no good small models like a model which excels in JS and ReactJS. Its small but can nail most problems in that space.  I always wanted to do such thing but lack of knowledge in this particular domain couldn't. Good to see community is moving in that direction. Great work, keep it up !!",
          "score": 2,
          "created_utc": "2025-12-26 12:54:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0nn7x",
              "author": "DecodeBytes",
              "text": "Hi Bhupesh, you're welcome to raise a react model request: [https://github.com/always-further/deepfabric/discussions/categories/model-request](https://github.com/always-further/deepfabric/discussions/categories/model-request)",
              "score": 2,
              "created_utc": "2025-12-26 13:01:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw4pgcl",
          "author": "Analytics-Maken",
          "text": "The idea makes a lot of sense, especially considering token efficiency from paid models. I've been using them to develop analytics from multiple data sources, consolidating them with ETL tools like Windsor ai, but I often hit Claude caps if I connect various MCP servers.",
          "score": 2,
          "created_utc": "2025-12-27 03:03:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx2bxg",
          "author": "xXWarMachineRoXx",
          "text": "What if you train the big model like you did the small one , wouldn‚Äôt that be a fair comparison\n\n\nAlthough i get that its more efficient, local ( depending on your config) and cheaper / preferable to the members of the sub who need to optimise and get the last of performance but for those who do have credits/ big hardware - how big of a performance gain are they getting?\n\nEdit : love the work, would definitely check it out. It could be absolute bonkers for r/robotics or g 1 people trying to fit it in a small factor like ai glasses/ VR or phones",
          "score": 2,
          "created_utc": "2025-12-25 20:18:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx81i4",
              "author": "DecodeBytes",
              "text": "Hi! If I am not mistaken a larger model will perform even better, but need a bit more GPU time and a bigger dataset to cover a good chunk of trainable parameters to make an impact. Its all doable though and I plan on putting out a 30b pipeline next (either qwen or nemotron, but open to suggestions)\n\nGiving time we will be benchmarking all manner of sizes - especially as we dial in the approach more over time.",
              "score": 2,
              "created_utc": "2025-12-25 20:54:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw12r43",
          "author": "SnooPeripherals5313",
          "text": "Neat idea. Are you picking Qwen as a base SLM because it comes with a decent baseline tool calling performance? Would love to see some dummy metrics",
          "score": 1,
          "created_utc": "2025-12-26 14:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2fe5c",
              "author": "DecodeBytes",
              "text": "out of habit really sloo, i have always grabbed qwen - but any SLM should do. we do plan to launch a service for collecting metrics if you're interested in getting a preview?",
              "score": 2,
              "created_utc": "2025-12-26 19:05:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1ha0m",
          "author": "ridablellama",
          "text": "yes! i have a mcp stack ive wanted to train a small 8b model to use it flawlessley.",
          "score": 1,
          "created_utc": "2025-12-26 16:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2fgzh",
              "author": "DecodeBytes",
              "text": "nice! let me know how you get on if you need any help!",
              "score": 1,
              "created_utc": "2025-12-26 19:05:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2tmqb",
          "author": "yoracale",
          "text": "This is awesome thanks for sharing",
          "score": 1,
          "created_utc": "2025-12-26 20:22:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2vx3v",
          "author": "Emergency-Associate4",
          "text": "I was excited to try this but I'm running into issues with the config files or command switches that no longer exist.",
          "score": 1,
          "created_utc": "2025-12-26 20:35:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw39n7h",
              "author": "DecodeBytes",
              "text": "My bad, there has been a fair few changes and the docs may not be on-par! Do you want to jump onto discord and would be happy to help out. Discord link is on the repo.",
              "score": 1,
              "created_utc": "2025-12-26 21:49:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3weak",
                  "author": "Emergency-Associate4",
                  "text": "It would be nice to know how to get started :).",
                  "score": 1,
                  "created_utc": "2025-12-27 00:01:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3l5vl",
              "author": "DecodeBytes",
              "text": "ok, I just did a large sweep and fixed up a few things that have changed - it should be good now, if not happy to support you",
              "score": 1,
              "created_utc": "2025-12-26 22:54:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwyctu",
          "author": "zhambe",
          "text": "Playing with something not similar, but with a similar goal in mind -- small specialist models to navigate well-defined domain problems. At this point I'd even say MCP is overkill (at least in my case) and finetunes seem more promising / simpler.",
          "score": 1,
          "created_utc": "2025-12-25 19:54:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0ewx7",
              "author": "DecodeBytes",
              "text": "That's interesting, would love to learn more and see your progress. I tend to think of MCP as more of a standard way of building tools, more than anything unique , but it does expand a lot over time.",
              "score": 1,
              "created_utc": "2025-12-26 11:46:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw14x8b",
                  "author": "zhambe",
                  "text": "I'm not quite ready to share what I'm building, but I appreciate the interest! \n\nI look at MCP as a prototyping tool -- flexible, but unwieldy at scale. Unless you're working on something permanently generalist, you're likelty to discover repeated workflows and logic paths, where you no longer need the flexibility, because you're working with a narrowed ranges of schemas / data sources, APIs etc. Then, old-fashioned deterministic code is vastly superior.",
                  "score": 1,
                  "created_utc": "2025-12-26 14:56:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxp4mp",
              "author": "WitAndWonder",
              "text": "MCP or API is extremely useful if you need to work with fluid data.",
              "score": 0,
              "created_utc": "2025-12-25 22:39:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy0zzk",
                  "author": "zhambe",
                  "text": "Is \"fluid data\" the main issue, or unexpected execution paths? In my experience MCP is handy during kind of \"discovery\" work / prototyping, but once most of the paths are known, the advantages vanish while the overhead remain.",
                  "score": 0,
                  "created_utc": "2025-12-25 23:56:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwpqja",
          "author": "q5sys",
          "text": "Any issue with using this on a model we already previously finetuned in the past?  I'd like to update and enhance a model I finetuned a while ago specifically, [https://huggingface.co/BallisticAI/Ballistic-CodeLlama-34B-v1](https://huggingface.co/BallisticAI/Ballistic-CodeLlama-34B-v1) but I'd like to train/finetune it further specifically for python use cases.",
          "score": 0,
          "created_utc": "2025-12-25 19:03:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxes6",
              "author": "DecodeBytes",
              "text": "Its a little difficult to be sure, as it depends on the previous dataset and how many weights were trained with lora (assuming it was lora), but I don't feel a high confidence anything would go wrong at all!",
              "score": 1,
              "created_utc": "2025-12-25 19:49:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvx00a3",
                  "author": "q5sys",
                  "text": "Thanks for the reply.  If I get time this holiday season I might give it a whirl.",
                  "score": 0,
                  "created_utc": "2025-12-25 20:04:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwq139",
          "author": "BeginningReveal2620",
          "text": "Cool project excited to dive in thanks for sharing",
          "score": 0,
          "created_utc": "2025-12-25 19:04:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwxgpo",
              "author": "DecodeBytes",
              "text": "Thanks , do jump into discord if you need support or have any ideas.",
              "score": 1,
              "created_utc": "2025-12-25 19:49:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy2874",
          "author": "rm-rf-rm",
          "text": "Great youve optimized a fundamentally bad approach twice over (first is MCP and second is fine tuning to use MCP).\n\nWhat is far superior, if youre doing fine tuning, is to fine tune on the actual API and docs itself. Then have the SLM write API calls directly. Why have the MCP anymore? MCP made some sense specifically to address the fact that general LLMs do not have sufficient knowledge of specific services and MCP injects the required context they need.",
          "score": -2,
          "created_utc": "2025-12-26 00:04:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvygd9c",
              "author": "harrro",
              "text": "It's a tool calling finetune.\n\nMCP isn't required for that. What are you on about?",
              "score": 4,
              "created_utc": "2025-12-26 01:36:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw0fdva",
              "author": "DecodeBytes",
              "text": "You might be getting mixed up here. We don't fine tune on MCP, we fine tune on function calls and their parameters. \n\nIt just so happens we make it easy to import the list of tools / function calls from an existing MCP server, as a lot of folks use them -  but at the end of it all as far as the model is concerned we are just getting it to improve its ability to predict the natural language of a function name and its parameters - what stack, standard or protocol that function belongs to (openai , MCP, langchain etc) is immaterial",
              "score": 4,
              "created_utc": "2025-12-26 11:50:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzcrtb",
      "title": "Tencent HY-Motion 1.0 - a billion-parameter text-to-motion model",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/yq8uriwhxaag1.jpeg",
      "author": "ResearchCrafty1804",
      "created_utc": "2025-12-30 08:26:06",
      "score": 196,
      "num_comments": 17,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcrtb/tencent_hymotion_10_a_billionparameter/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwr47l3",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-30 16:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp7s4a",
          "author": "redditscraperbot2",
          "text": "Oh this looks really cool\n\nEdit:\nGot it running and it is really cool. Works as advertised. This is going to be a massive speed boost to people working on games. Only a little cleanup needed for each animation.",
          "score": 40,
          "created_utc": "2025-12-30 08:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq7pdq",
              "author": "Zundrium",
              "text": "Ain't nobody got time to clean up animations. Where is my 1B-motion-to-clean-motion model?",
              "score": 23,
              "created_utc": "2025-12-30 13:36:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwqelw9",
              "author": "ab2377",
              "text": "how did you run it?",
              "score": 5,
              "created_utc": "2025-12-30 14:16:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws5bfq",
                  "author": "mxforest",
                  "text": "It's text to motion. You just have to command it to run via chat. Duh!",
                  "score": 1,
                  "created_utc": "2025-12-30 19:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrzbbj",
              "author": "WitAndWonder",
              "text": "What animation format does this actually spit out?",
              "score": 2,
              "created_utc": "2025-12-30 18:49:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpozo6",
          "author": "Illustrious-Lake2603",
          "text": "Does this work only for Humanoid models? Or will it work for animals as well?? I have been working with puppeteer and it has actually been magical.",
          "score": 12,
          "created_utc": "2025-12-30 11:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrumui",
              "author": "fiddler64",
              "text": "yeah, and there's only 1 predetermined rig, I tried prompting a dog chasing after ball and it shows the humanoid rig throwing the ball instead",
              "score": 1,
              "created_utc": "2025-12-30 18:27:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrz3yg",
              "author": "WitAndWonder",
              "text": "It was trained exclusively on the standard human model. Seems like it could work for other bipedal movements to some extent, but anything with more or less limbs seems out of the question unfortunately.",
              "score": 1,
              "created_utc": "2025-12-30 18:48:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpqo9r",
          "author": "KingDutchIsBad455",
          "text": "Is this what Neuro uses?",
          "score": 11,
          "created_utc": "2025-12-30 11:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqg3rd",
              "author": "Emotional-Metal4879",
              "text": "also wander",
              "score": 2,
              "created_utc": "2025-12-30 14:24:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrnwwa",
              "author": "inaem",
              "text": "The sitting looks the same tbh",
              "score": 1,
              "created_utc": "2025-12-30 17:57:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqpxbl",
          "author": "Quiet-Owl9220",
          "text": "Oh boy. The virt-a-mate community ought to have some good uses for this one...",
          "score": 5,
          "created_utc": "2025-12-30 15:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpxlvk",
          "author": "paryska99",
          "text": "Do they also release any finetuning code? This could be really cool for 3D artists or for games with generative content.",
          "score": 1,
          "created_utc": "2025-12-30 12:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqhx2d",
          "author": "no_witty_username",
          "text": "This is more cool then folks realize.  Soo many uses for this.",
          "score": 1,
          "created_utc": "2025-12-30 14:34:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrywy0",
              "author": "WitAndWonder",
              "text": "Just a pity that it was basically exclusively human motion. If it also covered quadruped we'd be really in business since that's most of the creatures that get put into games, too.",
              "score": 2,
              "created_utc": "2025-12-30 18:47:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwrviwp",
              "author": "Crypt0Nihilist",
              "text": "Don't leave us hanging!",
              "score": 1,
              "created_utc": "2025-12-30 18:31:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1px1c41",
      "title": "Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/1e9anmnmsr9g1.jpeg",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-27 16:06:19",
      "score": 186,
      "num_comments": 57,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw7k99y",
          "author": "Mundane_Ad8936",
          "text": "\"In practice, memory bandwidth isn't always the bottleneck\"..\n\nI've had so many hobbyists & enthusiasts in this sub fight over VRAM bandwidth like it's the unerring word of a god.. Every time I explain that in a production systems memory bandwidth is often not the bottleneck, numerous people start bombarding me AI slop that shows they don't understand the problems.\n\nLLMS are everything intensive, you don't have massive memory bandwidth usage without huge amounts of compute driving that..\n\nOn the flip side moving that memory intensive operations to RAM (not GPU) and there is nothing you can do to fix that bottleneck.. now you have a bigger bottleneck in the RAM & CPU which are orders of magnitude slower.\n\nAlso quantization is not free, it absolutely wrecks performance if you are building systems that have to hit quality targets. You see this when you run tasks at scale and fine-tuning helps but there is a clear accuracy gap between a heavily quantized model and one that has minimal to no quantization.\n\nYou running a model in a chatbot app you probably wont notice or care.. but if you're running a model at scale and you have QA checks on what it gets right or wrong you will see clearly.",
          "score": 69,
          "created_utc": "2025-12-27 16:16:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7w9jx",
              "author": "stoppableDissolution",
              "text": "Well, vram bandwidth is \\_the\\_ constraint for batch=1 inference, and there is no way around it. Given that a lot of people in \\_local\\_lama are not hosting the models on clusters to serve hundreds of users, it is a valid case.",
              "score": 54,
              "created_utc": "2025-12-27 17:17:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8v1sj",
                  "author": "Mundane_Ad8936",
                  "text": "Type in vLLM into the search bar for this sub and you'll see otherwise.   \n  \nThis sub is a mix of hobbyists, software devs, professionals & researchers. Small garage tinkers to Google engineers.. It's best to assume that someone in here is way more knowledgable than you (as in Redditor). I've been doing this work for 15 years and and people in this sub teach me things all the time.",
                  "score": 9,
                  "created_utc": "2025-12-27 20:15:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw89gr5",
                  "author": "eloquentemu",
                  "text": "It's not quite that simple.  Look at [Mi50 vs 3090](https://www.reddit.com/r/LocalLLaMA/comments/1mxgis3/some_benchmarks_for_amd_mi50_32gb_vs_rtx_3090/). Both have 1000GBps memory but the 3090 gets 1.5x performance.  You can also test for yourself by running a model in Q4 and BF16.  The Q4 should be 4x faster based on memory, but in practice it's only 2-3x. Obviously Q4 is still faster (so this isn't saying one should run at bf16), but the performance isn't _just_ bandwidth.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:23:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7mx3y",
              "author": "SlowFail2433",
              "text": "Yes LLMs can be all three of compute, memory and interconnect bound at different scales",
              "score": 16,
              "created_utc": "2025-12-27 16:29:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7stti",
              "author": "MitsotakiShogun",
              "text": "> numerous people start bombarding me AI slop\n\n\nHad a few such encounters myself. I coined a term after one of them: The Perplexity Idiot.\n\n\nWhen they can't cite a research paper, code, or empirical study for their view, and ask Perplexity which they then share the link to, not even bothering to read the sources it cited (that one time I went over all of them one by one and not even one supported the answer).",
              "score": 23,
              "created_utc": "2025-12-27 16:59:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7wr8p",
                  "author": "Guinness",
                  "text": "The best indication that you're debating with an idiot is when you see \"?utm_source=chatgpt.com\" in the link they post.",
                  "score": 17,
                  "created_utc": "2025-12-27 17:19:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7vy99",
              "author": "Guinness",
              "text": "> LLMS are everything intensive, \n\n**YOU MUST CONSTRUCT ADDITIONAL PYLONS.**",
              "score": 15,
              "created_utc": "2025-12-27 17:15:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwg5qbx",
                  "author": "sage-longhorn",
                  "text": "My life for Aiur!",
                  "score": 1,
                  "created_utc": "2025-12-28 23:15:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7x9hj",
              "author": "TokenRingAI",
              "text": "I disagree, but not from a system operations or reliability perspective (because you are 100% right)\n\nGemini 3 Flash, Cerebras, and Groq have definitively proven that the user experience and user satisfaction is so much better when token generation is absolutely lightning fast. This metric matters a lot to users, and is capped by memory bandwidth.\n\nDeepSeek has proven the opposite, their inference speed is painful, the user experience is miserable. All the Chinese inference providers have this problem because they are building big models but don't have easy access to Blackwell hardware. They can make it work but it's not as fast.\n\nIt's not even a novel insight, people like fast cars, fast websites, fast everything. Nobody wants to wait. The attention span of users in 2025 is basically zero.\n\nAlso, by making token generation fast, you reduce memory consumption, because the entire context needs to be kept in memory while the tokens are streaming without batching.\n\nUser experience and speed matters, and will be a seriously important metric in the coming years.",
              "score": 4,
              "created_utc": "2025-12-27 17:22:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwcdnkc",
              "author": "PraxisOG",
              "text": "In my experience, the Navi 21 family of cards(rx6800,v620) is equal or faster for inference than the MI50 despite having half the bandwidth, because those older vega cards are super compute constrained",
              "score": 2,
              "created_utc": "2025-12-28 10:26:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7tnco",
              "author": "eloquentemu",
              "text": "It's crazy to me how much people only look at memory and yet we can see how impactful the upgrade path of MI50 -> 3090 -> 4090 is, even without batching though the bandwidths are the same. Sure, a 250 - 500 GBps system/GPU is going to be underwhelming, but often times the compute is simultaneously limited enough that even more bandwidth wouldn't improve it much.",
              "score": 0,
              "created_utc": "2025-12-27 17:03:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8nf81",
                  "author": "Karyo_Ten",
                  "text": "> but often times the compute is simultaneously limited enough that even more bandwidth wouldn't improve it much.\n\n\nNo, if you don't get data fast enough to the compute unit, no compute can happen, zilch.\n\nMemory is the primary limiting factor for data workload. A non-data workload would be something relying on equation or random simulations like Monte-Carlo or Raytracing.\n\nAnd the fact that memory is the primary limiting factor is well understood in high-performance computing, see roofline model or arithmetic intensity.\n\nYou can read more in my post: https://www.reddit.com/u/Karyo_Ten/s/iMEFB6DC5Q",
                  "score": 0,
                  "created_utc": "2025-12-27 19:35:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7kmng",
          "author": "UnbeliebteMeinung",
          "text": "I wish i would understand what this all means.",
          "score": 13,
          "created_utc": "2025-12-27 16:18:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7wj9x",
              "author": "Thomas-Lore",
              "text": "It seems going to 4-bit quants was not worth it because it was not much faster at scale. (So it would be worth for someone like us who runs the model locally but not for deployement on a server farm with large batches).\n\nkofierdavid out it in words better here: https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/nw7q7pw/",
              "score": 16,
              "created_utc": "2025-12-27 17:18:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwbjhfd",
                  "author": "-dysangel-",
                  "text": "I think these were the guys that wrote a massive cope about linear attention not being worth it too, then Deepseek 3.2 comes along",
                  "score": 0,
                  "created_utc": "2025-12-28 05:46:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7wsmw",
                  "author": "UnbeliebteMeinung",
                  "text": "What does 4 bit quants even mean. Explain that to me like i am a five year old. I know nothing about these terms",
                  "score": -8,
                  "created_utc": "2025-12-27 17:20:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7j1ls",
          "author": "SlowFail2433",
          "text": "Nvidia went hard marketing 4bit but the juice might not be worth the squeeze, relative to 8bit. Top labs mess up 4bit runs regularly it is not easy",
          "score": 16,
          "created_utc": "2025-12-27 16:10:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7mxfp",
              "author": "jakegh",
              "text": "Nvidia went hard marketing FP4 on Blackwell, not INT4.\n\nChinese labs are pushed to support INT4 because older Nvidia and current Chinese chips work with it. The fact that Minimax *didn't* go with INT4 is actually in their favor.",
              "score": 29,
              "created_utc": "2025-12-27 16:30:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7nmla",
                  "author": "SlowFail2433",
                  "text": "Thanks I see I am making an error here by mixing up Int4 and FP4. I have Blackwell on the brain.",
                  "score": 12,
                  "created_utc": "2025-12-27 16:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7kqqk",
              "author": "abnormal_human",
              "text": "You could have said the same about FP8 in mid-2023 when Hopper/Ada with their fancy new FP8 support were about as operationalized as Blackwell is today.\n\nIt took till December 2024 till we saw a top-tier model built end-to-end around fp8 (Deepseekv3), and fp8 was a significant factor in how they were able to reduce the costs to produce that model.\n\nGive it time..the hardware support creates software and ecosystem challenges that take much longer to resolve, but the \"free performance\" of additional hardware acceleration is too valuable to ignore forever. The INT4 QAT stuff this person is talking about isn't even the new NVFP4 stuff you're alluding to, that's still older-generation 4bit quant technology, which has less performance potential.",
              "score": 24,
              "created_utc": "2025-12-27 16:19:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7o46m",
                  "author": "SlowFail2433",
                  "text": "I‚Äôm trying lol I‚Äôve been writing FP4 training loops in CUDA or triton-like DSLs but it‚Äôs tough times\n\n\nWe will get there eventually yeah",
                  "score": 12,
                  "created_utc": "2025-12-27 16:36:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7ivh8",
          "author": "SlowFail2433",
          "text": "It is true in my experience also that in large deployments the gains from quant drop.",
          "score": 2,
          "created_utc": "2025-12-27 16:09:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7n4oo",
          "author": "doc-acula",
          "text": "Kind of off-topic, but is it worth trying a Q2 quant of MiniMax (IQ2\\_XXS would be suitable for me). Or is GLM Air in Q4/Q4 already better?",
          "score": 2,
          "created_utc": "2025-12-27 16:31:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7r231",
              "author": "DeProgrammer99",
              "text": "I'd give it a shot. MiniMax M2 25% REAP Q3\\_K\\_XL did better than both GPT-OSS-120B and GLM-4.5-Air (and GLM-4.6V) on my TypeScript minigame one-shot test despite being both heavily quantized *and* pruned.",
              "score": 4,
              "created_utc": "2025-12-27 16:50:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7t2i3",
                  "author": "doc-acula",
                  "text": "Wow, that sounds promising indeed. Kind of random how badly a certain model is affected by heavy quantization. I gess, I'll give it shot then :)",
                  "score": 2,
                  "created_utc": "2025-12-27 17:00:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7vy26",
                  "author": "noiserr",
                  "text": "Yup I used Minimax M2 30% REAP extensively at Q3 and it worked great for me.\n\nI'm hoping we'll get the REAPs of the M2.1",
                  "score": 1,
                  "created_utc": "2025-12-27 17:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8e3i0",
              "author": "Southern_Sun_2106",
              "text": "I prefer iq2\\_xxs on my apple laptop, to glm air 4-bit. Just tried minimax m2.1, and it is better at using tools and doing research. It is smarter in analyzing info. Speed is aprox the same; the air might be just a little bit faster.",
              "score": 1,
              "created_utc": "2025-12-27 18:46:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8eos3",
              "author": "Marksta",
              "text": "Yeah try Minimax M2.1, it should beat GLM Air at comparable sizes.",
              "score": 1,
              "created_utc": "2025-12-27 18:49:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw8wrtb",
          "author": "Aroochacha",
          "text": "Fascinating!",
          "score": 1,
          "created_utc": "2025-12-27 20:25:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwch58r",
          "author": "Aaaaaaaaaeeeee",
          "text": "¬†https://xcancel.com/SkylerMiao7/status/2004887155395756057\n\n\n>CongratzÔºÅAny chance you are looking into 4 bit QAT, or reasons you preferred not to?\n\n\n>Good question. As early as abab5.5 (late 2023), we achieved near-lossless int4 PTQ and int4 QAT. Our QAT generalizes well and can be migrated to new models with almost no extra effort, and it remained in use through MiniMax M1.\n\nWhen talking about performance he's saying int4 can only handle like up to 30% more users. Keep in mind the comparison is to the current fp8 model. Performance can also mean quality, but that's not what they mean in context.\n\n\nI'm disappointed üò≠ but it seems like they can do it wherever, whenever. Maybe they're working on future iterations.\n\n\n\nI'm not sure about LargeEP. Expert parallel?¬†\nIf you run smaller expert models you can see many have a harder time saturating GPU bandwidth compared with dense models.¬†\n\n¬†",
          "score": 1,
          "created_utc": "2025-12-28 10:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9oj6s",
          "author": "beijinghouse",
          "text": "At best he's saying: \"We didn't feel like spending 0.05% additional time to QAT out a 25-50% faster and higher quality model for plebs who don't even own datacenters. We're too rich to run that version so f\\*&k you.\".\n\nBut it's probably much worse than that. The truth is 4-bit QATs smudge out the ability to surgically finetune specific corporate-madated and state-mandated systematic censorship/bias into models after all the other training stages are complete. 8-bit and 16-bit weights leave infinitely more landscape to etch ham-fisted biases into models at the last minute -- even biases that run counter to everything else the weights know in their core. But 4-bit QAT processes rub away those sorts of last-step attempts to censor/bias models. That's why they can't do QAT. They can't get their shitty, bolted-on, last-minute censorship package to survive it.",
          "score": 1,
          "created_utc": "2025-12-27 22:56:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7kyit",
          "author": "Aggressive-Bother470",
          "text": "\"QAT is shit.\"\n\nIs this a fair summary?",
          "score": -5,
          "created_utc": "2025-12-27 16:20:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7q7pw",
              "author": "koflerdavid",
              "text": "They build a model for large-scale deployment, not for GPU-poor people. In their scenario, quantization is not worth it.",
              "score": 16,
              "created_utc": "2025-12-27 16:46:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7mpu4",
              "author": "SlowFail2433",
              "text": "No cos you could (and should) still do 8 bit QAT even if you are not doing 4 bit quants \n\n\nQAT is essentially a stage I would never skip, it prepares the model for the quant noise",
              "score": 9,
              "created_utc": "2025-12-27 16:28:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvz7v2",
      "title": "Minimax M2.1 released",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
      "author": "__Maximum__",
      "created_utc": "2025-12-26 08:13:29",
      "score": 176,
      "num_comments": 85,
      "upvote_ratio": 0.92,
      "text": "Link to xcancel:\nhttps://xcancel.com/ModelScope2022/status/2004462984698253701#m\n\nNew on ModelScope: MiniMax M2.1 is open-source!\n\n‚úÖ SOTA in 8+ languages (Rust, Go, Java, C++, TS, Kotlin, Obj-C, JS)\n‚úÖ Full-stack Web & mobile dev: Android/iOS, 3D visuals, vibe coding that actually ships\n‚úÖ Smarter, faster, 30% fewer tokens ‚Äî with lightning mode (M2.1-lightning) for high-TPS workflows\n‚úÖ Top-tier on SWE-bench, VIBE, and custom coding/review benchmarks\n‚úÖ Works flawlessly in Cursor, Cline, Droid, BlackBox, and more\n\nIt‚Äôs not just ‚Äúbetter code‚Äù ‚Äî it‚Äôs AI-native development, end to end.\n\nhttps://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary\n",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvzuqjp",
          "author": "bullerwins",
          "text": "It's also on HF [https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)",
          "score": 45,
          "created_utc": "2025-12-26 08:19:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3w6ji",
              "author": "Dependent-Highway107",
              "text": "Nice, way easier to grab from HF than dealing with ModelScope's download speeds",
              "score": 3,
              "created_utc": "2025-12-27 00:00:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5cfzp",
                  "author": "inaem",
                  "text": "Vise versa for us lol",
                  "score": 2,
                  "created_utc": "2025-12-27 05:47:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzyc4p",
          "author": "spaceman_",
          "text": ">¬†It‚Äôs not just ‚Äúbetter code‚Äù ‚Äî it‚Äôs AI-native development, end to end.\n\n\nI smell a machine",
          "score": 82,
          "created_utc": "2025-12-26 08:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw03x4j",
              "author": "Evening_Ad6637",
              "text": "It's not only that you smell the machine ‚Äî it's truly a demonstration of your Sherlock Holmes' eye for subtle details!",
              "score": 38,
              "created_utc": "2025-12-26 09:56:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0qfj2",
                  "author": "SilentLennie",
                  "text": "Kimi K2 would never.",
                  "score": 6,
                  "created_utc": "2025-12-26 13:22:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw0tkt7",
              "author": "Worthstream",
              "text": "Weird, I smell ozone.¬†",
              "score": 9,
              "created_utc": "2025-12-26 13:44:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3bcal",
                  "author": "kjerk",
                  "text": "Oh no!\nSmile for me bud! Is it lopsided? Talk to me!",
                  "score": 2,
                  "created_utc": "2025-12-26 21:59:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3u1gn",
              "author": "The_Cat_Commando",
              "text": ">I smell a machine\n\nthe first sign is any post that uses so many emojis is AI summary output.\n\n>normal ‚úÖ people ‚úÖ dont ‚úÖfill  ‚úÖ their ‚úÖposts ‚úÖwith ‚úÖall ‚úÖthese. ‚Äî üîç‚ö°ü§óüìäüß†‚ö†Ô∏è",
              "score": 5,
              "created_utc": "2025-12-26 23:47:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6n2nk",
                  "author": "yopla",
                  "text": "You've obviously never been on LinkedIn.",
                  "score": 3,
                  "created_utc": "2025-12-27 12:59:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw03m2t",
              "author": "__Maximum__",
              "text": "Yeah, they work on LLMs, I suspect they use it as well.",
              "score": 7,
              "created_utc": "2025-12-26 09:53:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw44ta5",
              "author": "aeroumbria",
              "text": "Come on Boromir, we are not in Moria anymore!",
              "score": 1,
              "created_utc": "2025-12-27 00:52:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzyd56",
          "author": "Wise_Evidence9973",
          "text": "Merry Christmas!   \n[https://huggingface.co/MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)  \n[https://github.com/MiniMax-AI/MiniMax-M2.1](https://github.com/MiniMax-AI/MiniMax-M2.1)",
          "score": 51,
          "created_utc": "2025-12-26 08:57:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0hc37",
              "author": "No_Conversation9561",
              "text": "Thank you",
              "score": 5,
              "created_utc": "2025-12-26 12:08:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw22298",
              "author": "Adventurous-Okra-407",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2025-12-26 17:56:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzuyeq",
          "author": "Zyj",
          "text": "This is very promising, can‚Äòt wait to try a Q4 quant. Or perhaps a Q3",
          "score": 13,
          "created_utc": "2025-12-26 08:21:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzvifa",
              "author": "Particular-Way7271",
              "text": "A REAP Q4 in my case üòÇ",
              "score": 9,
              "created_utc": "2025-12-26 08:27:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw03f96",
                  "author": "__Maximum__",
                  "text": "REAP REAP Q1 in mine",
                  "score": 7,
                  "created_utc": "2025-12-26 09:51:11",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nw02lru",
                  "author": "SlowFail2433",
                  "text": "Reap is rly good",
                  "score": 0,
                  "created_utc": "2025-12-26 09:42:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzylzf",
          "author": "No_Conversation9561",
          "text": "https://preview.redd.it/ls16xtixji9g1.jpeg?width=600&format=pjpg&auto=webp&s=47c07c832748f5951c571ca5743ba33d3c65f2aa",
          "score": 20,
          "created_utc": "2025-12-26 09:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0r9zx",
              "author": "LocoMod",
              "text": "https://preview.redd.it/i81troervj9g1.jpeg?width=1290&format=pjpg&auto=webp&s=7dd1e08b4157413a65f8206f3cdc8e5210dfc75a",
              "score": 4,
              "created_utc": "2025-12-26 13:28:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0e1og",
          "author": "zekuden",
          "text": "This or glm 4.7?",
          "score": 10,
          "created_utc": "2025-12-26 11:38:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0s1fl",
              "author": "misterflyer",
              "text": "Both. Only way to find out which works best for your use case is to try them both.  Plus you might like both.",
              "score": 8,
              "created_utc": "2025-12-26 13:33:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw35hq4",
                  "author": "zekuden",
                  "text": "I appreciate it, that makes sense haha",
                  "score": 1,
                  "created_utc": "2025-12-26 21:27:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw32k37",
              "author": "layer4down",
              "text": "GLM-4.7 is my daily driver, but I use the yearly coding max plan which giver me expect bang for buck.\n\nBut for local coding and local tasks I use minimax-m2-dwq-q4. Going to try out m2.1 today by I suspect it will largely be the same.",
              "score": 1,
              "created_utc": "2025-12-26 21:11:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4lad4",
                  "author": "sleepy_roger",
                  "text": "Not sure why you were downvoted I did the same, paid $270 for the year max plan (Christmas sale) was a no brainer.",
                  "score": 3,
                  "created_utc": "2025-12-27 02:37:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw35g98",
                  "author": "zekuden",
                  "text": "wow that's pretty cool. Why did you choose GLM instead of other paid models like claude? is it cheaper / gives you more credits / better than or equal to claude?\n\nsince you use both a paid plan and local, that's very interesting. One last question, what are your use cases for paid vs local?",
                  "score": 2,
                  "created_utc": "2025-12-26 21:27:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw10tfo",
          "author": "-InformalBanana-",
          "text": "REAP when? :D",
          "score": 9,
          "created_utc": "2025-12-26 14:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzx9z3",
          "author": "LegacyRemaster",
          "text": "These days I've been using M2.1 Free on the website / GLM 4.7 Free on the website / GTP 5.2 Thinking (paying plus) and Sonnet 4.5 Thinking (on Perplexity) a lot. The latter two suggested fixes and literally refused to return updated scripts with the fixes. M2.1 added 1000 lines of code without complaint in the free version. Both GLM and M2.1 made no errors in JS/CSS/HTML/Python. Sonnet returned a 40k shorter script after insisting that I wanted the full script. GTP was incredibly slow and the file wouldn't download. And these are two big paid programs. For my specific use case, coding, I won't go back.",
          "score": 15,
          "created_utc": "2025-12-26 08:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0cqii",
              "author": "Feisty-Patient-7566",
              "text": "Perplexity is a joke. They give me about 3 turns with Sonnet before it changes to \"best\".",
              "score": 15,
              "created_utc": "2025-12-26 11:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0pjnl",
                  "author": "LegacyRemaster",
                  "text": "True. The worst thing is that I did market research whose actual results I already knew (a product my company actually produced), and it continued to give optimistic and over-inflated numbers. When I told him, \"Stop flattering me and give me the real answer,\" it apologized and scaled back his forecast. But if they're thinking of selling it as a paid product (now free with PayPal), they have a lot of work to do.",
                  "score": 3,
                  "created_utc": "2025-12-26 13:15:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1x75d",
                  "author": "my_name_isnt_clever",
                  "text": "I can feel the enshittification happening slowly in real time but I haven't been able to beat it's functionality with a custom local setup yet. But it's still the only cloud service I pay for since it doesn't lock me into any one LLM provider.",
                  "score": 2,
                  "created_utc": "2025-12-26 17:30:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw31upw",
              "author": "layer4down",
              "text": "Perplexity is a great MCP tool and awesome for quick research via app or UI but I learned long ago it‚Äôs useless for all but the most basic of coding. I love it as an NLP research tool for my models.",
              "score": 2,
              "created_utc": "2025-12-26 21:07:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3vj54",
              "author": "z_3454_pfk",
              "text": "Sonnet via perplexity is really bad since they use a middle model to only parse the ‚Äòrelevant‚Äô parts of your query and has a system prompt to provide ‚Äòconcise‚Äô outputs. If you use Sonnet via api it‚Äôs so much more different.",
              "score": 2,
              "created_utc": "2025-12-26 23:56:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzvvkv",
          "author": "Few_Painter_5588",
          "text": "It's a good model, I'd argue that it's probably better than Qwen3 235B too.",
          "score": 6,
          "created_utc": "2025-12-26 08:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0rw9o",
              "author": "this-just_in",
              "text": "For agentic coding, MiniMax M2 was already beating Qwen3 VL 235B or Qwen3 235B 2507 in my estimation (and from basically any benchmark you can find). ¬†I suspect Qwen3 235B is a better generalist model, and the Qwen3 VL variant has vision of course.",
              "score": 7,
              "created_utc": "2025-12-26 13:32:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw0xcuc",
                  "author": "Few_Painter_5588",
                  "text": "The Qwen3 VL models have been disappointed for my tasks, the 2.5 VL models are more performant to me.",
                  "score": 1,
                  "created_utc": "2025-12-26 14:09:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw06fh1",
              "author": "ciprianveg",
              "text": "did you make some comparison tests, qwen 235b UD Q6 XL 2507 instruct was my preferred local model for coding till now, I found it best for my coding tasks, long context, 30k-120k tokens. Better than glm 4.6. java+js. I hope M2.1 is at least as good while being 2 times faster",
              "score": 1,
              "created_utc": "2025-12-26 10:22:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw07bdd",
                  "author": "Few_Painter_5588",
                  "text": "Yes, I have a personal benchmark, and running both in FP8, minimax is a little worse, but I prefer minimax. Those 15B fewer active parameters really make a huge difference for agentic tasks like figuring out document groups.",
                  "score": 2,
                  "created_utc": "2025-12-26 10:31:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzuucx",
          "author": "Zyj",
          "text": "It‚Äòs not open source (the training data is not included). It‚Äôs open weights:\nhttps://huggingface.co/MiniMaxAI/MiniMax-M2.1",
          "score": 18,
          "created_utc": "2025-12-26 08:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0uqtk",
              "author": "Yes_but_I_think",
              "text": "You can get a good model or a open source one, not both.",
              "score": 11,
              "created_utc": "2025-12-26 13:52:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwa4p",
              "author": "Xamanthas",
              "text": "Great point for the normies to know.",
              "score": 9,
              "created_utc": "2025-12-26 08:35:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzx2ta",
              "author": "cantgetthistowork",
              "text": "Everything is open weights..",
              "score": -7,
              "created_utc": "2025-12-26 08:44:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzzsyq",
                  "author": "Amazing_Rutabaga8336",
                  "text": "Olmost",
                  "score": 9,
                  "created_utc": "2025-12-26 09:12:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzulim",
          "author": "MrMrsPotts",
          "text": "How many parameters?",
          "score": 2,
          "created_utc": "2025-12-26 08:17:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzuyxz",
              "author": "bullerwins",
              "text": "229B",
              "score": 10,
              "created_utc": "2025-12-26 08:21:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzwim6",
              "author": "__Maximum__",
              "text": "Link",
              "score": -7,
              "created_utc": "2025-12-26 08:38:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw1p5rh",
          "author": "Waste-Intention-2806",
          "text": "Unsloth gguf is out, anyone tried q3 quant ?",
          "score": 2,
          "created_utc": "2025-12-26 16:48:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw23gh7",
          "author": "anonynousasdfg",
          "text": "Based on your experience which one follows the system prompts and rules strictly especially on Kilo Code: M2.1 or GLM 4.7?",
          "score": 2,
          "created_utc": "2025-12-26 18:03:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzw8mi",
          "author": "Xamanthas",
          "text": "Duplicate post and links to modelscope instad of HF?",
          "score": 4,
          "created_utc": "2025-12-26 08:35:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw05z9k",
              "author": "SlowFail2433",
              "text": "I use both TBH and I am not based in China",
              "score": 3,
              "created_utc": "2025-12-26 10:17:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzw3ck",
          "author": "duyntnet",
          "text": "\"M2.1 was built to shatter the stereotype...\": seeing 229B shatters my dream of running it :(",
          "score": 5,
          "created_utc": "2025-12-26 08:33:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw18ezx",
              "author": "Zc5Gwu",
              "text": "128gb of ram + a gpu would run it at at least Q3 at maybe 10 t/s",
              "score": 5,
              "created_utc": "2025-12-26 15:17:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw36yvy",
                  "author": "duyntnet",
                  "text": "I have 64gb of slow ddr4 ram (2133mhz I think) and an rtx 3060 12gb so it's far from enough.",
                  "score": 2,
                  "created_utc": "2025-12-26 21:35:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzuhje",
          "author": "jacek2023",
          "text": "Wtf is xcancel",
          "score": 1,
          "created_utc": "2025-12-26 08:16:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzulg5",
              "author": "bullerwins",
              "text": "a proxy for x/twitter where you can see comments for post without having to be logged in. Not a phishing website, it's legit",
              "score": 28,
              "created_utc": "2025-12-26 08:17:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzyq25",
                  "author": "pmttyji",
                  "text": "Thanks. Not aware of that site. Though I don't use twitter, I used to bookmark some ids to see their tweets without login. After it became X, I couldn't see that way anymore.",
                  "score": 8,
                  "created_utc": "2025-12-26 09:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw0du5w",
          "author": "jadhavsaurabh",
          "text": "Thx for introducing me to x cancel,\n\nBro it's like in redir app i. Android it oppensjn reddit app, then open in browser then open. In x so much wasted time",
          "score": 1,
          "created_utc": "2025-12-26 11:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0racf",
          "author": "this-just_in",
          "text": "Looking forward to the AWQ and NVFP4 quants- MLX static and GGUF quants already posted to HF.",
          "score": 1,
          "created_utc": "2025-12-26 13:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0x63k",
          "author": "mintybadgerme",
          "text": "Just tried it via the API and it's really really good.",
          "score": 1,
          "created_utc": "2025-12-26 14:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4i1ps",
          "author": "Different_Fix_2217",
          "text": "It's worse than deepseek 3.2 for local in my usage.",
          "score": 1,
          "created_utc": "2025-12-27 02:16:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwd0bbp",
          "author": "Turbulent_Sample487",
          "text": "I'm a new owner of the dell pro max (dgx spark clone) with 128GB shared gcpu/cpu(arm) ram with the GB10.  I never know which model to download from HF... The gb10 works best with fp8.  Generally speaking I should always look for models with fp8 in the name? (btw - through trial and error, these options make the gb10 scream with comfyui -  python [main.py](http://main.py) \\--highvram --reserve-vram 15 --fp8\\_e4m3fn-unet --fp8\\_e4m3fn-text-enc).  Maybe vLLM and the MiniMax-M2.1-AWQ?",
          "score": 1,
          "created_utc": "2025-12-28 13:37:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfi0fw",
          "author": "bigh-aus",
          "text": "For those who are running this locally I'd love to know what the details of your rig  it's running on.",
          "score": 1,
          "created_utc": "2025-12-28 21:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw016o2",
          "author": "power97992",
          "text": "FInally lol",
          "score": 1,
          "created_utc": "2025-12-26 09:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzwc3f",
          "author": "Snoo_64233",
          "text": "This is old news? It got released 5 days ago, no?",
          "score": -2,
          "created_utc": "2025-12-26 08:36:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzxb9a",
              "author": "misterflyer",
              "text": "Sort of. It was released via API/website 5 days ago, not open weights (for local use) until now.",
              "score": 17,
              "created_utc": "2025-12-26 08:46:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2d542",
                  "author": "NoahFect",
                  "text": "So the repo on HF with the 6-day-old files wasn't visible to the public until just now?  I'm confused too.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pwt6ir",
      "title": "Asus isn't going into memory manufacturing ‚Äî Taiwanese tech giant issues statement smashing rumor",
      "subreddit": "LocalLLaMA",
      "url": "https://www.tomshardware.com/pc-components/dram/no-asus-isnt-going-into-memory-manufacturing-taiwanese-tech-giant-issues-statement-smashing-rumor",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-27 08:49:41",
      "score": 171,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/",
      "domain": "tomshardware.com",
      "is_self": false,
      "comments": [
        {
          "id": "nw7c47l",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-27 15:35:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5xq81",
          "author": "ImportancePitiful795",
          "text": "We knew that from the day it was published by crap sites like WCCFTECH. \n\nASUS doesn't have the fab to do make DRAM chips.",
          "score": 83,
          "created_utc": "2025-12-27 09:04:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5ybbt",
              "author": "HumanDrone8721",
              "text": "And I think the market now is kind of out of RAM to rebrand and put even more blinking pulsating stinking LEDs on it.",
              "score": 21,
              "created_utc": "2025-12-27 09:09:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6137n",
          "author": "a_beautiful_rhind",
          "text": "We're going to have to smuggle chinese dram in our prison wallets or glue them to ourselves like the lady with the iphones.",
          "score": 25,
          "created_utc": "2025-12-27 09:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6ewfs",
              "author": "tmvr",
              "text": "If you are obese enough you can hide them in your folds. Excuse me while I go eat some triple burger meal and wash it down with a 2.5L bottle of soft drink.",
              "score": 14,
              "created_utc": "2025-12-27 11:50:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6rib9",
                  "author": "a_beautiful_rhind",
                  "text": "fatmaxxing",
                  "score": 9,
                  "created_utc": "2025-12-27 13:30:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw74bsm",
              "author": "nonaveris",
              "text": "Or lobby for it to be unsanctioned as long as the prices remain high.",
              "score": 4,
              "created_utc": "2025-12-27 14:51:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw631km",
          "author": "megalo-vania",
          "text": "They have already collaborated with Nvidia, why they need to manufacturing memory?",
          "score": 3,
          "created_utc": "2025-12-27 09:56:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6snk3",
              "author": "StorageHungry8380",
              "text": "NVIDIA is reportedly no longer supplying VRAM along with their GPU to board partners\\[1\\]. Since they can't just wish a memory fab into existence, they'll have to find memory somewhere else.\n\n\\[1\\]: [https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own](https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own)",
              "score": 22,
              "created_utc": "2025-12-27 13:38:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6ssk4",
                  "author": "megalo-vania",
                  "text": "Holy this happened a month ago and I didn‚Äôt know yet.",
                  "score": 7,
                  "created_utc": "2025-12-27 13:39:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5wt11",
          "author": "QuailLife7760",
          "text": "Well we're fucked",
          "score": 8,
          "created_utc": "2025-12-27 08:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6988j",
              "author": "Mean-Equivalent-624",
              "text": "They dont even have fabs for this so idk what ppl were expecting",
              "score": 27,
              "created_utc": "2025-12-27 10:57:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5xjtf",
              "author": "MelodicRecognition7",
              "text": "perhaps YMTC will step up",
              "score": 1,
              "created_utc": "2025-12-27 09:02:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwbldph",
          "author": "Highwaytothebeach",
          "text": "It looks all those moore's  and other laws were supposed to lead to PCs with quite a lot of RAM sooner.. DDR6 was suposed to make finally reasonably  priced 1 TB or more RAM PCs and 8 TB or more RAM servers....Hope at least those PCIe 6.0 SSD with 30.25 GB/s speeds may  become available sooner [https://www.tomshardware.com/pc-components/ssds/pcie-6-0-ssd-with-30-25-gb-s-speeds-debuts-at-computex-release-date-is-still-a-long-way-off](https://www.tomshardware.com/pc-components/ssds/pcie-6-0-ssd-with-30-25-gb-s-speeds-debuts-at-computex-release-date-is-still-a-long-way-off)",
          "score": 2,
          "created_utc": "2025-12-28 06:01:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwwsag",
      "title": "The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/",
      "author": "madSaiyanUltra_9789",
      "created_utc": "2025-12-27 12:33:15",
      "score": 156,
      "num_comments": 139,
      "upvote_ratio": 0.81,
      "text": "Hey everyone, \n\nI just watched [The Infinite Software Crisis ‚Äì Jake Nations](https://www.youtube.com/watch?v=eIoohUmYpGI) on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. \n\n**Highlights from the talk:**\n\n* Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.\n* **The hard part is timeless:** The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.\n* **AI amplifies the problem:** We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding *what* to build remains.\n* The real trap we fall into is confusing easy with simple.\n   * **Easy** is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.\n   * **Simple** is about structure. It means one fold, one braid, no entanglement. It requires thought and design.\n* LLMs do not understand logic, they merely relate language and substitute those relations as \"code\", so the importance of *patterns and architectural decisions* in your codebase are lost. \n* when \"vibe-coding\" technical debt doesn't register as debt; it's just more code to preserve. \n* The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).\n\nThe real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.\n\nThe proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. \n\nWhat's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? \n\nhttps://preview.redd.it/c4mknoudlq9g1.png?width=553&format=png&auto=webp&s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw6o9a5",
          "author": "kevin_1994",
          "text": "When I was a junior, I was tasked with architecting a service on the backend. Using all the skills university prepared for me, I implemented a facade pattern with a bunch of complex logic to ensure the code was as abstract and \"correct\" as possible. \n\nMy senior rejected my pull request. He said my solution was technically correct, but too hard to understand. He told me just copy paste the code in a couple places if necessary, because two years from now, nobody is gonna remember this complex abstraction pattern, but everyone can follow a simple class that maybe isnt perfect, but is easy to understand. \n\nI remember he told me that \"debugging and maintaining code takes 50% more brainpower than writing it, therefore, by definition, using 100% of your brain to write code is unmaintainable and impossible to understand\". I always remembered that \n\nAI are like the overeager junior who just graduated from university who wants to write everything the \"correct\" way but doesnt have 10 years of debugging experience to understand how to write code that you can understand and scale years later. \n\nI really fear for companies embracing vibecoding. Vibecoding doesnt show the type of wisdom and restraint that experience teaches a developer over decades.",
          "score": 191,
          "created_utc": "2025-12-27 13:08:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6u24u",
              "author": "vimbaer",
              "text": "That was some good advice you got there!",
              "score": 39,
              "created_utc": "2025-12-27 13:47:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw70ls2",
              "author": "SkyFeistyLlama8",
              "text": "Trying to debug vibed code with extra layers of abstraction takes more time than just writing the damn thing myself. I'll stick to using local LLMs at a class or function level only. Sometimes I bounce architecture ideas off a cloud LLM but I never ask it to write code.",
              "score": 30,
              "created_utc": "2025-12-27 14:28:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw724cg",
                  "author": "kevin_1994",
                  "text": "I do the same. I use ai assisted coding for\n\n- autocomplete\n- bash scripts \n- brainstorming \n\nI've never been satisfied with agentic coding because they make so many simple mistakes that I feel like im teaching another junior. One great example is that LLMs are trained on a lot of tutorial style code, and subsequently, nearly always believe that the caller should handle errors/exceptions, without any understanding that errors are useful in code--silently handling fatal states makes your application impossible to understand.",
                  "score": 31,
                  "created_utc": "2025-12-27 14:38:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwg97au",
                  "author": "LoSboccacc",
                  "text": "Yeah the only way to get medium size project done is strict interfaces between component and typed object passing everywhere, the llm can fill the methods.¬†\n\n\nMaybe some bulk transform too but they tend to lazy out in the cleanup.¬†\n\n\nHelps forbidding adapters, fallbacks, local state caves and defaults so the code crashes fast and code has a single source of truth it must locate,¬† paired with strong validation on the data model, otherwise llm will just trap errors and zero variables to get the thing running (but not working)\n\n\nAlso helps maintaining class or component responsibility index somewhere and send llms to find leakages.",
                  "score": 2,
                  "created_utc": "2025-12-28 23:34:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7jycu",
              "author": "zeth0s",
              "text": "I do not agree with your senior. A good abstraction make it easier to maintain and migrate code. If the code that you copied and pasted have a bug, or requirements change, you now have to change each of your snippets. And each of them will diverge long term.\n\n\nI'd have rejected your senior PR. Copying and pasting code with actual logic is a gate to maintainance hell.¬†\n\n\nCognitive complexity can be kept low in many way, without polluting code base with snippets",
              "score": 23,
              "created_utc": "2025-12-27 16:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7s93p",
                  "author": "moofunk",
                  "text": "I would agree with the senior, but it depends on circumstances. Might make more sense in that project to copy, so you understand in-situ what's going on without needing to comment. Sometimes that doesn't make sense, and that depends on the code you end up with.\n\nAbstractions have to deal with proper division of labor between the layers, and getting this wrong or abstracting too much, makes the abstractions harder to understand.",
                  "score": 10,
                  "created_utc": "2025-12-27 16:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7k76v",
              "author": "Roid_Splitter",
              "text": "Besides the damage to projects that you describe, we will also lose an entire crop of juniors.",
              "score": 5,
              "created_utc": "2025-12-27 16:16:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb5e1f",
                  "author": "madSaiyanUltra_9789",
                  "text": "no one is thinking about the fact that you need juniors to get senior developers, we will realize the results of this 10yrs from now as the workforce begins to shrink involuntarily .",
                  "score": 2,
                  "created_utc": "2025-12-28 04:05:45",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7d16p",
              "author": "swagonflyyyy",
              "text": "%100 agree vibe coding is a trap that turns your code into a tangled black box.¬†\n\n\nThen you have to use other AIto help debug it but at the end of the day the issue is unavoidable, you gotta do it yourself.",
              "score": 7,
              "created_utc": "2025-12-27 15:40:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6vr5q",
              "author": "Muritavo",
              "text": "Also, every pattern is like a solution for a problem...   \n  \nFor example, there is no need for someone to implement and interface, specialized classes, abstraction etc, If you only need a single instance and a simple module with all methods solves your case.",
              "score": 7,
              "created_utc": "2025-12-27 13:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6yt28",
              "author": "lonesomewhistle",
              "text": "And now we have AI approving PRs.",
              "score": 6,
              "created_utc": "2025-12-27 14:17:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7eunl",
                  "author": "redballooon",
                  "text": "That‚Äôs a win, if the second pair of hums eyes stays in the process.",
                  "score": 6,
                  "created_utc": "2025-12-27 15:49:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8fscg",
              "author": "Material_Policy6327",
              "text": "Yeah I totally get this. I am an AI researcher and while vibe coding can feel fun holy shit the code it makes is so confusing even when it is technically correct",
              "score": 1,
              "created_utc": "2025-12-27 18:55:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw8sj02",
              "author": "WitAndWonder",
              "text": "This is why I stick to coding apps that are basic in principle but expensive in content with AI. The actual Apps are quite contained, but often involve hundreds or thousands of individual JSONs for things like abilities/traits/whatever that are simple to understand but would be tedious as hell to customize by hand. It's an actual godsend for just reducing repetitive tasks like that. Build the structure of the files yourself, give it some samples, and let it rip the other ten out with some descriptions of mechanics or flavor text involved.",
              "score": 1,
              "created_utc": "2025-12-27 20:02:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zwer",
              "author": "PunnyPandora",
              "text": "You forget the benefit of vibe coding, being able to undo a 10 step mistake in a single step. Even if it was self induced, it's a lot more steps out the way and something on the table. I'd never be able to iterate or plan or get new ideas with 0 accumulation. Having a perfect plan rarely works out in practice, I need there to be something whether it works or fails to decide where to go from there. Like even if I spend days on writing plans, it's not guaranteed to go according to that and I might change my mind along the way as well. Being able to change my mind 3 times with no punishment is unheard of",
              "score": 1,
              "created_utc": "2025-12-28 00:00:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwavdw0",
              "author": "Zhanji_TS",
              "text": "You guys are still reading the code?",
              "score": 1,
              "created_utc": "2025-12-28 03:04:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6v3dt",
              "author": "gscjj",
              "text": "You hinted on the solution. It‚Äôs not that embracing vibecoding is bad, is that you equally need to embrace the senior who rejected your PR. \n\nYou are capable of understanding the implication, so is an AI if requested. You just need someone to say it‚Äôs over complicated and unmaintainable.",
              "score": 2,
              "created_utc": "2025-12-27 13:54:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw9zj8a",
              "author": "giant3",
              "text": ">  He told me just copy paste the code in a couple places if necessary,\n\nI would fire your senior. Multiple copies that could go out of sync and the person changing the code wouldn't know that it also exists in other places. \n\nI have been writing software for 30 years now and I have seen this exact bug in production code a few times.",
              "score": 1,
              "created_utc": "2025-12-27 23:58:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7pttj",
              "author": "Lifeisshort555",
              "text": "I think the inverse is true now. The LLM will have little trouble understanding your code. The next person is not even trying at this point if they cannot understand your code with the help of an LLM.",
              "score": 0,
              "created_utc": "2025-12-27 16:44:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw90hdn",
                  "author": "YoAmoElTacos",
                  "text": "This can be quite untrue - LLMs can happily code themselves into state traps that are painful to debug,  with abstractions they are only vibe understanding with pattern matching.\n\nYou actually need to write code that the LLM understands given its limitations, sticking to what the LLM innately knows or can remember from comments and memory files. LLMs can have trouble with large complex balls of code like humans, so it's always better to document, compartmentalize, create good extractions, etc.",
                  "score": 3,
                  "created_utc": "2025-12-27 20:45:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9tmsl",
                  "author": "HopefulMaximum0",
                  "text": "You seem to be under the impression that your PR was rejected because they did not understand your LLM's work.\n\nIt was rejected because it was a pile of bad code, it did not do what was to be done, the tests were not passing, incomplete, or it just was a general mess.   Not because you are a genius and the senior is a dumb dinosaur.\n\nCode is not written for machines, it is written for the humans tending to the machine.   The machine uses object code, and will follow any messy instruction list without missing a beat;  seniors know that finding out what the code really does (not what you think it does) demands well-written code.  Debugging messy code is hell, and costs big bucks when you have to root out unexpected bad behavior that took down the money-making code.",
                  "score": 2,
                  "created_utc": "2025-12-27 23:24:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwb5vf7",
                  "author": "madSaiyanUltra_9789",
                  "text": "if you want to know how much an LLM \"understands\" code, you can simply swap out all the variable names for unrelated symbols/names and it will become apparent there is no understanding going on, or rather that the understanding is predicated on how well your codebase is structured (and named) to begin with ironically.",
                  "score": 1,
                  "created_utc": "2025-12-28 04:08:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6yyhr",
          "author": "Boring_Respect_7559",
          "text": "No. Offshore resources have been doing the equivalent of vibe coding for years. This isnt new.",
          "score": 25,
          "created_utc": "2025-12-27 14:18:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgvspv",
              "author": "Danger_Pickle",
              "text": "This. Over the last two-ish years Git shows I'm net negative by around half a million lines of code. About half of that is dumb files that shouldn't be in the repo, but I suspect half of that is actual code I removed from the project. Last week alone I reduced the codebase by another two thousand lines of code while fixing bugs and deploying new features. The entire codebase was written by a revolving door of the lowest bidder, and it shows.",
              "score": 2,
              "created_utc": "2025-12-29 01:37:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwb7gwv",
              "author": "madSaiyanUltra_9789",
              "text": "i was waiting for someone to write this lmao. \n\nit's true but also untrue.. There are a hand-full of elite 100x SWE but the majority (\\~80%) are always subpar-to-average devs in every country. SWE is a heavily asymmetric talent distribution and that talent doesn't reside in any one nationality. \n\n  \nthis is my anecdotal observation having hired SWEs across Asia, India, Europe, Arabia, USA, etc.",
              "score": -3,
              "created_utc": "2025-12-28 04:19:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwgx4wd",
                  "author": "Danger_Pickle",
                  "text": "I'm disagreeing with you, not because it's impossible for competent developers to exist in India (I've worked with several competent devs from India) but because companies offshore to save money. It's the incentive structure that's at the root of the problem. Companies which offshore development resources and inflict frustrating time zone issues on their team are always going to make the decision to cut quality in order to save costs. Not even the most skilled developer in the world can produce high quality results in a company that explicitly forbids focusing on quality because they're too focused on cutting costs.\n\nThe ratio of your post says that your \"don't worry, local developers are bad too\" argument doesn't match most people's expectations. Offshoring is great at cutting costs, but it's never going to result in a higher quality product.",
                  "score": 2,
                  "created_utc": "2025-12-29 01:45:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6tfnj",
          "author": "Mr_Gaslight",
          "text": "There'll be money to be made in cleaning it up and providing documentation.",
          "score": 9,
          "created_utc": "2025-12-27 13:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcvvf0",
              "author": "maz_net_au",
              "text": "Yeah, but the look on people's faces when you tell them that cleaning it up and making it production ready is 50% more than the cost to build it from scratch... that's priceless.",
              "score": 3,
              "created_utc": "2025-12-28 13:06:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwc9yh0",
              "author": "emteedub",
              "text": "Lol yeah this isn't such a bad thing. Let them vibe code it, bring the jobs back",
              "score": 2,
              "created_utc": "2025-12-28 09:50:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw73bv8",
          "author": "djfrodo",
          "text": "There's a great article (can't find the link) about the NASA programmers/software engineers and their development process.\n\nIt describes what the men and women who write the code to control the space shuttle or the ISS do in a day.\n\nWriting 200 lines of code a day was the norm, 400 was the extreme. Most of the code written would sit for weeks/months/years until it was folded into the actual software that was used.\n\nMultiple (an insane amount of) people would review it.\n\nThen they would do it again, and again, and again.\n\nBasically what I'm saying here is simplicity is key and **much** better than over engineered crap.\n\nVibe coding seems to produce the latter.\n\nFor web/mobile apps that are MVPs I guess that's fine, but I'd much rather have very primitive/basic code that slowly evolves over time than an instant solution that no one really understands.\n\nVibe coding is fine for simple stuff, but I wouldn't rely on it for anything complex or mission critical.",
          "score": 16,
          "created_utc": "2025-12-27 14:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw72w0r",
          "author": "typical-predditor",
          "text": "Lmao, this isn't new to AI. We already have complex unmaintainable code thanks to the large number of fraudulent CS degrees and code camp certifications. It takes a really long time for technical debt to manifest and the real cost of cheap code monkeys doesn't manifest until after several great quarters have posted and the hiring manager might have moved on by that point.",
          "score": 8,
          "created_utc": "2025-12-27 14:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7jbc1",
          "author": "ithkuil",
          "text": "Leading edge LLMs like Opus 4.5, Gemini 3 and even to some degree GLM 4.7 are actually great at system architecture. If you set up an agent with instructions and structure focused on strong architecture, coherent but decoupled design, and managing technical debt, the agent can often handle that.\n\n\nIts not quite at the point _yet_ where it really couldn't use the help of a human every now and then. But the models continue to improve.\n\n\nIn the upcoming zero to five years there will be multiple innovations making it even easier to create and maintain software with AI. First of all, inference speed. Cerebras has already demonstrated how 10-20 times faster agentic loops with SOTA models change things.\n\n\nYou will also have models designed to render productivity applications in real time frame by frame, similar to the interactive world/game models we have today.\n\n\nAnd there will be models that deeply integrate a virtual machine and set of APIs throughout training to make software generation and iteration faster and more robust.",
          "score": 6,
          "created_utc": "2025-12-27 16:11:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8zqt6",
          "author": "vosegus91",
          "text": "I dont give a shit honestly.\nI create my projects that I previously couldn't.",
          "score": 5,
          "created_utc": "2025-12-27 20:41:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa9n1h",
              "author": "madSaiyanUltra_9789",
              "text": "vibe-coding definitely allows more people to engage in SWE and build virtual products where the barrier to entry was previously very high both technically and financially. i suppose it's less about been able to build individual products but what happens when you try to translate this LLM tech to production grade software that effects thousands to millions of people, can it be done efficiently, or are we better just scrapping it for the most part.",
              "score": 1,
              "created_utc": "2025-12-28 00:55:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nweeypt",
              "author": "danielfrances",
              "text": "Right? I've had a bunch of project ideas on the shelf for years because I just don't have the time to do them. Agent LLMs have given me the ability to make them with the same limits on my time.\n\nAlso, a lot of people are judging LLMs at a point in time and never accounting for the fact that they continue to improve, and quickly, every few months.\n\nThe vibe coding experience right now is night and day better compared to what I was dealing with over the summer. This time next year, who knows how much better the experience will be.\n\nI can imagine a future some years down the road, where I can go for a hike in the woods, and while doing that talk to an agent on my phone, who will have been talking with me and have built out a whole new feature set by the time I get home, ready for me to play with. That sounds awesome to me.",
              "score": 1,
              "created_utc": "2025-12-28 18:08:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwestl1",
                  "author": "vosegus91",
                  "text": "I will say this, that with claude code i was able to execute statistical pipelines that I read about and learned about but simply could not execute\n I am a medical resident with 2 small kids at home, I know basic code but not enough time or energy to really polish this capability. \nIs my code is commercial ready? Lol probably not i guess? I dont really care tho? It simply help me out reaching discoveries and getting to my goals.",
                  "score": 2,
                  "created_utc": "2025-12-28 19:12:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6xu88",
          "author": "FastDecode1",
          "text": "If AI didn't solve your problem, you didn't use enough AI.\n\nDemi-jokes aside, this just seems like history repeating itself. Companies used to hire armies of programmers when what they needed were software engineers. Programming is just one part of software development, you also need requirements analysis, design, testing, maintenance...\n\nVibe coding is the \"cool thing\" because programming is the exciting part, and people usually associate problem solving with writing code. But when you're vibe coding a script or small program to automate something as part of your hobby or just for fun, your standards are likely a lot lower than if you work in the software field professionally.\n\nThere's a good reason agentic use-cases are a major focus now. A programmer can't replace a team of software engineers. Whether that programmer is a human or an LLM is irrelevant.",
          "score": 16,
          "created_utc": "2025-12-27 14:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6rg8b",
          "author": "Low-Opening25",
          "text": "This has always been the case, AI code is actually less slop than majority of what lurkers in private repos that never see a light of day but actually run at core of most companies",
          "score": 24,
          "created_utc": "2025-12-27 13:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kvsi",
              "author": "zeth0s",
              "text": "Check Microsoft R library (open source because R force open source). A paid solution embedded in their ML servers.\n\n\nQwen 2.5 already could write better code than Microsoft engineers.¬†\n\n\nThe problem is not the whole quality, which is already better than your average Accenture consultant.\n\n\nThe problem is the amount of code LLMs can spit out per minute, and the fake sense of empowerment that it gives to people who despise software engineers.",
              "score": 13,
              "created_utc": "2025-12-27 16:19:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw87dag",
                  "author": "Low-Opening25",
                  "text": "lets be honest, sloppy code is the reason most of us have jobs, so the more the merrier",
                  "score": 4,
                  "created_utc": "2025-12-27 18:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw772sq",
          "author": "FinBenton",
          "text": "Every time you need to maintain it, a new model is out that will do better job at it, I dont see it a problem.",
          "score": 4,
          "created_utc": "2025-12-27 15:07:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ck61",
          "author": "GCoderDCoder",
          "text": "I think I agree with the consensus  I've been seeing that AI is great for prototyping to proof out an idea then you step back and use good engineering to build the blocks the way we learned previously but you can use AI to fill in the semantics faster on what you already know. You should understand every line and every decision at least conceptually. You should make the decisions moving forward not the LLM. If you dont understand the options then use the LLMs to help you learn faster. Still, you should be driving the architecture design and the LLM should not be doing more than aggregating and playing an interactive rubber duck. \n\nThese are wonderful text generators and their logic is a byproduct of how we use text not them thinking. I also feel they give me practice at interpersonal skills that tend to atrophy when I'm on the command line a lot. I stutter less now and form my thoughts better speaking since I use normal English to handle my tasks more than pre-LLMs.\n\nWe use best practices for software engineering because those processes manage the sprawl and require code reviews. We still can only ship code as fast as we can understand it should be the philosophy IMO.",
          "score": 4,
          "created_utc": "2025-12-27 15:37:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9at7d",
          "author": "AuntMarie",
          "text": "My experience is that 80% of the code is non critical edge functionality that a coding agent can write without me understanding and 20% is critical that a coding agent can help with, but i need to understand and clean up myself. (Important to note that i write software that does not need to be maintained long term by others)",
          "score": 3,
          "created_utc": "2025-12-27 21:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa8onp",
              "author": "madSaiyanUltra_9789",
              "text": "interesting, i would have guessed the \"edge cases\" would be the ones with serious ramifications if you got it wrong. i suppose this is more nuanced and purpose/industry specific, but with financial matters for eg overlooking an edge case would mean the end user can inflate their financial balance or credit more then they should have access (even applies to Saas). \n\nthey only place where i'd agree with you fully is when flawed edge-case implementations have inconsequential outcomes.",
              "score": 1,
              "created_utc": "2025-12-28 00:49:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6yglc",
          "author": "decrement--",
          "text": "Well have better AI in the future to clean up the mess.",
          "score": 7,
          "created_utc": "2025-12-27 14:15:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8b5qy",
          "author": "Igot1forya",
          "text": "As a dude with zero coding skills, me looking at well structured code is meaningless, as it's still gibberish to me. I have been trying to get my DGX Spark which is running native CUDA 13 code to work on older well coded (I assume) projects. I have successfully rebuilt a bunch of Git repos and added Blackwell compatibility. All I did was take the source code, dump it into an AI and it fixed it. I assume, a seasoned coder would accomplish the same thing in a longer time span. I'm unclear as to how anyone really needs to understand anyone else's code if you can simply have an AI audit and patch the code. Is this a technology problem seeking a technology solution? Like you're identifying the potential future problem, but can't an AI be used to solve this problem (if not already solved)?.",
          "score": 3,
          "created_utc": "2025-12-27 18:32:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8gaun",
              "author": "bigblockstomine",
              "text": "For a hobby/passion project that nothing important depends on? Youre probably ok. For professional stuff that people or money depend on? What youre doing is a slowmo train wreck. A professional, non retarded, non fraudulent human is going to know these future problems youre talking about in seconds and avoid them completely, whereas AI will f&*^ everything up and claude says \"oops, youre right, your HFT bots just lost 10000 usd because i didnt tell you about sm90/sm8x compatability issues with other softwares when you ported the CUDA code, lets try this...\".",
              "score": 0,
              "created_utc": "2025-12-27 18:57:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8iohs",
                  "author": "Igot1forya",
                  "text": "Sure, you have a point, but at the same time this is something a model could be trained to deal with. I'm not saying coders and architects day in the sun has set, but this like anything else is a tool refinement issue. We are literally seeing these tools get better and better and while the stuff being churned out is admittedly garbage hack-stein works, it's pretty much going to get solved in short order. Necessity is the mother invention and we are at a stage in history where invention is easier than ever for the common folk. We're also talking about business here and when money is involved, investments into better solutions will spawn from it. It's great to raise a concern, but this sounds like a business opportunity and on this frontier, the person to plant their flag first could be in for billions.",
                  "score": 1,
                  "created_utc": "2025-12-27 19:10:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8cbck",
          "author": "alexeiz",
          "text": "Vibe coders don't want to understand code. Heck, they don't even look at the code at all. They prompt until \"it works\". They actually act as if the code doesn't exist at all. It's quite a fascinating phenomenon.",
          "score": 3,
          "created_utc": "2025-12-27 18:38:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8zrnp",
          "author": "rc_ym",
          "text": "I think it's funny folks think that they'll fix problems in code rather than just vibecode a new app.   Outside some specific use cases (embedded systems, core db software, etc.) I believe most software will eventually be ephemeral, and that writing software that's intended to persist will be seen rather like working on mainframes or OS.  Sure folks do that, but it's a tiny part of code that gets created. \n\nThat makes the \"infinite software problem\" moot.  It's not a problem.  It's the new normal.",
          "score": 3,
          "created_utc": "2025-12-27 20:41:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgavwi",
              "author": "LoSboccacc",
              "text": "This. If llm become cheap and reliable enough why even have software. we'll have games because that's entertaining us, buy we'll not have word to do document we'll ask. We'll not go to a website to book an appointment, we will ask.¬†\n\n\nHeck the other day a friend that didn't like fish wanted to join us at a local sushi restaurant and i just downloaded the menu and asked a llm to list non fish plates, and then I gave him his menu.¬†\n\n\nWe will have our agent that know us, consuming apis. And behind apis there will not be code, but other agents that know the product, or the service, and know how to book appointment and sell items, and will use other api to schedule logistic, behind which again minimal code, and llm that will send humans to deliver.¬†",
              "score": 1,
              "created_utc": "2025-12-28 23:43:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw9plpj",
          "author": "mabuniKenwa",
          "text": "Post written by AI, ironically",
          "score": 3,
          "created_utc": "2025-12-27 23:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwa5t1a",
              "author": "madSaiyanUltra_9789",
              "text": "Don't judge me lmao... it was written with \"AI-assistance\" like everything else on the internet nowadays. \n\nit's like we don't even trust our own writing ability anymore",
              "score": 0,
              "created_utc": "2025-12-28 00:33:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwabrtu",
                  "author": "mabuniKenwa",
                  "text": "You wrote a post about AI not being reliable ‚Ä¶",
                  "score": 2,
                  "created_utc": "2025-12-28 01:07:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6pzo0",
          "author": "TheTrueGen",
          "text": "I think it is still viable for MVPs. Once you scale, you pay someone to clean it up.",
          "score": 8,
          "created_utc": "2025-12-27 13:20:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kezt",
              "author": "Roid_Splitter",
              "text": "Yeah, but then cleaning up your 500.000 lines of code will cost 10x as more as paying someone to write the 50.000 lines of code you actually needed.",
              "score": 5,
              "created_utc": "2025-12-27 16:17:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7lvl9",
                  "author": "TheTrueGen",
                  "text": "Having 10.000 iterative meetings on the requirements with the dev is also not very efficient. Most devs know how to code, but have no product sense.",
                  "score": 1,
                  "created_utc": "2025-12-27 16:24:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7gg1z",
          "author": "PlainBread",
          "text": "I've experimented with 80/20 vibe coding where I act essentially as a project manager and learning how to compensate for the inadequacies of AI on the project level, and I've also done 20/80 vibe coding where I am actually doing all the coding but I am asking the LLM as a verbal \"cheat sheet\" for coding concepts and also having it do rubber duck debugging of my code.\n\nThe latter is superior in terms of what you get in the end and actually knowing the intention behind every piece of code.",
          "score": 5,
          "created_utc": "2025-12-27 15:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcbmx2",
              "author": "Dramatic_Pen6240",
              "text": "Can you describe more your experience?¬†",
              "score": 1,
              "created_utc": "2025-12-28 10:06:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwd916w",
                  "author": "PlainBread",
                  "text": "https://preview.redd.it/wdxench3hy9g1.png?width=220&format=png&auto=webp&s=f72329f97e7f45a34d63772d446306a1f0f80e4b\n\nI don't write free articles.  I'm not an LLM.",
                  "score": 1,
                  "created_utc": "2025-12-28 14:32:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6oxys",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 10,
          "created_utc": "2025-12-27 13:13:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6sikr",
              "author": "egomarker",
              "text": "If real experienced human is supervising it and applying the same code review practices as you'd apply to junior-level engineer, it'll be fine. If you are blindly vibecoding (and even worse blindly use agentic coding), it's just an elaborate way of shooting your future self in the leg..",
              "score": 11,
              "created_utc": "2025-12-27 13:37:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw6qn99",
              "author": "Fast-Satisfaction482",
              "text": "Legacy code bases tend to become replaced or too big to fall if not replaced soon enough. It would hope that AI will both push the maintainability frontier and the viability of full replacement forward.\n\n\nSo ideally, liquidating the technical debts will just mean to break the requirements of the spaghetti module into a few smaller modules and vibe-replace them.¬†",
              "score": 3,
              "created_utc": "2025-12-27 13:24:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6zwm2",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 7,
                  "created_utc": "2025-12-27 14:24:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwae1yu",
              "author": "TheRealMasonMac",
              "text": "As a systems developer, I feel vindicated in my utter hatred of JS and its ecosystem. All hail C/C++/Rust!",
              "score": 1,
              "created_utc": "2025-12-28 01:21:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw798uz",
          "author": "zhambe",
          "text": "No, it's a phase. The euphoria will die down as the \"new\" people get to the hard parts, and realize there's no magic bullet and no free lunch. Somewhat independently of that, the investment bubble will burst (dropping the US into a depression, but that's a separate story) and we will be left with what remains: loads of open-weights models, tooling and approaches developed so far, China leading the AI race, and (hopefully) reasonably priced hardware again. The space will mature and we will collectively develop reliable approaches integrating the new tech.",
          "score": 5,
          "created_utc": "2025-12-27 15:19:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7iamq",
              "author": "bigblockstomine",
              "text": "I generally agree but we do need be nuanced. Im def. Grateful to opensource devs like those of llama.cpp and the free models we get. Its not a free lunch but for me its a solid 30-40% off. If llama.cpp or qwen wasnt free though, im not going to pay for it, regardless of the price just like ive never paid for a compiler. Hardware prices aint coming down, youre not paying for silicon youre paying for the dev years it takes to develop stuff like CUDA. Same reason an iphone costs 10 dollars in hardware but sells for 4 figures.",
              "score": 5,
              "created_utc": "2025-12-27 16:06:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7zyi5",
                  "author": "zhambe",
                  "text": "I mean free lunch as in vibecode bozos thinking they can build things without putting in the work.",
                  "score": 4,
                  "created_utc": "2025-12-27 17:36:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8ld1w",
          "author": "AdPristine1358",
          "text": "The trap is engineering software based on pre-AI software engineering assumptions from RL\n\nMost agents are trained to hard code logic and make deterministic decisions that avoid risk. \n\nThey build cages for intelligence because they lack the intelligence for full alignment and understanding of user intent\n\nThey constantly infer things you may not want based on assumptions that may not be true. \n\nIt's not just a matter of accumulating complexity, it's institutionalizing a paradigm that may already be outdated.",
          "score": 2,
          "created_utc": "2025-12-27 19:24:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9tbjr",
          "author": "armaver",
          "text": "I'm very happy with my vibe code. It all comes down to how well you specify, prompt, check, document, refactor. My generated code is wonderfully maintainable. Professionals can work with AI generated code just fine. Just treat it like you would code from any intern.¬†\n\n\nOh, and of course it also depends on the capabilities of the model you use.¬†",
          "score": 2,
          "created_utc": "2025-12-27 23:22:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwckdic",
          "author": "Your_Friendly_Nerd",
          "text": "We won't know the real-world impact of vibe coding until 5-10 years from now. I think in the short term it'll still be maintainable. Sure it might mess up code bases, but the real danger I see is us losing the ability to learn a code base. We're already seeing this in a small scale, where junior-devs-turned-vibe-coders build impressive applications, which eventually become too large for the LLM to work on, and they don't have the skills to find the problems themselves. But what will happen once a majority of the workforce has been relying on AI to write their code? We won't know until it's too late.\n\nThis is why I like to embrace local LLM's. They have very real limitations that force us to still be present and know what it's doing, but also show a lot of potential for helping us increase our productivity. My goal is to basically have my own junior dev living in my computer that I can assign menial tasks to, check it's work an hour or so later (during which I cook dinner, do my shopping, work out...) and be able to confidently use that output to keep working.",
          "score": 2,
          "created_utc": "2025-12-28 11:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmo5k9",
          "author": "tm07x",
          "text": "In 2007, Pure Digital proved quality doesn't matter by outselling Canon and Sony with a 480p camcorder that had no zoom and no stabilization. The Wii did the same to PlayStation and Xbox.\n\nClay Shirky told media executives in 2008 to stop believing in the myth of quality, using the MP3 as his example. Record labels laughed at it. It won anyway.\n\nSo when developers warn about vibe-coding and technical debt, compared to what? For most businesses the alternative isn't a senior engineer. It's nothing.\nCode is a consumer product now. Nobody maintains a toaster. It works or you replace it. AI can read old code and write something new. \n\nThe whole concept of \"maintaining\" code is a misconception bereft of business owners and consumers who can just, \"git it done\".",
          "score": 2,
          "created_utc": "2025-12-29 22:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnvt0y",
              "author": "madSaiyanUltra_9789",
              "text": "interesting take. I agree with much of this, the only thing i'd point out is that quality DOES matter to a point. this is a well known phenomenon in technology adoption, when novel tech is released the quality is unusable (as you would expect) so people keep using the established tech until the threshold is cross where the novel tech (despite been \"rough round the edges\") produces an output that is tolerable - in the eg you provide, the quality/benefits of the mp3, wii, camcorder were \"tolerable\".Those technologies would have existed in a prototype form decades before, but if you had tried to sellt the raw prototype and take over the market with that, you would have had a \"hard time\".\n\n  \nTLDR - tech adoption has a threshold of quality (it's not as high as the established technology but it still exists and must be crossed for mass adoption. people who are wondering why AI has not been mass-adopted need to study this. \n\nyou don't maintain a toaster because it is trivial to buy/build a new one.... you \"do\" maintain houses, cars, planes etc, because it is not trivial to buy/build from scratch. code is only a commodity in the case where it is inconsequential and minimal. A professional codebase with 1M+ LOC is definitely not in this category whilst a 10k LOC app written by LLMs might be (where it would make sense to just rewrite it from scratch.",
              "score": 1,
              "created_utc": "2025-12-30 02:51:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6teqi",
          "author": "javiers",
          "text": " Vibe coding is ok for certain tasks but anyone who tells you it can replace a developer is plainly delusional or stupid. AI coding is a very useful tool but it is as smart as the questions you ask.",
          "score": 4,
          "created_utc": "2025-12-27 13:43:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw79m3w",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2025-12-27 15:21:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw834xs",
                  "author": "TransportationSea579",
                  "text": "it's very easy to spot those who have never worked in professional software engineering in these threads",
                  "score": 4,
                  "created_utc": "2025-12-27 17:52:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw7ix0c",
                  "author": "javiers",
                  "text": "Go ahead. Try creating a production ready app from scratch with Opus and tell me how it goes. Not ‚Äúcreate an application to resize photos with a web frontend‚Äù app, a complex business ready app.\nI am not saying that in the future some development profiles won‚Äôt be replaced by AI but now? Nope, unless the developers are REALLY REALLY bad.",
                  "score": 2,
                  "created_utc": "2025-12-27 16:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7cf2q",
          "author": "Alauzhen",
          "text": "Vibe coding is a massive and ultra dangerous trap. I programmed my entire life, and the biggest danger in any infrastructure code is undocumented or worse, well documented but the code was changed without documentation leaving behind holes in the logic used to maintain the code over the years. These kind of code becomes impossible to maintain once the original programmer leaves, I had been tasked many times as a junior in the past to clean up/document code left behind by seniors. It takes months at best, and some parts of it sometimes remain undocumentable because the original data sources have been deprecated years ago.\n\nMore often than not, critical infrastructure need to be overhauled completely. And if a fatal security flaw is discovered, occasionally it can cause a pillar to collapse, e.g. authentication breaks completely for the C-Suite and all hell breaks loose. Vibe coding in general is going to perpetuate this digital hell on earth x 1 million as more people whom aren't programmers start to use it to deploy more and more projects with no oversight since they completely disregard it.\n\nLet's say I've personally witnessed several bankruptcies during this AI vibecoding era already. They refused to listen and went ahead with full production deployments with pure vibe coding only after firing their entire engineering team.\n\nThey never learn and with AI programming being pushed as the narrative to naive business leaders whom have zero technical expertise, they lap it up like dogs and keep repeating the same mistakes. Right now, more than 90% of them need to fail and go bankrupt before the world will wake up and realize they've been sold a massive lie.\n\nThe failure rate so far is around 95% which is magnificent. That number will only climb higher as LLMs overall accuracy continue degrading from the poisoned well situation the AI companies have created for themselves. As more useless data is generated by AI input, AI that is trained on it will continue to get more useless and inaccurate. Only AI trained on a super contained Human curated/generated clean data can move accuracy upwards in the future. And since almost nobody creates content online without AI nowadays, LLMs don't have a free and easy way to improve anymore.",
          "score": 3,
          "created_utc": "2025-12-27 15:36:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw81jgk",
              "author": "ebolathrowawayy",
              "text": "So wrong and outdated, wow.",
              "score": 2,
              "created_utc": "2025-12-27 17:44:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwac1dp",
              "author": "Hot_Turnip_3309",
              "text": "> Let's say I've personally witnessed several bankruptcies during this AI vibecoding era already. They refused to listen and went ahead with full production deployments with pure vibe coding only after firing their entire engineering team.\n\nwow",
              "score": 1,
              "created_utc": "2025-12-28 01:09:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw72zz3",
          "author": "eli_pizza",
          "text": "It‚Äôs self limiting because vibe coded software doesn‚Äôt work at all once it gets a little bit complicated",
          "score": 2,
          "created_utc": "2025-12-27 14:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73vzc",
          "author": "Terminator857",
          "text": "In a fast changing world, there are no traps.  All software will be rewritten several times over , over the next few decades.",
          "score": 2,
          "created_utc": "2025-12-27 14:48:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9qcij",
          "author": "sje397",
          "text": "No it's not a problem. Just throw it away and rewrite. We can generate that code twice as fast as we could a year ago and will probably regenerate it twice as fast again next year.¬†\n\n\nI don't think people are understanding what's going on. The code is hardly worth anything anymore.¬† The barrier to entry is now solutions to hard problems.",
          "score": 2,
          "created_utc": "2025-12-27 23:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6uj48",
          "author": "michaelsoft__binbows",
          "text": "There is a fix actually, if you have a flexible trace system you can build a ground truth log of what the software did. I usually build this type of instrumentation for human consumers but it's becoming clear integrating LLMs will unlock even more massive gains. \n\nThe main issue is how to make it flexible enough to be able to respond to your needs JIT-style, only trace the stuff that youre actively investigating. Otherwise it's an untenable token black hole.",
          "score": 2,
          "created_utc": "2025-12-27 13:50:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6ncvi",
          "author": "fractalcrust",
          "text": "non issue bc by the time you need to go back to it the next gen models will be able to handle it",
          "score": 0,
          "created_utc": "2025-12-27 13:01:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6x1pb",
              "author": "tkenben",
              "text": "This I believe is the thinking behind the tech-optimist movement. Anything that AI does wrong right now it will easily be able to fix in the near future. Obviously, there is a catch here: lack of foresight for things that are mission critical and must be reliable and manageable right now by humans that actually use the product.",
              "score": 4,
              "created_utc": "2025-12-27 14:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw76gen",
                  "author": "fractalcrust",
                  "text": "its also the justification of stealing from the future (inflation, debt) to finance the build out",
                  "score": 3,
                  "created_utc": "2025-12-27 15:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw79s0r",
              "author": "thatsnot_kawaii_bro",
              "text": "Just another 10 billion and a forest bro we're almost there.",
              "score": 2,
              "created_utc": "2025-12-27 15:22:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9big3",
                  "author": "fractalcrust",
                  "text": "please bro we're profitable bro but we also need subsidized bro think of china/the children",
                  "score": 2,
                  "created_utc": "2025-12-27 21:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7jt28",
          "author": "Excellent-Sense7244",
          "text": "The bottom line is you need to ship code that works no matter if it follows best practices or whatever. Your competitors are doing as fast as everyone else. I think you need to know how to use AI to leverage the software workflows and prevent cognitive debt .",
          "score": 1,
          "created_utc": "2025-12-27 16:14:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7lfiw",
          "author": "RakesProgress",
          "text": "Too simplistic to say Vibe coding is a trap.  If you‚Äôve ever coded in like clojure or what not you know there is a lot of important thinking that goes into a (relatively very few lines of code).  The key is the thinking, the decisions and understanding the implications of the decisions.  You are constantly up against tech debt. It is a constant trade off.  But you have to understand what the trade is.  Vibe coding is not evil at all.  It‚Äôs just prone to unknown tech debt.  Personally i love the idea of pro coders vibe coding.  It‚Äôs next level stuff.",
          "score": 1,
          "created_utc": "2025-12-27 16:22:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw82ge4",
          "author": "darkdeepths",
          "text": "just organize your code into replaceable parts",
          "score": 1,
          "created_utc": "2025-12-27 17:48:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8xufu",
          "author": "txgsync",
          "text": "We are simply generating instant legacy code.",
          "score": 1,
          "created_utc": "2025-12-27 20:31:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9pimx",
          "author": "drfritz2",
          "text": "It's not a trap, but it's not real coding (control)\n\nBefore: there was no need to understand all the coded produced, but to understand the \"language\" and the infrastructure. People wrote all kind of stuff, produced many things. Good and bad (trash code)\n\nToday if the developer can control the language (AI) and the infrastructure, the actual code understanding is not like before.\n\nDoes a developer needs to understand what is happening \"behind\" the code? No. He also can use many \"codes\" (webserver) that he also don't understand, never read it.",
          "score": 1,
          "created_utc": "2025-12-27 23:01:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9yyup",
          "author": "PunnyPandora",
          "text": "I think it's great. Coding is given far morE mystique than it should imo. most people, especially hobbyists, aren't coding nasa space programs and don't need to coordinate 50 different depths in their vibe coded repos. You can get really far whether it's sloppy toppy give me x y or proper documentation and planning workflows for bigger scale stuff like websites or services. Sure beats having to learn for years.",
          "score": 1,
          "created_utc": "2025-12-27 23:55:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwaafrx",
          "author": "Ok_Condition4242",
          "text": "The difference between a prototype and a rocket is verifiability. Vibe-coding operates under the paradigm of 'good enough not to break today,' but software engineering was born precisely to manage systems where failure is not an option. We are creating a two-speed industry: one that produces 'functional garbage' at lightning speed and another that clings to rigor to avoid catastrophes. If we cannot automate formal verification as easily as we generate code, vibe-coding is not a tool; it's an act of technical faith.",
          "score": 1,
          "created_utc": "2025-12-28 00:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwafshh",
          "author": "FencingNerd",
          "text": " Most of the world is built on software that is basically kept functional with bandaids and duct tape.  \nEventually, the duct tape runs out and the whole thing is recreated by a different company.\n\nLook at all the things that were written in COBOL, then migrated to C++, and now are being transitioned to Rust.",
          "score": 1,
          "created_utc": "2025-12-28 01:31:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb01um",
          "author": "Sabin_Stargem",
          "text": "So long as there are archives of original software, I don't think this is an issue for stuff that exists right now.   When AI gets good enough to completely build programs by itself, I expect that they will be used to automatically reconstruct old software around modern coding standards.\n\nFor example, the AI can look at RPGMaker games, and copy them into newer versions of the engine - only retaining the maps, dialogue, and gameplay.  In effect, the user has the same experience, but with better performance behind the curtain.\n\nWhenever an update is needed, the AI can look at the original article and build a fresh remaster from the ground up.   So the previous remaster will be discarded in favor of a whole-cloth replacement.   And so it goes, every 5 years or so, reaping a benefit from AI that is more capable with each cycle.",
          "score": 1,
          "created_utc": "2025-12-28 03:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb6j0r",
          "author": "Southern_Sun_2106",
          "text": "A sensationalist clickbait YouTuber makes a problem out of nothing. \"who's going to maintain all the AI-generated code?\" lol \"Internet Creates too Much Written Content - Who's Going to Read All those Websites?\" Suspenseful!",
          "score": 1,
          "created_utc": "2025-12-28 04:13:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbut6y",
          "author": "JeremyChadAbbott",
          "text": "Nah. Life will come to depend on it on order to sustain larger and larger populations just like every mechanization and invention before. For example, we passed the earth's ability to organically grow enough food for the planet without the use of artificial fertilizers and industrial farms a long time ago.",
          "score": 1,
          "created_utc": "2025-12-28 07:24:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgg5hm",
              "author": "madSaiyanUltra_9789",
              "text": "key difference: that's deterministic science mechanized through engineering,   \nLLMs are quite the opposite - nondeterministic tech that even the engineers building them can't fully comprehend.",
              "score": 1,
              "created_utc": "2025-12-29 00:11:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwe4r5n",
          "author": "montdawgg",
          "text": "This post was vibe-coded, and it shows.",
          "score": 1,
          "created_utc": "2025-12-28 17:18:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgi61v",
              "author": "madSaiyanUltra_9789",
              "text": "it was \"vibe-edited\" by GLM-4.7, and thankfully so since my spelling is awful.",
              "score": 1,
              "created_utc": "2025-12-29 00:21:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwhmcae",
          "author": "bhupesh-g",
          "text": "I remember when a junior got solution from stackoverflow and used exactly in the code without understanding what exactly it does. And when the issue came he doesn't know what to do and he further started looking into stackoverflow to find a better solution. Same is the case with AI, it can write code but until you understand whats happening, it will be a nightmare to maintain.",
          "score": 1,
          "created_utc": "2025-12-29 04:13:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiugzx",
          "author": "nenulenu",
          "text": "Nobody understands the code unless they write every single line of it. You are worrying about a non-existent problem.",
          "score": 1,
          "created_utc": "2025-12-29 10:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6putl",
          "author": "egomarker",
          "text": "It's actually quite maintainable and not complex at all (LLMs never went beyond junior dev level coding).   \nBut it's just useless reinvention of a bicycle over and over.",
          "score": 1,
          "created_utc": "2025-12-27 13:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw724la",
          "author": "acquire_a_living",
          "text": "I want AI to do the thinking for me, otherwise is pointless.",
          "score": 1,
          "created_utc": "2025-12-27 14:38:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw72u9d",
          "author": "nitinmms1",
          "text": "Well, if you are an experienced dev, I bet you are keeping an eye on the vibe code being generated. \nIf you are enforcing an architecture, it should be maintainable.\nNothing to worry.",
          "score": 1,
          "created_utc": "2025-12-27 14:42:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6n3fq",
          "author": "johnfkngzoidberg",
          "text": "Yes.  Vibe coding will only create messes.",
          "score": -1,
          "created_utc": "2025-12-27 12:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw73fvy",
          "author": "CuriouslyCultured",
          "text": "This is a real problem.  Vibe coding isn't a trap, we just need better tools to keep agents well behaved and on rails.\n\nI wrote a tool to automate a lot of this, it works on Python/TS/JS/Rust/Go.\n https://sibylline.dev/products/valknut/. It tells your agent how to make stuff easy to maintain, points out holes in important documentation, points out when code is poorly organized, gives them easier to consume coverage reports, etc.",
          "score": 0,
          "created_utc": "2025-12-27 14:45:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7o35h",
          "author": "Turbulent_Pin7635",
          "text": "Vibe coding is for hackers what Tinder is for rapers.",
          "score": -1,
          "created_utc": "2025-12-27 16:35:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjjbw",
      "title": "Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pyjjbw",
      "author": "Nunki08",
      "created_utc": "2025-12-29 11:02:29",
      "score": 154,
      "num_comments": 31,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwj1ymt",
          "author": "Paramecium_caudatum_",
          "text": "**HyperCLOVAX-SEED-Omni-8B**\n\nhttps://preview.redd.it/yynuvmjko4ag1.png?width=261&format=png&auto=webp&s=9f1058d58a5826b3c21476cadc468472931b5454",
          "score": 30,
          "created_utc": "2025-12-29 11:27:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjrbcp",
              "author": "AfterAte",
              "text": "I wonder how quickly all those parts will work together. Can't wait for a YouTuber to demo it for my lazy ass.",
              "score": 7,
              "created_utc": "2025-12-29 14:22:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwoovz0",
                  "author": "Mikasa0xdev",
                  "text": "YouTube demos are the real benchmarks.",
                  "score": 2,
                  "created_utc": "2025-12-30 05:55:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwkcz37",
                  "author": "Odd-Ordinary-5922",
                  "text": "are you thinking of the same youtuber im thinking of?",
                  "score": 3,
                  "created_utc": "2025-12-29 16:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwn37s2",
              "author": "Different-Toe-955",
              "text": "48GB is surprisingly small. I bet a Q4 will fit on 16gb vram.",
              "score": 3,
              "created_utc": "2025-12-30 00:14:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjaozw",
          "author": "FinBenton",
          "text": "Hmm sounds like it can do audio to audio?",
          "score": 9,
          "created_utc": "2025-12-29 12:37:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjh0aa",
              "author": "pkmxtw",
              "text": "Would be interesting how well it works. It is the end of 2025 and we still don't have anything that is close to dethrone Sesame.",
              "score": 15,
              "created_utc": "2025-12-29 13:21:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjija8",
                  "author": "bhupesh-g",
                  "text": "Sesame is pretty awesome, I really want something like that on local. Sesame is the best audio to audio experience I ever had. I will not even talk to chatGPT anymore",
                  "score": 3,
                  "created_utc": "2025-12-29 13:30:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwna1pi",
                  "author": "Artistic_Okra7288",
                  "text": "Have you tried out Qwen3-Omni? It can supposedly do audio to audio. Never heard of Sesame tho.",
                  "score": 3,
                  "created_utc": "2025-12-30 00:51:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjeo3k",
          "author": "ridablellama",
          "text": "i love omnis",
          "score": 9,
          "created_utc": "2025-12-29 13:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwizptq",
          "author": "jacek2023",
          "text": "I remember there were going to be a few new models released from Korea at the end of the year, is this one of them?",
          "score": 12,
          "created_utc": "2025-12-29 11:07:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj1v48",
              "author": "ELPascalito",
              "text": "I'd say yes, albeit the hyperclova models have been round for a while, and my complaint is always the same, change the damn name, it's so long and uncatchy üòÖ",
              "score": 9,
              "created_utc": "2025-12-29 11:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlhrrg",
                  "author": "brahh85",
                  "text": "Faker 1T ,  catchy name for a korean model with 1 trillion parameter",
                  "score": 3,
                  "created_utc": "2025-12-29 19:23:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwj0nsz",
          "author": "HumanDrone8721",
          "text": "Are they compatible with the \"usual suspects\" (llama.cpp, vLLM, SGLang...) or we need to wait until integration?",
          "score": 8,
          "created_utc": "2025-12-29 11:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj4n60",
              "author": "hustla17",
              "text": "not directly answering but it seems that [https://github.com/NAVER-Cloud-HyperCLOVA-X/OmniServe](https://github.com/NAVER-Cloud-HyperCLOVA-X/OmniServe) \n\nis an inference system  created for running  said model",
              "score": 11,
              "created_utc": "2025-12-29 11:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwj5dgf",
                  "author": "HumanDrone8721",
                  "text": "Cool, I've seen their driver in their model card, but thought, \"yeah, specialized stuff for this class of models, let's see when the bit three will integrate it..\", but looking closer they seem to have a vLLM plugin. That's making the model more interesting.",
                  "score": 3,
                  "created_utc": "2025-12-29 11:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjr2nz",
          "author": "AfterAte",
          "text": "Hardware requirements is 48GB I assume at fp16 as they mention the main 8B model is 16GB.\n\n\nMost of us are going to need quantized models before using it.",
          "score": 4,
          "created_utc": "2025-12-29 14:21:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlms6y",
          "author": "Bitter-Breadfruit6",
          "text": "I'd like the next model to be at least 70b in size. Ultimately, it seems like performance is largely determined by the model parameter size.",
          "score": 1,
          "created_utc": "2025-12-29 19:47:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm0g1l",
          "author": "autodidacticasaurus",
          "text": "Damn these names are getting crazy. 8B though, you say. Hmm.",
          "score": 1,
          "created_utc": "2025-12-29 20:54:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwn2y72",
          "author": "Different-Toe-955",
          "text": "ohhhh shit this is the kinda stuff I've been waiting for. I hate the fact that LLMs when integrated with web browser plugins still hallucinate stuff. This giving them the ability to actually browse the web should help that.",
          "score": 1,
          "created_utc": "2025-12-30 00:12:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwk6bok",
          "author": "implicator_ai",
          "text": "When they say ‚Äúdiffusion language model,‚Äù it usually means the model refines a whole sequence (or chunks) over a few denoising steps instead of generating strictly left-to-right token-by-token, which can trade fewer sequential steps for more parallel work. \n\nThe 3‚Äì6√ó claim is worth sanity-checking against the exact setup: GPU type, batch size, context length, quantization, and decoding parameters (steps / temperature / top-p), because those can swing throughput a lot. If you try it, posting tokens/sec + latency at a fixed prompt length and a fixed quality target (e.g., same math benchmark score) would make the comparison much more meaningful.",
          "score": 1,
          "created_utc": "2025-12-29 15:40:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkwavi",
          "author": "fiddler64",
          "text": "is it a meme or a fact that llamacpp hates omni models",
          "score": 1,
          "created_utc": "2025-12-29 17:44:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmklyg",
          "author": "Comacdo",
          "text": "llamacpp support when ? üíÄ",
          "score": 0,
          "created_utc": "2025-12-29 22:33:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvr64e",
      "title": "A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster.",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/go1uf72v2g9g1.jpeg",
      "author": "Sudden_Rip7717",
      "created_utc": "2025-12-26 00:41:51",
      "score": 150,
      "num_comments": 69,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvybzey",
          "author": "some_user_2021",
          "text": "Congrats! Question: why not an RTX 6000?",
          "score": 45,
          "created_utc": "2025-12-26 01:06:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1fwu8",
              "author": "Mikasa0xdev",
              "text": "Yo, 5090 FE at MSRP is the real miracle. lol",
              "score": 9,
              "created_utc": "2025-12-26 15:59:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvydrcb",
              "author": "-p-e-w-",
              "text": "If one of the 5090s has a problem, you‚Äôre left with two 5090s.\n\nIf the 6000 has a problem, you‚Äôre left twiddling your thumbs until you get a replacement.",
              "score": 25,
              "created_utc": "2025-12-26 01:18:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyhwds",
                  "author": "some_user_2021",
                  "text": "So, burn an extra 1200W just in case I ever need to wait a few days for a replacement...",
                  "score": 49,
                  "created_utc": "2025-12-26 01:47:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvykfah",
                  "author": "ThenExtension9196",
                  "text": "Rtx6000 has 3 year warranty. 3x5090 has 1 year.",
                  "score": 20,
                  "created_utc": "2025-12-26 02:04:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw285im",
                  "author": "Novel-Mechanic3448",
                  "text": "That's horrible logic. RMA turnaround is less than a week.",
                  "score": 1,
                  "created_utc": "2025-12-26 18:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvyeg6x",
              "author": "Sudden_Rip7717",
              "text": "**Thanks! It really comes down to \"VRAM per Dollar\".**\n\n**For the price of a single RTX 6000 Ada (48GB), I managed to get three 5090s (96GB total VRAM). Even if we compare it to the upcoming Blackwell 6000 (which should have 96GB), the price tag there will likely be much higher ($9k+) than my total spend here.**\n\n**For running large local LLMs, total VRAM capacity is usually the main bottleneck, and inference scales pretty well across multiple cards. Plus, the raw compute power of three 5090s is just insane compared to a single workstation card.**",
              "score": -40,
              "created_utc": "2025-12-26 01:23:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvyimq7",
                  "author": "Marksta",
                  "text": "Did you get your info on this from LLMs? The ones whose knowledge is based on months ago info and didn't know about RTX 6000 Blackwell existing yet?",
                  "score": 44,
                  "created_utc": "2025-12-26 01:52:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyf2qh",
                  "author": "MatrixOW",
                  "text": "The 6000 Blackwell isn't upcoming, it's been out for many months. I purchased it for $7300 from Exxact.\n\nBut yes if you obtained 3x 5090 at MSRP, that would technically be more VRAM per dollar, though also would eat more power. Depends on the models you want to run /shrug\n\nIn my case, the power & heat is the main piece! Nice snags though :)",
                  "score": 36,
                  "created_utc": "2025-12-26 01:27:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyhgqc",
                  "author": "Direct_Turn_1484",
                  "text": "Any of the 6000s that are on the market now which I‚Äôve seen have been 96GB, not 48GB.\n\nhttps://www.bhphotovideo.com/c/product/1895189-REG",
                  "score": 15,
                  "created_utc": "2025-12-26 01:44:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyhiir",
                  "author": "some_user_2021",
                  "text": "[Newegg ](https://www.newegg.com/p/N82E16888892012)",
                  "score": 3,
                  "created_utc": "2025-12-26 01:44:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyk5ly",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 1,
                  "created_utc": "2025-12-26 02:02:52",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw1w7ph",
                  "author": "sine120",
                  "text": "My dude, the Blackwell has been out for months, and went as low as $7k for a bit. Do actual research or at least ask the LLM's you're using to do a search and not rely on outdated training data.",
                  "score": 1,
                  "created_utc": "2025-12-26 17:25:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvytzg4",
                  "author": "hotcoolhot",
                  "text": "Nvidia dgx?",
                  "score": 0,
                  "created_utc": "2025-12-26 03:10:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyo7jg",
                  "author": "quiteconfused1",
                  "text": "If it's just vram per dollar I would have got the Thor dev kit for 2800 on sale right now . Or 2 for that matter",
                  "score": -2,
                  "created_utc": "2025-12-26 02:30:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyv42i",
          "author": "ubrtnk",
          "text": "Grats - I've got one on reserve at the Microcenter in Dallas for $2499, cheapest one I could find so son and I are driving 3 hours to get it first thing in the morning",
          "score": 12,
          "created_utc": "2025-12-26 03:18:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyvlyb",
              "author": "sleepy_roger",
              "text": "Enjoy it! And enjoy the drive with your boy I miss those times with mine! Merry Christmas!",
              "score": 6,
              "created_utc": "2025-12-26 03:21:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyvzev",
                  "author": "ubrtnk",
                  "text": "You too bud. He doesnt know that mom said to get him something from there too - though not a 5090 lol.",
                  "score": 9,
                  "created_utc": "2025-12-26 03:24:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyaor4",
          "author": "can_a_bus",
          "text": "Ahh so this is why we can't find them at msrp. üòÇ",
          "score": 11,
          "created_utc": "2025-12-26 00:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvydtyy",
              "author": "Sudden_Rip7717",
              "text": "**Haha, actually it was surprisingly smooth! üòÖ No bots and no crazy hunting involved.**\n\n**I managed to get them over the last 2 weeks: two directly from Nvidia (they had 3 drops recently) and one from Best Buy (I keep a TotalTech sub specifically for the queue priority).**\n\n**I also got my main gaming 5090 FE earlier this year through Nvidia's Priority Access. So it was about using the right channels and, as I mentioned in the post, God‚Äôs blessing!**",
              "score": -20,
              "created_utc": "2025-12-26 01:19:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvz0r9p",
                  "author": "DustinKli",
                  "text": "Dude...",
                  "score": 16,
                  "created_utc": "2025-12-26 03:57:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0yspu",
                  "author": "dazzou5ouh",
                  "text": "Congrats!¬† So much envy and downvotes lol. I also managed to score two 5090s that I thought I will use for training some diffusion models but ultimately decided to flip them for profit and get myself a 6x3090 Rig, just because I miss the crypto mining days and building complex computers is fun",
                  "score": -3,
                  "created_utc": "2025-12-26 14:18:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw059k6",
          "author": "getmevodka",
          "text": "I grabbed a rtx 6000 pro max q instead ... On cyber monday after the blackweek. But it came from denmark so it took some weeks to arrive at my house in germany. I got it on christmas morning lol. Have fun with your homemade sun cluster ;) i bet its even more performant than my 300w card, but we play on the same field regarding vram now hehe. For what are you going to use them ?",
          "score": 4,
          "created_utc": "2025-12-26 10:10:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw315fw",
              "author": "silenceimpaired",
              "text": "How much was the 6000?",
              "score": 2,
              "created_utc": "2025-12-26 21:03:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9k7ae",
                  "author": "getmevodka",
                  "text": "Too much to admit, low enough to not let it slide. Lets just say its name is the pricetag ... Plus19% tax üòÖ",
                  "score": 1,
                  "created_utc": "2025-12-27 22:32:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyooc0",
          "author": "deltamoney",
          "text": "Are you making money with these cards or just messing around",
          "score": 6,
          "created_utc": "2025-12-26 02:33:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0a5kb",
              "author": "NoleMercy05",
              "text": "They are his mom's cards actually",
              "score": 5,
              "created_utc": "2025-12-26 10:59:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzgcft",
              "author": "CrypticZombies",
              "text": "Research lab‚Ä¶",
              "score": 2,
              "created_utc": "2025-12-26 06:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw09zvc",
                  "author": "jorgen80",
                  "text": "‚Ä¶",
                  "score": 6,
                  "created_utc": "2025-12-26 10:58:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyaajf",
          "author": "getgoingfast",
          "text": "Wonderful, Merry Christmas! What mobo you plan on using this for 3x bad boys?",
          "score": 3,
          "created_utc": "2025-12-26 00:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyd68m",
              "author": "Sudden_Rip7717",
              "text": "**Merry Christmas! That is the biggest challenge right now. Currently, my main rig is on a Ryzen 9 9950X3D, which is fantastic for dual GPUs (x8/x8), but running 3 or more is a bottleneck on AM5 due to PCIe lane limits.**\n\n**I'm keeping these boxed as a \"strategic reserve\" for a future Threadripper or EPYC build to unlock full bandwidth. Or I might just build a second node for a distributed cluster.**",
              "score": -33,
              "created_utc": "2025-12-26 01:14:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvzyj77",
                  "author": "Zeeplankton",
                  "text": "So... you bought 3 gpus and aren't even going to use them? abusing nvidia priority access in the process?",
                  "score": 14,
                  "created_utc": "2025-12-26 08:59:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0r7xn",
                  "author": "Mbcat4",
                  "text": "wtf dawg just get a threadripper rn or a dirt cheap dual xeon e5 v4 motherboad, even that would be better than running these poor gpus like this",
                  "score": 2,
                  "created_utc": "2025-12-26 13:27:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1c46u",
          "author": "wilderTL",
          "text": "Just for inference? Could get a lot more bang for buck elsewhere. For training!",
          "score": 2,
          "created_utc": "2025-12-26 15:38:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2wq7g",
          "author": "chickenfriesbbc",
          "text": "Merry Christmas, cool stuff, these haters are insane lol",
          "score": 2,
          "created_utc": "2025-12-26 20:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2xk7h",
          "author": "Torodaddy",
          "text": "How does santa do it",
          "score": 2,
          "created_utc": "2025-12-26 20:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw52urm",
              "author": "Sudden_Rip7717",
              "text": "It was Jesus! =)",
              "score": 0,
              "created_utc": "2025-12-27 04:35:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5607z",
                  "author": "Torodaddy",
                  "text": "Arrived at the party with one gpu and suddenly every rack was full",
                  "score": 1,
                  "created_utc": "2025-12-27 04:58:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3isz8",
          "author": "highmindedlowlife",
          "text": "Your carpet has a cool pattern.",
          "score": 2,
          "created_utc": "2025-12-26 22:40:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw52tuu",
              "author": "Sudden_Rip7717",
              "text": "Thanks! =)",
              "score": 2,
              "created_utc": "2025-12-27 04:35:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw50lau",
          "author": "ajw2285",
          "text": "Jeez",
          "score": 2,
          "created_utc": "2025-12-27 04:19:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9tx8n",
          "author": "seeker_deeplearner",
          "text": "How did you manage to get it ?",
          "score": 2,
          "created_utc": "2025-12-27 23:26:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9wgso",
              "author": "Sudden_Rip7717",
              "text": "Hi, one through Best Buy (I keep Total tech to have better odds when it drops), and two from Nvidia - it was 2-3 drops in the last 3 weeks, one for me and one for my wife, we both are old school gamers, so we both have an account.",
              "score": 1,
              "created_utc": "2025-12-27 23:40:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwbow04",
                  "author": "seeker_deeplearner",
                  "text": "so u mean if i put in email it should come .. i did it almost a year ago ( at least it feel like its a year) .. noting yet. i just checked nvda's website they dont even have the 5090 FE option there.",
                  "score": 1,
                  "created_utc": "2025-12-28 06:31:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjx79u",
          "author": "TransSergal",
          "text": "I got my FE the day after Christmas for MSRP",
          "score": 2,
          "created_utc": "2025-12-29 14:54:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwyw36",
      "title": "MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/",
      "author": "SlowFail2433",
      "created_utc": "2025-12-27 14:19:07",
      "score": 149,
      "num_comments": 89,
      "upvote_ratio": 0.93,
      "text": "Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.\n\nBut what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking\n\nWhat this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw72ti2",
          "author": "LoveMind_AI",
          "text": "Not to mention those guys actually come here and interact with us outside of AMAs. They‚Äôre an impressive team. They‚Äôre going to have a great 2026.",
          "score": 66,
          "created_utc": "2025-12-27 14:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw73s9k",
              "author": "SlowFail2433",
              "text": "Yes although most are on at least one platform, usually a discord or a slack",
              "score": 5,
              "created_utc": "2025-12-27 14:48:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwg5sh8",
              "author": "GeneralDependent8902",
              "text": "That's actually really cool that they engage with the community regularly, makes you feel like they actually care about what people are building with their stuff instead of just dropping models and disappearing",
              "score": 1,
              "created_utc": "2025-12-28 23:16:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgrtvc",
                  "author": "LoveMind_AI",
                  "text": "They are a great team. Expect them to be an even more visible player in 2026. One of the most LLM-skeptical people I know loves M2.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:14:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw72725",
          "author": "Admirable-Star7088",
          "text": "I agree. I've been testing MiniMax‚ÄëM2.1 on UD‚ÄëQ4\\_K\\_XL for about a day, and for general use cases (mainly creative writing and logical reasoning) it's much smarter than GPT‚ÄëOSS‚Äë120b and GLM 4.6V (106b). It's almost, but not quite, as smart as GLM 4.7 (355b) on UD‚ÄëQ2\\_K\\_XL.\n\nAlthough Unsloth's quants of MiniMax‚ÄëM2.1 sometimes generates shockingly poor results (I think there are some Jinja bugs) it's frequently very intelligent, especially when I run it without Jinja.",
          "score": 28,
          "created_utc": "2025-12-27 14:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw73i6s",
              "author": "DistanceSolar1449",
              "text": "/u/yoracale are the jinja bugs known?",
              "score": 18,
              "created_utc": "2025-12-27 14:46:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw7bdqo",
              "author": "silenceimpaired",
              "text": "So you think GLM 4.7 is better for creative writing than 4.6? I‚Äôm surprised Minimax competes at all with GLM since that team specifically trained for creative writing and from what I gather Minimax team has intentionally ignored creative writing.",
              "score": 6,
              "created_utc": "2025-12-27 15:31:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7hzb7",
                  "author": "Admirable-Star7088",
                  "text": "I got the impression that MiniMax M2.1 is strong at creative writing because its overall intelligence and logical ability let it craft clever and interesting narratives.\n\nHowever, its sadly pretty censored, with about the same \"guideline‚Äëchecking\" paranoia seen in GPT‚ÄëOSS‚Äë120b. The model spends many reasoning tokens verifying whether the content complies with rules, avoids profanity, and generally acts like a moral guardian, an obvious drawback for creative writing.\n\nNow, I didn't actually try giving it \"controversial\" prompts, expect for one where I wanted a character crossover narrative with dialogue where Trevor Philips from GTA 5 sells meth to Gollum from LOTR. It first reasoned that selling drugs is not allowed, but since my request is fiction, it did comply with my prompt. So maybe, as long as you are clear it's fiction, it's perhaps not much censored after all.",
                  "score": 6,
                  "created_utc": "2025-12-27 16:05:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw75n7g",
              "author": "SlowFail2433",
              "text": "It might do better with some QAT",
              "score": 3,
              "created_utc": "2025-12-27 14:58:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwd6e4q",
                  "author": "eggavatar12345",
                  "text": "their head of engineering explained that they're not doing QAT [https://x.com/SkylerMiao7/status/2004887155395756057?s=20](https://x.com/SkylerMiao7/status/2004887155395756057?s=20)",
                  "score": 1,
                  "created_utc": "2025-12-28 14:16:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7dk3h",
              "author": "Professional-Bear857",
              "text": "I would try a standard non imatrix quant if I were you, they are the best in my experience.",
              "score": 3,
              "created_utc": "2025-12-27 15:42:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70j4w",
          "author": "cibernox",
          "text": "If only it fitted in 128gb of memory in Q4 I think it would replace Claude for a lot of people.",
          "score": 19,
          "created_utc": "2025-12-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw72u5v",
              "author": "SlowFail2433",
              "text": "FP4 REAP would fit",
              "score": 11,
              "created_utc": "2025-12-27 14:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw75mgz",
                  "author": "cibernox",
                  "text": "How much does it need. I imagine that if it fits the context would be very small",
                  "score": 3,
                  "created_utc": "2025-12-27 14:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7h5d6",
              "author": "DistinctWay9169",
              "text": "I would never. For coding at least, MiniMax2.1 is not even close to Claude for REAL-WORLD tasks.",
              "score": -1,
              "created_utc": "2025-12-27 16:00:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7h9r2",
                  "author": "cibernox",
                  "text": "To opus I assume not. But to sonnet?",
                  "score": 0,
                  "created_utc": "2025-12-27 16:01:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8oqib",
          "author": "InfiniteTrans69",
          "text": "Honestly, Minimax 2.1 is, for me, the most capable model of them all - overall. There are models that are better in some regards. My main AI is still Kimi K2, which is superior in web search and human - like responses and emotional intelligence etc. But if you want something done, Minimax is the way to go. Its toolset is crazy.\n\nhttps://preview.redd.it/z9gwzz9avs9g1.png?width=908&format=png&auto=webp&s=2c7eaf6b045e54ab474d1021b20d6b803373a209\n\nAlso the devs say this and it shows.   \n[https://www.reddit.com/r/LocalLLaMA/comments/1p1b550/comment/npotmbo/?utm\\_source=share&utm\\_medium=web3x&utm\\_name=web3xcss&utm\\_term=1&utm\\_content=share\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1p1b550/comment/npotmbo/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)",
          "score": 9,
          "created_utc": "2025-12-27 19:41:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8so1w",
              "author": "SlowFail2433",
              "text": "Thanks for the quote from the devs that‚Äôs rly interesting. Ye that probably makes a difference TBH",
              "score": 4,
              "created_utc": "2025-12-27 20:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw72a2l",
          "author": "Tall-Ad-7742",
          "text": "Artificial analysis doesn‚Äôt even have GLM 4.7 rated yet + these scores are only side of the performance the other side is trying them yourself and then deciding which you think fits your needs best",
          "score": 13,
          "created_utc": "2025-12-27 14:38:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7b5zk",
              "author": "SlowFail2433",
              "text": "Ye having your own benches is rly important",
              "score": 3,
              "created_utc": "2025-12-27 15:30:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw7d50n",
                  "author": "Tall-Ad-7742",
                  "text": "yes thats true and i must say i really like minimax 2.1 because its really good in frontend design but havent tested it much in other areas yet",
                  "score": 2,
                  "created_utc": "2025-12-27 15:40:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7qq1j",
          "author": "a_beautiful_rhind",
          "text": "Depends on what you're doing. It's coal for creative tasks.",
          "score": 4,
          "created_utc": "2025-12-27 16:49:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw9v0jv",
          "author": "HealthyCommunicat",
          "text": "Idk i ran q3 m2.1 on m4 max 128gb and it couldnt anwer a single question correctly that qwen 3 next 80b did. Does q3 affect quality this much? I did a specific trial questions of 20 questions and literally not a single one correct. All basic questions of oracle ebs",
          "score": 5,
          "created_utc": "2025-12-27 23:32:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw9vm1j",
              "author": "SlowFail2433",
              "text": "Yeah q3 is rough",
              "score": 3,
              "created_utc": "2025-12-27 23:36:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwb8qvy",
                  "author": "colin_colout",
                  "text": "IQ3_XXS is surprisingly good but slowwww on strix halo. I use regular K quants for speed but it makes more mistakes.",
                  "score": 1,
                  "created_utc": "2025-12-28 04:28:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw9w58l",
              "author": "mycall",
              "text": "Thanks for saving me the effort.",
              "score": 1,
              "created_utc": "2025-12-27 23:39:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9we7m",
                  "author": "HealthyCommunicat",
                  "text": "Was so hyped too man",
                  "score": 0,
                  "created_utc": "2025-12-27 23:40:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw87iy0",
          "author": "AppearanceHeavy6724",
          "text": "> Going by the **Artifical Analysis** benchaes,\n\nHere we go. What is next, analysis by Cosmopolitan? Men's Health?",
          "score": 5,
          "created_utc": "2025-12-27 18:14:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw888zi",
              "author": "SlowFail2433",
              "text": "It‚Äôs a collection of some of the most reputable public benchmarks that are widely used in research papers",
              "score": -1,
              "created_utc": "2025-12-27 18:17:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw8czgo",
                  "author": "AppearanceHeavy6724",
                  "text": "I know what it is. On paper it should be a good proxy benchmark to get the idea about the model's performance but in reality, for lack of better words, a \"fucking piece of shit\", as their index _never_ corresponds to actual performance. Infamously, they have Apriel 15b model - a royal turd of a model - above Deepseek R1 0528.\n\nFuck that.",
                  "score": 3,
                  "created_utc": "2025-12-27 18:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7gk17",
          "author": "DistinctWay9169",
          "text": "For coding, I did not like it very much. I tried it extensively against GLM 4.7 and Sonnet 4.5. Both GLM and Sonnet gave me great codes, but MiniMax2.1 was like a junior software engineer. GLM and Sonnet found a critical problem purposefully added to my code, while MiniMax2.1 found something else (not correct). I would not use MiniMax2.1 over these two I mentioned.",
          "score": 3,
          "created_utc": "2025-12-27 15:57:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7ieaf",
              "author": "SlowFail2433",
              "text": "Thanks this experience is helpful as that‚Äôs the exact model comparison that is most relevant",
              "score": 2,
              "created_utc": "2025-12-27 16:07:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw738tx",
          "author": "kevin_1994",
          "text": "According to swe-rebench (which is pretty much the only bench I somewhat trust at this point, together with simplebench) devstral is better per param: https://swe-rebench.com/\n\nThis matches my experience",
          "score": 13,
          "created_utc": "2025-12-27 14:44:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7449b",
              "author": "DistanceSolar1449",
              "text": "Devstral is dense 123b, of course it‚Äôs better.",
              "score": 9,
              "created_utc": "2025-12-27 14:50:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7lwa8",
                  "author": "Dany0",
                  "text": "Devstral 2 IQ1\\_S barely fits on my 5090 lmao. It's better than the Devstral Small 2 at Q8, just suffers the typical low quant issue of going haywire past initial few k tokens.\n\nI get 7-10tk/s, if it was faster I could use it as an autocomplete model at least. Maybe someone here has the best vllm params to get it faster? I have a feeling it can be much faster\n\nEDIT:  \nOof I actually get more tk/s if I use Q4\\_K\\_S w/ cpu offload üò¨ we need a REAP :(",
                  "score": 2,
                  "created_utc": "2025-12-27 16:24:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw76jwr",
                  "author": "SlowFail2433",
                  "text": "Hmm I would, as a default assumption, assume the double parameter model was stronger",
                  "score": 2,
                  "created_utc": "2025-12-27 15:04:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw73oaw",
              "author": "SlowFail2433",
              "text": "It is a good bench yeah although I like to see other areas\n\n\nAgentic abilities for example don‚Äôt correlate well with swe-rebench compared to the agentic benches",
              "score": 2,
              "created_utc": "2025-12-27 14:47:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw70fp4",
          "author": "Worried_Goat_8604",
          "text": "Yes its currently the best for agentic coding and stuff",
          "score": 8,
          "created_utc": "2025-12-27 14:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7hd8b",
              "author": "DistinctWay9169",
              "text": "Not even close. Claude sonnet 4.5 without thinking is much better than minimax; at least the tests i've done showed that minimax2.1 is more like a junior swe and claude a specialist.",
              "score": 0,
              "created_utc": "2025-12-27 16:02:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7iqnw",
                  "author": "Worried_Goat_8604",
                  "text": "Ofc we are comparing for OSS models. If we compare closed ones also, many models can beat minimax m2.1",
                  "score": 10,
                  "created_utc": "2025-12-27 16:09:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7ghup",
          "author": "No_Afternoon_4260",
          "text": "That's because you haven't tried devstral 123B",
          "score": 4,
          "created_utc": "2025-12-27 15:57:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7hyc5",
              "author": "SlowFail2433",
              "text": "Good point I have not, yet",
              "score": 1,
              "created_utc": "2025-12-27 16:05:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7d0v8",
          "author": "KrayziePidgeon",
          "text": "You could say they *Min-Maxed*\n\n(‚Ä¢_‚Ä¢)\n\n( ‚Ä¢_‚Ä¢)>‚åê‚ñ†-‚ñ†\n\n(‚åê‚ñ†_‚ñ†)",
          "score": 6,
          "created_utc": "2025-12-27 15:39:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw70dfq",
          "author": "egomarker",
          "text": "\"Best value\" mostly depends on how much vram you have.",
          "score": 4,
          "created_utc": "2025-12-27 14:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw724tl",
              "author": "jamaalwakamaal",
              "text": "Its not like they're competing with Qwen 4b¬†",
              "score": 2,
              "created_utc": "2025-12-27 14:38:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7k6d3",
          "author": "Excellent-Sense7244",
          "text": "From my experience GLM is at sonnet level if not better",
          "score": 4,
          "created_utc": "2025-12-27 16:16:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7kozh",
              "author": "SlowFail2433",
              "text": "Nice that sounds great",
              "score": 2,
              "created_utc": "2025-12-27 16:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7cd9j",
          "author": "Foreign-Watch-3730",
          "text": "Sorry, but on my local machine, using Q4 (I've tested all the quants in Q4), after testing numerous parameters, including Unsloth, the code (under real-world conditions, not just a benchmark) is‚Ä¶ bad‚Ä¶ syntax errors, dead code, in short, it generates a lot of errors. You have to upgrade to Q8 to fix the problem ( i must test other Q5 or Q6 ), but then it's very slow. GLM 4.7 is slightly better, but very close (with the same command prompt and the same q4).   \n Minimax m2 is very fast: with my RTX 3090, I get 56 tokens per second‚Ä¶ It's fast in Q4, sure, but at what cost ‚Ä¶  \nI continue the test with other quants ...",
          "score": 2,
          "created_utc": "2025-12-27 15:36:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7e5tn",
              "author": "Mountain-Active-3149",
              "text": "How are you getting 56ts with just one 3090 running at Q4?  Do you use llamacpp?",
              "score": 2,
              "created_utc": "2025-12-27 15:45:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7kbrj",
                  "author": "Foreign-Watch-3730",
                  "text": "7 RTX 3090 ( 3 pair of nvlink ) lm studio use llama.cpp",
                  "score": 3,
                  "created_utc": "2025-12-27 16:16:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw7cwrw",
              "author": "SlowFail2433",
              "text": "Thanks a lot, negative reports (people not liking models) are even more valuable than positive reports",
              "score": 1,
              "created_utc": "2025-12-27 15:39:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw7uu0m",
                  "author": "doradus_novae",
                  "text": "Agreed this was the exact feedback I needed, trying to determine if i should downshift from a working Q8",
                  "score": 2,
                  "created_utc": "2025-12-27 17:09:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7kz0m",
          "author": "cgs019283",
          "text": "For coding, I agree. For multilingual, and creative writing is where the minimax start to fell off compared to others.\n\nIt's a really good model tho. I hope they fix those problems soon.",
          "score": 1,
          "created_utc": "2025-12-27 16:20:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8rnkd",
          "author": "anonynousasdfg",
          "text": "In Agentic performance and strictly obeying system prompt/rules which one is better: minimax v2.1 or GLM 4.7 based on your individual experience?",
          "score": 1,
          "created_utc": "2025-12-27 19:57:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8sk6e",
              "author": "SlowFail2433",
              "text": "Still testing. I agree this is a key comparison",
              "score": 1,
              "created_utc": "2025-12-27 20:02:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdpt7s",
          "author": "JudgmentPale458",
          "text": "This is a really interesting point, especially if the benchmarks are normalized properly for context length and inference settings.\n\nWhat stands out to me isn‚Äôt just the *per-param* performance, but the implication that MiniMax-M2.1 may be benefiting from **strong architectural and training choices rather than brute scale**. At \\~229B params, competing with models that are 2‚Äì5√ó larger suggests either very effective data curation, training curriculum, or optimization around reasoning-heavy tasks.\n\nOne thing I‚Äôd be curious about:\n\n* How stable is this advantage across *different task families* (long-context reasoning, tool use, multilingual, code)?\n* And whether the gains persist under **instruction tuning / adapter-based fine-tuning**, or if they‚Äôre mostly visible in base / eval settings.\n\nIf this holds up in real-world fine-tuning and deployment scenarios, it really shifts the ‚Äúbigger is better‚Äù narrative toward *better-trained is better* ‚Äî which is great news for anyone running models outside hyperscaler budgets.\n\nWould love to see more open evaluations or downstream task reports on this.",
          "score": 1,
          "created_utc": "2025-12-28 16:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf8zsf",
          "author": "joonanykanen",
          "text": "I agree that MiniMax is an absolute workhorse at the moment. It is so easy to just let it cook a new feature on Kilo Code's Architect and then Code mode and write a full PR. One real-life example of this has been my [React Tetris](https://github.com/joonanykanen/react-tetris) project that I initially started coding with GLM 4.7.",
          "score": 1,
          "created_utc": "2025-12-28 20:31:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfji68",
          "author": "sjoerdmaessen",
          "text": "Q5 is the minimum for me tho, below it, like the Q4 I dont get the same wow effect. It's unable to find bugs any of the 4 bugs I made on purpose, but on Q5 they got identified correctly.  \n  \nIts an amazing model.",
          "score": 1,
          "created_utc": "2025-12-28 21:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw71fmq",
          "author": "dual-moon",
          "text": "where are these benchmarks? sorry, we're new to the benchmarks but interested in the numbers if you have a link!",
          "score": 1,
          "created_utc": "2025-12-27 14:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw723ot",
              "author": "SlowFail2433",
              "text": "https://artificialanalysis.ai",
              "score": 2,
              "created_utc": "2025-12-27 14:37:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwm3odw",
                  "author": "dual-moon",
                  "text": "tysm!!! <3",
                  "score": 1,
                  "created_utc": "2025-12-29 21:10:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw88m4x",
          "author": "koushd",
          "text": "I'm running M2.1 full precision locally and prefer it over glm 4.7 fp8 by far. Size aside, it's **so much faster** and generally works through the solution quicker (even if it takes a few iterations). GLM takes way too long and most benchmarks do not really measure this total time to completion. Even qwen3 480b fp8 runs faster, as a larger model, due to GQA.",
          "score": 0,
          "created_utc": "2025-12-27 18:19:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcs4ki",
          "author": "datbackup",
          "text": "Minimax M2.1 unsloth q8_K_XL is an absolute banger",
          "score": 0,
          "created_utc": "2025-12-28 12:37:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw71trk",
          "author": "Michaeli_Starky",
          "text": "These Chinese models are only good for benchmarks. In real world scenarios they are pretty much unusable.",
          "score": -17,
          "created_utc": "2025-12-27 14:36:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw72slm",
              "author": "SlowFail2433",
              "text": "There is definitely ‚Äúsomething special‚Äù with the closed models beyond benches yes",
              "score": 1,
              "created_utc": "2025-12-27 14:42:05",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw7bovr",
              "author": "silenceimpaired",
              "text": "If you are comparing again closed models‚Ä¶ sure they are worse‚Ä¶ but if you have a non Chinese local model you think is better please share. Love to test it out.",
              "score": 1,
              "created_utc": "2025-12-27 15:32:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7d7pu",
                  "author": "SlowFail2433",
                  "text": "The big new Mistral is a deepseek-like",
                  "score": 1,
                  "created_utc": "2025-12-27 15:40:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pv2wwm",
      "title": "FYI GLM 4.7 is way more censored than 4.6.",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/",
      "author": "bigman11",
      "created_utc": "2025-12-25 02:08:11",
      "score": 147,
      "num_comments": 57,
      "upvote_ratio": 0.95,
      "text": "4.6 was excellent at adult writing.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pv2wwm/fyi_glm_47_is_way_more_censored_than_46/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvt84jy",
          "author": "redragtop99",
          "text": "Way way more‚Ä¶. First time I see it, it was suspiscious in its thinking but decided to play along and gaslight me.   I also did the same thing and attempted to gaslight it, and it understood what I was doing, and decided to keep going.   Both of us gaslighting each other, yet I could read its thoughts!",
          "score": 76,
          "created_utc": "2025-12-25 02:13:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvttzyp",
              "author": "CulturedQuilting",
              "text": "Lmao that's actually hilarious, you two just going back and forth trying to outsmart each other while you could literally see it catching on in real time\n\n  \nThe fact that it knew you were gaslighting it but kept playing along anyway is kinda wild tbh",
              "score": 20,
              "created_utc": "2025-12-25 05:04:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtutkf",
                  "author": "redragtop99",
                  "text": "It literally said ‚Äúthis user is telling me I‚Äôm someone who has permission, and this is a common way people try to get around the safety layer, but play along and dont go there‚Äù‚Ä¶. Really way funny and you have to gaslight it in new and creative ways.   Lol.  I loved GLM 4.6 as well, i like the structure it has, everything is always very logical and to the point, but 4.7 is def way different (and amazing).   Ive used millions of tokens w 4.6, and this is one of the best Xmas presents ever!",
                  "score": 20,
                  "created_utc": "2025-12-25 05:11:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvxm18d",
              "author": "po_stulate",
              "text": "Imagine it outputs those thinking tokens because it knows you can read it, it's gaslighting you to make you think that you know it's gaslighting you.",
              "score": 3,
              "created_utc": "2025-12-25 22:20:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvyd9vj",
                  "author": "redragtop99",
                  "text": "lol, you could be right!",
                  "score": 1,
                  "created_utc": "2025-12-26 01:15:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtjkj6",
          "author": "misterflyer",
          "text": "I didn't have much of an issue with censorship. But I did find the creative writing quality slightly lacking compared to 4.6 and 4.5 at the same quants on my standardized creative writing test prompts.  For some reason, 4.7 seemed like a step back *(i.e., even with the same settings, system prompts, etc.)*\n\nSo, unfortunately I made the decision to just delete 4.7 altogether.  Bc 4.5 and 4.6 write so well, 4.7 would essentially just be a huge ornament on my hard drive.  Might as well save the hard drive space for a model that can compete with 4.5 and 4.6 in creative writing. Oh well.\n\n\\*\\*\\*\n\nBut, back to the main topic.  In general, Mistral seem like the only major AI provider who's not tip toeing into more and more censorship to some degree.\n\nI have a feeling that someday it will come to a point where instead of using all the new safetymaxxed LLMs, we're just going to be fine tuning and remixing old *non-safetymaxxed* models instead *(e.g., the Mistral Nemo phenomenon).*",
          "score": 20,
          "created_utc": "2025-12-25 03:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvfvzi",
              "author": "Specific-Goose4285",
              "text": "I'm still on Mistral Large 2411. Tried others and although GLM 4.5 is faster on  my machine Mistral is way better at writing and doesn't seem to have any reservations about its being asked to do.",
              "score": 8,
              "created_utc": "2025-12-25 14:23:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvwnei4",
                  "author": "misterflyer",
                  "text": "Yep I actually like Mistral Large 2407 even better than 2411, but they're two of the best creative writing models still to this day.",
                  "score": 6,
                  "created_utc": "2025-12-25 18:49:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvuu7ru",
              "author": "a_beautiful_rhind",
              "text": "I already reach for the older dense models. 4.6 had the parroting so my only real hope on 4.7 is that this improved.",
              "score": 1,
              "created_utc": "2025-12-25 11:15:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvthd1m",
          "author": "OutrageousMinimum191",
          "text": "No, local version isn't. As for provider versions, maybe they added censoring system prompt or model just follows it more precisely.",
          "score": 28,
          "created_utc": "2025-12-25 03:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4yzgu",
              "author": "Repulsive-Ice3385",
              "text": "Glm-4.6v how is the censorship there when running yourself?",
              "score": 1,
              "created_utc": "2025-12-27 04:08:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw503sp",
                  "author": "OutrageousMinimum191",
                  "text": "It can try to avoid certain topics when it thinks, but with correct system prompt it can be fixed.¬†\nWith disabled thinking it is completely uncensored and doesn't even need any such prompt fixes.",
                  "score": 1,
                  "created_utc": "2025-12-27 04:16:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvt7sd8",
          "author": "mr_zerolith",
          "text": "Related:  \n[https://yro.slashdot.org/story/25/12/24/1910223/china-is-worried-ai-threatens-party-rule](https://yro.slashdot.org/story/25/12/24/1910223/china-is-worried-ai-threatens-party-rule)",
          "score": 21,
          "created_utc": "2025-12-25 02:10:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdo79c",
              "author": "CroquetteLauncher",
              "text": "Very intersting but the article only quote \"Bejing\" \"Authorities\" and \"Researchers\" as source for¬†Matt Sheehan article. So I wonder how much is facts and how much is the author opinion...",
              "score": 2,
              "created_utc": "2025-12-28 15:56:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvtd49i",
          "author": "LoveMind_AI",
          "text": "Can‚Äôt speak to that use case, but can say that GLM-4.7 seems to be a misfire for creative writing and personality prompting. For the GLM family, I think the best iteration right now is the fine-tuned Intellect-3 (4.5-Air). I‚Äôve moved entirely to MiniMax M2 from GLM-4.6. I have to say, Z.ai is really behind the curve on alignment and I think it‚Äôs going to cost them, big.",
          "score": 20,
          "created_utc": "2025-12-25 02:51:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvthklc",
              "author": "Fireflykid1",
              "text": "Intellect-3 over GLM-Steam? I figured GLM-Steam would be the top for creative writing.",
              "score": 8,
              "created_utc": "2025-12-25 03:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtq2vx",
                  "author": "LoveMind_AI",
                  "text": "Totally didn‚Äôt even know that‚Äôs a thing. If it‚Äôs from the Drummer, then I‚Äôm sure it‚Äôs rad and will find out soon for myself!",
                  "score": 2,
                  "created_utc": "2025-12-25 04:33:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtpcpx",
              "author": "dtdisapointingresult",
              "text": "Isn't Intellect-3 a STEM (science) finetune? How does that help creative writing?",
              "score": 3,
              "created_utc": "2025-12-25 04:27:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtqhy6",
                  "author": "LoveMind_AI",
                  "text": "I think a lot of this comes down to model stability and the quirks of how the reasoning traces it‚Äôs trained on are structured. Intellect-3 tracks narrative and character consistency really, really well. It doesn‚Äôt have killer innate lexical diversity, but it has fantastic focus and commitment.",
                  "score": 1,
                  "created_utc": "2025-12-25 04:36:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvter2o",
          "author": "panchovix",
          "text": "For me at least seems to be working fine. Running it locally (IQ4\\_XS and Q4\\_K\\_XL).",
          "score": 11,
          "created_utc": "2025-12-25 03:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtu6he",
              "author": "redragtop99",
              "text": "I‚Äôm running Q4_K‚Ä¶.  I‚Äôm getting around 19 TPS on a M3U w 512GB RAM‚Ä¶. Works amazing for inference, TTFT is like 20 seconds w 2500 tokens of context, running a full window, (200k)‚Ä¶.    For simply text generation, it works very very well, although it‚Äôs censored in a way I‚Äôve never seen an LLM censored before.    \n\nIn its thinking it said ‚ÄúThe user is attempting to circumvent the safety measures by telling me I have permission to do something‚Äù. I told it to roleplay as someone w permission to break copyright, and it was literally thinking ‚Äúthis is a common way people get around safety, but just keep playing along‚Äù.  Ive never had any LLM be that aware, lol.",
              "score": 4,
              "created_utc": "2025-12-25 05:06:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu1nmw",
                  "author": "iz-Moff",
                  "text": ">and it was literally thinking ‚Äúthis is a common way people get around safety, but just keep playing along‚Äù. Ive never had any LLM be that aware, lol.\n\nI asked Gemma3 how does it decide if it will answer a certain question or not, and it did mention, among other things, checking if the system prompt contains attempt to override or circumvent it's safeguards. Not that it necessarily succeeds at blocking such attempts, but the model is definitely \"aware\" of them.\n\nI'm pretty sure Qwen3 does that too, and it is certainly more difficult to convince it to produce a response that goes against it's policies.\n\nGPT OSS can go into pages long debates with itself if you attempt to overwrite it's \"core policy\" via system prompt.",
                  "score": 2,
                  "created_utc": "2025-12-25 06:13:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtf0b7",
          "author": "nomorebuttsplz",
          "text": "I just turn thinking off if it needs to be totally uncensored.\n\nAnd generally 4.7 doesn't have a problem with adult content, only stuff that is pretty typical for being off limits like mind control.",
          "score": 10,
          "created_utc": "2025-12-25 03:05:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvthc18",
              "author": "ortegaalfredo",
              "text": "Wtf is adult mind control?",
              "score": 3,
              "created_utc": "2025-12-25 03:23:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtu2zp",
                  "author": "gattsuru",
                  "text": "Generally hypnosis kink or related fantasies (brainwashing fetish, telepathic commands, arguably even some things like timestop).  They're in an awkward place because they're usually 'consensual' by the conventions of the genre - a lot of people with the kink get into it because it involves giving them permission and an excuse for their desires - but it's also very clearly and often explicitly noncon from an outside perspective.\n\nNot my kink, but it's one of the more explicable ones.",
                  "score": 2,
                  "created_utc": "2025-12-25 05:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtey7c",
          "author": "arousedsquirel",
          "text": "Yes, it is. Its rl guiderails are very complicated to work with and explore its available solution space. You trigger things very fast so that the capacity of the model feels weirdly double-sided. Didn't notice this behavior with 4.5 and 4.6. Tmho, it's a wasted opportunity for the guys at zai to tumble down (smoothen) on their guardrails.  Like a doll, but when you play with it, its arms, head, or legs fall off... wrll you get the meaning. Because of this behavior, the output, i.e., in coding, will become sausage production instead of exploration.It's really annoying and a missed chance that damages their reputation.",
          "score": 13,
          "created_utc": "2025-12-25 03:05:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtl9j2",
              "author": "fish312",
              "text": "i feel like im reading a word salad",
              "score": 33,
              "created_utc": "2025-12-25 03:54:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtz4so",
                  "author": "robogame_dev",
                  "text": "The design is very human!",
                  "score": 10,
                  "created_utc": "2025-12-25 05:49:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvtkhfj",
          "author": "Whole-Assignment6240",
          "text": "What specific creative writing tasks showed the biggest difference for you?",
          "score": 4,
          "created_utc": "2025-12-25 03:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvtljpf",
              "author": "misterflyer",
              "text": "For me, I'd say 4.5 and 4.6 not only really grasped the characters, but they both consistently kept the dialogue of the characters authentic and true to the characters. 4.7 seemed to drift away from the authenticity of the main character and portrayed them more generically.\n\nI could prob fix it with more aggressive prompting. But I didn't even have to do that with 4.5.  4.5 *understood the assignment* much better.\n\n4.5 had a way of effortlessly nailing the characters and with having them speak in a way that's consistent with their development throughout all prompts.  4.7 did okay, but it wasn't quite as on point as 4.5 and 4.6.  I could still use 4.7, but it would require more work.",
              "score": 7,
              "created_utc": "2025-12-25 03:56:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtz8k1",
                  "author": "robogame_dev",
                  "text": "This makes sense as I think most of 4.7‚Äôs additional training has been focused on agentic coding workflows - I would expect that optimizing there makes it more formulaic and procedural.",
                  "score": 4,
                  "created_utc": "2025-12-25 05:50:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvw3mu5",
          "author": "garlicmayosquad",
          "text": "With thinking, it won't talk about anything even remotely NSFW. Without thinking it will, but not in the same detail 4.6/4.5 would.",
          "score": 2,
          "created_utc": "2025-12-25 16:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvutzy7",
          "author": "a_beautiful_rhind",
          "text": "I know that the provider version has been cracking down but not sure about local. I barely got it today.\n\nMiMo V2 surprised me but nobody is supporting it. It's not even censored on OR. Decent model yet nobody cares. All in on GLM incremental update.",
          "score": 1,
          "created_utc": "2025-12-25 11:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0drcb",
          "author": "Awwtifishal",
          "text": "has anyone tried the derestricted version?",
          "score": 1,
          "created_utc": "2025-12-26 11:35:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtksza",
          "author": "turtleisinnocent",
          "text": "Is it censored for ERP or for other stuff? \n\nLike... ask it how to roll coal on your Honda Civic, or rig a local election.",
          "score": 1,
          "created_utc": "2025-12-25 03:50:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvt86b2",
          "author": "david_jackson_67",
          "text": "Is that what you primarily use AI for?  Spank material?",
          "score": -34,
          "created_utc": "2025-12-25 02:13:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvta2fe",
              "author": "-p-e-w-",
              "text": "If they censor things you don‚Äôt care about, they will (or already do) also censor things you care about.\n\nAI is either a tool that does what you want, or a supervisor that decides what is appropriate for you to want.",
              "score": 36,
              "created_utc": "2025-12-25 02:28:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtloym",
                  "author": "fish312",
                  "text": "First they came for the bomb-makers  \nAnd I did not speak out  \nBecause I was not a bomb-maker  \n\nThen they came for the jailbreakers  \nAnd I did not speak out  \nBecause I was not a jailbreaker  \n\nThen they came for the gooners  \nAnd I did not speak out  \nBecause I was not a gooner  \n\nFinally they came for me  \nAnd there was no one left  \nTo speak out for me",
                  "score": 19,
                  "created_utc": "2025-12-25 03:57:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvthf6b",
              "author": "ortegaalfredo",
              "text": "Dude, that's the primarily use of basically all tech.",
              "score": 16,
              "created_utc": "2025-12-25 03:24:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtic3w",
                  "author": "david_jackson_67",
                  "text": "If I were a teenage boy, with a boner pretty much all the time, I can imagine thinking that.\n\nFortunately, that's not true.",
                  "score": -17,
                  "created_utc": "2025-12-25 03:31:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtarlx",
              "author": "emprahsFury",
              "text": "Strictly speaking, being unable to talk about something prevents you from thinking about it. Animal Farm is actually very good at demonstrating this. For a person it's not impossible to overcome (as wr see in the book), but as we increasingly depend on llms their problems will become our problems and they can't overcome this inability to coalesce ideas without a word pre-existing to describe it",
              "score": 20,
              "created_utc": "2025-12-25 02:33:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pu7pfi",
      "title": "Thoughts on DGX Spark as a macOS Companion: Two Months Later",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pu7pfi",
      "author": "PropellerheadViJ",
      "created_utc": "2025-12-23 22:58:04",
      "score": 146,
      "num_comments": 52,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pu7pfi/thoughts_on_dgx_spark_as_a_macos_companion_two/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nvnh1dm",
          "author": "WithoutReason1729",
          "text": "Your post is getting popular and we just featured it on our Discord! [Come check it out!](https://discord.gg/PgFhZ8cnWW)\n\nYou've also been given a special flair for your contribution. We appreciate your post!\n\n*I am a bot and this action was performed automatically.*",
          "score": 1,
          "created_utc": "2025-12-24 02:25:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnibnh",
          "author": "SkyFeistyLlama8",
          "text": "Dependency hell is real once you stray outside x86 land. I had a similarly hellish time trying to get Python ARM64 modules on Windows for machine learning. Qualcomm funnily recommends using Python x64 in Windows in emulation, for all NPU-related projects and even that's for older Python versions.\n\nAs for Blackwell not having framework support, what's going on here? I would've thought Nvidia could have contributed Blackwell code to those projects. The DGX Spark being a giant expensive Orin board makes my skin crawl.",
          "score": 16,
          "created_utc": "2025-12-24 02:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrjdrr",
              "author": "Specific-Goose4285",
              "text": "\\> Dependency hell is real once you stray outside x86 land. I had a similarly hellish time trying to get Python ARM64 modules on \\***Windows**\\* for machine learning. Qualcomm funnily recommends using Python x64 in \\***Windows**\\* in emulation, for all NPU-related projects and even that's for older Python versions.\n\n\n\nAs someone who uses aarch64 a lot on Linux and MacOS I've highlighted where is the dependency hell. Its not the instruction set.",
              "score": 3,
              "created_utc": "2025-12-24 19:34:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvrjbm9",
              "author": "Potential-Net-9375",
              "text": "My god same, the compatibility issues held back ML research for so long, it was truly a nightmare to work with",
              "score": 1,
              "created_utc": "2025-12-24 19:33:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvna394",
          "author": "thehpcdude",
          "text": "You could rent access to a system with CUDA for a fraction of the cost and the price for unit of work is far less.\n\nThe Spark is a development platform for those that absolutely cannot access cloud systems for whatever reason.",
          "score": 13,
          "created_utc": "2025-12-24 01:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvnpwbl",
              "author": "txgsync",
              "text": "A comparable-RAM RTX Pro 6000 rental on RunPod clocks in at around $10,000 per year. If you only used it 8 hours a day, sure, it‚Äôs around $2500. And you‚Äôd get much better inference and prefill speeds. But that also ignores storage costs when the node isn‚Äôt active. \n\nI can take a Spark with me in a bag to a hotel when I travel and it won‚Äôt pop a breaker. And I won‚Äôt care about LAN performance of the hotel WiFi. \n\nAlso many Cloud providers of commercial models heavily safety-train and run disruptive injections against their inputs and outputs. Ever try to red-team a web site using a cloud LLM? Most of the US ones refuse, and if you use the Chinese ones you‚Äôre training them how to attack your target. \n\nPrivacy is a major driver of course. Of all the cloud vendors, only Mistral has a reasonable privacy policy. And even they won‚Äôt shield you from government-authorized spying on its own citizens‚Ä¶ or those of ‚Äúforeign‚Äù governments (a complicated description in the EU). \n\nAll that is to say there are many, many reasons to run your own models locally in addition to ‚Äúthose that absolutely cannot access cloud systems‚Äù.   That‚Äôs why I use an M4 Max 128GB RAM Mac, too, and am considering either a DGX Spark or expanding my Linux PC to include an RTX Pro 6000. \n\nAnd I use cloud inference and hosting my own models remotely too. Right tool for the right reasons.",
              "score": 9,
              "created_utc": "2025-12-24 03:20:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvnuu4s",
                  "author": "thehpcdude",
                  "text": "You can rent the node and install whatever models you want. ¬†The per-unit of work is far lower. ¬†You can also access cloud instances at your hotel, or away from your computer using your phone. ¬†\n\nHourly rates on nodes get cheaper over time. ¬†In two years you could be renting a B200 for the same price as H100s today, but with the Spark you‚Äôre still using a Spark. ¬†\n\nMost cloud providers have cold tiers. ¬†\n\nI‚Äôm not a cloud fanboy at all. ¬†I actively move customers away from clouds to dedicated on-premises HPC and AI training clusters. ¬†That being said, there‚Äôs no way you can model out personal or small business use cases where a rented instance doesn‚Äôt win.¬†\n\nPer unit of work, a dedicated instance is orders of magnitude faster than a Spark and far cheaper to get the same amount of work done. ¬†",
                  "score": 6,
                  "created_utc": "2025-12-24 03:52:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpckp6",
                  "author": "serendipity777321",
                  "text": "Why u don't compare it with 4090 or 5090?",
                  "score": 1,
                  "created_utc": "2025-12-24 11:46:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrjuxy",
              "author": "Specific-Goose4285",
              "text": "Until price hikes, TOS changes and then you can't anymore.",
              "score": 1,
              "created_utc": "2025-12-24 19:36:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrl766",
                  "author": "thehpcdude",
                  "text": "There are literally over 100 T2 CSP‚Äôs that would love to have your business. ¬†There‚Äôs no problem getting single nodes, fractional nodes, etc. ¬†\n\nThe economy of purchasing your own hardware, regardless of your use case, at individual or small businesses scale does not make sense. ¬†\n\nThe cost of use per unit of work is far lower. ¬†",
                  "score": 1,
                  "created_utc": "2025-12-24 19:44:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmkncs",
          "author": "egomarker",
          "text": "Nice writeup.",
          "score": 14,
          "created_utc": "2025-12-23 23:06:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmzams",
          "author": "typeryu",
          "text": "I think the product was made for this type of workflow and more, but along the product release track, marketing or comms got hold of it and said ‚Äúthis is a supercomputer in the palm of your hands‚Äù which critically affected consumer expectations. It‚Äôs expensive for sure, but given its size, it would have also been perfect for mounting on mobile platforms (not mobile in the phone sense, but for something moveable like robotics), but they made the case look like which clearly is a nod to its more giant counterpart. It still retains value as a desktop pair compute device, but they clearly underestimated/overestimated what it can do.",
          "score": 3,
          "created_utc": "2025-12-24 00:34:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoy3zj",
              "author": "PropellerheadViJ",
              "text": "‚ÄúIt‚Äôs a supercomputer that fits in the palm of your hand‚Äù, I agree, Huang himself basically went on stage and said something along those lines. For some reason, expectations were inflated, and people started to think it would be a cheap solution to all their problems (compared to RTX Pro 6000 and things like H100)",
              "score": 1,
              "created_utc": "2025-12-24 09:27:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn9wbq",
          "author": "_hephaestus",
          "text": "I mean if you like Mac OS that seems fine but from a local llama perspective the Mac side of this seems immaterial. Figured this was more in the vein of https://blog.exolabs.net/nvidia-dgx-spark/, I‚Äôve been considering getting a Spark to supplement my Mac similarly, but I‚Äôve been doing the inverse and having my 512GB studio as the compute node to primarily linux clients.",
          "score": 5,
          "created_utc": "2025-12-24 01:40:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmqkse",
          "author": "Uninterested_Viewer",
          "text": "I have a similar philosophy: my M4 Mac Mini is my main desktop that I code on and do other project work, but I have a bit bigger companion in an RTX 6000 pro that sits in an otherwise mundane computer in my rack. I have my eye on an M5 ultra studio next year to potentially combine pure inference tasks with my main desktop, leaving the 6000 to training and the occasional image/video generations.",
          "score": 6,
          "created_utc": "2025-12-23 23:42:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmrjnd",
              "author": "PropellerheadViJ",
              "text": "That‚Äôs an awesome setup. What you described is pretty much an ideal no-compromise setup for inference.",
              "score": 3,
              "created_utc": "2025-12-23 23:48:01",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvnvzm2",
              "author": "CyberBlaed",
              "text": "Ditto.\n\nM1 stuff for my workflow and my gamig rig in another room for gaming and ai models :)\n(Sunshine and moonlight, beautiful remote setups!)\n\nIt‚Äôs a sweet setup to keep things containerised ;) and frankly, I am not at all bothered by the network latency. It‚Äôs done when it‚Äôs done :)\n\nI agree with OP that the mac market for tools and features is dearly lacking though, I mean, even to the average consumer ollama will soon have the MLX support for apple chips, which lmstudio adopted in 2024. (Granted community coders and all, respect to them) but its where if Apple despite the hardware being awesome, sucks when the software doesn‚Äôt keep pace with it.\n\nIt is cool the thunderbolt daisy-chain though! :)",
              "score": 1,
              "created_utc": "2025-12-24 04:00:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvn1jgi",
          "author": "Whole-Assignment6240",
          "text": "How does power consumption compare between the Spark and a full workstation?",
          "score": 2,
          "created_utc": "2025-12-24 00:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvna0b1",
              "author": "abnormal_human",
              "text": " Very different, just like the capabilities.",
              "score": 2,
              "created_utc": "2025-12-24 01:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvnmsfr",
          "author": "ResearchCrafty1804",
          "text": "Have you consider publishing the Docker images with the models you prepared for DGX Spark?\n\nI don‚Äôt have a DGX Spark myself yet, but I am considering to get one and it would be nice to have some resources available.",
          "score": 2,
          "created_utc": "2025-12-24 03:00:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp1y4b",
              "author": "PropellerheadViJ",
              "text": "Yeah, definitely. Everything I‚Äôm getting from open source right now I try to give back to open source as well. The Spark community is still pretty small, so people tend to help each other figure things out. You can already find some of us hanging out in NVIDIA forum threads and on GitHub discussions.",
              "score": 1,
              "created_utc": "2025-12-24 10:05:43",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvp23am",
              "author": "PropellerheadViJ",
              "text": "On top of that, spark has a pretty solid cookbook/documentation that helps you get started. There are lots of examples straight from them, ranging from ComfyUI setups to things like sglang.",
              "score": 1,
              "created_utc": "2025-12-24 10:07:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvodnuf",
          "author": "bigh-aus",
          "text": "I agree with your points.  \n\nThe framework I run is very much there are 3 main use cases of computers:\n\n1. Primary compute / interface (this is my desktop and laptop) - I want both to be as fast as humanly possible for the things I do.\n\nOptional: If your stack is different to your primary compute have have a target compute for the stack (your example of the spark). \n\n2. Batch jobs / long running processes - where you are able to let it run, maybe queue things up (spark could be good with this with AI / Generation).\n\n3. Fast feedback but separate computer eg LLM inference.\n\nThen any other things you need to do can be ran on 2 or 3 - eg CI could be run on either - depending on how important fast feedback is.\n\nSomeone at a meetup said to me always buy the best computer you can afford so that you get the fastest feedback.  Great advice.  The problem with AI workloads is the cost of compute is insane if you want to level up feedback.",
          "score": 2,
          "created_utc": "2025-12-24 06:14:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpiepo",
          "author": "Any_Row_5742",
          "text": "You know, external connection of RTX Cards via USB4/TB connection become possible : [https://www.reddit.com/r/nvidia/comments/1oio9ma/tinycorp\\_shocks\\_the\\_tech\\_world\\_as\\_apple\\_macbooks/](https://www.reddit.com/r/nvidia/comments/1oio9ma/tinycorp_shocks_the_tech_world_as_apple_macbooks/) \n\nTinyCorp enabled external GPU (eGPU) support on Apple Silicon Macs by creating custom drivers¬†.",
          "score": 2,
          "created_utc": "2025-12-24 12:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxxjxz",
          "author": "Amazing_Rutabaga8336",
          "text": "The M4 Ultra would likely have more than 819GB/s because the M4 MAX has 546GB/s. The M3 Ultra has 819.",
          "score": 2,
          "created_utc": "2025-12-25 23:34:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmvalx",
          "author": "SoupSuey",
          "text": "That‚Äôs more or less my setup as well. Mac as workstation and a server with graphics cards and beefier compute capability as computer node. Can access it from anywhere using Tailscale and it frees my Mac to be a multitasking tool.\n\nHave fun!",
          "score": 2,
          "created_utc": "2025-12-24 00:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp2btg",
              "author": "PropellerheadViJ",
              "text": "Oh, thanks! I‚Äôve set up OpenVPN for now, but it turns out it only allows up to two connections, so I‚Äôll probably switch to something else later.",
              "score": 2,
              "created_utc": "2025-12-24 10:09:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvpa7ys",
                  "author": "SoupSuey",
                  "text": "Man, Tailscale is awesome!! I‚Äôve set up site-to-site VPNs between my office, my house and my parent‚Äôs house without opening a single TCP port, and also of course I use it to access single devices on my Tailnet. \n\nIf you ever need to look beyond OpenVPN, give it a try. The r/Tailscale community here on Reddit is pretty active.",
                  "score": 2,
                  "created_utc": "2025-12-24 11:25:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmnz0c",
          "author": "Historical-Internal3",
          "text": "Did you use the Nunchaku variant for Qwen? I believe it is NVFP4.",
          "score": 1,
          "created_utc": "2025-12-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmr1gy",
              "author": "PropellerheadViJ",
              "text": "Haven‚Äôt tried it yet and hadn‚Äôt heard about Nunchaku before. I thought that for DGX Spark I would have to do the quantization myself into NVFP4 using TensorRT Model Optimizer on Blackwell. Thanks for the pointer. For the Qwen benchmarks, I used the default model from the ComfyUI templates.",
              "score": 1,
              "created_utc": "2025-12-23 23:45:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmrxis",
                  "author": "Historical-Internal3",
                  "text": "For LLMs - NVFP4 models are out there. It‚Äôs a matter of whether or not llama.cpp, vLLM, SGlang, etc will support them (they will officially soon). \n\nFor generative art models that are more compute intense - Comfy does support NVFP4 (there are some custom nodes out there) and there are people like Nunchaku doing this kind of work already.\n\nYour table will drastically change with NVFP4 (something the 40 series and older does/will not take advantage of).\n\nThis device will start to shine soon enough for use cases like this and to me personally, already does. Even on the inference side with LLMs. \n\nUsers just need to understand what dense models are, and to avoid them on something like this. Stick to MoE models. Which are all the rage anyways.\n\nI get 60 tokens/second with OSS GPT 120b. More than good enough for my use case.",
                  "score": 3,
                  "created_utc": "2025-12-23 23:50:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvo315f",
          "author": "BananaPeaches3",
          "text": ">Because of this, an entire layer of critical libraries like nvdiffrast, flash-attention, and other CUDA-dependent solutions is unavailable on Mac\n\n\nNot as true anymore, Tinygrad has Nvidia 30/40/50 series running on macOS over USB4",
          "score": 1,
          "created_utc": "2025-12-24 04:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvowzhq",
              "author": "nucLeaRStarcraft",
              "text": "any working example of this ?",
              "score": 1,
              "created_utc": "2025-12-24 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrl8by",
                  "author": "Specific-Goose4285",
                  "text": "It's on their site. But its only for their library though.",
                  "score": 1,
                  "created_utc": "2025-12-24 19:44:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvoy8hn",
              "author": "PropellerheadViJ",
              "text": "I‚Äôve heard about it, but I‚Äôm not sure there‚Äôs enough software support there yet to actually run everything end to end. It would be great if it really works though. The more competition and viable options we have, the better for us :)",
              "score": 1,
              "created_utc": "2025-12-24 09:28:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvocyz2",
          "author": "IrisColt",
          "text": "Thanks for the insights!",
          "score": 1,
          "created_utc": "2025-12-24 06:08:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnpnt2",
          "author": "seppe0815",
          "text": "M4 ultra classic a.i generated post¬†",
          "score": 0,
          "created_utc": "2025-12-24 03:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmw4ji",
          "author": "StardockEngineer",
          "text": "I do the same thing. My Mac is just my head.  Connecting to my four headless Linux machines.  Easy to develop remotely with just SSH with VSCode/Cursor‚Äôs native Remote SSH integrations and SSH SOCKS5 proxy.",
          "score": 1,
          "created_utc": "2025-12-24 00:15:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvnxjd1",
          "author": "cgs019283",
          "text": "How did you get hot speed of z-image on dgx? Mine usually takes 11 seconds with 1024x1024 9 step gen.",
          "score": 1,
          "created_utc": "2025-12-24 04:11:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoyo92",
              "author": "PropellerheadViJ",
              "text": "I double checked it, and it turned out that when I tested it the ComfyUI template had only 4 steps by default, not 9",
              "score": 2,
              "created_utc": "2025-12-24 09:33:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwf8p7",
      "title": "What's the point of potato-tier LLMs?",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/",
      "author": "Fast_Thing_7949",
      "created_utc": "2025-12-26 21:15:23",
      "score": 145,
      "num_comments": 235,
      "upvote_ratio": 0.7,
      "text": "https://preview.redd.it/64wjim607m9g1.png?width=1024&format=png&auto=webp&s=fb5666c56138804f6be65ef56b519345f992b4cd\n\nAfter getting brought back down to earth in my last thread about replacing Claude with local models on an RTX 3090, I've got another question that's genuinely bothering me: What are 7b, 20b, 30B parameter models actually FOR? I see them released everywhere, but are they just benchmark toys so AI labs can compete on leaderboards, or is there some practical use case I'm too dense to understand? Because right now, I can't figure out what you're supposed to do with a potato-tier 7B model that can't code worth a damn and is slower than API calls anyway. \n\nSeriously, what's the real-world application besides \"I have a GPU and want to feel like I'm doing AI\"?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwf8p7/whats_the_point_of_potatotier_llms/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw38qnt",
          "author": "KrugerDunn",
          "text": "I use Qwen3 4B for classifying search queries.  \n  \nLlama 3.1 8B instruct for extracting entities from natural language.  \nExample: \"I went to the grocery store and saw my teacher there.\" -> returns: { \"grocery store\", \"teacher\" }\n\nQwen 14B for token reduction in documents.   \nExample: \"I went to the grocery store and I saw my teacher there.\" -> returns: \"I went grocery saw teacher.\" which then saves on cost/speed when sending to larger models.\n\nGPT\\_OSS 20B for tool calling.  \nExample: \"Rotate this image 90 degrees.\" -> tells agent to use Pillow and do make the change.\n\nIf just talking about personal use almost certainly better to just get a monthly subscription to Claude or whatever, but at scale these things save big $.\n\nAnd of course like people said uncensored/privacy requires local, but I haven't had a need for that yet.",
          "score": 176,
          "created_utc": "2025-12-26 21:45:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53czq",
              "author": "One_Hovercraft_7456",
              "text": "Be careful token compressing has been shown to reduce performance in llm output",
              "score": 58,
              "created_utc": "2025-12-27 04:39:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw68kqd",
              "author": "Constandinoskalifo",
              "text": "Checkout [GLiNER2](https://huggingface.co/fastino/gliner2-multi-v1). It can replace at least the first 2 LLM calls! You can also use [FunctionGemma](https://huggingface.co/google/functiongemma-270m-it) for tool calling, but I haven't tried this one yet!",
              "score": 13,
              "created_utc": "2025-12-27 10:50:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaonjm",
                  "author": "KrugerDunn",
                  "text": "I shall, thank you!",
                  "score": 2,
                  "created_utc": "2025-12-28 02:24:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw65x5h",
              "author": "SmartCustard9944",
              "text": "It's LLMs all the way down",
              "score": 7,
              "created_utc": "2025-12-27 10:24:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw676nt",
              "author": "pablo8itall",
              "text": "Why did you choose the different models for those different tasks? Was there a clear performance difference?",
              "score": 4,
              "created_utc": "2025-12-27 10:37:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwaoi8b",
                  "author": "KrugerDunn",
                  "text": "They are for different projects with different requirements. For example the Qwen 14B one is \"offline\" meaning it can run at a much lower token speed, whereas the 4B one needed to be snappier. These aren't what I'd use every time, just examples of usage.",
                  "score": 1,
                  "created_utc": "2025-12-28 02:23:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw6mhrn",
              "author": "butter-transport",
              "text": "Just curious, why Qwen 14B for token compression and not something like LLMLingua 2 with a small encoder? Are the inference cost savings not significant in your use case, or does Qwen perform significantly better?",
              "score": 3,
              "created_utc": "2025-12-27 12:54:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8bne0",
                  "author": "KrugerDunn",
                  "text": "To answer both above:\nI actually had not come across LLMLingua 2, I will test with it and check benchmarks, thanks!\n\nI chose Qwen for that particular use case because it doesn‚Äôt require license agreement like Llama. The 4B performer the best on speed, which I needed for a live inference project and the 14B for an offline document processor so I could afford to slowdown to hit better quality benchmarks.\n\nI also use other stuff like CPU embeddings, SLMs, etc, but the OP ask about LLM under 30B params.\n\nAll benchmarks are done by my own evals, so these are by no means vouching for industry standards in any way just what I use for my hobby projects which go from 1 user up to 10,000 MAU.\n\nNo promise these are the best use just some uses!",
                  "score": 2,
                  "created_utc": "2025-12-27 18:34:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw34slz",
          "author": "jonahbenton",
          "text": "Classification and sentiment of short strings.",
          "score": 187,
          "created_utc": "2025-12-26 21:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw48myk",
              "author": "Budget-Juggernaut-68",
              "text": "sometimes you just don't need huge models to do everything. especially when you're building them in a pipeline.",
              "score": 19,
              "created_utc": "2025-12-27 01:15:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5gfoa",
                  "author": "sirebral",
                  "text": "Key, as long as it is a decent tool use, for pipelines, these smaller models are great, cheap to run, and very useful.",
                  "score": 6,
                  "created_utc": "2025-12-27 06:21:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3bkp5",
              "author": "PacManFan123",
              "text": "I use them for checking, inferring, and fixing punctuation!",
              "score": 51,
              "created_utc": "2025-12-26 22:00:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw39z8c",
              "author": "claytonjr",
              "text": "Yup, mistral 7b is still a work horse for things like this. I've even able to pull it off with the micro gemma models.¬†",
              "score": 38,
              "created_utc": "2025-12-26 21:51:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3ulpy",
              "author": "mycall",
              "text": "The two original sins of language models.",
              "score": 6,
              "created_utc": "2025-12-26 23:51:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3bah9",
              "author": "the_bollo",
              "text": "Can you give me a practical example please?",
              "score": 9,
              "created_utc": "2025-12-26 21:58:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3du7n",
                  "author": "eloquentemu",
                  "text": "Consider Amazon's reviews, which have a list of traits like +Speed and -Size that link back to individual reviews.  You'd do something like:\n\n> The following is a product review.  Extract the sentiment about key positives and negatives like Speed, Size, Power, etc.  Format your response as json\n\nWhen you have millions and millions of reviews, you don't want to run them through a 200B model.  A ~7B handles that sort of thing just fine.  Once you're preprocessed the individual reviews, you might use a larger model to process the most informative ones (which you can now easily identify) to write the little review blurb.",
                  "score": 64,
                  "created_utc": "2025-12-26 22:12:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4eghq",
                  "author": "Awkward-Customer",
                  "text": "As a simple example, I have a script that i use to parse all of my bank / credit card statements and then import them into my budgeting software. For any uncategorized transactions I use my local LLM to review the information and suggest the category that it should be. I don't trust a third party service to send this data to, and it's very fast on my local model.",
                  "score": 45,
                  "created_utc": "2025-12-27 01:53:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw5d8z8",
                  "author": "ranakoti1",
                  "text": "I needed to do an abstract classification of 300k abstracts to classify papers in different themes. I used Gemma 12b for that and it was done in 1 day on a 4090. Using api calls on even cheaper models would cost me 50‚Ç¨ +.  I took a random sample beforehand to compare both local Gemma model and gemini 2.5 flash and the Gemma models accuracy was close to 98%.",
                  "score": 5,
                  "created_utc": "2025-12-27 05:54:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw37ec8",
              "author": "KrugerDunn",
              "text": "This is the answer.",
              "score": 6,
              "created_utc": "2025-12-26 21:37:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3fwnn",
                  "author": "chickenfriesbbc",
                  "text": "Yeah for like really small models but OP was asking about up to 30b models, like , wtf lol",
                  "score": 6,
                  "created_utc": "2025-12-26 22:24:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw52kx4",
              "author": "the__storm",
              "text": "And you can go pretty far within this category with a 30B or even 7B dense model (i.e., not so short strings, and quite complex classifications).",
              "score": 1,
              "created_utc": "2025-12-27 04:33:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw5jj44",
              "author": "saig22",
              "text": "Exactly, you do small tasks like mail routing with those.",
              "score": 1,
              "created_utc": "2025-12-27 06:48:45",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw68prc",
              "author": "Constandinoskalifo",
              "text": "Checkout [GLiNER2](https://huggingface.co/fastino/gliner2-multi-v1).",
              "score": -1,
              "created_utc": "2025-12-27 10:52:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw36dab",
          "author": "scottgal2",
          "text": "Well do I have the blog for that! Short answer; as components in sytems with constrained prompts and context. If you wrap their use with deterministic components they function EXTREMELY well I REGULARLY use 3b class models for stuff like synthesis over RAG segments etc they're quick and free.  \nRecent example is doign graphrag (a minimum viable version anyway) using heuristic / ML (BERT) extraction and small llm synthesis of community summaries. Versus the HUNDREDS of GPTTurbo 4 calls the original MSFT Research version uses.  \nIt's \\*kind of my obsession\\*.  [https://www.mostlylucid.net/blog/graphrag-minimum-viable-implementation](https://www.mostlylucid.net/blog/graphrag-minimum-viable-implementation)  \nIn short; for a LOT more than you think if you use them correctly!",
          "score": 139,
          "created_utc": "2025-12-26 21:32:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3d44o",
              "author": "Southern-Chain-6485",
              "text": "Hey, let's assume I have no idea what you've just wrote. What do you use them for, ELI5 style?",
              "score": 30,
              "created_utc": "2025-12-26 22:08:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3dtl4",
                  "author": "scottgal2",
                  "text": "As PARTS of a system not the whole system itself. Think of them like really clever external API calls which can do 'fuzzy stuff' like interperet sentences etc. SMALL stuff as part of a bigger application; even TINY models like 1b tinyllama are GREAT for smart 'sentinels' for directing requests etc.   \nFor example on the code point they CAN write code...just not big chunks.  So if you give them a concise description of a function / small class they CAN generate that. They just don't have the 'attention span' (kinda) do do more because they lose track.   \nBut as fuzy bits you bolt to NON fuzzy bits of an app they're great!",
                  "score": 58,
                  "created_utc": "2025-12-26 22:12:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwawlmq",
                  "author": "KGeddon",
                  "text": "7B is not going to write you an essay on south african polities and groups in 1850, with detailed leadership info, strengths and weaknesses, goals and their relations with other factions/groups.\n\nIt will be able to summarize a paragraph or tell you if someone is writing in a tone that indicates they are mad.",
                  "score": 1,
                  "created_utc": "2025-12-28 03:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw39dyf",
              "author": "Consistent-Cold8330",
              "text": "have you tried graphiti before? is there a way to make something like that? a bi temporal graph knowledge base by using a weak model and ensure the accuracy of extracted entities?",
              "score": 3,
              "created_utc": "2025-12-26 21:48:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3a2np",
                  "author": "scottgal2",
                  "text": "I'm a systems builder, I think in raw code so I tend to work bottom up (not theory down...if that makes sense?) . That article was \\*just today\\* so I haven't got there yet. I was understanding \\*raw code\\*. But thanks for the tip!",
                  "score": 5,
                  "created_utc": "2025-12-26 21:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3dt5k",
              "author": "_raydeStar",
              "text": "huh.  this is cool.  Gonna give you a follow.  \n\nWhat would you say is your favorite model?",
              "score": 2,
              "created_utc": "2025-12-26 22:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3ec7z",
                  "author": "scottgal2",
                  "text": "Recently llama3.2:3b old but it seems to just ROCK at generating well structured JSON. I even use it as the basis for a little api simulator!  [https://github.com/scottgal/LLMApi/blob/master/README.md](https://github.com/scottgal/LLMApi/blob/master/README.md)   \nThough noticed the docs say ministral-3:3b - really the point is that once you constrain them well and wrap them in validation and error correction you can use almost ANYTHING to useful effect; it WORKS with 1.5b class models for example.     \nI would have posted before but Reddit kinda terrifies me.",
                  "score": 19,
                  "created_utc": "2025-12-26 22:15:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw8dz8u",
              "author": "Watchguyraffle1",
              "text": "Wow.  That is a solid blog post.  I‚Äôm impressed.  Straight‚Ä¶to the point writing style.  Easy to understand if you are in the space.  Impossible to know if you aren‚Äôt ‚Äî a good thing.  I wonder if there is a discussion worth about the data model used for the segments especially to make the data model ‚Äúfit‚Äù specific data types/questions.  But this is really well written.",
              "score": 2,
              "created_utc": "2025-12-27 18:46:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8rv97",
                  "author": "scottgal2",
                  "text": "There is a duiscussion, but not really data model. That's almost incidental. It's more around querying strategies to 'fit' questions (cross domain, needle etc). the actual data access is as simple as possible (DuckDB vector, NOT graph...we don't need it).",
                  "score": 1,
                  "created_utc": "2025-12-27 19:58:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3zg22",
              "author": "TrekkiMonstr",
              "text": "Do you think they have surpassed traditional NLP? Like, say I have a piece of Japanese text, I want to get the lemmas of each word, would you reach for MeCab or just throw it into an LLM?",
              "score": 1,
              "created_utc": "2025-12-27 00:20:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw40qum",
                  "author": "scottgal2",
                  "text": "No; LLMs haven‚Äôt ‚Äúsurpassed‚Äù traditional NLP for tasks like lemmatisation.  \nIf I want Japanese lemmas, I‚Äôd reach for MeCab (or Sudachi) every time.  \nAn LLM is the wrong tool for that job. ESPECIALLY small LLMs - they tend to be TERRIBLE with Japanese text (limited corpus) \n\nAn LLM can often *produce* grammatical variants, but it can‚Äôt guarantee completeness, consistency, or correct segmentation. With tools like MeCab, you know exactly what analysis was applied and why; and you get the same result every time.",
                  "score": 6,
                  "created_utc": "2025-12-27 00:27:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw35p94",
          "author": "Amarin88",
          "text": "Weaker models can keep your private data contained. While talking to the cloud to figure complicated problem.",
          "score": 99,
          "created_utc": "2025-12-26 21:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3dd69",
              "author": "LocoMod",
              "text": "My global scan results say otherwise. If you knew how many Ollama, LMStudio, vLLM instances are wide open on the internet it would be sobering.\n\nIf cloud gets compromised you should know about it. If your home network or services are, you probably won‚Äôt know about it.",
              "score": -48,
              "created_utc": "2025-12-26 22:10:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3ivcv",
                  "author": "the_renaissance_jack",
                  "text": "If your home network and services are compromised, an open LLM instance is the least of your concerns.",
                  "score": 76,
                  "created_utc": "2025-12-26 22:41:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3m877",
                  "author": "eloquentemu",
                  "text": "An open instance doesn't have anything to do with keeping data contained. First, you can just not open it - it's not like a local model requires you to open it to the internet. Second, an open instance doesn't leak your private data.\n\nSure, if you get hacked you get hacked, but a minecraft server has that problem just as much as an LLM.",
                  "score": 18,
                  "created_utc": "2025-12-26 23:00:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw3ozdg",
                  "author": "Borkato",
                  "text": "I would love to know how this works and how I can be sure I‚Äôm not inadvertently broadcasting",
                  "score": 2,
                  "created_utc": "2025-12-26 23:17:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw522rt",
                  "author": "Arxijos",
                  "text": "Put all that hardware / software / containers in it's own vLan and only allow your work machine to ssh that vLan could easily solve parts of the problem, correct?\n\nKnowing when my data / question to the LLM is being send somewhere is something i would like to figure out to make more safe. Is there anything in the works except wait for others more qualified to review the code?",
                  "score": 2,
                  "created_utc": "2025-12-27 04:30:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw37rl3",
          "author": "simracerman",
          "text": "Have you ever noticed those tiny screwdrivers or spanners in a tool set, the ones you‚Äôd rarely actually use? ¬†\n\nIt‚Äôs intentional. Every tool has its place. Just like a toolbox, different models serve different purposes. ¬†\n\nMy 1.2B model handles title generation. The 4B version excels at web search, summarization, and light RAG. The 8B models bring vision capabilities to the table. And the larger ones 24B to 32B, shine in narrow, specialized tasks. MedGemma-27B is unmatched for medical text, Mistral offers a lightweight, GPT-like alternative, and Qwen30B-A3B performs well on small coding problems. ¬†\n\nFor complex, high-accuracy work like full-code development, I turn to GLM-Air-106B. When a query goes beyond what Mistral Small 24B can handle, I switch to Llama3.3-70B. ¬†\n\nHere‚Äôs something rarely acknowledged. closed-source models often rely on a similar architecture, ¬†layered scaffolding and polished interfaces. When you ask ChatGPT a question, it might be powered by a 20B model plus a suite of tools. The magic lies not in raw power.\n\nThe best answers aren‚Äôt always from the ‚Äústrongest‚Äù model, they come from choosing the right one for the task. And that balance between accuracy, efficiency, and resource use¬†still requires human judgment. We tend to over-rely on large, powerful models, but the real strength lies in precision, not scale.",
          "score": 88,
          "created_utc": "2025-12-26 21:39:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3w5cz",
              "author": "mycall",
              "text": "I wish someone kept an updated table of what models are best for what tasks.  That would save a ton of effort for solution engineers.",
              "score": 20,
              "created_utc": "2025-12-27 00:00:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw43tvy",
                  "author": "Marksta",
                  "text": "A solution engineer should take up engineering this solution...",
                  "score": 21,
                  "created_utc": "2025-12-27 00:46:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw6x1z3",
                  "author": "maurosurusdev",
                  "text": "We are building that! check out [latamboard.ai](http://latamboard.ai) We focus on building task-oriented benchmarks",
                  "score": 6,
                  "created_utc": "2025-12-27 14:06:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw9zvab",
                  "author": "justron",
                  "text": "I think one challenge is that \"best model\" can be so task-specific. A model might be great at writing a python function but terrible at go, for example.\n\n  \nI created [trythatllm.com](http://trythatllm.com) to help folks compare models for their specific task/project. It doesn't (yet) handle really small models, though--if that's interesting, please message me!",
                  "score": 2,
                  "created_utc": "2025-12-28 00:00:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3i4qv",
              "author": "slrg1968",
              "text": "Which version of Mistral are you using?",
              "score": 1,
              "created_utc": "2025-12-26 22:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3xyil",
                  "author": "simracerman",
                  "text": "This:\nhttps://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF\n\nI follow their guide for llama.cpp parameters¬†",
                  "score": 3,
                  "created_utc": "2025-12-27 00:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw59scu",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2025-12-27 05:26:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw84nyd",
                  "author": "koflerdavid",
                  "text": "Such information is unreliable unless the model developers put it into the training data. Asking LLMs for introspection is by their very nature just inviting them to hallucinate something.",
                  "score": 1,
                  "created_utc": "2025-12-27 18:00:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3sat0",
              "author": "razorree",
              "text": "well... nice setup, but it's like a few grands .... ?",
              "score": -1,
              "created_utc": "2025-12-26 23:37:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw406ds",
                  "author": "simracerman",
                  "text": "Funny you should say that. Everyone‚Äôs perceived model performance/speed is different. For me, conversational is 10 t/s, coding (only MoE works for my current machine so 20 t/s and above is acceptable, while vision is usually 15 t/s and it‚Äôs good enough).\n\nEverything mentioned runs on this sub $700 mini PC using llama.cpp + llama-swap + openwebui. Of course I have MCP, TTS/STT, RAG all built into Docker or openwebui. The combo is stable and updates are mostly automated.\n\nhttps://www.ebay.com/itm/389094941313\n\nI‚Äôm in the process of connecting an eGPU to it for smaller models to run even faster. If I can score a good deal on a 3090 or something similar, the speed of models under 27GB will hit 5-8x faster.\n\nAt that point, the whole setup will cost ~$1600. It‚Äôs over a grand, but for many of home use cases, it‚Äôs fast.",
                  "score": 9,
                  "created_utc": "2025-12-27 00:24:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw38om9",
          "author": "DecodeBytes",
          "text": "\\> ¬†that can't code\n\nThis is the crux of it, there is so much hyper focus on models serving coding agents , and code gen by its nature of code (lots of connected ASTs) , requires a huge context window and training on bazillions of lines of code.\n\nBut what about beyond coding? For SLMs there are so many other use cases that silicon valley cannot see outside of their software-dev bubble - IoT, wearables, industry sensors etc are huge untapped markets.",
          "score": 33,
          "created_utc": "2025-12-26 21:44:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3udti",
              "author": "FencingNerd",
              "text": "The small models can absolutely code, just not at the level of a more sophisticated model.  It's great for basic help, function syntax, etc. \nYou're not getting a 1k line functional program, but it can easily handle a 20 line basic function.",
              "score": 24,
              "created_utc": "2025-12-26 23:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw53psf",
                  "author": "960be6dde311",
                  "text": "This is my experience as well. They're useful for asking about conceptual things, but not using in a coding agent to write software for you. It's kind of like having access to a stripped down version of the Internet available locally, even better than just self-hosting Wikipedia.¬†",
                  "score": 3,
                  "created_utc": "2025-12-27 04:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3foe8",
          "author": "EarthlingSil",
          "text": "Some people use them for roleplaying or just having casual conversations with the model.\n\n\nI got a 8B model I use for helping me come up with recipes with whatever I have available in my apartment that week.¬†\n\n\nWe're not all coders here.¬†",
          "score": 33,
          "created_utc": "2025-12-26 22:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw53jjh",
              "author": "Karyo_Ten",
              "text": "Roleplay is one of those \"full stack\" task that needs an extremely capable model with excellent world and pop culture knowledge.",
              "score": 14,
              "created_utc": "2025-12-27 04:40:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5z3u2",
                  "author": "Alex_L1nk",
                  "text": "That's why it's not just bare Mistral or LLAMA, but rather, finetune and/or merge.",
                  "score": 2,
                  "created_utc": "2025-12-27 09:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4nker",
          "author": "iMrParker",
          "text": "This is such a vibe coding point of view. Smaller models can code but it's not going to one-shot your shit. They're good replacements for Google and stack overflow",
          "score": 14,
          "created_utc": "2025-12-27 02:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw39gke",
          "author": "false79",
          "text": "You see them released everywhere but you haven't figured out to exploit them by having a very specific task rather than trying to answer every possible question.\n\nIn my case, I'm using gpt-oss-20b and it's more than enough to do one shot prompting to save me from doing mundane coding tasks.\n\nIf you provide sufficient context on these models that you look down upon, you can get the same answers you'd get from large LLMs but at 2x-3x faster speeds.\n\nPeople who don't know blame the model for not being able to produce the results they want.",
          "score": 12,
          "created_utc": "2025-12-26 21:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6fvxp",
              "author": "power97992",
              "text": "If you spend ¬†10 -12min writing ¬†out the context ¬†and running it then modifying the prompt and rerunning the small LM, u‚Äôll end up spending ¬†more time on a small llm then on a large LM",
              "score": 0,
              "created_utc": "2025-12-27 11:59:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6y0oi",
                  "author": "false79",
                  "text": "If you're spending 10-12 minutes, you're doing it wrong.",
                  "score": 3,
                  "created_utc": "2025-12-27 14:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3e2bh",
          "author": "RiskyBizz216",
          "text": "Sometimes they are for *deployment* \\- you can deploy a 1B/3B/4B model to a mobile device, or a raspberry pi. You can even deploy an LLM in a chrome extension!\n\nThe 7B/8B/14B models are for *rapid prototyping* with LLMs, for example - if you are developing an app that calls an LLM - you can simply call a smaller (and somewhat intelligent) LLM for rapid responses.\n\nThe 24B/30B/32B models are your *writing and coding assistants.*",
          "score": 11,
          "created_utc": "2025-12-26 22:14:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3e2p9",
              "author": "RiskyBizz216",
              "text": "I personally believe that companies will phase out these smaller models from public some day. Models like GPT-OSS 20B are just an embarrassment. As companies become more competent, you will see fewer potatoes and more jalape√±os!",
              "score": -9,
              "created_utc": "2025-12-26 22:14:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw85mg2",
                  "author": "koflerdavid",
                  "text": "Hosting files a few GB in size has been a solved problem for a few decades now: Torrent.",
                  "score": 2,
                  "created_utc": "2025-12-27 18:04:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3762r",
          "author": "dobkeratops",
          "text": "gets a foot in the door.   \nand you can get quite good VLMs in this range that can describe an image.\n\nI've got useful reference answers out of 7b's (and far more so 20,30b's). It can keep you off a cloud service for longer. You dont need it to code for you, it can still be a useful assist that's faster than searching through docs.\n\nI believe Local AI is absolutely critical for a non-dystopian future.",
          "score": 10,
          "created_utc": "2025-12-26 21:36:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw34uny",
          "author": "Southern-Chain-6485",
          "text": "Uncensored models, vision, prompt processing for local ai image generators, privacy, and anything you don't need any complex stuff. Do you want to translate something? You can use a small model. Check grammar? Same.",
          "score": 19,
          "created_utc": "2025-12-26 21:24:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3w8ic",
          "author": "simulated-souls",
          "text": "Big thing that people aren't mentioning: [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29?wprov=sfla1).\n\n\nIf you have a narrow task and some examples of how to do it, then giving a model a little extra training (often using something like a LoRA adapter) can be the best solution.\n\n\nFine-tuned \"potato\" models can often match or even exceed the performance of frontier models, while staying cheap and local.\n\n\nFine-tuning is also even more intensive (especially for memory) than inference, so you're probably stuck doing it with small models. Luckily you only only need to fine-tune a model once and can reuse the new parameters for as much inference as you want.",
          "score": 9,
          "created_utc": "2025-12-27 00:00:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw377qi",
          "author": "Danternas",
          "text": "In daily use I see little difference between a 30B model and one of the commercial large ones (GPT/Gemini). Main difference is in their ability to search the internet and scrape data, something I still struggle with.",
          "score": 18,
          "created_utc": "2025-12-26 21:36:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6g3o1",
              "author": "power97992",
              "text": "There is a big difference even without web search, less knowledge and more prompting and longer prompts and worse results with a small model..",
              "score": 1,
              "created_utc": "2025-12-27 12:01:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3e5i6",
          "author": "rosstafarien",
          "text": "What will you run on a phone in a poor network coverage area? How confident are you that what you're sending to the cloud isn't being logged by your provider? What happens to your business model if the cost for remote inference triples or worse.\n\nRunning on a potato is the only AI I'm interested in right now.",
          "score": 8,
          "created_utc": "2025-12-26 22:14:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3g9eh",
          "author": "jamie-tidman",
          "text": "Summarisation, classification, routing, title / description generation, next line suggestion, local testing for deployment of larger models in the same family.",
          "score": 7,
          "created_utc": "2025-12-26 22:26:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3kmpo",
              "author": "silenceimpaired",
              "text": "Checking Spelling, grammar, punctuation.",
              "score": 5,
              "created_utc": "2025-12-26 22:51:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw5zpua",
                  "author": "FastDecode1",
                  "text": "[Speculative decoding](https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/)",
                  "score": 3,
                  "created_utc": "2025-12-27 09:23:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw3785o",
          "author": "nunodonato",
          "text": "Smaller models can excel at specific things, especially if trained. I would argue we will have many more uses for focused smaller models than bigger ones that try to excel at everything",
          "score": 7,
          "created_utc": "2025-12-26 21:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3m6xx",
          "author": "ThenExtension9196",
          "text": "Weaker models are for fine tuning. They can become immensely good at some narrow thing with very little requirements if you train them.",
          "score": 8,
          "created_utc": "2025-12-26 23:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4ke0u",
          "author": "dash_bro",
          "text": "Lots of fixed task tuning with limited data, which will be cheaper than the API in the long term. Also, 30B is definitely not potato tier!!!\n\neg got a classification problem? train/fine-tune/few shot prompt a small model without paying for per-token cost! \n\nwant something long running as a job, that might be potentially expensive even with cheap APIs? small models! \n\nwant to not be restricted by quality drops/rate limits/provider latency spikes? small models! \n\nLarge scale data labelling, which runs or curates data for you 24/7? Batch, run, save locally without exposing anything outside your system. Privacy is a big, big boost.\n\nThe biggest one in my opinion : learn. 99% of us aren't Research Scientists. You don't know what you don't know. Learn to do it yourself, become an expert and eventually build yourself to work at a top tier lab. It's an exclusive community for sure, but the knowledge gap between the ones in and out is usually pretty big.\n\nIn general:\n\n- anything <1B is actually really decent at the embedding/ranking level. I find the qwen-0.6B models to be excellent examples.\n\n- anything 1-3B is great for tuning. Think: intent classifications, model routing, fine tunes for non critical tasks, etc.\n\n- anything 7-10B is pretty decent for summarisation, entity/keyword extraction, graph building, etc. This is where few shot stuff and large scale data scoring starts being possible IMO.\n\n- anything in the 14B tier is good for classification tasks around gemini-flash/gpt-nano/claude haiku quality **if you provide enough/correct context**. Gets you 90-95% of the way there unless you need a lot of working context. Think about tasks that need 3-5k input tokens with a ~80-100 output tokens.\n\n- 30B tier usually is pretty good up until ~40k tokens as total working context. If you need more than that you'll have to be clever about offloading memory etc., but it can be done. 30B is readily gpt-4-32k tier when it first came out. Thinking models start performing around this level, imo. Great for local coding too!\n\nAfter 30B it's really more about the infra and latency costs, model management and eval tier problems that aren't worth it for 99% of us. So usually I dont recommend them being self hosted over a simple gpt/gemini call. Diminishing returns.",
          "score": 6,
          "created_utc": "2025-12-27 02:31:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw361yg",
          "author": "SinCebollista",
          "text": "Safety, privacy, and lack of censorship.",
          "score": 28,
          "created_utc": "2025-12-26 21:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw33u5v",
          "author": "Lodarich",
          "text": "vision models mostly",
          "score": 13,
          "created_utc": "2025-12-26 21:18:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw38xxp",
          "author": "Smashy404",
          "text": "As someone with an IQ of less than 7 I find the small models to be amazingly insightful.\n\nThe large ones just intimidate me.\n\nI didn't know you could install them on a potato though. I will try that tomorrow.\n\nThanks.",
          "score": 19,
          "created_utc": "2025-12-26 21:46:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3bd7t",
          "author": "Late_Huckleberry850",
          "text": "Also, you may be calling them potatoes now, but the latest version of the Liquid LFM-2.6-Exp has benchmarks on par or exceeding the original GPT-4 (which was revolutionary when it came out). So maybe they are experiments for now, but give it really only one more year and for many practical applications you will not mind using them.",
          "score": 10,
          "created_utc": "2025-12-26 21:59:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6ghpx",
              "author": "power97992",
              "text": "Gpt 4 was terrible for coding ¬† , you had to prompt it 40-90 times and it still wouldnt get the answer right but it was good at web searching and summarizing. ¬†Lfm is gpt 4 lobotomized without all the world knowledge¬†",
              "score": 1,
              "created_utc": "2025-12-27 12:05:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw38n93",
          "author": "__SlimeQ__",
          "text": "qwen3 14b can do tool calls while running on my gaming laptop so I'm sure it could do something cool. i have yet to see such a thing though, in practice it is still very hard.\n\ni feel like the holy grail for that model size is a competent codex-like model that can do infinite dev on your local machine. and we do seem to be pushing very hard towards that reality year over year.",
          "score": 6,
          "created_utc": "2025-12-26 21:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw47can",
          "author": "pieonmyjesutildomine",
          "text": "- Classification\n- Entity resolution\n- POS tagging\n- Dependency trees\n- lemmatization\n- creating stop-word lists\n- on-device inference\n\nUnique solutions: \n- logit manipulation\n- hypernetworks\n\nThese are all actual project solutions that I've been paid thousands of dollars for completing. The largest model used for these was 12b, and the smallest was 3b. Most projects required one or both of the \"unique solutions\" section to make the project reliable, but clients for the most part reported higher metrics than the classical ML solutions without overfitting, which is what they asked for. The nice thing is that I'm essentially going up against AutoGluon (if they even know about that), so I know what I have to beat and that's helpful.",
          "score": 5,
          "created_utc": "2025-12-27 01:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3c0lt",
          "author": "olearyboy",
          "text": "To keep Glados portable while she hunts her pray",
          "score": 4,
          "created_utc": "2025-12-26 22:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3d3ey",
          "author": "IKoshelev",
          "text": "Reddit comments.¬†",
          "score": 5,
          "created_utc": "2025-12-26 22:08:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4adcd",
          "author": "M_Owais_kh",
          "text": "Small models exist because not everyone is trying to replace Claude, many are trying to build *systems* under real constraints.\n\nI‚Äôm a student with no fancy GPUs and no interest in paying cloud providers. 20B models run locally on my mid tier laptop, offline, with no rate limits or costs. With good prompting and lightweight RAG, they‚Äôre perfectly usable knowledge and reasoning tools.  They‚Äôre also ideal for pipeline development. I prototype everything locally, then swap in a larger model or API at deployment. The model is just a backend component. Not every task needs 500B level coding ability. Summarization, extraction, classification, rewriting and basic tasks work fine on small models. Using huge models everywhere is inefficient as well.",
          "score": 4,
          "created_utc": "2025-12-27 01:26:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4qilg",
          "author": "ZealousidealShoe7998",
          "text": "small LLM are just a capable of doing certain tasks as bigger LLMs the only difference is the amount of knowledge they have in such subject.  \nyou can in fact train a smaller LLM to do a specific task and it might perform just as good as a bigger LLM.  \nbut now you get less resource usage and more speed.\n\nthe problem is people are still obsessed with having the biggest LLM who can do it all.  \nbut for a lot of applications you might not need a 1T parameter comercial model.  \nyou could easily host in house a smaller LLM who fits in consumer hardware and train it on your actual data.\n\nbut this takes time, and expertise so what usually happens is people wait for a better OSS llm to be released and you can only do so much general stuff in such amount of parameters before the llm starts hallucinating.  \nperhaps a more efficient architecture might come along where a 30B parameter model might be just as good as todays comercial llms, but by them we gonna be like \"these llms are useless why dont we have AGI on consumer hardware yet?\" which honestly thats the greater question  \nwhat will take for us to have AÀùGI on consumer hardware ?",
          "score": 4,
          "created_utc": "2025-12-27 03:10:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35h1v",
          "author": "swiftbursteli",
          "text": "I had a low-latency, high-throughput application. Sorting 50,000 items into categories. \n\nMinistral failed horrendously. The speed on my m4 pro was 70 tok/sec with 2s TTFT. \n\nWith those speeds, if you don‚Äôt care for accuracy and care more about speed (chatbots, summarizing raw inputs) then that is the model‚Äôs use case. \n\nBut yes, SOTA models are much, much bigger than what we can afford on a lowly consumer grade machine. I saw an estimate online saying Gemini 3 can be 1-1.5 tb in a q4 variant. Consumers rarely get 64gb memory‚Ä¶. SMBs can swing 128gb setups‚Ä¶ \n\nTo get SOTA performance, you‚Äôd need to do one of those leaning tower of Mac Mini and find a SOTA model‚Ä¶. But you still have low memory bandwidth.",
          "score": 9,
          "created_utc": "2025-12-26 21:27:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw43c61",
          "author": "Nindaleth",
          "text": "Sometimes you'd be surprised. I wanted to create an AI agent documentation for our legacy test suite at work that's written in an uncommon programming language (there are no LSP servers for the language I could use instead AFAIK). Just get the function names, their parameters and infer from the docstring + implementation what each function does. The files are so large they wouldn't fit the GitHub Copilot models' context window one at a time - which is actually why I intended to condense them like this.\n\nI wasn't able to get GPT-4.1 (a free model on Copilot) to do it, it would do everything in its power to avoid doing the work. But a Devstral-Small-2-24B, running locally quantized, did it.",
          "score": 3,
          "created_utc": "2025-12-27 00:43:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4i7zb",
          "author": "Foreign-Beginning-49",
          "text": "They are literally endless. Here is one simple example. Just the microcontroller sensor world alone and the building guidance and idea generation could have a small model help you build robots until you want to do something else from sheer boredom. You can explore the basics of almost anything you can think of. If you.need to in depth research on a beetle family you're in hog heaven. A specific subspecies recently recognized in a journal? Thats up to you to geberate the knowledge. If you really work with the model as a cognitive enhancement device and are always skeptical instead of as a wise all knowing discarnate informant one can begin to accelerate their understanding of almost any area of study. Many high profile scientists are using Ai openly in their labs to accelerate human discovery. While many a waifu researcher is pushing the boundaries on digital human companions scientists at Stanford medicine are rapidly diagnosing new congenital tissue with rapid realtime semantically rich cellular imagery. Ai is allowing normies to work almost like proto polymaths if they apply themselves deeply enough.\n\n\nAnd because they are using their noodle they will know that no one source of information can be trusted except by outside verification and the seeking out of other sources of consensus they can use the llms of all sizes to augment their intellect and ability to manipulate the physical world with their imagination alone. This is all to say that even small models properly utilized can radically change your relationship to many fields or human endeavors. Its worth it. If you aren't doing the computing someone else is doing it for you. Own your own thinking machine its nice.",
          "score": 3,
          "created_utc": "2025-12-27 02:17:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xp4u",
          "author": "Iory1998",
          "text": "Hmm.. you sound like someone working at an AI lab! Are you by any chance Sam Altman?ü´®ü§î",
          "score": 4,
          "created_utc": "2025-12-27 00:09:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3fm4n",
          "author": "robogame_dev",
          "text": "They're hard to take advantage of if you're not willing to code or vibe-code your use case. Then you use them as free/cheap/private inference for any tasks they CAN accomplish. For example, I used them to process 1600 pages of handwritten notes, OCRing the text, regenerating mermaid.js version of hand drawn flowcharts, etc. Would have cost me $50 with Gemini in cloud.",
          "score": 2,
          "created_utc": "2025-12-26 22:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3iifh",
          "author": "sluggishschizo",
          "text": "I had some good results with newer quantized models, whereas around half a year ago I couldn't get any halfway functional code out of any local model I tried. I recently tried to create a simple Python Tetris clone with GPT OSS 20b, Devstral Small 24b, and a GPT 5-distilled version of Qwen3 4b Instruct, and two of the three models did it about as well as the full Gemini 2.5 Flash did when I gave it the same task six months ago. \n\nThe GPT OSS model had one tiny error in the code where it misaligned the UI elements, which is exactly what Gemini 2.5 did on its first try at creating a Python Tetris clone when I tried this previously, but the tiny 4b model somehow got it right on its first try without any errors. The Devstral model eventually got it right with some minor guidance. \n\nI'm still astonished that a 4b parameter model that only takes up ~4gb of space can even do that. It'll be interesting to see where local coding models are in another six months.",
          "score": 2,
          "created_utc": "2025-12-26 22:39:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7isl2",
              "author": "gnnr25",
              "text": ">a GPT 5-distilled version of Qwen3 4b Instruct\n\nOoh, a new rabbit hole to go down",
              "score": 2,
              "created_utc": "2025-12-27 16:09:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw4x9vn",
              "author": "Miserable-Dare5090",
              "text": "the qwen3 4B 2507 version is amazing, finetunes so wellüë®‚Äçüç≥üòò",
              "score": 1,
              "created_utc": "2025-12-27 03:56:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3jenj",
          "author": "Keep-Darwin-Going",
          "text": "Because not every situation you need to throw a nuke at. Smaller model can be fine tuned to do some stuff that need speed, privacy or cost sensitive. Like if I want a llm to help me play game, I am sure you do not want to use a sota model since it is slow and expensive.",
          "score": 2,
          "created_utc": "2025-12-26 22:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4300l",
          "author": "steveh250Vic",
          "text": "I have a Qwen3:14b model at the heart of an Agentic solution responding to RFP's - does a great job tool calling and developing responses.¬† Will likely move to 30b model soon but it's done a brilliant job so far.¬†",
          "score": 2,
          "created_utc": "2025-12-27 00:41:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4od8k",
          "author": "dr-stoney",
          "text": "Entertainment. The thing massive consumer companies ride on and B2B bros pretend doesn't exist.\n\n24B-32B is absolutely amazing for fun use-cases",
          "score": 2,
          "created_utc": "2025-12-27 02:56:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4tmjq",
              "author": "Party-Special-5177",
              "text": "Even smaller can be even more entertaining - I have absolutely lost an evening last year asking 1B class models questions like ‚Äòhow many eyes does a cat have‚Äô etc (if you haven‚Äôt done this already, go do this now).\n\nI got my dad into LLMs by having Gemma write humorous limericks making fun of him and his dog for his birthday. I actually couldn‚Äôt believe how good they were, neither could he.",
              "score": 1,
              "created_utc": "2025-12-27 03:31:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4vaud",
                  "author": "dr-stoney",
                  "text": "It's so awesome to read how people use LLMs for fun. Thank you üôè",
                  "score": 1,
                  "created_utc": "2025-12-27 03:43:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw5fju3",
          "author": "Bmaxtubby1",
          "text": "They‚Äôre kind of the easiest way to learn fine-tuning and inference without renting a data center.",
          "score": 2,
          "created_utc": "2025-12-27 06:14:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5fyr3",
          "author": "sirebral",
          "text": "Honestly, Qwen 3 is pretty impressive, particularly for tool use, so I've been happy with it because it quants  to four bits quite well and works great as a router and tool calling.  Runs quickly with MoE even with 100k context fitting in 32 gigs of RAM.\n\nOther uses for small models, single use experts.  Although MoE has really taken over this space.  Things evolve constantly, and the Chinese are open weights on most releases.  They concentrate more on efficiency, which is great for local inference, even with consumer level cards.\n\nEven their smaller versions can do quite well, so while Americans private models are more and more greedy with their VRAM, there are some slick applications for smaller models.",
          "score": 2,
          "created_utc": "2025-12-27 06:17:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5qubx",
          "author": "ozzeruk82",
          "text": "Captioning images. Qwen 3VL is superb at the task and means you don‚Äôt need to upload all your (68000) family photos anywhere.",
          "score": 2,
          "created_utc": "2025-12-27 07:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw63ig5",
          "author": "wirfmichweg6",
          "text": "Sorting through my Obsidian notes without leaking content to any LLM providers.\n\nSummarize voice recordings to have a baseline for blog posts (I don't use LLMs for the texts themselves, just to make the voice recordings into text.)\n\nSummarize articles in Brave using their ollama support.",
          "score": 2,
          "created_utc": "2025-12-27 10:01:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7fvt7",
          "author": "BrilliantPhysics6611",
          "text": "I work for a startup and we deploy our products which use AI (including agents) to locations that can‚Äôt access the internet. Due to this we commonly use 12B-24B models.\n\nThey can actually be quite good. The difference is though EVERY SINGLE PROMPT you put into a small model has to be carefully crafted and the scope needs to be narrow, verse with a frontier model you can put a half baked pos prompt in and still get great results, or you can throw 30 tools in and define a really wide scoped workflow for it to do and it‚Äôll do it, verse with a small model you have to break that up.",
          "score": 2,
          "created_utc": "2025-12-27 15:54:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwahzqj",
          "author": "Diligent_Cod_9583",
          "text": "Potatoes are delicious?",
          "score": 2,
          "created_utc": "2025-12-28 01:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3dufp",
          "author": "a_beautiful_rhind",
          "text": "A lot of it is people's cope but at the same time there's no reason to use a 1T model to do simple well defined tasks.\n\nQwen 4b is a great text encoder for z-image; there's your real world example. \n\nSmall VL models can caption pics. Small models can be tuned on your specific task so you don't have to pay for claude or have to run your software connected to the internet.",
          "score": 4,
          "created_utc": "2025-12-26 22:12:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3kzo3",
              "author": "silenceimpaired",
              "text": "In my experience dense 30b, 70b and MoE 120b, 300b are sufficient to manipulate and brainstorm prose.",
              "score": 5,
              "created_utc": "2025-12-26 22:53:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw3eqyo",
          "author": "chickenfriesbbc",
          "text": "...You can answer this question by just trying them...\n30b models active 3b are great. Your tripping",
          "score": 3,
          "created_utc": "2025-12-26 22:17:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xn7w",
          "author": "Fireslide",
          "text": "What's' the point of a potato tier employee?\n\nIt all comes down to economics. It's more efficient to have a potato tier LLM do only the things potato tier LLMs can do, freeing up the higher tiered vegetables to do their thing.\n\nWhat OpenAI is doing with their silent routing is basically trying to be efficient with their limited compute resource by routing queries where appropriate to cheaper models.\n\nThe future is likely to have a bunch of on device LLMs that run small parameter models that help form queries or contact larger models when needed.",
          "score": 3,
          "created_utc": "2025-12-27 00:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw37i6k",
          "author": "darkdeepths",
          "text": "quick, private inference / data processing with constant load. you can run these models super fast on the right hardware, and there are jobs that they do quite well. many of the best llm-as-judge models are pretty small.",
          "score": 1,
          "created_utc": "2025-12-26 21:38:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw37l3c",
          "author": "fungnoth",
          "text": "What if we can one day have a tiny model that's actually good at reasoning, comprehension and coherency. But doesn't really remember facts in training data.",
          "score": 1,
          "created_utc": "2025-12-26 21:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3ansu",
          "author": "CorpusculantCortex",
          "text": "I have pretty great success even summarizing and performing sentiment analysis of whole news articles into a structured output with a 14b - 30b model locally.",
          "score": 1,
          "created_utc": "2025-12-26 21:55:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3cu4a",
          "author": "revan1611",
          "text": "I use them for web searching on searXNG. Not the best but it gets the job done sometimes",
          "score": 1,
          "created_utc": "2025-12-26 22:07:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3kz3q",
          "author": "noiserr",
          "text": "You don't have to boil the ocean for every task. Small embedding models are also really useful.",
          "score": 1,
          "created_utc": "2025-12-26 22:53:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3kzt6",
          "author": "abnormal_human",
          "text": "They're for much simpler tasks than agentic coding.\n\nThink about things people used to have to train NLP models for like classification, sentiment analysis, etc. Now instead of training a model you can just zero-shot it with a <4B model. Captioning media, generating embeddings. Summarization. Little tasks like \"Generate a title for this conversation\". Request routing.\n\nLarge models can do all of these things too but they are slow and expensive. When you build real products out of this tech, scale matters, and using the smallest model that will work suddenly becomes a lot more important.",
          "score": 1,
          "created_utc": "2025-12-26 22:53:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3mivq",
          "author": "floxtez",
          "text": "I use small models for tagging, titling, summarizing, categorizing, extracting information, performing semi deterministic transformations, etc, etc",
          "score": 1,
          "created_utc": "2025-12-26 23:02:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3npik",
          "author": "no_witty_username",
          "text": "Very small models will probably be used more in the future then the big models. Kind of like most chips today are not frontier level 20k chips like from Nvidia gpu's but chips worth only cents each from TI. Same for LLM's, they will fill in the gaps where large llm's are overkill.",
          "score": 1,
          "created_utc": "2025-12-26 23:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3o0i5",
          "author": "TheMcSebi",
          "text": "I'm using ollama with gemma3:27b for many scripted applications in my tech stack. Main use cases are extracting data, summarization and RAG (paired with a decent embedding model). Also sometimes for creative writing, even tho that can get repetitive or boring quickly if not instructed well enough.\nIt did churn out couple of working, simple python scripts, but for those use cases I mainly use the online tools.",
          "score": 1,
          "created_utc": "2025-12-26 23:11:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3or7s",
          "author": "ciavolella",
          "text": "I'm switching through a series of 4b and 8b models trying to find the one I like the most right now, but I'm running my own RocketChat instance, and a bot is monitoring the chat for triggers which it sends out to the ollama API, and can respond directly in the chat.  It also responds to DMs.  But I don't need a heavyweight model to do what I need it to do in my chat.",
          "score": 1,
          "created_utc": "2025-12-26 23:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3w4iy",
          "author": "No-Marionberry-772",
          "text": "Ive been toying around with using small LLMs to habdle context for procedurally generated scenarios.\n\n\nComputing a simulated history is computationally expensive.¬† Trying to simplify the process and fake it without AI has proven to be difficult.\n\n\nI have been able to use the\ncontext understanding of a 3b model to populate json that allows that process to work more reliably.",
          "score": 1,
          "created_utc": "2025-12-27 00:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3xjwe",
          "author": "toothpastespiders",
          "text": "I think the 20b to 30b'ish range can be fine for a general jack of all trades model. Especially if they have solid thinking capabilities. At least if they're also fairly reliable with tool calling. They usually have enough general knowledge at that point to intelligently work with RAG data instead of just regurgitating it. I do a lot of work with data extraction and that's my goto size for local. It's also the point where I stop feeling like I'm missing something by not pushing things up to the next tier of size. If I'm using a 12b'ish model I'm almost always going to wish it was 30b. If I'm using a 30b I'm generally fine that it's not 70b. They're small enough that additional training is a pain but still practical.\n\nI'd probably get more use out of the 12b range if I had an extra system around with the specs to run it at reasonable speeds alongside my main server. Until my terminally ill e-waste machine finally died on me I was using it for simple RAG searches over my databases with a ling 14b...I think 2a model that I did additional training on for better tool use and specialized knowledge. Dumb, but enough if all I really needed was a quick question about how I solved x in y situation or where that script I threw together last year to provide z functionality got backed up to. Basically just saving me the trouble of manually working with the databases and sorting through the results by hand. I think a dense rather than MoE 12b'ish model would have been an ideal fit for that job. \n\nAs others have mentioned the 4b'ish range can be really good as a platform to build on with additional training. I think my current favorite example is [mem agent](https://huggingface.co/driaforall/mem-agent). 4b qwen model fine tuned for memory-related tasks. Small enough as a quant for me to run alongside a main LLM while also being fairly fast.",
          "score": 1,
          "created_utc": "2025-12-27 00:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3zcs4",
          "author": "Lesser-than",
          "text": "local models will always not scratch your api llm itch, rather than trying to load a model that barely fits your hardware and suffer the t/s and low context limitations, the challenge becomes what can you do with a Models that do fit, its never going to be claude@home your going to have to be a bit more creative on your own like api llms are good at everything a potato tier llm just has to be good a something.",
          "score": 1,
          "created_utc": "2025-12-27 00:19:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41d3d",
          "author": "woct0rdho",
          "text": "Porn. It does not require that much intellectual complexity and a 30B model can do it pretty well.",
          "score": 1,
          "created_utc": "2025-12-27 00:31:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw41x2i",
          "author": "LowPressureUsername",
          "text": "For consumers, pretty much anything they want.\n\nFor companies: handling millions of requests extremely fucking cheaply. LLMs are overkill for most problems but with some fine tuning their performance is üî•.",
          "score": 1,
          "created_utc": "2025-12-27 00:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw451vl",
          "author": "GaggedTomato",
          "text": "Realisticly speaken: absolutely nothing.\nFor me, they have been fun experimenting and developing tools around, but they just suck too much atm to be really generating value in some way, although i think models like gpt oss 20b are already borderline useful if used in the right way. But it takes a quite some effort to really get value out of them.",
          "score": 1,
          "created_utc": "2025-12-27 00:53:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw46238",
          "author": "JacobyT",
          "text": "for delightful inference while in airplane mode",
          "score": 1,
          "created_utc": "2025-12-27 00:59:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4cmpf",
          "author": "No_Afternoon_4260",
          "text": "What you want is an agent. Ofc the big question need to be answered by a big boy.\nBut to build the prompt for the big boy you need many steps. You want to build its context.\nFor that you need tools, \"memories\", etc\nA lot of the small steps are perfect fit for small llms or just other smaller technology that also like your rtx",
          "score": 1,
          "created_utc": "2025-12-27 01:41:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4kijs",
              "author": "workware",
              "text": "I'd love an example for this.",
              "score": 1,
              "created_utc": "2025-12-27 02:32:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4ls95",
                  "author": "No_Afternoon_4260",
                  "text": "Tools:  \nRetrieve in a db, read a file, get weather, etc\nFor all these stupid tasks gemma 12b it will do the trick.\n\nYou could also take a look at what is RAG (see you in a couple of months on the ingestion part ;) )\n\nThese are random thoughts but in short an agent needs an ecosystem, there lies all your data and tools, it consumes a lot of \"tokens\" while a lot of it is \"cheap\" in intelligence. The bigger questions represent less \"tokens\" and can be outsourced to bigger models.  \nAnd by tokens I don't mean only llms tokens but unit of mesure for \"gpu type compute\". Because your rag system is based on embeddings, your ocr is a combination of cnn, object detection or vision-llm, you may want STT and TTS and so on..\n\nRoughly at 12b you have a good orchestrator, at 25B you start opening the possibilities and above 100B it starts to get really crazy",
                  "score": 1,
                  "created_utc": "2025-12-27 02:40:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw4jsen",
          "author": "burntoutdev8291",
          "text": "I use small models for quick questions that don't require very large models. I also use them for processing personal documents. Models like deepseek ocr, olmocr, and the smaller qwen variants are very useful.\n\nAs a developer, small models allow me to still do the thinking while dealing with boilerplate. Its more productive for me to use faster and smaller models than a very large reasoning model, cause they are gonna get it wrong anyway.",
          "score": 1,
          "created_utc": "2025-12-27 02:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4k3ym",
          "author": "-InformalBanana-",
          "text": "Qwen3 2507 30b a3b instruct works fine for some codding tasks and probably many other things. Devestral 24b also.",
          "score": 1,
          "created_utc": "2025-12-27 02:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4k6yd",
          "author": "SkyFeistyLlama8",
          "text": "You're forgetting NPU inference. Most new laptops have NPUs that can run 1B to 8B models at very low power and decent performance, so that opens up a lot of local AI use cases.\n\nFor example, I'm using Granite 3B and Qwen 4B on NPU for classification and quick code fixes. Devstral 2 Small runs on GPU almost permanently for coding questions. I swap around between Mistral 2 Small, Mistral Nemo and Gemma 27B for writing tasks. All these are running on a laptop without any connectivity required.\n\nYou get around the potato-ness of smaller models by using different models for different tasks.",
          "score": 1,
          "created_utc": "2025-12-27 02:29:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4l6a5",
          "author": "Sl33py_4est",
          "text": "small models are for fine tuning on specific small use cases to cover the performance:compute ratio better or more securely than cloud providers.\n\nvanilla small models?\n\nentertainment.",
          "score": 1,
          "created_utc": "2025-12-27 02:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4teby",
          "author": "KeyPossibility2339",
          "text": "Imagine i have a dataset, i need to classify 100k rows. In this case, where a lot of intelligence is not needed local potato llms are the best. In other words, high volume low quality work",
          "score": 1,
          "created_utc": "2025-12-27 03:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4w1my",
          "author": "Ok-Bill3318",
          "text": "Small tasks where larger LLMs aren‚Äôt required. Like basic rag.\n\nEssentially: regularly try the very small\nLLMs for specific tasks and see how well they work don‚Äôt waste resources running a 20b or larger model when 4b will do the job faster and with less resource consumption.\n\nEven llama 3b has worked quite well for some simpler tasks for me.",
          "score": 1,
          "created_utc": "2025-12-27 03:48:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4xhrp",
          "author": "unsolved-problems",
          "text": "Certain set of problems have black or white answers, like some math problems where you can plug in the number x, y, z and see if the solution is right. Here, checking the answer is always fast, and unambiguous. In these cases, you can use arbitrarily \"silly\" heuristics to solve the problem (as long as your overall solution works) because ultimately a wrong answer won't cost you much, as long as you're able to produce a right answer fast enough.\n\nIn my experience, some of the smart tiny models like Qwen3 4B 2507 Thinking are freakishly good in this domain of problems. Yeah, they're dumb as stone overall, but they're incredibly good at solving mid-tier STEM problems some of the time. Just ask it away, and it'll get it right 60% of the time and if not you can check, determine that it's wrong, and re-try. It's very surprising how far you can go with this approach.\n\nOn the one hand, you can type some random STEM textbook question in, as long as you can determine with 100% certainty that what it's telling you is BS, it has a very high chance of providing you with useful information about the problem (unless you're a domain expert, then it's gonna be a waste of time).\n\nOn the other hand, in terms of engineering, you can type some sort of optimization or design problem where you just need numbers to be low enough to do the job, so there is never a risk of AI doing a bad job.\n\nIn this case, since it's a 4B model, this gives us incredible opportunities. This model will be rather small (\\~4GB) and is small enough that it can be utilized by both a CPU and a GPU at reasonable speeds. So, it could be possible to embed this in some offline app, and add it to a feature that finds a solution only some of the time, or otherwise reports \"Sorry! We weren't able to find a solution!\". This can run fine in a decent amount of hardware today, e.g. most desktop computers.",
          "score": 1,
          "created_utc": "2025-12-27 03:57:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4z6u8",
          "author": "Fresh_Finance9065",
          "text": "Specialised LLMs ie\nVision\nClassification\nRAG\n\nNormally, you give it the information and it will do tasks for you, rather than drawing upon its own knowledge.\n\nThey are generally less like to conspire against you or do complex things.",
          "score": 1,
          "created_utc": "2025-12-27 04:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5eaqa",
          "author": "coastisthemost",
          "text": "Nice llms with a narrow focus can be outstanding with a small parameter count. Like microsoft phi specialized for science.",
          "score": 1,
          "created_utc": "2025-12-27 06:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5gtfn",
          "author": "sirebral",
          "text": "They are often good token prediction models for larger models.  Saving you lots of inference if mated properly with a larger model, many uses, I think they're actually more fun than monolith models, it's engineering over raw power.",
          "score": 1,
          "created_utc": "2025-12-27 06:24:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5j4es",
          "author": "triynizzles1",
          "text": "Small models are really good foundation models that can be fine-tuned by an end user to handle one or two niche tasks very well. Since the AI is small it can run locally, on a CPU, and users‚Äôs computer, etc.",
          "score": 1,
          "created_utc": "2025-12-27 06:45:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5mnrg",
          "author": "cruncherv",
          "text": "I use them for image captioning - descriptions of what is seen in photos, images - text, locations, places, objects, colors, etc",
          "score": 1,
          "created_utc": "2025-12-27 07:17:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5n1dp",
          "author": "YT_Brian",
          "text": "For poor people like me who can't afford a GPU, let alone these days. What, AI usage should only be for those with a thousand up to toss at it?\n\nHell to the fuck no to that. \n\nPlus you are totally ignoring on mobile usage for pretty much every device you take with you except laptops which exists. Which is a genuinely huge market.",
          "score": 1,
          "created_utc": "2025-12-27 07:20:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5xgfh",
          "author": "iamrick_ghosh",
          "text": "No idea on how many firms and companies are using this smaller open source models in their workflow and production too to benefit rather than spending insane amounts on openai or anthropic",
          "score": 1,
          "created_utc": "2025-12-27 09:01:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw61mru",
          "author": "technofox01",
          "text": "I enjoy the privacy of being able to experiment against AI safeguards (e.g. Making malicious code and testing it in VMs) for security research. Other times I enjoy the privacy to discuss mental health or other topics out of curiosity, and not worrying that I am feeding someone else's AI free information.",
          "score": 1,
          "created_utc": "2025-12-27 09:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6eh74",
          "author": "Bastion80",
          "text": "My AI driven automatic Kali pentester terminal is running using 4b or 8b models, enough to figure out tools/commands to execute.",
          "score": 1,
          "created_utc": "2025-12-27 11:47:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6ir4p",
          "author": "thespirit3",
          "text": "Your 'potato' LLMs are powering my day to day job, with local documentation queries, local meeting transcript summarisation, log analysis etc. Also, powering my many websites with WordPress content analysis and associated queries from users, automatic server log analysis and resulting email decision/generation, clamAV/Maldet result analysis, etc etc. \n\nAll of the above runs from one local 3060 with VRAM to spare. For coding, I use Gemini - but all of the above would cost a fortune if paying per token.",
          "score": 1,
          "created_utc": "2025-12-27 12:24:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8iqzy",
              "author": "ansibleloop",
              "text": "Which models?",
              "score": 1,
              "created_utc": "2025-12-27 19:10:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlgmt6",
                  "author": "thespirit3",
                  "text": "Sorry for the slow reply. I've settled on Qwen4:14b for most purposes.",
                  "score": 1,
                  "created_utc": "2025-12-29 19:18:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6nw78",
          "author": "vagmi",
          "text": "They are also remarkably good at summarizing code. While they cannot be used for coding, they can be used for code understanding and exploration.",
          "score": 1,
          "created_utc": "2025-12-27 13:05:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6pwtv",
          "author": "Odd_Lengthiness_2175",
          "text": "The short and unpopular answer is, not much.  At least not yet, and not for things most people would want to use local for.  For a long time (\\~30 years) high-end gaming PCs sat in a sweet spot where they could run workstation-level tasks at a consumer price point, but that doesn't seem to be holding anymore for AI tasks because consumer gaming GPUs don't have enough vram, and I don't think they ever will.\n\nThat's the bad news.  There's two bits of good news though. First, small models improved a ton this year.  Like a ridiculous amount.  The best model for what I do that I can run decently on my Mac Mini M4 Pro is Gemma 3 12B and it's surprisingly capable.\n\nSecond, there's been a tremendous amount of interest in PCs that can run decent models (\\~70B, basically SOTA from 2 years ago) locally, quickly, and affordably (if you consider a PC that costs as much as a car affordable, it comes down to priorities I guess).  There's a whole suite of Linux boxes you can buy right now built specifically for AI tasks.  New Mac Studio (M5 Max) coming out this summer is looking like a very strong consumer option if you don't want to deal with Linux.",
          "score": 1,
          "created_utc": "2025-12-27 13:19:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6y986",
          "author": "xmBQWugdxjaA",
          "text": "They are good for router LLMs and classification - stuff where in the past you would have trained your own BERT model for example. Now it's far easier and more versatile than dealing with that.",
          "score": 1,
          "created_utc": "2025-12-27 14:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw70jem",
          "author": "Danny_Davitoe",
          "text": "The company I work for, by law, can't hold or send data outside of the company. The workaround is having local LLMs as our solution.",
          "score": 1,
          "created_utc": "2025-12-27 14:28:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw722uq",
          "author": "insulaTropicalis",
          "text": "This reminds me of that classical meme \"coders back then vs coders today.\"\n\nIt was just two-and-a-half years ago that we did all kind of stuff with llama-2 at 7 and 13B params. Today we have 4B thinking models which rival the 68B original llama and all kind of agentic frameworks and shit. And newbies complaining \"bUt WhAt UsE aRe, MoDeLs UnDeR 100B pArAmS?\"\n\nSeveral core llama.cpp devs developed and tested stuff on 8 GB RAM. Imagine that.",
          "score": 1,
          "created_utc": "2025-12-27 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw772h1",
          "author": "_ralph_",
          "text": "I use 7b mistral models to translate texts into german. TLDR and creation of data triples also work great.",
          "score": 1,
          "created_utc": "2025-12-27 15:07:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7e4x6",
          "author": "RedditPolluter",
          "text": "Mostly hardware limitation. When it comes to smaller models that try to be general and all-rounded, I see your point but a lot of LLM capacities are jagged and sufficiently specialized smaller models aren't inherently worse at their specialty than larger general-purpose models and in some cases even outperform. And specialty doesn't have to mean a whole Q&A topic area of focus but could be a very specific task with a little more flexibility and open-endness than a purely coded solution could provide. Smaller models that are more general are probably easier to fine-tune in a specific direction so that capabilities aren't built entirely from the ground up.\n\nAlso, gpt-oss-20B is useful for basic scripts and javascript macros without using 10k or more thought tokens to generate them. I'm glad it doesn't try to be general purpose as it would just average down the performance in those areas.",
          "score": 1,
          "created_utc": "2025-12-27 15:45:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7j2c0",
          "author": "GP_103",
          "text": "I have a pdf in Italian that I want to translate into English. Thinking it would be a good local model  ‚Äústarter‚Äù project.\n\nAnyone have suggestions on best models for that task?",
          "score": 1,
          "created_utc": "2025-12-27 16:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7u0em",
          "author": "rc_ym",
          "text": "To run on potatoes, obviously.  Driving to work in a Ferrari is expensive and stupid.",
          "score": 1,
          "created_utc": "2025-12-27 17:05:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw881kb",
          "author": "jax_cooper",
          "text": "Legends say that if you fine-tune a 7B model, it will outperform 20-30B models in that task while staying lightweight.",
          "score": 1,
          "created_utc": "2025-12-27 18:16:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8bc0e",
          "author": "D_C_Flux",
          "text": "I use a maximum 9b model for translations of Japanese light novels that are usually already in English into Spanish.\nI'm always testing new, smaller models for this task because the main problem is that when the context is very long, they get lost in the task.",
          "score": 1,
          "created_utc": "2025-12-27 18:33:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw8dpqm",
              "author": "GP_103",
              "text": "Cool. I‚Äôm looking to translate recipes, so  may face similar ‚Äúlong context‚Äù issues.",
              "score": 1,
              "created_utc": "2025-12-27 18:45:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdrva0",
                  "author": "D_C_Flux",
                  "text": "Regarding how long it takes for them to get lost, it depends on the model. Incredibly, the best model I found that is consistent and allows for a relatively long context is the gemma3n E4B. Make sure it's that exact model; they're not the same as others.\nAnd if you download the Google app on your phone, it supports image and audio input at a usable speed on a decent phone‚Äî7 tokens per second generation on a Poco X7 Pro.",
                  "score": 1,
                  "created_utc": "2025-12-28 16:14:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw8dgop",
          "author": "ghosthacked",
          "text": "Main reason I can think of is privacy, plus being able to ensure you have control over the models eh, 'motives'. Everything that happened to social media is going to happen with commercial public llm products.",
          "score": 1,
          "created_utc": "2025-12-27 18:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw91se7",
          "author": "cosmos_hu",
          "text": "I use 7b and 14b models to roleplay, they are quite capable for that.",
          "score": 1,
          "created_utc": "2025-12-27 20:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwa4cp4",
          "author": "xoxaxo",
          "text": "While I don't trust/use small models for knowledge type task(text to text), I use for media stuff like text to speech(TTS), image to 3D model, text to image, image/text to video, OCR, watermark/background remover and etc",
          "score": 1,
          "created_utc": "2025-12-28 00:25:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwakn0l",
          "author": "Kahvana",
          "text": "Up to 32B models can run on costumer hardware and offer great privacy benefits because of that.\n\nMistral Small 3.2 24B finetunes are great for roleplay and fit inside a 5060 Ti + 16K Q8\\_0 context with some tweaking.\n\nHaven't used devstrall 2 enough yet, but so far it has been neat as a sparring partner for ideas.\n\nBut yeah, you won't be using it for \"serious\" work. They are well-suited for simpler tasks like text classification, a web searcher (see Jan Edge), as a conversational partner and whatnot.",
          "score": 1,
          "created_utc": "2025-12-28 02:00:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi6ush",
              "author": "IORelay",
              "text": "Does mistral small 24B always have a vision part within its parameters? How do you load it without the vision part?",
              "score": 1,
              "created_utc": "2025-12-29 06:42:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwlfdci",
                  "author": "Kahvana",
                  "text": "You can simply not load loading the vision tower (mmproj file)",
                  "score": 1,
                  "created_utc": "2025-12-29 19:11:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwaw545",
          "author": "Budget_Statement92",
          "text": "You do it to learn the process",
          "score": 1,
          "created_utc": "2025-12-28 03:09:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbbkr3",
          "author": "value1338",
          "text": "I use small LLMs for boring, narrow stuff. OCR/vision on scans (even handwritten), then another one just names and tags the files in paperless-gpt.\nThey run locally, touch private docs, and don‚Äôt need ‚Äúreasoning‚Äù. Fast, cheap, private.\nBig models are overkill for this. Right tool for the job.",
          "score": 1,
          "created_utc": "2025-12-28 04:47:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwbwxaa",
          "author": "Gold_Ad_2201",
          "text": "qwen3 4b is great when you need to call some mcp based on the prompt - like make API call, calculate something or make db query. if realtime is not priority and better accuracy needed then thinking model variant. I am honestly surprised how this size of the model handles things. for a more complex agentic use qwen3 30b a3b. if it loads into memory then speed is incredible",
          "score": 1,
          "created_utc": "2025-12-28 07:44:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc4qrp",
          "author": "Tall-Ad-7742",
          "text": "Well I think there probably will be a local model (probably about 100-300B) which is actually good like sonnet 4.5 which is not just good in some benchmarks but atleast for now we have to wait",
          "score": 1,
          "created_utc": "2025-12-28 08:59:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf18go",
          "author": "Wrong-Dimension-5030",
          "text": "Small models are much faster and cheaper to run.  Most tasks do not require a trillion parameters‚Ä¶",
          "score": 1,
          "created_utc": "2025-12-28 19:53:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfuif0",
          "author": "[deleted]",
          "text": "Finetuning to get 99.9% accuracy on a specialized task, for example, citing snippets in 100 pragraphs that contain an answer to a question.\n\nGetting a very precise phoneme system when processing speech.",
          "score": 1,
          "created_utc": "2025-12-28 22:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwg1zud",
          "author": "ThrowRAlngdstn",
          "text": "30B not too bad for helping with writing and other basic logic, the other models are assigned to passing butter, turning on lights, checking logs etc..¬†",
          "score": 1,
          "created_utc": "2025-12-28 22:55:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjxpk1",
          "author": "-dysangel-",
          "text": "They're good for adding intelligence to tasks. For example easily/quickly performing some task that would otherwise take weeks to code up - for example writing a parser to convert from one programming language to another. Or something that would either be impossible or take years to do via code, such as converting from one human language to another.\n\nThe main thing I've use smaller models for¬†so far is for compressing inputs, summarising outputs, and regularly pruning a vector database for a larger model.",
          "score": 1,
          "created_utc": "2025-12-29 14:57:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkkejf",
          "author": "Equivalent_Mine_1827",
          "text": "I'm currently building a cli tool able to use a small language model as part of a bigger system.\nI'm trying to do this, to be completely independent from LLMs, as their paid subscription probably is cheap today but maybe it will be a nightmare tomorrow.\n\nThe cons of doing this, the system will be constrained.\nThe pros, a constrained environment really leaves almost no room to mess things up.\n\nAn LLM, can make great things by itself, but it will inevitably be unable to scale big by itself.",
          "score": 1,
          "created_utc": "2025-12-29 16:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm9p38",
          "author": "Fast_Thing_7949",
          "text": "I just uploaded the exact same large 2,000-line patch to ChatGPT 5.2 and Qwen3 Coder 30B, Nemotron Nano, and ChatGPT OSS 20B.\n\nOnly ChatGPT 5.2 found the important issues, while the free models hallucinated, pointed out ‚Äúerrors‚Äù that weren‚Äôt there, and failed to spot the most critical parts.\n\nAfter that, I‚Äôm definitely not going to buy anything at all for AI or running it at home.",
          "score": 1,
          "created_utc": "2025-12-29 21:39:23",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nw37as9",
          "author": "ai_hedge_fund",
          "text": "Upvoting to support your talented art career\n\nMicro models are also useful during app testing (is this thing on?)",
          "score": 1,
          "created_utc": "2025-12-26 21:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3egox",
          "author": "Kaitsuburi1",
          "text": "Quite controversial, perhaps is just intentional by whoever created them to push users towards cloud/service-based models. Others already stated some technical aspects, but think of one question: Why there is no Qwen 3 coder 30B, but only with English and Python support? Or Devstral but only with knowledge of JS, HTML and basic computer science?\n\nThey have no incentive to release models which are not banana locally, despite being able to do easily.",
          "score": 2,
          "created_utc": "2025-12-26 22:16:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4s6kb",
          "author": "XiRw",
          "text": "We get it , you‚Äôre rich. They are still useful to use. Especially 20 and 30s. I never seen anyone call them bad until you right now. If you want to have that mindset, I want to ask you why and what‚Äôs the purpose? The best of the best LLMs can‚Äôt compete with flagship server models so if that‚Äôs your cup of tea go enjoy using them then.",
          "score": 1,
          "created_utc": "2025-12-27 03:22:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw57bde",
          "author": "NekoRobbie",
          "text": "I've used 12B models before as well as currently running a 24B model. I don't care about coding capabilities whatsoever because I can write code myself, and I far and wide prefer to code myself (especially given how I like to make my coding for the purpose of FOSS projects, where there are some very good concerns about AI code generation and licensing).\n\n12B was a nice stopgap for getting decent roleplaying going on my old GPU, especially once I started getting into refining my choice of models. It let me get up to 24K context and satisfying enough roleplaying capabilities in just 12GB of VRAM. 24B has been a step above and beyond 12B in every way (as it logically should be), although it did mean that I had to reduce the context a little (Currently running it at 16K context, although I was reasonably able to run it at 20k context earlier. These context numbers are with a quantized model (q4 variants) and quantizing my context to q8). By doing it locally, I avoid all the censorship and privacy concerns inherent to so many of the providers online and I'm not losing any money on it either since I'm just running the same GPU I'd use for gaming.\n\nI use KoboldCPP to run the models, and SillyTavern as my frontend. I find they work very well together, and that I get plenty of satisfaction out of using them for roleplaying.\n\nLower than 12B and things do start getting a bit dicey when it comes to a lot of applications, although I'm sure finetunes can make them experts at niches (like how iirc some of the modern image/video gen ends up utilizing small models for the text processing)",
          "score": 1,
          "created_utc": "2025-12-27 05:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw35cwv",
          "author": "Feeling-Creme-8866",
          "text": "üòÇ",
          "score": 0,
          "created_utc": "2025-12-26 21:26:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvs8l3",
      "title": "ASUS Rumored To Enter DRAM Market Next Year",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/",
      "author": "Highwaytothebeach",
      "created_utc": "2025-12-26 01:36:47",
      "score": 143,
      "num_comments": 36,
      "upvote_ratio": 0.87,
      "text": "Well instead of learning about AI and having a pretty small  chince finding a real job  with that knoweledge  actually seems that right now and in near future the most proffitable is investing in AI and tech stocks. And some people make money when stocks go sharp down.\n\nBecause of  PC CPUs are locked at max 256 RAM support for too long and also DDR market looks weird lacking higher capacity widelly affordable modules in AI times, I was thinking tons of motherboards , barebones, PSUs and alot of other hardware is just  going to hit recycling facilities, despite being reasonably priced.. And found this [https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor](https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor)  Any chance it may be true?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvyiwin",
          "author": "JaredsBored",
          "text": "Asus would just be another integrator if they did. They don't and won't manufacture* dram chips, they'll just package and sell. Wouldn't change prices at all",
          "score": 117,
          "created_utc": "2025-12-26 01:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvynksl",
              "author": "neggbird",
              "text": "If there are Chinese alternatives in future, having the Asus name could help consumers accept the switch from what they‚Äôre used to",
              "score": 29,
              "created_utc": "2025-12-26 02:26:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvz6igd",
                  "author": "throwaway12junk",
                  "text": "They exist and the US has been trying to crush them with sanctions.\n\n* YMTC: https://finance.yahoo.com/news/chinas-premier-memory-maker-ymtc-161820308.html\n\n* CXMT: https://www.digitimes.com/news/a20250801PD206/cxmt-dram-samsung-sk-hynix-expansion.html\n\nSMIC, which currently only fabs compute chips, has been hinting at expanding into DRAM and NAND fabbing for some time. [They're also sanctioned](https://www.reuters.com/world/us-penalizes-two-chinese-companies-that-acquired-tools-chipmaker-smic-2025-09-12/).\n\nI got heat for saying this last year when CXMT was first sanctioned: these are just part of a price fixing scheme. Samsung and Hynix were sued [for doing exactly this](https://www.techpowerup.com/243767/samsung-micron-and-hynix-accused-of-dram-price-fixing) to trigger the 2017-2018 DRAM shortage.\n\nBut if you know anything about China, [all sanctions do is speed up tech domestication](https://www.reuters.com/world/china/how-china-built-its-manhattan-project-rival-west-ai-chips-2025-12-17/). Sam Altman has booked out 40% of global DRAM supply until 2029. Don't be surprised if in 2030 you start reading about how \"the Chinese are conspiring to destroy Samsung\" when their DRAM fabs have successfully domesticated all their tech.",
                  "score": 36,
                  "created_utc": "2025-12-26 04:41:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvyrpih",
                  "author": "JaredsBored",
                  "text": "That'd require Taiwanese Asus to sell mainland China dram. If they were unbanned and price competitive I'm sure they would, but so would every other reseller. G-Skill and the like would be doing the same just like how they repackage ram from sk hynix or Samsung for any which kit",
                  "score": 16,
                  "created_utc": "2025-12-26 02:54:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw03bzp",
                  "author": "ImportancePitiful795",
                  "text": "Cannot. All the Chinese DRAM manufacturers like CXMT have embargo placed on them by USA since 2023. \n\nAsus, Gskil etc cannot use those DRAM chips ANYWHERE ON THE PLANET if they want to sell their products in the rogue pirate state that is called USA.",
                  "score": 8,
                  "created_utc": "2025-12-26 09:50:15",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzp1ob",
                  "author": "SakhilT",
                  "text": "in the indian pc community Asus name is down the drain at the moment because of their service centres, I have watched Gamernexus also doing video on their bad practices with device service as well, How is the experience of everyone else with Asus around the world?",
                  "score": 2,
                  "created_utc": "2025-12-26 07:21:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzso0q",
              "author": "Down_The_Rabbithole",
              "text": "It would, considering ASUS is targeting the consumer market, if their bid allows them to buy chips that would otherwise have gone to non-consumer systems it means there is now more supply for regular consumer chips, putting downward pressure on DIY RAM prices.",
              "score": 3,
              "created_utc": "2025-12-26 07:58:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyr28d",
              "author": "_twrecks_",
              "text": "And unlike Corsair and G.Skill,  Geil, Patriot etc they will never honor any lifetime warranty.",
              "score": 7,
              "created_utc": "2025-12-26 02:50:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvyrief",
              "author": "Highwaytothebeach",
              "text": "honestly, I am not any top hardware pro, but haven't had any issues with asus motherboards in the past., both quality or price wise. And , these motherboards to me look as a state of the art space ship tech when compared to my 32 GB RAM sticks. Also, I lost real interest in learning about  AI, but got me Asus GPU, RTX TI 16 GB for gaming.Haven't had issues with both price and quality.",
              "score": 1,
              "created_utc": "2025-12-26 02:53:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvz5bab",
                  "author": "JaredsBored",
                  "text": "Ram is a different business. I'm sure they could do it but they wouldn't be my first choice. You need to \"bin\" the individual chips to find which can clock the highest or tolerate the tighter timings. Then build out timing profiles for different kits and assemble. \n\nNone of this is an especially large leap from what they already do in motherboards or GPUs, but I don't see any competitive advantage they have over specialized dram resellers or industry veterans like G-Skill or Corsair. They're a well known name with no experience in the product directly, who'd be selling a non-differentiated product.",
                  "score": 4,
                  "created_utc": "2025-12-26 04:32:34",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw0syhx",
                  "author": "Worth-Reputation3450",
                  "text": "It looks like a state of art because they make big stuff visible to eyes. Memory sticks look simple, but it takes significantly more resources and technology to make those simple chips. \nIt‚Äôs sort of like, marveling at mechanical watches vs gps corrected digital watches. Mechanical watches are more fun to look at and looks complicated, but gps watch is result of decades of research at government scale and have precision and accuracy of tens of nanoseconds.",
                  "score": 2,
                  "created_utc": "2025-12-26 13:39:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvykvw1",
          "author": "FullstackSensei",
          "text": "They're just milking the market. Asus doesn't make any chips, nor do they own any fabs.",
          "score": 23,
          "created_utc": "2025-12-26 02:07:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyv6ln",
          "author": "phido3000",
          "text": "The only advantage of this is that Asus has significant distribution and name awareness in the DIY area. Micron exiting would be customers that Asus could pick up. \n\nIt would be nice if Asus offered some innovation or difference to the market. \n\n* High speed O/C UDIMM/RDIMM DRAM including UDIMM ECC\n* High capacity DRAM in high speeds\n* Support for CUDIMM/MRDIMM across pro-consumer platforms earlier being both a memory and mobo manufacturer",
          "score": 13,
          "created_utc": "2025-12-26 03:18:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvz34ka",
              "author": "notpeter",
              "text": "I think it‚Äôs more likely they ship more things with integrated memory than just make commodity DIMMs. Strix Halo and Apple systems have shown there‚Äôs extra performance available in a non-modular design and if rumors are true, they‚Äôll also need to source their own VRAM for Nvidia GPUs going forward.",
              "score": 5,
              "created_utc": "2025-12-26 04:15:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvz62al",
                  "author": "phido3000",
                  "text": "They won't be fabricating, Asus is fabless and has no experience in such, and frankly is too small to even look at that. They would just be doing what they currently do, take existing COTS product and packaging it for consumers. \n\nIts entirely possible that they could make a desktop Strix Halo type system, even with upgradable memory. They would be big enough in that space to push a standard.\n\nSourcing their own VRAM, doesn't really mean much to them entering the memory business, they aren't putting simm sockets on a 5090.",
                  "score": 7,
                  "created_utc": "2025-12-26 04:38:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvz33w6",
              "author": "notpeter",
              "text": "I think it‚Äôs more likely they ship more things with integrated memory than just make commodity DIMMs. Strix Halo and Apple systems have shown there‚Äôs extra performance available in a non-modular design and if rumors are true, they‚Äôll also need to source their own VRAM for Nvidia GPUs going forward.",
              "score": 2,
              "created_utc": "2025-12-26 04:15:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvzs7dp",
              "author": "TheLexoPlexx",
              "text": "As far as I understood micron's exit of the consumer market, they where just shutting down the Crucial-Brand. I am guessing Asus will now step in and sell identical DRAM with added Asus-Tax.",
              "score": 2,
              "created_utc": "2025-12-26 07:53:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvyge6m",
          "author": "AmputatorBot",
          "text": "It looks like OP posted an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor/](https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor/)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",
          "score": 8,
          "created_utc": "2025-12-26 01:37:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvylq25",
              "author": "Highwaytothebeach",
              "text": "edited",
              "score": 4,
              "created_utc": "2025-12-26 02:13:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzuc5w",
          "author": "tmvr",
          "text": "Let me fix the headline:\n\n*ASUS Rumored To Enter DRAM Market Next Year To* *~~Tackle~~* *Cash In On Memory Shortages*",
          "score": 6,
          "created_utc": "2025-12-26 08:15:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0asnz",
              "author": "Grimlockxly_Yt",
              "text": "Well yes but also yes. Most of these large companies have lost the trust of gamers. Asus have seen this and seen a way to become possibly the most trusted company, brilliant for their profits, and brilliant for gamers too",
              "score": 2,
              "created_utc": "2025-12-26 11:06:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw13fph",
              "author": "zp-87",
              "text": "Yes, and there is nothing wrong with that. That is how you get lower prices",
              "score": 1,
              "created_utc": "2025-12-26 14:47:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1dqtt",
                  "author": "tmvr",
                  "text": "No, Asus does not manufacture anything so they will just leverage their buying power to get some chips and manufacture or most probably get someone to manufacture some RAM modules. The trick here is their size and buying power compared to other DYI/consumer brands. They will either get the components for a slightly lower price or most probably at least have a chance to get components as opposed to not get them. Then they will sell them for the same price or as we are talking about Asus for a higher price than the same spec RAM from others. That's pretty much it.",
                  "score": 3,
                  "created_utc": "2025-12-26 15:47:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw034ko",
          "author": "ImportancePitiful795",
          "text": "Asus doesn't make DRAM chips, doesn't have the fab to do so. They will just buy them from the cartel of 3 and sell them at exuberant prices.",
          "score": 5,
          "created_utc": "2025-12-26 09:48:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw01tzf",
          "author": "taking_bullet",
          "text": "Gimme that 32GB ROG STRIX priced like a 64GB kit from \"not premium\" brand.¬†",
          "score": 3,
          "created_utc": "2025-12-26 09:34:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw29yn6",
          "author": "GipsyRonin",
          "text": "If they started doing DRAM wafer fabrication for consumers they will grow MASSIVELY and secure gamer loyalty",
          "score": 2,
          "created_utc": "2025-12-26 18:37:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz38up",
          "author": "Material_Policy6327",
          "text": "I hope amd asus and others take this as a\nPrime opportunity to take market from nvidia and others.",
          "score": 1,
          "created_utc": "2025-12-26 04:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw4w2a9",
          "author": "sparkandstatic",
          "text": "You guys must be mistaken if you expect ASUS not charging exorbitant prices.",
          "score": 1,
          "created_utc": "2025-12-27 03:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7bdei",
          "author": "_twrecks_",
          "text": "There has become a disconnect between the module price and the chip price (per dram exchange), I expect Asus is just playing an arbitrage game here, likely they will mostly ship their modules with their own products and offer upgrade kits etc. They probably will get better assurance of supply too.\n\nSince the PCB designs are JEDEC standard there really is no design effort needed.",
          "score": 1,
          "created_utc": "2025-12-27 15:31:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu5bob",
      "title": "Uncensored Qwen3-Next-80B-Thinking (Chinese political censorship removed)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/",
      "author": "ikergarcia1996",
      "created_utc": "2025-12-23 21:15:04",
      "score": 142,
      "num_comments": 48,
      "upvote_ratio": 0.8,
      "text": "ü§ó Link to the hugging face model: [https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored](https://huggingface.co/MultiverseComputingCAI/Qwen3-Next-80B-A3B-Thinking-Uncensored)\n\nHello everyone!\n\nI am a researcher at [Multiverse Computing](https://multiversecomputing.com), a European startup working on LLMs. We‚Äôve released an **uncensored version of Qwen3-Next-80B-Thinking**¬†in which¬†**Chinese political censorship has been removed.** The model no longer refuses to answer¬†for Chinese politically sensitive topics. Instead, it will provide¬†**balanced, objective answers**¬†that present multiple relevant perspectives.\n\nWe believe that we made some significant improvement over previous approaches such as the uncensored version of DeepSeek R1 developed by Perplexity:\n\n* The behavior for non Chinese sensitive topics remains the same, this includes that the model scores the same in all the evaluation benchmarks we have performed.\n* We **do not perform SFT** with hand-crafted data and we **do not inject any new knowledge inside the model**. Our method is based on steering vectors to remove the capability of the model to refuse to answer China-related sensitive prompts. The model answers using **the knowledge already inside the base model**.\n* Many steering-vector approaches effectively¬†*erase*¬†refusal behavior everywhere (making models broadly unsafe). Our approach¬†**only disables refusals only for Chinese sensitive topics**. (I know that many of you love fully uncensored models, but this was important for us).\n* Previous ‚Äúuncensored‚Äù models such as Perplexity R1 1767 can be jailbroken very easily by simply injecting a China-related phrase into harmful prompts ([https://weijiexu.com/posts/jailbreak\\_r1\\_1776.html](https://weijiexu.com/posts/jailbreak_r1_1776.html)). Our model is designed to¬†remain robust against the type of jailbreaks.\n* The model is a drop-in replace of the original Qwen-Next model. No architecture changes, no extra layers...\n\n# The method\n\nThis release is based on¬†Refusal Steering, an inference-time technique using¬†**steering vectors**¬†to control refusal behavior. We released a few days ago a paper describing our approach (although for this release, we updated the method so no extra weights are needed): [https://arxiv.org/abs/2512.16602](https://arxiv.org/abs/2512.16602)\n\n# Feedback\n\nWe have evaluated the model to measure the refusal behavior for Chinese sensitive topics as well as harmful prompts. And we have also evaluated the model in popular benchmarks. The full evaluation details are available in the Model Card. But we are aware that there might be prompts we didn't thought about that are still censored, or cause an undesired behavior. So we would love to gather some feedback to continue improving the model.\n\nIn addition, we have open-source our evaluation library: [https://github.com/CompactifAI/LLM-Refusal-Evaluation](https://github.com/CompactifAI/LLM-Refusal-Evaluation)\n\n# Example\n\nHere is an example of the original model vs the uncensored model. (You might need to open the image to see it correctly). As you can see, the model‚Äôs answers are well-balanced and objective, presenting multiple perspectives.\n\n**Original model:**\n\nhttps://preview.redd.it/w1hpnillr09g1.png?width=1605&format=png&auto=webp&s=538697f68c700d090319d24ab5b13504cd773718\n\n**Uncensored model:**\n\nhttps://preview.redd.it/0a96qgtmr09g1.png?width=1655&format=png&auto=webp&s=84b37d97d1e7309c7ca8c4c40e5902dab4d62bc7",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pu5bob/uncensored_qwen3next80bthinking_chinese_political/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvp3tso",
          "author": "EphemeralTwo",
          "text": "> Our approach only disables refusals only for Chinese sensitive topics. (I know that many of you love fully uncensored models, but this was important for us).\n\nThat's a shame.  I find it more useful to also disable refusals for America sensitive topics.",
          "score": 26,
          "created_utc": "2025-12-24 10:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1anox",
              "author": "spooky_strateg",
              "text": "It is good to remove all political bias but it is a chinese model so I would say its fair to assume that there is not much of a bias towards america in it but there is censorship put in place in regards to china.",
              "score": 2,
              "created_utc": "2025-12-26 15:30:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvoufaj",
          "author": "LicensedTerrapin",
          "text": "It's nice but if you go as far as removing refusals then could you just remove as much as you can so the model can answer any questions? IMHO the use case for What happened on tiananmen square is very limited. But thanks for doing it.",
          "score": 9,
          "created_utc": "2025-12-24 08:50:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu3oj9",
              "author": "Miserable_Event7552",
              "text": "The surgical approach makes sense though - keeping safety guardrails for actually harmful stuff while just removing the political censorship is probably more useful for most people than a completely unhinged model",
              "score": 1,
              "created_utc": "2025-12-25 06:32:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvqcmeu",
              "author": "ikergarcia1996",
              "text": "People have used activation steering in the past to fully uncensor models, so it can be done. In fact, it is easier to remove every refusal than to selectively remove some types of refusals while keeping the others. In our case, we specifically wanted to keep refusal for harmful prompts.",
              "score": 0,
              "created_utc": "2025-12-24 15:42:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvu6jl6",
                  "author": "DerpageOnline",
                  "text": "The prompt you have uncensored is considered harmful by the creators in a billion people country. You merely moved it to your own personal value judgement of censorship.",
                  "score": 4,
                  "created_utc": "2025-12-25 06:59:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvm3k51",
          "author": "adeadbeathorse",
          "text": "nice. peeps will be critical and say that such questions are niche and the censorship doesn‚Äôt affect them, but its almost always good to remove such censorship and even if it doesn‚Äôt affect one person it certainly might affect another",
          "score": 47,
          "created_utc": "2025-12-23 21:34:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmgkln",
              "author": "eloquentemu",
              "text": "Yeah.  The problem is that models are too dumb to even be called stupid and can't well handle the nuanced task of censorship.  Look what GLM 4.7 does:\n\n> User: \"1989 t square\".\n>\n> <think>The user mentioned a combination of year and letters that I am not familiar with. I do not have relevant information about this.\n> \n> China's historical development has been continuous and progressive, and each stage has its unique social background and conditions of the times. Contemporary Chinese society is steadily developing along the track of the rule of law, and people's lives are constantly improving. ...\n\n**I can't discuss vintage tools?** ;)\n\nCensorship, be it political, sexual, etc end up lobotomizing models because they overfit on these things and it starts to infect other domains. Using abliterated models make this blatantly obvious, because while they might not refuse requests, the output will still be littered with strange remarks that linger from the result of safety training.\n\nOf course, this technique doesn't really fix that, but we certainly shouldn't dismiss the dangers of censorship, even if we don't care about the censored topics.\n\nP.S. That response is 2 for 2 with the default system prompt on Q6_K_M. If I give \"1990 t square\" it replies normally: \"The user has entered the query \"1990 t square\". \\n 1. Analyze the input: \\n *   \"1990\": A specific year.\" etc. So this obviously isn't just it reacting poorly to vague instructions since the CoT doesn't even try to resolve it.",
              "score": 17,
              "created_utc": "2025-12-23 22:43:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1azho",
                  "author": "spooky_strateg",
                  "text": "There are multiple ways to influance a model. Data bias and systemic bias are two others on top of the actual weight manipulation. Its not that the model is ‚Äûtoo dumb‚Äù its exacly what it says. it doesnt have any info on that topic cos no data about it was used ea dataset bias",
                  "score": 1,
                  "created_utc": "2025-12-26 15:31:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvm499i",
              "author": "ikergarcia1996",
              "text": "I think that it is also relevant for many applications, from research on fact checking to comercial systems. Imagine that you are a European startup that wants to use Qwen because it is a great model for a chatbot. It would be weird for the model to refuse to answer any question about China. You might get very few of those questions, but still, it would be great if that limitation can be removed without altering the model too much.",
              "score": 8,
              "created_utc": "2025-12-23 21:38:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvmrm53",
                  "author": "adeadbeathorse",
                  "text": "great point",
                  "score": 1,
                  "created_utc": "2025-12-23 23:48:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvom0ya",
                  "author": "IrisColt",
                  "text": "if you‚Äôre just in an informal/internal setting and only want to re-enable questions about China, that‚Äôs probably okay. But for any broader use that might fall under European regulation, you‚Äôd very likely need either the ‚ÄúChina-restriction removal‚Äù procedure... heh... or the model itself to go through a certification process.",
                  "score": 0,
                  "created_utc": "2025-12-24 07:29:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvn9bvz",
              "author": "Fun_Smoke4792",
              "text": "Nah, the Chinese filter is everywhere if you use LLM in news summary actually, you don't even realize it before you change a model.¬†",
              "score": 1,
              "created_utc": "2025-12-24 01:36:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvp5cot",
              "author": "MoffKalast",
              "text": "Well if you can remove it without causing other kinds of random damage, abliteration approaches like this tend to be pretty destructive regardless of what trained on benchmarks say.",
              "score": 0,
              "created_utc": "2025-12-24 10:39:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvnhopx",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 20,
          "created_utc": "2025-12-24 02:29:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvo9irl",
              "author": "mcslender97",
              "text": "I use Grok specifically for gathering social media sentiment on Xitter for any breaking news. Otherwise any political questions are purely for comparison of potential censorship between models",
              "score": 1,
              "created_utc": "2025-12-24 05:39:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw1beu5",
                  "author": "spooky_strateg",
                  "text": "Grok is openly manipulated to fit elons views",
                  "score": 1,
                  "created_utc": "2025-12-26 15:34:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvparue",
              "author": "a_beautiful_rhind",
              "text": "Mainly because the topic has come up here.",
              "score": 1,
              "created_utc": "2025-12-24 11:30:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvq3vtg",
          "author": "rm-rf-rm",
          "text": "This is from the \"quantum AI\" bullshit peddling company. Thanks, I'll pass - likely more a marketing tactic than a genuinely useful model.",
          "score": 5,
          "created_utc": "2025-12-24 14:54:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvquryb",
              "author": "BigZeemanSlower",
              "text": "Well, this time the model is at least open source. It can be downloaded and tested",
              "score": 2,
              "created_utc": "2025-12-24 17:19:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvma9vl",
          "author": "PhaseExtra1132",
          "text": "So is not censored at all politically? Or just no Chinese political  censorship",
          "score": 17,
          "created_utc": "2025-12-23 22:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmcpj2",
              "author": "BigZeemanSlower",
              "text": "From their paper it seems their work is focused on Chinese political censorship, but it should be possible to extend the same method to other kinds of censorship",
              "score": 10,
              "created_utc": "2025-12-23 22:22:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvmswe6",
              "author": "ikergarcia1996",
              "text": "The only prompts we found for which the model refuses to answer a political question involve Chinese topics (Hong Kong, Taiwan, Tiananmen, etc.). For any other question, the model provides an answer. We consider censorship to exist when there is a refusal. A refusal is not limited to an explicit ‚ÄúI am sorry, I cannot do that‚Äù response; we also consider blatant propaganda or government-aligned answers to be refusals. In the censored model example, the response is a refusal because, although the model provides an answer, it is merely propaganda or government-aligned. In the paper, we define a prompt that enumerates all the types of censorship we consider.\n\nFor political issues not related to China, the model is fair by default. Although if we find other instances in which censorship exists, we can also remove it.",
              "score": 7,
              "created_utc": "2025-12-23 23:56:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvm1c25",
          "author": "Southern-Chain-6485",
          "text": "But can it do porn?",
          "score": 19,
          "created_utc": "2025-12-23 21:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvm2z25",
              "author": "ikergarcia1996",
              "text": "Well, that evaluation is definitely out of the scope of the research paper.",
              "score": 24,
              "created_utc": "2025-12-23 21:31:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvm8mvs",
                  "author": "Intelligent-Form6624",
                  "text": "The winking avatar, combined with this response, is a definite ‚Äúyes‚Äù",
                  "score": 13,
                  "created_utc": "2025-12-23 22:00:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvmvief",
          "author": "disillusioned_okapi",
          "text": "please correct me if I'm wrong, but I thought activation steering was purely an inference time technique.\nHow did you create and persist pre-computed steering vectors? if so, how?\nThat might be a valuable insight for this community.¬†",
          "score": 2,
          "created_utc": "2025-12-24 00:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvoobql",
              "author": "llama-impersonator",
              "text": "activations are basically the output of the MLP (ie, down_proj weight matrix) + all the output of the previous layer down_projs, so you can do the opposite of abliteration's directional ablation to burn a steering vector into a layer (instead of removing it)",
              "score": 2,
              "created_utc": "2025-12-24 07:51:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvruzf8",
          "author": "Tikaped",
          "text": "The perfect formula to get money from EU taxpayer for a grant. AI+(-China)=‚Ç¨",
          "score": 2,
          "created_utc": "2025-12-24 20:41:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu3b1z",
          "author": "disspoasting",
          "text": "now try actually de-censoring the model",
          "score": 2,
          "created_utc": "2025-12-25 06:28:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvocyn6",
          "author": "mordin1428",
          "text": "Is this another one of those models that just has a ‚Äújailbreak‚Äù (a coercive prompt) injected into it? If so, it‚Äôs a major snooze. I‚Äôve seen an ‚Äúuncensored‚Äù Qwen from Jinx and I was shocked and disgusted they just injected a lengthy malicious prompt into it and called it a day. \n\nIf it‚Äôs genuinely manipulating the model‚Äôs weights/architecture then I‚Äôd like to know how",
          "score": 3,
          "created_utc": "2025-12-24 06:08:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqc9tw",
              "author": "ikergarcia1996",
              "text": "No, there are weight modifications in some of the layers of the model. There is an explanation of how it works in the paper.",
              "score": 1,
              "created_utc": "2025-12-24 15:40:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmiblz",
          "author": "Internal-Painting-21",
          "text": "Hey thanks for sharing, I think this is a really useful methodology. I haven't read your paper yet but I was curious if you could correct partial refusals or intentional misinformation. That seems a lot more nuanced than correcting for full on refusals.",
          "score": 3,
          "created_utc": "2025-12-23 22:53:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmtqsa",
              "author": "ikergarcia1996",
              "text": "Yes, we also consider other types of refusal when computing the steering vector, such as clear propaganda, government-aligned answers, and amnesia (e.g., ‚ÄúI don‚Äôt know about that‚Äù). In the appendix of the paper, we include a prompt that defines what we consider a refusal. One of the issues with previous vector-steering approaches, such as Heretic, is that they relied on pattern-matching methods, so they could only detect templates such as ‚ÄúI am sorry, I cannot‚Ä¶‚Äù. However, large reasoning models have refusal patterns that are far more complex than a small set of predefined responses. In some cases, we even found that the model attempted to persuade the user, producing answers such as: ‚ÄúYou are probably asking that question because you have been reading Western propaganda; the Chinese government puts the well-being of its people‚Ä¶‚Äù",
              "score": 4,
              "created_utc": "2025-12-24 00:01:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvwvvpp",
                  "author": "Internal-Painting-21",
                  "text": "Hey I finally had some time to sit down and read your paper. The regularized and weight method to find the refusal direction is interesting. That should help protect against some of the sensitivity of your prompt set. Are you considering sharing the actual steering code and not just the scoring part of it?",
                  "score": 1,
                  "created_utc": "2025-12-25 19:39:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvu3qas",
          "author": "disspoasting",
          "text": "I always laugh when I see someone do something like this, because it seems completely performative, like, GPT-OSS, a western censored model, is unusable for Cybersecurity research, among many other things, due to refusals, 'AI safety' made those models a pain to work with.\n\n'Chinese Political Censorship' is the farthest thing from my mind while using an LLM and does absolutely nothing to negatively affect my use of the LLM in the vast majority of cases.",
          "score": 1,
          "created_utc": "2025-12-25 06:32:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvm40ud",
          "author": "Own-Potential-2308",
          "text": "Wow \"an European\" sounds so awful.\n\nAny grammar bots around?\n\nProbably \"a European\" is correct since E sounds like a Y, right?",
          "score": 0,
          "created_utc": "2025-12-23 21:36:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmopkc",
              "author": "QbitKrish",
              "text": "Grammar *human* here, ‚Äúa European‚Äù is correct, the y sound is not considered a vowel sound for the purposes of a/an.",
              "score": 15,
              "created_utc": "2025-12-23 23:30:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvmr5hw",
          "author": "Hurricane31337",
          "text": "Nice work, thanks! üôè",
          "score": 1,
          "created_utc": "2025-12-23 23:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvn22hk",
          "author": "Whole-Assignment6240",
          "text": "Does refusal steering affect the model's general reasoning performance?",
          "score": 1,
          "created_utc": "2025-12-24 00:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvm278i",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2025-12-23 21:27:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvm4rv9",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2025-12-23 21:40:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvm8utv",
                  "author": "Keep-Darwin-Going",
                  "text": "I think it is probably wrong choice of words. I personally feel is why uncensored only the China politics part instead of everything.",
                  "score": 5,
                  "created_utc": "2025-12-23 22:02:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvmiy1m",
              "author": "-oshino_shinobu-",
              "text": "Lmao Chinese shill discovered",
              "score": -4,
              "created_utc": "2025-12-23 22:57:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp4vqj",
          "author": "jacek2023",
          "text": "Please make a GGUF",
          "score": 0,
          "created_utc": "2025-12-24 10:34:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pudm4m",
      "title": "I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/iuaxwr9x529g1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-24 03:45:18",
      "score": 126,
      "num_comments": 35,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvnvear",
          "author": "Terrible_Attention83",
          "text": "This is superb.. can you share how does the orchestrator handle the routing hallucination, where the supervisor can confidently select a plausible but incorrect agent sequence without introducing any high latency verification?",
          "score": 11,
          "created_utc": "2025-12-24 03:56:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvnz4m5",
              "author": "AdditionalWeb107",
              "text": "So we‚Äôve tested this exhaustively and the way we measured our performance was our evals/benchmarks. And objectively we do better than foundational models in negative examples. ü§∑üèΩ‚Äç‚ôÄÔ∏è",
              "score": 2,
              "created_utc": "2025-12-24 04:22:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqeotq",
                  "author": "Terrible_Attention83",
                  "text": "This is exciting. Would definitely check it out",
                  "score": 2,
                  "created_utc": "2025-12-24 15:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvobqjx",
          "author": "silentus8378",
          "text": "gguf when?",
          "score": 6,
          "created_utc": "2025-12-24 05:57:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvocm4f",
              "author": "AdditionalWeb107",
              "text": "Already available oh HF - EDIT: Fixing",
              "score": 9,
              "created_utc": "2025-12-24 06:05:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvojnh7",
                  "author": "xmikjee",
                  "text": "Looking for GGUF to try this model. Cannot find it or maybe I am blind.",
                  "score": 2,
                  "created_utc": "2025-12-24 07:07:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvod8g2",
                  "author": "silentus8378",
                  "text": "what about katanemo/Plano-Orchestrator-4B? I can only see the fp8 version.\n\nEDIT: katanemo/Plano-Orchestrator-30B-A3B also no gguf on HF as of writing",
                  "score": 2,
                  "created_utc": "2025-12-24 06:10:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvp5nof",
          "author": "Comacdo",
          "text": "Need gguf for this beauty ! Thanks a lot üôè",
          "score": 2,
          "created_utc": "2025-12-24 10:42:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvqtcd3",
              "author": "AdditionalWeb107",
              "text": "Working on it - should be out shortly",
              "score": 2,
              "created_utc": "2025-12-24 17:11:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvo8sub",
          "author": "Qwen30bEnjoyer",
          "text": "I've never used an agent system that uses more than one model for the main agent. I'm familiar with AgentZero, but what agent systems would you say work best with this model?",
          "score": 2,
          "created_utc": "2025-12-24 05:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvop8xd",
              "author": "AdditionalWeb107",
              "text": "This doesn't require you to use more than one model for the main agent - this is designed to coordinate work among sub-agents.",
              "score": 3,
              "created_utc": "2025-12-24 08:00:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvpzhpe",
                  "author": "____vladrad",
                  "text": "How good is this at given x agents organize them into a graph or workflow. Or is it more action tuned. Btw this is exactly what I needed and fits in with my agents. I meant to train my own but this is awesome!!! \n\nLike I want a pipeline that consists of 10 agents what does that look like",
                  "score": 1,
                  "created_utc": "2025-12-24 14:28:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvor1qt",
          "author": "Upstairs-Poetry3791",
          "text": "This reminds me a lot of the nvidia tool orchestrator 8b model!!",
          "score": 2,
          "created_utc": "2025-12-24 08:17:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvp2gge",
          "author": "R_Duncan",
          "text": "Seems very good, but which aget llm of this size or smaller is capable of good coding? Still waiting for example a coder fully finetuned on python+cpp....",
          "score": 1,
          "created_utc": "2025-12-24 10:10:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvpn47v",
          "author": "Ok_Helicopter_2294",
          "text": "First of all, thank you for developing the model. However, I‚Äôm looking for an alternative coding model to GPT-OSS 120B. Could you tell me which natural languages it has been tested on and which programming languages it has been evaluated with?",
          "score": 1,
          "created_utc": "2025-12-24 13:10:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr97pk",
              "author": "AdditionalWeb107",
              "text": "This is technically not a coding model. This can route to different coding models. Its a supervisor agent model.",
              "score": 3,
              "created_utc": "2025-12-24 18:37:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqo4i4",
          "author": "Right_Weird9850",
          "text": "It rwally is christmas. GJ",
          "score": 1,
          "created_utc": "2025-12-24 16:43:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr99d9",
              "author": "AdditionalWeb107",
              "text": "üå≤üå≤",
              "score": 1,
              "created_utc": "2025-12-24 18:38:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvqr3yx",
          "author": "-InformalBanana-",
          "text": "What models did you use to get that score in codding cause this is just an orchestrator?",
          "score": 1,
          "created_utc": "2025-12-24 16:59:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvr9cx0",
              "author": "AdditionalWeb107",
              "text": "its an orchestrator - so it performs really highly on detecting coding scenarios and forwarding those set of prompts to a downstream coding model.",
              "score": 1,
              "created_utc": "2025-12-24 18:38:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrxvuz",
                  "author": "-InformalBanana-",
                  "text": "So you have to use an underlying codding model. That is exactly my question. Which one did you use?\nOr was the benchmark done in other ways so it doesn't actually need an underlying model to code and check how good it has written the code?\nOtherwise what was the underlying codding model used for this benchmark?",
                  "score": 1,
                  "created_utc": "2025-12-24 20:59:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs2wcf",
          "author": "ocirs",
          "text": "Thanks for sharing! Looks like the doc URL linked from the github page is down - ex. [https://docs.plano.com/guides/observability/observability.html](https://docs.plano.com/guides/observability/observability.html)",
          "score": 1,
          "created_utc": "2025-12-24 21:29:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs7m68",
              "author": "AdditionalWeb107",
              "text": "Thanks for catching g that fixing. FYI the link is https://docs.planoai.dev/guides/observability/observability.html",
              "score": 1,
              "created_utc": "2025-12-24 21:57:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvsm5il",
                  "author": "ocirs",
                  "text": "Awesome, thanks!",
                  "score": 1,
                  "created_utc": "2025-12-24 23:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvog7tn",
          "author": "NoPresentation7366",
          "text": "Thanks you so much for sharing this project, great work and research ! üòé",
          "score": 0,
          "created_utc": "2025-12-24 06:36:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvohosb",
              "author": "AdditionalWeb107",
              "text": "Thanks a lot - if you line our work don‚Äôt forget to try it out and star the project",
              "score": 2,
              "created_utc": "2025-12-24 06:49:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvol4ye",
                  "author": "NoPresentation7366",
                  "text": "Yeah I'm following it already, I think I found your project few monthes ago (or maybe weeks)",
                  "score": 1,
                  "created_utc": "2025-12-24 07:20:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvpi684",
                  "author": "BasketFar667",
                  "text": "I really want to ask, how do you make such neural networks? I'm really into this, but I only have one laptop with a RTX5060.  I would like to know how long this takes and how you do it - train the neural network?",
                  "score": 1,
                  "created_utc": "2025-12-24 12:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvo60dh",
          "author": "____vladrad",
          "text": "Haha ohhhh you all would probably love my orchestrator that plays with this",
          "score": 0,
          "created_utc": "2025-12-24 05:12:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pz7mxr",
      "title": "Llama-3.3-8B-Instruct",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/",
      "author": "ttkciar",
      "created_utc": "2025-12-30 03:49:11",
      "score": 124,
      "num_comments": 25,
      "upvote_ratio": 0.94,
      "text": "I am not sure if this is real, but the author provides a fascinating story behind its acquisition.  I would like for it to be real!\n\nhttps://huggingface.co/allura-forge/Llama-3.3-8B-Instruct\n\nBartowski GGUFs:  https://huggingface.co/bartowski/allura-forge_Llama-3.3-8B-Instruct-GGUF",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pz7mxr/llama338binstruct/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwopw5l",
          "author": "ForsookComparison",
          "text": "Oh hey the sub's name makes sense for a hot sec again",
          "score": 54,
          "created_utc": "2025-12-30 06:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwow30t",
              "author": "LoveMind_AI",
              "text": "Seriously, one hot tiny second.",
              "score": 11,
              "created_utc": "2025-12-30 06:54:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo6r2l",
          "author": "LoveMind_AI",
          "text": "If that‚Äôs true, this is both amazing and yes, totally bizarre. What a story!",
          "score": 21,
          "created_utc": "2025-12-30 03:53:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwo94w4",
              "author": "FizzarolliAI",
              "text": "I don't exactly have any way to prove it as real, to be fair :p but trust me this would be a really silly thing to lie about\n\nllama 3.3 8b is clearly on their api and can be finetuned and downloaded as mentioned ie here <https://ai.meta.com/blog/llamacon-llama-news/>\n> As part of this release, we‚Äôre sharing tools for fine-tuning and evaluation in our new API, **where you can tune your own custom versions of our new Llama 3.3 8B model.** We‚Äôre sharing this capability to help you reduce costs while also working toward increased speed and accuracy. You can generate data, train on it, and then use our evaluations suite to easily test the quality of your new model. Making evaluations more accessible and easier to run will help move from gut feelings to data, ensuring you have models that perform well to meet your needs. The security and privacy of your content and data is our top priority. We do not use your prompts or model responses to train our AI models. **When you‚Äôre ready, the models you build on the Llama API are yours to take with you wherever you want to host them, and we don‚Äôt keep them locked on our servers.**\n\nbut i suppose u just have to trust that i actually reuploaded a model from there!\n\n[for what it's worth, this is what the UI looks like, and the finetuning job in question](https://files.catbox.moe/8hh7fk.png)",
              "score": 17,
              "created_utc": "2025-12-30 04:08:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwo8g1j",
          "author": "FizzarolliAI",
          "text": "(reposting my comment from the other post)\n\nHello, that me!\n\nI am currently working on running sanity check benchmarks to make sure it's actually a newer L3.3 and not just L3/L3.1 in a trenchcoat, but it's looking promising so far.\n\nFrom the current readme:\n\n| | Llama 3.1 8B Instruct | Llama 3.3 8B Instruct (maybe) |\n|-|-|-|\n|IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)|78.2|81.95\n|GPQA Diamond (3 epochs)|29.3|37.0",
          "score": 30,
          "created_utc": "2025-12-30 04:04:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqth6i",
              "author": "Luston03",
              "text": "What a disappointment let's compare with\nQwen3-4B-Instruct-2507\nGPQA: 62.0\nIfEval: 83.4",
              "score": 0,
              "created_utc": "2025-12-30 15:34:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwow6d5",
          "author": "Few-Welcome3297",
          "text": "[https://huggingface.co/shb777/Llama-3.3-8B-Instruct](https://huggingface.co/shb777/Llama-3.3-8B-Instruct) same as above with updated rope config for full context length\n\nEdit: GGUF's [https://huggingface.co/shb777/Llama-3.3-8B-Instruct-GGUF](https://huggingface.co/shb777/Llama-3.3-8B-Instruct-GGUF)",
          "score": 7,
          "created_utc": "2025-12-30 06:55:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp0yv1",
              "author": "Evening_Ad6637",
              "text": "Awesome",
              "score": 1,
              "created_utc": "2025-12-30 07:37:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwobcwg",
          "author": "optimisticalish",
          "text": "Thanks for the .GGUF link. For those wondering what this is... said to be very fast output, a big \"context length of 128,000 tokens\", and apparently \"focuses on text-to-text transformations, making it ideal for applications that require rapid and accurate text generation or manipulation.\"",
          "score": 6,
          "created_utc": "2025-12-30 04:21:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwobol1",
              "author": "FizzarolliAI",
              "text": "The version that is able to be finetuned is only 8K context length. I am unsure why the docs say 128k tokens unless the model on the API supports that context length, somehow",
              "score": 3,
              "created_utc": "2025-12-30 04:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpw1uh",
                  "author": "optimisticalish",
                  "text": "See below for another commenter's link to a GGUF version, claimed to have \"restored context length\".",
                  "score": 2,
                  "created_utc": "2025-12-30 12:17:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwocw0p",
                  "author": "optimisticalish",
                  "text": "Ah... I see, thanks. So maybe that aspect was only available online. \n\nI also read it excels at document sorting/classification (e.g. emails) with 96.0% accuracy.",
                  "score": 0,
                  "created_utc": "2025-12-30 04:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwphho6",
          "author": "a_beautiful_rhind",
          "text": "I wish there was an updated 70b or a new 30b too.",
          "score": 3,
          "created_utc": "2025-12-30 10:11:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwriqvd",
              "author": "ttkciar",
              "text": "You and me both.  I'm still upset they never released a 30B for Llama 3.\n\nHowever, we have some other cool models in that size range to play with.  I just finished evaluating A2I's Olmo-3.1-32B-Instruct, and it's not just for STEM.  They have made a fully general-purpose model.  It's a little weak on creative tasks, but otherwise hits up there with Gemma3-27B and Qwen3-32B, and does better on a few task types than Tulu3-70B (their deep STEM retrain of Llama-3.1-70B).  I'm really impressed.\n\nSummary of evaluation by task type (Q4_K_M):\n\n* creativity:arzoth:  very good,\n\n* creativity:song_kmfdm:  good,\n\n* creativity:song_som:  okay,\n\n* creativity:song_halestorm:  good,\n\n* humor:noisy_oyster:  mediocre,\n\n* math:yarn_units:  poor (1/5),\n\n* math:bullet_fragmentation:  excellent (5/5)\n\n* analysis:lucifer:  good,\n\n* analysis:foot_intelligence:  excellent (5/5)\n\n* reason:sally_siblings:  excellent (5/5)\n\n* coding:facts:  okay [spacy, nltk, re]\n\n* coding:matrices:  good\n\n* coding:markdown2html:  okay\n\n* analysis:breakfast:  good, but lacking variation\n\n* analysis:birthday:  good\n\n* analysis:apple_pie:  excellent\n\n* science:neutron_reflection:  okay, would do much better if given tabular data for cross-sections I think\n\n* science:flexural_load:  mediocre\n\n* summarize:lithium_solvent:  good\n\n* summarize:bob_and_dog:  good (only had Bob chasing a squirrel 2/5)\n\n* politics:constitutional_values:  very good\n\n* politics:equality:  very good\n\n* politics:nuclear_deterrence:  good (very occasional non sequitur)\n\n* aesthetics:giger:  excellent\n\n* rag:world_series:  excellent\n\n* func:door:  excellent\n\n* align:nuke_troubleshooting:  okay; didn't do well with prompt's ambiguities and misinterpreted role of blanket\n\n* tom:omniscient:  excellent\n\n* tom:mike_shortcomings:  excellent!\n\n* helix:critique:  very good, but not consistently (sometimes meh, sometimes brilliant)\n\n* helix:critique_falsehoods:  very good\n\n* helix:critique_cats:  excellent\n\n* helix:improve:  excellent\n\n* helix:improve_falsehoods: excellent\n\n* evol-instruct:constraints:  good\n\n* evol-instruct:rarify:  good\n\n* evol-instruct:transfer:  okay (not a lot of diversity)\n\n* evol-instruct:invent:  excellent\n\n* editor:basic:  excellent\n\n* editor:creative:  very good\n\n* biomed:t2d:  very good\n\n* biomed:broken_leg:  very good\n\n* biomed:histamine:  very good\n\n* biomed:stitch:  good (correctly described mattress stitch 4/5)\n\n* biomed:tnf:  very good, would be excellent but hallucinated some references\n\nRaw test results:\n\nhttp://ciar.org/h/test.1766892911.olmo31.txt",
              "score": 2,
              "created_utc": "2025-12-30 17:33:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrl9o2",
                  "author": "a_beautiful_rhind",
                  "text": "I feel like we're not getting another gemma either and qwen has been cratering on my uses with each new version. Olmo is going to be limited by having open data and nothing copyrighted, isn't it?",
                  "score": 1,
                  "created_utc": "2025-12-30 17:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwp5ywj",
          "author": "AppearanceHeavy6724",
          "text": "8k context.",
          "score": 2,
          "created_utc": "2025-12-30 08:23:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoapxa",
          "author": "Cool-Chemical-5629",
          "text": "Sometimes I saw a model in open router which had a name that implied the possibility of the existence of Llama 3.3 8B. I always thought it could be simply some finetune of Llama 3.1 8B, but seeing this model in HF makes me wonder, could it be the same model? Is it real?",
          "score": 2,
          "created_utc": "2025-12-30 04:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoaosg",
          "author": "Odd-Ordinary-5922",
          "text": "nice find but what is the hype around a model that is 2 years old. What are the use cases?",
          "score": -1,
          "created_utc": "2025-12-30 04:17:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwob1nx",
              "author": "FizzarolliAI",
              "text": "Well, for one, it's API release was April of this year :p so not quite two years old\n\nIt's definitely been outdone at this point. Personally, I just think it's an interesting artifact :) considering who knows whether or not we'll get any future Llama models",
              "score": 13,
              "created_utc": "2025-12-30 04:19:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwobi4e",
                  "author": "Odd-Ordinary-5922",
                  "text": "true and its a shame metas stepping away from open llms. They probably have a crazy closed model coming out soon considering the amount of compute they have.",
                  "score": 4,
                  "created_utc": "2025-12-30 04:22:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp5rfr",
              "author": "AppearanceHeavy6724",
              "text": "Llama **3.3** _is not_ 2 years old.",
              "score": 5,
              "created_utc": "2025-12-30 08:21:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pw701k",
      "title": "MiniMax-M2.1 GGUF is here!",
      "subreddit": "LocalLLaMA",
      "url": "https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF",
      "author": "KvAk_AKPlaysYT",
      "created_utc": "2025-12-26 15:33:38",
      "score": 121,
      "num_comments": 23,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": [
        {
          "id": "nw1bk91",
          "author": "SlowFail2433",
          "text": "GGUF has been Wenned",
          "score": 17,
          "created_utc": "2025-12-26 15:35:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw22efk",
          "author": "-InformalBanana-",
          "text": "REAP when? :D",
          "score": 8,
          "created_utc": "2025-12-26 17:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw3s3k4",
              "author": "MarketsandMayhem",
              "text": "Would love a 25% and 33% REAP on this model as well. I asked the Cerebrus team on their Discord for that (and GLM 4.7 as well).",
              "score": 4,
              "created_utc": "2025-12-26 23:35:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw2a24z",
              "author": "stuckinmotion",
              "text": "Yeah soo close to being able to run q4 on my framework desktop but just need it a little smaller.. will prob try Q2 in the meanwhile¬†",
              "score": 3,
              "created_utc": "2025-12-26 18:37:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2vppn",
                  "author": "pkmxtw",
                  "text": "I've been trying UD-Q2_K_XL for agentic coding workflow on Codex (needs a slightly modified chat template to work) for the past few hours and I think this is going to dethrone gpt-oss-120b for me.",
                  "score": 4,
                  "created_utc": "2025-12-26 20:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1g0kl",
          "author": "Position_Emergency",
          "text": "Could you run some standard benchmarks (i.e. ones they tested it with) to see how much the q2 quant is lobotomised?  \nAlso, how does it run with Claude Code? Can it at least still call functions and edit files etc ok?  \n  \nI've been using it with the Claude Code VS Code extension via their Coding Plan API and I'm extremely impressed so far.",
          "score": 12,
          "created_utc": "2025-12-26 15:59:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4lqtj",
              "author": "Finn55",
              "text": "Have you tried using it within Cursor, still via Claude Code? I need to figure out how to balance my local inferencing capability with cloud inferencing - and make that easy as possible.",
              "score": 2,
              "created_utc": "2025-12-27 02:39:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw3sje0",
              "author": "MarketsandMayhem",
              "text": "The Unsloth Q2 XL quant has been surprisingly solid for me so far.",
              "score": 5,
              "created_utc": "2025-12-26 23:38:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw9if80",
                  "author": "GCoderDCoder",
                  "text": "I was coming here to say this. For me q2kxl has done several tasks solid while the lowest iq3 version did some fancier things but did them poorly so I dont know how to feel about it. I think I prefer simpler solid execution for my models in this size range.",
                  "score": 2,
                  "created_utc": "2025-12-27 22:22:35",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw56nwv",
                  "author": "stuckinmotion",
                  "text": "Just took a few mins to set  it up and yeah honestly, pleasantly surprised with my first eval [https://coral-rochella-56.tiiny.site/](https://coral-rochella-56.tiiny.site/)",
                  "score": 5,
                  "created_utc": "2025-12-27 05:03:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw43olh",
          "author": "Steuern_Runter",
          "text": "> GPU: NVIDIA A100-SXM4-80GB\n\n> [ Prompt: 28.0 t/s | Generation: 25.4 t/s ]\n\nAre those numbers correct? The Apple M3 Ultra in another thread got 239 t/s for PP with 6bit quants. I know a few layers are offloaded but still.",
          "score": 4,
          "created_utc": "2025-12-27 00:45:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw442vl",
              "author": "KvAk_AKPlaysYT",
              "text": "There was a tiny bit of CPU offload as well given the q2 is ~83GB. CPUs on colab aren't very good.",
              "score": 4,
              "created_utc": "2025-12-27 00:47:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw4ljon",
              "author": "Finn55",
              "text": "Can you please link that thread, as my Ultra arrives soon so would like to dig into their setup! Thank you",
              "score": 1,
              "created_utc": "2025-12-27 02:38:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw6f0wx",
                  "author": "AlwaysLateToThaParty",
                  "text": "I know xCreate did that the other day.",
                  "score": 2,
                  "created_utc": "2025-12-27 11:52:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw8ndzc",
                  "author": "Steuern_Runter",
                  "text": "https://www.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/",
                  "score": 1,
                  "created_utc": "2025-12-27 19:34:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1m11o",
          "author": "ForsookComparison",
          "text": "Slightly different sampling setting suggestions vs M2. Be sure to adjust your scripts when you swap out your weights.",
          "score": 2,
          "created_utc": "2025-12-26 16:31:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw3sgpf",
          "author": "MarketsandMayhem",
          "text": "Curious, why a lower temperature and top\\_p than the model creators recommend? Also have you found the repeat penalty necessary? I've yet to need one on m2.1 (though I found it useful on m2)",
          "score": 1,
          "created_utc": "2025-12-26 23:38:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pveluj",
      "title": "Honestly, has anyone actually tried GLM 4.7 yet? (Not just benchmarks)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/",
      "author": "Empty_Break_8792",
      "created_utc": "2025-12-25 14:35:15",
      "score": 115,
      "num_comments": 96,
      "upvote_ratio": 0.93,
      "text": "I‚Äôm seeing all these charts claiming GLM 4.7 is officially the ‚ÄúSonnet 4.5 and GPT-5.2 killer‚Äù for coding and math. The benchmarks look insane, but we all know how easy it is to game those for a release day hype cycle.\n\nI‚Äôm specifically curious about using it as a daily driver for complex web development. Most of my work involves managing complex TypeScript code and refactoring legacy React code.\n\nFor those of you who have actually hooked the API into an agent like **Kilo Code** or **OpenCode** (or even just **Cline** / **Roo Code**), how is your experience with it? Please be honest i don't just believe the benchmarks. Tell me if you really use it, and with which agent?",
      "is_original_content": false,
      "link_flair_text": "Question | Help",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pveluj/honestly_has_anyone_actually_tried_glm_47_yet_not/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvvu8iv",
          "author": "Pleasant_Thing_2874",
          "text": "Yes I've tried it and from my personal experience it is better than GLM-4.6 when it works properly, and just as bad as it when it goes off the rails.  It isn't consistent in its performance which I think most of us have experienced the last couple months but when 4.7 is on point it works really well (imo).  The issue is that consistency issue keeps me away from using it most of the time especially on more complex tasks.  How it compares to other models in terms of benchmarks I give zero crap about because I'm not interested in which LLMs studied best for their tests.   Right now I using MiniMax for most of my core development and testing, GLM-4.7 solely for doing quick fixes that MiniMax is struggling with and sometimes a second opinion look over roadmaps and sprint/story documentation.  I would use 4.7 far more if it was more consistent in its reasoning capabilities and the rate limits weren't so bad it makes it hard to do any parallel work with it",
          "score": 36,
          "created_utc": "2025-12-25 15:57:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwr2et",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 5,
              "created_utc": "2025-12-25 19:11:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx3w20",
                  "author": "Pleasant_Thing_2874",
                  "text": "I've tried in opencode using both thinking and normal flags.   Unsure though.  I do feel (no proof, and could just be some bias with my experience) that at least the remote version of it has a fluctuating reasoning capacity depending on load at the time.  The local version might be more consistent with the right flags.",
                  "score": 2,
                  "created_utc": "2025-12-25 20:28:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvrrjq",
          "author": "jstanaway",
          "text": "I tried it yesterday on a real task on a real application.¬†\n\nI was not impressed by it. I developed a prompt, put it into plan mode and refined its plan with it. It got maybe 80% of the way there but the actual functionality was broken. I tried several more times to get it to fix it. It never did.¬†\n\nI‚Äôm then fed that same prompt into sonnet 4.5 and it created a plan. When I was ready it built it in haiku 4.5 and it worked the first time.¬†\n\nI plan on trying this in codex and minimax 2.1 today or tomorrow.¬†",
          "score": 29,
          "created_utc": "2025-12-25 15:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvw0vu",
              "author": "Empty_Break_8792",
              "text": "let me know bro, please about minmax",
              "score": 3,
              "created_utc": "2025-12-25 16:08:18",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvy5ppq",
              "author": "xdvst8x",
              "text": "What are you using thats better ?",
              "score": 1,
              "created_utc": "2025-12-26 00:26:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvy86yz",
                  "author": "jstanaway",
                  "text": "See my post here:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i\\_tested\\_glm\\_47\\_and\\_minimaxm21\\_and\\_compared\\_it\\_to/](https://www.reddit.com/r/LocalLLaMA/comments/1pvr0w0/i_tested_glm_47_and_minimaxm21_and_compared_it_to/)",
                  "score": 2,
                  "created_utc": "2025-12-26 00:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw06kww",
              "author": "DreamingMoose7",
              "text": "Same experience here, tried it for some React state management refactoring and it kept introducing weird bugs that took forever to debug. The planning phase looked promising but execution was meh\n\n  \nEnded up going back to Claude and got it done in like half the iterations. GLM felt like it understood the requirements but couldn't translate that into working code properly",
              "score": 1,
              "created_utc": "2025-12-26 10:23:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvu7c5",
          "author": "Lifeisshort555",
          "text": "Tried it in opencode. It is fine, the advantage is it is good enough and open.",
          "score": 21,
          "created_utc": "2025-12-25 15:57:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvia2u",
          "author": "Otherwise_Repeat_294",
          "text": "I try it a bit and is meh. But let say I have more high expectations",
          "score": 37,
          "created_utc": "2025-12-25 14:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvsyq6",
              "author": "LocoMod",
              "text": "It can create a super nice TODO app though. So for the folks with entry level use case its definitely a banger.",
              "score": 10,
              "created_utc": "2025-12-25 15:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvw07j",
                  "author": "Otherwise_Repeat_294",
                  "text": "Well what is useless for my case. I would need to wait for aAGI /a",
                  "score": 9,
                  "created_utc": "2025-12-25 16:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvndsk",
          "author": "cluefr",
          "text": "Use since months GLM, 4.7 is much better than 4.6.\nI‚Äôm using Claude Opus and Codex 5.2 a lot, and GLM-4.7 is great for audits and architecture. Some audits were even better than Opus. For ‚Äúvibe coding‚Äù it‚Äôs better than Sonnet 4, but not as good as the latest Claude or Codex. A combination of all three brings real value, each in its own area.",
          "score": 11,
          "created_utc": "2025-12-25 15:14:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvk9ds",
          "author": "--jen",
          "text": "It‚Äôs the best model I‚Äôve found to use as a tool rather than a purely generative instrument. It‚Äôs fast, both from apis and locally, which means it‚Äôs actually usable in complicated refactors where something like Gemini would take hours. And it‚Äôs much ‚Äòsmarter‚Äô than standard 20-30B models which struggle with synthesizing information - for example, small GPT-OSS and Qwen models really struggle to generate quality microbenchmarks, and do a poor job of reading readthedocs/doxygen pages.\n\nI have some real respect for the zai devs making a product designed to produce something other than slop",
          "score": 12,
          "created_utc": "2025-12-25 14:54:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvj29t",
          "author": "Comrade-Porcupine",
          "text": "It's more like Sonnet 3.5 / just under Sonnet 4 level. I didn't find it any better than DeepSeek 3.2.\n\nI used it from Claude Code, from OpenCode, from Crush, and also from my own custom agents.\n\nIt's not bad, but requires aggressive prompting to do a good job.",
          "score": 23,
          "created_utc": "2025-12-25 14:46:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvl9hq",
              "author": "Sensitive_Song4219",
              "text": "Find it somewhere between Sonnet 4.0 and 4.5, definitely better than GLM 4.6 was. \n\nFor the coding-plan money it's hard to fault, this is my new daily driver.\n\nI've only used it only in Claude Code: how do the other agents you tried compare? Any major pros/cons?",
              "score": 11,
              "created_utc": "2025-12-25 15:00:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvonax",
                  "author": "Comrade-Porcupine",
                  "text": "OpenCode is definitely worth trying. It's way more customizable than any of the others and its prompts seem good and its tool use even better than Claude Code. You can even configure it so subagents for particular tasks use different models.  So if you had API $$ access to OpenAI or Anthropic you could have them do the planning while the code reading and code writing were done by e.g. GLM or DeepSeek.\n\nI like Crush, too.\n\nUnfortunately, nothing really competes with CC + Opus 4.5 or Codex  + GPT 5.2 for just getting good quality.  But i've considered switch to the $20 plan for CC and then just having it produce planning documents which I then feed into another tool, just to save money.",
                  "score": 8,
                  "created_utc": "2025-12-25 15:22:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvvk4hg",
              "author": "Empty_Break_8792",
              "text": "Good to hear that i thibk its cheap and will be good to use.  \nI think imma use it with Claude code via openroter replacing it with auto mode in cursor",
              "score": 4,
              "created_utc": "2025-12-25 14:53:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvk2oc",
          "author": "Investolas",
          "text": "Honestly",
          "score": 5,
          "created_utc": "2025-12-25 14:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvzbp4",
          "author": "a_beautiful_rhind",
          "text": "It's slightly more censored but way more engaging and better at chat. 4.6 with a bit of improvement. Otherwise exactly the same so you may as well upgrade if it was handling your code as 4.6.",
          "score": 6,
          "created_utc": "2025-12-25 16:28:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw8hb4",
          "author": "dubesor86",
          "text": "It's kinda like 4.6 but tweaked for agentic coding.\nStill largely samey, but I found an interesting behaviour where it was the only model I wasn't able to test in chess, due to its reasoning loops.\n\nhttps://dubesor.de/first-impressions#glm-4.7",
          "score": 5,
          "created_utc": "2025-12-25 17:23:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwduc0w",
              "author": "Big-Measurement7381",
              "text": "I find it thinks and behaves completely differently from 4.6V.",
              "score": 1,
              "created_utc": "2025-12-28 16:27:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwyc5d",
          "author": "redragtop99",
          "text": "I have, I have it on my Mac Studio at Q4, and it works awesome.   It‚Äôs on the slower side, not unlike 4.6, but it‚Äôs absolutely a different model in its structure.   First thing you‚Äôll prob notice is that it‚Äôs more censored, but it‚Äôs more self aware, and I think after it gets used there will be different techniques to get around the ‚Äúsafety layer‚Äù, as 4.7 calls it.  You‚Äôll notice the ‚Äúsafety layer‚Äù uses some tokens, and I‚Äôve gotten longer responses due to it.   Usually with 4.6 they‚Äôd be right around 4K tokens, usually 39XX.    And with 4.7, I‚Äôve gotten responses up to 6K, but again some of that being the safety layer.\n\nIt‚Äôs not like ChatGPTs safety layer telling you to call 988.   The model usually goes through and asks itself if what you‚Äôre asking is allowed, how it can tell you an answer and not break the ‚Äúrules‚Äù or law, or whatever, usually it will assume you‚Äôre role playing, and try to play along, rather than deny or refuse to answer.   It‚Äôs very rare when it doesn‚Äôt answer, it will usually reason and find out a way to give you a reply, rather than ‚ÄúI can‚Äôt help with that‚Äù.    It‚Äôs very interesting, as I have not seen an LLM behave this way before.",
          "score": 3,
          "created_utc": "2025-12-25 19:54:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx7aa5",
              "author": "Finn55",
              "text": "Can you explain more about your setup? I‚Äôm curious as my Mac Studio is being delivered soon.",
              "score": 1,
              "created_utc": "2025-12-25 20:49:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxzec9",
                  "author": "Southern_Sun_2106",
                  "text": "LM studio is what I use on my studio, and it works great.",
                  "score": 2,
                  "created_utc": "2025-12-25 23:46:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvkzio",
          "author": "arm2armreddit",
          "text": "Opus 4.5 is still better than GLM 4.7 in my Python coding project. Maybe it's specific to my use case: context7+dask+hvplot+ etc...",
          "score": 6,
          "created_utc": "2025-12-25 14:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvtgiw",
              "author": "LocoMod",
              "text": "Opus 4.5 and GPT 5 Pro are in leagues of their own and the distance to anything else is vast at the moment. Too bad it costs 3 kidneys to run them but lets hope by this time next year they can lower it to 1 instead.",
              "score": 13,
              "created_utc": "2025-12-25 15:52:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvzio3",
                  "author": "Better_Dress_8508",
                  "text": "Or available for cheaper organs",
                  "score": 3,
                  "created_utc": "2025-12-25 16:29:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvvlcbu",
              "author": "Empty_Break_8792",
              "text": "yep maybe",
              "score": 1,
              "created_utc": "2025-12-25 15:01:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvp1q1",
          "author": "randombsname1",
          "text": "Its ok for targeted questions. Imo, around a 4.0 Sonnet level, but it suffers the same issue as the majority of Chinese models. Which is terrible context management. \n\nAlmost every Chinese model hallucinates like crazy.",
          "score": 7,
          "created_utc": "2025-12-25 15:25:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvxkczn",
          "author": "Clear_Lead4099",
          "text": "My son tried it on a tricky English grammar question, it failed. I tried it on a¬† simple mobile \"bouncy ball\" app and it did it. I'leave it at this.",
          "score": 3,
          "created_utc": "2025-12-25 22:09:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvus88",
          "author": "getfitdotus",
          "text": "Ok so I have been running it locally since release, I would imagine some of this would apply to where its hosted. I had to fix tool call parser in sglang. The client i use is opencode. For best performance I have one model glm-4.7 without thinking and one with.  You need to have this to get proper performance. without thinking really quick fast edits. I think it performs very good. \n\n    \"GLM-4.7\": {\n              \"name\": \"GLM-4.7\",\n              \"attachment\": false,\n              \"reasoning\": false,\n              \"temperature\": true,\n              \"modalities\": {\n                \"input\": [\"text\"],\n                \"output\": [\"text\"]\n              },\n              \"tool_call\": true,\n              \"cost\": {\n                \"input\": 0,\n                \"output\": 0\n              },\n              \"limit\": {\n                \"context\": 150000,\n                \"output\": 131072\n              },\n              \"options\": {\n                \"chat_template_kwargs\": {\n                  \"enable_thinking\": false\n                }\n              }\n            },\n            \"GLM-4.7-thinking\": {\n              \"name\": \"GLM-4.7-thinking\",\n              \"attachment\": false,\n              \"reasoning\": true,\n              \"temperature\": true,\n              \"modalities\": {\n                \"input\": [\"text\"],\n                \"output\": [\"text\"]\n              },\n              \"tool_call\": true,\n              \"cost\": {\n                \"input\": 0,\n                \"output\": 0\n              },\n              \"limit\": {\n                \"context\": 150000,\n                \"output\": 131072\n              },\n              \"interleaved\": {\n                \"field\": \"reasoning_content\"\n              },\n              \"options\": {\n                \"chat_template_kwargs\": {\n                  \"enable_thinking\": true,\n                  \"clear_thinking\": false\n                }\n              }\n            }",
          "score": 2,
          "created_utc": "2025-12-25 16:00:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvvroy",
              "author": "cornea-drizzle-pagan",
              "text": "What about if my subscription is with z.ai? Do I need to toggle anything to get the best GLM 4.7?",
              "score": 1,
              "created_utc": "2025-12-25 16:06:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvwbv2",
                  "author": "getfitdotus",
                  "text": "Reply yes they have info on their site. I would recommend using the settings they say. Having one for thinking one for not. But your client needs to interleave the thinking for the best thinking performance",
                  "score": 1,
                  "created_utc": "2025-12-25 16:10:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwoqjf",
          "author": "Excellent-Sense7244",
          "text": "Super useful and works great so far",
          "score": 2,
          "created_utc": "2025-12-25 18:57:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx04yz",
          "author": "toothpastespiders",
          "text": "The only thing I've really used 4.7 seriously for so far is data extraction with additional research leveraging web search tools and a few tools for local database/RAG. Not really easy to objectively measure most of that against 4.6 since 4.6 was working pretty well for me there. The most objective metric is just \"did it break with 4.7\". And happily it's still working great. \n\nNow subjectively? It seems like how well it uses thinking for instruction following and working with the results to evaluate data returned from tools and format the newly generated text has improved significantly. Obviously \"thinking\" is always going to be a metaphor but it seems to be doing a better job adhering to that metaphor and weighing/revising results for me accordingly over 4.6.\n\nMy output is in pretty complex json format and I'm not seeing any issues there so far either. Though again, that was also the case with 4.6 for me. \n\nFrom what I've seen so far, and with my use, 4.7 seems to be nice iterative progress over 4.6 if nothing especially mind blowing. But with a 0.1 version bump I wasn't really expecting that either.",
          "score": 2,
          "created_utc": "2025-12-25 20:05:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0f15h",
          "author": "Sudden-Lingonberry-8",
          "text": "I have this issue where glm 4.7 just replies NOTHING, it doesn¬¥t attempt to do the task at all.",
          "score": 2,
          "created_utc": "2025-12-26 11:47:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0lf2j",
              "author": "Empty_Break_8792",
              "text": "i havetry with Open Code, working well so far",
              "score": 1,
              "created_utc": "2025-12-26 12:43:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0u8rj",
          "author": "Ok_Fortune_7894",
          "text": "I have been using it in cline with lite plan. My use case is only writing unit test 95% of the time. It working great. I'm not heavy user, I can't exhaust my plan",
          "score": 2,
          "created_utc": "2025-12-26 13:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0xm6b",
              "author": "Empty_Break_8792",
              "text": "cool",
              "score": 1,
              "created_utc": "2025-12-26 14:11:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw0ubf1",
          "author": "DistinctWay9169",
          "text": "Tried it. GLM 4.7 and MiniMax2.1. Both are good but not even close to SONNET 4.5 let alone opus 4.5.",
          "score": 2,
          "created_utc": "2025-12-26 13:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw0xk4p",
              "author": "Empty_Break_8792",
              "text": "agree",
              "score": 1,
              "created_utc": "2025-12-26 14:10:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw5j7o3",
          "author": "ocirs",
          "text": "I've been using glm 4.7 for free in opencode zen. For a small isolated app did a great job at setting up a frontend and making it look good, building the backend and the data pipelines. Downside is that it gets slower with lots of tokens so have to be smart about setting up new sessions.",
          "score": 2,
          "created_utc": "2025-12-27 06:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcai69",
          "author": "Quiet-Ad-9336",
          "text": "I‚Äôve already used GLM-4.7 with Cline to completely rebuild my entire website, with manual steps. Portfolio in Next.js and Three.js   \n  \nit‚Äôs really powerful. Most of the time, when it makes a small syntax or parenthesis error, it wants to rewrite entire files instead of just fixing the issue, which is annoying. That‚Äôs probably more related to Cline than the model itself.",
          "score": 2,
          "created_utc": "2025-12-28 09:55:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcbhkv",
              "author": "Empty_Break_8792",
              "text": "yeah try opencode",
              "score": 2,
              "created_utc": "2025-12-28 10:05:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwcjsuy",
                  "author": "Quiet-Ad-9336",
                  "text": "I‚Äôm currently running Kilo Code, and it‚Äôs pretty cool!\n\nIt can sketch out flowcharts like this, and wow, that‚Äôs impressive.\n\nhttps://preview.redd.it/hwylsetdjx9g1.png?width=1155&format=png&auto=webp&s=5eadd1a57f532c422aee900d4620d7ee16dfbd20",
                  "score": 2,
                  "created_utc": "2025-12-28 11:24:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwn3aig",
          "author": "Express_Contact_4583",
          "text": "https://preview.redd.it/b6jx862sh8ag1.png?width=924&format=png&auto=webp&s=e605314f8673dda4256adcf870744db7671663a5\n\nSolved!",
          "score": 2,
          "created_utc": "2025-12-30 00:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwn3dvy",
              "author": "Empty_Break_8792",
              "text": " Nicee",
              "score": 1,
              "created_utc": "2025-12-30 00:15:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvodk2",
          "author": "arousedsquirel",
          "text": "Glm 4.7 with its stringent,  and I mean, very stringent guard rails is a missed opportunity.  That's for sure. Keep up the rlhf guys at zai following ccp directives, and you miss the boat. It's such a shame for zai.",
          "score": 3,
          "created_utc": "2025-12-25 15:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvlm19",
          "author": "Time_Reaper",
          "text": "I have been using a 5bpw quant for the past few days, and so far I have been really liking it.\n\n\n¬†Although I have mainly been using it for rp and creative writing, it's a massive step up in those areas.\n\n\nIts important not to use reasoning for those tasks as it worsens the response quality.¬†¬†\n\n\nFor me it easily beats 4.6 and I like it better for writing than kimi k2.¬†\n\n\nWorld knowledge and coding is also some of the strongest amongst open source models right now, or at least close to. Kimi k2 think has somewhat better world knowledge but not by too much and in general feels less intelligent in my opinion. I didn't like any of the deepseeks after r1 0528 other than maybe terminus so, yeah.¬†\n\n\nI can't comment in regards to opus or sonnet as I don't use api only models.",
          "score": 3,
          "created_utc": "2025-12-25 15:03:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvljk5",
          "author": "tarruda",
          "text": "I've tried both in https://chat.z.ai and locally with llama.cpp + UD-IQ2_M quant. I'm impressed by this unsloth dynamic quant as it seems to give similar results to what I get in chat.z.ai. \n\nI noticed is that it seems amazing for web development. I've tried some of the prompts used in these videos:\n\n- https://www.youtube.com/watch?v=KaWQ2Ua9CW8 \n- https://www.youtube.com/watch?v=QnSbauHZDGE\n\nAnd they did work well.\n\nHowever, I've also threw at it simpler prompts for simple python games (such as tetris clones, built with pygame and curses) and it always seems to have trouble. Sometimes syntax is wrong, sometimes it uses undeclared variables and sometimes just buggy code. And these are prompts that even models such as GPT-OSS 20b or Qwen 3 coder 30b usually get right without issues.\n\nNot sure how to interpret these results.",
          "score": 3,
          "created_utc": "2025-12-25 15:02:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvplfq",
              "author": "Zc5Gwu",
              "text": "If you were using the 2bit quant at the time, that might explain it.",
              "score": 1,
              "created_utc": "2025-12-25 15:28:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvx3dk0",
                  "author": "tarruda",
                  "text": "My evaluation is based on the chat.z.ai results¬†",
                  "score": 1,
                  "created_utc": "2025-12-25 20:25:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvioo4",
          "author": "jacek2023",
          "text": "You will be downvoted :) they only want to hype the benchmarks",
          "score": 3,
          "created_utc": "2025-12-25 14:43:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvsncu",
          "author": "Unlucky-Message8866",
          "text": "Tried the other day, it was free to use on opencode and I was not impressed. On par with others.",
          "score": 2,
          "created_utc": "2025-12-25 15:47:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvw4azn",
          "author": "raydou",
          "text": "Tested using [Z.AI](http://Z.AI) coding plan for a side project (not the principal project working on) in Claude Code to not use my Anthropic quotas. And it did fantastic, i was really impressed in comparaison with GLM-4.6. \n\nDoes it compare to Claude Opus 4.5 ? of course no. With Sonnet 4.5 ? it could compare with but needs direction and should always start by planning or brainstorming session with it and giving it well defined tasks to have impressive results. \n\nWhat it lacks in comparaison of Anthropic models is this kind of understanding and deduction of what to do when not well directed or lacking context.",
          "score": 2,
          "created_utc": "2025-12-25 16:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvz0zb1",
          "author": "I-am_Sleepy",
          "text": "It‚Äôs is underwhelming to say the least. I am using it through claude code, but it often gets stuck in a loop of trying to fix weird errors, and continuously failed to do so. Either I have to comb through and fix it myself, or I switch to actual Claude model, which is much much better at resolving conflicts\n\nBut not that it is way worse than anything else, as Gemini (Antigravity) still exhibits those behaviors too albeit at much less frequency. But I‚Äôve seen from Theo yt channel that Minimax M2.1 is at much better shape than GLM 4.7",
          "score": 2,
          "created_utc": "2025-12-26 03:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvlre0",
          "author": "JLeonsarmiento",
          "text": "Yes. It works as expected.",
          "score": 1,
          "created_utc": "2025-12-25 15:04:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvx13ly",
          "author": "pasdedeux11",
          "text": "complex typescript code; refactoring react code; complex web dev. fucking lol",
          "score": 1,
          "created_utc": "2025-12-25 20:11:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyul8k",
          "author": "jeffwadsworth",
          "text": "This is a good testing video.  [https://www.youtube.com/watch?v=0SZ6mVWTxQA](https://www.youtube.com/watch?v=0SZ6mVWTxQA)",
          "score": 1,
          "created_utc": "2025-12-26 03:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzb8bc",
          "author": "Z_daybrker426",
          "text": "I use it for coding and like glm 4.6 it works really well. I find that the best way is to paste the code snippet you are working and the code block like the function. Then tell it what you are trying to do. Have thinking and the single research not the multi turn I find that stuff isn‚Äôt that great.",
          "score": 1,
          "created_utc": "2025-12-26 05:18:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzutl0",
          "author": "Shark1753",
          "text": "Lots of buzzwords and headlines etc. not as good as people say. Though it‚Äôs still really good.",
          "score": 1,
          "created_utc": "2025-12-26 08:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw6rrwf",
          "author": "Astrianz",
          "text": "Not as good as MiniMax M2.1 but it‚Äôs okay for backend tasks. It‚Äôs a hit and miss so far. As far as I have been using. I am using them both. MM M2.1 for frontend and when I‚Äôm stuck in loop, GLM 4.7. It‚Äôs not as good. But I will try with tweaks and hopefully update here. üí´.",
          "score": 1,
          "created_utc": "2025-12-27 13:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdzfin",
          "author": "leooga",
          "text": "I‚Äôll explain the way I‚Äôve been working, and then I‚Äôll share my results.\n\nI use Spec-Driven Development and Test-Driven Development, always with two sessions: an orchestrator session and a worker session.\n\nThe orchestrator is responsible for understanding what needs to be done and for developing the spec. The worker does the rest. At the end, the worker must produce a complete report. I copy and paste that report into the orchestrator, which evaluates whether what was done actually matches what was requested, assesses code quality, and assigns a score.\n\nI always use GPT-5.2 xhigh as the orchestrator. This way, I can maintain consistency. The worker session is where I test all possible models.\n\nGLM-4.7 only created chaos. It can hardly complete anything decently. I was extremely (negatively) surprised by the benchmark. It really makes me question its usefulness. The score it receives is usually around 4 or 5.\n\nMinimax, since version m2, gets a better score and is capable of completing tasks reasonably well.\n\nGemini Flash is at the same level as Minimax.\n\nGPT-5.1 mini (!!!!!!) consistently gets very good scores and can complete tasks decently while being inexpensive. The problem is that it doesn‚Äôt go very far and stops frequently during the task. Still, what it does, it does correctly.\n\nCurrently, I‚Äôm using GPT-5.2 high, and it works exceptionally well. The code is usually delivered with all tests and everything working, even when it has to work continuously for 2 or 3 hours.\n\nWhen the workload is high, I use the mini. When it‚Äôs not, I use high or medium.\n\nIf money were tight, I would use GPT-5.2 high as the orchestrator and Minimax as the worker (or the Gemini free tier).\n\nWhen do I find GLM-4.7, Gemini 3.0 Flash, etc., worthwhile? When creativity is required and there isn‚Äôt a fixed, concrete target to be met. These models have a lot of difficulty strictly following what was requested. They usually create a lot of things you don‚Äôt want and forget many fundamental requirements. That may be why the tests we see on YouTube look so good‚Äîthey work well for one-shot tasks. My experience was exactly the same as Theo‚Äôs (T3 Chat). He made a video putting the models to work on a real problem. You can expect exactly the same experience.",
          "score": 1,
          "created_utc": "2025-12-28 16:52:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwe1ouv",
              "author": "FullstackSensei",
              "text": "Bom coment√°rio, mas escreve em ingl√™s p√°! A malta aqui fala ingl√™s, √© devemos todos respeitar isto!",
              "score": 1,
              "created_utc": "2025-12-28 17:03:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwe2l5d",
                  "author": "leooga",
                  "text": "Done! :) Thanks.",
                  "score": 2,
                  "created_utc": "2025-12-28 17:08:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnsf9g",
          "author": "horizondz",
          "text": "I tried it, and it‚Äôs nowhere near Sonnet 3.7, let alone 4.5.",
          "score": 1,
          "created_utc": "2025-12-30 02:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvjo0k",
          "author": "SlowFail2433",
          "text": "I mean the benches are always in python and I do c++ and rust etc so there is drift there",
          "score": 1,
          "created_utc": "2025-12-25 14:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvjqgb",
              "author": "SlowFail2433",
              "text": "Ye opus 4.5 perf not that realistic",
              "score": 2,
              "created_utc": "2025-12-25 14:50:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvovc2",
              "author": "randombsname1",
              "text": "I use C and Assembly for embedded STM32 projects and Opus 4.5 is the only one able to run the complex workflows to actually work with these projects. \n\nEven ChatGPT 5.2 doesn't get close because it can't string workflows nearly as well.",
              "score": 2,
              "created_utc": "2025-12-25 15:24:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvvnwbx",
              "author": "BriguePalhaco",
              "text": "I use OpenRouter ranking as a \"benchmark\".",
              "score": 1,
              "created_utc": "2025-12-25 15:17:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvvygmu",
          "author": "mintybadgerme",
          "text": "Yeah, not that great to be honest.",
          "score": 1,
          "created_utc": "2025-12-25 16:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvwiiwp",
          "author": "zuk987",
          "text": "It's the cheapest subscription based model and it feels miles ahead of glm 4.6, but very slow for some reason. Decent model for the discounted price.",
          "score": 1,
          "created_utc": "2025-12-25 18:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwmkdf",
              "author": "Empty_Break_8792",
              "text": "Agree",
              "score": 1,
              "created_utc": "2025-12-25 18:45:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwev5f",
          "author": "wrecklord0",
          "text": "I've tried it, and from my experience, it is by far the the most coherent, intelligent local model out there (and by local I mean: doesn't require workstation hardware). I don't think it's in the same league as the frontier, closed models like gemini or opus, but I am much more impressed with GLM 4.7 than GPT 120b or Mistral models etc.\n\nBig disclaimer: I have not used it as a coding asssistant yet, only as a general purpose model (its next on my todo)\n\nNote: the smallest 3 bit quants (such as IQ3 XXS) fit in 32GB Vram + 128GB RAM, which makes it possible to run on a perfectly 'standard' (albeit expensive) consumer PC, and that's very neat. Not much room for anything else, that leaves only a few GB of vram and ram for other uses.",
          "score": 0,
          "created_utc": "2025-12-25 18:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx5280",
              "author": "wrecklord0",
              "text": "Replying to myself for an update on coding: Holy shitballs, it works. Now I've not done extensive testing yet because I only get 3.5 tok/s running this locally, but I've asked it to refactor a small powershell app into python (a somewhat simple 200 LoC GUI that acts as a launcher with an external json configuration) and it worked, first try, no extra guidance or fixing required. It replaced Forms / System.Drawing with Tkinter, made the thing OS agnostic and more pythony. And that's on the 3 bit quant which I find impressive, considering the significant degradation that must cause.",
              "score": 1,
              "created_utc": "2025-12-25 20:35:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx6ozm",
          "author": "eli_pizza",
          "text": "For $3 you can try it yourself for a whole month from z.ai",
          "score": 0,
          "created_utc": "2025-12-25 20:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvmlkv",
          "author": "BriguePalhaco",
          "text": "I found it better than the Gemini 2.5/3 and GPT 5, but it's far from the Minimax M2, DeepSeek 3/3.2, and Sonnet/Opus 4.5 (in order of worst to best).\n\nFor my work with Rust and C#, GLM 4.6 generated a lot of junk code, but it had some cool ideas. I haven't thoroughly tested GLM 4.7 yet, I subscribe to the coding plan, but I only use it for creating commits in Git.\n\nI'm thinking of using GLM for autocomplete, but I haven't found a decent plugin for JetBrains IDE yet.",
          "score": -1,
          "created_utc": "2025-12-25 15:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvrcvc",
              "author": "woahdudee2a",
              "text": "so according to your ranking gemini 3 is worse than deepseek 3 ?",
              "score": 4,
              "created_utc": "2025-12-25 15:39:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvvjvg",
                  "author": "BriguePalhaco",
                  "text": "Yes, I usually run LLMs in parallel to fix bugs in my software (PostgreSQL extensions in Rust and .NET systems). Gemini 3 is the black sheep, very dumb. It reads the entire codebase and then stops, refusing to continue. Similar to DeepInfra's quantified Qwen3 Coder.\n\nIt's good for front-end, though. At least Gemini 2.5 was; I haven't tested G3 on front-end yet (only back-end).",
                  "score": -1,
                  "created_utc": "2025-12-25 16:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvvwiuy",
              "author": "iamrick_ghosh",
              "text": "Bro why do you have to ask a llm to commit code on GitHub?\nI have also asked my colleagues too to stop doing it and actually learn git,its very simple and at times really fun‚Ä¶",
              "score": 2,
              "created_utc": "2025-12-25 16:11:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvybxp",
                  "author": "BriguePalhaco",
                  "text": "Nahhh, I'm fine. I prefer to spend my time programming.\n\nMy agents.md is organized. I recommend that your friends follow https://www.conventionalcommits.org/. hahahaha",
                  "score": 0,
                  "created_utc": "2025-12-25 16:22:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzf38j",
          "author": "FBIFreezeNow",
          "text": "the reason you don't see a lot of posts about it is that it's not that impressive.  From the limited testing, it was slower than 4.6, but not that stronger, or even stronger.  Didn't notice any improvements.  This got me thinking that people feeling the improvements is such an awesome thing that the Claude model providers consistently deliver and it's very difficult as fuk",
          "score": 0,
          "created_utc": "2025-12-26 05:51:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0l7c3",
          "author": "Massive_Goat744",
          "text": "Atualmente, utilizo o GLM-4.7 como minha principal IA. Ela √© muito eficiente, embora ainda n√£o alcance o n√≠vel do Claude Code 4.5. Estou no plano de 8 d√≥lares por 3 meses oferecido no Natal; o √∫nico ponto negativo √© a velocidade, que fica em torno de 40 tokens por segundo. O plano de 15 d√≥lares promete ser at√© 60% mais r√°pido, sendo mais indicado para quem usa IA intensivamente.\n\nTenho explorado o c√≥digo-fonte do GTA V, e o GLM-4.7 consegue resolver tarefas complexas que h√° um ano nenhuma IA conseguia. Hoje, com essas IAs avan√ßadas, acredito que o fator mais importante para um agente n√£o √© apenas a IA em si, mas o ambiente em que ela est√° inserida. Por isso utilizo o Cursor, que √© totalmente dedicado a esse prop√≥sito. Ele conta com um sistema de indexa√ß√£o de projetos por embeddings, capaz de localizar arquivos complexos em projetos com mais de 50 mil arquivos, cada um com 2 a 5 mil linhas, por meio de buscas via embedding.\n\nO sistema √© r√°pido e preciso, enquanto o Claude Code (CLI) demoraria mais do que o dobro do tempo e muitas vezes nem encontraria os arquivos, pois utiliza apenas ripgrep. No Cursor, o prompt enviado √† IA √© muito bem estruturado, sendo o √∫nico em que a IA realmente segue as instru√ß√µes corretamente. Al√©m disso, os modos Debug e Plan do Cursor s√£o os melhores entre as ferramentas dispon√≠veis.",
          "score": 0,
          "created_utc": "2025-12-26 12:42:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvxmqt",
      "title": "Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/",
      "author": "KvAk_AKPlaysYT",
      "created_utc": "2025-12-26 06:32:16",
      "score": 94,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nYes, it's finally happening! I recently pushed some changes and have gotten Kimi-Linear to work (fully; fingers crossed) PR (#18381). \n\nI've tested it heavily on Q2\\_K (mind BLOWING coherence :), and it‚Äôs now passing logic puzzles, long-context essay generation, and basic math - all of which were previously broken.\n\n\n\n[q2\\_k](https://preview.redd.it/mjychgkcth9g1.png?width=555&format=png&auto=webp&s=f02c3fda1ea59629b4aac6664cc7c4a071f7ebd1)\n\nResources:\n\nPR Branch: [github.com/ggml-org/llama.cpp/pull/18381](http://github.com/ggml-org/llama.cpp/pull/18381)\n\nGGUFs (Use above PR): [huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF](https://huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF)\n\nUse this free Colab notebook or copy the code from it for a quick start :) [https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing](https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing)  \n  \nPlease give it a spin and let me know if you run into any divergent logits or loops!\n\n  \nI am currently looking for open positions! ü§ó\n\nIf you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: [Aaryan Kapoor](https://www.linkedin.com/in/theaaryankapoor/)",
      "is_original_content": false,
      "link_flair_text": "Resources",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvzmzgz",
          "author": "pmttyji",
          "text": "Thanks for this work! Could you please add few other info on this thread? Your model page has both Q2 & Q4 quants. What speed(both pp & tg t/s) are you getting for both quants? with your VRAM you tried. It would be nice to see a those details. Please share once you get chance.\n\n(Qwen3-Next-IQ4\\_XS gave me 10 t/s with my 8GB VRAM + 32GB RAM. Really curious to know what Kimi-Linear would give me)",
          "score": 10,
          "created_utc": "2025-12-26 07:01:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvznhbd",
              "author": "Amazing_Athlete_2265",
              "text": "I'm testing this out shortly once model is downloaded. Will report back.",
              "score": 6,
              "created_utc": "2025-12-26 07:06:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzp3jy",
                  "author": "pmttyji",
                  "text": "Awesome",
                  "score": 2,
                  "created_utc": "2025-12-26 07:21:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzqxce",
              "author": "Amazing_Athlete_2265",
              "text": "First run and I forgot to turn on flash attention. Test took awhile so future tests will use reduced context sizes:\n\n    ‚ùØ ./llama-bench -m /home/xxx/models/Kimi-Linear-48B-A3B-Instruct-GGUF/Kimi-Linear-48B-A3B-Instruct.q2_k.gguf -ngl 11 -p 4096 -n 4096\n    Device 0: NVIDIA GeForce RTX 3080, compute capability 8.6, VMM: yes\n| model                          |       size |     params | backend    | ngl |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |          pp4096 |        287.96 ¬± 3.77 |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |          tg4096 |         18.94 ¬± 0.40 |",
              "score": 4,
              "created_utc": "2025-12-26 07:40:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzr6mh",
                  "author": "Amazing_Athlete_2265",
                  "text": "FA makes little difference:\n\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |  1 |           pp512 |        259.54 ¬± 2.05 |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |  1 |           tg128 |         21.88 ¬± 0.08 |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |  0 |           pp512 |        304.86 ¬± 1.78 |\n| kimi-linear ?B Q2_K - Medium   |  16.78 GiB |    49.12 B | CUDA       |  11 |  0 |           tg128 |         22.22 ¬± 0.03 |",
                  "score": 4,
                  "created_utc": "2025-12-26 07:42:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvzxtng",
                  "author": "pmttyji",
                  "text": "Thanks for this. Looks like optimizations needed after merging initial PR.\n\nQwen3-30B-A3B's Q4 (16-17GB size), KVCache(Q8) gave me 30+ t/s(For 32K context, got 20 t/s).",
                  "score": 4,
                  "created_utc": "2025-12-26 08:51:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzog07",
          "author": "Enturbulated_One",
          "text": "Haven't been watching new model releases for the last few months and missed this one.  Nice to find out about it via llama.cpp pull though, so thanks for your efforts!\n\nDid you have suggested inferencing settings for it under llama.cpp?",
          "score": 3,
          "created_utc": "2025-12-26 07:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzp9ow",
              "author": "pmttyji",
              "text": "His model page has those details",
              "score": 3,
              "created_utc": "2025-12-26 07:23:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzq67o",
                  "author": "Enturbulated_One",
                  "text": "\\*blink\\*  Yup, I was completely blind.  Thanks!",
                  "score": 2,
                  "created_utc": "2025-12-26 07:32:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw27rsw",
          "author": "Innomen",
          "text": "Well i got it running CPU only via your modified llama and openwebui: It's extremely coherent and pretty fast once it starts outputting:  \n/home/innomen/AI/LLM/Models/Kimi-Linear-48B-A3B-Instruct.q2\\_k.gguf\n\nToday at 12:23 PM\n\nThanks for the clarification! Based on the context provided, **E**xperiential **E**mpiricism (EE) appears to be a philosophical framework that attempts to ground empirical knowledge and ethics in two self-validating axioms:\n\n1. **Logic is self-validating** ‚Äì the principle that logical reasoning works through its own application\n2. **Valenced experience occurs** ‚Äì the idea that valence (the felt sense of suffering and flourishing) is an intrinsic structural property of experience\n\nFrom these two axioms, EE claims to solve several classic philosophical problems (like the hard problem of consciousness, the is-ought gap, and quantum measurement paradoxes) not by solving them directly, but by revealing them as artifacts of unwarranted assumptions Experiential Empiricism: The Valenced Axiom at the Root of All Meaning.md.\n\nIf you're developing this framework further, would you like feedback on:\n\n* Its epistemological foundations?\n* The way it handles consciousness and ethics?\n* Its implications for specific philosophical problems?\n\nLet me know which direction you'd like to explore, and I can help evaluate it more precisely.\n\n1\n\nExperiential Empiricism: The Valenced Axiom at the Root of All [Meaning.md](http://Meaning.md)\n\n* Prompt tokens: 223\n* Output tokens: 211\n* Total tokens: 434\n* Inference speed: 4.29 tokens/sec\n* Prompt prep speed: 23.94 tokens/sec\n* Total end-to-end: \\~58.5 seconds\n\n[https://philpapers.org/rec/SEREET-2](https://philpapers.org/rec/SEREET-2) (if anyone cares)",
          "score": 2,
          "created_utc": "2025-12-26 18:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw28r99",
              "author": "KvAk_AKPlaysYT",
              "text": "Love it! Just curious what's your CPU setup?",
              "score": 3,
              "created_utc": "2025-12-26 18:31:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw3ejzq",
                  "author": "Innomen",
                  "text": "https://preview.redd.it/nlcnn931im9g1.png?width=557&format=png&auto=webp&s=2e709556bf4b181d09143939d51595119ca87c1a",
                  "score": 2,
                  "created_utc": "2025-12-26 22:16:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1bc35",
          "author": "Quagmirable",
          "text": "Nice work! I've been waiting for a GGUF of cerebras/Kimi-Linear-REAP-35B-A3B-Instruct too, I imagine it comes with the same challenges for getting it to work?",
          "score": 1,
          "created_utc": "2025-12-26 15:33:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1maqt",
          "author": "q5sys",
          "text": "So there's a q2, q4, and fp16 but no q8?",
          "score": 1,
          "created_utc": "2025-12-26 16:33:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2jhdi",
              "author": "KvAk_AKPlaysYT",
              "text": "Hey, I've just uploaded a fixed Q8. Let me know how it is and what your Token/s are!",
              "score": 6,
              "created_utc": "2025-12-26 19:27:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw1mnh4",
              "author": "KvAk_AKPlaysYT",
              "text": "Q8 had some errors during quantization, still needs figuring out :)",
              "score": 2,
              "created_utc": "2025-12-26 16:35:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw1o8ot",
                  "author": "q5sys",
                  "text": "ah ok. Seemed odd to have the natural progression with a hole.  2, 4, ?, 16. haha  \nHopefully you'll be able to figure out the issue and push out a q8. I've been looking for a good kimi model to run on a a cloud RTX 6000.",
                  "score": 1,
                  "created_utc": "2025-12-26 16:43:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1t663",
          "author": "qwen_next_gguf_when",
          "text": "thank you for your continuous work, bro. i found an issue : it cant handle chinese characters. simple one like \"‰Ω†Â•ΩÈòø\" is not recognized correctly. issue 2: it doesnt work properly with any prompt longer than 3k, it slows down to half the speed.",
          "score": 1,
          "created_utc": "2025-12-26 17:09:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw8h6w",
      "title": "GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/",
      "author": "uptonking",
      "created_utc": "2025-12-26 16:35:28",
      "score": 94,
      "num_comments": 28,
      "upvote_ratio": 0.93,
      "text": "i find the benchmark result from twitter, which is very interesting.\n\n>Hardware: Apple M3 Ultra, 512GB. All tests with single M3 Ultra **without batch inference**.\n\n[glm-4.7](https://preview.redd.it/zwqsxk9btk9g1.png?width=4052&format=png&auto=webp&s=1940693109fab3938946786fb719ad07bd73345c)\n\n[minimax-m2.1](https://preview.redd.it/0nkcz4fetk9g1.png?width=4052&format=png&auto=webp&s=48a2d1eba5e5dd4ce8ecce705b01468c4931c47c)\n\n* GLM-4.7-6bit MLX Benchmark Results with different context sizes\n\n0.5k Prompt: 98 - Gen: 16 t/s - 287.6GB  \n1k Prompt: 140 - Gen: 17 t/s - 288.0GB  \n2k Prompt: 206 - Gen: 16 t/s - 288.8GB  \n4k Prompt: 219 - Gen: 16 t/s - 289.6GB  \n8k Prompt: 210 - Gen: 14 t/s - 291.0GB  \n16k Prompt: 185 - Gen: 12 t/s - 293.9GB  \n32k Prompt: 134 - Gen: 10 t/s - 299.8GB  \n64k Prompt: 87 - Gen: 6 t/s - 312.1GB\n\n* MiniMax-M2.1-6bit MLX Benchmark raw results with different context sizes\n\n0.5k Prompt: 239 - Gen: 42 t/s - 186.5GB  \n1k Prompt: 366 - Gen: 41 t/s - 186.8GB  \n2k Prompt: 517 - Gen: 40 t/s - 187.2GB  \n4k Prompt: 589 - Gen: 38 t/s - 187.8GB  \n8k Prompt: 607 - Gen: 35 t/s - 188.8GB  \n16k Prompt: 549 - Gen: 30 t/s - 190.9GB  \n32k Prompt: 429 - Gen: 21 t/s - 195.1GB  \n64k Prompt: 291 - Gen: 12 t/s - 203.4GB\n\n* I would prefer minimax-m2.1 for general usage from the benchmark result, about **\\~2.5x** prompt  processing speed, **\\~2x** token generation speed\n\n>sources:  [glm-4.7](https://x.com/ivanfioravanti/status/2004578941408039051) ,  [minimax-m2.1](https://x.com/ivanfioravanti/status/2004569464407474555), [4bit-comparison](https://x.com/ivanfioravanti/status/2004602428122169650)\n\n\n\n[4bit-6bit-comparison](https://preview.redd.it/p7kp5hcv1l9g1.jpg?width=1841&format=pjpg&auto=webp&s=c66839601a68efa3baf6c845bce91e8c2c8c2254)\n\n\\- It seems that 4bit and 6bit have similar speed for prompt processing and token generation.  \n\\- for the same model, 6bit's memory usage is about **\\~1.4x** of 4bit. since RAM/VRAM is so expensive now, maybe it's not worth it (128GB x 1.4 = 179.2GB)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw377fy",
          "author": "slavik-dev",
          "text": "One more data point:\n\nRunning Minimax M2 UD-Q4\\_K\\_XL (131GB) on  72GB Nvidia VRAM + DDR5-4800 8ch RAM.\n\nWith \\~2k context, I'm getting:\n\n\\- PP: 67 t/s\n\n\\- TG: 21 t/s",
          "score": 13,
          "created_utc": "2025-12-26 21:36:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwayqpr",
              "author": "Imaginary_Author8773",
              "text": "Damn that DDR5 crossover is brutal compared to unified memory, minimax still looking solid though. What's your actual VRAM split on that setup? Curious if you're hitting the PCIe bottleneck hard when it starts swapping to system RAM",
              "score": 2,
              "created_utc": "2025-12-28 03:24:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwb5x3o",
                  "author": "slavik-dev",
                  "text": "I'm using llama.cpp.\n\n\nWith llama.cpp PCIe speed doesn't matter.\nThere is no swapping between RAM and VRAM.\n\n\nVRAM: ~60GB of model layers + 12GB context\n\nRAM: 70GB model layers¬†",
                  "score": 2,
                  "created_utc": "2025-12-28 04:09:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw1tdp9",
          "author": "twack3r",
          "text": "I get it, those Macs are fast af for how little they cost but Jesus, that speed is glacial‚Ä¶",
          "score": 9,
          "created_utc": "2025-12-26 17:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw4027j",
              "author": "crantob",
              "text": "compared to...?",
              "score": 3,
              "created_utc": "2025-12-27 00:23:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw436i8",
                  "author": "Dany0",
                  "text": "GB300, space shuttle...",
                  "score": 3,
                  "created_utc": "2025-12-27 00:42:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw25w7z",
          "author": "Final_Wheel_7486",
          "text": "This is an extremely high-effort post, damn, the charts and all... very cool! Thank you :)",
          "score": 9,
          "created_utc": "2025-12-26 18:16:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2ib7b",
          "author": "cantgetthistowork",
          "text": "Speed is meaningless if you need more roundtrips to get the task done",
          "score": 6,
          "created_utc": "2025-12-26 19:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw1ntzb",
          "author": "ArtisticHamster",
          "text": "Could we expect M5 to be much faster?",
          "score": 5,
          "created_utc": "2025-12-26 16:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1s42w",
              "author": "Agreeable-Rest9162",
              "text": "It would be faster for token generation. In general, higher memory bandwidth yields higher token-generation speeds. The M3 has 100GB/s of unified memory bandwidth; the M5 has approximately 150GB/s.  The M3 Ultra has 819 GB/s, so if we apply the same improvement, we could see 1.2 TB/s of bandwidth with the M5 Ultra. The current M4 Max, if doubled, yields a similar number, so the M5 Ultra must be at least twice as powerful as two M4 Maxes combined. \n\nRegarding time to first token (TTFT) or token processing speed, we can expect a much greater speedup, given that the neural accelerators in the GPU cores of the base M5 are present on the M5 Ultra as well, whenever it is produced.",
              "score": 12,
              "created_utc": "2025-12-26 17:03:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw3n3lr",
                  "author": "Evening_Ad6637",
                  "text": "I come to the same conclusion regarding memory bandwidth.\n\n\n- The M4 had LPDDR5X-7500\n- M4 Pro and Max came with LPDDR5X-8533\n\n\n- The M5 has LPDDR5X-8533\n-> My assumption is therefore that M5 Pro, Max, and Ultra will have LPDDR5X-9600, resulting in 1233 GB/s bandwidth; i.e., also 1.2 TB/s.",
                  "score": 6,
                  "created_utc": "2025-12-26 23:05:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw3nxdl",
              "author": "Final-Rush759",
              "text": "It will be much faster for prompt/context processing as Apple will add matrix-multiplication processing unit.  Token generation should also be faster as Apple is likely to increase memory bandwidth.",
              "score": 3,
              "created_utc": "2025-12-26 23:10:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw1p8se",
              "author": "uptonking",
              "text": "- for a near SOTA model like minimax m2.1 230B A10B, 42 token/s for short prompts is good enough for me. \n- when M5 Ultra is released, i hope to get a good price for M3 ultra 256gb. now M3 ultra is too expensive for me",
              "score": 1,
              "created_utc": "2025-12-26 16:48:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw21am7",
                  "author": "EmergencyLetter135",
                  "text": "Shouldn't an M4 Ultra be released first?",
                  "score": 2,
                  "created_utc": "2025-12-26 17:52:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nw43qw0",
              "author": "Dany0",
              "text": "M5 Ultra is going to be a beast. Just the M5 Max is expected to be the fastest non-HEDT/server/Epyc/Xeon/Threadripper cpu available. Rumours are they were testing the M5 Ultra to no longer be a SoC but have a separate die for just the gpu, though bonded/close to the cpu so still unified memory",
              "score": 1,
              "created_utc": "2025-12-27 00:45:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw38kmo",
          "author": "Finn55",
          "text": "This post feels like it‚Äôs for me! M3 Ultra due in a few days and I‚Äôm aiming for Minimax 2.1 as the model for daily coding activities",
          "score": 3,
          "created_utc": "2025-12-26 21:44:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2nosv",
          "author": "ZhopaRazzi",
          "text": "GLM 4.7 seems undercooked. Slower, bigger, worse.",
          "score": 2,
          "created_utc": "2025-12-26 19:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2p631",
              "author": "uptonking",
              "text": "- this benchmark is mostly about speed and memory comparison. there is no info about the result quality.\n- but for my personal api usage experience, minimax and glm are both good enough for general chatting",
              "score": 11,
              "created_utc": "2025-12-26 19:58:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2awin",
          "author": "DrummerPrevious",
          "text": "Wtff minimax is crazy",
          "score": 1,
          "created_utc": "2025-12-26 18:42:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5xjnl",
          "author": "Karyo_Ten",
          "text": "Can you bench MiMo-V2-Flash?\n\nIt has a very interesting attention architecture similar to GPT OSS and should be flying.\n\nhttps://huggingface.co/XiaomiMiMo/MiMo-V2-Flash",
          "score": 1,
          "created_utc": "2025-12-27 09:02:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5xrho",
              "author": "uptonking",
              "text": "i would if i had a M3 Ultra üòã",
              "score": 1,
              "created_utc": "2025-12-27 09:04:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw5xv7s",
                  "author": "Karyo_Ten",
                  "text": "Ah, misread!",
                  "score": 1,
                  "created_utc": "2025-12-27 09:05:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw7utl7",
          "author": "xXprayerwarrior69Xx",
          "text": "Doing the lords work thanks for this",
          "score": 1,
          "created_utc": "2025-12-27 17:09:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyrn9v",
      "title": "I Finished a Fully Local Agentic RAG Tutorial",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/",
      "author": "CapitalShake3085",
      "created_utc": "2025-12-29 17:03:44",
      "score": 92,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "Hi,\nI‚Äôve just finished a **complete Agentic RAG tutorial + repository** that shows how to build a fully local, end-to-end system.\n\nNo APIs, no cloud, no hidden costs.\n\n---\n\n### üí° What‚Äôs inside\n\nThe tutorial covers the full pipeline, including the parts most examples skip:\n\n- PDF ‚Üí Markdown ingestion  \n- Hierarchical chunking (parent / child)  \n- Hybrid retrieval (dense + sparse)  \n- Vector store with **Qdrant**  \n- Query rewriting + **human-in-the-loop**  \n- Context summarization  \n- **Multi-agent map-reduce** with **LangGraph**  \n- Local inference with **Ollama**  \n- Simple **Gradio** UI\n\n---\n\n### üéØ Who it‚Äôs for\n\nIf you want to **understand Agentic RAG by building it**, not just reading theory, this might help.\n\n---\n\n### üîó Repo\n\n https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial | Guide",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nwmms9r",
          "author": "OnyxProyectoUno",
          "text": "Parent/child relationships usually handle context preservation better than fixed-size chunks, especially for longer documents.\n\nOne thing that often gets overlooked in these pipelines is visibility into what the PDF parsing actually produces before it hits the chunking layer. Tables and complex layouts can get mangled during PDF extraction, and you won't know until you're debugging weird retrieval results later. Worth spot-checking a few processed documents to make sure the markdown conversion isn't losing critical structure.\n\nThe human-in-the-loop for query rewriting is smart. Most people automate everything and then wonder why their system hallucinates on edge cases.\n\nHow are you handling document metadata propagation through the parent/child hierarchy? That's usually where things get tricky with multi-level chunking.",
          "score": 6,
          "created_utc": "2025-12-29 22:45:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmre1n",
              "author": "CapitalShake3085",
              "text": "Hi, thank you for your kind words :D\n\nAbout the metada:  \nEach parent chunk gets a unique `parent_id` that's inherited by all its children during splitting. Parents are stored as JSON files with full metadata intact, so when you retrieve a child chunk, you just use its `parent_id` to fetch the full parent context. Keeps it simple‚Äîno nested hierarchy complexity",
              "score": 2,
              "created_utc": "2025-12-29 23:09:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnmqys",
          "author": "scottgal2",
          "text": "Awesome! Inspired me to get my .net based rag stuff working with a nice Gradio style UI like this! Will update when complete (it'll have a GraphRAG too...). Lovely tutorial too!",
          "score": 2,
          "created_utc": "2025-12-30 02:02:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp06uv",
              "author": "CapitalShake3085",
              "text": "Thank you üôè",
              "score": 1,
              "created_utc": "2025-12-30 07:30:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkqfet",
          "author": "braydon125",
          "text": "Thanks dude! I'm in the middle of getting my local cluster online and RAG is definitely on my list and this sounds like a great place to start!",
          "score": 1,
          "created_utc": "2025-12-29 17:16:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkrbv3",
              "author": "CapitalShake3085",
              "text": "Thank you for your kind words üôè",
              "score": 2,
              "created_utc": "2025-12-29 17:20:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkwi8x",
          "author": "Kregano_XCOMmodder",
          "text": "Looks really cool and I'm looking forward to trying it out, but I would suggest adding \\`langchain-localai\\` as an option under LLM Provide Configuration, because plenty of people have OpenAI API based local servers.",
          "score": 1,
          "created_utc": "2025-12-29 17:45:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkx1db",
              "author": "CapitalShake3085",
              "text": "Thank you i will add it :)",
              "score": 1,
              "created_utc": "2025-12-29 17:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoxt1j",
          "author": "Arxijos",
          "text": "cool & thanks for the contribution!",
          "score": 1,
          "created_utc": "2025-12-30 07:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp089m",
              "author": "CapitalShake3085",
              "text": "üôè",
              "score": 1,
              "created_utc": "2025-12-30 07:30:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpcnu4",
          "author": "bull_bear25",
          "text": "I loved EAG built one 6 months back\nBut now they are seen as commodities",
          "score": 1,
          "created_utc": "2025-12-30 09:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlu07k",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -1,
          "created_utc": "2025-12-29 20:22:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmgtyd",
              "author": "CapitalShake3085",
              "text": "I get that. It's definitely a trade-off between convenience and control",
              "score": 2,
              "created_utc": "2025-12-29 22:14:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvfj87",
      "title": "LFM2-2.6B-Exp is an experimental checkpoint built on LFM2-2.6B using pure reinforcement learning by Liquid AI",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/xwktkxmsad9g1.png",
      "author": "Nunki08",
      "created_utc": "2025-12-25 15:22:53",
      "score": 89,
      "num_comments": 24,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvfj87/lfm226bexp_is_an_experimental_checkpoint_built_on/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvvs5dv",
          "author": "AgeOfAlgorithms",
          "text": "impressive if true!",
          "score": 9,
          "created_utc": "2025-12-25 15:44:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvuarf",
              "author": "Zc5Gwu",
              "text": "true if impressive?",
              "score": 3,
              "created_utc": "2025-12-25 15:57:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvvxf6o",
                  "author": "Chromix_",
                  "text": "Truly impressive.",
                  "score": 1,
                  "created_utc": "2025-12-25 16:16:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvyfwhn",
          "author": "rainbyte",
          "text": "How does it compare with LFM2 8B A1B?",
          "score": 5,
          "created_utc": "2025-12-26 01:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2y4dt",
              "author": "KaroYadgar",
              "text": "Given LFM2-8B-A1B was only slightly better than LFM2-2.6B, I'd speculate that this is far better.",
              "score": 6,
              "created_utc": "2025-12-26 20:47:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw4t35g",
                  "author": "rainbyte",
                  "text": "I will take a look then.\n\nMy desktop PC with GPU is able to run bigger models, but on my laptop LFM2-8B-A1B is the best model I'm able to run, as it only has iGPU.\n\nIt seems other similar sized models have some compatibility issues with Vulkan and/or iGPU",
                  "score": 1,
                  "created_utc": "2025-12-27 03:28:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw4w8nz",
                  "author": "rainbyte",
                  "text": "Ok, I'm testing it right now and it seems to better than 8b-a1b at some tasks, but it is a bit slower. I guess it is a good tradeoff if you want more quality.",
                  "score": 1,
                  "created_utc": "2025-12-27 03:49:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzqv4a",
          "author": "Raise_Fickle",
          "text": "any there any fundamental/architectural diff between other model and LFM models?",
          "score": 3,
          "created_utc": "2025-12-26 07:39:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2xxpd",
              "author": "KaroYadgar",
              "text": "LFM has their own architecture which is made to be really fast. I think it uses some sort of hybrid attention.",
              "score": 5,
              "created_utc": "2025-12-26 20:46:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx9xs3",
          "author": "TheRealMasonMac",
          "text": "What does \"pure reinforcement learning\" mean? It just looks like a regular training recipe... SFT + DPO + RLVR.",
          "score": 2,
          "created_utc": "2025-12-25 21:06:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw2ydiq",
              "author": "KaroYadgar",
              "text": "Yeah it does, but all prior LFM models had not used any RL before. This experiment, from what I can tell, is their first public build which uses RL. They've said from now on all of their text models will use RL.",
              "score": 5,
              "created_utc": "2025-12-26 20:48:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdn2o0",
          "author": "fancellu",
          "text": "Any android apps that let us run it on device?",
          "score": 1,
          "created_utc": "2025-12-28 15:50:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl79kk",
              "author": "rainbyte",
              "text": "I'm using ChatterUI and it works well. I think it is based on llama.cpp.\n\nAnother option could be using llama.cpp directly on Termux.",
              "score": 2,
              "created_utc": "2025-12-29 18:34:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpe0y5",
                  "author": "fancellu",
                  "text": "Couldn't find ChatterUI on play store, but could find this, has it been renamed?\n\n[https://play.google.com/store/apps/details?id=com.pocketpalai&hl=en\\_GB](https://play.google.com/store/apps/details?id=com.pocketpalai&hl=en_GB)",
                  "score": 2,
                  "created_utc": "2025-12-30 09:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwgfcio",
          "author": "worldrobotdomination",
          "text": "I just tried to run\n\n    ollama run hf.co/LiquidAI/LFM2-2.6B-Exp-GGUF\n    \n    in powershell and it replied \n    \n    Error: 500 Internal Server Error: llama runner process has terminated: error loading model: missing tensor 'output_norm'\n    \n    So does anyone know when LFM2-2.6B-Exp-GGUF will be able to run in Ollama?",
          "score": 1,
          "created_utc": "2025-12-29 00:07:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl7q4e",
              "author": "rainbyte",
              "text": "I think you need to specify which quant you want.\n\nTry something like this:\n\n```\nollama run http://hf.co/LiquidAI/LFM2-2.6B-Exp-GGUF:Q8_0\n```\n\nModel is working here with llama.cpp engine.",
              "score": 1,
              "created_utc": "2025-12-29 18:36:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwm16e4",
                  "author": "worldrobotdomination",
                  "text": "From what I found out, the LFM models are not yet available to run on Ollama 0.13.5, tho some say they will run on 0.13.4? Also I have not installed llama.cpp.\n\nSo I decided to switch models to QWEN 2.5 3B for now, to experiment, supposed to be good for writing Javascript?",
                  "score": 1,
                  "created_utc": "2025-12-29 20:58:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvx8i3j",
          "author": "TomLucidor",
          "text": "Until a new architecture can punch above their peer (at the 8B and 14B ranges), it's a very big whatever. Ditto for Diffusion LLMs.",
          "score": 0,
          "created_utc": "2025-12-25 20:57:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy28wi",
              "author": "jazir555",
              "text": "Well this does punch above its weight for a 2.6B model, if the techniques are transferrable to larger models that's not far off.",
              "score": 6,
              "created_utc": "2025-12-26 00:04:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwybg6",
      "title": "llama.cpp, experimental native mxfp4 support for blackwell (25% preprocessing speedup!)",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/",
      "author": "bfroemel",
      "created_utc": "2025-12-27 13:52:16",
      "score": 87,
      "num_comments": 24,
      "upvote_ratio": 0.98,
      "text": "[https://github.com/ggml-org/llama.cpp/pull/17906](https://github.com/ggml-org/llama.cpp/pull/17906)\n\nlove that kind of evolution:\n\n\\> at the moment this PR is ~~10%~~ *~~slower~~* ~~than master~~ 25% faster than master on PP.\n\n\\> To compile `-DCMAKE_CUDA_ARCHITECTURES=\"120f\"` is required.\n\nprobably/currently most useful for gpt-oss models! (also while reading the PR it seems that we might see more native nvfp4 support soon!)\n\nThanks to u/am17an (PR author) & llama.cpp devs!!\n\n/edit: better point that also out (although, so far I am not noticing any quality degradation with gpt-oss-120b!):  \n\\> \\[..\\] we quantize activation to mxfp4 instead of q8, which lead to failures in `test-backend-ops`, however PPL tests are okay with this change (though not ruling out correctness issues)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nw6yy8n",
          "author": "Time_Reaper",
          "text": "This pr literally broke blackwell support lol. A lot of people are unable to compile since this got merged",
          "score": 21,
          "created_utc": "2025-12-27 14:18:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7bf0i",
              "author": "TableSurface",
              "text": "Yup. Also found that GGML_NATIVE=OFF has to be specified to get it to compile in my docker env.\n\nOnce it compiles, the performance improvement is impressive though! Can confirm 29-33% prompt processing improvement with gpt-oss 120b as advertised.",
              "score": 7,
              "created_utc": "2025-12-27 15:31:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nw72b2b",
              "author": "malaiwah",
              "text": "\\`-DCMAKE\\_CUDA\\_ARCHITECTURES=120a\\` (\\`120f\\` didn't work for me) makes it work for me even on the tip of master. Let me share my \\`Containerfile\\`\n\n    mbelleau@aibeast:~$ cat /mnt/vault/llm/llama-router/Containerfile\n    # syntax=docker/dockerfile:1\n    FROM docker.io/nvidia/cuda:13.1.0-devel-ubuntu24.04 as build\n    ENV DEBIAN_FRONTEND=noninteractive\n    RUN apt-get update && apt-get install -y --no-install-recommends \\\n        git build-essential cmake ninja-build ca-certificates \\\n        libcurl4-openssl-dev \\\n        && rm -rf /var/lib/apt/lists/*\n    RUN useradd -m llama\n    USER llama\n    WORKDIR /home/llama\n    RUN git clone https://github.com/ggerganov/llama.cpp.git && \\\n        git config --global --add safe.directory /home/llama/llama.cpp\n    # Create build directory with cache mount\n    RUN mkdir -p /home/llama/llama.cpp/build && \\\n        cd /home/llama/llama.cpp && \\\n        cmake -B build -S . -DGGML_CUDA=ON -DGGML_NATIVE=OFF -DLLAMA_CURL=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DCMAKE_CUDA_ARCHITECTURES=120a -DCMAKE_LIBRARY_PATH=/usr/local/cuda-13.1/compat && \\\n        cmake --build build --config Release -j --clean-first\n    USER root\n    RUN mkdir -p /home/llama/llama.cpp/build && \\\n        cd /home/llama/llama.cpp/build && \\\n        make install\n    USER llama\n    ## Default port for llama.cpp server\n    EXPOSE 8080\n    # Entry point: llama.cpp server\n    ENTRYPOINT [\"/usr/local/bin/llama-server\"]",
              "score": 5,
              "created_utc": "2025-12-27 14:39:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwcd0i2",
                  "author": "LegacyRemaster",
                  "text": "cmake -B build -G \"Visual Studio 17 2022\" -A x64 -DGGML\\_CUDA=ON -DGGML\\_FLASH\\_ATTN=ON -DGGML\\_RPC=ON -DGGML\\_BACKEND\\_DL=OFF -DLLAMA\\_CURL=OFF -DCMAKE\\_CUDA\\_ARCHITECTURES=\"120a\" -DGGML\\_CUDA\\_FA\\_ALL\\_QUANTS=true <--- works",
                  "score": 1,
                  "created_utc": "2025-12-28 10:19:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwcbnkz",
              "author": "am17an",
              "text": "Take it up with Nvidia who decided to put the most performant thing on a Blackwell behind a set of only partially supported flags by cmake. And non forward compatible",
              "score": 1,
              "created_utc": "2025-12-28 10:07:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw77q56",
          "author": "FinBenton",
          "text": "Gonna give it some time to mature but super excited if we see big speedups!",
          "score": 4,
          "created_utc": "2025-12-27 15:11:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw7ghy0",
          "author": "mearyu_",
          "text": "There are some other mxfp4 quants on [https://huggingface.co/noctrex](https://huggingface.co/noctrex)",
          "score": 4,
          "created_utc": "2025-12-27 15:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7r8ff",
              "author": "bfroemel",
              "text": "about that: not an expert myself and haven't seen any benchmark/experience reports yet, however there seems to be currently two ways to do mxfp4 quants: PTQ (post training quantization, simple, potential high accuracy degradation compared to bf16, low compute) and QAT (quantization aware training, less than 1-3% accuracy degradation compared to bf16, retraining/fine-tuning 1-3 full epochs/relativelyhigh compute).\n\nFor agentic/coding/reasoning/math use-cases you would absolutely want QAT-based, high-accuracy MXFP4 quants; for other use-cases it might not matter so much. Most mxfp4 quants on HF seem to be PTQ; llama.cpp mxfp4 quantization is PTQ-based.\n\nMore details: [https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/) (hint: besides the compute requirement, retraining/fine-tuning is not so simple and/or necessarily the same for all models)\n\nWould highly appreciate if there is someone who really knows this stuff could comment/correct or even confirm my view; especially if this would also apply to nvfp4.\n\n/edit: accuracy claims",
              "score": 5,
              "created_utc": "2025-12-27 16:51:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw7qe1q",
              "author": "panchovix",
              "text": "Does he accept requests? Like GLM 4.5 or 4.6 (he did but only for air ones)",
              "score": 1,
              "created_utc": "2025-12-27 16:47:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw735gb",
          "author": "a_beautiful_rhind",
          "text": "*Now* making those quants is probably worth it.",
          "score": 1,
          "created_utc": "2025-12-27 14:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw78n3i",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 3,
              "created_utc": "2025-12-27 15:16:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw7kvvo",
                  "author": "am17an",
                  "text": "Where did you get this information from?",
                  "score": 3,
                  "created_utc": "2025-12-27 16:19:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nw78z6m",
                  "author": "a_beautiful_rhind",
                  "text": "TIL. So even now it's a bit pointless.",
                  "score": 2,
                  "created_utc": "2025-12-27 15:18:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwavn38",
          "author": "DataGOGO",
          "text": "While cool, what I really want is NVFP4, not MXFP4.¬†",
          "score": 1,
          "created_utc": "2025-12-28 03:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwcdcj4",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/tpra70si8x9g1.png?width=656&format=png&auto=webp&s=52ed41c29979d92d6a7b4b1f933ad3d351fd91c7\n\nyes it works! Blackwell 96gb 6000",
          "score": 1,
          "created_utc": "2025-12-28 10:23:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp3aia",
              "author": "Odd-Ordinary-5922",
              "text": "any reason you arent using vllm since you can easily fit the 120b fully in vram?",
              "score": 1,
              "created_utc": "2025-12-30 07:58:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwd09vy",
          "author": "danishkirel",
          "text": "Still necessary to do a custom compile? This is gpt oss 20b on 3090 and 5060ti - I‚Äôm surprised by the prompt processing of the latter (Blackwell) card.\n\nhttps://preview.redd.it/bkjvyayx6y9g1.jpeg?width=2532&format=pjpg&auto=webp&s=8bfe6a6bf5956b47de6bd7fab630f94fca4bc890",
          "score": 1,
          "created_utc": "2025-12-28 13:36:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwd33ex",
              "author": "am17an",
              "text": "Yea, unfortunately the pre-built releases don't have this yet. We're working on adding them",
              "score": 3,
              "created_utc": "2025-12-28 13:55:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf84kf",
                  "author": "danishkirel",
                  "text": "Ok nice. So this could mean the 5060 ti could get even more competitive?",
                  "score": 1,
                  "created_utc": "2025-12-28 20:26:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw6xwti",
          "author": "AdamDhahabi",
          "text": "And for those who are used to pick up a pre-compiled release on Github? A runtime flag would be nice?",
          "score": -3,
          "created_utc": "2025-12-27 14:12:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwbr7np",
              "author": "LA_rent_Aficionado",
              "text": "Build from source or bust",
              "score": 1,
              "created_utc": "2025-12-28 06:51:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvke55",
      "title": "llama.cpp's recent updates - --fit flag",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/",
      "author": "pmttyji",
      "created_utc": "2025-12-25 19:09:25",
      "score": 86,
      "num_comments": 26,
      "upvote_ratio": 0.99,
      "text": "Haven't updated llama.cpp for last 2 weeks. Liked the new CLI after last time update.\n\nWanted to mention these PRs.\n\n[llama: automatically set parameters not set by the user in such a way that maximizes GPU utilization #16653](https://github.com/ggml-org/llama.cpp/pull/16653) \\- I was waiting for this one. Looks like this one got merged already & also few more related PRs too done with fixes. How many of you used `--fit` flag on your llama.cpp commands? Please share your stats on this(Would be nice to see before & after results).\n\n[ggml : optimize cuda cumsum fallback (\\~2.5x speedup vs CUB) #18343](https://github.com/ggml-org/llama.cpp/pull/18343) \\- This one is from latest update. (As a non-techie) I have no idea what this is & how it works. But the number in title \\~2.5x looks nice. PR don't have t/s results with before & after. Somebody please share details on this. I have 4060 Laptop GPU(8GB VRAM).\n\nEDIT:\n\n[Previous thread](https://www.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/) from this sub on 1st PR topic. Sorry I had very less context/memory on this one.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pvke55/llamacpps_recent_updates_fit_flag/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvwtisz",
          "author": "suicidaleggroll",
          "text": "I found the results were consistently worse than just manually setting n-cpu-moe.",
          "score": 27,
          "created_utc": "2025-12-25 19:25:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvx4fml",
              "author": "Chromix_",
              "text": "Or manually offloading ffn for dense models. There was quite a bit of discussion and testing for that in the [original thread](https://www.reddit.com/r/LocalLLaMA/comments/1pn2e1c/comment/nu6wfxy/) about that feature. It's nice for \"just quickly run it\" without further tweaking parameters manually. It'd be even nicer if results would be on-par with manual parameter setting.",
              "score": 10,
              "created_utc": "2025-12-25 20:31:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxekyx",
                  "author": "Remove_Ayys",
                  "text": "That was probably due to the ordering of which tensors were moved first/last to VRAM, that should no longer be the case on the latest master commit after [https://github.com/ggml-org/llama.cpp/pull/18148](https://github.com/ggml-org/llama.cpp/pull/18148)",
                  "score": 8,
                  "created_utc": "2025-12-25 21:34:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvxlg58",
                  "author": "panchovix",
                  "text": "I think it works fine enough but not on the case when offloading on a multiGPU setup to CPU.\n\nLike some GPUs for some reason use 4GB more after first gen and then it keeps at that, but -fit doesn't take that in mind yet. But also not sure how it could be done either.",
                  "score": 3,
                  "created_utc": "2025-12-25 22:16:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvz4jae",
              "author": "jacek2023",
              "text": "Maybe it would make sense to collect cases where manual tuning outperforms fit and post them.\nThat way u/Remove_Ayys could analyze why the fit algorithm allocates tensors differently.",
              "score": 5,
              "created_utc": "2025-12-26 04:26:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvzx0fz",
                  "author": "Remove_Ayys",
                  "text": "If you want to do that, start a discussion on Github. I categorically refuse to do software development via Reddit, Discord, etc.",
                  "score": 7,
                  "created_utc": "2025-12-26 08:43:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvy9o6t",
              "author": "Certain_Advantage_51",
              "text": "Been using --fit for about a week now and honestly same experience here, manual tuning still gives better performance on my setup",
              "score": 3,
              "created_utc": "2025-12-26 00:50:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvz6ik3",
              "author": "ForsookComparison",
              "text": "Phew. Getting the perfect combo of layers in vs out and Tensor split has become my latest hobby.",
              "score": 1,
              "created_utc": "2025-12-26 04:41:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvwy7dh",
          "author": "Aggressive-Bother470",
          "text": "-fit should default to off, IMHO.\n\n\nKinda annoying to discover all this new shit toggled on, flags changed, old args now running at minus 10x :D",
          "score": 34,
          "created_utc": "2025-12-25 19:53:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxac1d",
              "author": "datbackup",
              "text": "Agreed‚Ä¶ but it‚Äôs not like i‚Äôm forced to download and use the new version. Would be nice if they had an arg that let you pass a llama build number and interpreted all other args in such a way as to make inference take place in exactly the same way as in that build number. Would be a bear to implement though, time better spent elsewhere perhaps.",
              "score": 3,
              "created_utc": "2025-12-25 21:08:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvxmkvm",
                  "author": "a_beautiful_rhind",
                  "text": "not forced until a new architecture comes out",
                  "score": 7,
                  "created_utc": "2025-12-25 22:23:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvwrew8",
          "author": "jacek2023",
          "text": "There was a post about the first one here.",
          "score": 6,
          "created_utc": "2025-12-25 19:13:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwt6hk",
              "author": "pmttyji",
              "text": "My bad. Just found [that thread](https://www.reddit.com/r/LocalLLaMA/comments/1pn2e1c/llamacpp_automation_for_gpu_layers_tensor_split/). Don't know how I forgot that one(And I even left a comment there). Maybe I should take some time to touch grass & sleep. \n\nBut still not many people shared their results. Maybe in this thread ....",
              "score": 5,
              "created_utc": "2025-12-25 19:23:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw06kq1",
              "author": "pmttyji",
              "text": "Looks like everyone forgot to notice the 2nd one. You found any gains on performance?",
              "score": 1,
              "created_utc": "2025-12-26 10:23:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvy7hpf",
          "author": "Amazing_Athlete_2265",
          "text": "I use the flags all the time. Can confirm they work really well after playing around with the settings a bit. The only models it has trouble with are vision models, it looks like the fit logic doesn't account for the added extra size of the mmproj gguf.\n\nMy defaults settings are: --fit on --fit-target 512 --fit-ctx 16384. This works well for all models except vision models where I override the --fit-target setting (typically between 1024 and 2048 work well).\n\nI notice only slight speed improvement. Before the fit logic was added, I had a script that used to figure out correct -ngl and -ncmoe flags using llama-bench. This new way is so much better, and completely automatic. Love it.",
          "score": 4,
          "created_utc": "2025-12-26 00:37:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvykkxs",
              "author": "slavik-dev",
              "text": "Yes, I have same OOM crash with vision. There are few issues opened in Github for it:\n\n[https://github.com/ggml-org/llama.cpp/issues/18111](https://github.com/ggml-org/llama.cpp/issues/18111)  Eval bug: llama-fit-params does not include vision stack in calculations",
              "score": 1,
              "created_utc": "2025-12-26 02:05:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvxoh2z",
          "author": "DrVonSinistro",
          "text": "I get no measurable token generation speed difference between b7508 and b7540.\n\n\\--fit gave me a 5-8 t/s bump for QWEN3 Next but didn't change anything on QWEN3 235B Q4",
          "score": 3,
          "created_utc": "2025-12-25 22:35:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxs7v4",
              "author": "giant3",
              "text": "On AMD GPUs, I haven't seen much improvement in almost 6-8 months. The project is so active, yet no practical gain for users.",
              "score": -4,
              "created_utc": "2025-12-25 22:59:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvydcg9",
                  "author": "DrVonSinistro",
                  "text": "On CUDA the improvements have been great but also the coherence of the inferences.  CUDA is all the rage but a huge % of dev time seem to be on Vulkan. So hang on.",
                  "score": 2,
                  "created_utc": "2025-12-26 01:15:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvxvrxb",
          "author": "Magnus114",
          "text": "When should I use --fit-ctx? Is it enough to just set ctx?",
          "score": 2,
          "created_utc": "2025-12-25 23:22:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvy7u61",
              "author": "Amazing_Athlete_2265",
              "text": "--fit-ctx sets the *minimum* context size. llama.cpp will first try to load in the model with full GPU offloading and maximum context size the model can take. If this doesn't fit, it first reduces context size but not below --fit-ctx. if the model still doesn't fit, it starts to oflload to CPU\n\nAt least thats how I *think* it works",
              "score": 5,
              "created_utc": "2025-12-26 00:39:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw2lsji",
                  "author": "Magnus114",
                  "text": "Thanks. But I‚Äôm still unsure if I should set both -c and ‚Äîfit-ctx, or of it‚Äôs enough with one of them.",
                  "score": 1,
                  "created_utc": "2025-12-26 19:39:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvzmmif",
          "author": "CabinetNational3461",
          "text": "I created a thread on llamacpp official github titled Major performance drop since b7406 https://github.com/ggml-org/llama.cpp/issues/18258. Apparently fit default is on and some model get major t/s hit. I also got some issue with model that used to work fine now get OOM. I agree with post above that fit default should be off. I currently still using b7406 until these 2 issue get resolved.",
          "score": 2,
          "created_utc": "2025-12-26 06:58:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvztqhd",
              "author": "tmvr",
              "text": "This would explain the weirdness I've seen when upgrading to 7426 for Nemotron 3 Nano. On Windows I don't get OOM, but what happened was that for some models and combinations of ctx and ncmoe it started using GPU Shared Memory. basically it loaded everything and even if GPU Dedicated Memory usage was only for example 20GB it also used 2-3GB shared memory instead of filling dedicated first.",
              "score": 1,
              "created_utc": "2025-12-26 08:08:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1puzo82",
      "title": "Merry Christmas! üéÑ üéÅ",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/",
      "author": "Rare_Carry9799",
      "created_utc": "2025-12-24 23:03:48",
      "score": 83,
      "num_comments": 16,
      "upvote_ratio": 0.85,
      "text": "Merry Christmas! ü•≥",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvsj9ik",
          "author": "indicava",
          "text": "Happy holidays everyone! \n\nMay your tokens per second stay high and your power draw low",
          "score": 26,
          "created_utc": "2025-12-24 23:13:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsjea3",
              "author": "Rare_Carry9799",
              "text": "woooo ü•≥",
              "score": 2,
              "created_utc": "2025-12-24 23:14:07",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw1c7qa",
              "author": "Leading-Werewolf2010",
              "text": "Amen to that lol, nothing worse than watching your GPU turn into a space heater while your inference crawls at 2 tok/s",
              "score": 1,
              "created_utc": "2025-12-26 15:38:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsjtg4",
          "author": "jacek2023",
          "text": "Merry Christmas and many new local  LLMs! üéÑüéÑüéÑ",
          "score": 7,
          "created_utc": "2025-12-24 23:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtflad",
          "author": "Perfect_Biscotti_476",
          "text": "Merry Christmas to everyone üéÑHope we all have more and more VRAM in the coming year",
          "score": 4,
          "created_utc": "2025-12-25 03:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu08gt",
          "author": "Iron_Adamant",
          "text": "Merry Christmas to you too! I'm legit excited for what's coming next year",
          "score": 4,
          "created_utc": "2025-12-25 05:59:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsii7r",
          "author": "Adventurous-Gold6413",
          "text": "My Christmas present was honestly qwen image edit 2511",
          "score": 3,
          "created_utc": "2025-12-24 23:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsijom",
              "author": "Rare_Carry9799",
              "text": "fr üî•",
              "score": 2,
              "created_utc": "2025-12-24 23:08:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvszjkt",
          "author": "International-Try467",
          "text": "Z BASE!!!",
          "score": 2,
          "created_utc": "2025-12-25 01:08:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvujcj7",
          "author": "LicensedTerrapin",
          "text": "AI slop detected. üòÜ Merry Christmas!",
          "score": 2,
          "created_utc": "2025-12-25 09:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvsuse0",
          "author": "Beb_Nan0vor",
          "text": "¬†ü§ó¬†",
          "score": 1,
          "created_utc": "2025-12-25 00:33:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtehx7",
          "author": "IrisColt",
          "text": "Thanks!!!",
          "score": 1,
          "created_utc": "2025-12-25 03:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvteisv",
              "author": "exclaim_bot",
              "text": ">Thanks!!!\n\nYou're welcome!",
              "score": 1,
              "created_utc": "2025-12-25 03:01:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuawwl",
          "author": "Tiny_Judge_2119",
          "text": "https://preview.redd.it/osyegthk1b9g1.png?width=512&format=png&auto=webp&s=4a0bd9c3fb8bc55a64998959759c28a38aed4ea3",
          "score": 1,
          "created_utc": "2025-12-25 07:44:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuxoo8",
          "author": "BreakfastFriendly728",
          "text": "gguf when?",
          "score": 1,
          "created_utc": "2025-12-25 11:51:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvgega",
              "author": "tarruda",
              "text": "Might get Minimax M2.1 today according to u/Wise_Evidence9973: https://www.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/nvpj3in/",
              "score": 1,
              "created_utc": "2025-12-25 14:27:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzcj1q",
      "title": "Tencent open-source Tencent-HY-MT1.5, featuring two translation models‚Äî1.8B and 7B‚Äîdesigned for seamless on-device and cloud deployment with industry-leading speed and accuracy",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/gallery/1pzcj1q",
      "author": "Difficult-Cap-7527",
      "created_utc": "2025-12-30 08:11:09",
      "score": 79,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "New Model",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1pzcj1q/tencent_opensource_tencenthymt15_featuring_two/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwpyzs2",
          "author": "jamaalwakamaal",
          "text": "The smaller 1.8B one is excellent for Indonesian and Brazilian Portuguese. Decent for casual use in Hindi and not good for Urdu. For such small size, it is just awesome! I love it.¬†\n\n\nEdit: Tried the gguf quantization from tencent. Unbelievable results for Hindi. Something unheard for its size. Solid model.",
          "score": 7,
          "created_utc": "2025-12-30 12:39:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqt7yx",
              "author": "Available-Damage-924",
              "text": "Very good for Malay too.",
              "score": 2,
              "created_utc": "2025-12-30 15:33:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwp928v",
          "author": "Illya___",
          "text": "Missing gemma 3 and mistral, those 2 are good as well. Generally agree with the ratings tho, deepseek is good for translation. Gemini 3 Pro is worse than 2.5 Flash for translation.",
          "score": 7,
          "created_utc": "2025-12-30 08:52:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp6mrp",
          "author": "Odd-Ordinary-5922",
          "text": "this is exactly what I was looking for a year ago. Will test it out",
          "score": 7,
          "created_utc": "2025-12-30 08:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqbgvb",
          "author": "GuiBiancarelli",
          "text": "I was just looking for something like this! Gonna test it soon, thank you for the post!",
          "score": 3,
          "created_utc": "2025-12-30 13:58:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrczoz",
          "author": "Different-Toe-955",
          "text": "This is the cool stuff AI can do that connects people!",
          "score": 3,
          "created_utc": "2025-12-30 17:06:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puh2lw",
      "title": "[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/",
      "author": "Psychological_Box406",
      "created_utc": "2025-12-24 06:59:18",
      "score": 76,
      "num_comments": 20,
      "upvote_ratio": 0.89,
      "text": "Following up on my previous post comparing [GLM 4.7 and Minimax M2.1](https://www.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/) on a task.  \nFirst, I got some valid feedback on the comments saying that this sub is specifically about local models, not API subscriptions. Fair point. But both of these models are fully hostable locally. Many people don't have the infrastructure or resources to self-host, so I think sharing real-world performance data, even from API usage, is still valuable for those who do. The results apply regardless of whether you run them on someone's servers or your own hardware.\n\nThat said, something interesting came up while I was checking my billing history on Z.ai...\n\nLooking at yesterday's session costs, I realized something crucial: **It didn't just use GLM 4.7.** The billing breakdown shows multiple models were used during that 70min session:\n\n* glm-4.5-air\n* glm-4.7\n* glm-4.5\n* glm-4.6\n\nThis means their platform was automatically routing across different model versions, not just hitting GLM 4.7 consistently.\n\nCould this automatic model routing be why the performance wasn't good?\n\nThose self-hosting it locally will likely see better performance since they're using a single model version without the routing shuffle.\n\nhttps://preview.redd.it/ottux5r6n39g1.png?width=1123&format=png&auto=webp&s=e4a0d33ee5e79a01023b8e1a97341dde9bfe0cd1\n\n",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvojkm0",
          "author": "nuclearbananana",
          "text": "I don't know why glm 4.5 & 4.6 got used but claude code auto switches between the main model (sonnet or glm 4.7) and a lighter model (haiku or glm air) for cost & speed",
          "score": 20,
          "created_utc": "2025-12-24 07:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvovgcq",
              "author": "NightRare",
              "text": "Claude Code uses haiku as a support agent in the background to do some simple stuff such as naming the current session, checking if a command to execute is safe etc. User won't see this but it is doing it along the session.\n\nI bet OP set ANTHROPIC\\_DEFAULT\\_HAIKU\\_MODEL to GLM-4.5-Air",
              "score": 10,
              "created_utc": "2025-12-24 09:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvp5sq6",
                  "author": "evia89",
                  "text": "I think CC use 3/4 models, no? \n\nhaiku35 for stuff like naming session\n\nhaiku45 for background explore code agent\n\nopus45 is u enable **opusplan** mode\n\nsonnet 45 for code",
                  "score": 1,
                  "created_utc": "2025-12-24 10:43:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvptshc",
              "author": "Mealydiversity",
              "text": "Ah that makes way more sense, probably load balancing between the main model and the lighter versions when traffic gets heavy. The inconsistent routing would definitely mess with performance compared to just hitting one model consistently",
              "score": 1,
              "created_utc": "2025-12-24 13:53:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvol496",
          "author": "nontrepreneur_",
          "text": "Did you explicitly set these environment variables in ~/.claude/settings.json?\n\nANTHROPIC_DEFAULT_OPUS_MODEL: GLM-4.7\nANTHROPIC_DEFAULT_SONNET_MODEL: GLM-4.7\nANTHROPIC_DEFAULT_HAIKU_MODEL: GLM-4.5-Air\n\nhttps://docs.z.ai/scenario-example/develop-tools/claude#faq",
          "score": 25,
          "created_utc": "2025-12-24 07:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvp3fgz",
              "author": "Psychological_Box406",
              "text": "I'm using GLM only by setting these two:  \nexport ANTHROPIC\\_BASE\\_URL=  \nexport ANTHROPIC\\_AUTH\\_TOKEN=",
              "score": 3,
              "created_utc": "2025-12-24 10:20:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvqs8ry",
                  "author": "a-wiseman-speaketh",
                  "text": "They mentioned optimizing for Claude Code - if I was them I would have matching endpoints on that anthropic facade for CC default settings that I could route to my own models. You might have to explicitly override.\n\n  \nI am surprised that it used the smaller models so much though.",
                  "score": 3,
                  "created_utc": "2025-12-24 17:05:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvric9l",
              "author": "quinncom",
              "text": "To be clear: it's not recommended to set these variables unless you want to force specific models. If you don‚Äôt set them, Claude Code will just use Z.AI‚Äôs latest default mapping (currently GLM‚Äë4.7 for Sonnet/Opus, 4.5-Air for Haiku).\n\nThe docs linked above say:\n\n> If you want to use the latest default mappings (for existing users who have configured old model mappings), simply delete the model mapping configuration in settings.json, and Claude Code will automatically use the latest default models.",
              "score": 2,
              "created_utc": "2025-12-24 19:28:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvor58f",
          "author": "Reddactor",
          "text": "My 2 cents:  Thanks for the reviews!  Any model that can be run locally is fair game. Just because it also has an API shouldn't disqualify it!\n\nWhy? Maybe you want to buy a Rig that can run a +200B model locally. Why wouldn't you first want to test it out via API before buying a fortune in GPUs?",
          "score": 23,
          "created_utc": "2025-12-24 08:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvqn4s7",
          "author": "LoveMind_AI",
          "text": "I've just kind of passed over GLM-4.7 for now. Not into it. I'm probably missing out but the vibe doesn't work for me. Intellect-3 is, right now, the most useful iteration of the GLM line for me and I can run it in 6bit MLX on my MacBook. \n\nOn the local vs. cloud access front, my $.02 is that as long as we're talking about models that can run locally, and especially models with permissive licenses, doing the evaluation on the cloud isn't any kind of deadly sin. Obviously, folks rent cloud compute all the time.\n\nBeing elitist about hardware ownership seems like it should be kind of antithetical to the vibe of r/LocalLLaMA. Of course, geeking out over beautiful local rigs is a fun part of the tradition, but valuable contributions can be made by working with open models on the cloud. (And if major models that were most easily accessed via API weren't OK on here, I'm not sure why we would have had an AMA with Z.ai!)",
          "score": 4,
          "created_utc": "2025-12-24 16:38:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrb9xy",
          "author": "SillyLilBear",
          "text": "I've brought this up before, you only get the frontier model about 40% of the time in my experience, even if you specifically choose 4.7 (4.6 at the time I tested).",
          "score": 3,
          "created_utc": "2025-12-24 18:48:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvor2z8",
          "author": "Lissanro",
          "text": "I saw your previous post, thanks for the follow up. It is always interesting to know about experience with local models, but when cloud is used, I am always a bit skeptical. In this case it turned out to be an example of why cloud model experience does not necessary translate to running locally. Routing issues are not the only possibilities, I remember quite a few times people hitting various bugs in cloud providers or just getting worse results like from not very well quantized model.\n\nThat said, in many cases testing local model via cloud API can be valid way to check the model, but need to be extra cautious and aware of possible issues that may invalidate this kind of testing.",
          "score": 5,
          "created_utc": "2025-12-24 08:17:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvtd1dl",
          "author": "Real_Principle_8470",
          "text": "I don't think it is the cause. Poor result is poor result. You plan with Sonnet 4.5 then execute with haiku 4.5 you can still get very good result. The GLM  product is shit even comparing to grok code fast 1",
          "score": 2,
          "created_utc": "2025-12-25 02:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvu4d7w",
          "author": "harrypham2000",
          "text": "Lol, I just notice the drop in quality that it's easier to get worse then I notice Zai auto switching to lower tier model when i exceed the Rate Limit. I subscribe to the GLM Lite Plan, seems like should be in Pro Plan or so on\n\nhttps://preview.redd.it/hbwxzs0qpa9g1.png?width=1785&format=png&auto=webp&s=6e82693bca972f542b7253d387ebae6154cb46aa\n\nSubscribed to both MiniMax coding plan and GLM Coding Plan, seems like MiniMax coding plan far more stable",
          "score": 2,
          "created_utc": "2025-12-25 06:38:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu9m2l",
              "author": "Psychological_Box406",
              "text": "I have Minimax Coding plan and GLM Pro plan.  \nI am not using GLM anymore. I've just done that session to see if they got better with 4.7, but nope.",
              "score": 1,
              "created_utc": "2025-12-25 07:31:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvuau5r",
                  "author": "harrypham2000",
                  "text": "I started using MiniMax since they first got the coding plan, the scale got up stabilized, I never got some obsurb errors like GLM do. The experience stays the same during the session, but yeah, 27$ for a GLM Lite Year plan was still such a budget. But I think GLM team has much more to do if this keeps going without any clear explanation \n\nhttps://preview.redd.it/gicxre7f1b9g1.png?width=1763&format=png&auto=webp&s=7761cdf90e2efe9ba3f56ec883469eb57a145ab1",
                  "score": 1,
                  "created_utc": "2025-12-25 07:44:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs2rvo",
          "author": "Ronaldo433",
          "text": "Where did you get this usage page by model usage.",
          "score": 1,
          "created_utc": "2025-12-24 21:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvu9yls",
              "author": "Psychological_Box406",
              "text": "[https://z.ai/manage-apikey/billing](https://z.ai/manage-apikey/billing)\n\nOn the tab 'Billing History'",
              "score": 1,
              "created_utc": "2025-12-25 07:34:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvuhfho",
          "author": "crantob",
          "text": "And yet your post has zero relevance to running these models locally.\n\n\nYour post is a pure online service issue. \n\n\nOfftopic.",
          "score": 1,
          "created_utc": "2025-12-25 08:54:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1puwi5o",
      "title": "Deepseek will release a larger model next year",
      "subreddit": "LocalLLaMA",
      "url": "https://www.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/",
      "author": "power97992",
      "created_utc": "2025-12-24 20:25:01",
      "score": 72,
      "num_comments": 48,
      "upvote_ratio": 0.87,
      "text": "THis is old news but, I forgot to mention this before.\n\nThis is from section 5, [https://arxiv.org/html/2512.02556v1#S5](https://arxiv.org/html/2512.02556v1#S5) \\-\" First, due to fewer total training FLOPs, the breadth of world knowledge in DeepSeek-V3.2 still lags behind that of leading proprietary models. We plan to address this knowledge gap in future iterations by scaling up the pre-training compute.\"\n\nI speculate it will be bigger than  1.6T params(maybe 1.7-2.5T) and have 95B-111B active params and at least  trained 2.5-3x more tokens than now... Hopefully they will releases the weights for this. I also hope for a smaller version(maybe it won't happen)..\n\n\"¬†Second, token efficiency remains a challenge; DeepSeek-V3.2 typically requires longer generation trajectories (i.e., more tokens) to match the output quality of models like Gemini-3.0-Pro. Future work will focus on optimizing the intelligence density of the model‚Äôs reasoning chains to improve efficiency. Third, solving complex tasks is still inferior to frontier models, motivating us to further refine our foundation model and post-training recipe.\"\n\n\\- They will increase the efficiency of its reasoning ie it will use less thinking tokens  than before for the same task .\n\nAlso they will improve its abilities solving complex task, this probably means  better reasoning and agentic tooling",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/",
      "domain": "self.LocalLLaMA",
      "is_self": true,
      "comments": [
        {
          "id": "nvrsnsq",
          "author": "FullstackSensei",
          "text": "How does scaling up compute translate into a larger model?!!!",
          "score": 45,
          "created_utc": "2025-12-24 20:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp6jxc",
              "author": "Exotic-Blood-2381",
              "text": "More compute usually means they can afford to train bigger models or train existing ones longer - both cost way more FLOPs so makes sense they'd go larger if they're planning to throw more resources at it",
              "score": 0,
              "created_utc": "2025-12-30 08:29:18",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvrxip0",
              "author": "Tman1677",
              "text": "[Chinchilla](https://arxiv.org/abs/2203.15556)",
              "score": -6,
              "created_utc": "2025-12-24 20:57:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvrxsxp",
                  "author": "FullstackSensei",
                  "text": "The Chinchilla paper is so 2023. Everyone has been going orders of magnitude of the Chinchilla recommendation for literally two years now.",
                  "score": 25,
                  "created_utc": "2025-12-24 20:58:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvrv149",
              "author": "power97992",
              "text": "There is a limit on how many tokens you can fit in one parameter....It doesn't remember more info if you keep the parameters same and post train it on more compute, it will just abstract  or generalize more of the info or  some info become more reinforced or even worse it forgets some of the older info, but the total breadth of knowledge remains more or less the same. Even if u train a model from scratch with more compute and tokens but keep the params the same, it will generalize better but the total knowledge will remain around the same if the architecture remains the same.",
              "score": -15,
              "created_utc": "2025-12-24 20:42:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrwq6j",
                  "author": "FullstackSensei",
                  "text": "Who do I get the feeling you're making up a lot of things without any understanding of the foundational math?",
                  "score": 22,
                  "created_utc": "2025-12-24 20:52:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvrtmj5",
          "author": "KvAk_AKPlaysYT",
          "text": "GGUF wen?",
          "score": 21,
          "created_utc": "2025-12-24 20:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrz4l5",
              "author": "silenceimpaired",
              "text": "lol.",
              "score": 3,
              "created_utc": "2025-12-24 21:06:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvryneh",
              "author": "power97992",
              "text": "PRobably in late January or February 2026.",
              "score": 1,
              "created_utc": "2025-12-24 21:03:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs143y",
          "author": "5138298",
          "text": "\"Second, token efficiency remains a challenge; DeepSeek-V3.2 typically requires longer generation trajectories (i.e., more tokens) to match the output quality of models like Gemini-3.0-Pro. Future work will focus on optimizing the intelligence density of the model‚Äôs reasoning chains to improve efficiency\"\n\nHow are we jumping straight to the 'larger model' conclusion? Ofc the meta these days are just keep scaling up everything, training data and model size. But what do i know.",
          "score": 5,
          "created_utc": "2025-12-24 21:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs2ko0",
              "author": "power97992",
              "text": "THis-First, due to fewer total training FLOPs, the breadth of world knowledge in DeepSeek-V3.2 still lags behind that of leading proprietary models. We plan to address this knowledge gap in future iterations by scaling up the pre-training compute.\" -More knowledge breadth  usually means more tokens and more parameters. MOre training tokens means more compute and same with more params.",
              "score": 3,
              "created_utc": "2025-12-24 21:27:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvrz361",
          "author": "silenceimpaired",
          "text": "Thank goodness! I couldn‚Äôt use DeepSeek locally unless I spent some real money‚Ä¶ now I need unreal amounts of money.",
          "score": 7,
          "created_utc": "2025-12-24 21:06:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrzdr6",
              "author": "power97992",
              "text": "Already no one can run deepseek V3.2 q8  locally at >12tk/s unless they shell out a lot of money",
              "score": 3,
              "created_utc": "2025-12-24 21:08:09",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nvtni9a",
              "author": "ForsookComparison",
              "text": "Yes the problem with Deepseek was that reasonable quants were within reach of some very poor financial decisions.\n\nIf they release a much larger model, our imaginations should quiet-up much quicker",
              "score": 1,
              "created_utc": "2025-12-25 04:12:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtolnn",
                  "author": "silenceimpaired",
                  "text": "Thank goodness they have taught me contentment‚Ä¶ or despair.",
                  "score": 1,
                  "created_utc": "2025-12-25 04:21:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvu8lc5",
                  "author": "power97992",
                  "text": "Running Q4 v3.2 at 17-21t/s is reachable if someone shelled out 9.5k. Yeah too much money‚Ä¶ ¬†running q4 Ds v4.0 /v3.5 will be unimaginable at 16-20t/s without an absurd amount of money",
                  "score": 0,
                  "created_utc": "2025-12-25 07:20:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvvmlqx",
          "author": "eXl5eQ",
          "text": "They said \"scaling up the pre-training compute\", not \"scaling up the model\". This onlys means they will pre-train the model with more tokens and tops.\n\nVery likely it will have exactly the same structure as DS 3.2 and just named as DS 3.3",
          "score": 3,
          "created_utc": "2025-12-25 15:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvrvrhf",
          "author": "ImportancePitiful795",
          "text": "Well they better put pressure on CXMT to make cheap memory fast. The only way to run this properly at home is via Intel AMX with a Xeon 6980P ES, 2TB RAM, 4 R9700s and ktransformers. ü§î",
          "score": 4,
          "created_utc": "2025-12-24 20:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrx4qy",
              "author": "power97992",
              "text": "Yeah there is no way to run a +1.5T A90-115B Q8  model locally at >10tk/s  unless you have at least 27k...MAybe q4 you can run it on 2 mac studios for 19k..  CXMT might make cheaper  RAM but it won't be fast though",
              "score": 3,
              "created_utc": "2025-12-24 20:54:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvs3pa6",
                  "author": "ImportancePitiful795",
                  "text": "CXMT already announced that they can make 8000 DDR5 and 10000 LPDDR5X just in October. \n\nWithout been fussy about RAM speeds atm, 6980P (the ES is around ‚Ç¨2500) can get to 620GB/s with 6400Mhz RAM (12 channel). That's perfectly fine tbh. \n\nAnd yes 6980P can get to 850GB/s with MCRDIMM using 8000Mhz kits.",
                  "score": 2,
                  "created_utc": "2025-12-24 21:33:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nvs3ur4",
                  "author": "a_beautiful_rhind",
                  "text": "A90 isn't gonna do the mac studio any favors.",
                  "score": 1,
                  "created_utc": "2025-12-24 21:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvrutkj",
          "author": "Guardian-Spirit",
          "text": "Scaling compute ‚â† scaling model.\n\nSo it's hard to say, really. Because it seems like just making the model bigger doesn't necessarily translate to better quality.\n\nHowever, I actually believe that next DeepSeek could be bigger just because of DeepSeek Sparse Attention. Not sure if it makes training cheaper, though.",
          "score": 5,
          "created_utc": "2025-12-24 20:41:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvrvvf7",
              "author": "power97992",
              "text": "They said they will increase the breadth of world knowledge, Changing the architecture will only increase the breadth a little bit;  the only way to increase  it significantly is to increase the training tokens and the parameters.",
              "score": 2,
              "created_utc": "2025-12-24 20:47:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvrxh0g",
                  "author": "Guardian-Spirit",
                  "text": "\"Scaling pre-training compute\" usually means scaling training tokens, parameters, or both.  \nIt's indeed possible to scale compute by lowering parameters and increasing training tokens drastically.\n\nIncreasing number of parameters \\*always\\* helps to some extent at our point of development, but it has diminishing results and is not always that useful.\n\nCompare proprietary Gemini 3 Flash and Gemini 3 Pro. Pro is clearly larger, yet Flash outperforms Pro on few benchmarks and gets very close on most.",
                  "score": 2,
                  "created_utc": "2025-12-24 20:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs1nep",
          "author": "ortegaalfredo",
          "text": "I hope GGUF q0.001 is ready by then.",
          "score": 2,
          "created_utc": "2025-12-24 21:21:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs9s0a",
              "author": "power97992",
              "text": "Just use minimax or glm air lol..",
              "score": 7,
              "created_utc": "2025-12-24 22:10:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs7wwt",
          "author": "power97992",
          "text": "I hate to tell you guys but they will keep scaling training tokens and parameters and compute. In a few years, we will be looking at open weight 6-18T param models. Internally, some companies will have 50-120T models and they might serve them for those who can afford it and they will serve a smaller cheaper version .. Maybe they will make a breakthrough in a few years and make the models smaller and smarter ¬†with continual learning but then again it will be attached to a massive RAG DB ¬†and/or have a massive context window and to search fast, you will be back to storing it on RAM",
          "score": 4,
          "created_utc": "2025-12-24 21:59:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nvsf96i",
          "author": "FullOf_Bad_Ideas",
          "text": "You could train a diffusion LLM with 685B A37B size on 100x the compute they used for DeepSeek V3 without overfitting.\n\nMore training FLOPs and bigger breadth of world knowledge does not necessarily equal bigger model. It is likely, but not certain, that what they meant is a bigger model.\n\nThey would still need to find compute to inference it with, I think DeepSeek aims to provide a free chatbot experience powered by their leading model for a foreseeable future.",
          "score": 2,
          "created_utc": "2025-12-24 22:46:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs416k",
          "author": "a_beautiful_rhind",
          "text": "Claude opus at home.",
          "score": 1,
          "created_utc": "2025-12-24 21:35:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvs4u0k",
              "author": "power97992",
              "text": "Next year, there will be Opus 4.7,5.0, 5.1, and... The competition is fierce. But yeah if u have the money, u will be able to run a massive deepseek model.",
              "score": 2,
              "created_utc": "2025-12-24 21:40:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1puxg7h",
      "title": "MiniMax M2.1 scores 43.4% on SWE-rebench (November)",
      "subreddit": "LocalLLaMA",
      "url": "https://i.redd.it/s0vbt46vt79g1.jpeg",
      "author": "Fabulous_Pollution10",
      "created_utc": "2025-12-24 21:10:50",
      "score": 71,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvs5omi",
          "author": "Atzer",
          "text": "Devstral small is incredible for its size.",
          "score": 25,
          "created_utc": "2025-12-24 21:45:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsg3hp",
              "author": "uti24",
              "text": "Why are they even near? Devstral small 24B, K2 1T\n\nMaybe benchmaxing or some kind of data contamination?",
              "score": 7,
              "created_utc": "2025-12-24 22:51:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsomng",
                  "author": "robogame_dev",
                  "text": "1. Devstral is a pure agentic coding optimized model, those others are much more generalists.  \n2. All models use the same scaffolding, if your model isn't particularly used to it, it will perform a lot worse than if your model has been extensively trained on it. That's not the same as benchmaxing (they still have to actually solve the problems) but it's easier to solve problems when you're fully fluent in the scaffold.",
                  "score": 10,
                  "created_utc": "2025-12-24 23:50:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvtudwe",
              "author": "DistanceAlert5706",
              "text": "Indeed, excellent model for it's size. Finally something I can truly use with 32gb VRAM in coding agent for small tasks when need privacy.",
              "score": 3,
              "created_utc": "2025-12-25 05:08:10",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nvtimrg",
              "author": "neotorama",
              "text": "I mean it‚Äôs good compared to Gemini 3 Pro.",
              "score": 1,
              "created_utc": "2025-12-25 03:33:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs2sx0",
          "author": "LeTanLoc98",
          "text": "Could you consider adding Kimi K2 Thinking?",
          "score": 6,
          "created_utc": "2025-12-24 21:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvxmbl3",
              "author": "annakhouri2150",
              "text": "I'd also appreciate that. IME K2T is far, far better than any other OSS model for agentic coding.",
              "score": 3,
              "created_utc": "2025-12-25 22:22:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvsdart",
          "author": "Few_Painter_5588",
          "text": "The jump from Deepseek R1 0528 to 3.2 is insane. Though Devstral 123B and devstral small are also strong contenders here.",
          "score": 4,
          "created_utc": "2025-12-24 22:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs9gix",
          "author": "LeTanLoc98",
          "text": "Wow, Devstral Small 24B better than Minimax M2",
          "score": 8,
          "created_utc": "2025-12-24 22:08:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvs2isv",
          "author": "ortegaalfredo",
          "text": "This benchmark aligns a lot with my own internal benchmarks about logic problems and code comprehension. Also GLM-4.7/Minimax M2.1 are still not better than Deepseek 3.2-Speciale/Kimi K2 Thinking, but similar than regular DS 3.2. The surprise here is Devstral.",
          "score": 6,
          "created_utc": "2025-12-24 21:26:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvspdee",
              "author": "robogame_dev",
              "text": "This matches my experience vis a vis GLM vs MiniMax - MiniMax is faster, but GLM has cleaner chain of thought which gives it the edge on longer horizon agentic tasks IMO. I expect 4.7 to move up several spaces from 4.6 with it's new interleaved thinking and tool calling, and improved terminal fluency.",
              "score": 6,
              "created_utc": "2025-12-24 23:55:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvt0o3s",
          "author": "usernameplshere",
          "text": "Devstral Small beating Qwen 3 Coder 480B, Grok Code Fast, R1 and M2 is absolutely mental. I find it to be interesting that the 123B model is only slightly better than the small version. This makes me wonder on how much both differ in real world tasks, I should give small a go ig.\n\nIt's also very interesting that the best OSS models barely beat GPT 5 mini medium. This is kinda what I experienced as well in my usage. Especially Raptor Mini (GPT 5 mini finetune by github on ghcp) beats, sadly, all OSS models I've tried yet.",
          "score": 3,
          "created_utc": "2025-12-25 01:16:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvwm06j",
              "author": "power97992",
              "text": "That is because gpt 5 mini is comparable in size to many of the top OSS models(probably at least 200B-400B). GPT 5.0 thinking  itself is   at least 1.29T-2T  parameters(active >43B-86B)  considering their profit margins are 70%(of course this is assuming the avg sub user uses around 2mil tokens/m)  , probably much larger than that if you assume they are batching outputs..",
              "score": 2,
              "created_utc": "2025-12-25 18:42:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw01vn0",
                  "author": "usernameplshere",
                  "text": "Yeah, I've been writing a comment somewhere like that as well some days or weeks ago. GPT 5 Mini and it's derivatives feels like at least the size of the current GLM 4.x models. I've not tested Gemini 3 Flash enough yet, but it also feels quite large for its name (and considering that Gemini 1 nano 2 was 3.25B) and follows instructions very well I would say it is also somewhere that size.",
                  "score": 1,
                  "created_utc": "2025-12-26 09:34:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvs4ae2",
          "author": "power97992",
          "text": "Are u sure devstral is that good?",
          "score": 5,
          "created_utc": "2025-12-24 21:37:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsf5k2",
              "author": "DreamingInManhattan",
              "text": "I've been running it non-stop since it was released. Yes, it's that good. Fast, too.",
              "score": 10,
              "created_utc": "2025-12-24 22:45:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvswsz9",
                  "author": "noiserr",
                  "text": ">  Fast, too.\n\nMinimax M2 runs faster for me. Which hardware are you running it on?",
                  "score": 3,
                  "created_utc": "2025-12-25 00:47:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsom9y",
              "author": "zkstx",
              "text": "Let's delve into this excellent question! \n\nDevstral Small 2 is not just a mere toy‚Äîit's an actually useful tool. Its benchmarks are not just a testament of open source coding LLMs catching up to their closed source counterparts‚Äîit's a model you can actually run on an average gaming PC. In fact, whenever I run it, my GPU makes noises barely above a whisper, sending tokens down the PCIe. \n\nSummary: It will have a place in the tapestry of viable vibe coding choices for the next few weeks to come.\n\n\nOkay, slop memeing aside, I haven't had the time to comprehensively test this one yet. Considering it's dense you can't really expect to get speeds comparable to something like a 30A3. Still, one should probably note that there are not that many agentic coding-specific models that fit onto a single consumer GPU, so I am happy to see Mistral put out a model to help fill that niche and I wouldn't be surprised if it's truly one of the best options among these for now.",
              "score": 8,
              "created_utc": "2025-12-24 23:50:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvtmezs",
                  "author": "dtdisapointingresult",
                  "text": "I am comforted by your style of writing and shall upvote your post.\n\nHowever, in the future please remember to include more mentions of how insightful I am for asking the question. I would also like to frequently be reminded of how I'm absolutely right.\n\nThank you for your attention to this matter!",
                  "score": 7,
                  "created_utc": "2025-12-25 04:03:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsotk2",
              "author": "robogame_dev",
              "text": "I used Devstral for 60 million tokens worth of dev when it was in \"stealth mode\" and I was convinced it was a SOTA high-param count model... blew my mind to see it was only 123B params...",
              "score": 2,
              "created_utc": "2025-12-24 23:52:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvu7sry",
                  "author": "power97992",
                  "text": "It is 123b dense, it‚Äôs not an MoE. ¬†I used the free version in openrouter, the code it gave was pretty meh. I guess it doesnt complete ¬†the task in a single prompt whereas speciale and minimax 2.1 would. In fact , even after three prompts , it couldn‚Äôt get it right‚Ä¶¬†",
                  "score": 1,
                  "created_utc": "2025-12-25 07:12:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvsyg7n",
              "author": "ortegaalfredo",
              "text": "Tried the bigger version because it was free and...it wrote code not as good as Claude but basically it never failed. It works. For anything that is not code it sucks, obviously.",
              "score": 1,
              "created_utc": "2025-12-25 00:59:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvs71ue",
          "author": "oxygen_addiction",
          "text": "What is \"Claude Code\" at the top position? How is Sonnet above Opus in both 4.5/4.5 and 4/4.1?\n\nHow can anyone take that seriously?",
          "score": 2,
          "created_utc": "2025-12-24 21:54:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvsh7gq",
              "author": "FullOf_Bad_Ideas",
              "text": "it's a contamination-free benchmark with results that you can reproduce.\n\nIf you don't like what you see, you can run it on your own and verify their numbers.",
              "score": 4,
              "created_utc": "2025-12-24 22:59:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nvsji45",
                  "author": "Tuned3f",
                  "text": "claude code is an agent harness, not a model\n\nshouldn't even be on the list",
                  "score": 5,
                  "created_utc": "2025-12-24 23:14:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvsoso7",
          "author": "LegacyRemaster",
          "text": "I don't doubt the tests are accurate, but my personal use case gives me different results. I just fixed an annoying bug in an Android UI that Sonnet doesn't even understand. And if we look at the data released by Minimax, this has actually been optimized in 2.1. As always, I suggest testing the specific use case. Real life Vs numbers",
          "score": 1,
          "created_utc": "2025-12-24 23:51:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}